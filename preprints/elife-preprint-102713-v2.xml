<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">102713</article-id>
<article-id pub-id-type="doi">10.7554/eLife.102713</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102713.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>A stimulus-computable rational model of visual habituation in infants and adults</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Raz</surname>
<given-names>Gal</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
<email>galraz@mit.edu</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Cao</surname>
<given-names>Anjie</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Saxe</surname>
<given-names>Rebecca</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Frank</surname>
<given-names>Michael C</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Department of Brain and Cognitive Sciences, MIT</institution></institution-wrap>, <city>Cambridge</city>, <country country="US">United States</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap>, <city>Stanford</city>, <country country="US">United States</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ymca674</institution-id><institution>McGovern Institute for Brain Research, MIT</institution></institution-wrap>, <city>Cambridge</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country country="GB">United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors contributed equally to this work.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-01-08">
<day>08</day>
<month>01</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-09-29">
<day>29</day>
<month>09</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP102713</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-09-09">
<day>09</day>
<month>09</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-08-24">
<day>24</day>
<month>08</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.21.609039"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-01-08">
<day>08</day>
<month>01</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102713.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.102713.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.102713.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.102713.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Raz et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Raz et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-102713-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>How do we decide what to look at and when to stop looking? Even very young infants engage in active visual selection, looking less and less as stimuli are repeated (habituation) and regaining interest when novel stimuli are subsequently introduced (dishabituation). The mechanisms underlying these looking time changes remain uncertain, however, due to limits on both the scope of existing formal models and the empirical precision of measurements of infant behavior. To address this, we developed the Rational Action, Noisy Choice for Habituation (RANCH) model, which operates over raw images and makes quantitative predictions of participants’ looking behaviors in a classic visual habituation paradigm. In a series of pre-registered experiments, we exposed infants and adults to stimuli for varying durations and measured looking time to familiar and novel stimuli. We found that these data were well captured by RANCH. Using RANCH’s stimulus-computability, we also tested its out-of-sample predictions about the magnitude of dishabituation in a new experiment in which we manipulated the similarity between the familiar and novel stimulus. By framing looking behaviors as rational decision-making, this work identified how the dynamics of learning and exploration guide our visual attention from infancy through adulthood.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>This is a revised version of the manuscript after having received reviews from eLife. eLife asked us to update the preprint upon revision.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>From birth, humans engage in active learning. Even before they gain independent mobility, infants make choices about what to look at and when to stop looking [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c44">44</xref>]. Developmental psychologists have both studied this process of visual decision-making as a phenomenon itself and also relied on it as a key methodological tool. Two key phenomena have been especially important: habituation and dishabituation. Habituation occurs when infants show reduced interest upon repeated exposure to the same stimulus, whereas dishabituation entails renewed interest following the subsequent introduction of a novel stimulus. Dishabituation, also known as novelty preference, is often leveraged to infer infants’ perceptual and cognitive abilities [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c17">17</xref>]: in order for an infant to dishabituate, they must distinguish between the original stimulus and the novel one. Despite the robust documentation of habituation and dishabituation in the literature, the mechanisms underlying these changes in gaze duration remain poorly understood. In this paper, we bridge this gap by introducing a rational model of visual habituation and dishabituation. We show that this rational model quantitatively predicts looking times toward different stimuli in visual habituation paradigms.</p>
<p>An important early conceptual model of infant looking time by Hunter and Ames posited that habituation and dishabituation are influenced by the amount of information to be encoded by the infants [<xref ref-type="bibr" rid="c26">26</xref>]. Initially, infants are assumed to look longer at stimuli containing learnable, unprocessed information, showing a familiarity preference shortly after initial exposure (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). As exposure accumulates, less information remains unprocessed, leading to shorter looking time at the original stimulus; by comparison, relatively longer looking times to a new stimulus are predicted (i.e. novelty preference). While this model has had significant influence, the absence of specificity regarding the encoding process and a way of measuring the amount of information to be encoded in a stimulus have both permitted the model to be used in post-hoc explanations of looking time measurements. Due to this lack of specificity, researchers can allude to this model to argue that infants fell on the part of the preference curve most consistent with the observed data pattern. This interpretive ambiguity has spurred repeated concerns regarding whether looking time measurements should form the basis for key assertions in developmental psychology [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c40">40</xref>]. To address this criticism, developmental psychology needs formal models that make quantitative predictions, beyond the qualitative foundations of the Hunter and Ames [<xref ref-type="bibr" rid="c26">26</xref>] model.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Conceptual models of looking to familiar vs. novel items according to three models of infant attention.</title>
<p>(A) The partial encoding model proposed by Hunter and Ames [<xref ref-type="bibr" rid="c26">26</xref>] which suggests a shift from familiarity to novelty preferences, (B) The Goldilocks model proposed by Kidd et al [<xref ref-type="bibr" rid="c30">30</xref>] which suggests that infants prefer to attend to intermediately surprising events, and (C) the ‘optimal curiosity’ model introduced by Cao et al [<xref ref-type="bibr" rid="c10">10</xref>] which suggests that infants are maximizing expected information gain from noisy perceptual samples.</p></caption>
<graphic xlink:href="609039v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Recent computational models have taken steps towards formalizing the mechanisms underlying looking time changes. One family of models characterizes infants’ looking behaviors through information-theoretic metrics derived from ideal observer models: the models acquire probability distributions from event sequences and derive information-theoretic measures from the ideal learner’s belief before and after each event [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c42">42</xref>]. For instance, Kidd, Piantadosi, and Aslin [<xref ref-type="bibr" rid="c30">30</xref>] found that an ideal learner’s surprisal at an event was related to infants’ looking behavior via a U-shaped curve: infants were least likely to look away when the surprisal of the event was neither too high nor too low (<xref rid="fig1" ref-type="fig">Figure 1B</xref>, model predictions derived in Supplementary Information, Figure 8).</p>
<p>While these models provide quantitative fits to variation in looking time, they are not models of how an agent makes the decisions of whether to keep looking at the same stimulus or to look at something else. Rather than producing the target behavior directly, these models describe correlations between model-derived, information-theoretic measures and infants’ overall looking towards whole sequences of events. Relatedly, these models do not explain why infants would ever look at one single stimulus for a long time, rather than encode it instantaneously – there is no account of perceptual noise to be overcome by taking multiple perceptual samples that would justify why looking extends over time [c.f. 9, <xref ref-type="bibr" rid="c29">29</xref>]. Owing to this constraint, these models can only be used to predict infant behaviors in a specific paradigm: looking to a continuous stream of discrete stimuli. Thus, existing models are challenging to generalize to the more common looking time paradigms, in which infants gradually habituate to an individual object or event, and then dishabituate to a new exemplar.</p>
<p>To address these issues, we developed the Rational Action, Noisy Choice for Habituation (RANCH) model [<xref ref-type="bibr" rid="c10">10</xref>]. While habituation is a broadly studied phenomenon across cognitive domains – including language acquisition, probabilistic learning, and concept formation – our focus here is on visual habituation, where infants adjust their attention based on repeated exposure to a visual stimulus. Notably, this domain contrasts with the event-based paradigms used in prior modeling work [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c42">42</xref>], but corresponds well with a substantial portion of the infant habituation literature.</p>
<p>RANCH describes an agent’s looking behavior as rational exploration based on a sequence of noisy perceptual samples. The model construes the looking time paradigm as a series of binary decisions: to keep sampling from the current stimulus, or to look at anything else. The model makes sampling decisions based on the Expected Information Gain (EIG) of the upcoming perceptual sample, choosing to keep looking or look away based on which one would in expectation yield the most information. The RANCH model is therefore a rational analysis of looking behavior [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c1">1</xref>]: looking is analyzed as optimally adapted to extract information from the environment, given specific constraints on processing (e.g., perceptual noise).</p>
<p>Crucially, RANCH is the first stimulus-computable model of habituation, allowing us to derive quantitative predictions from raw visual stimuli. Previous theoretical accounts have described broad principles of habituation, but they do not generate testable, trial-by-trial predictions of looking behavior. As a result, direct comparisons between RANCH and these models remain challenging: existing models do not specify how an agent decides when to continue looking or disengage, nor do they provide a mechanistic link between stimulus properties and looking time. By explicitly modeling these decision processes, RANCH moves beyond post-hoc explanations and offers a computational framework that can be empirically validated and generalized to new contexts.</p>
<p>We previously validated this rational analysis using a large dataset of adults engaging in a self-paced looking paradigm. We asked adults to look at sequences of stimuli consisting of a repeating familiar stimulus (resulting in habituation), and a single, novel stimulus after varying exposure durations (resulting in dishabituation). Effects of prior exposure and novelty on adults’ looking time were well captured by RANCH [<xref ref-type="bibr" rid="c10">10</xref>]. <xref rid="fig1" ref-type="fig">Figure 1C</xref> shows schematic predictions of RANCH for the experimental setting described above: variable prior exposure to a familiar stimulus, followed by a novel or a familiar stimulus. Rational exploration of a simple perceptual concept results in a monotonic decrease of attention to familiar items as a function of prior exposure.</p>
<p>Here we directly tested hypotheses about the processes underlying infants’ looking time. To do so, we addressed three challenges. A first critical challenge for differentiating formal theories of looking time in infants is that looking time measurements are typically imprecise estimates, due to low test-retest reliability and small sample sizes [<xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c36">36</xref>]. In the current work, we developed a novel, within-subject experimental design that allowed us to sensitively measure infants’ looking time in experiments. In our first experiment, we used this paradigm to measure the dynamics of habituation and dishabituation across different amounts of exposure to a stimulus. Beyond validating RANCH’s predictions in infants, this paradigm enabled us to conduct interpretable developmental comparisons of parameter fits between infants and adults – since we can conduct matched experiments in adults – thereby identifying the processes underlying looking time that may change over development.</p>
<p>A second critical challenge for differentiating theories has been their stimulus-dependency. Researchers can rarely make quantitative predictions about how people would respond to different visual stimuli; even previous computational models have relied on using arbitrary representations that were not directly generated from the stimulus materials [e.g. object location, 30, hand-specified binary feature vector, <xref ref-type="bibr" rid="c10">10</xref>]. To address this issue, we incorporated recent progress in convolutional neural networks (CNNs) to allow RANCH to learn from raw images. The representations of many trained CNNs show striking similarities to how primate brains respond to stimuli, and investigating the representations of CNNs has therefore offered insights into how the visual system encodes objects [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c53">53</xref>]. The activations of these brain-inspired neural networks form embedding spaces, each of which can be seen as a quantitative hypothesis about how humans embed objects in a lower-dimensional perceptual space [<xref ref-type="bibr" rid="c46">46</xref>]. In RANCH, we leveraged these principled stimulus representations to enable RANCH to learn from raw images. In other words, RANCH can ‘see’ an experiment in a way similar to infants, and make predictions for each unique trial.</p>
<p>A final challenge is the generalizability of models to new contexts. Because RANCH can be applied across stimulus sets without additional assumptions, we were able to test its predictions in a new dataset. To conduct these tests, we fit RANCH’s parameters to the dataset from the initial habituation-dishabituation experiment. Then, we used the best-fitting parameters to generate predictions for a new experiment designed to measure differences in dishabituation magnitude. In this experiment, we systematically varied the similarity between the familiar and novel stimulus such that the novel stimulus differed in its pose angle, number, identity, or animacy. This experiment tested a prediction derived from many theories of habituation and dishabituation [<xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c19">19</xref>]: that observers’ dishabituation magnitude should be related to the similarity between the habituated stimulus and the novel stimulus. The more dissimilar two stimuli are, the more one should dishabituate to the novel stimulus.</p>
<p>Addressing these three challenges allowed us to empirically test competing hypotheses about habituation and dishabituation using our experimental data (<xref rid="fig1" ref-type="fig">Figure 1</xref>). However, because existing models do not generate quantitative predictions, we could not directly compare RANCH to alternative computational models. Instead, we evaluated whether RANCH accurately captured key behavioral patterns in looking time.To preview our results, we found that RANCH provided the best description of behavior in Experiment 1, in which we systematically varied prior exposure to a stimulus before measuring looking time to either the same, familiar stimulus or a novel stimulus. Fitting RANCH to infant and adult data separately and comparing the best-fitting parameters revealed that infants’ learning was tuned for noisier processing than adults. Then, using these best fitting parameters, we generated predictions for a novel task used in Experiment 2 in which we varied the similarity between the familiar and novel stimulus. RANCH generalized to a novel task context by predicting the dishabituation magnitude as a function of stimulus dissimilarity. Together, these model fits provide evidence that habituation and dishabituation to sequential visual stimuli are well described by a rational analysis of looking time. All materials, including the code and data, are available at the project OSF page: <ext-link ext-link-type="uri" xlink:href="https://osf.io/d7jz8/">https://osf.io/d7jz8/</ext-link></p>
</sec>
<sec id="s2">
<label>2</label>
<title>Rational Action, Nosy Choice for Habituation model</title>
<p>The Rational Action, Noisy Choice for Habituation model (RANCH) is a Bayesian perception and action model in which a learning model is used to derive optimal perceptual sampling decisions [<xref ref-type="bibr" rid="c10">10</xref>]. In RANCH, the learning problem in habituation is conceptualized as learning a visual concept – a single representation underlying a group of observed stimuli. The model learns by observing a series of noisy perceptual samples from each stimulus. During learning, the model makes decisions about how many samples to receive from each stimulus before moving to the next stimulus. We treat the number of samples for each stimulus as being linearly related to looking time duration.</p>
<p>RANCH provides a probabilistic framework for making perceptual decisions. In this section, we will describe the three modular components of RANCH: the perceptual representation, the learning model, and the decision model.</p>
<sec id="s2a">
<label>2.1</label>
<title>Perceptual representations</title>
<p>We used the deep convolutional neural network (CNN) ResNet-50 to encode the stimuli used in behavioral experiments. ResNet-50 is a standard architecture that is widely used in computer vision as well as in comparisons to human behavior [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c32">32</xref>]. A total of 50 layers are implemented in this architecture: it starts with a convolution layer, includes several groups of residual blocks that help preserve information through the network, and ends with a global average pooling layer and a fully connected layer for classification. We used a version of ResNet-50 that was pretrained on ImageNet [<xref ref-type="bibr" rid="c13">13</xref>]. Training on ImageNet optimizes the ResNet-50’s activations for image classification, and we can use these activations as a perceptual embedding space. When we pass new stimuli through a pre-trained network, we extract these activations to obtain principled embeddings of stimuli (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). We use the first three principal components of the embedding space to limit the dimensionality of the downstream learning problem (ResNet-50 is by no means the only possible method for deriving perceptual representations; instead it is a simple, standard choice. We also explored alternative models, such as a CNN trained on egocentric videos from the child’s perspective and CNN whose representations were aligned to human categorization behavior [<xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c33">33</xref>]. However, we did not find substantial differences between the embedding spaces provided by ResNet-50 and other (often more complicated) models (Supplementary Information, Figure 12).).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Three components of RANCH.</title>
<p>(A) Perceptual representation: we plotted the first two principal components of stimulus embeddings from ResNet-50 final layer activations, (B) Learning model: plate diagram for Gaussian concept learning from noisy observations, (C) Decision model: RANCH repeatedly samples until environmental EIG outweighs EIG from another sample from the same stimulus.</p></caption>
<graphic xlink:href="609039v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Learning model</title>
<p>RANCH models looking behaviors in habituation experiments as hierarchical Bayesian concept learning [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c51">51</xref>]. The concept to be learned is a multivariate normal distribution parameterized by <italic>µ, σ</italic>, which represents beliefs about the location and variance of the presented concept in the embedding space. The concept <italic>µ, σ</italic> generates exemplars <italic>y</italic>, which is equivalent to the stimulus seen by the human participants. But RANCH does not directly learn from the stimulus. Instead, it learns from the series of noisy perceptual samples <inline-formula><inline-graphic xlink:href="609039v3_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> generated by the exemplar. The noisy perceptual sample is corrupted by zero-mean gaussian noise, represented by ϵ. A plate diagram is shown in <xref rid="fig2" ref-type="fig">Figure 2B</xref>. At each time step, the model uses one perceptual sample to update its representation of the concept, following Bayes Rule:
<disp-formula id="ueqn1">
<graphic xlink:href="609039v3_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Decision model and linking hypothesis</title>
<p>The Bayesian learning model provides estimates of the target concept based on the observed data, but it does not contain a decision rule for whether to keep sampling from the same stimulus or to move on to the next stimulus. To make this decision, RANCH uses the expected information gain (EIG) of the next perceptual sample. EIG is a metric used under the rational analysis of information-seeking behavior [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c37">37</xref>], and is computed as the expected value of the KL divergence between the hypothetical posterior at the next timepoint <italic>t</italic> + 1 and the current posterior. Formally: <italic>EIG</italic> = <italic>E</italic><sub><italic>t</italic>+1</sub>(<italic>KL</italic>(<italic>P</italic> (<italic>µ, σ</italic> | <italic>z</italic><sub><italic>t</italic>+1</sub>), <italic>P</italic> (<italic>µ, σ</italic> | <italic>z</italic><sub><italic>t</italic></sub>))). RANCH then uses a softmax choice (with temperature = 1) between the EIG of the next sample and a constant, which is assumed to represent the amount of information gain provided by the environment when the learner looks away from the stimulus. An algorithm table describing the overall sampling procedure is shown in <xref rid="fig2" ref-type="fig">Figure 2C</xref>.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Implementation</title>
<p>RANCH has four free parameters: the learner’s priors over the concept to be learned (<italic>µ, σ</italic>), the learner’s priors over their perceptual noise (<italic>σ</italic><sub><italic>ϵ</italic></sub>) and the constant representing EIG from the environment (<italic>EIG</italic><sub><italic>env</italic></sub>). We conducted a grid search over these parameters.</p>
<p>While simple Gaussian concept learning models can be computed analytically, hierarchical models must be approximated numerically; the addition of perceptual noise to our model puts it in the latter category. We used a grid approximation for inference, creating discrete grids over <italic>µ, σ, y</italic> (exemplars) and <italic>z</italic> (noisy samples) to perform inference. Further, because the EIG computation requires computing an expectation over possible next observations, we approximated this expectation via a grid of possible next observations.</p>
<p>In the simulations below, we ran RANCH for each stimulus ordering. To remove approximation noise resulting from the grid approximations and the stochastic nature of the sampling algorithm, we averaged model results for each stimulus and parameter setting over 100 runs. To avoid bias, these 100 simulations used 20 different random perturbations to the approximation grids such that the exact location of the points in the grids was randomly shifted by small increments. We used the average number of samples the model decided to take from each stimulus as a proxy for looking time.</p>
<p>This process required significant computational resources. The grid approximation, combined with repeated simulations, demanded high-performance cluster GPUs to perform vectorized computation using a PyTorch implementation of our model. Running a single parameter setting through an experiment took approximately 10 hours on a GPU. By distributing different parameter settings across ∼15 GPUs at a time, a full parameter search took about 3 days to complete.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>RANCH predicts habituation and dishabituation in relation to exposure duration</title>
<p>The competing models of habituation and dishabituation shown in <xref rid="fig1" ref-type="fig">Figure 1</xref> make contrasting predictions with regard to the effect of increasing prior exposure on looking time. To directly test these predictions, we collected looking time data from both infants and adults in an experiment where we varied cumulative exposure to a familiar stimulus, and measured subsequent looking time to a novel or familiar stimulus. We then let RANCH “see” the same stimulus sequences used in our behavioral experiments and compared our model’s behavior to that of humans.</p>
<sec id="s3a">
<label>3.1</label>
<title>Behavioral experiments</title>
<sec id="s3a1">
<label>3.1.1</label>
<title>Infant experiment</title>
<p>We tested infants in a novel experimental paradigm in which exposure duration to the familiar stimulus was manipulated in a within-subject design shown in <xref rid="fig3" ref-type="fig">Figure 3</xref> (Experiment 1, Infants). Infants were recruited remotely and tested on Zoom video chat software; this testing method increased the efficiency of recruitment and has been shown to yield comparable effect sizes to in-person testing [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c11">11</xref>]. Each infant saw six blocks. In each block there was an exposure phase and a test trial. During the exposure phase, an animated animal (designated the familiar stimulus) was presented 1 to 9 times. Next, infants’ looking time was measured on a test trial. The test trial showed either the familiar stimulus, or a novel animal. We ran three versions of this experiment, each with three different numbers of exposure events. We tested a combined sample of 103 7-10 month old infants, with 31 infants in the first sample (0, 4 or 8 exposure events), 35 infants in the second sample (1, 3 or 9 exposure events) and 37 infants in the third sample (2, 4 or 6 exposure events; total n=103, <italic>M</italic><sub><italic>age</italic></sub> = 9.38 months, 47 female).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Experimental design for Experiments 1 and 2, infants and adults.</title>
<p>The vertical line separates the exposure phase and the test trial. Experiment 1 varied exposure to a stimulus and measured looking to familiar or novel stimuli. Experiment 2 varied how the familiar stimulus was violated, by changing the pose, identity, number or animacy of the stimulus. Infants had fixed prior exposure durations (5 second per exposure) and looking was measured until a 2-second lookaway. Adults responded on each trial via a keypress to continue to the next trial.</p></caption>
<graphic xlink:href="609039v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3a2">
<label>3.1.2</label>
<title>Adult experiment</title>
<p>We ran a self-paced looking time experiment on Prolific, collecting data from 470 adult participants (Experiment 1, Adults; see <xref rid="fig3" ref-type="fig">Figure 3</xref> for design). Stimuli used in the adult experiment were drawn from the same set used in the infant experiment. During the experiment, participants were asked to watch sequences of stimuli at their own pace, and they could press a key to proceed to the next stimulus whenever they wanted to. The time between the onset of the stimulus and the key press was used as a proxy for looking time. Each sequence began with a familiar stimulus repeating 1 to 10 times, followed by either the same familiar or a novel stimulus.</p>
</sec>
<sec id="s3a3">
<label>3.1.3</label>
<title>Modeling Procedure</title>
<p>RANCH received the ResNet-50-derived stimulus embeddings as input, presented in sequences mirroring the structure in our infant and adult experiments (<xref rid="fig3" ref-type="fig">Figure 3</xref>). To simulate a block of the infant experiment, the model was first presented with the “familiar” stimulus. For each stimulus, the model took 5 samples each and then moved on to the next stimulus. This phase was designed to be the exposure phase infants went through, since during the exposure phase infants had no control over the progression of the stimulus sequence. After the exposure phase, the model was presented with the test trial, which was either familiar or novel. We used the number of samples the model took on this test trial as a proxy for infant looking time. To simulate the self-paced adult experiment, we computed the number of samples taken by RANCH on every trial in each stimulus sequence.</p>
</sec>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Behavioral and model results</title>
<p>The paradigm successfully captured habituation in both infants and adults. In infants, as the number of trials in exposure phase increased, the looking time at the familiar stimulus in the test trials decreased (<italic>β</italic> = -4.55; <italic>SE</italic> = 0.92; <italic>t</italic> = -4.92; <italic>p</italic> &lt; 0.001). Infants clearly dishabituated on trials with longer exposures, looking longer at the novel stimulus than the familiar stimulus after long exposure (8 exposures: <italic>β</italic> = 0.82; <italic>SE</italic> = 0.22; <italic>t</italic> = 3.81; <italic>p</italic> &lt; 0.001, 9 exposures: <italic>β</italic> = 0.51; <italic>SE</italic> = 0.12; <italic>t</italic> = 4.21; <italic>p</italic> &lt; 0.001). The adult experimental paradigm also successfully captured habituation and dishabituation (Trial number: <italic>β</italic> = -0.01; Trial Type: <italic>β</italic> = 0.22; Both <italic>SE</italic> &lt; 0.001; Both <italic>p</italic> &lt; 0.001). We found no evidence for familiarity preferences or non-linearities in the shape of the dishabituation curve in infants or adults (cf. <xref rid="fig1" ref-type="fig">Figure 1A&amp;B</xref>).</p>
<p>RANCH predictions qualitatively matched habituation and dishabituation in both infants and adults. To quantitatively evaluate these predictions, we fit a linear model (adjusting model-generated samples by an intercept and scaling factor) and then assessed two complementary metrics. First, the root mean squared error (RMSE) captures the absolute error in the same units as looking time. Second, the coefficient of determination (<italic>R</italic><sup>2</sup>) measures the relative variation in looking time that is explained by the scaled model predictions. Since each metric relies on different assumptions and highlights distinct aspects of predictive accuracy, they together provide a more robust assessment of model performance. We minimized overfitting by employing cross-validation – using a split-half design for infant data and ten-fold for adult data – to compute both RMSE and <italic>R</italic><sup>2</sup> on held-out samples. Cross-validation provides numerical estimates of out-of-sample generalization performance that can be used for numerical model comparison.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Cross-validated RMSE across linking hypotheses and lesioned models.</title><p>Mean RMSE and <italic>R</italic><sup>2</sup> is averaged across all parameter values, best RMSE and <italic>R</italic><sup>2</sup> is the single best-performing parameter set.</p></caption>
<graphic xlink:href="609039v3_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Comparison with alternative linking hypotheses</title>
<p>Here we evaluated different ways of specifying RANCH’s decision-making mechanism (i.e., different “linking hypotheses” within RANCH). EIG is the optimal method for making sampling decisions [<xref ref-type="bibr" rid="c35">35</xref>]. We compared this method to two other information-theoretic measures proposed in prior literature: Kullback-Leibler divergence (KL-divergence) and surprisal. KL-divergence measures the information gained when one’s beliefs are updated, effectively quantifying “learning progress”, while surprisal captures how inconsistent incoming information is with prior expectations. Both have been shown to be associated with infants’ looking behaviors [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. We then repeated the cross-validation process using these linking hypotheses and compared the model’s performance based on the parameter that provides the best average fits to the test sets according to RMSE. Out of the three linking hypotheses, we found that EIG and KL-divergence were comparable to each other but surprisal-based RANCH performed worse in both populations (Table <bold>??</bold>). For both infants and adults, this poorer fit seems to have been primarily driven by the mismatch between our participants’ and RANCH’s looking to familiar stimuli (blue curves, <xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig5" ref-type="fig">5</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Experiment 1, Infant and RANCH behavior (different linking hypotheses and lesioned models).</title>
<p>The x-axis shows the number of prior exposures shown to infants and RANCH before measuring looking time / number of samples on the test trial, which was novel or familiar. The y-axis shows the mean looking time in seconds for the behavioral panel, and the scaled model samples for RANCH. The scaling procedure was applied to each linking hypotheses/lesioned model separately. We found evidence for habituation and dishabituation (after long prior exposures), but no evidence for familiarity preferences. Error bars show the standard error of the mean.</p></caption>
<graphic xlink:href="609039v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Experiment 1, Adult and RANCH behavior (different linking hypotheses and lesioned models).</title>
<p>For adults and the corresponding RANCH simulations, looking time was measured on every trial, shown on the x-axis. The y-axis shows the mean looking time in seconds for the behavioral panel, and the scaled model samples for RANCH. Similar to infants, we found evidence for habituation and dishabituation, but no evidence for familiarity preferences. Error bars show the standard error of the mean.</p></caption>
<graphic xlink:href="609039v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Comparison with lesioned model</title>
<p>We tested the importance of two of RANCH’s key components by creating lesioned models. In the first lesioned model, we removed noisy sampling by setting the learner’s noise prior, <italic>σ</italic><sub><italic>ϵ</italic></sub>, to 0 (“No noise”). In the second lesioned model, we removed the learning model, and replaced the EIG-based sampling with a random sampling policy (“No learning”). Both lesioned models provided poor fits to the behavioral datasets (Table <bold>??</bold>). This suggests that both noisy perception and the learning model are critical to RANCH’s performance.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>Parameter interpretation and developmental comparison</title>
<p>One advantage of RANCH is the interpretability of its parameters. Beyond finding that RANCH’s fit to the data is robust across parameters, we can also ask which specific parameters maximize that fit. In particular, comparing the best fitting parameters between infants and adults could provide insights into the origin of the developmental differences in behavior (see “Joint Scaling” under Methods for technical details). When investigating which parameters were most influential in achieving better fit, we found that <italic>σ</italic><sub><italic>ϵ</italic></sub>, the perceptual noise parameter, played a key role in improving fit. A high <italic>σ</italic><sub><italic>ϵ</italic></sub> improved fit to the infant data, but a low <italic>σ</italic><sub><italic>ϵ</italic></sub> improved fit to the adult data (Supplementary Information, Figure 11). Conceptually, <italic>σ</italic><sub><italic>ϵ</italic></sub> represents an agent’s beliefs about how much noise there is in their own perceptual input; the difference in <italic>σ</italic><sub><italic>ϵ</italic></sub> suggests that infants may have adjusted their learning behavior to higher perceptual noise.</p>
</sec>
<sec id="s3f">
<label>3.6</label>
<title>Discussion</title>
<p>Using within-subject measurements of looking time, we examined the sensitivity of looking to familiar vs. novel stimuli as a function of prior exposure. We did not find evidence for non-linearities predicted by prior models (<xref rid="fig1" ref-type="fig">Fig. 1A &amp; 1B</xref>), and instead found that habituation and dishabituation followed monotonic trends as predicted by RANCH (<xref rid="fig1" ref-type="fig">Fig. 1C</xref>). Although these non-linearities could in principle still remain, our experiments had substantially larger samples and more graded manipulations of exposure duration than previous experiments with infants or adults. Next, we tested whether RANCH can make predictions in a novel looking time experiment.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>RANCH predicts stimulus similarity-based dishabituation in infants and adults</title>
<p>Beyond predicting non-linearities in attentional preferences during encoding (<xref rid="fig1" ref-type="fig">Fig. 1B</xref>), Hunter and Ames [<xref ref-type="bibr" rid="c26">26</xref>] also predicted that after complete encoding of a stimulus, dishabituation magnitude should be linked to the similarity between two stimuli: the more dissimilar the novel stimulus is to the habituation stimulus, the more participants should dishabituate. This prediction is important theoretically because it underlies the use of looking time as a continuous measure of stimulus similarity (as in, e.g., [<xref ref-type="bibr" rid="c49">49</xref>]).</p>
<p>We designed an experiment testing this prediction by systematically varying the similarity between the familiar and novel stimuli within each block. RANCH provides an explicit notion of similarity through the use of ResNet50-derived image embeddings. We can therefore test whether RANCH also predicts such similarity-based differences in dishabituation magnitude. We further tested RANCH by assessing whether the best-fitting parameters in our previous experiment can be used to predict infants’ and adults’ looking time in a new experiment.</p>
<sec id="s4a">
<label>4.1</label>
<title>Behavioral Experiment</title>
<sec id="s4a1">
<label>4.1.1</label>
<title>Infant experiment</title>
<p>To study whether infants’ dishabituation was modulated by similarity between the habituation and novel stimulus, we ran an experiment, again recruiting via Zoom (n = 57, age range = 7-10 months, <italic>M</italic><sub><italic>age</italic></sub> = 8.99 months). We also ran an exact replication via Children Helping Science (CHS), an asynchronous online platform for online developmental data collection (Scott and Schulz [<xref ref-type="bibr" rid="c47">47</xref>]; n = 66, age range = 6-12 months, <italic>M</italic><sub><italic>age</italic></sub> = 9.69 months). We presented infants with a series of 4 blocks, each again consisting of a exposure phase and a test trial (<xref rid="fig3" ref-type="fig">Figure 3</xref>, Experiment 2, Infants). Instead of modifying prior exposure as in the previous experiment, here infants were always familiarized 8 or 9 times with an animal. We chose 8 and 9 exposures based on the results of Experiment 1, where these durations caused robust dishabituation. Moreover, using slightly variable exposure durations reduces the risk that infants develop fixed expectations about when a novel stimulus will appear. After the exposure phase, infants saw a test trial, which could relate to the familiar animal in one of five ways: they either saw the familiar animal again (familiar), the same animal facing in the opposite direction (pose), a different animal facing the same way (identity), two instances of the same animal side-by-side (number), or an inanimate vegetable or fruit (animacy).</p>
<p>In a separate experiment, we also tested baseline interest in these stimuli without prior exposure, to ensure that condition differences in the main experiments were due to violations, rather than intrinsic to the stimuli (n = 35, age range = 7-10 months, <italic>M</italic><sub><italic>age</italic></sub> = 9.21 months). This experiment revealed no baseline differences in looking time between categories (Supplementary Information, Figure 13).</p>
</sec>
<sec id="s4a2">
<label>4.1.2</label>
<title>Adult experiment</title>
<p>We tested adults in an experiment that was procedurally similar to Experiment 1 but whose stimuli were similar to the current infant experiment. Participants (N = 468) saw 24 blocks of animations, each of which was either 2, 4 or 6 trials long, and pressed a button to continue to the next trial in the sequence. 8 blocks showed a single, familiar stimulus repeatedly. 16 blocks always began with one stimulus shown 1, 3 or 5 times, and ended with another, novel stimulus. Unlike the infant experiment, which always started with a single animal, the adult experiment was long enough to show all permutations of familiar vs. novel blocks: Sequences could start with any stimulus type, i.e. either animals or vegetables, facing either left or right, presented either singly or in a pair. The novel stimulus then would violate one of the properties (pose, identity, number or animacy).</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Behavioral and model results</title>
<sec id="s4b1">
<label>4.2.1</label>
<title>Infant results</title>
<p>In the infant experiments, we found similar results for the samples tested synchronously over Zoom and asynchronously via CHS. For the Zoom sample, We fit a mixed effects model to predict log-transformed looking times from the violation type, and found that infants dishabituated to all novel test trials (identity: <italic>β</italic> = 0.33; <italic>SE</italic> = 0.16; <italic>t</italic> = 2.08; <italic>p</italic> = 0.04, number: <italic>β</italic> = 0.32; <italic>SE</italic> = 0.17; <italic>t</italic> = 1.92; <italic>p</italic> = 0.057, animacy: <italic>β</italic> = 0.33; <italic>SE</italic> = 0.16; <italic>t</italic> = 2.02; <italic>p</italic> = 0.046), except the pose violation, which elicited no significant dishabituation (pose: <italic>β</italic> = 0.13; <italic>SE</italic> = 0.16; <italic>t</italic> = 0.83; <italic>p</italic> = 0.406).</p>
<p>We applied the same mixed effects model to the CHS sample. We again found that animacy and number resulted in the strongest dishabituation in infants (animacy: <italic>β</italic> = 0.34; <italic>SE</italic> = 0.14; <italic>t</italic> = 2.47; <italic>p</italic> = 0.015, number: <italic>β</italic> = 0.35; <italic>SE</italic> = 0.13; <italic>t</italic> = 2.68; <italic>p</italic> = 0.008), and pose violations did not elicit dishabituation (<italic>β</italic> = 0.02; <italic>SE</italic> = 0.13; <italic>t</italic> = 0.14; <italic>p</italic> = 0.888). However, unlike the Zoom study, the identity violation did not elicit significant dishabituation in our CHS study (<italic>β</italic> = 0.07; <italic>SE</italic> = 0.14; <italic>t</italic> = 0.46; <italic>p</italic> = 0.646) (Note that we pre-registered a significant effect of identity violation on looking time as a data quality check, since we had replicated that effect in both infant experiments. However, surprisingly, the identity condition was the only condition that differed from the Zoom experiment, so here we deviated from pre-registration and report findings from this sample given the high value of having a second sample of infant data.).</p>
<p>We then combined the two infant datasets (total n = 123) to gain additional power. In an exploratory analysis, we ran the same linear model as on the individual datasets, since we found no additional variance explained by a random effect of dataset. We again found significant dishabituation to animacy and number violations (number: <italic>β</italic> = 0.34; <italic>SE</italic> = 0.11; <italic>t</italic> = 3.19; <italic>p</italic> = 0.002, number: <italic>β</italic> = 0.35; <italic>SE</italic> = 0.1; <italic>t</italic> = 3.37; <italic>p</italic> = 0.001), marginally significant dishabituation to identity violations (<italic>β</italic> = 0.19; <italic>SE</italic> = 0.11; <italic>t</italic> = 1.77; <italic>p</italic> = 0.078) and no dishabituation to the pose violation (<italic>β</italic> = 0.07; <italic>SE</italic> = 0.1; <italic>t</italic> = 0.65; <italic>p</italic> = 0.514). Together, we found evidence that the magnitude of dishabituation was related to the similarity of the novel trial, as infants’ looking time followed our expected ordering: animacy &gt; number &gt; identity &gt; pose &gt; familiar (<xref rid="fig6" ref-type="fig">Figure 6</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Experiment 2, Infant and RANCH behavior.</title>
<p>The x-axis shows how the familiar animal was violated and the y-axis shows the mean looking time in seconds / scaled model samples for RANCH during test trials following the exposure phase. Error bars show the standard error of the mean. Both infants and RANCH showed a graded pattern of dishabituation depending on the violation type. We show the results using the combined dataset of the Zoom and Children Helping Science experiments.</p></caption>
<graphic xlink:href="609039v3_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next, we tested whether infants’ dishabituation was better described by a “same or different” account, in which dishabituation responds similarly to any novel stimulus, or a “graded” account, in which the type of violation influences the magnitude of dishabituation. To do so, we conducted a model comparison in which violation type was either parsed into two levels (familiar or novel) or five levels (familiar, animacy, number, identity or pose). The model comparison significantly favored the more granular model (<italic>χ</italic>(3) = 8.9; <italic>p</italic> = 0.031), suggesting that infants were sensitive to the type of violation rather than just to whether the test stimulus was familiar or novel.</p>
</sec>
<sec id="s4b2">
<label>4.2.2</label>
<title>Adult results</title>
<p>In adults, as in previous experiments, we found evidence for habituation (<xref rid="fig7" ref-type="fig">Figure 7</xref>; <italic>β</italic> = -0.02, <italic>SE</italic> = 0, <italic>p</italic> &lt; .001). More importantly, we also found evidence for graded dishabituation: the dishabituation magnitude to animacy violations was larger than to number violations (<italic>β</italic> = 0.17, <italic>SE</italic> = 0.04, <italic>p</italic> &lt; .001) and to pose violations (<italic>β</italic> = 0.18, <italic>SE</italic> = 0.04, <italic>p</italic> &lt; .001). The same pattern was found for identity violations (<italic>β</italic> = 0.18, <italic>SE</italic> = 0.04, <italic>p</italic> &lt; .001). However, animacy violations were not different from identity violations, nor were number violations different from the pose violations (all p &gt; 0.1). Qualitatively, we found that the ordering for dishabituation magnitude was animacy &gt; identity &gt; pose &gt; number (<xref rid="fig7" ref-type="fig">Figure 7</xref>).</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><title>Experiment 2, Adult and RANCH behavior.</title>
<p>The x-axis shows the position in the block (which could be of length 2, 4 or 6), and y-axis shows looking time/scaled model samples. Both adults and RANCH showed a similar pattern of graded dishabituation in this task. Error bars show the standard error of the mean.</p></caption>
<graphic xlink:href="609039v3_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><title>Illustration of how predictions from the Goldilocks model in <xref rid="fig1" ref-type="fig">Fig. 1</xref> were derived.</title></caption>
<graphic xlink:href="609039v3_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4b3">
<label>4.2.3</label>
<title>Model results</title>
<p>We next tested whether RANCH made similar predictions. We used the modeling procedures for the infant and adult experiments described above.</p>
<p>Using the best-fitting parameters from Experiment 1, we found that RANCH generalized to Experiment 2 successfully (Infants: <italic>RMSE</italic> = 1.26; <italic>R</italic><sup>2</sup> = 0.66; Adults: <italic>RMSE</italic> = 0.16; <italic>R</italic><sup>2</sup> = 0.72). Moreover, RANCH also showed a qualitative ordering of the dishabituation magnitude that was similar to those seen in infants and adults: animacy &gt; identity &gt; number &gt; pose. This ordering was in line with the ResNet-50-derived embedding distances between the different stimulus categories (<xref rid="fig12" ref-type="fig">Figure 12</xref>).</p>
<p>Taken together, these data suggest that RANCH is not only capable of capturing habituation and dishabituation, but also can generalize to a new task and make predictions about dishabituation magnitude in relation to stimulus similarity. We did observe some differences in the relative ordering of dishabituation magnitudes between infants, adults, and RANCH – we discussed these in more depth in the General Discussion.</p>
</sec>
</sec>
</sec>
<sec id="s5">
<label>5</label>
<title>General discussion</title>
<p>Even though habituation has been a key method for gaining insight into the infant mind, the underpinnings of this behavior are not well understood. Here, we investigated how well a specific form of visual habituation and dishabituation can be understood as rational exploration of noisy perceptual samples. To do so, we introduced the RANCH model, which takes noisy samples from stimuli and makes decisions about whether or not to continue sampling based on the expected information gain of a new sample. We used RANCH to make predictions about infants’ and adults’ looking time across a range of exposures, and the model showed good fit across a range of parameters. In a second experiment, we found that the same parameters in RANCH could be used to make predictions about looking time in a new task where we varied the similarity between the familiar and novel stimulus. These results suggest that RANCH provides a quantitative account of looking time as resulting from optimal exploration of noisy perceptual samples.</p>
<p>Our work successfully addressed the three critical challenges in testing models of infant looking time: imprecise measurement, stimulus-dependency, and poor generalizability. First, our novel within-subject experiment design, combined with online testing and automatic gaze coding, allowed us to sensitively measure the infant looking time in a relatively large sample. Second, our model leveraged principled stimulus representations to learn from raw images, overcoming the stimulus-dependency of previous models. Lastly, our model can be applied across different stimulus sets, and therefore be used to make out-of-sample predictions about new experiments.</p>
<p>This current work focuses on visual habituation, a fundamental but specific form of habituation that applies to sequential visual stimuli. While habituation has been studied across various domains, our model is specifically designed to account for looking time changes in response to repeated visual exposure. This focus aligns with our choice of perceptual representations derived from CNNs, which process visual inputs rather than abstract probabilistic structures. Visual habituation plays a foundational role in infant cognition, as it provides a mechanism for concept learning based on visual experience. However, it does not encompass all forms of habituation, particularly those involving complex rule learning, linguistic structures. Similarly, RANCH does not capture more global attention dynamics, such as block-to-block attentional drift independent of stimulus properties. Future work should investigate whether models like RANCH can be extended to capture habituation mechanisms in broader learning contexts.</p>
<sec id="s5a">
<label>5.1</label>
<title>Unique strengths of the RANCH framework</title>
<p>A key strength of RANCH is the cognitive interpretability of its parameters. RANCH integrates structured perceptual representations with Bayesian inference, allowing for stimulus-computable predictions of looking behavior and interpretable parameters at the same time. This integrated approach has been used to study selective attention [<xref ref-type="bibr" rid="c7">7</xref>]. Moreover, previous Bayesian models of infant attention have leveraged parameter interpretability to draw conclusions about the computational mechanisms underlying individual differences in attention [<xref ref-type="bibr" rid="c43">43</xref>]. Here, we used parameter fits of RANCH to study population differences: By applying a “joint scaling” procedure in which we scaled model samples to infant and adult simultaneously, we could identify which parameters had to be different between infant and adults to account for their different patterns of looking time. Using this procedure, a key developmental difference was in <italic>σ</italic><sub><italic>ϵ</italic></sub>, the learner’s prior on their perceptual noise. Given that <italic>σ</italic><sub><italic>ϵ</italic></sub> was preferred to be higher in infants than in adults, this suggests that infants’ learning in our experiments may be tuned for noisier perceptual processing. This finding is consistent with previous works showing that infants’ visual contrast sensitivity is indeed constrained by higher level of neural noise [<xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c27">27</xref>].</p>
<p>Another strength is RANCH’s ability to model looking on a more fine-grained timescale. Prior research has largely resorted to modeling on the trial level, and asking about correlations between behavior on a given trial (e.g. looking time) and information-theoretic properties of that trial relative to prior training, such as KL divergence or surprisal [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c42">42</xref>]. Rational analysis relies on computing expected information gain. However, the resolution of these prior models does not support calculating this metric on sample-by-sample basis, since they carve up time only in terms of trials, rather than samples. In contrast, the sample-based modeling framework used in RANCH allows for implementation of the full rational analysis [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c1">1</xref>], in which learners ask “How much information do I expect to gain from continuing to look at the stimulus?”. The ability to conduct the rational analysis enhances our understanding of habituation as optimally adapted to environmental constraints [see e.g. 19, for a different rational analysis of habituation]. Still, replacing EIG with information-theoretic proxies in our setting can help us examine whether participants’ behavior seems to follow an EIG-based, or a proxy-based, linking hypothesis. In our analyses we found that KL divergence performed comparably to EIG, and the surprisal-based model was worse at fitting both infant and adult data in Experiment 1.</p>
<p>Given that EIG is defined as the expected value of KL-divergence, this suggests that our participants’ strategy is consistent with a “learning progress” metric when making attentional decisions, either backward-looking (KL) or forward looking (EIG) [<xref ref-type="bibr" rid="c21">21</xref>]. A surprisal-based strategy however is less consistent with our data, in contrast with other models which relate infant looking to a form of surprise [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c30">30</xref>].</p>
<p>More generally, while there is no single threshold for what constitutes a “good” model fit, the strength of our approach lies in the relative comparisons across model variants, linking hypotheses, and ablation studies. In this way, we treat model fit not as an absolute benchmark, but as an empirical tool to adjudicate among alternative explanations and assess the mechanistic plausibility of the model’s components.</p>
</sec>
<sec id="s5b">
<label>5.2</label>
<title>Within-subject design reveals graded dishabituation</title>
<p>To test our model, we developed novel paradigms for within-infant measurements of habituation and dishabituation. These novel designs allowed us to go beyond testing simple qualitative predictions (i.e. longer versus shorter looking duration). Instead, we validated RANCH’s continuous predictions about the relationship between prior exposure and looking to familiar vs. novel stimuli (Experiment 1) and between perceptual distance and looking (Experiment 2), within subjects. It is rare to observe such graded differences in looking: Most looking time studies measure a simple contrast between familiar and novel, or expected vs. unexpected stimuli and therefore do not capture nuances in dishabituation magnitude as a function of experimental parameters [for an exception see; Téglás et al. [<xref ref-type="bibr" rid="c49">49</xref>]]. Our findings revealed quantitative nuances of looking time patterns and provide a higher bar for fitting computational models to such data.</p>
<p>However, Experiment 2 also revealed interesting discrepancies across age groups and RANCH, particularly for number violations: The number of samples RANCH took from number violations was less than identity violations but more than pose violations. In contrast, adults looked at the number violations less than the pose violations, and infants looked at number violations more than identity and pose violations. This discrepancy between the model and adults on the one hand, and infants on the other hand could be explained by different underlying representations for number.</p>
<p>RANCH is designed for representing single objects, not multiple objects, therefore any dishabituation to number violations resulted primarily from simple, low-level perceptual distances, rather than dishabituation to number per-se. To the extent that the dishabituation observed in the data exceeded RANCH’s, this points to other, conceptual sources of new information that cannot be explained purely by low-level visual differences. This is consistent with infant literature showing that infants are sensitive to changes in number specifically, rather than visual changes generally [<xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c18">18</xref>]. Adults on the other hand did not show this exaggerated effect of number violations, possibly because adults’ processing was shallower in our task, which is supported by their shorter reaction time.</p>
</sec>
<sec id="s5c">
<label>5.3</label>
<title>No evidence for familiarity preferences</title>
<p>A key feature of our experimental data, as well as our model, is that at no point, even for the shortest exposure durations, did we observe longer looking to familiar than to novel stimuli. This finding contrasted with predictions made by a prominent model of infant attention (<xref rid="fig1" ref-type="fig">Figure 1A</xref>), which posited that at intermediate levels of encoding, familiar stimuli should be more interesting than novel stimuli. The intuition is that some initial familiarity with a stimulus may allow a viewer to “break into” the information there is to gain from the stimulus – a completely novel stimulus may be too complex or foreign for extended attention to be worth it. However, in the learning setting we used here, neither the rational analysis nor data from human learners showed this strategy. This result suggests at least that limited exposure does not generically lead to familiarity preferences in all experimental paradigms.</p>
<p>The absence of familiarity preferences in our results does not rule out their existence in general. Familiarity preferences may be more subtle than novelty preferences, so that the statistical power that is needed to find familiarity preferences is higher than that achieved in the current study. A current large-scale study by the ManyBabies consortium which aims to test the predictions made by Hunter and Ames [<xref ref-type="bibr" rid="c26">26</xref>] may give insight into this possibility [<xref ref-type="bibr" rid="c31">31</xref>]. Such subtle effects would not be consistent with the claims of larger familiarity preferences that have appeared in the literature, however.</p>
</sec>
<sec id="s5d">
<label>5.4</label>
<title>Formal theories of learning and attention</title>
<p>The lack of familiarity preferences in the current study can also be attributed to the particular learning problems participants are solving: whether familiarity preferences occur can depend on the particular learning context and the particular learning problem. This context-dependence is reflected in meta-analyses investigating familiarity preferences across paradigms. For example, when tested on word segmentation in their native language, infants showed preferences for familiar stimuli throughout the first year [<xref ref-type="bibr" rid="c5">5</xref>]. In contrast, when tested on statistical learning of novel words, infants showed consistent preferences for novel stimuli, from 4 months to 11 months of age [<xref ref-type="bibr" rid="c6">6</xref>].</p>
<p>The seemingly contradictory results on the direction of preferences in infant research highlight the need for formal theories of how the learning problem influences attention. Dubey and Griffiths [<xref ref-type="bibr" rid="c15">15</xref>] gave an example of such a formal account by showing that when past and present events are correlated, rational agents, under some assumptions, develop a tendency to attend to familiar stimuli to prepare for the most likely future events, while in uncorrelated environments, novelty preferences are optimal. Ideal learners attempting to maximize their expected information gain consistently seek novelty when trying to learn a single concept. Formal models of the learning problem being solved in experiments may therefore give more principled and precise predictions beyond identifying factors like “prior exposure” as determinants of attentional preferences.</p>
<p>Importantly, RANCH’s predictions outlined in this paper apply to the specific case when the learner makes noisy observations of stimuli in a perceptual space, updates their beliefs about the location of a single concept in that space, and makes decisions about whether to continue observing based on their expected gain of information relative to this belief updating process. The modular structure of RANCH allows for each of these components (perceptual embedding space, learning model, and linking hypothesis) to be modified; such modifications could lead to different predictions. For example, the embedding space could be chosen to more closely resemble a psychologically validated perceptual space [e.g. 33], though in this case, we found that such alternatives did not affect embedding structure qualitatively (Supplemental Figure 12). Similarly, the simple concept learning model we used could be modified to account for more complex, or more general, representation learning [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. For example, we could model our participants as learning multiple concepts, deciding whether each stimulus is part of the same concept or a new concept.</p>
<p>Another promising direction is to explore RANCH’s applicability to finer timescales of looking behavior, enabling a more detailed examination of within-trial fluctuations in attention. Recent work suggests that analyzing moment-by-moment dynamics can help disentangle distinct learning mechanisms [<xref ref-type="bibr" rid="c43">43</xref>].Since RANCH models decision-making at the level of individual perceptual samples, it is well-suited to capture these fine-grained attentional shifts.</p>
</sec>
<sec id="s5e">
<label>5.5</label>
<title>Conclusion</title>
<p>From birth, infants actively explore their environment through selective attention. Looking time, a widely used measure in developmental psychology, has long been used to make inferences about infants’ conceptual and perceptual capacity. Here, we measured nuances of key looking time phenomena, such as habituation and dishabituation, by varying familiarity with a stimulus prior to dishabituation (Experiment 1), as well as the similarity between the familiar and novel stimulus (Experiment 2). We found that these behavioral nuances are well captured by RANCH, an image-computable, rational model that learns from noisy perceptual samples and decides how many samples to take based on their expected information gain. Beyond modeling infant looking during perceptual learning in particular, our work provides a general framework that instantiates and tests hypotheses about the computations underlying infant looking.</p>
</sec>
</sec>
<sec id="s6">
<label>6</label>
<title>Methods and Materials</title>
<sec id="s6a">
<label>6.1</label>
<title>Model specifications</title>
<sec id="s6a1">
<label>6.1.1</label>
<title>Perceptual representations</title>
<p>We derived principled low dimensional representations of our stimuli from ResNet-50 [<xref ref-type="bibr" rid="c24">24</xref>]. We ran a Principal Component Analysis and used the first three principal components. This decision was made due to the computational demands of our model. Each added dimension would increase the total run time exponentially. The first three components captured 57.9% of the variance.</p>
<p>Given that we were using discrete grid approximation in our inference, the model was sensitive to the absolute values of the embeddings. When these embeddings values were close to or exceeded the range of the approximation grids, this would cause bias in our simulation result. We therefore scaled our embeddings down to fall squarely into our grid. Simulations have shown that embedding that were too large would result in biased model behaviors (Supplementary Information, Figure 10). Since embedding distances were larger for stimulus type experiment, especially for animacy violations, the downscaling had to be more extreme.</p>
</sec>
<sec id="s6a2">
<label>6.1.2</label>
<title>Linking hypotheses</title>
<p>We compared two additional information theoretic measures with Expected Information Gain (EIG): Kullback– Leibler divergence and Surprisal. Each of these measures has been invoked in the literature as a plausible account for explaining looking behaviors across developmental stages [KL divergence: 41, surprisal: 30].</p>
<p>Our key linking hypothesis, EIG, is a forward-looking measure. It is computed as the product of the posterior predictive probability of the next perceptual sample (<italic>p</italic>(<italic>z</italic><sub><italic>t</italic>+1</sub> |<italic>θ</italic><sub><italic>t</italic></sub>)) and the information about the concept <italic>θ</italic> gained conditioned on that next hypothetical sample, using a grid of possible subsequent samples(s): <inline-formula><inline-graphic xlink:href="609039v3_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula></p>
<p>Both KL-divergence and surprisal are backward-looking measures. The KL divergence at any given point is calculated by the posterior distribution after seeing the current perceptual sample and before seeing the current perceptual sample: <inline-formula><inline-graphic xlink:href="609039v3_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula></p>
<p>Surprisal is calculated as the negative log likelihood of the current perceptual sample under the prior distribution: <italic>surprisal</italic>(<italic>z</italic><sub><italic>t</italic></sub>) = <italic>−log</italic>(<italic>p</italic>(<italic>z</italic>|<italic>θ</italic><sub><italic>t−</italic>1</sub>))</p>
<p>We ran individual simulation and parameter search for each of the three linking hypotheses to ensure fair comparison.</p>
</sec>
<sec id="s6a3">
<label>6.1.3</label>
<title>Scaling procedure for developmental interpretation</title>
<p>To find the best fitting parameters for each age group we employed a strategy we called “joint scaling”. To evaluate model fit, we first mapped model samples onto looking time by finding the best linear scaling of model samples in a regression of the form <monospace>LT ∼ model_samples.</monospace> The parameter set that minimized the RMSE between the scaled model samples and looking time was what we considered the best parameter set. In the case of “joint scaling”, we regressed both the adult and the infant simulations against their looking time simultaneously, to find how varying the infant and adult parameters affected the fit of the scaled model samples to the adult and infant data. This joint scaling procedure put pressure on the parameters, rather than the scaling, to account for the fact that adults’ looking time were a lot shorter than infants’ looking time.</p>
<p>The interpretable parameters of RANCH were the priors on <italic>µ</italic> and <italic>σ</italic> and the priors on the noise. The priors on <italic>µ</italic> and <italic>σ</italic> were parameterized by a normal inverse-gamma prior with <italic>µ</italic><sub><italic>prior</italic></sub>, <italic>ν</italic><sub><italic>prior</italic></sub>, <italic>α</italic><sub><italic>prior</italic></sub> and <italic>β</italic><sub><italic>prior</italic></sub>, the conjugate prior to a normal distribution with unknown mean and variance. While the mean was fixed to be 0 (<italic>µ</italic><sub><italic>prior</italic></sub>), variation in the other parameters expressed distinct hypotheses about the precision of the location of the concept (<italic>ν</italic><sub><italic>prior</italic></sub>), as well as the variance of the concept (<italic>α</italic><sub><italic>prior</italic></sub> and <italic>β</italic><sub><italic>prior</italic></sub>). The learner’s prior on noise was parameterized by zero-mean Gaussian distribution with standard deviation (<italic>σ</italic><sub><italic>epsilon</italic></sub>).</p>
</sec>
</sec>
<sec id="s6b">
<label>6.2</label>
<title>Experimental procedure</title>
<p>Below we described in detail the experimental procedure. We used Experiment 1 to denote the experiment that manipulates exposure duration, and Experiment 2 to denote the experiment that manipulates stimuli similarity.</p>
<sec id="s6b1">
<label>6.2.1</label>
<title>Infant experiment</title>
<sec id="s6b1a">
<label>6.2.1.1</label>
<title>Participants</title>
<p>For Experiment 1, we tested a combined sample of 103 7-10 month old infants, with 31 in the first sample, 35 in the second sample and 37 in the third sample (<italic>M</italic><sub><italic>age</italic></sub> = 9.38 months, 47 female). 10 participants were excluded completely due to fussiness. An additional 58 individual trials were excluded.</p>
<p>For Experiment 2, we tested a combined sample of 123 7-10 month old infants, with 57 collected via Zoom (7 complete exclusions, 42 trial exclusions), 66 collected via CHS (3 complete exclusions, 26 trial exclusions).</p>
<p>Complete or trial exclusions occurred because (1) infants fussed out of the experiment at an earlier stage of the experiment, (2) infants looked at the stimuli for less than a total of 2 seconds, (3) there were momentary external distractions in the home of the infant or (4) the gaze classifier performed so poorly that manual corrections were not viable.</p>
</sec>
<sec id="s6b1b">
<label>6.2.1.2</label>
<title>Stimuli</title>
<p>In Experiment 1, we presented infants with a series of animated animals, created using “Quirky Animals” assets from Unity (<xref rid="fig3" ref-type="fig">Figure 3</xref>; link to assets). The animals were walking, crawling or swimming, depending on the species. In a previous study not included here (but see OSF repository), we showed infants a different stimulus set and failed to elicit replicable habituation, novelty or familiarity preferences. As a result, we modified the stimuli to be more appealing. We also separated exposure trials with curtains opening and closing to accentuate exposure trials as separated occurrences of the stimulus.</p>
<p>In Experiment 2 (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), we introduced animacy violations using the “3D Prop Vegetables and Fruits”(link to assets) assets from Unity and animated bouncing and rotating trajectories for them. To create number violations, we duplicated the videos of single animal Unity assets and pasted them side-by-side.</p>
</sec>
<sec id="s6b1c">
<label>6.2.1.3</label>
<title>Procedure</title>
<p>Data collection was performed primarily synchronously on Zoom, except for the CHS replication of Experiment 2. In all cases, infants were recruited from CHS [<xref ref-type="bibr" rid="c47">47</xref>] and Facebook. Parents were instructed to find a quiet room with minimal distractions, place their child in a high chair (preferred) or on their lap, and to remain neutral throughout the experiment. Infants were placed as close as possible to the screen without allowing them to interact with the keyboard.</p>
<p>Both main experiments followed a block structure, where each block was divided into two sections: (1) a exposure phase and (2) a test event. Each block was preceded by an “attention getter”, a salient rotating star. During the exposure period, the infant was familiarized to a particular animal in a series of exposure trials. Each exposure trial was a 5 second sequence: curtains opened for 1 second, the animated animal moved in place for 3 seconds, and then the curtains closed for 1 second. In Experiment 1, the number of exposure trials (the “exposure duration”) varied between blocks. In Experiment 2, the number of exposure trials was always 8 or 9.</p>
<p>During the test event, the infant saw either the same familiar animal again, or a novel stimulus. The onset of the test event was not marked by any visual markers, but a bell sound played as the curtains opened, to maximize the chance of engagement during the test event. On Zoom, the test event used an infant-controlled procedure: the experimenter terminated the trial when the infant looked away for more than three consecutive seconds. On CHS, test trials would always be played for 40 seconds. In both cases, looking time was then defined as the total time that the infant spent looking at the screen from the onset of the stimulus until the first two consecutive seconds of the infant looking away from the screen. If the infant did not look away after 60 seconds of being presented with the test event, the next block automatically began and infants’ looking time for that test event was recorded as 60 seconds.</p>
<p>In Experiment 1, the novel stimulus was always another animal. In Experiment 2, novel stimuli violated one of four properties of the familiar animal: pose, identity, number or animacy.</p>
<p>In Experiment 1, each infant saw six blocks: Three different exposure durations ({0,4,8}, {1,3,9} or {2,4,6}) appeared twice each, once for each test event type (familiar or novel). The longer exposure durations (8 or 9) were chosen based on our previous pilot studies with a different stimulus set (OSF repository), and the shorter durations were chosen to provide limited learning experience with the familiar stimuli. The order of blocks was counterbalanced between infants, and pairs of animals (familiar and novel) were counterbalanced to be associated with each block type. Which animal was shown as the familiar and which as the novel (if it was a novel test trial) was randomized in each block.</p>
<p>In Experiment 2, each infant saw four blocks within a session - the familiar trial was always one of the four test trials, and the remaining three test trials were picked randomly from the remaining four violation types (pose, identity, number, animacy).</p>
<p>The control experiment showed six standalone trials: Infants saw two single animals, two pairs of animals and two single vegetables, to measure baseline interest in these stimulus categories.</p>
</sec>
<sec id="s6b1d">
<label>6.2.1.4</label>
<title>Looking time coding</title>
<p>To code the infants’ looking time we used iCatcher+, a validated tool developed for robust and automatic annotation of infants’ gaze direction from video [<xref ref-type="bibr" rid="c16">16</xref>]. To obtain trial-wise looking time, we merged iCatcher+ annotations with trial timing information, thereby fully replacing manual coding of looking time. The correctness of iCatcher+ coding was manually supervised to check for failed face detections or distractions.</p>
</sec>
<sec id="s6b1e">
<label>6.2.1.5</label>
<title>Statistical analysis</title>
<p>We ran the pre-registered statistical analyses for both experiments. All statistical analyses were ran in R using the package <monospace>lme4</monospace> [<xref ref-type="bibr" rid="c4">4</xref>].</p>
<p>For Experiment 1, we ran a lme4 model with the following specification: <monospace>log(looking time) ∼ poly(exposure_duration, 2) * test_type [identity or familiar] + log(block_number) + (1|subject).</monospace> The <monospace>exposure duration</monospace> term referred to the amount of prior exposure received by the infant (0 to 9 prior exposures), and the second polynomial tested for non-linearity in the effect on looking time, as predicted by some theories of infant attention. This non-linearity could occur primarily for looking to the familiar stimuli (<xref rid="fig1" ref-type="fig">Figure 1A</xref>), or both familiar and novel stimuli (<xref rid="fig1" ref-type="fig">Figure 1B</xref>), hence the interaction with <monospace>test type.</monospace> We controlled for block number to account for general decreasing interest in our stimuli as the experiment goes on.</p>
<p>For Experiment 2, we ran a linear mixed effects regression, again predicting looking time, using violation type as the main predictor: <monospace>log(looking_time) ∼ test_type [animacy, pose, identity, number, familiar] + log(block_number) + (1|subject).</monospace> Here, we set familiar as the reference level for <monospace>test type</monospace>, and asked whether the other test types were significantly different from that familiar reference level. We again controlled for block number.</p>
<p>In our pre-registered model comparison analysis, we used a chi-square test to compare the above regression model above to a regression model in which test type only had two levels, familiar or novel.</p>
</sec>
</sec>
<sec id="s6b2">
<label>6.2.2</label>
<title>Adult experiment</title>
<sec id="s6b2a">
<label>6.2.2.1</label>
<title>Participants</title>
<p>Both experiments were run on Prolific (Experiment 1: N = 522; Experiment 2: N = 536). Two experiments had the same pre-registered exclusion criteria. We excluded participants if (1) the standard deviation of their reaction time across all trials was less than 0.15 (indicating key-smashing, Experiment 1: N = 0; Experiment 2: N = 0), (2) spent more than three absolute deviations above the median of the task completion time as reported by Prolific (Experiment 1: N = 47; Experiment 2: N = 43), and (3) provided the wrong response to more than 20% of the memory task (Experiment 1: N = 13; Experiment 2: N = 33). A total of 52 and 68 participants met at least one of the three criteria and were excluded from the final analysis for Experiment 1 and Experiment 2, respectively. We also excluded a trial if the trial was three absolute deviations away from the median in the log-transformed space across reaction time from the remaining participants.</p>
<p>The final sample included 470 participants for Experiment 1 and 468 participants for Experiment 2 (Experiment 1: <italic>M</italic><sub><italic>age</italic></sub> = 33.0 years, <italic>SD</italic> = 12.3; Experiment 2: <italic>M</italic><sub><italic>age</italic></sub> = 31.8 years, <italic>SD</italic> = 11.3).</p>
</sec>
<sec id="s6b2b">
<label>6.2.2.2</label>
<title>Stimuli</title>
<p>The stimuli used in adult experiment were identical to the ones used in infant experiment. In Experiment 1, the stimuli were selected from the animal unity set. In Experiment 2, both the animal and vegetable sets were used.</p>
</sec>
<sec id="s6b2c">
<label>6.2.2.3</label>
<title>Procedure</title>
<p>Procedurally, Experiment 1 and Experiment 2 were similar. Both were self-paced visual presentation studies. Participants were instructed that they would be looking at a sequence of animated stimuli at their own pace. At each trial, they can press a key on the keyboard to move on to the next trial after a minimum viewing time of 500 ms. Each study consisted of multiple blocks. Each block consisted of multiple trials. Between blocks, participants answered a simple memory question (“Have you seen this animation before?”). This memory question was used as an attention check.</p>
<p>Each block contained multiple trials of one stimulus being repeatedly presented (the familiar trials) and one trial that showed a different stimulus (the novel trial). The novel trial always appeared as the last trial. In Experiment 1, the total number of trials ranged from two to eleven. The novel trial was randomly drawn from the stimuli set that researchers haven’t seen before. In Experiment 2, the block consisted of either two, four, or six trials. There were a total of eight types of stimuli in the familiar trials. They were all combinations of the three features that each include two levels: animacy (e.g. animate or inanimate), number (singleton or pair), and pose (facing left or facing right). The novel trial can differ from the familiar trials in one of the four dimensions: pose, number, identity, and animacy. Both experiments also had blocks that only include one stimulus (the familiar block).</p>
<p>The order of the block was semi-randomized in both experiments. Experiment 1 has 15 blocks in total. We grouped the total number of trials in each block into five pairs of two: {2, 3}, {4, 5}, {6, 7}, {8, 9}, {10, 11}, and randomly sampled 1 block length from each pair. In other words, each participant would receive 5 different lengths of repeating familiar trials, stratified by 5 levels. We controlled the distribution of the block lengths by making sure the first half and the second half of the experiment each had the same number of blocks with different block lengths. Experiment 2 had 24 blocks in total. We grouped the twenty-four blocks into four groups. Each group consisted of two familiar blocks and one block from each of the four novelty types. The order of blocks within each group was randomized.</p>
</sec>
<sec id="s6b2d">
<label>6.2.2.4</label>
<title>Statistical Analysis</title>
<p>We ran the pre-registered statistical analyses for both experiments. All statistical analyses were ran in R using the package <monospace>lme4</monospace> [<xref ref-type="bibr" rid="c4">4</xref>].</p>
<p>For experiment 1, we ran a linear mixed effect regression predicting the log transformed of looking time with the following specification: <monospace>log(looking time) ∼ trial_number + is_first_trial + trial_number * trial_type + log(block_number) + (trial_number * trial_type | subject) + (is_first_trial + trial_number | subject)</monospace>. This model allowed us to investigate whether the number of familiar trials has an impact on the looking time (i.e. habituation), and whether the trial type (familiar or novel) mediated the influences. This model failed to converge. Following the pre-registered procedure, we pruned the model to include only by-subject random intercept. The pruned model suggests that there is evidence for habituation and dishabituation.</p>
<p>For experiment 2, we ran a similar model and incorporated the effect of different novelty type. The particular model specification is as follows: <monospace>log(total_rt) ∼trial_number + is_first_trial + (trial_number + is_first_trial) * stimulus_number + (trial_number + is_first_trial) * stimulus_pose +(trial_number + is_first_trial) * stimulus_animacy + (trial_number + is_first_trial) * novelty_type + log(block_number). novelty_type</monospace> had five levels, including the familiar trial and four types of violation. This model can help us test two hypotheses: (1) whether our experimental paradigm captured habituation and dishabituation and (2) whether the magnitude of dishabituation was influenced by the similarity between the novel trials and the familiar trials.</p>
</sec>
</sec>
</sec>
</sec>
</body>
<back>
<sec id="s7">
<label>7</label>
<title>Supplementary Information</title>
<sec id="s7a">
<label>7.1</label>
<title>Deriving predictions for Goldilocks model</title>
<p>Below is an illustration of how predictions for a habituation/dishabituation experiment were derived for the Goldilocks model. In all cases, as infants are familiarized, the surprisal of the familiar stimulus goes down (Blue arrows pointing left), and the surprisal of the novel stimulus has to go up so that probabilities assigned to events add up to 1 (Red arrows pointing right). Depending on the assumed surprisal of the exposure stimulus, this model predicts three qualitatively different trajectories of preferences to familiar vs. novel stimuli as a function of prior exposure: (1) If the assumed surprisal is below the optimal level to maximize attention, the model predicts an increase, and then a decrease in novelty preferences. (2) If the assumed surprisal is at the optimal level, then no preference is predicted. (3) If the assumed surprisal is higher than optimal surprisal, the model predicts an increasing, and then decreasing familiarity preference.</p>
<p>In our work, we assumed a scenario consistent with the first assumption – i.e. that the initial surprisal of stimuli is below the optimal level. This is the only regime in which novelty preferences arise, and therefore had the highest chance of agreeing with with our data, as well as predictions made by Hunter and Ames [<xref ref-type="bibr" rid="c26">26</xref>].</p>
</sec>
<sec id="s7b">
<label>7.2</label>
<title>Parameter robustness</title>
<p>To evaluate RANCH’s robustness to different parameters, we tested its fit for all possible parameter settings. Performance across the 162 parameter settings was relatively stable, yielding a moderate range RMSE values for infants and adults across linking hypotheses. In both cases most parameter settings outperformed the fit achieved by baseline models (Table <bold>??</bold>). The distribution of RMSE values across parameter settings are shown in the histograms below (<xref rid="fig9" ref-type="fig">Figure 9</xref>).</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9:</label>
<caption><title>Histograms of RMSE for different linking hypotheses show that RANCH is largely robust to variations in parameters.</title></caption>
<graphic xlink:href="609039v3_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10:</label>
<caption><title>The effects of scaling on model results.</title>
<p>(A) Experiment 1 with unscaled embeddings resulted in a reversal of familiar vs. novel. (B) Experiment 2 with unscaled embeddings resulted in an unintuitive ordering.</p></caption>
<graphic xlink:href="609039v3_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig11" position="float" fig-type="figure">
<label>Figure 11:</label>
<caption><title>Histograms of model fit colored by priors.</title>
<p>X-aixs shows the RMSE of the model fit with the behavioral data. Results suggest that when the learner’s noise is high, RANCH fits infant data better.</p></caption>
<graphic xlink:href="609039v3_fig11.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig12" position="float" fig-type="figure">
<label>Figure 12:</label>
<caption><title>Embedding distances of different violation types for (1) a base ResNet50 model, (2) a perceptually aligned embedding model [<xref ref-type="bibr" rid="c33">33</xref>], and (3) a ResNet50 model trained on SAYCAM data [<xref ref-type="bibr" rid="c39">39</xref>]</title></caption>
<graphic xlink:href="609039v3_fig12.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig13" position="float" fig-type="figure">
<label>Figure 13:</label>
<caption><title>Control experiment for Experiment 2 (infants), checking for baseline differences in interest.</title>
<p>We found no significant differences in baseline looking.</p></caption>
<graphic xlink:href="609039v3_fig13.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s7c">
<label>7.3</label>
<title>Scaling the embedding space</title>
<p>In order to achieve sensible results, it was important to bring the stimulus embeddings in line with the approximation grid used for inference. Below we show what happens when embeddings exceeded or came close to the edges of the our approximation grids. In Experiment 1, unscaled embeddings resulted in a reversal of familiar vs. novel model samples, such that the model takes more samples from the familiar than the novel. In Experiment 2, unscaled embeddings resulted in an unintuitive ordering which is not in line with the embedding distances (<xref rid="fig10" ref-type="fig">Figure 10</xref>).</p>
</sec>
<sec id="s7d">
<label>7.4</label>
<title>Developmental comparison of parameter fits</title>
<p>A naive approach to comparing parameter fits between infants and adults would be the following: After joint scaling, pick the set of parameters for infants and adults that together minimized RMSE. If we do this, we obtain the following parameter sets:
<disp-formula id="ueqn2">
<graphic xlink:href="609039v3_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>If we compare these two sets of parameters to each other, and interpret these differences, we may come to the following conclusion: RANCH fits best when infants’ <italic>σ</italic><sub><italic>ϵ</italic></sub> is higher than that of adults, and <italic>β</italic><sub><italic>prior</italic></sub> is higher than that of adults, suggesting that infants both expect their perception to have more noise, and that infants have more prior uncertainty about the size of the standard deviation.</p>
<p>However, a direct comparison between these parameter sets does not give insight into the relative stability of RANCH’s fit to the data if we change any of these parameters: It may be that if a parameter was slightly different, it would not make a big difference for model fit.</p>
<p>Thus we checked the sensitivity of the model fit to changes in the parameters. To do so, we plotted RMSE histograms colored by the value of a certain parameter, from a certain age group. Here we see that the overall modes of the RMSE histograms are dominated by world EIG, likely because this parameter is the main threshold that can account for the difference in scale between infant and adult looking time. However, of the other parameters, we see that RMSE only shows sensitivity to <italic>σ</italic><sub><italic>ϵ</italic></sub>. Other parameters that are different between the best fits are not stable across settings. We therefore conclude that the main interpretable parameter difference is in <italic>σ</italic><sub><italic>ϵ</italic></sub>, the learner’s noise prior, being higher in infants than in adults.</p>
</sec>
<sec id="s7e">
<label>7.5</label>
<title>Embedding distances of different violations categories, by embedding model</title>
<p>We chose ResNet-50 as our embedding space, but there are other embedding space available. Below we plotted the embedding distances of the different violations. The lines connect a single stimulus, and the y-axis represents the embedding distances to all the stimuli that would constitute a certain violation. For example, for a single animal stimulus, the y-value for ‘animacy’ corresponds to the distance between the single animal and all the single fruit/vegetable stimuli. Since the embedding distance between different categories were similar to each other, we did not run the full simulations on each of the embedding space.</p>
</sec>
<sec id="s7f">
<label>7.6</label>
<title>Control experiment for Experiment 2 in infants</title>
<p>Given that the exposure phase always consisted of a single animal, it is possible that differences in looking time during test trials of different types were a result of differences in interest in the stimuli themselves (e.g. our inanimate stimuli may have been intrinsically more interesting than our animals). To test whether infants were differently interested in our stimuli due to intrinsic stimulus properties, we ran a control experiment in which we showed infants our novel stimulus types without any prior exposure. Infants saw three types of stimuli for as long as they wanted, up to 60 seconds: single animals, fruits/vegetables, or pairs of animals. There were six trials per session, so infants saw two of each stimulus type. We tested 35 infants for this experiment, with 2 complete subject exclusions and 10 single trial exclusions.</p>
<p>We found no evidence for differences in baseline interest in our stimuli: neither looking to inanimate stimuli nor animal pairs differed significantly from looking to single animals (inanimate stimuli: <italic>β</italic> = -0.02; <italic>SE</italic> = 0.08; <italic>t</italic> = -0.31; <italic>p</italic> = 0.758; animal pairs: <italic>β</italic> = -0.11; <italic>SE</italic> = 0.08; <italic>t</italic> = -1.38; <italic>p</italic> = 0.169). In sum, infants showed equal interest in all categories of test stimuli, prior to habituation; the condition differences observed in Experiment 2 must derive from the cognitive processes of habituation and dishabituation.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>John R</given-names> <surname>Anderson</surname></string-name></person-group>. “<article-title>Is human cognition adaptive?</article-title>” <source>Behavioral and brain sciences</source> <volume>14</volume>.<issue>3</issue> (<year>1991</year>), pp. <fpage>471</fpage>–<lpage>485</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard N</given-names> <surname>Aslin</surname></string-name></person-group>. “<article-title>What’s in a look?</article-title>” <source>Developmental science</source> <volume>10</volume>.<issue>1</issue> (<year>2007</year>), pp. <fpage>48</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Renee</given-names> <surname>Baillargeon</surname></string-name>, <string-name><given-names>Elizabeth S</given-names> <surname>Spelke</surname></string-name>, and <string-name><given-names>Stanley</given-names> <surname>Wasserman</surname></string-name></person-group>. “<article-title>Object permanence in five-month-old infants</article-title>”. <source>Cognition</source> <volume>20</volume>.<issue>3</issue> (<year>1985</year>), pp. <fpage>191</fpage>–<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Douglas</given-names> <surname>Bates</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Package ‘lme4’</article-title>”. <source>convergence</source> <volume>12</volume>.<issue>1</issue> (<year>2015</year>), p. <fpage>2</fpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Christina</given-names> <surname>Bergmann</surname></string-name> and <string-name><given-names>Alejandrina</given-names> <surname>Cristia</surname></string-name></person-group>. “<article-title>Development of infants’ segmentation of words from native speech: A meta-analytic approach</article-title>”. <source>Developmental science</source> <volume>19</volume>.<issue>6</issue> (<year>2016</year>), pp. <fpage>901</fpage>–<lpage>917</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Alexis</given-names> <surname>Black</surname></string-name> and <string-name><given-names>Christina</given-names> <surname>Bergmann</surname></string-name></person-group>. “<article-title>Quantifying infants’ statistical word segmentation: A meta-analysis</article-title>”. <conf-name>39th annual meeting of the cognitive science society. Cognitive Science Society</conf-name>. <year>2017</year>, pp. <fpage>124</fpage>–<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sam</given-names> <surname>Blakeman</surname></string-name> and <string-name><given-names>Denis</given-names> <surname>Mareschal</surname></string-name></person-group>. “<article-title>Selective particle attention: Rapidly and flexibly selecting features for deep reinforcement learning</article-title>”. <source>Neural Networks</source> <volume>150</volume> (<year>2022</year>), pp. <fpage>408</fpage>–<lpage>421</lpage>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark S</given-names> <surname>Blumberg</surname></string-name> and <string-name><given-names>Karen E</given-names> <surname>Adolph</surname></string-name></person-group>. “<article-title>Protracted development of motor cortex constrains rich interpretations of infant cognition</article-title>”. <source>Trends in cognitive sciences</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Frederick</given-names> <surname>Callaway</surname></string-name>, <string-name><given-names>Antonio</given-names> <surname>Rangel</surname></string-name>, and <string-name><given-names>Thomas L</given-names> <surname>Griffiths</surname></string-name></person-group>. “<article-title>Fixation patterns in simple choice reflect optimal information sampling</article-title>”. <source>PLoS computational biology</source> <volume>17</volume>.<issue>3</issue> (<year>2021</year>), <fpage>e1008863</fpage>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Anjie</given-names> <surname>Cao</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Habituation reflects optimal exploration over noisy perceptual samples</article-title>”. <source>Topics in Cognitive Science</source> <volume>15</volume>.<issue>2</issue> (<year>2023</year>), pp. <fpage>290</fpage>–<lpage>302</lpage>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aaron</given-names> <surname>Chuey</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Conducting developmental research online vs. in-person: A meta-analysis</article-title>”. <source>Open Mind</source> <volume>8</volume> (<year>2024</year>), pp. <fpage>795</fpage>–<lpage>808</lpage>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aaron</given-names> <surname>Chuey</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Moderated online data-collection for developmental research: Methods and replications</article-title>”. <source>Frontiers in psychology</source> <volume>12</volume> (<year>2021</year>), p. <fpage>734398</fpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Jia</given-names> <surname>Deng</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Imagenet: A large-scale hierarchical image database</article-title>”. <conf-name>2009 IEEE conference on computer vision and pattern recognition</conf-name>. <publisher-name>Ieee</publisher-name>. <year>2009</year>, pp. <fpage>248</fpage>–<lpage>255</lpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Fenil R</given-names> <surname>Doshi</surname></string-name> and <string-name><given-names>Talia</given-names> <surname>Konkle</surname></string-name></person-group>. “<article-title>Cortical topographic motifs emerge in a self-organized map of object space</article-title>”. <source>Science Advances</source> <volume>9</volume>.<issue>25</issue> (<year>2023</year>), <fpage>eade8187</fpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rachit</given-names> <surname>Dubey</surname></string-name> and <string-name><given-names>Thomas L</given-names> <surname>Griffiths</surname></string-name></person-group>. “<article-title>Reconciling novelty and complexity through a rational analysis of curiosity</article-title>.” <source>Psychological Review</source> <volume>127</volume>.<issue>3</issue> (<year>2020</year>), p. <fpage>455</fpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yotam</given-names> <surname>Erel</surname></string-name> <etal>et al.</etal></person-group> “<article-title>iCatcher+: Robust and Automated Annotation of Infants’ and Young Children’s Gaze Behavior From Videos Collected in Laboratory, Field, and Online Studies</article-title>”. <source>Advances in methods and practices in psychological science</source> <volume>6</volume>.<issue>2</issue> (<year>2023</year>), p. <fpage>25152459221147250</fpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Robert L</given-names> <surname>Fantz</surname></string-name></person-group>. “<article-title>Pattern vision in newborn infants</article-title>”. <source>Science</source> <volume>140</volume>.<issue>3564</issue> (<year>1963</year>), pp. <fpage>296</fpage>–<lpage>297</lpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Lisa</given-names> <surname>Feigenson</surname></string-name>, <string-name><given-names>Stanislas</given-names> <surname>Dehaene</surname></string-name>, and <string-name><given-names>Elizabeth</given-names> <surname>Spelke</surname></string-name></person-group>. “<article-title>Core systems of number</article-title>”. <source>Trends in cognitive sciences</source> <volume>8</volume>.<issue>7</issue> (<year>2004</year>), pp. <fpage>307</fpage>–<lpage>314</lpage>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Samuel J</given-names> <surname>Gershman</surname></string-name></person-group>. “<article-title>Habituation as optimal filtering</article-title>”. <source>iScience</source> <volume>27</volume>:<elocation-id>110523</elocation-id> (<year>2024</year>).</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Noah D</given-names> <surname>Goodman</surname></string-name> <etal>et al.</etal></person-group> “<article-title>A rational analysis of rule-based concept learning</article-title>”. <source>Cognitive science</source> <volume>32</volume>.<issue>1</issue> (<year>2008</year>), pp. <fpage>108</fpage>–<lpage>154</lpage>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nick</given-names> <surname>Haber</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Learning to play with intrinsically-motivated, self-aware agents</article-title>”. <source>Advances in neural information processing systems</source> <volume>31</volume> (<year>2018</year>).</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>Marshall M</given-names> <surname>Haith</surname></string-name></person-group>. <source>Rules that babies look by: The organization of newborn visual activity</source>. <publisher-name>Lawrence Erlbaum Associates</publisher-name>, <year>1980</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Marshall M</given-names> <surname>Haith</surname></string-name></person-group>. “<article-title>Who put the cog in infant cognition? Is rich interpretation too costly?</article-title>” <source>Infant behavior and development</source> <volume>21</volume>.<issue>2</issue> (<year>1998</year>), pp. <fpage>167</fpage>–<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Kaiming</given-names> <surname>He</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Deep residual learning for image recognition</article-title>”. <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>. <year>2016</year>, pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Martin N</given-names> <surname>Hebart</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</article-title>”. <source>Nature human behaviour</source> <volume>4</volume>.<issue>11</issue> (<year>2020</year>), pp. <fpage>1173</fpage>–<lpage>1185</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Michael A</given-names> <surname>Hunter</surname></string-name> and <string-name><given-names>Elinor W</given-names> <surname>Ames</surname></string-name></person-group>. “<article-title>A multifactor model of infant preferences for novel and familiar stimuli</article-title>.” <source>Advances in infancy research</source> (<year>1988</year>).</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Scott P</given-names> <surname>Johnson</surname></string-name></person-group>. “<article-title>How infants learn about the visual world</article-title>”. <source>Cognitive science</source> <volume>34</volume>.<issue>7</issue> (<year>2010</year>), pp. <fpage>1158</fpage>–<lpage>1184</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Charles</given-names> <surname>Kemp</surname></string-name> and <string-name><given-names>Joshua B</given-names> <surname>Tenenbaum</surname></string-name></person-group>. “<article-title>The discovery of structural form</article-title>”. <source>Proceedings of the National Academy of Sciences</source> <volume>105</volume>.<issue>31</issue> (<year>2008</year>), pp. <fpage>10687</fpage>–<lpage>10692</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel</given-names> <surname>Kersten</surname></string-name>, <string-name><given-names>Pascal</given-names> <surname>Mamassian</surname></string-name>, and <string-name><given-names>Alan</given-names> <surname>Yuille</surname></string-name></person-group>. “<article-title>Object perception as Bayesian inference</article-title>”. <source>Annu. Rev. Psychol</source>. <volume>55</volume> (<year>2004</year>), pp. <fpage>271</fpage>–<lpage>304</lpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Celeste</given-names> <surname>Kidd</surname></string-name>, <string-name><given-names>Steven T</given-names> <surname>Piantadosi</surname></string-name>, and <string-name><given-names>Richard N</given-names> <surname>Aslin</surname></string-name></person-group>. “<article-title>The Goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex</article-title>”. <source>PloS one</source> <volume>7</volume>.<issue>5</issue> (<year>2012</year>), <fpage>e36399</fpage>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Jessica</given-names> <surname>Kosie</surname></string-name> <etal>et al.</etal></person-group> “<article-title>ManyBabies 5: A large-scale investigation of the proposed shift from familiarity preference to novelty preference in infant looking time</article-title>”. <source>PsyArXiv</source>: (<year>2023</year>).</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Michael J</given-names> <surname>Lee</surname></string-name> and <string-name><given-names>James J</given-names> <surname>DiCarlo</surname></string-name></person-group>. “<article-title>An empirical assay of view-invariant object learning in humans and comparison with baseline image-computable models</article-title>”. <source>bioRxiv</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="thesis"><person-group person-group-type="author"><string-name><given-names>Michael Jinsuk</given-names> <surname>Lee</surname></string-name></person-group>. “<source>Rapid Visual Object Learning in Humans is Explainable by Low-Dimensional Image Representations</source>”. <comment>PhD thesis</comment>. <publisher-name>Massachusetts Institute of Technology</publisher-name>, <year>2022</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Falk</given-names> <surname>Lieder</surname></string-name> and <string-name><given-names>Thomas L</given-names> <surname>Griffiths</surname></string-name></person-group>. “<article-title>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</article-title>”. <source>Behavioral and brain sciences</source> <volume>43</volume> (<year>2020</year>), <fpage>e1</fpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Dennis V</given-names> <surname>Lindley</surname></string-name></person-group>. “<article-title>On a measure of the information provided by an experiment</article-title>”. <source>The Annals of Mathematical Statistics</source> <volume>27</volume>.<issue>4</issue> (<year>1956</year>), pp. <fpage>986</fpage>–<lpage>1005</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Lisa M</given-names> <surname>Oakes</surname></string-name></person-group>. “<article-title>Sample size, statistical power, and false conclusions in infant looking-time research</article-title>”. <source>Infancy</source> <volume>22</volume>.<issue>4</issue> (<year>2017</year>), pp. <fpage>436</fpage>–<lpage>469</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mike</given-names> <surname>Oaksford</surname></string-name> and <string-name><given-names>Nick</given-names> <surname>Chater</surname></string-name></person-group>. “<article-title>A rational analysis of the selection task as optimal data selection</article-title>.” <source>Psychological Review</source> <volume>101</volume>.<issue>4</issue> (<year>1994</year>), p. <fpage>608</fpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A</given-names> <surname>Emin Orhan</surname></string-name> and <string-name><given-names>Brenden M</given-names> <surname>Lake</surname></string-name></person-group>. “<article-title>Learning high-level visual representations from a childâs perspective without strong inductive biases</article-title>”. <source>Nature Machine Intelligence</source> <volume>6</volume>.<issue>3</issue> (<year>2024</year>), pp. <fpage>271</fpage>–<lpage>283</lpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emin</given-names> <surname>Orhan</surname></string-name>, <string-name><given-names>Vaibhav</given-names> <surname>Gupta</surname></string-name>, and <string-name><given-names>Brenden M</given-names> <surname>Lake</surname></string-name></person-group>. “<article-title>Self-supervised learning through the eyes of a child</article-title>”. <source>Advances in Neural Information Processing Systems</source> <volume>33</volume> (<year>2020</year>), pp. <fpage>9960</fpage>–<lpage>9971</lpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Markus</given-names> <surname>Paulus</surname></string-name></person-group>. “<article-title>Should infant psychology rely on the violation-of-expectation method? Not anymore</article-title>”. <source>Infant and Child Development</source> <volume>31</volume>.<issue>1</issue> (<year>2022</year>), <fpage>e2306</fpage>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F</given-names> <surname>Poli</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Infants tailor their attention to maximize learning</article-title>”. <source>Science advances</source> <volume>6</volume>.<issue>39</issue> (<year>2020</year>), <fpage>eabb5053</fpage>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesco</given-names> <surname>Poli</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Eight-month-old infants meta-learn by downweighting irrelevant evidence</article-title>”. <source>Open Mind</source> <volume>7</volume> (<year>2023</year>), pp. <fpage>141</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesco</given-names> <surname>Poli</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Individual differences in processing speed and curiosity explain infant habituation and dishabituation performance</article-title>”. <source>Developmental Science</source> <volume>27</volume>.<issue>3</issue> (<year>2024</year>), <fpage>e13460</fpage>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gal</given-names> <surname>Raz</surname></string-name> and <string-name><given-names>Rebecca</given-names> <surname>Saxe</surname></string-name></person-group>. “<article-title>Learning in infancy is active, endogenously motivated, and depends on the prefrontal cortices</article-title>”. <source>Annual Review of Developmental Psychology</source> <volume>2</volume> (<year>2020</year>), pp. <fpage>247</fpage>–<lpage>268</lpage>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Melanie S</given-names> <surname>Schreiner</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Limited evidence of test-retest reliability in infant-directed speech preference in a large preregistered infant experiment</article-title>”. <source>Developmental Science</source> (<year>2022</year>), <fpage>e13551</fpage>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Martin</given-names> <surname>Schrimpf</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Integrative benchmarking to advance neurally mechanistic models of human intelligence</article-title>”. <source>Neuron</source> <volume>108</volume>.<issue>3</issue> (<year>2020</year>), pp. <fpage>413</fpage>–<lpage>423</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Kimberly</given-names> <surname>Scott</surname></string-name> and <string-name><given-names>Laura</given-names> <surname>Schulz</surname></string-name></person-group>. “<article-title>Lookit (part 1): A new online platform for developmental research</article-title>”. <source>Open Mind</source> <volume>1</volume>.<issue>1</issue> (<year>2017</year>), pp. <fpage>4</fpage>–<lpage>14</lpage>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ann M</given-names> <surname>Skoczenski</surname></string-name> and <string-name><given-names>Anthony M</given-names> <surname>Norcia</surname></string-name></person-group>. “<article-title>Neural noise limitations on infant visual sensitivity</article-title>”. <source>Nature</source> <volume>391</volume>.<issue>6668</issue> (<year>1998</year>), pp. <fpage>697</fpage>–<lpage>700</lpage>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ernő</given-names> <surname>Téglás</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Pure reasoning in 12-month-old infants as probabilistic inference</article-title>”. <source>science</source> <volume>332</volume>.<issue>6033</issue> (<year>2011</year>), pp. <fpage>1054</fpage>–<lpage>1059</lpage>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Joshua B</given-names> <surname>Tenenbaum</surname></string-name> <etal>et al.</etal></person-group> “<article-title>How to grow a mind: Statistics, structure, and abstraction</article-title>”. <source>science</source> <volume>331</volume>.<issue>6022</issue> (<year>2011</year>), pp. <fpage>1279</fpage>–<lpage>1285</lpage>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="thesis"><person-group person-group-type="author"><string-name><given-names>Joshua Brett</given-names> <surname>Tenenbaum</surname></string-name></person-group>. “<source>A Bayesian framework for concept learning</source>”. <comment>PhD thesis</comment>. <publisher-name>Massachusetts Institute of Technology</publisher-name>, <year>1999</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Karen</given-names> <surname>Wynn</surname></string-name></person-group>. “<article-title>Psychological foundations of number: Numerical competence in human infants</article-title>”. <source>Trends in cognitive sciences</source> <volume>2</volume>.<issue>8</issue> (<year>1998</year>), pp. <fpage>296</fpage>–<lpage>303</lpage>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel LK</given-names> <surname>Yamins</surname></string-name> <etal>et al.</etal></person-group> “<article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>”. <source>Proceedings of the national academy of sciences</source> <volume>111</volume>.<issue>23</issue> (<year>2014</year>), pp. <fpage>8619</fpage>–<lpage>8624</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102713.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>important</bold> study, the authors provide <bold>compelling</bold> evidence that the likelihood of looking behaviour is predicted by the expected information gain, hence constituting an invaluable formal model and explanation of habituation. Such modelling represents a crucial advance in explanation, over-and-above less specified models that can be fitted post hoc to any empirical pattern. The findings would be of interest to researchers studying cognitive development, and perception and learning more broadly.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102713.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper proposes a new model of perceptual habituation and tests it over two experiments with both infants and adults. The model combines a neural network for visual processing with a Bayesian rational model for attention (i.e., looking time) allocation. This Bayesian framework allows the authors to measure elegantly diverse factors that might drive attention, such as expected information gain, current information gain, and surprise. The model is then fitted to infant and adult participants' data over two experiments, which systematically vary the amount of habituation trials (Experiment 1) and the type of dishabituation stimulus (familiarity, pose, number, identity and animacy). Results show that a model based on (expected) information gain performs better than a model based on surprise. Additionally, while novelty preference is observed when exposure to familiar stimuli is elevated, no familiarity preference is observed when exposure to familiar stimuli is low or intermediate, which is in contrast with past work.</p>
<p>Strengths:</p>
<p>There are three key strengths of this work:</p>
<p>(1) It integrates a neural network model with a Bayesian rational learner, thus bridging the gap between two fields that have often been disconnected. This is rarely seen in the cognitive science field, but the advantages are very clear from this paper: It is possible to have computational models that not only process visual information, but also actively explore the environment based on overarching attentional processes.</p>
<p>(2) By varying parametrically the amount of stimulus exposure and by testing the effects of multiple novel stimulus types, this work allowed the authors to put classical theories of habituation to the test on much finer scales than previous research has done.</p>
<p>(3) The Bayesian model allows the authors to test what specific aspects are different in infants and adults, showing that infants display greater values for the noise parameter.</p>
<p>Weaknesses:</p>
<p>This model pertains visual habituation. What drives infants' (dis)engagement of attention more broadly, for example, when learning the probabilistic structures of the environment around them (e.g., language, action prediction) may follow different principles and dynamics.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.102713.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Raz</surname>
<given-names>Gal</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cao</surname>
<given-names>Anjie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Saxe</surname>
<given-names>Rebecca</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Frank</surname>
<given-names>Michael C</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Weakness:</p>
<p>Although a familiarity preference is not found, it is possible that this is related to the nature of the stimuli and the amount of learning that they offer. While infants here are exposed to the same perceptual stimulus repeatedly, infants can also be familiarised to more complex stimuli or scenarios. Classical statistical learning studies for example expose infants to specific pseudo-words during habituation/familiarisation, and then test their preference for familiar vs novel streams of pseudo-words. The amount of learning progress in these probabilistic learning studies is greater than in perceptual studies, and familiarity preferences may thus be more likely to emerge there. For these reasons, I think it is important to frame this as a model of perceptual habituation. This would also fit well with the neural net that was used, which is processing visual stimuli rather than probabilistic structures. If statements in the discussion are limited to perceptual paradigms, they would make the arguments more compelling.</p>
</disp-quote>
<p>Thank you for your thoughtful feedback. We have now qualified our claims more explicitly throughout the manuscript to clarify the scope of our study. Specifically, we have made the following revisions:</p>
<p>(1) Title Update: We have modified the title to “A stimulus-computable rational model of visual habituation in infants and adults” to explicitly specify the domain of our model.</p>
<p>(2) Qualifying Language Throughout Introduction: We have refined our language throughout the introduction to ensure the scope of our claims is clear. Specifically, we have emphasized that our model applies to visual habituation paradigms by incorporating qualifying language where relevant. At the end of Section 1, we have revised the statement to: &quot;Habituation and dishabituation to sequential visual stimuli are well described by a rational analysis of looking time.&quot; This clarification makes sure that our model is framed within the context of visual habituation paradigms, particularly those involving structured sequences of stimuli, while acknowledging that habituation extends beyond the specific cases we study.</p>
<p>(3) New Paragraph on Scope in the Introduction: We have added language in the Introduction acknowledging that while visual habituation is a fundamental mechanism for learning, it is not the only form of habituation. Specifically, we highlight that: “While habituation is a broadly studied phenomenon across cognitive domains—including language acquisition, probabilistic learning, and concept formation—our focus here is on visual habituation, where infants adjust their attention based on repeated exposure to a visual stimulus.”</p>
<p>(4) New Paragraph on Scope in the General Discussion: We have also revisited this issue in the General Discussion. We added a dedicated paragraph discussing the scope: “This current work focuses on visual habituation, a fundamental but specific form of habituation that applies to sequential visual stimuli. While habituation has been studied across various domains, our model is specifically designed to account for looking time changes in response to repeated visual exposure. This focus aligns with our choice of perceptual representations derived from CNNs, which process visual inputs rather than abstract probabilistic structures. Visual habituation plays a foundational role in infant cognition, as it provides a mechanism for concept learning based on visual experience. However, it does not encompass all forms of habituation, particularly those involving complex rule learning or linguistic structures. Future work should investigate whether models like RANCH can be extended to capture habituation mechanisms in other learning contexts.”</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>There are no formal tests of the predictions of RANCH against other leading hypotheses or models of habituation. This makes it difficult to evaluate the degree to which RANCH provides an alternative account that makes distinct predictions from other accounts. I appreciate that because other theoretical descriptions haven't been instantiated in formal models this might be difficult, but some way of formalising them to enable comparison would be useful.</p>
</disp-quote>
<p>We appreciate the reviewer's concern regarding formal comparisons between RANCH and other leading hypotheses of habituation. A key strength of RANCH is that it provides quantitative, stimulus-computable predictions of looking behavior—something that existing theoretical accounts do not offer. Because previous models can not generate predictions about behaviors, we can not directly compare the previous model with RANCH.</p>
<p>The one formal model that the reviewer might be referring to is the Goldilocks model, discussed in the introduction and shown in Figure 1. We did in fact spend considerable time in an attempt to implement a version of the Goldilocks model as a stimulus-computable framework for comparison. However, we found that it required too many free parameters, such as the precise shape of the inverted U-shape that the Goldilocks model postulates, making it difficult to generate robust predictions that we would feel confident attributing to this model specifically. This assertion may come as a surprise to a reader who expects that formal models should be able to make predictions across many situations, but prior models 1) cannot be applied to specific stimuli, and 2) do not generate dynamics of looking time within each trial. These are both innovations of our work. Instead, even prior formal proposals derive metrics (e.g., surprisal) that can only be correlated with aggregate looking time. And prior, non-formalized theories, such as the Hunter and Ames model, are simply not explicit enough to implement.</p>
<p>To clarify this point, we have now explicitly stated in the Introduction that existing models are not stimulus-computable and do not generate predictions for looking behavior at the level of individual trials:</p>
<p>“Crucially, RANCH is the first stimulus-computable model of habituation, allowing us to derive quantitative predictions from raw visual stimuli. Previous theoretical accounts have described broad principles of habituation, but they do not generate testable, trial-by-trial predictions of looking behavior. As a result, direct comparisons between RANCH and these models remain challenging: existing models do not specify how an agent decides when to continue looking or disengage, nor do they provide a mechanistic link between stimulus properties and looking time. By explicitly modeling these decision processes, RANCH moves beyond post-hoc explanations and offers a computational framework that can be empirically validated and generalized to new contexts.”</p>
<p>We also highlight that our empirical comparisons in Figure 1 evaluate theoretical predictions based on existing conceptual models using behavioral data, rather than direct model-to-model comparisons:</p>
<p>“Addressing these three challenges allowed us to empirically test competing hypotheses about habituation and dishabituation using our experimental data (Figure</p>
<p>\ref{fig:conceptual}). However, because existing models do not generate quantitative predictions, we could not directly compare RANCH to alternative computational models. Instead, we evaluated whether RANCH accurately captured key behavioral patterns in looking time.”</p>
<disp-quote content-type="editor-comment">
<p>The justification for using the RMSEA fitting approach could also be stronger - why is this the best way to compare the predictions of the formal model to the empirical data? Are there others? As always, the main issue with formal models is determining the degree to which they just match surface features of empirical data versus providing mechanistic insights, so some discussion of the level of fit necessary for strong inference would be useful.</p>
</disp-quote>
<p>Thank you for recommending additional clarity on our choice of evaluation metrics. RMSE is a very standard measure (for example, it’s the error metric used in fitting standard linear regression!). On the other hand, it captures absolute rather than relative errors. Correlation-based measures (e.g., r and r<sup>2</sup>-type measures) provide a measure of relative distance between predictive measures. In our manuscript we reported both RMSE and R². In the revised manuscript, we have now:</p>
<p>(1) Added a paragraph in the main text explaining that RMSE captures the absolute error in the same units as looking time, whereas r² reflects the relative proportion of variance explained by the model:</p>
<p>“RANCH predictions qualitatively matched habituation and dishabituation in both infants and adults. To quantitatively evaluate these predictions, we fit a linear model (adjusting model‐generated samples by an intercept and scaling factor) and then assessed two complementary metrics. First, the root mean squared error (RMSE) captures the absolute error in the same units as looking time. Second, the coefficient of determination ($R^2$) measures the relative variation in looking time that is explained by the scaled model predictions. Since each metric relies on different assumptions and highlights distinct aspects of predictive accuracy, they together provide a more robust assessment of model performance. We minimized overfitting by employing cross‐validation—using a split‐half design for infant data and ten‐fold for adult data—to compute both RMSE and $R^2$ on held‐out samples.”</p>
<p>(2) We updated Table 1 to include both RMSE and R² for each model variant and linking hypothesis. We now reported both RMSE and R² across the two experiments.</p>
<p>We hope these revisions address your concerns by offering a more comprehensive and transparent assessment of our model’s predictive accuracy.</p>
<p>Regarding your final question, the desired level of fit for insight, our view is that – at least in theory development – measures of fit should always be compared between alternatives (rather than striving for some absolute level of prediction). We have attempted to do this by comparing fit within- and across-samples and via various ablation studies. We now make this point explicit in the General Discussion:</p>
<p>More generally, while there is no single threshold for what constitutes a “good” model fit, the strength of our approach lies in the relative comparisons across model variants, linking hypotheses, and ablation studies. In this way, we treat model fit not as an absolute benchmark, but as an empirical tool to adjudicate among alternative explanations and assess the mechanistic plausibility of the model’s components.</p>
<disp-quote content-type="editor-comment">
<p>The difference in model predictions for identity vs number relative to the empirical data seems important but isn't given sufficient weight in terms of evaluating whether the model is or is not providing a good explanation of infant behavior. What would falsification look like in this context?</p>
</disp-quote>
<p>We appreciate the reviewer’s observation regarding the discrepancy between model predictions and the empirical data for identity vs.~number violations. We were also very interested in this particular deviation and we discuss it in detail in the General Discussion, noting that RANCH is currently a purely perceptual model, whereas infants’ behavior on number violations may reflect additional conceptual factors. Moreover, because this analysis reflects an out-of-sample prediction, we emphasize the overall match between RANCH and the data (see our global fit metrics) rather than focusing on a single data point. Infant looking time data also exhibit considerable noise, so we caution against over-interpreting small discrepancies in any one condition. In principle, a more thorough “falsification” would involve systematically testing whether larger deviations persist across multiple studies or stimulus sets, which is beyond the scope of the current work.</p>
<disp-quote content-type="editor-comment">
<p>For the novel image similarity analysis, it is difficult to determine whether any differences are due to differences in the way the CNN encodes images vs in the habituation model itself - there are perhaps too many free parameters to pinpoint the nature of any disparities. Would there be another way to test the model without the CNN introducing additional unknowns?</p>
</disp-quote>
<p>Thank you for raising this concern. In our framework, the CNN and the habituation model operate jointly to generate predictions, so it can be challenging to parse out whether any mismatches arise specifically from one component or the other. However, we are not worried that the specifics of our CNN procedure introduces free parameters because:</p>
<p>(1) The  CNN introduces no additional free parameters in our analyses, because it is a pre‐trained model not fitted to our data.</p>
<p>(2) We tested multiple CNN embeddings and observed similar outcomes, indicating that the details of the CNN are unlikely to be driving performance (Figure 12).</p>
<p>Moreover, the key contribution of our second study is precisely that the model can generalize to entirely novel stimuli without any parameter adjustments. By combining a stable, off‐the‐shelf CNN with our habituation model, we can make out‐of‐sample predictions—an achievement that, to our knowledge, no previous habituation model has demonstrated.</p>
<disp-quote content-type="editor-comment">
<p>Related to that, the model contains lots of parts - the CNN, the EIG approach, and the parameters, all of which may or may not match how the infant's brain operates. EIG is systematically compared to two other algorithms, with KL working similarly - does this then imply we can't tell the difference between an explanation based on those two mechanisms? Are there situations in which they would make distinct predictions where they could be pulled apart? Also in this section, there doesn't appear to be any formal testing of the fits, so it is hard to determine whether this is a meaningful difference. However, other parts of the model don't seem to be systematically varied, so it isn't always clear what the precise question addressed in the manuscript is (e.g. is it about the algorithm controlling learning? or just that this model in general when fitted in a certain way resembles the empirical data?)</p>
</disp-quote>
<p>Thank you for highlighting these points about the model’s components and the comparison of EIG- vs. KL-based mechanisms. Regarding the linking hypotheses (EIG, KL, and surprisal), our primary goal was to assess whether rational exploration via noisy perceptual sampling could account for habituation and dishabituation phenomena in a stimulus-computable fashion. Although RANCH contains multiple elements—including the CNN for perceptual embedding, the learning model, and the action policy (EIG or KL)—we did systematically vary the “linking hypothesis” (i.e., whether sampling is driven by EIG, KL, or surprisal). We found that EIG and KL gave very similar fits, while surprisal systematically underperformed.</p>
<p>We agree that future experiments could be designed to produce diverging predictions between EIG and KL, but examining these subtle differences is beyond the scope of our current work. Here, we sought to establish that a rational model of habituation, driven by noisy perceptual sampling, can deliver strong quantitative predictions—even for out-of-sample stimuli—rather than to fully disentangle forward- vs. backward-looking information metrics.</p>
<p>We disagree, however, that we did not evaluate or formally compare other aspects of the model. In Table 1 we report ablation studies of different aspects of the model architecture (e.g., removal of learning and noise components). Further, the RMSE and R² values reported in Table 1 and Section 4.2.3 can be treated as out-of-sample estimates of performance and used for direct comparison (because Table 1 uses cross-validation and Section 4.2.3 reports out of sample predictions).</p>
<p>Perhaps the reviewer is interested in statistical hypothesis tests, but we do not believe these are appropriate here. Cross-validation provides a metric of out-of-sample generalization and model selection based on the resulting numerical estimates. Significance testing is not typically recommended, except in a limited subset of cases (see e.g. Vanwinckelen &amp; Blokeel, 2012 and Raschka, 2018).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>&quot;We treat the number of samples for each stimulus as being linearly related to looking time duration.&quot; Looking times were not log transformed?</p>
</disp-quote>
<p>Thank you for your question. The assumption of a linear relationship between the model’s predicted number of samples and looking time duration is intended as a measurement transformation, not a strict assumption about the underlying distribution of looking times. This linear mapping is used simply to establish a direct proportionality between model-generated samples and observed looking durations.</p>
<p>However, in our statistical analyses, we do log-transform the empirical looking times to account for skewness and stabilize variance. This transformation is standard practice when analyzing infant looking time data but is independent of how we map model predictions to observed times. Since there is no a priori reason to assume that the number of model samples must relate to looking time in a strictly log-linear way, we retained a simple linear mapping while still applying a log transformation in our analytic models where appropriate.</p>
<disp-quote content-type="editor-comment">
<p>It would be nice to have figures showing the results of the grid search over the parameter values. For example, a heatmap with sigma on x and eta on y, and goodness of fit indicated by colour, would show the quality of the model fit as a function of the parameters' values, but also if the parameters estimates are correlated (they shouldn't be).</p>
</disp-quote>
<p>Thank you for the suggestion. We agree that visualizing the grid search results can provide a clearer picture of how different parameter values affect model fit. In the supplementary materials, we already present analyses where we systematically search over one parameter at a time to find the best-fitting values.</p>
<p>We also explored alternative visualizations, including heatmaps where sigma and eta are mapped on the x and y axes, with goodness-of-fit indicated by color. However, we found that the goodness of fit was very similar across parameter settings, making the heatmaps difficult to interpret due to minimal variation in color. This lack of variation in fit reflects the observation that our model predictions are robust to changes in parameter settings, which allows us to report strong out of sample predictions in Section 4. Instead, we opted to use histograms to illustrate general trends, which provide a clearer and more interpretable summary of the model fit across different parameter settings. Please see the heatmaps below, if you are interested.</p>
<fig id="sa2fig1">
<label>Author response image 1.</label>
<caption>
<title>Model fit (measured by RMSE) across a grid of prior values for Alpha, Beta, and V shows minimal variation.</title>
<p>This indicates that the model’s performance is robust to changes in prior assumptions.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-102713-sa2-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>Regarding section 5.4, paragraph 2: It might be interesting to notice that a potential way to decorrelate these factors is to look at finer timescales (see Poli et al., 2024, Trends in Cognitive Sciences), which the current combination of neural nets and Bayesian inference could potentially be adapted to do.</p>
</disp-quote>
<p>Thank you for this insightful suggestion. We agree that examining finer timescales of looking behavior could provide valuable insights into the dynamics of attention and learning. In response, we have incorporated language in Section 5.4 to highlight this as a potential future direction:</p>
<p>Another promising direction is to explore RANCH’s applicability to finer timescales of looking behavior, enabling a more detailed examination of within-trial fluctuations in attention. Recent work suggests that analyzing moment-by-moment dynamics can help disentangle distinct learning mechanisms \autocite{poli2024individual}.Since RANCH models decision-making at the level of individual perceptual samples, it is well-suited to capture these fine-grained attentional shifts.</p>
<disp-quote content-type="editor-comment">
<p>Previous work integrating neural networks with Bayesian (like) models could be better acknowledged: Blakeman, S., &amp; Mareschal, D. (2022). Selective particle attention: Rapidly and flexibly selecting features for deep reinforcement learning. Neural Networks, 150, 408-421.</p>
</disp-quote>
<p>Thank you for this feedback. We have now incorporated this citation into our discussion section:</p>
<p>RANCH integrates structured perceptual representations with Bayesian inference, allowing for stimulus-computable predictions of looking behavior and interpretable parameters at the same time. This integrated approach has been used to study selective attention \autocite{blakeman2022selective}.</p>
<disp-quote content-type="editor-comment">
<p>Unless I missed it, I could not find an OSF repository (although the authors refer to an OSF repository for a previous study that has not been included). In general, sharing the code would greatly help with reproducibility.</p>
</disp-quote>
<p>Thanks for this comment. We apologize that – although all of our code and data were available through github, we did not provide links in the manuscript. We have now added this at the end of the introduction section.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>Page 7 &quot;infants clearly dishabituated on trials with longer exposures&quot; - what are these stats comparing? Novel presentation to last familiar?</p>
</disp-quote>
<p>Thank you for pointing out this slightly confusing passage. The statistics reported are comparing looking time in looking time between the novel and familiar test trials after longer exposures. We have now added the following language:</p>
<p>Infants clearly dishabituated on trials with longer exposures, looking longer at the novel stimulus than the familiar stimulus after long exposure.</p>
<disp-quote content-type="editor-comment">
<p>Order effects were covaried in the model - does the RANCH model predict similar order effects to those observed in the empirical data, ie can it model more generic changes in attention as well as the stimulus-specific ones?</p>
</disp-quote>
<p>Thank you for this question. If we understand correctly, you are asking whether RANCH can capture order effects over the course of the experiment, such as general decreases in attention across blocks. Currently, RANCH does not model these block-level effects—it is designed to predict stimulus-driven looking behavior rather than more general attentional changes that occur over time such as fatigue. In our empirical analysis, block number was included as a covariate to account for these effects statistically, but RANCH itself does not have a mechanism to model block-to-block attentional drift independent of stimulus properties. This is an interesting direction for future work, where a model could integrate global attentional dynamics alongside stimulus-specific learning. To address this, we have added a sentence in the General Discussion saying:</p>
<p>Similarly, RANCH does not capture more global attention dynamics, such as block-to-block attentional drift independent of stimulus properties.</p>
<disp-quote content-type="editor-comment">
<p>&quot;We then computed the root mean squared error (RMSE) between the scaled model results and the looking time data.&quot; Why is this the most appropriate approach to considering model fit? Would be useful to have a brief explanation.</p>
</disp-quote>
<p>Thank you for pointing this out. We believe that we have now addressed this issue in Response to Comment #2 from Reviewer 1.</p>
<disp-quote content-type="editor-comment">
<p>The title of subsection 3.3 made me think that you would be comparing RANCH to alternate hypotheses or models but this seems to be a comparison of ways of fitting parameters within RANCH - I think worth explaining that.</p>
</disp-quote>
<p>We have now added a sentence in the subsection to make the content of the comparison more explicit:</p>
<p>Here we evaluated different ways of specifying RANCH's decision-making mechanism (i.e., different &quot;linking hypotheses&quot; within RANCH).</p>
<disp-quote content-type="editor-comment">
<p>3.5 would be useful to have some statistics here - does performance significantly improve?</p>
</disp-quote>
<p>As discussed above, we systematically compared model variants using cross-validated RMSE and R² values, which provide quantitative evidence of improved performance. While these differences are substantial, we do not report statistical hypothesis tests, as significance testing is not typically appropriate for model comparison based on cross-validation (see Vanwinckelen &amp; Blockeel, 2012; Raschka, 2018). Instead, we rely on out-of-sample predictive performance as a principled basis for evaluating model variants.</p>
<disp-quote content-type="editor-comment">
<p>It would be very helpful to have a formal comparison of RANCH and other models - this seems to be largely descriptive at the moment (3.6).</p>
</disp-quote>
<p>We believe that we have now addressed this issue in our response to the first comment.</p>
<disp-quote content-type="editor-comment">
<p>Does individual infant data show any nonlinearities? Sometimes the position of the peak look is very heterogenous and so overall there appears to be no increase but on an individual level there is.</p>
</disp-quote>
<p>Thank you for your question. Given our experimental design, each exposure duration appears in separate blocks rather than in a continuous sequence for each infant. Because of this, the concept of an individual-level nonlinear trajectory over exposure durations does not directly apply. Instead, each infant contributes looking time data to multiple distinct conditions, rather than following a single increasing-exposure sequence. Any observed nonlinear trend across exposure durations would therefore be a group-level effect rather than a within-subject pattern.</p>
<disp-quote content-type="editor-comment">
<p>In 4.1, why 8 or 9 exposures rather than a fixed number?</p>
</disp-quote>
<p>We used slightly variable exposure durations to reduce the risk that infants develop fixed expectations about when a novel stimulus will appear. We have now clarified this point in the text.</p>
<disp-quote content-type="editor-comment">
<p>Why do results differ for the model vs empirical data for identity? Is this to do with semantic processing in infants that isn't embedded in the model?</p>
</disp-quote>
<p>Thank you for your comment. The discrepancy between the model and empirical data for identity violations is related to the discrepancy we discussed for number violations in the General Discussion. As noted there, RANCH relies on perceptual similarity derived from CNN embeddings, which may not fully capture distinctions that infants make.</p>
<disp-quote content-type="editor-comment">
<p>The model suggests the learner’s prior on noise is higher in infants than adults, so produces potentially mechanistic insights.</p>
</disp-quote>
<p>We agree! One of the key strengths of RANCH is its ability to provide mechanistic insights through interpretable parameters. The finding that infants have a higher prior on perceptual noise than adults aligns with previous research suggesting that early visual processing in infants is more variable and less precise.</p>
</body>
</sub-article>
</article>