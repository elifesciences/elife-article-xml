<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95524</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95524</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95524.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Identifying images in the biology literature that are problematic for people with a color-vision deficiency</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Stevens</surname>
<given-names>Harlan P</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Winegar</surname>
<given-names>Carly V</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Oakley</surname>
<given-names>Arwen F</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Piccolo</surname>
<given-names>Stephen R</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Biology, Brigham Young University</institution>, Provo, UT, <country>USA</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Weissgerber</surname>
<given-names>Tracey L</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Coimbra</institution>
</institution-wrap>
<city>Coimbra</city>
<country>Portugal</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Rodgers</surname>
<given-names>Peter</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>eLife</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Please address correspondence to S.R.P. at <email>stephen_piccolo@byu.edu</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-03-14">
<day>14</day>
<month>03</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2024-08-29">
<day>29</day>
<month>08</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95524</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-11">
<day>11</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-11-30">
<day>30</day>
<month>11</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.29.569308"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-03-14">
<day>14</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95524.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.95524.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95524.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95524.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.95524.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Stevens et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Stevens et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95524-v2.pdf"/>
<abstract>
<title>Abstract</title><p>To help maximize the impact of scientific journal articles, authors must ensure that article figures are accessible to people with color-vision deficiencies (CVDs), which affect up to 8% of males and 0.5% of females. We evaluated images published in biology-and medicine-oriented research articles between 2012 and 2022. Most included at least one color contrast that could be problematic for people with deuteranopia (“deuteranopes”), the most common form of CVD. However, spatial distances and within-image labels frequently mitigated potential problems. Initially, we reviewed 4,964 images from <italic>eLife</italic>, comparing each against a simulated version that approximated how it might appear to deuteranopes. We identified 636 (12.8%) images that we determined would be difficult for deuteranopes to interpret. Our findings suggest that the frequency of this problem has decreased over time and that articles from cell-oriented disciplines were most often problematic. We used machine learning to automate the identification of problematic images. For hold-out test sets from <italic>eLife</italic> (n = 879) and PubMed Central (n = 1,191), a convolutional neural network classified the images with areas under the precision-recall curve of 0.75 and 0.38, respectively. We created a Web application (<ext-link ext-link-type="uri" xlink:href="https://bioapps.byu.edu/colorblind_image_tester">https://bioapps.byu.edu/colorblind_image_tester</ext-link>); users can upload images, view simulated versions, and obtain predictions. Our findings shed new light on the frequency and nature of scientific images that may be problematic for deuteranopes and motivate additional efforts to increase accessibility.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>accessibility</kwd>
<kwd>color vision deficiency</kwd>
<kwd>deuteranopia</kwd>
<kwd>scientific publishing</kwd>
<kwd>image classification</kwd>
<kwd>convolutional neural network</kwd>
</kwd-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have responded to a variety of comments and suggestions from reviewers. Most notably, we have included results for an additional 2000 images.</p></fn>
</fn-group>
<fn-group content-type="external-links">
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://osf.io/8yrkb">https://osf.io/8yrkb</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/srp33/bio_image_colorblindness">https://github.com/srp33/bio_image_colorblindness</ext-link>
</p></fn>
<fn fn-type="dataset"><p>
<ext-link ext-link-type="uri" xlink:href="https://github.com/srp33/colorblind_image_tester">https://github.com/srp33/colorblind_image_tester</ext-link>
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Most humans have trichromatic vision: they perceive blue, green, and red colors using three types of retinal photoreceptor cells that are sensitive to short, medium, or long wavelengths of light, respectively. Color-vision deficiency (CVD) affects between 2% and 8% of males (depending on ancestry) and approximately 0.5% of females<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Congenital CVD is commonly caused by mutations in the genes (or nearby promoter regions) that code for red or green cone photopigments; these genes are proximal to each other on the X chromosome<sup><xref ref-type="bibr" rid="c2">2</xref></sup>.</p>
<p>CVD is divided into categories, the most common being <italic>deutan CVD</italic>, affecting approximately 6% of males of European descent, and <italic>protan CVD</italic>, affecting 2% of males of European descent<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Both categories are commonly known as red-green colorblindness. Within each category, CVD is subclassified according to whether individuals are <italic>dichromats</italic>—able to see two primary colors—or <italic>anomalous trichromats</italic>—able to see three primary colors but differently from <italic>normal trichromats</italic>. Anomalous trichromats differ in the degree of severity with which they can distinguish color patterns. Individuals with deuteranopia (“deuteranopes”) or protanopia do not have corresponding green or red cones, respectively<sup><xref ref-type="bibr" rid="c3">3</xref></sup>. Individuals with deuteranomaly do not have properly functioning green cones, and those with protanomaly do not have properly functioning red cones. People with any of these conditions often see green and red as brown or beige colors. Thus, when images contain shades of green and red—or when either is paired with brown—parts of the image may be indistinguishable. Furthermore, it can be problematic when some pinks or oranges are paired with greens. These issues can lead to incorrect interpretations of figures in scientific journal articles for individuals with CVD.</p>
<p>Efforts have been made to ensure that scientific figures are accessible to people with CVD. For example, researchers have developed algorithms that attempt to recolor images so that people with CVD can more easily interpret them<sup><xref ref-type="bibr" rid="c4">4</xref>–<xref ref-type="bibr" rid="c6">6</xref></sup>. However, these tools are not in wide use, and more work is needed to verify their efficacy in practice. In the meantime, as researchers prepare scientific figures, they can take measures to improve accessibility for people with CVD. For example, they can avoid rainbow color maps that show colors in a gradient; they can use color schemes or color intensities that are CVD friendly<sup><xref ref-type="bibr" rid="c7">7</xref></sup>; additionally, they can provide labels that complement information implied by color differences. However, for the millions of images that have already been published in scientific articles, little is known about the frequency with which these images are CVD friendly. The presence or absence of particular color pairings—and distances between them—can be quantified computationally to estimate this frequency. However, a subjective evaluation of individual images is necessary to identify whether color pairings and distances are likely to affect scientific interpretation.</p>
<p>In this paper, we focus on deuteranopia and its subtypes. To estimate the extent to which the biological and medical literature contains images that may be problematic to deuteranopes, we manually reviewed a “training set” of 4,964 images and two “test sets” of 879 and 1,191 images, respectively. These images were published in articles between the years 2012 and 2022. After identifying images that we deemed most likely to be problematic or not, we used machine-learning algorithms to identify patterns that could discriminate between these two categories of images and thus might be useful for automating the identification of problematic images. If successful, such an algorithm could be used to alert authors, presenters, and publishers that scientific images could be modified to improve visual accessibility and thus make biological and medical fields more inclusive.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Image acquisition</title>
<p>We evaluated images in articles from <italic>eLife</italic>, an open-access journal that publishes research in “all areas of the life sciences and medicine”. Article content from this journal is released under a Creative Commons Attribution license. On June 1, 2022, we downloaded all available images from an Amazon Web Services storage bucket provided by journal staff. We also cloned a GitHub repository that <italic>eLife</italic> provides (<ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences/elife-article-xml">https://github.com/elifesciences/elife-article-xml</ext-link>); this repository contains text and metadata from all articles published in the journal since its inception. For each article, we parsed the article identifier, digital object identifier, article type, article subject, and publication date. We excluded any article that was not published with the “Research article” type. These articles were published between the years 2012 and 2022.</p>
<p>On March 21, 2024, we downloaded a list of articles from the PubMed Central Open Access Subset<sup><xref ref-type="bibr" rid="c8">8</xref></sup>. We filtered the articles to those published between 2012 and 2022 that used a CC BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org">https://creativecommons.org</ext-link> and were categorized as research articles. This filtering resulted in 2,730,256 article candidates.</p>
</sec>
<sec id="s2b">
<title>Image summarization metrics</title>
<p>For each available image, we identified whether the image was either grayscale or contained colors. For each color image, we calculated a series of metrics to summarize the colors, contrasts, and distances between potentially problematic colors. These metrics have similarities to those used to assess recoloring algorithms, including global luminance error<sup><xref ref-type="bibr" rid="c9">9</xref></sup>, local contrast error<sup><xref ref-type="bibr" rid="c10">10</xref></sup>, and global chromatic diversity<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref></sup>. Before calculating the metrics, we sought to make the images more comparable to each other and to reduce the computational demands of analyzing the images. We scaled each image to a height of 300 pixels and generated a quantized version with a maximum of 256 colors. For each image, we then created a second version that simulated how a deuteranope would see the image. To facilitate these simulations, we used the <italic>colorspace</italic> package<sup><xref ref-type="bibr" rid="c13">13</xref></sup> and specified a “severity” value of 0.8. Severity values range between 0 and 1 (with 1 being the most severe). We chose this threshold under the assumption that a mild severity level might not be stringent enough to identify a lack of contrast in the images. However, because many people with deuteranomaly do not have complete deuteranopia, this threshold reflects more moderate cases.</p>
<p>Our approach and rationale for these metrics are described below. In these descriptions, we refer to the quantized, resized images as “original” images and their simulated counterparts as “simulated” images.
<list list-type="bullet">
<list-item><p><italic>Mean, pixel-wise color distance between the original and simulated image</italic>. Our rationale was that the most problematic images would show relatively large overall differences between the original and simulated versions. When calculating these differences, we used version 2000 of Hunt’s distance<sup><xref ref-type="bibr" rid="c14">14</xref></sup>, which quantifies red/green/blue (RGB) color differences in a three-dimensional space. This metric is symmetric, so the results are unaffected by the order in which the colors were specified; we used the absolute value of these distances.</p></list-item>
<list-item><p><italic>Color-distance ratio between the original and simulated images for the color pair with the largest distance in the original image</italic>. First, we excluded black, white, and gray colors. Second, we calculated the color distance (Hunt’s method) between each unique pair of colors in the original image. Third, we calculated the color distance between the colors at the same locations in the simulated image. Fourth, we calculated the ratio between the original distance and the simulated distance. Our rationale was that problematic color pairs would have relatively high contrast (large distances) in the original images and relatively low contrast (small distances) in the simulated images. This approach is similar to that described by Gregor Aisch<sup><xref ref-type="bibr" rid="c15">15</xref></sup>.</p></list-item>
<list-item><p><italic>Number of color pairs that exhibited a high color-distance ratio between the original and simulated images</italic>. This metric is similar to the previous one. However, instead of using the maximum ratio, we counted the number of color pairs with a ratio higher than five; this threshold was used by Gregor Aisch<sup><xref ref-type="bibr" rid="c15">15</xref></sup>. Our rationale was that even if one color pair did not have an extremely high ratio, the presence of many high-ratio pairs would indicate a potential problem.</p></list-item>
<list-item><p><italic>Proportion of pixels in the original image that used a color from one of the high-ratio color pairs</italic>. Again, using a threshold of five, we identified unique colors among the color-distance pairs and counted the number of pixels in the original image that used any of these colors. Our rationale was that a relatively large number of pixels with potentially problematic colors may make an image as difficult for a deuteranope to interpret as an image with a few extremely low-contrast pixels.</p></list-item>
<list-item><p><italic>Mean Euclidean distance between pixels for high-ratio color pairs</italic>. First, we identified color pairs with a ratio higher than five. For each color pair, we identified pixels in the original image that used the two colors and calculated the Euclidean distance between those pixels in the image’s two-dimensional layout. Then, we calculated the mean of these distances. Our rationale was that potentially problematic color pairs close together in an image would be more likely to cause problems than color pairs that are distant within the image.</p></list-item>
</list>
After calculating these metrics for each available image, we calculated a ranked-based score. First, we assigned a rank to each image based on each of the metrics separately. For the “Mean Euclidean distance between pixels for high-ratio color pairs,” relatively large values were given relatively high ranks (indicating that they were less problematic). For the other metrics, relatively small values were given relatively high ranks. Finally, we averaged the ranks to calculate a combined score for each image.</p>
<p>When analyzing images, calculating metrics, and creating figures and tables, we used the R statistical software (version 4.2.1)<sup><xref ref-type="bibr" rid="c16">16</xref></sup> and the following packages:
<list list-type="bullet">
<list-item><p>colorspace (version 2.0-3)<sup><xref ref-type="bibr" rid="c13">13</xref></sup></p></list-item>
<list-item><p>diptest (0.77-1)<sup><xref ref-type="bibr" rid="c17">17</xref></sup></p></list-item>
<list-item><p>doParallel (1.0.17)<sup><xref ref-type="bibr" rid="c18">18</xref></sup></p></list-item>
<list-item><p>knitr (1.44)<sup><xref ref-type="bibr" rid="c19">19</xref></sup></p></list-item>
<list-item><p>magick (2.7.3)<sup><xref ref-type="bibr" rid="c20">20</xref></sup> - interfaces with the ImageMagick software (6.9.10.23)<sup><xref ref-type="bibr" rid="c21">21</xref></sup></p></list-item>
<list-item><p>pROC (1.17.0.1)<sup><xref ref-type="bibr" rid="c22">22</xref></sup></p></list-item>
<list-item><p>spacesXYZ (1.2-1)<sup><xref ref-type="bibr" rid="c23">23</xref></sup></p></list-item>
<list-item><p>tidyverse (2.0.0)<sup><xref ref-type="bibr" rid="c24">24</xref></sup></p></list-item>
<list-item><p>xml2 (1.3.3)<sup><xref ref-type="bibr" rid="c25">25</xref></sup></p></list-item>
</list>
</p>
</sec>
<sec id="s2c">
<title>Qualitative image evaluation</title>
<p>We manually reviewed images to assess qualitatively whether visual characteristics were likely to be problematic for deuteranopes. Our intent was to establish a reference standard for evaluating the quantitative metrics we had calculated. Initially, we randomly sampled 1,000 <italic>eLife</italic> images from those we had downloaded. Two authors of this paper (HPS and AFO) reviewed each of the original (non-quantized, non-resized) images and the corresponding image that was simulated to reflect deuteranopia (severity = 0.8). Neither of these authors has been diagnosed with deuteranopia. This ensured the reviewers could compare the images with and without deuteranopia simulation. To avoid confirmation bias, neither author played a role in defining the quantitative metrics described above. Both authors reviewed the images and recorded observations based on four criteria:
<list list-type="order">
<list-item><p>Did an image contain shades of red, green, and/or orange that might be problematic for deuteranopes?</p></list-item>
<list-item><p>When an image contained potentially problematic color shades, did the color contrasts negate the potential problem? (The reviewers examined the images in their original and simulated forms when evaluating the contrasts.)</p></list-item>
<list-item><p>When an image contained potentially problematic color shades, did within-image labels mitigate the potential problem?</p></list-item>
<list-item><p>When an image contained potentially problematic color shades, were the colors sufficiently, spatially distant from each other so that the colors were unlikely to be problematic?</p></list-item>
</list>
After discussing a given image, the reviewers recorded a joint conclusion about whether the image was “Definitely problematic,” “Probably problematic,” “Probably okay,” or “Definitely okay.” For images that had no visually detectable color, the reviewers recorded “Gray-scale.”</p>
<p>After this preliminary phase, we randomly selected an additional 4,000 images from <italic>eLife</italic> and completed the same process. During the review process, we identified 36 cases where multiple versions of the same image had been sampled. We reviewed these versions manually and found that subsequent versions either had imperceptible differences or slight differences in the ways that sub-figures were laid out. None of these changes affected the colors used. Thus, we excluded the duplicate images and retained the earliest version of each image. The resulting 4,964 images constituted a “training set,” which we used to evaluate our calculated metrics and to train classification models (see below).</p>
<p>Later, we randomly selected an additional 1,000 images from <italic>eLife</italic>, which we used as a “hold-out test set.” Again, we excluded duplicate images and those for which different versions were present in the training set and hold-out test set. The same authors (HPS and AFO) performed the manual review process for these images.</p>
<p>From the candidate articles in the PubMed Central Open Access Subset, we randomly selected 2,000 articles. Two authors (HPS and AFO) manually reviewed these images.</p>
</sec>
<sec id="s2d">
<title>Classification analyses</title>
<p>We used classification algorithms to discriminate between images that we had manually labeled as either “Definitely problematic” or “Definitely okay.” Although it reduced our sample size, we excluded the “Probably problematic” and “Probably okay” images with the expectation that a smaller but more definitive set of examples would produce a more accurate model. Removing these images reduced our training set to 4,501 images.</p>
<p>First, we evaluated our ability to classify images as “Definitely problematic” or “Definitely okay” based on the five metrics we devised. For this task, we used the following classification algorithms, which are designed for one-dimensional data: Random Forests<sup><xref ref-type="bibr" rid="c26">26</xref></sup>, k-nearest neighbors<sup><xref ref-type="bibr" rid="c27">27</xref></sup>, and logistic regression<sup><xref ref-type="bibr" rid="c28">28</xref></sup>. We used implementations of these algorithms in scikit-learn (version 1.1.3)<sup><xref ref-type="bibr" rid="c29">29</xref></sup> with default hyperparameters, other than two exceptions. We used the “liblinear” solver for logistic regression, and we set the “class_weight” hyperparameter to “balanced” for Random Forests and Logistic Regression. For evaluation, we used three iterations of five-fold cross validation. For the test samples in each fold, we calculated the area under the receiver operating characteristic curve (AUROC)<sup><xref ref-type="bibr" rid="c30">30</xref>,<xref ref-type="bibr" rid="c31">31</xref></sup> using the <italic>yardstick</italic> package (1.2.0)<sup><xref ref-type="bibr" rid="c32">32</xref></sup>; we calculated the area under the precision-recall curve (AUPRC) using the <italic>PRROC</italic> package (1.3.1)<sup><xref ref-type="bibr" rid="c33">33</xref></sup>. We calculated the median AUROC and AUPRC across the folds and then averaged them across the three iterations.</p>
<p>Second, we evaluated our ability to classify the images as “Definitely problematic” or “Definitely okay” based on the images themselves. We used a convolutional neural network (CNN) because CNN models are capable of handling two-dimensional inputs and accounting for spatial patterns and colors within images. To generate the CNN models, we used the Tensorflow (2.10.0) and Keras (2.10.0) packages<sup><xref ref-type="bibr" rid="c34">34</xref>,<xref ref-type="bibr" rid="c35">35</xref></sup>. To support transfer learning (described below), we scaled both dimensions of each image to 224. To select from different configurations, we again used three iterations of five-fold cross validation (with the same assignments as the earlier classification analysis). Each model configuration extended a baseline configuration that had eight, two-dimensional, convolutional layers; each layer used batch normalization and the <italic>ReLU</italic> activation function. Subsequent layers increased in size, starting with 32 nodes and increasing to 64, 128, 256, 512, and 728. We trained for 30 epochs with an Adam optimization set, a learning rate of 1e-3, and the binary cross-entropy loss function. The output layer used a sigmoid activation function.</p>
<p>In addition to the baseline configuration, we tested 22 model configurations based on combinations of the following techniques:
<list list-type="bullet">
<list-item><p><italic>Class weighting</italic> - To address class imbalance (most images were “Definitely okay” in the training set), we increased the weight of the minority class (“Definitely problematic”) proportionally to its frequency in the training set.</p></list-item>
<list-item><p><italic>Early stopping</italic> - During model training, classification performance on the (internal) validation set is monitored to identify an epoch when the performance is no longer improving or has begun to degrade; the goal of this technique is to find a balance between underfitting and overfitting.</p></list-item>
<list-item><p><italic>Random flipping and rotation</italic> - In an attempt to prevent overfitting, we enabled random, horizontal flipping of training images and data augmentation via differing amounts of random image rotation<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. We evaluated rotation thresholds of 0.2 and 0.3.</p></list-item>
<list-item><p><italic>Dropout</italic> - Again to prevent overfitting, we temporarily removed a subset of neurons from the network. We evaluated dropout rates of 0.2 and 0.5.</p></list-item>
<list-item><p><italic>Transfer learning</italic> - This technique uses a corpus of ancillary images such that model building is informed by patterns observed previously. We evaluated two corpuses: MobileNetV2<sup><xref ref-type="bibr" rid="c37">37</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup> and ResNet50<sup><xref ref-type="bibr" rid="c39">39</xref></sup>. MobileNetV2 is a 53-layer convolutional neural network trained on more than a million images from the ImageNet database to classify images with objects into 1,000 categories. ResNet50 is a 50-layer convolutional neural network, similarly trained. MobileNetV2 is designed for use on mobile devices, so it is optimized to be lightweight. As such, MobileNetV2 uses 3.4 million parameters, while ResNet50 uses over 25 million trainable parameters. When we applied transfer learning from either ResNet50 or MobileNetV2, we did not use the baseline configuration. Instead, we added a global pooling function into a dense layer. Because our training dataset was relatively small, we expected that adding fewer layers might reduce the risk of overfitting.</p></list-item>
<list-item><p><italic>Fine tuning</italic> - In combination with transfer learning, we sometimes employed a two-phase training process involving an initial training phase and a fine-tuning phase. In the initial phase, we used a pre-trained model (ResNet50 or MobileNetV2) as the base model. During this phase, we froze the layers of the base model to retain the learned weights and trained the combined model for 30 epochs with a learning rate of 0.001. In the fine-tuning phase, we unfroze the layers of the base model to allow the entire model to be retrained. To avoid large adjustments that could disrupt the pre-trained weights, we reduced the learning rate to 1e-5 and trained the model for an additional 15 epochs. This phase enabled the model to make subtle updates to the pre-trained weights. After fine-tuning, the layers of the base model were refrozen.</p></list-item>
</list>
When training each model configuration, we used AUROC to evaluate the predictive performance on the internal validation sets. After comparing the model configurations via cross validation, we used the full training set to train a model, which we used to make predictions for the hold-out test sets. We calculated the following: true positives, false positives, true negatives, false negatives, accuracy, precision (positive predictive value), recall (sensitivity), AUROC, and AUPRC.</p>
</sec>
<sec id="s2e">
<title>Web application</title>
<p>We created a Web application using the <italic>Node.js</italic> framework<sup><xref ref-type="bibr" rid="c40">40</xref></sup>. The application enables researchers to evaluate uploaded images. First, users upload an image in PNG or JPEG format. The application displays the image alongside a deuteranopia-simulated version of the image. For simulation, we implemented the Machado et al.<sup><xref ref-type="bibr" rid="c41">41</xref></sup> matrix for deuteranopia in Javascript with a “severity” value of 0.8, the same parameter used in training. If the user requests it, the application predicts whether the image is likely to be problematic for a deuteranope; the prediction includes a probabilistic score so that users can assess the model’s confidence level. With the intent to maximize the generalizability of these predictions, we trained a model using images from the training set and both holdout test sets. To facilitate execution of the CNN within the Web application, we used Tensorflow.js (version 4.0.0)<sup><xref ref-type="bibr" rid="c42">42</xref></sup>.</p>
</sec>
<sec id="s2f">
<title>Code and data availability</title>
<p>The images we used for evaluation and the trained TensorFlow models are stored in an Open Science Framework repository (<ext-link ext-link-type="uri" xlink:href="https://osf.io/8yrkb">https://osf.io/8yrkb</ext-link>). It also includes folders with images marked as “friendly” or “unfriendly” to facilitate examination of images in either category. The code for processing and analyzing the images is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/srp33/bio_image_colorblindness">https://github.com/srp33/bio_image_colorblindness</ext-link>. That repository also includes the calculated metrics, cross-validation assignments, results of image curation, and outputs of the classification algorithms. The Web application code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/srp33/colorblind_image_tester">https://github.com/srp33/colorblind_image_tester</ext-link>.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>We downloaded images from research articles published in the <italic>eLife</italic> journal. Not counting duplicate versions of the same image, we obtained 66,253 images. Of these images, 1,744 (2.6%) were grayscale (no color). Of these images, 56,816 (85.6%) included at least one color pair for which the amount of contrast might be problematic to people with moderate-to-severe deuteranopia (“deuteranopes”). To characterize potentially problematic aspects of each color-based image, we calculated five metrics based on color contrasts and distances; we also compared the color profiles against what deuteranopes might see. The <italic>mean, pixel-wise color distance between the original and simulated image</italic> exhibited a bimodal distribution, according to Hartigans’ Dip Test for Unimodality (p &lt; 0.001)<sup><xref ref-type="bibr" rid="c43">43</xref></sup>. Specifically, 4,708 images (7.3%) had a difference smaller than 0.01, while the median difference for the remaining images was 0.05 (Figure S1). Most other metrics showed similar patterns, although bimodality was less apparent through visual inspection (Figures S2-S4). The exception was the <italic>proportion of pixels in the original image that used a color from one of the high-ratio color pairs</italic>, which was unimodal (p = 1; Figure S5).</p>
<p>We determined that many images with the highest (or lowest, as would be the case for the “Mean Euclidean distance between pixels for high-ration color pairs”) scores for these metrics would be problematic for deuteranopes. However, we noted that certain color pairs were more problematic than others, and the use of effective labels and/or spacing between colors often mitigated potential problems. Thus, to better estimate the extent to which images are problematic for deuteranopes, we manually reviewed a sample of 4,964 images and judged whether it would be likely for deuteranopes to recognize the scientific message behind each image. Additional Data File 1 contains a record of these evaluations, along with comments that indicate either problematic aspects of the images or factors that mitigated potential problems. We concluded that 636 (12.8%) of the images were “Definitely problematic,” whereas 3,865 of the images (77.9%) were “Definitely okay.” The remaining images were grayscale (n = 179), or we were unable to reach a confident conclusion (n = 284). For the images that were “Definitely okay,” we visually detected shades of green and red or orange in 2,348 (60.8%) images; however, in nearly all (99.3%) of these cases, we deemed that the contrasts between the shades were sufficient that a deuteranope could interpret the images. Furthermore, distance between the colors and/or labels within the images mitigated potential problems in 54.2% and 48.4% of cases, respectively.</p>
<p>We evaluated longitudinal trends and differences among biology subdisciplines. In some cases for the <italic>eLife</italic> articles, multiple images came from the same journal article. Therefore, to avoid pseudoreplication, we categorized each <italic>article</italic> as either “Definitely okay” or “Definitely problematic.” If an article included at least one “Definitely problematic” image, we categorized the entire article under this category. The percentage of “Definitely problematic” articles declined steadily between 2012 and 2021, with a modest increase in 2022 (<xref rid="fig1" ref-type="fig">Figure 1</xref>). (Fewer articles were available for 2022 than for prior years.) Using a generalized linear model with a binomial family to perform logistic regression, we found this decline to be statistically significant (p &lt; 0.001). A χ² goodness-of-fit test revealed that the number of “Definitely problematic” articles differed significantly by subdiscipline (p &lt; 0.001). The subdisciplines with the highest percentages of problematic articles were <italic>Cell Biology</italic>, <italic>Developmental Biology</italic>, and <italic>Stem Cells and Regenerative Medicine</italic> (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Longitudinal trends for the <italic>eLife</italic> articles.</title>
<p>For the training set, we summarized our findings per article. This graph shows article counts for the “Definitely okay” and “Definitely problematic” categories for each year evaluated.</p></caption>
<graphic xlink:href="569308v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Trends by biology subdiscipline for the <italic>eLife</italic> articles.</title>
<p>For the training set, this graph shows the percentage of articles categorized as “Definitely problematic” for a given subdiscipline, as indicated in the article metadata. In many cases, a single article was associated with multiple subdisciplines; these articles are shown as “Multidisciplinary.” We used a χ² goodness-of-fit test to calculate the p-value, with the overall proportion of each discipline as the expected probability.</p></caption>
<graphic xlink:href="569308v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Despite the benefits of manual review, this process is infeasible on a large scale. Therefore, we evaluated techniques for automating image classification. As an initial test, we used the five image-quantification metrics. We also combined these into a single, ranked-based score for each image. In all cases, the metrics differed significantly between the “Definitely okay” and “Definitely problematic” images (Figures S6-S11). To estimate their predictive performance, we performed cross validation on the training set. Values relatively close to 1.0 indicate relatively high performance. A value of 0.5 indicates that predictions are no better than random guessing. The best-performing metric was <italic>number of color pairs that exhibited a high color-distance ratio between the original and simulated images</italic> (AUROC: 0.75; AUPRC: 0.34). All the other metrics—except <italic>mean, pixel-wise color distance between the original and simulated image</italic>—performed better than random guessing (Table S1). As an alternative to the combined rank score, we used classification algorithms to make predictions with the five metrics as inputs. In cross validation on the training set, the best-performing algorithm was Logistic Regression (AUROC: 0.82; AUPRC: 0.43; Table S2).</p>
<p>Additionally, we created a convolutional neural network (CNN) to make predictions according to visual and spatial patterns in the images. CNNs are highly configurable and often sensitive to model parameters and configurations. Accordingly, we performed multiple iterations of cross validation on the training set and compared a variety of hyperparameters and configurations. All non-default options performed better than Logistic Regression based on the image-quantification metrics (Table S3). The best-performing approach used class weighting; early stopping; random flipping and rotation (threshold: 0.2); a dropout rate of 0.5; the ResNet pre-trained model for transfer learning; and model fine tuning.</p>
<p>We manually reviewed a hold-out test set that consisted of 1,000 additional images from <italic>eLife</italic> (Additional Data File 2). After we removed images that were not “Definitely okay” or “Definitely problematic,” 879 images remained. For the Logistic Regression algorithm and CNN, we trained models using the full training set and classified each hold-out image as “Definitely okay” or “Definitely problematic.” Logistic Regression classified the images with an AUROC of 0.82 (AUPRC: 0.49; Figures S12-S14). The CNN classified the images with an AUROC of 0.89 (AUPRC: 0.77; <xref rid="fig3" ref-type="fig">Figures 3</xref>-<xref rid="fig4" ref-type="fig">4</xref>, S15). Additional Data File 3 indicates the performance of both models for a variety of classification metrics.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Receiver operating characteristic curve for the Convolutional Neural Network predictions for the images in the <italic>eLife</italic> hold-out test set.</title>
<p>This curve illustrates tradeoffs between sensitivity and specificity. The area under the curve is 0.89.</p></caption>
<graphic xlink:href="569308v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Precision-recall curve for the Convolutional Neural Network predictions for the images in the <italic>eLife</italic> hold-out test set.</title>
<p>This curve illustrates tradeoffs between precision and recall. The area under the curve is 0.75.</p></caption>
<graphic xlink:href="569308v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the 92 hold-out images that were misclassified by the CNN model, we comparing them against our manual annotations and determined that in 13 cases, the reviewers had missed subtle patterns; we conclude that it would be justified to change these labels (Additional Data File 4). For 31 of the misclassified images, we visually identified patterns that might have confused the CNN; however, upon reevaluation, we maintain that the original labels were valid. For the remaining 48 misclassified images, we were unable to identify patterns that seemed likely to have confused the model.</p>
<p>Lastly, we used the Logistic Regression and CNN models to predict “Definitely okay” or “Definitely problematic” status for a second hold-out set with images from PubMed Central (Additional Data File 5). Of the non-grayscale images, 104 were labeled as “Definitely problematic” and 1,087 as “Definitely okay.” Again, the CNN model (AUROC: 0.78; AUPRC: 0.39; <xref rid="fig5" ref-type="fig">Figures 5</xref>-<xref rid="fig6" ref-type="fig">6</xref>, S16) outperformed the Logistic Regression model (AUROC: 0.73; AUPRC: 0.16; Figures S17-S19; Additional Data File 3).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Receiver operating characteristic curve for the Convolutional Neural Network predictions for the images in the PubMed Central hold-out test set.</title>
<p>This curve illustrates tradeoffs between sensitivity and specificity. The area under the curve is 0.78.</p></caption>
<graphic xlink:href="569308v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Precision-recall curve for the Convolutional Neural Network predictions for the images in the PubMed Central hold-out test set.</title>
<p>This curve illustrates tradeoffs between precision and recall. The area under the curve is 0.39.</p></caption>
<graphic xlink:href="569308v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>To learn more about the prevalence of images in the biology literature that may be inaccessible to deuteranopes, we examined thousands of articles from the <italic>eLife</italic> journal. <italic>eLife</italic> uses a content-licensing scheme that made it possible to perform this study in a transparent manner. Additionally, we selected articles from PubMed Central, as images from these articles represent life-science journals more broadly. After manual review, we estimate that 12.8% of the figures in <italic>eLife</italic> would be challenging to interpret for scientists with moderate-to-severe deuteranopia. The percentage of “Definitely problematic” figures in PubMed Central articles was considerably lower (5.2%). One reason is that a much higher percentage (38.5%) of the images from PubMed Central were grayscale compared to those from <italic>eLife</italic> (4.4%). The findings for both sources indicate that color accessibility is a problem for thousands of journal articles per year.</p>
<p>Significant work has been done to address and improve accessibility for individuals with CVD<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. This work can be categorized into four types of studies: simulation methods, recolorization methods, estimating the frequency of accessible images, and educational. Simulation methods have been developed to better understand how images appear to individuals with CVD. Brettel et al. first simulated CVDs using the long, medium, and short (LMS) colorspace<sup><xref ref-type="bibr" rid="c45">45</xref></sup>. For dichromacy, the colors in the LMS space are projected onto an axis that corresponds to the non-functional cone cell. Viénot et al. expanded on this work by applying a 3x3 transformation matrix to simulate images in the same LMS space<sup><xref ref-type="bibr" rid="c46">46</xref></sup>. Machado et al. created matrices to simulate CVDs based on the shift theory of cone cell sensitivity<sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c47">47</xref></sup>. These algorithms allow individuals without CVD to qualitatively test how their images might appear to people with CVD. The simulation algorithms and matrices are freely available and accessible via websites and software packages<sup><xref ref-type="bibr" rid="c48">48</xref>–<xref ref-type="bibr" rid="c51">51</xref></sup>.</p>
<p>CVD simulations have facilitated the creation of colorblind-friendly palettes<sup><xref ref-type="bibr" rid="c52">52</xref></sup>, and they have led to algorithms that recolor images to become more accessible to people with CVD. Recolorization methods focus on enhancing color contrasts and preserving image naturalness<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. Many algorithms have been developed to compensate for dichromacy<sup><xref ref-type="bibr" rid="c53">53</xref>–<xref ref-type="bibr" rid="c64">64</xref></sup>. These algorithms apply a variety of techniques including hue rotation, customized difference addition, node mapping, and generative adversarial networks<sup><xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c60">60</xref></sup>. Many of these methods have been tested for efficacy, both qualitatively and quantitatively<sup><xref ref-type="bibr" rid="c44">44</xref></sup>. Recolorization algorithms have been applied to PC displays, websites, and smart glasses<sup><xref ref-type="bibr" rid="c65">65</xref></sup>. Despite the prevalence of these algorithms, current techniques have not been systematically compared and may sacrifice image naturalness to increase contrast. Additionally, recoloring may not improve the accessibility of some scientific figures because papers often reference colors in figure descriptions; recoloring the image could interfere with matching colors between the text and images.</p>
<p>An increase in available resources for making figures accessible to individuals with CVD has prompted some researchers to investigate whether these resources have been impactful in decreasing the frequency of publishing scientific figures with problematic colors. Frane examined the prevalence of images in psychology journals that could be confusing to people with CVD<sup><xref ref-type="bibr" rid="c66">66</xref></sup>. A group of panelists with CVD qualitatively evaluated 246 images and found that 13.8% of color figures caused difficulty for at least one panelist; this percentage is similar to our findings. They also found that in instructions to authors, journals rarely mentioned the importance of designing figures for CVD accessibility. Angerbauer et al. recruited crowdworkers to analyze a sample of 1,710 published images and to identify issues with the use of color<sup><xref ref-type="bibr" rid="c67">67</xref></sup>. On average, 60% of the sampled images were given a rating of “accessible” across CVD types. From 2000 to 2019, they observed a slight increase in CVD accessibility for published figures.</p>
<p>Educational resources are available to researchers looking to make their figures suitable for people with CVD. For example, Jambor, et al. provide guidelines and examples to help researchers avoid common problems<sup><xref ref-type="bibr" rid="c68">68</xref></sup>. <italic>JetFighter</italic> scans preprints from bioRxiv and searches for rainbow-based color schemes<sup><xref ref-type="bibr" rid="c69">69</xref></sup>. When these are identified, <italic>JetFighter</italic> notifies the authors about page(s) that might need to be adjusted. However, as we have shown, the presence of particular color combinations does not necessarily indicate that an image is problematic to people with deuteranopia. Frequently, a more nuanced evaluation is necessary.</p>
<p>The <italic>seaborn</italic> Python package includes a “colorblind” palette<sup><xref ref-type="bibr" rid="c70">70</xref></sup>. The <italic>colorBlindness</italic> package for R provides simulation tools and CVD-friendly palettes<sup><xref ref-type="bibr" rid="c71">71</xref></sup>. The <italic>scatterHatch</italic> package facilitates creation of CVD-friendly scatter plots for single-cell data<sup><xref ref-type="bibr" rid="c72">72</xref></sup>. When designing figures, researchers may find it useful to first design them so that key elements are distinguishable in grayscale. Then, color can be added—if necessary—to enhance the image. Color should not be used for the sole purpose of making an image aesthetically pleasing. Using minimal color avoids problems that arise from color pairing issues. Rainbow color maps, in particular, should be avoided. If a researcher finds it necessary to include problematic color pairings in figures, they can vary the saturation and intensity of the colors so they are more distinguishable to people with CVD. Many of the problematic figures that we identified in this study originated from fluorescence microscopy experiments, where red and green dyes were used. Choosing alternative color dyes could reduce this problem and improve the interpretability of microscopy images for people in all fields.</p>
<p>Our analysis has limitations. Firstly, it relied on deuteranopia simulations rather than the experiences of deuteranopes. However, by using simulations, the reviewers were capable of seeing two versions of each image: the original and a simulated version. We believe this is important in assessing the extent to which deuteranopia could confound image interpretations. Conceivably, this could be done with deuteranopes after recoloration, but it is difficult to know whether deuteranopes would see the recolored images in the same way that non-deuteranopes see the original images. Secondly, because we used a single, relatively high severity threshold, our simulations do not represent the full spectrum of experiences that scientists with deuteranopia have. Thus, our findings and tools should be relevant to some (but not all) people with deuteranopia. Furthermore, recent evidence suggests that commonly used mathematical representations of color differences are unlikely to reflect human perceptions perfectly<sup><xref ref-type="bibr" rid="c73">73</xref></sup>. As methods evolve for more accurately simulating color perception, we will be more capable of estimating the extent to which scientific figures are problematic for deuteranopes. Thirdly, our evaluations focused on deuteranopia, the most common form of CVD. It will be important to address other forms of CVD, such as protanopia, in future work. Fourthly, we identified some images as “Probably problematic” or “Probably okay.” Using our review process, we were unable to draw firm conclusions about these images. To avoid adding noise to the classification analyses—we excluded these images and provided notes reflecting our reasoning. Future work may help to clarify these labels. Finally, our CNN model performed well at differentiating between “Definitely okay” and “Definitely problematic” images in the <italic>eLife</italic> hold-out test set; however, the model’s predictive performance dropped considerably when applied to the PubMed Central hold-out test set. Many of the <italic>eLife</italic> images are from cell-related research, and we labeled many of these as problematic. Many other image types were also identified as unfriendly, including heat maps, line charts, maps, three-dimensional structural representations of proteins, photographs, network diagrams, etc. Our model may have developed a bias toward patterns specific to image types that are over-represented in <italic>eLife</italic>, affecting its performance for other journals. The PubMed Central Open Access Subset contains articles for thousands of journals, spanning diverse subdisciplines of biology and medicine. It seems likely that this diversity is a factor behind the drop in performance. Future efforts to review larger collections of PubMed Central articles could help to overcome this limitation.</p>
<p>By summarizing color patterns in more than 66,000 images and manually reviewing 8,000 images, we have created an open data resource that other researchers can use to develop their own methods. Using all of these images, we trained a machine-learning model that predicts whether images are friendly to deuteranopes. It is available as a Web application (<ext-link ext-link-type="uri" xlink:href="https://bioapps.byu.edu/colorblind_image_tester">https://bioapps.byu.edu/colorblind_image_tester</ext-link>).</p>
<p>Scientists and others can use it to obtain insights into whether individual images are accessible to scientists with deuteranopia. However, this tool should be used as a starting point only. Human judgment remains essential.</p>
</sec>
<sec id="d1e974" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1095">
<label>Supplementary Material</label>
<media xlink:href="supplements/569308_file03.docx"/>
</supplementary-material>
<supplementary-material id="d1e1102">
<label>Additional Data File 1</label>
<caption><title>Results of manual curation for 5,000 images from the training set.</title></caption>
<media xlink:href="supplements/569308_file04.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1112">
<label>Additional Data File 2</label>
<caption><title>Results of manual curation for 1,000 images from the <italic>eLife</italic> hold-out test set.</title></caption>
<media xlink:href="supplements/569308_file05.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1125">
<label>Additional Data File 3</label>
<caption><title>Performance metrics for predictions made on hold-out test sets.</title><p>This file provides a variety of classification metrics for the two hold-out test sets. AUROC = area under receiver operating characteristic curve. AUPRC = area under precision-recall curve.</p></caption>
<media xlink:href="supplements/569308_file06.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1137">
<label>Additional Data File 4</label>
<caption><title>Results of manual review of misclassified images from the hold-out test set.</title><p>We manually reviewed each image that we had previously classified as “Definitely okay” but that the model predicted as “Definitely problematic” (or vice versa). The Conclusion column indicates our categorical reevaluation of each image. “Unclear” = We continue to conclude that our manual label was correct, and it is unclear what confused the model. “Understandable” = We continue to conclude that our manual label was correct, and we think we understand what confused the model. “Agree” = We acknowledge that the manual label was incorrect, and the model helped us identify that.</p></caption>
<media xlink:href="supplements/569308_file07.xlsx"/>
</supplementary-material>
<supplementary-material id="d1e1148">
<label>Additional Data File 5</label>
<caption><title>Results of manual curation for 2,000 images used from the PubMed Central hold-out test set.</title></caption>
<media xlink:href="supplements/569308_file08.xlsx"/>
</supplementary-material>
</sec>
</body>
<back>
<sec id="s5">
<title>Declarations</title>
<sec id="s5a">
<title>Ethics approval and consent to participate</title>
<p>Not applicable.</p>
</sec>
<sec id="s5b">
<title>Consent for publication</title>
<p>Not applicable.</p>
</sec>
<sec id="s5c">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s5d">
<title>Financial disclosure</title>
<p>The authors received no specific funding for this work.</p>
</sec>
<sec id="s5e">
<title>Author contributions</title>
<p>The following contributions are described using the CRediT Taxonomy<sup><xref ref-type="bibr" rid="c74">74</xref></sup>.</p>
<p>HPS: Conceptualization, Data Curation, Formal Analysis, Investigation, Methodology, Software, Validation, Writing – Original Draft, Writing – Review and Editing</p>
<p>CVW: Conceptualization, Investigation, Methodology, Software, Writing – Original Draft, Writing – Review and Editing</p>
<p>AFO: Data Curation, Formal Analysis, Writing – Review &amp; Editing</p>
<p>SRP: Conceptualization, Formal Analysis, Investigation, Methodology, Project Administration, Resources, Software, Supervision, Visualization, Writing – Original Draft, Writing – Review &amp; Editing</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delpero</surname>, <given-names>W. T.</given-names></string-name>, <string-name><surname>O’Neill</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Casson</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Hovis</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Aviation-relevent epidemiology of color vision deficiency</article-title>. <source>Aviation, space, and environmental medicine</source> <volume>76</volume>, <fpage>127</fpage>–<lpage>133</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nathans</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Thomas</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Hogness</surname>, <given-names>D. S</given-names></string-name></person-group>. <article-title>Molecular genetics of human color vision: The genes encoding blue, green, and red pigments</article-title>. <source>Science</source> <volume>232</volume>, <fpage>193</fpage>–<lpage>202</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simunovic</surname>, <given-names>M. P.</given-names></string-name></person-group> <article-title>Colour vision deficiency</article-title>. <source>Eye</source> <volume>24</volume>, <fpage>747</fpage>–<lpage>755</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Flatla</surname>, <given-names>D. R</given-names></string-name></person-group>. <article-title>Accessibility for individuals with color vision deficiency</article-title>. in <source>Proceedings of the 24th annual ACM symposium adjunct on User interface software and technology</source> <fpage>31</fpage>–<lpage>34</lpage> (<publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <year>2011</year>). doi:<pub-id pub-id-type="doi">10.1145/2046396.2046412</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname>, <given-names>H.-Y.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.-Q.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>M.-L</given-names></string-name></person-group>. <article-title>Improving Discrimination in Color Vision Deficiency by Image Re-Coloring</article-title>. <source>Sensors</source> <volume>19</volume>, <fpage>2250</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsekouras</surname>, <given-names>G. E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A Novel Approach to Image Recoloring for Color Vision Deficiency</article-title>. <source>Sensors</source> <volume>21</volume>, <fpage>2740</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crameri</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Shephard</surname>, <given-names>G. E.</given-names></string-name> &amp; <string-name><surname>Heron</surname>, <given-names>P. J</given-names></string-name></person-group>. <article-title>The misuse of colour in science communication</article-title>. <source>Nat Commun</source> <volume>11</volume>, <fpage>5444</fpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="other"><source>PMC Open Access Subset. PubMed Central (PMC)</source></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuhn</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Oliveira</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Fernandes</surname>, <given-names>L. A. F</given-names></string-name></person-group>. <article-title>An improved contrast enhancing approach for color-to-grayscale mappings</article-title>. <source>Visual Comput</source> <volume>24</volume>, <fpage>505</fpage>–<lpage>514</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Naturalness-and information-preserving image recoloring for red–green dichromats</article-title>. <source>Signal Processing: Image Communication</source> <volume>76</volume>, <fpage>68</fpage>–<lpage>80</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Bao</surname>, <given-names>H</given-names></string-name></person-group>. <article-title>An Efficient Direct Volume Rendering Approach for Dichromats</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source> <volume>17</volume>, <fpage>2144</fpage>–<lpage>2152</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Gu</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Color discrimination enhancement for dichromats using self-organizing color transformation</article-title>. <source>Information Sciences</source> <volume>179</volume>, <fpage>830</fpage>–<lpage>843</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stauffer</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mayr</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Dabernig</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Zeileis</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Somewhere over the rainbow: How to make effective use of colors in meteorological visualizations</article-title>. <source>Bulletin of the American Meteorological Society</source> <volume>96</volume>, <fpage>203</fpage>–<lpage>216</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hunt</surname>, <given-names>R. W. G</given-names></string-name></person-group>. <source>The Reproduction of Colour</source>. (<publisher-name>John Wiley &amp; Sons</publisher-name>, <year>2005</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Aisch</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>I wrote some code that automatically checks visualizations for non-colorblind safe colors. Here’s how it works</article-title>. <source>vis4.net</source> (<year>2018</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>R Core Team</collab></person-group>. <source>R: A Language and Environment for Statistical Computing</source>. (<publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>, <year>2022</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Maechler</surname>, <given-names>M.</given-names></string-name></person-group> <article-title>Diptest: Hartigan’s Dip Test Statistic for Unimodality - Corrected</article-title>. <source>R package</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Corporation</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Weston</surname>, <given-names>S.</given-names></string-name></person-group> <article-title>doParallel: Foreach Parallel Adaptor for the ’Parallel’ Package</article-title>. <source>R package</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Xie</surname>, <given-names>Y</given-names></string-name></person-group>. <chapter-title>Knitr: A comprehensive tool for reproducible research</chapter-title> in R. in <source>Implementing reproducible computational research</source> (eds. <person-group person-group-type="editor"><string-name><surname>Stodden</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Leisch</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Peng</surname>, <given-names>R. D.</given-names></string-name></person-group>) (<publisher-name>Chapman and Hall/CRC</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Ooms</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Magick: Advanced Graphics and Image-Processing in R</article-title>. <source>R package</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Still</surname>, <given-names>M</given-names></string-name></person-group>. <source>The Definitive Guide to ImageMagick</source>. (<publisher-name>Apress</publisher-name>, <year>2006</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robin</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>pROC: An open-source package for R and S+ to analyze and compare ROC curves</article-title>. <source>BMC bioinformatics</source> <volume>12</volume>, <issue>77</issue> (<year>2011</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Davis</surname>, <given-names>G.</given-names></string-name></person-group> <article-title>spacesXYZ: CIE XYZ and Some of Its Derived Color Spaces</article-title>. <source>R package</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wickham</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Welcome to the tidyverse</article-title>. <source>Journal of Open Source Software</source> <volume>4</volume>, <fpage>1686</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Wickham</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hester</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Ooms</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Xml2: Parse XML</article-title>. <source>R package</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Random forests</article-title>. <source>Machine learning</source> <volume>45</volume>, <fpage>5</fpage>–<lpage>32</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fix</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Hodges</surname>, <given-names>J. L.</given-names></string-name></person-group> <article-title>Discriminatory analysis. Nonparametric discrimination: Consistency properties</article-title>. <source>International Statistical Review/Revue Internationale de Statistique</source> <volume>57</volume>, <fpage>238</fpage>–<lpage>247</lpage> (<year>1989</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nelder</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Wedderburn</surname>, <given-names>R. W.</given-names></string-name></person-group> <article-title>Generalized linear models</article-title>. <source>Journal of the Royal Statistical Society Series A: Statistics in Society</source> <volume>135</volume>, <fpage>370</fpage>–<lpage>384</lpage> (<year>1972</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source> <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanner</surname> <suffix>Jr</suffix>, <given-names>W. P.</given-names></string-name> &amp; <string-name><surname>Swets</surname>, <given-names>J.A.</given-names></string-name></person-group>. <article-title>A decision-making theory of visual detection</article-title>. <source>Psychological review</source> <volume>61</volume>, <issue>401</issue> (<year>1954</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Swets</surname>, <given-names>J. A</given-names></string-name></person-group>. <article-title>Measuring the accuracy of diagnostic systems</article-title>. <source>Science</source> <volume>240</volume>, <fpage>1285</fpage>–<lpage>1293</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Kuhn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Vaughan</surname>, <given-names>D.</given-names></string-name> &amp; <string-name><surname>Hvitfeldt</surname>, <given-names>E.</given-names></string-name></person-group> <article-title>Yardstick: Tidy Characterizations of Model Performance</article-title>. <source>R package</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grau</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Grosse</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Keilwagen</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>PRROC: Computing and visualizing precision-recall and receiver operating characteristic curves in R</article-title>. <source>Bioinformatics</source> <volume>31</volume>, <fpage>2595</fpage>–<lpage>2597</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abadi</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>TensorFlow: A System for Large-Scale Machine Learning</article-title>. in <source>12th USENIX symposium on operating systems design and implementation (OSDI 16)</source> <fpage>265</fpage>–<lpage>283</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Géron</surname>, <given-names>A</given-names></string-name></person-group>. <source>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</source> <publisher-name>O’Reilly Media, Inc.</publisher-name>, <year>2022</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Gatt</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Stamatescu</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>McDonnell</surname>, <given-names>M. D</given-names></string-name></person-group>. <article-title>Understanding Data Augmentation for Classification: When to Warp?</article-title> in <source>2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)</source> <fpage>1</fpage>–<lpage>6</lpage> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1109/DICTA.2016.7797091</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sandler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Howard</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhmoginov</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Chen</surname>, <given-names>L.-C.</given-names></string-name></person-group> <article-title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</article-title>. <source>arXiv</source>(<year>2019</year>) doi:<pub-id pub-id-type="doi">10.48550/arXiv.1801.04381</pub-id>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Hinton</surname>, <given-names>G. E</given-names></string-name></person-group>. <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Advances in neural information processing systems</source> <volume>25</volume>, (<year>2012</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>Deep residual learning for image recognition</article-title>. in <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name> <fpage>770</fpage>–<lpage>778</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="software"><source>Node.js</source></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Machado</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Oliveira</surname>, <given-names>M. M.</given-names></string-name> &amp; <string-name><surname>Fernandes</surname>, <given-names>L. A. F</given-names></string-name></person-group>. <article-title>A physiologically-based model for simulation of color vision deficiency</article-title>. <source>IEEE Trans Vis Comput Graph</source> <volume>15</volume>, <fpage>1291</fpage>–<lpage>1298</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="software"><source>TensorFlow.js</source></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartigan</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Hartigan</surname>, <given-names>P. M</given-names></string-name></person-group>. <article-title>The dip test of unimodality</article-title>. <source>The annals of Statistics</source> <fpage>70</fpage>–<lpage>84</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name> &amp; <string-name><surname>Mao</surname>, <given-names>X</given-names></string-name></person-group>. <article-title>Image recoloring for color vision deficiency compensation: A survey</article-title>. <source>Vis Comput</source> <volume>37</volume>, <fpage>2999</fpage>–<lpage>3018</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brettel</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Viénot</surname>, <given-names>F.</given-names></string-name> &amp; <string-name><surname>Mollon</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Computerized simulation of color appearance for dichromats</article-title>, <source>J. Opt. Soc. Am. A</source> <volume>14</volume>, <fpage>2647</fpage>–<lpage>2655</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Viénot</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Brettel</surname>, <given-names>H.</given-names></string-name> &amp; <string-name><surname>Mollon</surname>, <given-names>J. D</given-names></string-name></person-group>. <article-title>Digital video colourmaps for checking the legibility of displays by dichromats</article-title>. <source>Color Research &amp; Application</source> <volume>24</volume>, <fpage>243</fpage>–<lpage>252</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stockman</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Sharpe</surname>, <given-names>L. T</given-names></string-name></person-group>. <article-title>The spectral sensitivities of the middle-and long-wavelength-sensitive cones derived from measurements in observers of known genotype</article-title>. <source>Vision Research</source> <volume>40</volume>, <fpage>1711</fpage>– <lpage>1737</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="software"><source>Coblis — Color Blindness Simulator – Colblindor</source></mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="other">Color blind safe colors on color wheel Adobe Color</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>DaltonLens</collab></person-group> <article-title>DaltonLens-Python</article-title> (<year>2023</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Wilke</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Colorblindr</article-title>. <source>R package</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olson</surname>, <given-names>J. M.</given-names></string-name> &amp; <string-name><surname>Brewer</surname>, <given-names>C. A</given-names></string-name></person-group>. <article-title>An Evaluation of Color Selections to Accommodate Map Users with Color-Vision Impairments</article-title>. <source>Annals of the Association of American Geographers</source> <volume>87</volume>, <fpage>103</fpage>–<lpage>134</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Jefferson</surname>, <given-names>L.</given-names></string-name> &amp; <string-name><surname>Harvey</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>An interface to support color blind computer users</article-title>. in <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name> <fpage>1535</fpage>–<lpage>1538</lpage> (<publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <year>2007</year>). doi:<pub-id pub-id-type="doi">10.1145/1240624.1240855</pub-id>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>J.-B.</given-names></string-name>, <string-name><surname>Tseng</surname>, <given-names>Y.-C.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>S.-I.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>S.-J</given-names></string-name></person-group>. <article-title>Information Preserving Color Transformation for Protanopia and Deuteranopia</article-title>. <source>IEEE Signal Processing Letters</source> <volume>14</volume>, <fpage>711</fpage>–<lpage>714</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Rumiński</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Color transformation methods for dichromats</article-title>. in <conf-name>3rd International Conference on Human System Interaction</conf-name> <fpage>634</fpage>–<lpage>641</lpage> (<year>2010</year>). doi:<pub-id pub-id-type="doi">10.1109/HSI.2010.5514503</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rasche</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Geist</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Westall</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Re-coloring Images for Gamuts of Lower Dimension</article-title>. <source>Computer Graphics Forum</source> <volume>24</volume>, <fpage>423</fpage>–<lpage>432</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Machado</surname>, <given-names>G. M.</given-names></string-name> &amp; <string-name><surname>Oliveira</surname>, <given-names>M. M</given-names></string-name></person-group>. <article-title>Real-Time Temporal-Coherent Color Contrast Enhancement for Dichromats</article-title>. <source>Computer Graphics Forum</source> <volume>29</volume>, <fpage>933</fpage>–<lpage>942</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ching</surname>, <given-names>S.-L.</given-names></string-name> &amp; <string-name><surname>Sabudin</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Website image colour transformation for the colour blind</article-title>. in <conf-name>2010 2nd International Conference on Computer Technology and Development</conf-name> <fpage>255</fpage>–<lpage>259</lpage> (<year>2010</year>). doi:<pub-id pub-id-type="doi">10.1109/ICCTD.2010.5645874</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Gomes</surname>, <given-names>A. J. P</given-names></string-name></person-group>. <article-title>Recoloring Algorithms for Colorblind People: A Survey</article-title>. <source>ACM Comput. Surv</source>. <volume>52</volume>, <issue>72</issue>:<fpage>1</fpage>–<lpage>72</lpage>:37 (<year>2019</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Color vision deficiency datasets &amp; recoloring evaluation using GANs</article-title>. <source>Multimed Tools Appl</source> <volume>79</volume>, <fpage>27583</fpage>–<lpage>27614</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>X.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Fast contrast and naturalness preserving image recolouring for dichromats</article-title>. <source>Computers &amp; Graphics</source> <volume>98</volume>, <fpage>19</fpage>–<lpage>28</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nakauchi</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Onouchi</surname>, <given-names>T</given-names></string-name></person-group>. <article-title>Detection and modification of confusing color combinations for red-green dichromats to achieve a color universal design</article-title>. <source>Color Research &amp; Application</source> <volume>33</volume>, <fpage>203</fpage>–<lpage>211</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Processing images for red–green dichromats compensation via naturalness and information-preservation considered recoloring</article-title>. <source>Vis Comput</source> <volume>35</volume>, <fpage>1053</fpage>–<lpage>1066</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Gu</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Wang</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Color discrimination enhancement for dichromats using self-organizing color transformation</article-title>. <source>Information Sciences</source> <volume>179</volume>, <fpage>830</fpage>–<lpage>843</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tanuwidjaja</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Chroma: A wearable augmented-reality solution for color blindness</article-title>. in <conf-name>Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</conf-name> <fpage>799</fpage>–<lpage>810</lpage> (<publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <year>2014</year>). doi:<pub-id pub-id-type="doi">10.1145/2632048.2632091</pub-id>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frane</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>A Call for Considering Color Vision Deficiency When Creating Graphics for Psychology Reports</article-title>. <source>The Journal of General Psychology</source> <volume>142</volume>, <fpage>194</fpage>–<lpage>211</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Angerbauer</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures</article-title>. in <conf-name>Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</conf-name> <fpage>1</fpage>–<lpage>23</lpage> (<publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <year>2022</year>). doi:<pub-id pub-id-type="doi">10.1145/3491102.3502133</pub-id>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jambor</surname>, <given-names>H.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Creating clear and informative image-based figures for scientific publications</article-title>. <source>PLoS Biol</source> <volume>19</volume>, <fpage>e3001161</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Saladi</surname>, <given-names>S</given-names></string-name></person-group> <article-title>JetFighter: Towards figure accuracy and accessibility</article-title>. <source>eLife</source> (<year>2019</year>).</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waskom</surname>, <given-names>M. L</given-names></string-name></person-group>. <article-title>Seaborn: Statistical data visualization</article-title>. <source>Journal of Open Source Software</source> <volume>6</volume>, <fpage>3021</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Ou</surname>, <given-names>J.</given-names></string-name></person-group> <article-title>colorBlindness: Safe Color Set for Color Blindness</article-title>. <source>R package</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guha</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fertig</surname>, <given-names>E. J.</given-names></string-name> &amp; <string-name><surname>Deshpande</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Generating colorblind-friendly scatter plots for single-cell data</article-title>. <source>eLife</source> <volume>11</volume>, <fpage>e82128</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bujack</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Teti</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Caffrey</surname>, <given-names>E.</given-names></string-name> &amp; <string-name><surname>Turton</surname>, <given-names>T. L</given-names></string-name></person-group>. <article-title>The non-Riemannian nature of perceptual color space</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>119</volume>, <fpage>e2119753119</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brand</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Altman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hlava</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Scott</surname>, <given-names>J</given-names></string-name></person-group>. <article-title>Beyond authorship: Attribution, contribution, collaboration, and credit</article-title>. <source>Learned Publishing</source> <volume>28</volume>, <fpage>151</fpage>–<lpage>155</lpage> (<year>2015</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95524.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Weissgerber</surname>
<given-names>Tracey L</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Coimbra</institution>
</institution-wrap>
<city>Coimbra</city>
<country>Portugal</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>In this <bold>important</bold> study, the authors manually assessed randomly selected images published in eLife between 2012 and 2022 to determine whether they were accessible for readers with deuteranopia, the most common form of color vision deficiency. They then developed an automated tool designed to classify figures and images as either &quot;friendly&quot; or &quot;unfriendly&quot; for people with deuteranopia. Such a tool could be used by journals or researchers to monitor the accessibility of figures and images, and the evidence for its utility was <bold>solid</bold>: it performed well for eLife articles, but performance was weaker for a broader dataset of PubMed articles, which were not included in the training data. The authors also provide code that readers can download and run to test their own images, and this may be of most use for testing the tool, as there are already several free, user-friendly recoloring programs that allow users to see how images would look to a person with different forms of color vision deficiency. Automated classifications are of most use for assessing many images, when the user does not have the time or resources to assess each image individually.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95524.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors of this study developed a software application, which aims to identify images as either &quot;friendly&quot; or &quot;unfriendly&quot; for readers with deuteranopia, the most common color-vision deficiency. Using previously published algorithms that recolor images to approximate how they would appear to a deuteranope (someone with deuteranopia), authors first manually assessed a set of images from biology-oriented research articles published in eLife between 2012 and 2022, as well as an additional hold-out set of 2000 articles selected randomly from the PubMed Central Open Access Subset. The researchers identified 636 out of 4964 images as difficult to interpret (&quot;unfriendly&quot;) for deuteranopes in the eLife dataset. In the PubMed Central dataset 104 out of 1191 non-grayscale images were identified as unfriendly. The results for the eLife dataset show a decrease in &quot;unfriendly&quot; images over time and a higher probability for articles from cell-oriented research fields to contain &quot;unfriendly&quot; images.</p>
<p>The researchers used the manually classified images from eLife to develop, train, and validate an automated screening tool. They also created a user-friendly web application of the tool, where users can upload images and be informed about the status of each image as &quot;friendly&quot; or &quot;unfriendly&quot; for deuteranopes.</p>
<p>Strengths:</p>
<p>The authors have identified an important accessibility issue in the scientific literature: the use of color combinations that make figures difficult to interpret for people with color-vision deficiency. The metrics proposed and evaluated in the study are a valuable theoretical contribution. The automated screening tool they provide is well-documented, open source, and relatively easy to install and use. It has the potential to provide a useful service to the scientists who want to make their figures more accessible. The data are open and freely accessible, well documented, and a valuable resource for further research. The manuscript is well-written, logically structured, and easy to follow.</p>
<p>Weaknesses:</p>
<p>(1) The authors themselves acknowledge the limitations that arise from the way they defined what constitutes an &quot;unfriendly&quot; image. There is a missed chance here to have engaged deuteranopes as stakeholders earlier in the experimental design. This would have allowed to determine to what extent spatial separation and labelling of problematic color combinations responds to their needs and whether setting the bar at a simulated severity of 80% is inclusive enough. A slightly lowered barrier is still a barrier to accessibility.</p>
<p>(2) The use of training images from a single journal limits the generalizability of the empirical findings as well as of the automated screening tool itself. This is evidenced by a decrease in performance of the tool on the holdout dataset from PubMed Central. Machine-learning algorithms are highly configurable but also notorious for their lack of transparency and for being easily biased by the training data set. A quick and unsystematic test of the web application shows that the classifier works well for electron microscopy images but fails at recognizing the classical diagnostic images for color-vision deficiency (Ishihara test images) as &quot;unfriendly&quot;. A future iteration of the tool should be trained on a wider variety of images, ideally enriched with diagnostic images found in scientific publications.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95524.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>An analysis of images in the biology literature that are problematic for people with a color-vision deficiency (CVD) is presented, along with a machine learning-based model trained on an eLife dataset to identify such images and a web application that uses the model to flag problematic images. Their analysis reveals that about 13% of the images could be problematic for people with CVD and that the frequency of such images decreased over time. Their best model (convolutional neural network, CNN) yields 0.89 AUROC score and 0.77 AUPRC on held-out eLife articles, but lower scores (0.78 and 0.39, respectively). It is proposed that their approach could help making biology literature accessible to diverse audiences.</p>
<p>Strengths:</p>
<p>The manuscript focuses on an important yet mostly overlooked problem and makes contributions both in expanding our understanding of the extent of the problem and in developing solutions to mitigate the problem. The paper is generally well-written and clearly organized. Their CVD simulation combines five different metrics. The dataset has been assessed by two researchers and is likely to be of high-quality. Machine learning algorithm used (CNN) is an appropriate choice for the problem. The evaluation of various hyperparameters for the CNN model is extensive.</p>
<p>Weaknesses:</p>
<p>While the study has significant strengths, it also has some limitations. Specifically, the focus on one type of CVD (deuteranopia) and selecting images from a single journal (eLife) for training limit the generalizability of the models. This is, to some extent, shown by applying the model to PMC articles, which yields lower performance. &quot;Probably problematic&quot; and &quot;probably okay&quot; classes are excluded from the analysis.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95524.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work focuses on accessibility of scientific images for individuals with color vision deficiencies, particularly deuteranopia. The research involved an analysis of images from eLife and PubMed Central published in 2012-2022. The authors manually reviewed nearly 7,000 images, comparing them with simulated versions representing the perspective of individuals with deuteranopia, and also evaluated several methods to automatically detect such images including training a machine-learning algorithm to do so, which performed the best. The authors found that nearly 13% of the images could be challenging for people with deuteranopia to interpret. There was a trend toward a decrease in problematic images over time, which is encouraging.</p>
<p>After the first round of review, the addition of a random sample of biomedical articles in the evaluation set strengthens the generalizability of the algorithm, and the change to evaluate articles at the article level to address pseudoreplication is appropriate.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95524.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Stevens</surname>
<given-names>Harlan P</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Winegar</surname>
<given-names>Carly V</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Oakley</surname>
<given-names>Arwen F</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Piccolo</surname>
<given-names>Stephen R</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>eLife assessment</bold></p>
<p>In this important study, the authors manually assessed randomly selected images published in eLife between 2012 and 2020 to determine whether they were accessible for readers with deuteranopia, the most common form of color vision deficiency. They then developed an automated tool designed to classify figures and images as either &quot;friendly&quot; or &quot;unfriendly&quot; for people with deuteranopia. While such a tool could be used by publishers, editors or researchers to monitor accessibility in the research literature, the evidence supporting the tools' utility was incomplete. The tool would benefit from training on an expanded dataset that includes different image and figure types from many journals, and using more rigorous approaches when training the tool and assessing performance. The authors also provide code that readers can download and run to test their own images. This may be of most use for testing the tool, as there are already several free, user-friendly recoloring programs that allow users to see how images would look to a person with different forms of color vision deficiency. Automated classifications are of most use for assessing many images, when the user does not have the time or resources to assess each image individually.</p>
</disp-quote>
<p>Thank you for this assessment. We have responded to the comments and suggestions in detail below. One minor correction to the above statement: the randomly selected images published in eLife were from articles published between 2012 and 2022 (not 2020).</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>The authors of this study developed a software application, which aims to identify images as either &quot;friendly&quot; or &quot;unfriendly&quot; for readers with deuteranopia, the most common color-vision deficiency. Using previously published algorithms that recolor images to approximate how they would appear to a deuteranope (someone with deuteranopia), authors first manually assessed a set of images from biology-oriented research articles published in eLife between 2012 and 2022. The researchers identified 636 out of 4964 images as difficult to interpret (&quot;unfriendly&quot;) for deuteranopes. They claim that there was a decrease in &quot;unfriendly&quot; images over time and that articles from cell-oriented research fields were most likely to contain &quot;unfriendly&quot; images. The researchers used the manually classified images to develop, train, and validate an automated screening tool. They also created a user-friendly web application of the tool, where users can upload images and be informed about the status of each image as &quot;friendly&quot; or &quot;unfriendly&quot; for deuteranopes.</p>
<p>Strengths:</p>
<p>The authors have identified an important accessibility issue in the scientific literature: the use of color combinations that make figures difficult to interpret for people with color-vision deficiency. The metrics proposed and evaluated in the study are a valuable theoretical contribution. The automated screening tool they provide is well-documented, open source, and relatively easy to install and use. It has the potential to provide a useful service to the scientists who want to make their figures more accessible. The data are open and freely accessible, well documented, and a valuable resource for further research. The manuscript is well written, logically structured, and easy to follow.</p>
</disp-quote>
<p>We thank the reviewer for these comments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>(1) The authors themselves acknowledge the limitations that arise from the way they defined what constitutes an &quot;unfriendly&quot; image. There is a missed chance here to have engaged deuteranopes as stakeholders earlier in the experimental design. This would have allowed [them] to determine to what extent spatial separation and labelling of problematic color combinations responds to their needs and whether setting the bar at a simulated severity of 80% is inclusive enough. A slightly lowered barrier is still a barrier to accessibility.</p>
</disp-quote>
<p>We agree with this point in principle. However, different people experience deuteranopia in different ways, so it would require a large effort to characterize these differences and provide empirical evidence about many individuals' interpretations of problematic images in the &quot;real world.&quot; In this study, we aimed to establish a starting point that would emphasize the need for greater accessibility, and we have provided tools to begin accomplishing that. We erred on the side of simulating relatively high severity (but not complete deuteranopia). Thus, our findings and tools should be relevant to some (but not all) people with deuteranopia. Furthermore, as noted in the paper, an advantage of our approach is that &quot;by using simulations, the reviewers were capable of seeing two versions of each image: the original and a simulated version.&quot; We believe this step is important in assessing the extent to which deuteranopia could confound image interpretations. Conceivably, this could be done with deuteranopes after recoloration, but it is difficult to know whether deuteranopes would see the recolored images in the same way that non-deuteranopes see the original images. It is also true that images simulating deuteranopia may not perfectly reflect how deuteranopes see those images. It is a tradeoff either way. We have added comments along these lines to the paper.</p>
<disp-quote content-type="editor-comment">
<p>(2) The use of images from a single journal strongly limits the generalizability of the empirical findings as well as of the automated screening tool itself. Machine-learning algorithms are highly configurable but also notorious for their lack of transparency and for being easily biased by the training data set. A quick and unsystematic test of the web application shows that the classifier works well for electron microscopy images but fails at recognizing red-green scatter plots and even the classical diagnostic images for color-vision deficiency (Ishihara test images) as &quot;unfriendly&quot;. A future iteration of the tool should be trained on a wider variety of images from different journals.</p>
</disp-quote>
<p>Thank you for these comments. We have reviewed an additional 2,000 images, which were randomly selected from PubMed Central. We used our original model to make predictions for those images. The corresponding results are now included in the paper.</p>
<p>We agree that many of the images identified as being &quot;unfriendly&quot; are microscope images, which often use red and green dyes. However, many other image types were identified as unfriendly, including heat maps, line charts, maps, three-dimensional structural representations of proteins, photographs, network diagrams, etc. We have uploaded these figures to our Open Science Framework repository so it's easier for readers to review these examples. We have added a comment along these lines to the paper.</p>
<p>The reviewer mentioned uploading red/green scatter plots and Ishihara test images to our Web application and that it reported they were friendly. Firstly, it depends on the scatter plot. Even though some such plots include green and red, the image's scientific meaning may be clear. Secondly, although the Ishihara images were created as informal tests for humans, these images (and ones similar to them) are not in eLife journal articles (to our knowledge) and thus are not included in our training set. Thus, it is unsurprising that our machine-learning models would not classify such images correctly as unfriendly.</p>
<disp-quote content-type="editor-comment">
<p>(3) Focusing the statistical analyses on individual images rather than articles (e.g. in figures 1 and 2) leads to pseudoreplication. Multiple images from the same article should not be treated as statistically independent measures, because they are produced by the same authors. A simple alternative is to instead use articles as the unit of analysis and score an article as &quot;unfriendly&quot; when it contains at least one &quot;unfriendly&quot; image. In addition, collapsing the counts of &quot;unfriendly&quot; images to proportions loses important information about the sample size. For example, the current analysis presented in Fig. 1 gives undue weight to the three images from 2012, two of which came from the same article. If we perform a logistic regression on articles coded as &quot;friendly&quot; and &quot;unfriendly&quot; (rather than the reported linear regression on the proportion of &quot;unfriendly&quot; images), there is still evidence for a decrease in the frequency of &quot;unfriendly&quot; eLife articles over time.</p>
</disp-quote>
<p>Thank you for taking the time to provide these careful insights. We have adjusted these statistical analyses to focus on articles rather than individual images. For Figure 1, we treat an article as &quot;Definitely problematic&quot; if any image in the article was categorized as &quot;Definitely problematic.&quot; Additionally, we no longer collapse the counts to proportions, and we use logistic regression to summarize the trend over time. The overall conclusions remain the same.</p>
<disp-quote content-type="editor-comment">
<p>Another issue concerns the large number of articles (&gt;40%) that are classified as belonging to two subdisciplines, which further compounds the image pseudoreplication. Two alternatives are to either group articles with two subdisciplines into a &quot;multidisciplinary&quot; group or recode them to include both disciplines in the category name.</p>
</disp-quote>
<p>Thank you for this insight. We have modified Figure 2 so that it puts all articles that have been assigned two subdisciplines into a &quot;Multidisciplinary&quot; category. The overall conclusions remain the same.</p>
<disp-quote content-type="editor-comment">
<p>(4) The low frequency of &quot;unfriendly&quot; images in the data (under 15%) calls for a different performance measure than the AUROC used by the authors. In such imbalanced classification cases the recommended performance measure is precision-recall area under the curve (PR AUC: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371%2Fjournal.pone.0118432">https://doi.org/10.1371%2Fjournal.pone.0118432</ext-link>) that gives more weight to the classification of the rare class (&quot;unfriendly&quot; images).</p>
</disp-quote>
<p>We now calculate the area under the precision-recall curve and provide these numbers (and figures) alongside the AUROC values (and figures). We agree that these numbers are informative; both metrics lead to the same overall conclusions.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>An analysis of images in the biology literature that are problematic for people with a color-vision deficiency (CVD) is presented, along with a machine learning-based model to identify such images and a web application that uses the model to flag problematic images. Their analysis reveals that about 13% of the images could be problematic for people with CVD and that the frequency of such images decreased over time. Their model yields 0.89 AUC score. It is proposed that their approach could help making biology literature accessible to diverse audiences.</p>
<p>Strengths:</p>
<p>The manuscript focuses on an important yet mostly overlooked problem, and makes contributions both in expanding our understanding of the extent of the problem and in developing solutions to mitigate the problem. The paper is generally well-written and clearly organized. Their CVD simulation combines five different metrics. The dataset has been assessed by two researchers and is likely to be of high-quality. Machine learning algorithm used (convolutional neural network, CNN) is an appropriate choice for the problem. The evaluation of various hyperparameters for the CNN model is extensive.</p>
</disp-quote>
<p>We thank the reviewer for these comments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>The focus seems to be on one type of CVD (deuteranopia) and it is unclear whether this would generalize to other types.</p>
</disp-quote>
<p>We agree that it would be interesting to perform similar analyses for protanopia and other color-vision deficiencies. But we leave that work for future studies.</p>
<disp-quote content-type="editor-comment">
<p>The dataset consists of images from eLife articles. While this is a reasonable starting point, whether this can generalize to other biology/biomedical articles is not assessed.</p>
</disp-quote>
<p>This is an important point. We have reviewed an additional 2,000 images, which were randomly selected from PubMed Central, and used our original model to make predictions for those images. The corresponding results are now included in the paper.</p>
<disp-quote content-type="editor-comment">
<p>&quot;Probably problematic&quot; and &quot;probably okay&quot; classes are excluded from the analysis and classification, and the effect of this exclusion is not discussed.</p>
</disp-quote>
<p>We now address this in the Discussion section.</p>
<disp-quote content-type="editor-comment">
<p>Machine learning aspects can be explained better, in a more standard way.</p>
</disp-quote>
<p>Thank you. We address this comment in our responses to your comments below.</p>
<disp-quote content-type="editor-comment">
<p>The evaluation metrics used for validating the machine learning models seem lacking (e.g., precision, recall, F1 are not reported).</p>
</disp-quote>
<p>We now provide these metrics (in a supplementary file).</p>
<disp-quote content-type="editor-comment">
<p>The web application is not discussed in any depth.</p>
</disp-quote>
<p>The paper includes a paragraph about how the Web application works and which technologies we used to create it. We are unsure which additional aspects should be addressed.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>This work focuses on accessibility of scientific images for individuals with color vision deficiencies, particularly deuteranopia. The research involved an analysis of images from eLife published in 2012-2022. The authors manually reviewed nearly 5,000 images, comparing them with simulated versions representing the perspective of individuals with deuteranopia, and also evaluated several methods to automatically detect such images including training a machine-learning algorithm to do so, which performed the best. The authors found that nearly 13% of the images could be challenging for people with deuteranopia to interpret. There was a trend toward a decrease in problematic images over time, which is encouraging.</p>
<p>Strengths:</p>
<p>The manuscript is well organized and written. It addresses inclusivity and accessibility in scientific communication, and reinforces that there is a problem and that in part technological solutions have potential to assist with this problem.</p>
<p>The number of manually assessed images for evaluation and training an algorithm is, to my knowledge, much larger than any existing survey. This is a valuable open source dataset beyond the work herein.</p>
<p>The sequential steps used to classify articles follow best practices for evaluation and training sets.</p>
</disp-quote>
<p>We thank the reviewer for these comments.</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>I do not see any major issues with the methods. The authors were transparent with the limitations (the need to rely on simulations instead of what deuteranopes see), only capturing a subset of issues related to color vision deficiency, and the focus on one journal that may not be representative of images in other journals and disciplines.</p>
</disp-quote>
<p>We thank the reviewer for these comments. Regarding the last point, we have reviewed an additional 2,000 images, which were randomly selected from PubMed Central, and used our original model to make predictions for those images. The corresponding results are now included in the paper.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>N/A</p>
</disp-quote>
<p>Thank you.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>- The web application link can be provided in the Abstract for more visibility.</p>
</disp-quote>
<p>We have added the URL to the Abstract.</p>
<disp-quote content-type="editor-comment">
<p>- They focus on deuteranopia in this paper. It seems that protanopia is not considered. Why? What are the challenges in considered this type of CVD?</p>
</disp-quote>
<p>We agree that it would be interesting to perform similar analyses for protanopia and other color-vision deficiencies. But we leave that work for future studies. Deuteranopia is the most common color-vision deficiency, so we focused on the needs of these individuals as a starting point.</p>
<disp-quote content-type="editor-comment">
<p>- The dataset is limited to eLife articles. More discussion of this limitation is needed. Couldn't one also include some papers from PMC open access dataset for comparison?</p>
</disp-quote>
<p>We have reviewed an additional 2,000 images, which we randomly selected from PubMed Central, and used our original model to make predictions for those images. The corresponding results are now included in the paper.</p>
<disp-quote content-type="editor-comment">
<p>- An analysis of the effect of selecting a severity value of 0.8 can be included.</p>
</disp-quote>
<p>We agree that this would be interesting, but we leave it for future work.</p>
<disp-quote content-type="editor-comment">
<p>- &quot;Probably problematic&quot; and &quot;probably okay&quot; classes are excluded from analysis, which may oversimplify the findings and bias the models. It would have been interesting to study these classes as well.</p>
</disp-quote>
<p>We agree that this would be interesting, but we leave it for future work. However, we have added a comment to the Discussion on this point.</p>
<disp-quote content-type="editor-comment">
<p>- Some machine learning aspects are discussed in a non-standard way. Class weighting or transfer learning would not typically be considered hyperparameters.&quot;corpus&quot; is not a model. Description of how fine-tuning was performed could be clearer.</p>
</disp-quote>
<p>We have updated this wording to use more appropriate terminology to describe these different &quot;configurations.&quot; Additionally, we expanded and clarified our description of fine tuning.</p>
<disp-quote content-type="editor-comment">
<p>- Reporting performance on the training set is not very meaningful. Although I understand this is cross-validated, it is unclear what is gained by reporting two results. Maybe there should be more discussion of the difference.</p>
</disp-quote>
<p>We used cross validation to compare different machine-learning models and configurations. Providing performance metrics helps to illustrate how we arrived at the final configurations that we used. We have updated the manuscript to clarify this point.</p>
<disp-quote content-type="editor-comment">
<p>- True positives, false positives, etc. are described as evaluation metrics. Typically, one would think of these as numbers that are used to calculate evaluation metrics, like precision (PPV), recall (sensitivity), etc. Furthermore, they say they measure precision, recall, precision-recall curves, but I don't see these reported in the manuscript. They should be (especially precision, recall, F1).</p>
</disp-quote>
<p>We have clarified this wording in the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>- There are many figures in the supplementary material, but not much interpretation/insights provided. What should we learn from these figures?</p>
</disp-quote>
<p>We have revised the captions and now provide more explanations about these figures in the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>- CVD simulations are mentioned (line 312). It is unclear whether these methods could be used for this work and if so, why they were not used. How do the simulations in this work compare to other simulations?</p>
</disp-quote>
<p>This part of the manuscript refers to recolorization techniques, which attempt to make images more friendly to people with color vision deficiencies. For our paper, we used a form of recolorization that simulates how a deuteranope would see a figure in its original form. Therefore, unless we misunderstand the reviewer's question, these two types of simulation have distinct purposes and thus are not comparable.</p>
<disp-quote content-type="editor-comment">
<p>- relu -&gt; ReLU</p>
</disp-quote>
<p>We have corrected this.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>The title can be more specific to denote that the survey was done in eLife papers in the years 2012-2022. Similarly, this should be clear in the abstract instead of only &quot;images published in biology-oriented research articles&quot;.</p>
</disp-quote>
<p>Thank you for this suggestion. Because we have expanded this work to include images from PubMed Central papers, we believe the title is acceptable as it stands. We updated the abstract to say, &quot;images published in biology- and medicine-oriented research articles&quot;</p>
<disp-quote content-type="editor-comment">
<p>Two mentions of existing work that I did not see are to Jambor and colleagues' assessment on color accessibility in several fields: <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8041175/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8041175/</ext-link>, and whether this work overlaps with the 'JetFighter' tool</p>
<p>(<ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/labs/c2292989/jetfighter-towards-figure-accuracy-and-accessibility">https://elifesciences.org/labs/c2292989/jetfighter-towards-figure-accuracy-and-accessibility</ext-link>).</p>
</disp-quote>
<p>Thank you for bringing these to our attention. We have added a citation to Jambor, et al.</p>
<p>We also mention JetFighter and describe its uses.</p>
<disp-quote content-type="editor-comment">
<p>Similarly, on Line 301: Significant prior work has been done to address and improve accessibility for individuals with CVD. This work can be generally categorized into three types of studies: simulation methods, recolorization methods, and estimating the frequency of accessible images.</p>
<p>- One might mention education as prior work as well, which might in part be contributing to a decrease in problematic images (e.g., <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8041175/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8041175/</ext-link>)</p>
</disp-quote>
<p>We now suggest that there are four categories and include education as one of these.</p>
<disp-quote content-type="editor-comment">
<p>Line 361, when discussing resources to make figures suitable, the authors may consider citing this paper about an R package for single-cell data: <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/82128">https://elifesciences.org/articles/82128</ext-link></p>
</disp-quote>
<p>Thank you. We now cite this paper.</p>
<disp-quote content-type="editor-comment">
<p>The web application is a good demonstration of how this can be applied, and all code is open so others can apply the CNN in their own uses cases. Still, by itself, it is tedious to upload individual image files to screen them. Future work can implement this into a workflow more typical to researchers, but I understand that this will take additional resources beyond the scope of this project. The demonstration that these algorithms can be run with minimal resources in the browser with tensorflow.js is novel.</p>
</disp-quote>
<p>Thank you.</p>
<disp-quote content-type="editor-comment">
<p>General:</p>
<p>It is encouraging that 'definitely problematic' images have been decreasing over time in eLife. Might this have to do with eLife policies? I could not quickly find if eLife has checks in place for this, but given that JetFighter was developed in association with eLife, I wonder if there is an enhanced awareness of this issue here vs. other journals.</p>
</disp-quote>
<p>This is possible. We are not aware of a way to test this formally.</p>
</body>
</sub-article>
</article>