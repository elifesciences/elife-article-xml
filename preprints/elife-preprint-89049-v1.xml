<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">89049</article-id>
<article-id pub-id-type="doi">10.7554/eLife.89049</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89049.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Prior information enhances tactile representation in primary somatosensory cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Kassraian</surname>
<given-names>Pegah</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rabe</surname>
<given-names>Finn</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Enz</surname>
<given-names>Nadja</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Maathuis</surname>
<given-names>Marloes</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wenderoth</surname>
<given-names>Nicole</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Neural Control of Movement Lab, Department of Health Sciences and Technology, Swiss Federal Institute of Technology</institution>, Zurich, <country>Switzerland</country></aff>
<aff id="a2"><label>2</label><institution>Department of Neuroscience, The Kavli Institute for Brain Science, Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution>, New York, NY, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>School of Psychology, Trinity College Dublin</institution>, Dublin, <country>Ireland</country></aff>
<aff id="a4"><label>4</label><institution>Seminar for Statistics, Department of Mathematics, Swiss Federal Institute of Technology</institution>, Zurich, <country>Switzerland</country></aff>
<aff id="a5"><label>5</label><institution>Neuroscience Center Zurich (ZNZ), University of Zurich, Federal Institute of Technology Zurich, University and Balgrist Hospital 10 Zurich</institution>, 8057, Zurich, <country>Switzerland</country></aff>
<aff id="a6"><label>6</label><institution>Future Health Technologies, Singapore-ETH Centre, Campus for Research Excellence And Technological Enterprise 13 (CREATE)</institution>, <country>Singapore</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Makin</surname>
<given-names>Tamar R</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>University of Cambridge</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>†</label>Corresponding author: <email>pegahkf@gmail.com</email></corresp>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2023-06-23">
<day>23</day>
<month>06</month>
<year>2023</year>
</pub-date>
<volume>12</volume>
<elocation-id>RP89049</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2023-05-12">
<day>12</day>
<month>05</month>
<year>2023</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2023-04-30">
<day>30</day>
<month>04</month>
<year>2023</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.10.511201"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2023, Kassraian et al</copyright-statement>
<copyright-year>2023</copyright-year>
<copyright-holder>Kassraian et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-89049-v1.pdf"/>
<abstract>
<p>Perception and adaptive decision making rely on the integration of incoming sensory input with prior knowledge or expectations. While tactile stimuli play a significant role in shaping our perception and decision making, if and how prior information modulates the representation of tactile stimuli in early somatosensory cortices is largely unknown. Here, we employed functional magnetic resonance imaging (fMRI) and a vibrotactile detection paradigm to study the effect of prior information on tactile perception and tactile stimulus representation in early somatosensory areas. The supra-voxel somatotopic organization of early somatosensory areas allowed us to assess the effect of prior information on finger-specific representations. We found that vibrotactile stimuli congruent with expectations are associated with improved vibrotactile detection performance and a decrease of the mean blood-oxygen-level-dependent (BOLD) signal in the contralateral primary somatosensory cortex (S1). Concurrently, finger-specific activity associated with anticipated vibrotactile stimulation revealed higher multivariate decoding accuracies and better alignment with S1’s somatotopic organization. In addition, we observed that prior information induced somatotopically organized activity in contralateral S1 even before tactile stimulation onset. The accuracy of the multivariate decoding of stimulus-specific expectations was therefore strongly associated with upcoming behavioral detection performance. Thus, our results reveal a role for S1 in the integration of upcoming tactile stimuli with prior information based on its somatotopic organization as well as the presence of behaviorally relevant activity in S1 before stimulation onset.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Somatosensory cortex</kwd>
<kwd>Prior information</kwd>
<kwd>Vibrotactile stimulation</kwd>
<kwd>Multivariate pattern analysis</kwd>
<kwd>Functional magnetic resonance imaging</kwd>
</kwd-group>

</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>New Figures, added supplemental materials, correction language</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>A central mechanism facilitating perception and decision making is the integration of current sensory input with prior information or expectations. Previous research has shown that utilizing prior information can improve behavioral performance, leading to faster reaction times and improved stimulus detection (<xref ref-type="bibr" rid="c27">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="c43">Richter and de Lange, 2019</xref>). The neural mechanisms underlying the integration of sensory input with prior information remain, however, largely unknown. Additionally, ongoing research is attempting to determine if and how prior information modulates the representation of stimuli in early sensory areas at the initial stages of sensory processing.</p>
<p>Most of the current knowledge on the neural mechanisms underlying the integration of prior information with sensory input stem from studies of the visual system. Functional magnetic resonance imaging (fMRI) in combination with visual detection paradigms has shown that anticipated visual stimuli are associated with a decrease in the blood oxygenation level dependent (BOLD) signal in early visual areas (<xref ref-type="bibr" rid="c27">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="c42">Richter et al., 2018</xref>; <xref ref-type="bibr" rid="c43">Richter and de Lange, 2019</xref>). This result was initially surprising, as traditionally an increase of the BOLD signal had been linked to improved behavioral performance (<xref ref-type="bibr" rid="c38">Pessoa et al., 2003</xref>). The use of multivariate decoding analysis revealed, however, that representations of anticipated stimuli in the primary visual cortex can be decoded with higher accuracy. Together, these findings suggest that an enhanced signal-to-noise ratio during early sensory processing might underlie behavioral improvements when stimuli are anticipated (<xref ref-type="bibr" rid="c27">Kok et al., 2012</xref>, <xref rid="c26" ref-type="bibr">2014</xref>, <xref ref-type="bibr" rid="c28">2017</xref>; <xref ref-type="bibr" rid="c43">Richter and de Lange, 2019</xref>).</p>
<p>While touch is the first sense to develop prenatally (<xref ref-type="bibr" rid="c41">Reissland et al., 2014</xref>) and tactile perception is crucial in shaping decision making (<xref ref-type="bibr" rid="c17">Hernandez et al., 2000</xref>; <xref ref-type="bibr" rid="c32">Luna et al., 2005</xref>), it is largely unknown if the same mechanisms are at play in early somatosensory areas as relevant studies have remained scarce. Traditionally, early somatosensory regions have been thought to chiefly process afferent tactile stimuli. Recent work has demonstrated, however, that early somatosensory regions, such as the primary somatosensory cortex (S1), can be activated even in the absence of incoming sensory stimulation, for instance during touch observation, tactile imagery or during motor planning (<xref ref-type="bibr" rid="c30">Kuehn et al., 2014</xref>; de Borst et al., 2017, 2019; <xref ref-type="bibr" rid="c15">Gale et al., 2021</xref>; <xref ref-type="bibr" rid="c23">Kikkert et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>). Other work has shown that the history of vibrotactile stimulus presentation can bias the neural activity in brain regions related to encoding and memory (<xref ref-type="bibr" rid="c39">Preuschhof et al., 2010</xref>). Whether the expectation of tactile stimuli induces stimulus-specific activity patterns, if this activity is somatotopically organized, and if it is relevant for behavioral performance is, however, largely unknown. This is an important research question, as it has been hypothesized that such activity might influence the subsequent processing of anticipated stimuli (<xref ref-type="bibr" rid="c28">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="c6">Blom et al., 2020</xref>; <xref ref-type="bibr" rid="c1">Aitken et al., 2020</xref>).</p>
<p>Here, we assessed if and how prior information modulates vibrotactile stimulus representations in early somatosensory regions both during and before vibrotactile stimulation. To this end, we employed fMRI and a vibrotactile detection paradigm where participants had to discriminate between weak, near-threshold vibrotactile stimulation of the ring finger versus the thumb after receiving probabilistic prior information on where to expect the stimulation. The vibrotactile stimulation intensities were set around the participants’ detection thresholds to reinforce the use of probabilistic prior information (<xref ref-type="bibr" rid="c8">Chambers et al., 2017</xref>; <xref ref-type="bibr" rid="c11">de Lange et al., 2018</xref>). The functional organization of early somatosensory areas in the form of somatotopic maps that stretch over a range of voxels allowed us to assess finger-specific modulation of vibrotactile stimulus representation by prior information.</p>
<p>We found that participants could detect vibrotactile stimulation congruent with their expectations more accurately and observed a decrease of the mean BOLD signal in contralateral S1 for these stimuli. Furthermore, ring finger versus thumb representations congruent with participants’ expectations were associated with higher linear decoding accuracies. Decoding accuracy and behavioral performance revealed a significant association, demonstrating the behavioral relevance of the precision by which congruent stimuli are encoded in contralateral S1. In addition, whole-brain searchlight decoding revealed that voxels assigned with higher decoding accuracies were more proximal to peak voxels determined by a functional localizer and univariate analysis.</p>
<p>Our paradigm allowed us to compare stimulus-specific activity patterns elicited by afferent stimulation to those elicited by the expectation of a vibrotactile stimulus. When investigating the period before vibrotactile stimulation onset, we found that a linear classifier could decode expectation of thumb versus ring finger stimulation based on the activity of contralateral S1. Crucially, we observed that BOLD activity during informative cue-stimulation intervals was increased within the region of interest (ROI) of the cued finger and concurrently suppressed within the ROI of the non-cued finger. Furthermore, the ability of a decoder to generalize between the activity patterns from afferent stimuli and activity patterns elicited by expectation demonstrated shared properties of the two representations. Finally, we observed that the decoding accuracy of ring finger and thumb cues were predictive of the upcoming behavioral performance, in line with the hypothesis that preparatory activity in early sensory areas might enhance perception and decision making.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Prior information is associated with a decrease of the BOLD signal in contralateral S1</title>
<p>Twenty-five participants (15 males and 10 females) took part in a total of eight imaging runs over the course of two days (40 vibrotactile detection trials per run; <xref rid="fig1" ref-type="fig">Figure 1A</xref>; Materials and methods). Prior to each imaging run, the vibrotactile detection threshold of each participant’s ring finger and thumb were determined with an interleaved one-up one-down staircase (<xref rid="fig1" ref-type="fig">Figure 1B</xref>; Materials and methods).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Experimental paradigm.</title>
<p><bold>a.)</bold> Participants took part in a total of 8 fMRI imaging runs on two consecutive days where they performed 40 vibrotactile detection tasks per run. Imaging runs were preceded by the acquisition of functional localizers on Day 1 and followed by the acquisition of structural MRI scans on Day 2. <bold>b</bold>.) Vibrotactile stimulation intensities of each imaging run were sampled around the vibrotactile detection threshold of the respective finger, determined by an interleaved staircase prior to the imaging run. Shown are the Day 1 results of an example participant. <bold>c</bold>.) Task structure of a vibrotactile detection trial: Each trial within an imaging run started with either a ring finger cue (square on the left), thumb cue (square on the right) or a non-informative cue (square in the middle of screen), followed by vibrotactile stimulation of either the ring finger (80% likely after ring finger cue; 50% likely after non-informative cue) or thumb (80% likely after thumb cue; 50% likely after non-informative cue) of the participant’s left hand. During the response period participants indicated where they had perceived the stimulation (‘Thumb’, ‘Ring finger,’ or ‘None’ if no stimulation was detected). Illustrated is an example congruent trial (ring finger cue followed by ring finger stimulation; dashed squares show possible alternatives). ‘+’: Fixation cross; ITI: Intertrial interval.</p></caption>
<graphic xlink:href="511201v3_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Each vibrotactile detection trial consisted of three parts (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). First, a ‘finger cue’ which provided probabilistic prior information on upcoming stimulation location was shown, as described in the next paragraph. Second, there was vibrotactile stimulation of either the ring finger or the thumb of the participant’s left hand. Stimulation intensities were sampled around each participant’s detection threshold for the respective finger (Materials and methods). The use of weak vibrotactile stimulation with near detection threshold intensity was meant to reinforce the use of prior information. Third, there was a response period in which participants had to choose one of the three response options: ‘Thumb’, ‘Ring finger’, or ‘None’ if no stimulation was perceived (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). The arrangement of the three response options on the screen was randomized between trials to preclude preparatory motor movements. The three parts of the trial were separated by intervals of variable lengths during which subjects were instructed to fixate on the displayed cross.</p>
<p>The finger cue in the form of a square was displayed on different parts of the screen: left, center, or right. Subjects were informed that a square on the left indicated that there was an 80% probability vibrotactile stimulation of the ring finger would follow (ring finger cue), a square on the right indicated that there was an 80% probability that stimulation of the thumb would follow (thumb cue), and a square in the middle indicated that there was an equally likely probability that stimulation of the ring finger or thumb would follow (non-informative cue). The ring finger and thumb were selected since somatosensory representations of these two digits can be dissociated particularly well with both univariate and multivariate methods (<xref ref-type="bibr" rid="c34">Martuzzi et al., 2012</xref>; <xref ref-type="bibr" rid="c13">Ejaz et al., 2015</xref>). The display location of the finger cue was spatially consistent with the position of the cued finger of participants’ left hand, thus ensuring an intuitive mapping. We refer to the experimental condition where the finger cue was followed by stimulation of the indicated finger as ‘congruent’ (i.e., ring finger cue and stimulation of ring finger or thumb cue and stimulation of thumb), as ‘incongruent’ when followed by stimulation of the other finger (i.e., ring finger cue and stimulation of thumb or thumb cue and stimulation of ring finger) and else as ‘non-informative’.</p>
<p>We observed that vibrotactile detection performance was enhanced for the congruent condition (70.2±4.81%) when compared to the non-informative (61.05±6.8%) and the incongruent experimental conditions (56.02±6.7%, mean±SEM; <xref rid="fig2" ref-type="fig">Figure 2A</xref>). These results were also reflected in the psychometric curves where a left shift was apparent for the congruent condition, indicating a decrease in vibrotactile detection threshold (<xref rid="figS1" ref-type="fig">Supplementary Figure 1</xref>). Stimulation intensities were matched between the experimental conditions (Materials and methods and <xref rid="tblS1" ref-type="table">Supplementary Table 1</xref>), thus ensuring that differences in vibrotactile detection performance and BOLD activity levels were due to differences in prior information available to participants.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Effect of prior information on behavioral detection performance and BOLD signal.</title>
<p><bold>a.)</bold> Behavioral detection accuracies are enhanced for congruent (70.2±4.81%; mean±SEM) as compared to incongruent (56.02±6.7%) and non-informative trials (61.05±6.8%; one-way repeated measures ANOVA for Experimental condition: <italic>F</italic><sub><italic>2,48</italic></sub> = 87.839, p&lt;0.001, η<sup>2</sup> = 0.416). <bold>b.)</bold> Group-level GLM contrasts reveal distinct clusters of voxels in contralateral S1 for vibrotactile stimulation of ring finger versus thumb. Ring finger &gt; Thumb: peak t-value = 5.182 (MNI coordinates: 42, -24, 60; cluster of 73 voxels); Thumb &gt; Ring finger: peak t-value = 8.451 (MNI coordinates: 54, -14, 50; cluster of 128 voxels); p&lt;0.05, familywise error (FWE) corrected. <bold>c.)</bold> The mean BOLD signal in contralateral S1 is dampened for congruent (0.796±0.368) as compared to incongruent (3.887±0.539) and non-informative trials (2.029±0.415; mean±SEM in a.u.: arbitrary units; one-way repeated measures ANOVA for Experimental condition: <italic>F</italic><sub><italic>2,48</italic></sub> = 44.466, p&lt;0.001, η<sup>2</sup> = 0.252). Pairwise t-tests with post-hoc Bonferroni correction: *** p&lt;0.001, **** p&lt;0.0001. Single-participant-results are visualized by gray lines (N=25 throughout all Figures).</p></caption>
<graphic xlink:href="511201v3_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To assess the BOLD signal associated with the vibrotactile stimulation of each finger, we performed a univariate contrast of ring finger versus thumb stimulation with a general linear model (GLM; Materials and methods). The contrast revealed two distinct clusters of voxels, one coinciding with the somatosensory ring finger ROI and one with the somatosensory thumb ROI, as determined by an independent localizer where supra-threshold stimulation was applied to participants ring finger or thumb (<xref rid="fig2" ref-type="fig">Figure 2B</xref> and <xref rid="tblS2" ref-type="table">Supplementary Table 2</xref>). The location of the clusters was in line with somatotopic finger representations in contralateral S1 as identified by previous fMRI studies (<xref ref-type="bibr" rid="c34">Martuzzi et al., 2012</xref>; <xref ref-type="bibr" rid="c2">Akselrod et al., 2021</xref>). Next, we compared the magnitude of the BOLD signal in S1 during vibrotactile stimulation events separately for the three different experimental conditions. Studies of the visual system have previously observed a decrease of the BOLD signal when sensory stimuli were anticipated, i.e. congruent with participants’ expectations as compared to the BOLD signal of ‘incongruent’ sensory stimuli (<xref ref-type="bibr" rid="c27">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="c43">Richter and de Lange, 2019</xref>). Our results confirmed these findings: The average BOLD signal within contralateral S1 evoked by vibrotactile stimulation was significantly lower for the congruent as compared to the incongruent and non-informative experimental conditions (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). No differences between the BOLD signal evoked by the three experimental conditions were observed in the ipsilateral S1 (<xref rid="figS2" ref-type="fig">Supplementary Figure 2</xref>).</p>
</sec>
<sec id="s2b">
<title>Prior information enhances stimulus representation in contralateral S1</title>
<p>Due to their supra-voxel somatotopic organization, early somatosensory regions are particularly well suited to study stimulus-specific modulation of activity through top-down signals. Here, we used linear decoding to classify the stimulated finger (ring finger versus thumb) separately for the three experimental conditions based on beta estimates obtained from a first-level GLM within anatomically defined ROIs (ROI-based decoding). We found that decoding accuracies for contralateral S1 were enhanced for congruent vibrotactile stimuli (63.067±1.311%, p&lt;0.001) when compared to non-informative (56.51±1.419%, p&lt;0.001) or incongruent experimental conditions (49.872±1.003%, p=0.4; all values mean±SEM; <xref rid="fig3" ref-type="fig">Figure 3A</xref>). These results were in line with previous studies demonstrating that anticipated visual stimuli are associated with higher multivariate decoding accuracies despite the observed concurrent decrease of the mean BOLD signal in early sensory areas (<xref ref-type="bibr" rid="c27">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="c43">Richter and de Lange, 2019</xref>). Interestingly, decoding accuracies did not significantly differ from chance levels in ipsilateral and contralateral secondary somatosensory cortices. Decoding accuracies were moderately above chance for the contralateral primary motor cortex (<xref rid="figS3" ref-type="fig">Supplementary Figure 3A</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Multivariate decoding of vibrotactile ring finger versus thumb stimulation.</title>
<p><bold>a.)</bold> Contralateral S1 ROI-decoding accuracies are enhanced for congruent (63.067±1.311%, p&lt;0.001) as compared to incongruent (49.872±1.003%, p=0.4) and non-informative trials (56.51±1.419%, p&lt;0.001; mean±SEM; p-value calculation against empirical chance values from shuffled data). Decoding accuracies for ipsilateral S1 were not significantly greater than chance (p&gt;0.4). Repeated-measures two-way ANOVA: Interaction Experimental condition x ROI, <italic>F</italic><sub><italic>2,96</italic></sub> <italic>=</italic> 17.847, p&lt;0.0001, η<sup>2</sup> = 0.294. Pairwise t-tests with post-hoc Bonferroni correction: **** p&lt;0.0001. Single-participant-results are visualized by gray lines. <bold>b.)</bold> Improvements of behavioral detection accuracies (x-axis) and ROI-based decoding accuracies (y-axis) for congruent trials are strongly correlated for contralateral S1 (Spearman’s r, **** p&lt;0.001, robust regression slope: 0.594; shaded area with 95% CI) but not for ipsilateral S1 (Spearman’s r, p=0.575, robust regression slope: -0.272 with shaded area: 95% CI). <bold>c.)</bold> Group-level searchlight maps reveal voxels throughout S1 and adjacent M1 with significantly higher decoding accuracies for the Congruent &gt; Incongruent (left) comparison. The Congruent &gt; Non-informative comparison (right), revealed two distinct clusters proximal to univariate ring finger and thumb ROIs (one-sided t-tests with p&lt;0.01 and FWE-correction at the cluster-level with p&lt;0.01). <bold>d.)</bold> Contralateral S1 searchlight decoding accuracies assigned to a voxel (y-axis) as a function of its Euclidean distance to the peak BOLD voxel (x-axis; MNI coordinates of peak BOLD voxel obtained by a functional localizer) reveal a strong correlation for congruent trials (Spearman’s r: **** p&lt;0.001, * p&lt;0.05, shaded area: 95% CI around robust regression line). Results averaged across ring finger and thumb, for a detailed discussion see Materials and methods.</p></caption>
<graphic xlink:href="511201v3_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>To assess the behavioral relevance of the decoding results, we tested if the improvement in behavioral detection performance and decoding performance from the non-informative to the congruent experimental conditions are correlated (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). This analysis revealed a strong correlation between behavioral detection performance and decoding performance in contralateral S1, demonstrating the behavioral relevance of the precision by which congruent stimuli are encoded in contralateral S1. In contrast, no such correlation was observed for the contralateral M1 region (<xref rid="figS3" ref-type="fig">Supplementary Figure 3B</xref>).</p>
<p>Next, we employed a searchlight decoding approach where a linear classifier with the same decoding scheme (ring finger versus thumb decoding; beta estimates from a first-level GLM) was applied one-by-one to each voxel and surrounding voxels (i.e., with a searchlight radius = 1; <xref ref-type="bibr" rid="c29">Kriegeskorte et al., 2006</xref>). This allowed us to obtain spatially more fine-grained accuracy maps across the whole brain. Group-level results of the searchlight decoding revealed clusters of voxels with significant decoding accuracies only for the congruent condition (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). The largest cluster of voxels was located in the contralateral S1, with a subset of voxels localized in the contralateral primary motor cortex (787 voxels total with 62.6±4.3% decoding accuracy, mean±SEM). Significantly higher decoding accuracies between congruent versus incongruent or congruent versus non-informative experimental conditions were exclusively located in contralateral S1, proximal to the thumb and ring finger’s peak BOLD voxels (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, <xref rid="tblS3" ref-type="table">Supplementary Table 3</xref>). Crucially, we observed in contralateral S1 a strong correlation between searchlight decoding accuracies of the congruent condition associated with a voxel and the distance of the voxel to the peak BOLD voxel of the respective finger (<xref rid="fig3" ref-type="fig">Figure 3D</xref>). This relationship was weaker for the non-informative experimental condition and not significant for the incongruent experimental condition.</p>
<p>Taken together, our results suggest that finger representations of congruent vibrotactile stimulations are associated with higher multivariate information content, are more aligned with the somatotopic organization of contralateral S1, and that the enhanced representation of these stimuli is strongly associated with behavioral detection performance.</p>
</sec>
<sec id="s2c">
<title>Distinct representations of ring finger and thumb cues in contralateral S1 prior to vibrotactile stimulation</title>
<p>Recent studies have shown that the primary somatosensory cortex can be activated even in the absence of incoming tactile stimulation, for instance during touch observation or tactile imagery (de Borst et al. 2017, 2019; Kuehn et al., 2019). It is, however, not known if expectation elicits somatotopic activity in S1 before stimulus onset, how such activity compares to activity evoked by afferent vibrotactile stimulation, and what the behavioral relevance of such activity would be. To address these questions, we analyzed the evoked BOLD activity during a variable interval following finger cue display and before vibrotactile stimulation onset (‘cue-stimulation interval’, <xref rid="fig1" ref-type="fig">Figure 1C</xref>). We categorized these periods as ‘informative’ if they followed a ring finger or a thumb cue and as ‘non-informative’ if they followed an non-informative finger cue (<xref rid="fig4" ref-type="fig">Figure 4A</xref>). The non-informative experimental condition served as a control and was divided into two groups based on the succeeding vibrotactile stimulation site (ring finger versus thumb stimulation).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Somatotopic modulation of the BOLD signal during the Cue-stimulation interval (CSI).</title>
<p><bold>a.)</bold> The period between finger cue display and vibrotactile stimulation (CSI: ‘Cue-stimulation interval’; <xref rid="fig1" ref-type="fig">Figure 1C</xref>) was categorized as ‘informative’ when following a ring finger cue (square on left of screen) or a thumb cue (square on right of screen). Non-informative cue-stimulation intervals (square in the middle of screen) served as a control and were divided into two groups according to the following vibrotactile stimulation (ring finger vs. thumb). <bold>b.)</bold> After display of a finger cue, the BOLD signal was greater within the respective ROI (ring finger vs. thumb ROI) than within the ROI associated with the non-cued finger. No such modulation of BOLD activity could be observed after display of a non-informative finger cue. Ring finger and thumb ROIs were extracted for each participant from independent functional localizers (Materials and methods). A 2×2×2 way repeated measures ANOVA with CSI type x ROI x Cue condition revealed significant differences between the BOLD signal associated with ring finger vs. thumb cue for the informative CSI (p&lt;0.01, t&gt;4.98) but not for the non-informative CSI (p&gt;0.258, t&gt;0.0508). Pairwise t-tests with post-hoc Bonferroni correction: * p&lt;0.01, ** *p&lt;0.0001.</p></caption>
<graphic xlink:href="511201v3_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Notably, we observed that during informative cue-stimulation intervals the BOLD signal was increased within the ROI of the cued finger and concurrently suppressed within the ROI of the non-cued finger, thus revealing somatotopic organization (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, left panel). In contrast, the BOLD signal during the non-informative cue-stimulation interval did not reveal any such organization (<xref rid="fig4" ref-type="fig">Figure 4B</xref>, right panel).</p>
<p>We next used a linear classifier to decode the cued finger (ring finger versus thumb cue) based on first-level GLM beta estimates obtained from the informative cue-stimulation interval, and, as a control, from beta estimates obtained from the non-informative cue-stimulation interval (Materials and methods). We found that decoding accuracies of the informative cue-stimulation interval were significantly higher than chance level in contralateral but not in ipsilateral S1 (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). In contrast, it was not possible to decode the upcoming stimulation site from the non-informative cue-stimulation interval in any of the ROIs.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Decoding of ring finger versus thumb cue during the Cue-stimulation interval (CSI).</title>
<p><bold>a.)</bold> Accuracies of ring finger vs. thumb cue decoding are greater than chance for contralateral S1 during the informative but not during the non-informative CSI (57.327±2.8%, mean±SEM; p&lt;0.0001 vs. 49.024±5.501, p=0.72). Decoding accuracies are not greater than chance for either condition for ipsilateral S1 (49.451±4.712 vs. 50.122±2.514%; p&gt;0.62). Two-way repeated measures ANOVA: Interaction CSI type x ROI, F<sub>1,24</sub> = 63.26, p&lt;0.0001, η<sup>2</sup> = 0.423. <bold>b.)</bold> Results from decoders trained on the CSI (ring finger vs. thumb cue decoding) and tested on vibrotactile ring finger vs. thumb stimulation trials and vice versa. Decoding accuracies were significantly greater than chance for the informative but not for the non-informative CSI for contralateral S1 (accuracies averaged over both train-test schemes: 56.921±3.754% vs. 50.191±1.207%, p&lt;0.0001), but not for ipsilateral S1 (48.934±4.813%, p&gt;0.7 vs. 49.325±3.204%, p&gt;0.6). Two-way repeated measures ANOVA: Interaction CSI type x ROI, F<sub>1,24</sub> = 35.385, p&lt;0.0001, η<sup>2</sup> = 0.308. Pairwise t-tests with post-hoc Bonferroni correction: **** p&lt;0.0001. Single-participant-results are visualized by gray lines. <bold>c.)</bold> Improvement of behavioral detection accuracies (x-axis; from non-informative CSI to congruent stimulation trials) and ROI-based decoding accuracies (y-axis) for congruent trials are correlated for contralateral S1 (Spearman’s r, * p=0.039, robust regression slope: 0.539 with shaded area: 95% CI) but not for ipsilateral S1 (Spearman’s r, p=0.728, robust regression slope: -0.138 with shaded area: 95% CI). <bold>d.)</bold> Representational dissimilarity matrices visualizing the average cross-validated Mahalanobis distances (<xref ref-type="bibr" rid="c48">Walther et al., 2016</xref>) between ring finger (RF) and thumb (Th) stimulation (black labels) and ring finger and thumb cues during the CSI (green labels) for the contralateral (left) and the ipsilateral S1 (right).</p></caption>
<graphic xlink:href="511201v3_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We then asked if a linear decoder trained to discriminate between the anticipation of ring finger versus thumb stimulation would generalize to vibrotactile stimulation trials and vice versa. A decoder trained on informative cue-stimulation intervals and tested on vibrotactile stimulation periods and vice versa revealed significant accuracies for contralateral but not for ipsilateral S1 (<xref rid="fig5" ref-type="fig">Figure 5B</xref>). Training the decoder on non-informative cue-stimulation intervals resulted in chance level-accuracies for both ROIs, thus supporting the hypothesis that the anticipation of vibrotactile stimulation was required to drive informative activity during this period.</p>
<p>We then compared the improvement of decoding performance from non-informative to informative cue-stimulation intervals to the behavioral improvement from non-informative to congruent vibrotactile stimulation trials (<xref rid="fig5" ref-type="fig">Figure 5C</xref>). This comparison revealed a significant correlation in contralateral but not in ipsilateral S1, demonstrating the behavioral relevance of S1 activity even prior to vibrotactile stimulus onset, although the association was weaker than the association found during vibrotactile stimulation (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). Consistent with these results, we found that Mahalanobis distances were greatest between the activity evoked by vibrotactile stimulation of ring finger vs. thumb. In addition, activity evoked by a finger cue bore greater similarity to activity evoked by the vibrotactile stimulation of the corresponding finger (<xref rid="fig5" ref-type="fig">Figure 5D</xref>).</p>
<p>In summary, we observed that expectation of vibrotactile stimulation to a finger elicits somatotopic activity in contralateral S1, that this activity allows the decoding of finger-specific anticipation, and that it bears similarity to the activity elicited by afferent vibrotactile stimulation in contralateral S1.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The integration of afferent sensory information with prior expectations facilitates perception and decision making, for instance when sensory input is weak or ambiguous. In this study, participants received weak vibrotactile stimulation to either the ring finger or thumb preceded by a probabilistic finger cue, and had to indicate on which finger they had perceived the stimulation. We found that vibrotactile stimulation congruent with the participants’ expectations was associated with enhanced detection performance and observed a concurrent decrease of the BOLD signal in contralateral S1. This result is consistent with studies of the visual system demonstrating that anticipated visual stimuli are associated with enhanced detection performance and a concurrent dampening of the BOLD signal in primary visual cortex.</p>
<p>Multivariate decoding of the stimulated finger (ring finger versus thumb) revealed improved decoding accuracies for vibrotactile stimuli congruent with participants’ expectations when compared to stimuli which were incongruent or followed an non-informative finger cue. Importantly, we observed a significant association between improvements of detection performance and decoding accuracy for anticipated stimuli, demonstrating the behavioral relevance of the precision by which anticipated vibrotactile stimuli were encoded in S1. In contrast, we did not find linear decoding accuracies to be significantly higher than chance level in the ipsilateral and contralateral S2 regions. This is consistent with findings of neuroimaging studies demonstrating that individual finger representations cannot be resolved in S2 with 3T or even with 7T fMRI (<xref ref-type="bibr" rid="c44">Ruben et al., 2001</xref>; <xref ref-type="bibr" rid="c45">Sanchez-Panchuelo et al., 2018</xref>) and with primate electrophysiology showing that S2 neurons have larger and often bilateral receptive fields with a less fine-grained somatotopic organization (<xref ref-type="bibr" rid="c14">Fitzgerald et al., 2006</xref>). In light of findings that top-down signals such as attention can modulate activity both in primary and secondary somatosensory cortices (<xref ref-type="bibr" rid="c21">Johansen-Berg et al., 2000</xref>), this might indicate that a modulation of S2 activity by top-down signals relies less strongly on a somatotopic organization.</p>
<p>In addition, we found decoding accuracies in the ipsilateral and contralateral primary motor cortex to be only moderately above chance and not associated with behavioral detection performance. Whole brain searchlight decoding confirmed that the effects of prior information were largely confined to S1, with peak decoding accuracies being located proximal to univariate peak voxels as determined by an independent functional localizer. Thus, our results suggest that the behavioral improvements resulting from prior information are associated with higher separability of somatotopic stimulus representations in contralateral S1. These results provide evidence for the integration of prior information during early somatosensory processing stages, where the activity of a select neural population could be ‘tuned’ for the anticipated tactile stimulus.</p>
<p>The finding that prior information can modulate activity in early sensory areas that had been traditionally associated with the processing of afferent stimuli has prompted the question of whether top-down signals alone can activate these regions. Studies of the visual and auditory systems have revealed that expectation of a stimulus indeed increases baseline activity in the relevant early sensory areas, leading to the hypothesis that this allows for efficient perceptual processing (<xref ref-type="bibr" rid="c5">Bell et al., 2016</xref>; <xref ref-type="bibr" rid="c26">Kok et al., 2014</xref>, <xref ref-type="bibr" rid="c28">2017</xref>; <xref ref-type="bibr" rid="c6">Blom et al., 2020</xref>). Two recent studies have provided converging evidence for the involvement of S1 in motor planning (<xref ref-type="bibr" rid="c15">Gale et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>), with one study demonstrating evidence for finger-specific activity in S1 during motor planning (<xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>). How stimulus-specific expectation modulates activity in early somatosensory areas, and, importantly, how such activity compares to somatosensory activity elicited during perception of afferent tactile stimuli has so far, however, not been studied.</p>
<p>When assessing the period before vibrotactile stimulation, we found that informative but not non-informative probabilistic finger cues induce anticipatory activity patterns in contralateral S1. Importantly, we observed finger-specific modulation of the BOLD signal during this period.</p>
<p>In addition, ring finger and thumb cues could be decoded from BOLD activity after an informative cue (ring finger versus thumb cue) and the performance of the linear decoder was related to the upcoming detection performance of the participants. In a subsequent analysis, we assessed the similarity between the activity patterns elicited by anticipation of a vibrotactile stimulus and the activity patterns associated with vibrotactile stimulation. Crucially, we found that a linear decoder trained during the anticipation of a vibrotactile stimulus could decode activity patterns from the associated vibrotactile stimulation with above-chance accuracy and vice versa. These findings are in line with the hypothesis that top-down modulation of activity in primary sensory cortices rely on similar neural mechanisms as those relevant for perception.</p>
<p>In summary, our results demonstrate that prior information can drive somatotopic activity in early somatosensory areas even in the absence of afferent tactile stimulation, and that this activity is task relevant. This finding adds to the recent literature demonstrating that S1 is involved in higher order processing of tactile information based on top-down signals (de Borst et al., 2017, 2019; <xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>), and that it does so based on its somatotopic organization (<xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>). Taken together, our results are in line with the processing of prior information during early stages of sensory processing, as suggested by inferential models of perception.</p>
<p>Activity in both early and late sensory areas has been associated with a range of top-down cognitive processes, such as mental imagery or feature-based attention (<xref ref-type="bibr" rid="c31">Liu et al., 2016</xref>; <xref ref-type="bibr" rid="c25">Koenig-Robert and Pearson, 2019</xref>; <xref ref-type="bibr" rid="c6">Blom et al., 2020</xref>). With respect to the somatosensory domain, recent studies have demonstrated for instance that both tactile imagery as well as motor planning evoke informative activity patterns in somatosensory areas (de Borst et al., 2017, 2019; <xref ref-type="bibr" rid="c15">Gale et al., 2021</xref>; <xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>). It would be of interest to further refine the quantification of the involved top-down processes. It is, for instance, not yet known to what extent motor planning involves mental imagery, and while it has been shown that motor preparation elicits somatotopic activity in S1 similar to the activity induced by motor execution, neural representations associated with mental imagery have only been found to allow for the dissociation of the imagery modality (tactile versus auditory) but not for stimulus-specific imagery content (de Borst et al., 2017). This note also extends to our study, where we cannot dissociate between the expectation of an upcoming vibrotactile stimulus and the respective mental imagery. Thus, the assessment of the differences between different categories of anticipatory neural activity both in somatosensory areas and areas associated with other stimulus modalities constitutes a promising future line of research.</p>
</sec>
</body>
<back>
<sec id="s4">
<title>Author contributions</title>
<p>Conceptualization: PK, NW; Data collection: PK, NE; Methodology: PK, FR, MM, NW; Software and data analysis: PK, FR; Writing - original manuscript: PK, Writing - review and editing: PK, FR, MM, NW; Funding acquisition: PK, NW; Supervision: MM, NW.</p>
</sec>
<sec id="s5">
<title>Funding</title>
<p>PK was supported by the Swiss National Science Foundation (SNSF grants 320030_149561, P2EZP3-181896 and P500PB_203063). NW was supported by the Swiss National Science Foundation (SNSF grant 320030_149561). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></sec>
<ack>
<title>Acknowledgements</title>
<p>We thank the participants of the study, Daniel Woolley for feedback on the manuscript and initial support with the vibrotactile stimulation setup, members of the Wenderoth lab for participating in study pilots and Roger Luechinger for the technical support during fMRI data collection.</p>
</ack>
<sec id="s6">
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s7">
<title>Data availability</title>
<p>The data used to create the figures in this study can be found on Github <ext-link ext-link-type="uri" xlink:href="https://github.com/Pegahka/PriorInformationS1">https://github.com/Pegahka/PriorInformationS1</ext-link></p>
</sec>
<sec id="s8">
<title>Materials and methods</title>
<sec id="s8a">
<title>Participants, Power, Ethics</title>
<p>25 healthy volunteers participated in the study (15 males, 10 females, age 26±4 years; mean±SD; all participants right-handed as assessed by the Edinburgh Handedness Inventory). The sample size is in line with sample sizes employed in the field (<xref ref-type="bibr" rid="c27">Kok et al., 2012</xref>, <xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>). Participants’ informed consent was obtained according to the Declaration of Helsinki prior to study onset. Ethical approval was granted by the Kantonale Ethikkommission Zürich (KEK-2015-0537).</p>
</sec>
<sec id="s8b">
<title>Experimental design</title>
<p>After familiarization in a mock-scanner, participants performed vibrotactile detection tasks in a functional magnetic resonance imagining (fMRI) scanner during 2 sessions on two subsequent days. Each fMRI imaging session consisted of four imaging runs. Each imaging run (∼10 minutes) consisted of 40 vibrotactile detection trials and was preceded by a psychophysical staircase procedure (∼7 minutes). In addition, day 1 began with functional localizer runs (2 runs of each 10 minutes) and a structural scan was obtained for each participant at the end of day 2 (∼7 minutes), resulting in a duration of around 1.5-2 hours for each fMRI imaging session.</p>
</sec>
<sec id="s8c">
<title>Vibrotactile stimulation</title>
<p>Custom-made MR compatible piezo-electric devices (Dancer Design, UK) were used to deliver stimulation to the fingertips of participants’ ring finger and thumb of the left hand. Stimulators were fixated on the fingers using Velcro straps to ensure good contact with the skin throughout the experiment without compromising blood supply. A pulsed sinusoid with 5 bursts per stimulation at a frequency of 50 Hz with 40 ms gaps and a duration of 3.8 seconds was employed for vibrotactile stimulation throughout the entire study. Visual input was displayed on a screen visible to participants through a mirror placed on the head coil. Stimulation and display were controlled by in-house scripts implemented in MATLAB (2018a, <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link>) using the Psychophysics Toolbox (<xref ref-type="bibr" rid="c7">Brainard, 1997</xref>, <xref ref-type="bibr" rid="c37">Pelli, 1997</xref>, <xref ref-type="bibr" rid="c24">Kleiner et al., 2007</xref>). Two interleaved one-up one-down staircases were employed prior to each imaging run, to determine vibrotactile detection thresholds separately for the ring finger and thumb of the left hand of each participant. Each staircase started at 1 Volt stimulation intensity and consisted of 7 trials per finger in a randomized order where participants had to indicate on which finger they perceived the stimulation. To ensure an experimental environment akin to the main experiment, staircases were conducted inside the scanner while echo-planar images (EPIs) were acquired, but the corresponding MR scans were later discarded. For the following imaging session vibrotactile stimuli were presented around the participants’ detection threshold with stimulation intensities sampled for each finger individually from a skew-normal distribution with a skewness of 0.5 and a kurtosis of 4, while the mode of the distribution was set to the vibrotactile detection threshold of that finger.</p>
</sec>
<sec id="s8d">
<title>Experimental imaging runs and behavioral tasks</title>
<p>Each vibrotactile detection trial began with the display of a finger cue (1s duration). Next, in the vibrotactile stimulation period (3.8s), participants received stimulation to either the thumb or ring finger of the left hand. Finally, there was a response period (2s) where participants indicated the perceived vibrotactile stimulation site: ‘Ring finger’, ‘Thumb’ or ‘None’, the latter included in case they could not detect any vibrotactile stimulation. Finger cue display, vibrotactile stimulation and Response period were separated by intervals of variable lengths sampled uniformly from the respective range (3.5-4.5s, 2.5-3.5s or 1-2s, respectively). Participants were instructed to focus on a fixation cross when displayed.</p>
<p>The finger cue (a square on a black background) appeared either on the left, right or center of the screen. A square on the left indicated that stimulation was likely (80% chance) to be applied to the left ring finger (ring finger cue), while a square on the right indicated that stimulation was likely (80% chance) to be applied to the thumb (thumb cue). A square in the middle indicated that stimulation to the thumb or ring finger was equally likely (non-informative cue). Note that the finger cue location was spatially consistent with the position of the cued finger of participant’s left hand, ensuring an intuitive mapping for the participants.</p>
<p>Trials where the stimulated finger matched the cue were classified as the ‘congruent’ experimental condition (thumb cue – stimulation of thumb; ring finger cue – stimulation of ring finger), while trials where the stimulated finger was incongruent to the cue were classified as belonging to the ‘incongruent’ experimental condition (thumb cue – stimulation of ring finger; ring finger cue – stimulation of thumb). The trials with the non-informative cue formed the ‘non-informative’ experimental condition. Of the total 320 trials, 172 fell into the congruent category, 44 into the incongruent category and 104 into the non-informative category. The same randomized order of experimental conditions (congruent, incongruent, non-informative) was employed for all participants, and it was ensured that each experimental condition contained the same number of ring finger and thumb trials. During the response period, participants could select from the three response options (“Ring finger”, “Thumb”, or “None”) with their right hand through key presses on a button box. The response options changed color upon selection, and their arrangement was randomized to prevent preparatory motor artifacts prior to the response period.</p>
</sec>
<sec id="s8e">
<title>Statistical analysis of the behavioral task</title>
<p>Behavioral detection accuracy (<xref rid="fig2" ref-type="fig">Figure 2A</xref>) was calculated as the fraction of correctly recognized stimulation sites over all vibrotactile stimulation events over the 8 runs. Statistical analyses were performed using the R statistical software (4.2.1; <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org">www.R-project.org</ext-link>). Post hoc Bonferroni corrected p-values for two-sided pairwise t-tests were also included.</p>
<p>We performed a robust regression analysis (with robustbase in R) to assess if the improvement of behavioral detection accuracy and the improvement of decoding accuracy observed for congruent stimuli were related (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). To this end, the independent variable was calculated as the difference between behavioral accuracy for the congruent stimuli and the behavioral accuracy for non-informative stimuli for each subject. The dependent variable was calculated as the difference between decoding accuracy for the congruent stimuli and the decoding accuracy for non-informative stimuli for each subject either based on the BOLD signal in the contralateral S1 region (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, left panel) or the ipsilateral S1 region (<xref rid="fig3" ref-type="fig">Figure 3C</xref>, right panel). Next we assessed the relationship between the improvement of behavioral detection accuracy for congruent stimuli and the decoding accuracy of the informative cue-stimulation interval to assess the behavioral relevance of the BOLD signal induced by a finger cue prior to vibrotactile stimulation. Here, the independent variable was calculated as the difference between the decoding accuracy for the informative cue-stimulation interval minus the decoding accuracy for the non-informative cue-stimulation interval for each subject. The robust regression analysis was again performed for both the contralateral S1 region and the ipsilateral S1 region (<xref rid="fig5" ref-type="fig">Figure 5C</xref>).</p>
</sec>
<sec id="s8f">
<title>Functional Localizer and finger ROI</title>
<p>A functional localizer was performed over 2 runs on day 1 prior to the start of the imaging runs of the main experiment. Fingers were stimulated with a clearly perceivable, suprathreshold vibrotactile stimuli (4 Volts stimulation intensity, 4s duration). Each finger was stimulated 15 times with variable interstimulus intervals between the stimulation periods. The marsbar ROI toolbox (<ext-link ext-link-type="uri" xlink:href="http://marsbar.sourceforge.net">http://marsbar.sourceforge.net</ext-link>) was used to extract ring finger and thumb region of interest masks for each participant based on the significant cluster of activity (p&lt;0.05) in contralateral S1 from a vibrotactile stimulation &gt; resting baseline contrast.</p>
</sec>
</sec>
<sec id="s9">
<title>fMRI data acquisition</title>
<p>Functional images were acquired on a Philips Ingenia 3T with a fifteen element head coil. The T2*-weighted gradient-echo transversal EPI sequences were acquired with a spatial resolution of 2.75×2.75×3.3 mm<sup>3</sup>, TR/TE = 2500ms/35ms and an epi-factor of 39 and a sense acceleration factor of 2. The 40 acquired slices with no interslice gap were oriented parallel to the ACPC line. Anatomical images were acquired with a 1 mm×1 mm×1 mm spatial resolution, TR/TE = 8.38 ms/3.94 ms and a sense acceleration factor of 1.5 and sagittal orientation.</p>
<sec id="s9a">
<title>fMRI preprocessing and univariate analysis</title>
<p>The fMRI data was pre-processed using a standard pipeline (i.e., realignment, normalization, and smoothing) in FSL (v5.00; <ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL</ext-link>). Realignment of the functional images was followed by skull stripping applied to the structural image with the brain extraction tool BET (Smith et al., 2002; <xref ref-type="bibr" rid="c20">Jenkinson et al., 2005</xref>). Structural images were segmented with FAST (<xref ref-type="bibr" rid="c49">Zhang et al., 2001</xref>) and the boundary based co-registration tool FLIRT (<xref ref-type="bibr" rid="c18">Jenkinson and Smith, 2001</xref>; <xref ref-type="bibr" rid="c19">Jenkinson et al., 2002</xref>) was then used to co-register functional images to the structural image. Motion correction was applied using MCFLIRT (<xref ref-type="bibr" rid="c19">Jenkinson et al., 2002</xref>). Spatial smoothing with a 3 mm full-width half maximum Gaussian kernel and a high-pass filter with a cut-off of 100 seconds were applied to the co-registered functional images. Finally, functional images were normalized to the MNI template space with FNIRT (<xref ref-type="bibr" rid="c3">Andersson et al., 2007</xref>). The preprocessed images were analyzed with a general linear model (GLM). We defined separate regressors for each vibrotactile stimulation and each cue-stimulation interval (period between finger cue display and vibrotactile stimulation). Finger cue display and the response period were each included as regressors of no interest, as were head motion parameters. Single-subject (first-level) beta estimates (regression coefficients) were obtained based on the double-gamma hemodynamic response function (canonical HRF) and its temporal derivatives. The first-level analysis resulted in activation images (beta maps) for each of the vibrotactile stimulations and each of the cue-stimulation intervals for each of the participants. Contrasts for main effects and interactions were defined at the first level and employed for the group-level (second-level) analysis. Results are reported at p &lt; 0.05 and familywise error (FWE) corrected at the voxel level based on the Threshold-Free Cluster Enhancement (TFCE; Smith and Nichols, 2009). BOLD level analysis (<xref rid="fig2" ref-type="fig">Figure 2C</xref>) was performed by averaging the beta estimates for all stimulation events of an experimental condition over all voxels.</p>
</sec>
<sec id="s9b">
<title>Regions of interest definition</title>
<p>Regions of interest were extracted in MNI space with probabilistic cytoarchitectonic masks from the SPM Anatomy Toolbox (Version 3.0, Forschungszentrum Jülich GmbH, <xref ref-type="bibr" rid="c12">Eickhoff et al., 2005</xref>) and used for the ROI-based decoding analysis and BOLD level assessments. In particular, we extracted masks for the primary somatosensory cortices (S1; 3148 voxels), the secondary somatosensory cortices (S2; 2872 voxels), the primary motor cortex (M1; 4321 voxels); the primary visual cortex (V1; 1802 voxels) and for white matter regions (9321 voxels). We excluded voxels with a larger than 25% overlap on opposite sides of the central sulcus for a distinct analysis of S1 versus M1 (<xref ref-type="bibr" rid="c4">Ariani et al., 2022</xref>).</p>
</sec>
<sec id="s9c">
<title>Region of interest decoding</title>
<p>Samples for binary decoding were extracted within the relevant ROI from beta-maps obtained by the first-level GLM of each participant (vibrotactile finger stimulation &gt; baseline or cue-stimulation intervals &gt; baseline, resulting in 320 beta-maps used as decoding samples). The decoding analysis was implemented with a linear Support Vector Machine (SVM) with MATLAB’s fitcsvm function (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/stats/fitcsvm.html">https://www.mathworks.com/help/stats/fitcsvm.html</ext-link>). The linear SVM was employed for binary prediction of stimulated finger (ring finger or thumb; <xref rid="fig3" ref-type="fig">Figure 3A</xref>) or finger cue (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). To match the number of voxels between different ROIs, 300 voxels were randomly subsampled from each ROI for each iteration of a 10-fold cross-validation. Reported decoding accuracies are cross-validation accuracies averaged over 1000 rounds. The SVM cost parameter was used in its default setting (i.e., <italic>C</italic>=1). The decoding was balanced by subsampling to obtain the same number of stimulation events per condition (e.g., same number of ring finger versus thumb samples for each of the three experimental conditions). Decoding labels were permuted to assess the significance of decoding accuracies and the empirical p-value was calculated as the fraction of decoding accuracies from the permuted dataset larger than accuracies obtained from the original dataset based on 1000 such permutations (<xref ref-type="bibr" rid="c22">Kassraian-Fard et al., 2016</xref>; <xref ref-type="bibr" rid="c36">Ojala and Garriga, 2009</xref>).</p>
<p>To assess the generalization performance of linear decoders (<xref rid="fig5" ref-type="fig">Figure 5B</xref>) across cue-stimulation intervals and vibrotactile stimulation trails, SVM decoders were trained using the parameters described and by using data from the cue-stimulation interval (ring finger vs. thumb cue decoding) and tested on vibrotactile stimulation trials (ring finger versus thumb stimulation decoding) and vice versa. The same procedure was implemented for non-informative cue-stimulation trials which were divided based on the upcoming stimulation site (ring finger versus thumb stimulation). Reported decoding accuracies are averages across the two train-test schemes.</p>
</sec>
<sec id="s9d">
<title>Whole-brain searchlight decoding</title>
<p>The searchlight analysis (<xref rid="fig3" ref-type="fig">Figure 3C</xref> and <xref rid="fig3" ref-type="fig">3D</xref>) was implemented with the pyMVPA toolbox (<xref ref-type="bibr" rid="c16">Hanke et al., 2009</xref>) with a linear SVM (C=1) applied one by one to each voxel and its surrounding sphere with a radius of 1 voxel. The linear SVM was employed to predict stimulated fingers (ring finger or thumb) based on the 320 beta maps corresponding to the 320 stimulation events. Random subsampling of trials was implemented to ensure an equal representation of each of the two experimental conditions in train and test sets. Searchlight decoding resulted in a whole-brain accuracy map for each participant where the decoding accuracy was ascribed to the voxel in the center of the sphere. To obtain group-level results a one-tailed <italic>t</italic>-test across participants was performed against resting baseline or between experimental conditions. Group-level accuracy maps were thresholded and FWE corrected (p &lt; 0.01). Searchlight accuracy maps were plotted with the connectome workbench (<xref rid="fig3" ref-type="fig">Figure 3C</xref>) (<ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org/software/connectome-workbench">https://www.humanconnectome.org/software/connectome-workbench</ext-link>; <xref ref-type="bibr" rid="c33">Marcus et al., 2011</xref>).</p>
<p>Whole-brain searchlight decoding accuracy of a voxel (dependent variable) was compared to its Euclidean distance to the peak BOLD voxel associated with vibrotactile stimulation of ring finger or thumb (the independent variable; <xref rid="fig3" ref-type="fig">Figure 3D</xref>). This analysis was performed as follows: First, a searchlight accuracy map was obtained through binary classification of vibrotactile stimulation of each finger versus resting baseline, resulting in 50 searchlight accuracy maps (ring finger and thumb searchlight accuracy maps for 25 subjects). Then, we calculated for each voxel within the S1 region of a searchlight map its Euclidean distance to the MNI peak BOLD voxel associated with vibrotactile stimulation of the respective finger as obtained by an independent functional localizer. Finally, accuracies were averaged across both fingers for the respective distance bin. We performed a robust regression analysis (with robustbase in R) to determine the relationship between searchlight decoding accuracy and distance to peak BOLD voxel.</p>
</sec>
<sec id="s9e">
<title>Multivariate dissimilarity analysis</title>
<p>For the dissimilarity analysis between evoked activity from vibrotactile stimulation of ring finger and thumb or by ring finger and thumb cues (<xref rid="fig5" ref-type="fig">Figure 5D</xref>), we calculated the cross-validated Mahalanobis distance based on beta estimates from the first-level GLM of participants. The distance calculation was performed with a leave-one-run-out cross-validation schemed. Visualized results are averages over all voxels within the indicated ROI. Dissimilarity calculations were implemented by using the pyMVPA toolbox.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Aitken</surname> <given-names>F</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P.</given-names></string-name> <article-title>Prior expectations of motion direction modulate early sensory processing</article-title>. <source>Journal of Neuroscience</source>. <year>2020</year> <month>Aug</month> 12;<volume>40</volume>(<issue>33</issue>):<fpage>6389</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Akselrod</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Martuzzi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>van der Zwaag</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Blanke</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Serino</surname>, <given-names>A.</given-names></string-name> (<year>2021</year>). <article-title>Relation between palm and finger cortical representations in primary somatosensory cortex: A 7T fMRI study</article-title>. <source>Human Brain Mapping</source>, <volume>42</volume>(<issue>7</issue>), <fpage>2262</fpage>–<lpage>2277</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Andersson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> (<year>2007</year>). <article-title>Non-linear registration, aka spatial normalisation</article-title>. <source>Cold Spring Harbor Labs Journals: FMRIB Technical Report</source>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Ariani</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Pruszynski</surname>, <given-names>J.A.</given-names></string-name> and <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> <article-title>Motor planning brings human primary somatosensory cortex into action-specific preparatory states</article-title>. <source>Elife</source>. <year>2022</year> <month>Jan</month> 12;<volume>11</volume>:<fpage>e69517</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bell</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Morin</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Malecek</surname>, <given-names>N. J.</given-names></string-name> and <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name> (<year>2016</year>). <article-title>Encoding of Stimulus Probability in Macaque Inferior Temporal Cortex</article-title>. <source>Current Biology</source>, <volume>26</volume>(<issue>17</issue>), pp. <fpage>2280</fpage>–<lpage>2290</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Blom</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Feuerriegel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bode</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name> (<year>2020</year>). <article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title>. <source>PNAS</source>, <volume>117</volume>(<issue>13</issue>), <fpage>7510</fpage>–<lpage>7515</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname>, <given-names>D.</given-names></string-name> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), pp. <fpage>433</fpage>–<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Chambers</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Akram</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Adam</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Pelofi</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sahani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Shamma</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Pressnitzer</surname>, <given-names>D.</given-names></string-name> (<year>2017</year>). <article-title>Prior context in audition informs binding and shapes simple features</article-title>. <source>Nature communications</source>, <volume>8</volume>(<issue>1</issue>), <fpage>15027</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>de Borst</surname> <given-names>A. W.</given-names></string-name> and <string-name><surname>de Gelder</surname>, <given-names>B.</given-names></string-name> (<year>2017</year>). <article-title>fMRI-based Multivariate Pattern Analyses Reveal Imagery Modality and Imagery Content Specific Representations in Primary Somatosensory, Motor and Auditory Cortices</article-title>. <source>Cerebral Cortex</source>, <volume>27</volume>(<issue>8</issue>), pp. <fpage>3994</fpage>–<lpage>4009</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>de Borst</surname> <given-names>A. W.</given-names></string-name> and <string-name><surname>de Gelder</surname>, <given-names>B.</given-names></string-name> <article-title>Mental imagery follows similar cortical reorganization as perception: intra-modal and cross-modal plasticity in congenitally blind</article-title>. <source>Cerebral Cortex</source>. <year>2019</year> <month>Jul</month> 5;<volume>29</volume>(<issue>7</issue>):<fpage>2859</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name>, <string-name><surname>Heilbron</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name> (<year>2018</year>). <article-title>How do expectations shape perception?</article-title>. <source>Trends in cognitive sciences</source>, <volume>22</volume>(<issue>9</issue>), <fpage>764</fpage>–<lpage>779</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Stephan</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Mohlberg</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Grefkes</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fink</surname>, <given-names>G. R.</given-names></string-name>, <string-name><surname>Amunts</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Zilles</surname>, <given-names>K.</given-names></string-name> (<year>2005</year>). <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>. <source>NeuroImage</source>, <volume>25</volume>(<issue>4</issue>), pp. <fpage>1325</fpage>–<lpage>1335</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hamada</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> <article-title>Hand use predicts the structure of representations in sensorimotor cortex</article-title>. <source>Nature neuroscience</source>. <year>2015</year> <month>Jul</month>;<volume>18</volume>(<issue>7</issue>):<fpage>1034</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Fitzgerald</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Lane</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Thakur</surname>, <given-names>P. H.</given-names></string-name>, and <string-name><surname>Hsiao</surname>, <given-names>S. S.</given-names></string-name> (<year>2006</year>). <article-title>Receptive field (RF) properties of the macaque second somatosensory cortex: RF size, shape, and somatotopic organization</article-title>. <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>24</issue>), <fpage>6485</fpage>–<lpage>6495</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Gale</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Flanagan</surname>, <given-names>J. R.</given-names></string-name>, and <string-name><surname>Gallivan</surname>, <given-names>J. P.</given-names></string-name> (<year>2021</year>). <article-title>Human somatosensory cortex is modulated during motor planning</article-title>. <source>Journal of Neuroscience</source>, <volume>41</volume>(<issue>27</issue>), <fpage>5909</fpage>–<lpage>5922</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Hanke</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Halchenko</surname>, <given-names>Y. O.</given-names></string-name>, <string-name><surname>Sederberg</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Hanson</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name> and <string-name><surname>Pollmann</surname>, <given-names>S.</given-names></string-name> (<year>2009</year>). <article-title>PyMVPA: a Python Toolbox for Multivariate Pattern Analysis of fMRI Data</article-title>. <source>Neuroinformatics</source>, <volume>7</volume>(<issue>1</issue>), pp. <fpage>37</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Hernandez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zainos</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Romo</surname>, <given-names>R.</given-names></string-name> (<year>2000</year>). <article-title>Neuronal correlates of sensory discrimination in the somatosensory cortex</article-title>. <source>PNAS</source>, <volume>97</volume>(<issue>11</issue>), pp. <fpage>6191</fpage>–<lpage>6196</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> (<year>2001</year>). <article-title>A global optimisation method for robust affine registration of brain images</article-title>. <source>Medical Image Analysis</source>, <volume>5</volume>(<issue>2</issue>), pp. <fpage>143</fpage>–<lpage>156</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bannister</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> (<year>2002</year>). <article-title>Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images</article-title>. <source>NeuroImage</source>, <volume>17</volume>(<issue>2</issue>), pp. <fpage>825</fpage>–<lpage>841</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pechaud</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> (<year>2005</year>, <month>June</month>). <article-title>BET2: MR-based estimation of brain, skull and scalp surfaces</article-title>. <source>Human brain mapping</source> (Vol. <volume>17</volume>, No. <issue>3</issue>, p. <fpage>167</fpage>).</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Johansen-Berg</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Christensen</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Woolrich</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Matthews</surname>, <given-names>P. M.</given-names></string-name> (<year>2000</year>). <article-title>Attention to touch modulates activity in both primary and secondary somatosensory areas</article-title>. <source>Neuroreport</source>, <volume>11</volume>(<issue>6</issue>), <fpage>1237</fpage>–<lpage>1241</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Kassraian-Fard</surname>, <given-names>P.K.</given-names></string-name>, <string-name><surname>Matthis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Balsters</surname>, <given-names>J.H.</given-names></string-name>, <string-name><surname>Maathuis</surname> <given-names>M.H.</given-names></string-name> and <string-name><surname>Wenderoth</surname>, <given-names>N.</given-names></string-name> (<year>2016</year>). <article-title>Promises, Pitfalls, and Basic Guidelines for Applying Machine Learning Classifiers to Psychiatric Imaging data, with Autism as an example</article-title>. <source>Frontiers in Psychiatry</source>, <volume>7</volume>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Kikkert</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pfyffer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Verling</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Freund</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Wenderoth</surname>, <given-names>N.</given-names></string-name> (<year>2021</year>). <article-title>Finger somatotopy is preserved after tetraplegia but deteriorates over time</article-title>. <source>Elife</source>, <volume>10</volume>, <fpage>e67713</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Pelli</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ingling</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Murray</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Broussard</surname>, <given-names>C.</given-names></string-name> (<year>2007</year>). <article-title>What’s new in psychtoolbox-3, Pion Ltd</article-title>., <source>Perception</source>, <volume>36</volume>(<issue>14</issue>), pp. <fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Koenig-Robert</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Pearson</surname>, <given-names>J.</given-names></string-name> <article-title>Decoding the contents and strength of imagery before volitional engagement</article-title>. <source>Scientific Reports</source> <volume>9</volume>, <fpage>3504</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Failing</surname>, <given-names>M. F.</given-names></string-name> and <string-name><surname>Lange</surname>, <given-names>F. P. D.</given-names></string-name> (<year>2014</year>). <article-title>Prior Expectations Evoke Stimulus Templates in the Primary Visual Cortex</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>26</volume>(<issue>7</issue>), pp. <fpage>1546</fpage>–<lpage>1554</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jehee</surname>, <given-names>J. F.</given-names></string-name> and <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2012</year>). <article-title>Less Is More: Expectation Sharpens Representations in the Primary Visual Cortex</article-title>. <source>Neuron</source>, <volume>75</volume>(<issue>2</issue>), pp. <fpage>265</fpage>–<lpage>270</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Lange</surname>, <given-names>F. P. D.</given-names></string-name> (<year>2017</year>). <article-title>Prior expectations induce pre-stimulus sensory templates</article-title>. <source>PNAS</source>, <volume>114</volume> (<issue>39</issue>), pp. <fpage>10473</fpage>–<lpage>10478</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> (<year>2006</year>). <article-title>Information-based functional brain mapping</article-title>. <source>PNAS</source>, <volume>103</volume>(<issue>10</issue>), pp. <fpage>3863</fpage>–<lpage>3868</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Kuehn</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Mueller</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Schütz-Bosbach</surname>, <given-names>S.</given-names></string-name> <article-title>The functional architecture of S1 during touch observation described with 7 T fMRI</article-title>. <source>Brain Structure and Function</source>. <year>2014</year> <month>Jan</month>;<volume>219</volume>(<issue>1</issue>):<fpage>119</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Mangun</surname>, <given-names>G.R.</given-names></string-name> and <string-name><surname>Ding</surname>, <given-names>M.</given-names></string-name> (<year>2016</year>). <article-title>Top-down Modulation of Neural Activity in Anticipatory Visual Attention: Control Mechanisms Revealed by Simultaneous EEG-fMRI</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>2</issue>), pp. <fpage>517</fpage>–<lpage>529</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Luna</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hernandez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brody</surname>, <given-names>C. D.</given-names></string-name> and <string-name><surname>Romo</surname>, <given-names>R.</given-names></string-name> (<year>2005</year>). <article-title>Neural codes for perceptual discrimination in primary somatosensory cortex</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>(<issue>9</issue>), pp. <fpage>1210</fpage>–<lpage>1219</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Marcus</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Harwell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Olsen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hodge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Prior</surname>, <given-names>F.</given-names></string-name> and <string-name><surname>Essen</surname>, <given-names>D. C. V.</given-names></string-name> (<year>2011</year>). <article-title>Informatics and Data Mining Tools and Strategies for the Human Connectome Project</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>5</volume>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Martuzzi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zwaag</surname>, <given-names>W. V. D.</given-names></string-name>, <string-name><surname>Farthouat</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gruetter</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Blanke</surname>, <given-names>O.</given-names></string-name> (<year>2012</year>). <article-title>Human finger somatotopy in areas 3b, 1, and 2: A 7T fMRI study using a natural stimulus</article-title>. <source>Human Brain Mapping</source>, <volume>35</volume>(<issue>1</issue>), pp. <fpage>213</fpage>–<lpage>226</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="other"><collab>MATLAB, MATLAB and Statistics Toolbox Release 2018a</collab>, <source>The MathWorks, Inc., Natick, Massachusetts, United States</source>, <year>2018</year></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="other"><string-name><surname>Ojala</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Garriga</surname>, <given-names>G. C.</given-names></string-name> (<year>2009</year>). <article-title>Permutation Tests for Studying Classifier Performance</article-title>. <source>Ninth IEEE International Conference on Data Mining</source>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname>, <given-names>D. G.</given-names></string-name> (<year>1997</year>). <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), pp. <fpage>437</fpage>–<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Pessoa</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name> (<year>2003</year>). <article-title>Neuroimaging studies of attention: from modulation of sensory processing to top-down control</article-title>. <source>Journal of Neuroscience</source>, <volume>23</volume>(<issue>10</issue>), <fpage>3990</fpage>–<lpage>3998</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Preuschhof</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schubert</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Villringer</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Heekeren</surname>, <given-names>H. R.</given-names></string-name> (<year>2010</year>). <article-title>Prior Information Biases Stimulus Representations during Vibrotactile Decision Making</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>22</volume>(<issue>5</issue>), pp. <fpage>875</fpage>–<lpage>887</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="web"><collab>R Core Team</collab> (<year>2021</year>). <source>R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria</source>. ISBN <isbn>3-900051-07-0</isbn>, URL <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Reissland</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Francis</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Aydin</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Mason</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Schaal</surname>, <given-names>B.</given-names></string-name> (<year>2014</year>). <article-title>The development of anticipation in the fetus: A longitudinal account of human fetal mouth movements in reaction to and anticipation of touch</article-title>. <source>Developmental psychobiology</source>, <volume>56</volume>(<issue>5</issue>), <fpage>955</fpage>–<lpage>963</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Richter</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ekman</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2018</year>). <article-title>Suppressed sensory response to predictable object stimuli throughout the ventral visual stream</article-title>. <source>Journal of Neuroscience</source>, <volume>38</volume>(<issue>34</issue>), <fpage>7452</fpage>–<lpage>7461</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Richter</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2019</year>). <article-title>Statistical learning attenuates visual activity only for attended stimuli</article-title>. <source>Elife</source>, <volume>8</volume>, <fpage>e47869</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Ruben</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schwiemann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Deuchert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Meyer</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Krause</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Curio</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Villringer</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kurth</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Villringer</surname>, <given-names>A.</given-names></string-name> (<year>2001</year>). <article-title>Somatotopic organization of human secondary somatosensory cortex</article-title>. <source>Cerebral cortex</source>, <volume>11</volume>(<issue>5</issue>), <fpage>463</fpage>–<lpage>473</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Sanchez-Panchuelo</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Besle</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Schluppeck</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Humberstone</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Francis</surname>, <given-names>S.</given-names></string-name> (<year>2018</year>). <article-title>Somatotopy in the human somatosensory system</article-title>. <source>Frontiers in human neuroscience</source>, <volume>12</volume>, <fpage>235</fpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>Stephen M.</given-names></string-name> and <string-name><surname>Nichols Thomas</surname> <given-names>E.</given-names></string-name> <article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title>. <source>NeuroImage</source>, <year>2009</year>, <volume>44</volume>. Jg., Nr. 1, S. <fpage>83</fpage>–<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2002</year>). <article-title>Fast robust automated brain extraction</article-title>. <source>Human Brain Mapping</source>, <volume>17</volume>(<issue>3</issue>), pp. <fpage>143</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Walther</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nili</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ejaz</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). <article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title>. <source>Neuroimage</source>, <volume>137</volume>, <fpage>188</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> (<year>2001</year>). <article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>20</volume>(<issue>1</issue>), pp. <fpage>45</fpage>–<lpage>57</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s10">
<title>Supplementary Material</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Supplementary Figure 1:</label>
<caption><title>Psychometric curves for the vibrotactile detection task.</title>
<p>Psychometric curves for vibrotactile stimulus detection (% correct) during a total of 320 trials over 8 imaging runs as a function of vibrotactile stimulation intensity (Volt). The psychophysical curve of the congruent condition is shifted to the left as compared to the psychophysical curves of the incongruent and the non-informative experimental conditions, indicative of a lower vibrotactile detection threshold for the former. Curves represent logistic fits to mean values across all participants (N=25), error bands around psychophysical curves represent the 95% bootstrap confidence intervals.</p></caption>
<graphic xlink:href="511201v3_figS1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supplementary Table 1:</label>
<caption><title>Average vibrotactile stimulation intensities for experimental condition and finger.</title></caption>
<graphic xlink:href="511201v3_tblS1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figS2" position="float" fig-type="figure">
<label>Supplementary Figure 2:</label>
<caption><title>The ipsilateral S1 BOLD level is not modulated by prior information.</title>
<p>The mean BOLD signal in ipsilateral S1 is not significantly different for congruent (3.229±0.249) vs. incongruent (3.355±0.33) and non-informative trials (3.743±0.261; a.u.: arbitrary units; one-way repeated measures ANOVA for Experimental condition: <italic>F</italic><sub><italic>2,48</italic></sub> = 1.789, p=0.71). Single-participant-results are visualized by gray lines (N=25).</p></caption>
<graphic xlink:href="511201v3_figS2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supplementary Figure 3:</label>
<caption><title>ROI-based decoding of vibrotactile stimulation of ring finger vs. thumb for secondary somatosensory and primary motor cortex</title>
<p><bold>a.</bold> Left and center: Ipsilateral S2 ROI-decoding accuracies are not significantly different from chance level for any of the experimental conditions (Ipsilateral S2; congruent: 49.928±5.703%, incongruent: 49.726±6.392%, non-informative: 50.372±5.991% Contralateral S2; congruent: 49.527±5.382%, incongruent: 49.448±4.976%, non-informative: 48.89±5.832%, mean±SEM, p&gt;0.5). Right: Decoding accuracies for contralateral M1 were significantly greater than chance for congruent (54.605±5.78%) and non-informative trials (52.368±6.11%), but not incongruent trials (49.368±4.928%). Repeated-measures two-way ANOVA: Interaction Experimental condition x ROI, <italic>F</italic><sub><italic>2,96</italic></sub> = 9.452, p&lt;0.0001, η<sup>2</sup> = 0.311. Pairwise t-tests with post-hoc Bonferroni correction: * p&lt;0.05; **** p&lt;0.0001. Single-participant-results are visualized by gray lines. <bold>b.</bold> Improvements of behavioral detection accuracies (x-axis) and ROI-based decoding accuracies (y-axis) are not correlated for contralateral (Spearman’s r, p=0.181, robust regression slope: 0.594 with shaded area: 95% CI) or ipsilateral M1 (Spearman’s r, p=0.71, robust regression slope: -0.272 with shaded area: 95% CI).</p></caption>
<graphic xlink:href="511201v3_figS3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supplementary Table 2:</label>
<caption><title>Peak t-values associated with vibrotactile stimulation of ring finger versus thumb.</title></caption>
<graphic xlink:href="511201v3_tblS2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Supplementary Table 3:</label>
<caption><title>Group-level searchlight maximal accuracies for classification of ring finger versus thumb stimulation.</title></caption>
<graphic xlink:href="511201v3_tblS3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89049.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Kok</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study presents a <bold>valuable</bold> set of findings on how prior expectations modulate tactile sensory processing. The neuroimaging evidence supporting the main conclusions is <bold>solid</bold>, although the nature of the experimental task somewhat limits the interpretation of the findings. This work will be of interest to neuroscientists working on sensory processing and perception.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89049.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>This study explored how expectations influence tactile perception. In summary, anticipating a tactile event enhances detection compared to when knowledge is lacking or ambiguous. However, prior information can also impair performance if the expected and actual stimuli are incongruent. The authors used fMRI and multivariate decoding analyses to investigate the underlying mechanisms of this behavioural phenomenon.</p>
<p>They stimulated two fingers (thumb and ring) of the left hand and analysed activity patterns in contralateral and ipsilateral somatosensory regions during and before stimulation. They were able to distinguish activity patterns for the two fingers during both stimulation and the pre-stimulation stage, specifically for the congruent condition. The authors suggest that congruent vibrotactile stimulation leads to higher multivariate information content and improved behavioural detection performance. They also found that the expectation of vibrotactile stimulation elicits somatotopic activity in contralateral S1, similar to the activity generated by actual stimulation.</p>
<p>I thoroughly enjoyed reading this well-written and clear work. The incorporation of multivariate decoding analysis alongside univariate analysis is a good choice for addressing the claimed questions. In the following sections, I will highlight the strengths and weaknesses of the study. While I generally agree with the authors' conclusions regarding the functional mechanisms underlying behavioural improvements, I believe there are limitations in the experimental design and chosen measures that constrain the interpretations drawn from the results. I hope that my comments can contribute to clarifying certain details and improving aspects of the study that may be considered weak. I believe this study holds significance for the field and provides a foundation for future investigations into the influence of top-down processing on tactile processing.</p>
<p>Strengths:</p>
<p>1. The research question is highly intriguing as it delves into the unexplored territory of top-down processes within the tactile domain that still needs to be well characterised.</p>
<p>2. The addition of multivariate decoding analysis alongside the univariate analysis was a good choice in my opinion, since activity level per se may not accurately reflect the underlying information content. Both high activity levels and absence of activity (as observed in this study) can still contain information. To be more specific, Figure 2C shows no significant activity in the congruent condition, but significant decoding for the two finger activity patterns is still possible in this condition (Figure 3A).</p>
<p>3. The utilization of a staircase before each functional run was also a good approach, although a potential limitation is noted (discussed below). Considering that prior knowledge can be particularly influential in the presence of weak or noisy stimuli, it is crucial to confirm that the stimulation was at threshold to maximize the likelihood of detecting differences in the pre-stimulus stage.</p>
<p>Weaknesses:</p>
<p>1. My main concern regarding this study lies in the choice of a detection paradigm, which may introduce response biases and affect the interpretation of results. If the threshold was set too low for some participants, it is possible that they reported feeling the touch more frequently on the cued finger, even when no actual sensation was present. Consequently, accuracy may be inflated for the congruent condition and reduced for the incongruent condition, making it difficult to attribute the observed improvements solely to enhanced detection. I think it would have been more appropriate to use a discriminatory task (e.g., discriminating pin patterns), as employed in Kok et al., 2012, where behavioural performance can be directly linked to decoding accuracy between related activity patterns. Additionally, incorporating trials with no stimulation (I am not sure whether this was the case in this study) and utilizing &quot;None&quot; responses to calculate accuracy could provide a more reliable measure of performance. Using dprime as a performance measure, which is bias-free, may be more appropriate. However, I remain concerned that participant responses are influenced more by the cue than the actual detection of stimuli.</p>
<p>2. While I appreciate the use of the staircase method, I was somewhat surprised by the relatively short length of each staircase (only 7 trials). I might not have extensive experience in this area, therefore this might still be ok for fingers, but I want to emphasize the importance of accurately determining the threshold for this study (as discussed in the previous point). However, I can see from Figure 1B that there seems to be consistency across runs (at least in the shown participant).</p>
<p>3. The absence of significant decoding in the incongruent condition (Figure 3A) raises some questions. It seems reasonable to expect that discrimination between the two finger activity patterns should still be possible in this condition, albeit with reduced accuracy as observed in Kok et al., 2012. Could this lack of significant decoding result from the detection task or possibly due to the smaller number of trials in the incongruent condition?</p>
<p>4. I am a bit confused about which specific region of interest (ROI) was used for both the univariate and decoding analysis during the stimulation stage, and the decoding analysis and RSA during the pre-stimulation phase. From my understanding, the entire S1 region (as defined using the SPM Anatomy toolbox) was included, encompassing not only the hand territory but the entire body. However, I may have misinterpreted the methodology. Given that an independent localizer was used to define ROIs for the univariate analysis during the pre-stimulation phase, it raises the question of why the same approach was not applied to the analyses during the stimulation phase and the multivariate analysis during the pre-stimulation phase.</p>
<p>5. By using a large ROI for analysis (as mentioned in point 4), the straightforward interpretation of BOLD level (i.e., no significant activity) in the congruent condition (Figure 2C) becomes less clear. It raises the question of whether there is truly no activity in the congruent condition or if the activity would be observed with a smaller region. This aligns with the findings of Kok et al., 2012, where they demonstrate activity in both expected and unexpected conditions, albeit reduced in the expected condition.</p>
<p>6. Point 5 raises another issue regarding the suggestion that significant decoding results imply higher multivariate information content in finger representations of congruent vibrotactile stimulations. Suppose a smaller ROI were used, revealing activity in the congruent condition and differential activity between the two finger conditions. In that case, the substantial difference in activation levels suggests that increased decoding accuracy may not necessarily require higher multivariate information content. It is conceivable that discrimination between the two conditions could be achieved with just two voxels-one in the thumb territory and one in the index territory.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89049.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary</p>
<p>The authors conducted a study where participants were perceiving near-threshold touch at either the thumb or ring finger while lying in the MR scanner. Prior to stimulation, a visual cue indicated to them with 80% probability which finger would be touched next (thumb or ring finger), or did not provide meaningful information on which finger would be touched. Subsequently, participants were asked to indicate which finger was actually touched via button press. By showing that 1. participants were more accurate in responding which finger was touched in the congruent compared to the incongruent and neutral conditions, 2. S1 responses were higher in the incongruent compared to the congruent and neutral conditions, 3. decoding accuracies were higher for the congruent compared to incongruent and neutral conditions, and 4. decoding was also successful in the period after cueing and before stimulation, the authors argue that similar to V1, S1 shows decreased BOLD activation in response to expected versus non-expected stimuli, whereas the finger-specific response is more precise for expected versus non-expected stimuli. The authors also argue that behavioral improvement is associated to a tactile stimulus being predicted in location.</p>
<p>Strengths</p>
<p>The manuscript combines a behavioral threshold task that can be analyzed using psychophysics with BOLD responses in S1, providing a rich paradigm to understand the relationship between S1 responsively and tactile perception. The authors combine GLM with both ROI-based and whole-brain searchlight-based decoding analyses, and therefore offer different analyses methods to obtain a comprehensive picture of the S1 responsively during expected versus non-expected touch. It is also a strength of the paper that two different fingers were investigated, hence addressing the aspect of topography.</p>
<p>Weaknesses</p>
<p>The behavioral paradigm that was chosen is not ideal to address the authors' questions on whether or not behavior improves for expected versus non-expected touch. More precisely, in 80% of the cases when it was indicated that the ring finger would be touched, in fact later the ring finger was touched, whereas in 80% of the cases when it was indicated that the thumb would be touched, in fact later the thumb was touched. In the congruent conditions where later the indicated finger was indeed touched, participants showed on average 70% accuracy. Therefore, they could have reached this accuracy level simply by choosing the indicated finger unless they had a strong sensation that indicated to them to respond otherwise. In order to show that the cueing can improve behavioral performance, one would have to choose a tactile task that is not related to finger identity (which was cued), such as frequency detection or spatial acuity.</p>
<p>The correlation between accuracy and decoding accuracies as shown in Figure 3b does not seem to be correct. The decoding accuracies indicate how well the algorithm can differentiate between D1 versus D4 stimulation in the congruent condition, whereas the behavior indicates the difference between congruent and incongruent responses. I think those two measures should not directly be compared, in addition to the general problem that is inherent in the behavioral paradigm, as outlined above. I would therefore treat this correction and the behavioral analyses in general with great caution.</p>
<p>Alternative ways to interpret the data</p>
<p>It is worth noting that the incongruent stimulation condition did not reveal significant D1 versus D4 decoding results neither when ROI-based decoding was used nor when searchlight-based decoding was used (see Figure 3a,c). Therefore, it seems that when the wrong finger was cued, the finger representation of the actually touched finger did not respond. Given the decoding accuracy is even below 50% for the incongruent ROI-based decoding, this seems to indicate that the finger-specific response in S1 to the cued finger is even stronger than the finger-specific response in S1 to the actually touched finger. This may be the major take-home-message of the paper. This hypothesis could be directly tested by showing the the plot in Figure 2c for each finger: The results may show that the higher activation in the incongruent condition is actually due to the fact that in this condition, both the non-touched and finger the touched finger respond, whereas this is not the case for the other conditions.</p>
<p>When discussing this finding, the authors write that &quot;finger representations of congruent vibrotactile stimulations are associated with higher multivariate information content, are more aligned with the somatotropin organization in contralateral S1, and that the enhanced representation of these stimuli is strongly associated with behavioral detection performance.&quot; - A better formulation may be that for threshold tactile stimulation, the expectation of finger touch can override the actual finger touch, indicating a strong influence of top-down control on S1 finger maps. This is also supported by the analyses that there is finger-specific activation in the cue-stimulation interval. However, as indicated above, finger- and condition-specific BOLD activation needs to be shown to explore this in more detail.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.89049.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The authors have devised a clever experimental design involving the provision of cues to participants, indicating the finger that is more likely to be stimulated in each trial (e.g., ring finger or thumb). Employing fMRI analyses, the authors have leveraged the distinct and well-defined finger representations in the somatosensory cortex to investigate how prior knowledge influences the processing of haptic stimuli in a probability cueing paradigm. The authors successfully replicate key neural phenomena associated with predictive processing, encompassing expectation suppression, the sharpening of expected information representation, and the pre-activation of sensory templates associated with the anticipated stimulus. The methodology employed in this study is straightforward, and the obtained results are convincing.</p>
<p>However, it is worth noting that the cue-finger and finger associations were explicitly conveyed to the participants in this study. Additionally, the inter-stimulus interval (ISI) between the finger-cue and the cue varied randomly across trials, rendering the onset of the cue unpredictable (in time) for the participants. These experimental manipulations lead me to consider that the observed results may not be solely explained by predictive mechanisms but could also involve top-down controlled attention. It would be valuable for the authors to include a task similar to Experiment 2 in Kok et al. (2012), where participants' attention was diverted away from the gratings contrast, yet decoding sharpening for expected but task-irrelevant stimulus orientations was still evident. By incorporating such a task, it would help elucidate whether the authors would replicate similar results when predictive information remains intact but the predicted stimulus feature becomes task-irrelevant.</p>
<p>Furthermore, I have concerns regarding potential issues related to the training of the multivariate decoder. If I understand correctly, instead of using the functional localiser to train the SVM classifier, the authors directly employed the experimental data from the congruent, incongruent, and non-informative conditions together. It is noted that the number of trials used in each training fold was downsampled to achieve an equal number of trials from each condition, controlling for the asymmetry in number of trials between the incongruent and congruent conditions. However, I am concerned that if there are univariate differences between the activity patterns in the training datasets (e.g., congruent &lt; incongruent), the decoder might exhibit a bias towards relying more on the activity of one specific condition, thereby potentially performing better in decoding that particular condition. To address this, I suggest presenting Representational Similarity Analysis (RSA) results using the activity patterns evoked by congruent, incongruent, and non-informative stimuli. This analysis would offer a simpler, more interpretable representation of changes in the representational geometry of the stimuli based on previous predictions (see Blank &amp; Davis, 2016), and might shed some light on whether your results correspond on sharpening or dampening of the expected information.</p>
</body>
</sub-article>
</article>