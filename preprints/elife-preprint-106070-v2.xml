<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">106070</article-id>
<article-id pub-id-type="doi">10.7554/eLife.106070</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106070.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Dynamic fMRI networks of emotion</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9770-4458</contrib-id>
<name>
<surname>Janssen</surname>
<given-names>Niels</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>njanssen@ull.es</email>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Elvira</surname>
<given-names>Uriel KA</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7613-2067</contrib-id>
<name>
<surname>Janssen</surname>
<given-names>Joost</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>van Erp</surname>
<given-names>Theo GM</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01r9z8p25</institution-id><institution>Department of Psychology, Universidad de la Laguna</institution></institution-wrap>, <city>San Cristóbal de La Laguna</city>, <country country="ES">Spain</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01r9z8p25</institution-id><institution>Institute of Biomedical Technologies, Universidad de La Laguna</institution></institution-wrap>, <city>San Cristóbal de La Laguna</city>, <country country="ES">Spain</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01r9z8p25</institution-id><institution>Institute of Neurosciences, Universidad de la Laguna</institution></institution-wrap>, <city>San Cristóbal de La Laguna</city>, <country country="ES">Spain</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0111es613</institution-id><institution>Department of Child and Adolescent Psychiatry, Institute of Psychiatry and Mental Health, Hospital General Universitario Gregorio Marañón</institution></institution-wrap>, <city>Madrid</city>, <country country="ES">Spain</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0111es613</institution-id><institution>Ciber del Área de Salud Mental, Instituto de Investigación Sanitaria Gregorio Marañón</institution></institution-wrap>, <city>Madrid</city>, <country country="ES">Spain</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0575yy874</institution-id><institution>Department of Psychiatry, UMCU Brain Center, University Medical Center Utrecht</institution></institution-wrap>, <city>Utrecht</city>, <country country="NL">Netherlands</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04gyf1771</institution-id><institution>Clinical Translational Neuroscience Laboratory, Department of Psychiatry and Human Behavior, University of California Irvine</institution></institution-wrap>, <city>Irvine</city>, <country country="US">United States</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04gyf1771</institution-id><institution>Center for the Neurobiology of Learning and Memory, University of California Irvine</institution></institution-wrap>, <city>Irvine</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Hu</surname>
<given-names>Xiaoqing</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02zhqgq86</institution-id>
<institution>University of Hong Kong</institution>
</institution-wrap>
<city>Hong Kong</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-06-23">
<day>23</day>
<month>06</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-11-21">
<day>21</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP106070</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-02-05">
<day>05</day>
<month>02</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-02-10">
<day>10</day>
<month>02</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.05.636581"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-06-23">
<day>23</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106070.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.106070.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.106070.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.106070.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Janssen et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Janssen et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-106070-v2.pdf"/>
<abstract><p>The experience of emotions is that of dynamic, time changing processes. Yet, many fMRI studies of emotion average across time to focus on maps of static activations, overlooking the temporal dimension of emotional responses. In this study, we used time-resolved fMRI, group spatial independent component analysis (ICA), dual regression, and Gaussian curve fitting to examine both the spatial and temporal properties of whole-brain networks during a behavioral task. This task included trials that spanned over 25 seconds of watching short, emotionally evocative movie clips, making emotion-related decisions, and an intertrial rest period. We identified four whole-brain networks with unique spatial and temporal features that mapped onto different stages of the task. A network activated early in the course of the task included perceptual and affective evaluation regions, while two later networks supported semantic interpretation and decision-making, and a final network aligned with default mode activity. Both spatial and temporal properties of all four networks were modulated by the emotional content of the movie clips. Our findings extend current models of emotion by integrating temporal dynamics with large-scale network activity, offering a richer framework for understanding how emotions unfold across distributed circuits. Such temporal-spatial markers of emotional processing may prove valuable for identifying and tracking alterations in clinical populations.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05r0vyz12</institution-id>
<institution>Ministerio de Ciencia, Innovación y Universidades</institution>
</institution-wrap>
</funding-source>
<award-id>PID2021-127611NB-I00</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>This version of the manuscript was revised in accordance with the journal reviewer requests.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The relationship between emotions and brain function has been a central focus of affective neuroscience, with foundational studies elucidating the roles of structures such as the amygdala, insula, and prefrontal cortex in emotional processing (<xref ref-type="bibr" rid="c40">LaBar, LeDoux, Spencer, &amp; Phelps, 1995</xref>; <xref ref-type="bibr" rid="c52">Ochsner, Bunge, Gross, &amp; Gabrieli, 2002</xref>). Despite significant progress, most functional MRI (fMRI) studies have adopted a static view of emotions, often emphasizing localized brain activations or static connectivity patterns rather than the dynamic and distributed nature of emotional processing (see <xref ref-type="bibr" rid="c38">Kuppens &amp; Verduyn, 2017</xref>; <xref ref-type="bibr" rid="c58">Pessoa, 2017</xref>; <xref ref-type="bibr" rid="c72">Waugh, Shing, &amp; Avery, 2015</xref>, for discussion). This particular approach to studying emotions arises in part because analytical techniques collapse time-varying fMRI activity into static spatial maps and also because experimental paradigms typically use unchanging stimuli (<xref ref-type="bibr" rid="c51">Murphy, Nimmo-Smith, &amp; Lawrence, 2003</xref>; <xref ref-type="bibr" rid="c60">Phan, Wager, Taylor, &amp; Liberzon, 2002</xref>). In the present study, we collected fMRI data while participants performed a behavioral task with trials that paired the viewing of emotionally evocative short movie clips with a subsequent discrete emotion decision task as well as an intertrial rest period. We sought to address three key questions: (1) What distinct whole-brain networks underlie performance during this task? (2) Do the spatial distributions of these networks vary as a function of the emotional content of the stimuli? (3) Are the temporal dynamics of these networks—such as their time to peak, peak value or duration sensitive to the emotional characteristics of the stimuli? To answer these questions we relied on a data-driven approach that emphasized a combined spatial and temporal view on fMRI data analysis.</p>
<p>Previous studies investigating emotional processing with fMRI have generally revealed a static view of emotions in the brain. Specifically, recent research has highlighted that emotions engage widespread networks involving multiple interconnected regions, reflecting the complex and distributed nature of emotional processing (<xref ref-type="bibr" rid="c35">Kober et al., 2008</xref>; K. A. <xref ref-type="bibr" rid="c42">Lindquist, Wager, Kober, Bliss-Moreau, &amp; Barrett, 2012</xref>; <xref ref-type="bibr" rid="c56">Palomero-Gallagher &amp; Amunts, 2022</xref>; <xref ref-type="bibr" rid="c58">Pessoa, 2017</xref>; <xref ref-type="bibr" rid="c78">Zhou et al., 2021</xref>). For instance, <xref ref-type="bibr" rid="c61">Riedel et al. (2018)</xref> applied a meta-analytic clustering approach to over 1,700 neuroimaging experiments to delineate five distinct whole-brain networks associated with emotional processing. The first two networks were linked with visual and auditory perception, with convergent activation in the visual (occipital) and auditory (superior temporal) cortices, respectively. The third network captured the salience network, including the insula and dorsal anterior cingulate cortex, reflecting attention to emotionally salient information. The fourth network was associated with appraisal and prediction of emotional events, involving regions of the default mode network, such as the medial prefrontal and posterior cingulate cortices. The fifth network focused on the induction of emotional responses, with key contributions from the amygdala, parahippocampal gyri, and fusiform gyri, underscoring its role in generating contextually relevant emotional responses. While this and other studies highlight the involvement of large scale networks in the processing of emotion and therefore go beyond the traditional one-to-one mapping between brain regions and emotions (e.g., <xref ref-type="bibr" rid="c51">Murphy et al., 2003</xref>), they do not reveal the temporal component associated with these networks.</p>
<p>Traditionally, many fMRI studies of emotion have employed decontextualized static images of facial expressions or evocative pictures to elicit emotional responses (<xref ref-type="bibr" rid="c1">Adolphs, 2002</xref>; <xref ref-type="bibr" rid="c44">Liu, Liu, Zheng, Zhao, &amp; Fu, 2021</xref>; <xref ref-type="bibr" rid="c71">Wallenwein, Schmidt, Hass, &amp; Mier, 2024</xref>). In contrast, more recent studies have argued that movies offer a richer perceptual experience that is of a higher ecological validity (<xref ref-type="bibr" rid="c17">Finn &amp; Bandettini, 2021</xref>; <xref ref-type="bibr" rid="c68">Sonkusare, Breakspear, &amp; Guo, 2019</xref>). Filmmakers are adept at eliciting specific emotions in viewers (<xref ref-type="bibr" rid="c73">Westermann, Spies, Stahl, &amp; Hesse, 1996</xref>), and recent studies have shown that using movies as emotion-evoking stimuli can provide new insights into the relationship between emotions and the brain (<xref ref-type="bibr" rid="c28">Jääskeläinen, Sams, Glerean, &amp; Ahveninen, 2021</xref>; <xref ref-type="bibr" rid="c50">Morgenroth et al., 2023</xref>; <xref ref-type="bibr" rid="c63">Saarimäki, 2021</xref>). However, a limitation of current studies using movie clips is that they often employ lengthy segments lasting several minutes, during which multiple cognitive and emotional events can occur (e.g., <xref ref-type="bibr" rid="c69">Vemuri &amp; Surampudi, 2015</xref>; <xref ref-type="bibr" rid="c75">Xu et al., 2023</xref>). This complexity makes it challenging to disentangle the dynamic changes in emotional processing over time (but see, <xref ref-type="bibr" rid="c46">Meer, Breakspear, Chang, Sonkusare, &amp; Cocchi, 2020</xref>). To address this issue, shorter movie clips focusing on a single event can be utilized to better control and observe how emotions develop over time within events. In addition, an important aspect of emotions concerns not only their initial expression but also how they are regulated following that initial response (<xref ref-type="bibr" rid="c16">Etkin, Büchel, &amp; Gross, 2015</xref>; <xref ref-type="bibr" rid="c53">Ochsner &amp; Gross, 2008</xref>). This suggests that the study of the brain dynamics of emotions should focus not only on the stimulus event but should also examine how brain responses change after the event has concluded. Incorporating short periods of rest after the presentation of short movie clips may facilitate the investigation of post-event emotional regulation processes and how they are represented in the brain.</p>
<p>In the current study, participants viewed 12.5s audiovisual clips selected from the movie Forrest Gump (<xref ref-type="bibr" rid="c77">Zemeckis et al., 1994</xref>). Each clip was specifically chosen to evoke a single emotion—happiness, fear, or sadness—with each emotion equally represented across a total of 15 clips. Following each clip, participants completed a two-alternative forced-choice decision task using discrete emotion words denoting the emotion experienced by the movie’s protagonist. Finally, there was a 10s intertrial rest period. We collected whole-brain fMRI data at 2mm spatial resolution, and extracted the event-related fMRI signal at 2s temporal resolution across a 28s epoch for each emotion type and for each participant using slice-based fMRI (<xref ref-type="bibr" rid="c29">Janssen, Hernández-Cabrera, &amp; Foronda, 2018</xref>; <xref ref-type="bibr" rid="c30">Janssen &amp; Mendieta, 2020</xref>). In parallel with resting-state analyses, we then performed group spatial Independent Component Analysis (ICA) to identify large-scale networks with distinct spatial distributions and temporal profiles (<xref ref-type="bibr" rid="c6">Beckmann &amp; Smith, 2004</xref>; <xref ref-type="bibr" rid="c67">Smith et al., 2009</xref>). The spatial and temporal properties of the discovered networks were further quantified using dual regression (Beckmann, Mackay, Filippini, <xref ref-type="bibr" rid="c67">Smith, et al., 2009</xref>) and Gaussian curve fitting (<xref ref-type="bibr" rid="c30">Janssen &amp; Mendieta, 2020</xref>; <xref ref-type="bibr" rid="c37">Kruggel, Zysset, &amp; von Cramon, 2000</xref>; M. A. <xref ref-type="bibr" rid="c43">Lindquist, Loh, Atlas, &amp; Wager, 2009</xref>), respectively. Finally, we performed mixed-effects regression modeling and inferential statistics to establish the reliability of our results.</p>
<p>The emotions that are experienced by the protagonist in each clip developed gradually over the course of each clip rather than appearing immediately. In other words, while the visual and auditory perception of the movie clip is instantaneous, the comprehension of its emotional content requires more nuanced processing that lags behind the immediate perceptual input stream. Therefore, we expected to detect brain networks associated with initial sensory processing, involving primary visual and auditory cortices (<xref ref-type="bibr" rid="c61">Riedel et al., 2018</xref>), as well as networks related to higher-level cognitive and emotional processing, such as those involving the middle temporal gyrus and other association areas (<xref ref-type="bibr" rid="c27">Huth, De Heer, Griffiths, Theunissen, &amp; Gallant, 2016</xref>; <xref ref-type="bibr" rid="c70">Visser, Jefferies, Embleton, &amp; Lambon Ralph, 2012</xref>). Given that we analyzed the entire behavioral task—including the decision stage and the rest period—we anticipated identifying additional large-scale networks associated with these later stages of the task, such as a response initiation network involving primary motor cortices and the default mode network (<xref ref-type="bibr" rid="c64">Satpute &amp; Lindquist, 2019</xref>).</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Twenty-five native speakers of Spanish took part in the experiment (13 females, mean age 21.9 yrs, sd 3.9 yrs). Participants were recruited from the student population at the University of La Laguna, and received course credit or were paid 10 Euros. All participants were right-handed and had no neurological or psychiatric disorders and were not taking any medication. The study was conducted in compliance with the declaration of Helsinki, and all participants provided informed consent in accordance with the protocol established by the Ethics Commission for Research of the University of La Laguna (Comite de Etica de la Investigacion y Bienestar Animal). Participants answered a brief questionnaire about their knowledge of the movie Forrest Gump. Due to technical problems and on the basis of quality control procedures for both structural and functional data (see <xref rid="fig1" ref-type="fig">Figures 1A</xref> and <xref rid="fig1" ref-type="fig">B</xref>), data from three participants were excluded from further analyses.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Overview of Quality Control of structural data with Euler number (A), fMRI head motion scores (B), analysis of behavioral C) and reaction times (D) and overview of the task (E).</title>
</caption>
<graphic xlink:href="636581v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a1">
<title>Behavioral task</title>
<p>We selected 15 clips from the movie Forrest Gump (<xref ref-type="bibr" rid="c77">Zemeckis et al., 1994</xref>). The movie was dubbed with Spanish (mainland Spain) audio. Each clip was exactly 12.5 s long and was chosen such that it portrayed a happy, sad, or fearful experience for the Forrest Gump character (see <xref rid="fig1" ref-type="fig">Figure 1E</xref>). Clips were chosen such that they contained a natural onset and offset with no abrupt cuts and were played at their intended speed. For example, in one of the clips, Forrest walks in a garden and sees Jenny again after a long time, constituting a happy experience. The order of the fifteen video clips followed the natural story progression of the movie and therefore did not vary between participants. Please see <xref rid="tbl1" ref-type="table">Table 1</xref> for the specific order of the emotions expressed by Forrest across trials. Clips were presented with sound. To ensure the sound was audible over the scanner noise, participants wore headphones and we used the ffmpeg tool to boost the sound of the clips by 300%. Pilot testing ensured the sound was audible.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><title>Order of the movie clips in the experiment</title></caption>
<graphic xlink:href="636581v2_tbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>Prior to the experiment, participants were instructed to watch each clip and then make a two-alternative forced choice on the emotion experienced by Forrest. Participants were given handheld controllers in their left and right hands and instructed to press a button with their index finger on the left or right hand following the left-right location of their preferred emotion word on the screen. On a given trial, the movie clip was presented for 12.5 s followed by two emotion words for 2.5 s, and finally followed by a 10 s blank screen (see <xref rid="fig1" ref-type="fig">Figure 1E</xref> for a graphical overview). The total trial time was 25 s. The total duration of the experiment was 6.25 minutes.</p>
<p>The experiment involved a single run but was conducted in the context of a larger acquisition protocol in which an additional resting-state scan was acquired. Acquisition of these two fMRI runs was preceded by T1w and T2w image acquisitions (see below for sequence details). Stimulus presentation was controlled by Neurobs Presentations (v14). Participants in the scanner viewed and listened to the stimuli with MRI compatible goggles and headphones made by VisuaStim. The goggles provided an image resolution of 800 by 600 pixels at 60 Hz. The MRI compatible handheld controllers were also made by VisuaStim. Stimulus presentation was directly synchronized with the MRI machine.</p>
</sec>
<sec id="s2a2">
<title>MRI acquisition</title>
<p>All images were acquired using a 3T Signa Excite scanner (General Electric, Milwaukee, WI, USA) with a standard transmit/receive 8 channel head coil. Head movement was minimized by fixating MRI compatible goggles between a participant’s head and the coil using spongepads. fMRI data were acquired using Gradient-Echo EPI. The sequence parameters were chosen to provide high spatial-resolution while preserving whole brain coverage. We used an axial zoomed-acquisition where our FOV and acquisition matrix were reduced in the (left-right) phase encoding direction (<xref ref-type="bibr" rid="c55">Olman, Davachi, &amp; Inati, 2009</xref>). Specifically, the slice thickness was 2.6 mm (no gap), FOV was 256 x 128 mm and the matrix was 128 x 64 resulting in 2 x 2 x 2.6 mm voxels. To achieve whole-brain coverage we collected 39 slices and set the TR to a relatively long 3 s. However, because stimulus onset was jittered with respect to the TR, as explained below, the value of the TR does not determine the temporal resolution of the extracted fMRI signal (<xref ref-type="bibr" rid="c29">Janssen et al., 2018</xref>; <xref ref-type="bibr" rid="c33">Josephs, Turner, &amp; Friston, 1997</xref>). The echo-time was 33 ms and the flip angle 90 degrees. In the fMRI run 130 volumes were collected and lasted 6.5 minutes.</p>
<p>Finally, to comply with Human Connectome Project minimal preprocessing pipeline requirements (<xref ref-type="bibr" rid="c19">Glasser et al., 2013</xref>), both T1w and T2w whole-brain structural images were acquired. T1w images were obtained using the 3D FSPGR with ASSET sequence: TI/TR/TE: 650/8.8/1.8 ms, flip angle = 10 degrees, 196 slices, FOV 256 x 256 mm, slice thickness 1 mm, matrix 256 x 256 resulting in 1 mm isotropic voxel sizes. T2w images were acquired using a spin-echo sequence: TR/TE: 15000/106 ms, NEX 2, prescribing 110 sagittal slices, FOV 256 x 256 mm, slice thickness 1.5 mm, matrix 256 × 512 (with 2x parallel imaging acceleration) resulting in 0.5 x 0.5 x 1.5 mm voxels.</p>
</sec>
<sec id="s2a3">
<title>Preprocessing</title>
<p>Preprocessing of structural and functional data relied on the Human Connectome Project (HCP) minimal preprocessing pipeline (v4.7.0; <xref ref-type="bibr" rid="c19">Glasser et al., 2013</xref>). For the preprocessing of the T1 and T2w images we used the pre-freesurfer and freesurfer batch scripts with options that were applicable to our data (i.e., no readout distortion correction, 1 mm MNI HCP structural templates, no fieldmap correction). Freesurfer v6.0 was installed on our system (<xref ref-type="bibr" rid="c18">Fischl, 2012</xref>). This step therefore produced cleaned T1 and T2w images that were then fully processed using the Freesurfer v6.0 program. From the freesurfer output we were particularly interested in the volumetric cortical and subcortical segmentation produced in the aparc+aseg file. Note this file was produced in native T1w space by the pipeline.</p>
<p>For the preprocessing of the fMRI data we used the generic fMRI volume batch script from the HCP preprocessing pipeline. All registrations (EPI to T1w and T1w to MNI) were manually checked for inconsistencies (none were detected). Next, the functional data were temporally filtered at 2000 s and manually cleaned using Independent Component Analysis (<xref ref-type="bibr" rid="c22">Griffanti et al., 2014</xref>). We obtained Independent Components for all fMRI datasets using FSL Melodic v3.15 with default settings (i.e., automatic estimation of the number of components; <xref ref-type="bibr" rid="c6">Beckmann &amp; Smith, 2004</xref>). An experienced rater of Independent Components (NJ) then hand-classified all components as signal or noise for all participants using in-house tools (<ext-link ext-link-type="uri" xlink:href="https://github.com/iamnielsjanssen/display_melodics">https://github.com/iamnielsjanssen/display_melodics</ext-link>). Components identified as noise were then regressed out of the data using the FSL regfilt tool. In the end we obtained the cleaned fMRI datasets for each participant in native fMRI space that were bias corrected, motion corrected, brain-extracted, intensity corrected, temporally filtered and ICA-cleaned. Quality control of the structural data was performed by computing Euler’s number from the freesurfer output (<xref ref-type="bibr" rid="c62">Rosen et al., 2018</xref>), and by computing for all functional data a composite score of head motion displacements from the six motion regressors (<xref ref-type="bibr" rid="c31">Jenkinson, Bannister, Brady, &amp; Smith, 2002</xref>, see <xref rid="fig1" ref-type="fig">Figure 1A</xref> and <xref rid="fig1" ref-type="fig">B</xref> for a graphical presentation).</p>
</sec>
</sec>
<sec id="s2b">
<title>Analyses</title>
<sec id="s2b1">
<title>fMRI signal extraction</title>
<p>The first step in the analyses consisted of extracting the event-related fMRI signal using slice-based fMRI (see <xref rid="fig2" ref-type="fig">Figure 2A</xref> and <xref rid="fig2" ref-type="fig">B</xref> for a graphical explanation; <xref ref-type="bibr" rid="c29">Janssen et al., 2018</xref>). The slice-based fMRI technique offers a significant advancement over traditional fMRI analysis methods by providing improved temporal precision and flexibility in extracting the fMRI signal. FMRI data is typically acquired by sequentially sampling different sections of the brain (slices) over a single repetition time (TR). In standard fMRI analyses, whole-brain volumes are constructed from all slices acquired within a single TR, which is well-known to introduce temporal distortions (<xref ref-type="bibr" rid="c57">Parker, Liu, &amp; Razlighi, 2017</xref>; <xref ref-type="bibr" rid="c66">Sladky et al., 2011</xref>). In contrast, the slice-based technique avoids these temporal distortions by creating whole-brain volumes that are composed out of slices that are acquired at the same moment in time <italic>relative to a presented stimulus</italic>. A further advantage of this method is that because slices are acquired relatively rapidly, under optimal conditions, whole-brain volumes can be constructed with a temporal resolution that is equal to the TR divided by the number of prescribed slices (typically on the order of tens of milliseconds). This temporal resolution can then be flexibly adjusted by “binning” across time-points, which effectively reduces the temporal resolution while enhancing the signal-to-noise ratio (SNR). The ability to flexibly adjust the temporal resolution makes the slice-based technique especially valuable for studying the dynamics of brain activity underlying complex behavior.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Slice-based fMRI explanation (A,B), results (C) and subsequent processing steps (D).</title>
<p>In a hypothetical fMRI data acquisition protocol (A), three slices are acquired with TR=2s resulting in Slice-Acquisition Times (SAT) of 0, 0.7, 1.3s etc. The key point is that stimulus presentations are combined to extract the fMRI signal at high temporal resolution (<xref ref-type="bibr" rid="c29">Janssen et al., 2018</xref>). Three stimulus trials (T1.RT, T2.RT, T3.RT and red, yellow, green bars) whose onset coincides with each slice results in slices sampling the brain at all timepoints during a 6s epoch <italic>relative</italic> to the onset of the stimulus. Thus, for a specific voxel on slice 2 (B), after these three stimulus trials, signal intensities at a temporal resolution equal to the ΔSAT (0.7s) are acquired. Statistical models can extract the fMRI signal (B, grey dashed line) at this or at a higher powered binned, but lower temporal resolution. Results from group-level Slice-Based analysis of movie clips at 2s resolution for the separate happy, fear and sad trials (not all timepoints shown). Note results reveal complex increases (hot colors) and decreases (cool colors) of signals across the whole brain during the 26s epoch (C). The slice-based datasets for the happy, fear and sad trials for each subject were then concenated and entered into group spatial ICA for the detection of component maps and timecourses (D).</p></caption>
<graphic xlink:href="636581v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the current study we used the slice-based technique to extract the fMRI signal during a 28 s epoch separately for the happy, sad and fear trials at 2 s temporal resolution. Note we chose an epoch duration that was slightly longer than the task to accommodate for some uncertainty in the offset of fMRI activities. This meant that for each voxel, the fMRI signal was extracted by comparing fMRI signal intensities collected during an interval of 3 seconds prior to stimulus onset to fMRI signal intensities at all subsequent timepoints in the 28 s epoch using a simple linear model with the variable Time as a factor (see <xref rid="fig2" ref-type="fig">Figure 2B</xref> for a graphical explanation). This therefore produced for every participant three 4D fMRI files that represented the event-related changes for the happy, sad and fear trials. Each file contained 14 volumes, one for every 2 seconds. Note all these 4D fMRI files were in native fMRI space.</p>
</sec>
<sec id="s2b2">
<title>Group spatial-ICA</title>
<p>The goal of this step was to discover the spatial and temporal fMRI patterns that arise during the behavioral task. To this end, the set of 66 4D slice-based fMRI files (22 participants with 3 experimental conditions each) were transformed to 2 mm MNI space using non-linear warping and entered into group spatial Independent Component Analysis (ICA; see <xref rid="fig2" ref-type="fig">Figure 2D</xref> for a graphical overview). Here we used FSL Melodic (v3.15) with default settings, except that we disabled data reduction with MIGP (i.e., we used the full concatenation). To determine the optimal number of dimensions, Melodic uses a Bayesian approach, which involves evaluating how well different models (with varying numbers of dimensions) explain the observed data while accounting for model complexity (<xref ref-type="bibr" rid="c49">Minka, 2000</xref>). For our data the estimated optimal number of dimensions was 6. As we will show below, from these 6 Independent Components (ICs; herein also referred to as IC networks or simply brain networks), 4 ICs were identified as signal whereas 2 ICs were identified as noise. To make such classifications we relied on relatively straightforward protocols explained elsewhere (<xref ref-type="bibr" rid="c22">Griffanti et al., 2014</xref>).</p>
<p>Importantly, spatial ICA not only outputs spatial information in the form of statistical maps but also produces an estimated timecourse that is associated with each spatial map. More formally, ICA decomposes the fMRI data matrix <italic>X</italic> (with dimensions corresponding to time points and voxels) into a product of two matrices: <italic>X</italic> = <italic>W <sup>−</sup></italic><sup>1</sup><italic>M</italic>, where <italic>M</italic> represents the spatially independent components (spatial maps) and <italic>W <sup>−</sup></italic><sup>1</sup> represents the associated time courses of these components (mixing matrix). The rows of <italic>M</italic> correspond to spatial patterns, which are maximally independent, while the columns of <italic>W <sup>−</sup></italic><sup>1</sup> contain the time series for each component. These time series describe the temporal dynamics of each spatial component. Together, the spatial and temporal components offer a complete representation of the independent processes that contribute to the observed fMRI data. In the case of FSL Melodic, the resulting melodic_mix file contains the matrix <italic>W <sup>−</sup></italic><sup>1</sup>, where each column corresponds to the temporal activation pattern of an independent component. This temporal information allows us to examine how each spatial network’s activity fluctuates over time, providing critical insights into task-evoked brain activity. In the current study this temporal information is utilized in two ways: it is presented visually alongside the spatial map as the average across all subjects and experimental conditions (<xref ref-type="bibr" rid="c14">Duann et al., 2002</xref>; <xref ref-type="bibr" rid="c30">Janssen &amp; Mendieta, 2020</xref>), and it is used analytically to perform Gaussian curve-fitting analyses (explained below).</p>
</sec>
<sec id="s2b3">
<title>Dual Regression</title>
<p>To establish the reliability of the activity of brain regions involved in these four IC networks, we performed back-projection of the group spatial ICA map using Dual Regression in native space. We then extracted regional activities for each IC network using the Desikan-Killany atlas for each individual participant (<xref ref-type="bibr" rid="c11">Desikan et al., 2006</xref>, see <xref rid="fig2" ref-type="fig">Figure 2D</xref> for a graphical overview). Reliability of these regional activities was estimated using linear mixed-effect regression modeling using the following model formula:
<disp-formula id="eqn1">
<graphic xlink:href="636581v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>Euler</italic>_<italic>number</italic> represented the quality control estimate for a participant’s structural data, <italic>motion</italic>_<italic>score</italic> the quality control estimate for a participant’s functional data and <italic>Hemisphere</italic> the functional connectivity for each hemisphere (left vs right). Although the model includes all interactions, we were particularly interested in the interactions <italic>Region</italic> ∗ <italic>IC</italic>_<italic>network</italic> which tested the degree to which regional functional connectivity was the same across discovered networks, and <italic>Region</italic> ∗ <italic>Condition</italic> ∗ <italic>IC</italic>_<italic>network</italic>, which examined if the aforementioned interaction depended on the emotion condition. Finally, we included a random intercept for participant to account for potential between-participant variability.</p>
<p>This modeling procedure therefore allowed us to establish whether a given region within a given IC network was reliably activated across participants. In other words, this would allow us to obtain a list of both cortical and subcortical brain regions that are reliably activated within each IC network. Given that the Desikan-Killany atlas is fitted by Freesurfer to the native morphology of an individual’s brain, we improved spatial precision by performing these analyses in native fMRI space.</p>
</sec>
<sec id="s2b4">
<title>Curve-fitting</title>
<p>The final step involved quantifying the temporal activation dynamics of the independent components (ICs). As mentioned earlier, Melodic outputs the temporal information associated with each identified IC network. Since the input to the group spatial ICA was the 4D slice-based fMRI data for each participant and experimental condition, the resulting time course information was available for each IC network, participant, and condition. These time courses were then consolidated into a single dataset with the variables Participant, IC Network, Emotion Condition, and Time, facilitating further analysis of temporal activation patterns across experimental conditions. We then performed nonlinear regression (<xref ref-type="bibr" rid="c30">Janssen &amp; Mendieta, 2020</xref>; <xref ref-type="bibr" rid="c36">Kruggel &amp; von Cramon, 1999</xref>; <xref ref-type="bibr" rid="c37">Kruggel et al., 2000</xref>) for all combination of variables using a Gaussian function of the form:
<disp-formula id="eqn2">
<graphic xlink:href="636581v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>A</italic> represents the amplitude, <italic>µ</italic> the mean and <italic>σ</italic> the standard deviation. As can be seen in <xref rid="fig7" ref-type="fig">Figure 7A</xref>, these parameters can be used to compute the peak value (<italic>A</italic>), the time to peak (<italic>µ</italic>) and duration (the FWHM equal to 2.355 ∗ <italic>σ</italic>) of a fitted Gaussian function. Goodness of Fit (adjusted <italic>R</italic><sup>2</sup>) values can also be computed as a difference between observed and fitted datapoints. Gaussian fitting was performed in R using the package minpack.lm v1.2 (<xref ref-type="bibr" rid="c15">Elzhov, Mullen, Spiess, &amp; Bolker, 2022</xref>), which fits a function to a set of datapoints using Levenberg-Marquardt nonlinear least squares algorithm. Initial values were estimated from the data.</p>
<p>These curve-fitting analyses provided amplitude, time to peak and duration estimates for each participant, Emotion Condition and IC network. Finally, statistical analyses were used to test whether peak amplitude, time to peak, and duration varied as a function of IC network and Emotion Condition using a formula of the form:
<disp-formula id="eqn3">
<graphic xlink:href="636581v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>G</italic> referred to the peak amplitude, time to peak, or duration. <italic>R</italic>2 (a Goodness of Fit value) was included in the analyses as a control variable. Here we were particularly interested in the main effect of <italic>IC</italic>_<italic>network</italic> which would reveal whether peak value, time to peak and duration values were equal for the four IC networks. In addition, we were interested in the <italic>Emotion</italic>_<italic>Condition</italic> ∗ <italic>IC</italic>_<italic>network</italic> interaction, which would reveal whether peak amplitude, time to peak or duration estimates would be equal between the three emotion conditions (happy, sad, fear) across all IC networks.</p>
<p>All statistical modeling reported in the current study was performed in R v4.1.2 using the packages lme4 v1.1, lmerTest v3.1, and emmeans v1.8.4 (<xref ref-type="bibr" rid="c3">Bates, Mächler, Bolker, &amp; Walker, 2015</xref>; Kuznetsova, Brockhoff, Christensen, et al., 2017; <xref ref-type="bibr" rid="c41">Lenth, Singmann, Love, Buerkner, &amp; Herve, 2018</xref>). Note that to reduce complexity we controlled rather than explored the effects of Hemisphere and that we modeled random participant variability using a random intercept. All post-hoc tests were performed with emmeans and corrected for multiple comparisons using the Bonferroni correction.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>Analysis of the behavioral data showed that participants did not treat the three emotion categories in the same way. Specifically, as can be seen in <xref rid="fig1" ref-type="fig">Figure 1C</xref>, there was higher agreement among participants on the target response in the sad vs the other two emotion categories (Sad vs Happy <italic>p &lt;</italic> 0.02; Sad vs Fear <italic>p &lt;</italic> 0.04). In addition, as can be seen in <xref rid="fig1" ref-type="fig">Figure 1D</xref>, decisions for the sad condition were faster compared to the other two conditions (Sad vs Happy <italic>p &lt;</italic> 0.003; Sad vs Fear <italic>p &lt;</italic> 0.008).</p>
<p>Group spatial ICA of the 4D slice-based fMRI datasets first revealed that the estimated optimal model selection occurred with 6 dimensions. As can be seen in <xref rid="fig3" ref-type="fig">Figure 3</xref>, four of these networks corresponded to signal, two were considered noise. <xref rid="fig3" ref-type="fig">Figure 3</xref> shows both spatial distributions and mean temporal profiles associated with each IC network. Visual inspection of these temporal profiles enables an association with the different stages of the behavioral task. Specifically, IC0 and IC1 show an activation pattern consistent with the movie watching task where there was an immediate onset, a sustained activity and offset consistent with the presentation of the 12.5 s movie clips. Similarly, IC2 shows a pattern associated with the emotion decision stage with a peak in the decision-stage of the task. In addition, the spatial distribution of co-activities of IC4 are consistent with the default mode network (dmn). For expository reasons we labeled the IC networks in terms of a tentative functional interpretation. Specifically, for reasons we discuss in more detail below, we labeled IC1 with “input”, IC0 with “meaning”, IC2 with “response” and IC4 with “dmn”. We also ordered these IC networks on the basis of their assumed temporal order of activation (i.e., IC1→IC0→IC4→IC2. Finally, IC3 and IC5 were classified as noise due to the presence of “activation” in non gray-matter areas of the brain (CSF, veins; <xref ref-type="bibr" rid="c22">Griffanti et al., 2014</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Results from group spatial ICA with 6 dimensions.</title>
<p>Plotted are both the spatial maps as well as the component time courses averaged across subjects and conditions associated with each IC. Consideration of both spatial and temporal properties of these 6 ICs led to the identification of four ICs as signal (IC0, IC1, IC2, IC4) and two as noise (IC3, IC5). Note the noise components had spatial distributions associated with non-gray-matter regions (CSF, draining veins). Note also the networks are labeled and ordered as explained in the text. Bottom bar denotes the different stages of the behavioral task. The amplitude of the component time courses reflects the strength of engagement of each independent component over time, in z-scored units. Modulation of these timecourses by emotions is investigated further below.</p></caption>
<graphic xlink:href="636581v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Further investigation of the spatial maps using dual regression and statistical modeling revealed insight into the specific regions that were functionally co-activated within each IC network. Specifically, modeling of the functional connectivity data with <xref rid="eqn1" ref-type="disp-formula">equation 1</xref> revealed a significant interaction between <italic>IC</italic>_<italic>network</italic> and <italic>Region</italic> (p <italic>&lt;</italic> 0.0001; see <xref rid="tbl2" ref-type="table">Table 2</xref> for all stats), suggesting that the pattern of regional functional connectivity was not the same across the four IC networks. Post-hoc analyses with Bonferroni correction revealed patterns of regional functional connectivity that were unique to each IC network. These patterns are visualized in <xref rid="fig4" ref-type="fig">Figures 4</xref> and <xref rid="fig5" ref-type="fig">5</xref>. Whereas both IC0 (“meaning”) and IC1 (“input”) strongly relied on visual areas and temporal areas (lateral occipital, pericalcarine gyrus), IC1 (“input”) differed from IC0 (“meaning”) in its co-activity in the superior temporal gyrus, amygdala, and medial posterior cingulate regions. The IC2 (“response”) network had strong activity in the primary motor regions as well as the anterior cingulate. Finally, the IC4 (“dmn”) network showed strong activity in typical default mode network regions.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2</label>
<caption><title>Overview of the ANOVA table from the statistical modeling of the regional functional connectivity data.</title></caption>
<graphic xlink:href="636581v2_tbl2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Basic regional functional connectivity in the four large scale networks.</title>
<p>Note scale differences between networks. Black bars denote top 10 regions with highest functional connectivity within each network. All results are bonferroni corrected.</p></caption>
<graphic xlink:href="636581v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Differences in functional connectivity between IC networks.</title>
<p>Note here we focus on three main contrasts to highlight differences between IC0 and IC1, IC0 and IC2 and between IC2 and IC4.</p></caption>
<graphic xlink:href="636581v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Statistical modeling with <xref rid="eqn1" ref-type="disp-formula">equation 1</xref> also revealed a triple interaction between <italic>IC</italic>_<italic>network</italic>, <italic>Region</italic> and <italic>Emotion</italic>_<italic>Condition</italic> (p <italic>&lt;</italic> 0.0001; see <xref rid="tbl2" ref-type="table">Table 2</xref> for all stats), suggesting that the interaction between <italic>IC</italic>_<italic>network</italic> and <italic>Region</italic> outlined above was further modulated by the Emotion Condition. Post-hoc analyses of this three-way interaction revealed different effects of Emotion Condition for different regions within each IC network. As can be seen in <xref rid="fig6" ref-type="fig">Figure 6</xref>, discrete emotions modulated regions across the whole brain within all four IC networks. Specifically, discrete emotion modulated functional connectivity in visual, temporal, frontal, parietal and subcortical regions for all four networks.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Functional connectivity modulation of emotion conditions across networks.</title>
<p>Note all large scale networks involved in the task are modulated by emotions in all major lobes of the brain. H = Happy, F = Fear, S = Sadness. All results are bonferroni corrected.</p></caption>
<graphic xlink:href="636581v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Curve-fitting of a Gaussian function to the temporal profiles associated with the four IC networks revealed further insight into the temporal properties of the four networks. First, the results revealed relatively high R2 values suggesting that goodness of fit was adequate across the four IC networks (see <xref rid="fig7" ref-type="fig">Figure 7B</xref>). In addition, statistical modeling with <xref rid="eqn3" ref-type="disp-formula">equation 3</xref> revealed a main effect of <italic>IC</italic>_<italic>network</italic> for all three dependent variables suggesting that peak values, time to peak and durations were not equal for the four IC networks (see <xref rid="tbl3" ref-type="table">Table 3</xref> for details). Post-hoc analyses of <italic>IC</italic>_<italic>network</italic> for each dependent variable revealed that peak values differed between the four networks, where time to peak values were earlier for IC1 (“input”) than for IC0 (“meaning”), and durations were generally shorter for the networks associated in the later aspects of the task (see <xref rid="fig7" ref-type="fig">Figure 7C</xref>, <xref rid="fig7" ref-type="fig">D</xref> and <xref rid="fig7" ref-type="fig">E</xref> for further details).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Explanation of curvefitting procedure (A), goodness of fit results from Gaussian curvefitting (B), and graphical overview of how peak value (C), time to peak (D) and duration (E) differ between the four IC networks.</title>
<p>Note that goodness of fit values were relatively equal across IC networks, and that there were earlier time to peak values for IC1 compared to IC0. Different letters above bars indicate a significant difference (p &gt; 0.05, bonferroni corrected).</p></caption>
<graphic xlink:href="636581v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Finally, statistical modeling with <xref rid="eqn3" ref-type="disp-formula">equation 3</xref> also revealed an interaction between <italic>IC</italic>_<italic>network</italic> and <italic>Emotion</italic>_<italic>Condition</italic> for all three dependent variables, suggesting that peak value, time to peak and duration values for the happy, sad and fear conditions were not equal across the four IC networks (see <xref rid="tbl3" ref-type="table">Table 3</xref> for details). Post-hoc analyses revealed that the Emotion Condition modulated peak value and duration for IC1 (“input”), peak value and time to peak for IC0 (“meaning”), time to peak for IC2 (“response”) and duration for IC4 (“dmn”; see <xref rid="fig8" ref-type="fig">Figure 8A-C</xref> for additional details).</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3</label>
<caption><title>Overview of the ANOVA table from the statistical modeling of the curvefitting data.</title></caption>
<graphic xlink:href="636581v2_tbl3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Results of the pairwise contrasts of the Happy (H), Sadness (S) and Fear (F) conditions for the peak value (pv), time to peak (ttp) and duration (dur) across all four IC networks.</title>
<p>Only significant (<italic>p</italic> 0.05, bonferroni corrected) values are plotted.</p></caption>
<graphic xlink:href="636581v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In the current study we relied on time-resolved fMRI to discover the large scale networks associated with a behavioral task that consisted of three stages: watching a movie clip (12.5s), making a decision about the emotion experience by the protagonist in the clip (2.5s) and an intertrial rest period (10s). Group spatial ICA of slice-based fMRI data with increased temporal resolution across the entire trial period (28s) revealed four large scale brain networks that were associated with different aspects of the task. Two networks (IC0 and IC1) were associated with the movie stimulus, while a third network (IC2) activated during the later decision stage, and a fourth network (IC4) was activated throughout the task and showed a peak during the intertrial rest period. Further quantification of the spatial and temporal properties of these networks using dual regression and Gaussian curvefitting showed that the emotional content of the clip modulated the spatial distribution as well as the peak value, time to peak and duration properties of each network.</p>
<p>Brain networks IC0 and IC1 appear to play complementary roles during the movie-watching phase. IC1, which we labeled the “input” network, aligns with aspects of Networks 1 and 2 of <xref ref-type="bibr" rid="c61">Riedel et al. (2018)</xref>, involving primary visual and auditory processing regions critical for perceiving the movie stimulus (see <xref rid="fig4" ref-type="fig">Figure 4</xref>). However, IC1 (“input”) also showed a resemblance to Network 5 of <xref ref-type="bibr" rid="c61">Riedel et al. (2018)</xref> with activity in the amygdala, fusiform gyrus and parahippocampal gyrus that was linked to the evaluation of the emotional content of the stimulus. Note that no other network detected in our data consisted of co-activity in these three regions. In addition, time to peak values were earlier in the IC1 (“input”) network than in all other networks (see <xref rid="fig7" ref-type="fig">Figure 7D</xref>). The combined spatial and temporal properties of this network therefore suggests that this network is likely responsible for processing the visual and auditory input stream as well as the evaluation of the emotional content of the stimulus.</p>
<p>On the other hand, although also associated with the movie stimulus, the IC0 (“meaning”) network appeared to have spatial and temporal properties that were distinct from the IC1 (“input”) network. Specifically, IC0 (“meaning”) had increased co-activity in lateral frontal and middle temporal areas that are typically associated with semantic processing (see <xref rid="fig5" ref-type="fig">Figure 5</xref>; <xref ref-type="bibr" rid="c27">Huth et al., 2016</xref>; <xref ref-type="bibr" rid="c70">Visser et al., 2012</xref>). In addition, IC0 (“meaning”) aligned with Network 3 of <xref ref-type="bibr" rid="c61">Riedel et al. (2018)</xref> and appeared to contain regions associated with the salience network such as the insula and the anterior cingulate gyrus (<xref ref-type="bibr" rid="c65">Seeley, 2019</xref>), suggesting an attentional component. Importantly, even though the IC0 (“meaning”) network activated in response to the movie stimulus, its time to peak values lagged about 2s behind those of IC1 (“input”; see <xref rid="fig7" ref-type="fig">Figure 7</xref>). As was pointed out in the Introduction, the movie clips showed scenes whose intended meaning could not be discerned immediately, but instead required additional processing that lagged behind the input stream. As before, the combined spatial and temporal characteristics of this network allow us to interpret this network as involved in the processing of the meaning associated with the input stream.</p>
<p>In addition, a third network (IC2, “response”) showed temporal activity with a peak around 20s and can therefore be associated with the response stage (see <xref rid="fig3" ref-type="fig">Figure 3</xref>). The spatial properties of this network showed activity in cortical and subcortical regions associated with processing the visual input (the discrete emotion words), decision making associated regions like the caudate nucleus (<xref ref-type="bibr" rid="c21">Grahn, Parkinson, &amp; Owen, 2008</xref>), motor generation regions like the precentral gyrus, and task general attention related regions like the insula and the anterior cingulate cortex (<xref ref-type="bibr" rid="c65">Seeley, 2019</xref>). This therefore suggests that this network represents the combined emotion decision and motor output network. Note that with increased temporal resolution (and potentially restricting behavioral variance in response latency) one might be able to separate the decision and the motor output components of this network. Finally, the fourth network (IC4, “dmn”) showed an alignment with Network 4 of <xref ref-type="bibr" rid="c61">Riedel et al. (2018)</xref> and had spatial properties consistent with those of the default mode network (<xref ref-type="bibr" rid="c8">Buckner, Andrews-Hanna, &amp; Schacter, 2008</xref>). Although the exact role of the default mode network in emotion processing is currently debated (<xref ref-type="bibr" rid="c64">Satpute &amp; Lindquist, 2019</xref>), it is noteworthy that the network showed some co-activity throughout the task and had a peak during the rest period (see <xref rid="fig3" ref-type="fig">Figure 3</xref>), and showed activity in lateral prefrontal regions (see <xref rid="fig4" ref-type="fig">Figure 4</xref>). These spatial and temporal properties are consistent with an interpretation in terms of the regulation, mind-wandering and/or self-referential processing of the emotions (<xref ref-type="bibr" rid="c24">Gross, 2015</xref>).</p>
<p>Both the spatial and temporal properties of the four identified networks were influenced by the emotional content of the movie clips. Specifically, in IC1 (“input”), regions in the visual, parietal, and frontal lobes showed differential activation patterns across happy, fear, and sad clips. Temporal dynamics, including the duration and peak values of the responses, also varied between these emotional conditions (<xref rid="fig6" ref-type="fig">Figure 6A</xref> and <xref rid="fig8" ref-type="fig">8A</xref>). Similar modulations were observed for IC0 (“meaning”), IC2 (“response”), and IC4 (“dmn”), where emotional content influenced activations in visual, temporal, and frontal regions, as well as temporal metrics such as response duration, time to peak, and peak amplitude. Notably, the time-to-peak differences in IC2 (“response”) align closely with response latency results, with sad trials showing faster response latencies and earlier peak times (compare <xref rid="fig1" ref-type="fig">Figures 1D</xref> and <xref rid="fig8" ref-type="fig">8D</xref>). Critically, the same ordering between response latencies and the fMRI time-to-peak metric provides convergent temporal validity for using BOLD dynamics as a window into the speed of affective computations. Nevertheless, these results should not be taken as evidence that processing sad emotions is generally faster than other emotions; Other studies have demonstrated fast processing of fearful stimuli (<xref ref-type="bibr" rid="c7">Bo et al., 2022</xref>; <xref ref-type="bibr" rid="c23">Grootswagers, Kennedy, Most, &amp; Carlson, 2020</xref>; <xref ref-type="bibr" rid="c47">Méndez-Bértolo et al., 2016</xref>), and specific aspects of the current experimental paradigm likely influenced the speed of emotional processing. In general, the current findings suggest that emotional stimuli elicit widespread effects on brain activity, impacting not only the processing of emotion-laden input stimuli but also the spatial and temporal dynamics of broader neural responses.</p>
<p>Taken together, the results of this study paint a comprehensive picture of how the brain orchestrates spatially and temporally distinct networks to process emotional stimuli and execute task-relevant behaviors. The findings highlight a dynamic interplay between sensory processing, semantic interpretation, decision-making, and emotional regulation, all of which unfold within a distributed but highly coordinated network architecture. Although the networks demonstrated separable time courses, their temporal dynamics were largely overlapping, suggesting that processes such as those mediated by IC1 (“input”) and IC0 (“meaning”) likely operate in parallel rather than in isolation. This parallel engagement implies that cross-talk between networks is a critical feature of task-related brain activity (<xref ref-type="bibr" rid="c10">Cole et al., 2013</xref>), particularly during complex cognitive and emotional tasks. For example, the interplay between sensory-driven processing in IC1 and the semantic and attentional mechanisms in IC0 may be necessary for the integrated perception of emotionally salient stimuli. How such cross-talk is mediated remains an open question, representing an intriguing avenue for future research. In the decision phase, IC2 (“response”) reveals how emotion processing transitions from perception to cognitive evaluation, involving a convergence of sensory, attentional, and motor networks to guide task execution. Finally, IC4, (“dmn”) associated with the default mode network, demonstrates the presence of reflective, mind-wandering and/or regulatory processes (<xref ref-type="bibr" rid="c24">Gross, 2015</xref>), particularly during rest but with influence throughout the task (see <xref rid="fig3" ref-type="fig">Figure 3</xref>). These findings underscore the dynamic nature of brain activity, where emotional stimuli engage overlapping networks that continuously integrate sensory input, semantic processing, decision-making, and regulation. This fluid coordination reveals how emotions are adaptively represented and processed across time and space (<xref ref-type="bibr" rid="c32">John et al., 2022</xref>; <xref ref-type="bibr" rid="c59">Pessoa &amp; McMenamin, 2017</xref>; <xref ref-type="bibr" rid="c72">Waugh et al., 2015</xref>).</p>
<p>Our study has several limitations that warrant discussion. First, the interpretation of BOLD signal dynamics is inherently influenced by the vascular properties of the underlying tissue (<xref ref-type="bibr" rid="c54">Ogawa et al., 1992</xref>). This limitation has been discussed primarily in the context of studies that have attempted to determine the temporal sequence of activation of different brain regions where such differences could be confounded by variations in vascular responsiveness (<xref ref-type="bibr" rid="c2">Aguirre, Zarahn, &amp; D’Esposito, 1998</xref>; <xref ref-type="bibr" rid="c34">Kim, Richter, &amp; Ugurbil, 1997</xref>; <xref ref-type="bibr" rid="c48">Menon, Luknowsky, &amp; Gati, 1998</xref>). However, it is important to note that the temporal properties of the signal we analyzed do not represent those of individual brain regions but the aggregate activity of many regions working in concert, reducing the likelihood of significant confounding effects. A further limitation is that we did not require participants to directly regulate their emotions which complicates an interpretation in terms of emotion regulation (<xref ref-type="bibr" rid="c24">Gross, 2015</xref>). Likewise, the current research design only used three main emotional categories. Whether our conclusions hold for other emotions and emotion types remains to be studied in future studies. Another general limitation of the results presented here concerns the influence of the number of dimensions chosen in group spatial ICA on the spatial distribution of the observed large-scale networks. It is well-documented that increasing the number of dimensions can lead to the “splitting” of networks into finer components, while fewer dimensions may overgeneralize distinct functional patterns (<xref ref-type="bibr" rid="c4">Beckmann, 2012</xref>). This presents a challenge, as there is no a priori standard for the “correct” spatial distribution of task-related networks, especially for complex tasks like watching movie clips (<xref ref-type="bibr" rid="c12">Di, Gohel, Kim, &amp; Biswal, 2013</xref>; <xref ref-type="bibr" rid="c13">Dosenbach, Raichle, &amp; Gordon, 2025</xref>; <xref ref-type="bibr" rid="c25">Huang, De Brigard, Cabeza, &amp; Davis, 2024</xref>). In this sense, task-based fMRI studies of large-scale networks are currently in a similar position to early resting-state fMRI research, where the precise spatial distributions of networks were still being defined (e.g., <xref ref-type="bibr" rid="c76">Yeo et al., 2011</xref>). To address this, a concerted effort is needed to map the large-scale networks underlying diverse tasks, enabling more informed choices about dimensionality and mitigating the uncertainty in network characterization.</p>
<p>Finally, it is worth emphasizing that our approach bears a relationship to what is often termed “dynamic functional connectivity” (<xref ref-type="bibr" rid="c20">Gonzalez-Castillo &amp; Bandettini, 2018</xref>; <xref ref-type="bibr" rid="c26">Hutchison et al., 2013</xref>). In particular, our results demonstrate how whole-brain functional connectivity maps shift over time in response to different components of a complex, emotion-based behavioral task (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Although both event-related fMRI and group spatial ICA have been employed extensively in prior studies, combining them as in the current study is less common (<xref ref-type="bibr" rid="c30">Janssen &amp; Mendieta, 2020</xref>; <xref ref-type="bibr" rid="c45">Long et al., 2013</xref>). The particular slice-based event-related approach has been shown to offer enhanced temporal precision and greater flexibility in choosing different temporal resolutions than more traditional signal extraction schemes (<xref ref-type="bibr" rid="c29">Janssen et al., 2018</xref>). Indeed, when we analyzed our data at lower temporal resolution (i.e., at the 3 s TR level), some components with insufficient temporal separation (such as IC1 and IC0) became conflated, highlighting the importance of higher-resolution signal extraction in distinguishing these networks (<xref ref-type="bibr" rid="c9">Calhoun, Adali, Pearlson, &amp; Pekar, 2001</xref>). Moreover, we used standard group spatial ICA methods, which have been widely applied to resting-state data, to derive task-based whole-brain functional connectivity maps. This unified approach for finding networks in task-based and resting-state data may facilitate comparisons between networks obtained from different modalities (<xref ref-type="bibr" rid="c20">Gonzalez-Castillo &amp; Bandettini, 2018</xref>). Taken together, these findings demonstrate the value of combining slice-based event-related fMRI with group spatial ICA to characterize how functional connectivity patterns evolve over time. Future research may further explore the interplay of the temporal resolution of the extracted signal, group spatial ICA, and alternative approaches, such as FIR analysis (<xref ref-type="bibr" rid="c33">Josephs et al., 1997</xref>; <xref ref-type="bibr" rid="c74">Windischberger et al., 2008</xref>), to illuminate the spatiotemporal characteristics of dynamic network changes during complex tasks.</p>
<p>To conclude, the present study addressed three key questions: whether distinct whole-brain networks underlie the different stages of our emotional movie watching and decision task, whether their spatial distributions vary with emotional content, and whether their temporal dynamics—including time-to-peak and duration—are sensitive to emotion. Our results revealed four large-scale networks whose activity patterns not only differentiated task stages but revealed a separation between a network related to processing affective input and understanding the meaning of this input. Importantly, the spatial and temporal features of all four networks varied as a function of the emotional content of the stimuli. These results move beyond static, region-based models and suggest emotion reflects dynamic, distributed interactions among multiple networks (K. A. <xref ref-type="bibr" rid="c42">Lindquist et al., 2012</xref>; <xref ref-type="bibr" rid="c58">Pessoa, 2017</xref>). In turn, our findings underscore the utility of examining temporal metrics to capture subtle nuances of emotional processing that may remain undetectable using standard static analyses. These temporal properties may be especially relevant for clinical studies in which certain emotional disorders arise due to disturbances in the temporal regulation of emotion (e.g., dwelling too long on certain thoughts; <xref ref-type="bibr" rid="c38">Kuppens &amp; Verduyn, 2017</xref>; <xref ref-type="bibr" rid="c72">Waugh et al., 2015</xref>). The current study’s approach for quantifying these temporal properties can therefore offer valuable insights, both for basic science and for the development of novel clinical tools.</p>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>All behavioral and raw MRI data as well as all code for the analyses are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/xy43a/">https://osf.io/xy43a/</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>The authors would like to thank the Servicio de Resonancia Magnética para Investigaciones Biomédicas (SRMIB - SEGAI) at the University of La Laguna for their help.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adolphs</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Recognizing emotion from facial expressions: psychological and neurological mechanisms</article-title>. <source>Behavioral and cognitive neuroscience reviews</source>, <volume>1</volume> (<issue>1</issue>), <fpage>21</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aguirre</surname>, <given-names>G. K.</given-names></string-name>, <string-name><surname>Zarahn</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>D’Esposito</surname>, <given-names>M</given-names></string-name></person-group>. (<year>1998</year>). <article-title>The variability of human, bold hemodynamic responses</article-title>. <source>Neuroimage</source>, <volume>8</volume> (<issue>4</issue>), <fpage>360</fpage>–<lpage>369</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bates</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mächler</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bolker</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Walker</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Fitting linear mixed-effects models using lme4</article-title>. <source>Journal of Statistical Software</source>, <volume>67</volume> (<issue>1</issue>), <fpage>1</fpage>–<lpage>48</lpage>. doi: <pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beckmann</surname>, <given-names>C. F</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Modelling with independent components</article-title>. <source>Neuroimage</source>, <volume>62</volume> (<issue>2</issue>), <fpage>891</fpage>–<lpage>901</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Mackay</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Filippini</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2009</year>). <article-title>Group comparison of resting-state fmri data using multi-subject ica and dual regression</article-title>. <source>Neuroimage</source>, <volume>47</volume> (<issue>Suppl 1</issue>), <fpage>S148</fpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S. M</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Probabilistic independent component analysis for functional magnetic resonance imaging</article-title>. <source>IEEE transactions on medical imaging</source>, <volume>23</volume> (<issue>2</issue>), <fpage>137</fpage>–<lpage>152</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bo</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cui</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name>, <etal>…</etal> <string-name><surname>Ding</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Decoding the temporal dynamics of affective scene processing</article-title>. <source>NeuroImage</source>, <volume>261</volume>, 119532.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buckner</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Andrews-Hanna</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Schacter</surname>, <given-names>D. L</given-names></string-name></person-group>. (<year>2008</year>). <article-title>The brain’s default network: anatomy, function, and relevance to disease</article-title>. <source>Annals of the new York Academy of Sciences</source>, <volume>1124</volume> (<issue>1</issue>), <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Adali</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Pearlson</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Pekar</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2001</year>). <article-title>Spatial and temporal independent component analysis of functional mri data containing a pair of task-related waveforms</article-title>. <source>Human brain mapping</source>, <volume>13</volume> (<issue>1</issue>), <fpage>43</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cole</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Repovs</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Anticevic</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Braver</surname>, <given-names>T. S</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title>. <source>Nature neuroscience</source>, <volume>16</volume> (<issue>9</issue>), <fpage>1348</fpage>–<lpage>1355</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desikan</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Ségonne</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Quinn</surname>, <given-names>B. T.</given-names></string-name>, <string-name><surname>Dickerson</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Blacker</surname>, <given-names>D.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2006</year>). <article-title>An automated labeling system for subdividing the human cerebral cortex on mri scans into gyral based regions of interest</article-title>. <source>Neuroimage</source>, <volume>31</volume> (<issue>3</issue>), <fpage>968</fpage>–<lpage>980</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Gohel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>E. H.</given-names></string-name>, &amp; <string-name><surname>Biswal</surname>, <given-names>B. B</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Task vs. rest—different network configurations between the coactivation and the resting-state brain networks</article-title>. <source>Frontiers in human neuroscience</source>, <volume>7</volume>, <fpage>493</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dosenbach</surname>, <given-names>N. U.</given-names></string-name>, <string-name><surname>Raichle</surname>, <given-names>M. E.</given-names></string-name>, &amp; <string-name><surname>Gordon</surname>, <given-names>E. M</given-names></string-name></person-group>. (<year>2025</year>). <article-title>The brain’s action-mode network</article-title>. <source>Nature Reviews Neuroscience</source>, <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duann</surname>, <given-names>J.-R.</given-names></string-name>, <string-name><surname>Jung</surname>, <given-names>T.-P.</given-names></string-name>, <string-name><surname>Kuo</surname>, <given-names>W.-J.</given-names></string-name>, <string-name><surname>Yeh</surname>, <given-names>T.-C.</given-names></string-name>, <string-name><surname>Makeig</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hsieh</surname>, <given-names>J.-C.</given-names></string-name>, &amp; <string-name><surname>Sejnowski</surname>, <given-names>T. J</given-names></string-name></person-group>. (<year>2002</year>, apr). <article-title>Single-trial variability in event-related BOLD signals</article-title>. <source>NeuroImage</source>, <volume>15</volume> (<issue>4</issue>), <fpage>823</fpage>–<lpage>835</lpage>. doi: 10.1006/nimg.2001.1049</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Elzhov</surname>, <given-names>T. V.</given-names></string-name>, <string-name><surname>Mullen</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Spiess</surname>, <given-names>A.-N.</given-names></string-name>, &amp; <string-name><surname>Bolker</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2022</year>). <article-title>minpack.lm: R interface to the levenberg-marquardt nonlinear least-squares algorithm</article-title>. <source>CRAN</source> <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=minpack.lm">https://CRAN.R-project.org/package=minpack.lm</ext-link></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Etkin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Büchel</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Gross</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The neural bases of emotion regulation</article-title>. <source>Nature reviews neuroscience</source>, <volume>16</volume> (<issue>11</issue>), <fpage>693</fpage>–<lpage>700</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Movie-watching outperforms rest for functional connectivity-based prediction of behavior</article-title>. <source>NeuroImage</source>, <volume>235</volume>, 117963.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischl</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Freesurfer</article-title>. <source>Neuroimage</source>, <volume>62</volume> (<issue>2</issue>), <fpage>774</fpage>–<lpage>781</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glasser</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Sotiropoulos</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Coalson</surname>, <given-names>T. S.</given-names></string-name>, <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>J. L.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2013</year>). <article-title>The minimal preprocessing pipelines for the human connectome project</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>105</fpage>–<lpage>124</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Task-based dynamic functional connectivity: Recent findings and open questions</article-title>. <source>Neuroimage</source>, <volume>180</volume>, <fpage>526</fpage>–<lpage>533</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grahn</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Parkinson</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Owen</surname>, <given-names>A. M</given-names></string-name></person-group>. (<year>2008</year>). <article-title>The cognitive functions of the caudate nucleus</article-title>. <source>Progress in neurobiology</source>, <volume>86</volume> (<issue>3</issue>), <fpage>141</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Griffanti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Salimi-Khorshidi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Auerbach</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Douaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sexton</surname>, <given-names>C. E.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2014</year>). <article-title>Ica-based artefact removal and accelerated fmri acquisition for improved resting state network imaging</article-title>. <source>Neuroimage</source>, <volume>95</volume>, <fpage>232</fpage>–<lpage>247</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>B. L.</given-names></string-name>, <string-name><surname>Most</surname>, <given-names>S. B.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Neural signatures of dynamic emotion constructs in the human brain</article-title>. <source>Neuropsychologia</source>, <volume>145</volume>, <fpage>106535</fpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gross</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The extended process model of emotion regulation: Elaborations, applications, and future directions</article-title>. <source>Psychological inquiry</source>, <volume>26</volume> (<issue>1</issue>), <fpage>130</fpage>–<lpage>137</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>De Brigard</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Cabeza</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Davis</surname>, <given-names>S. W.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Connectivity analyses for task-based fmri</article-title>. <source>Physics of Life Reviews</source>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hutchison</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Womelsdorf</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Allen</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2013</year>). <article-title>Dynamic functional connectivity: promise, issues, and interpretations</article-title>. <source>Neuroimage</source>, <volume>80</volume>, <fpage>360</fpage>–<lpage>378</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>De Heer</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Theunissen</surname>, <given-names>F. E.</given-names></string-name>, &amp; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title>. <source>Nature</source>, <volume>532</volume> (<issue>7600</issue>), <fpage>453</fpage>–<lpage>458</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jääskeläinen</surname>, <given-names>I. P.</given-names></string-name>, <string-name><surname>Sams</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Glerean</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Ahveninen</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Movies and narratives as naturalistic stimuli in neuroimaging</article-title>. <source>NeuroImage</source>, <volume>224</volume>, <fpage>117445</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Janssen</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hernández-Cabrera</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Foronda</surname>, <given-names>L. E</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Improving the signal detection accuracy of functional magnetic resonance imaging</article-title>. <source>NeuroImage</source>, <volume>176</volume>, <fpage>92</fpage>–<lpage>109</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Janssen</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Mendieta</surname>, <given-names>C. C. R</given-names></string-name></person-group>. (<year>2020</year>). <article-title>The dynamics of speech motor control revealed with time-resolved fmri</article-title>. <source>Cerebral Cortex</source>, <volume>30</volume> (<issue>1</issue>), <fpage>241</fpage>–<lpage>255</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bannister</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Smith</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title>. <source>Neuroimage</source>, <volume>17</volume> (<issue>2</issue>), <fpage>825</fpage>–<lpage>841</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>John</surname>, <given-names>Y. J.</given-names></string-name>, <string-name><surname>Sawyer</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>Srinivasan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Müller</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Munn</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Shine</surname>, <given-names>J. M</given-names></string-name></person-group>. (<year>2022</year>). <article-title>It’s about time: Linking dynamical systems with human neuroimaging to understand the brain</article-title>. <source>Network Neuroscience</source>, <volume>6</volume> (<issue>4</issue>), <fpage>960</fpage>–<lpage>979</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Josephs</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Friston</surname>, <given-names>K</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Event-related fmri</article-title>. <source>Human brain mapping</source>, <volume>5</volume> (<issue>4</issue>), <fpage>243</fpage>–<lpage>248</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>S.-G.</given-names></string-name>, <string-name><surname>Richter</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Ugurbil</surname>, <given-names>K</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Limitations of temporal resolution in functional mri</article-title>. <source>Magnetic resonance in medicine</source>, <volume>37</volume> (<issue>4</issue>), <fpage>631</fpage>–<lpage>636</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kober</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name>, <string-name><surname>Joseph</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bliss-Moreau</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lindquist</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Functional grouping and cortical–subcortical interactions in emotion: a meta-analysis of neuroimaging studies</article-title>. <source>Neuroimage</source>, <volume>42</volume> (<issue>2</issue>), <fpage>998</fpage>–<lpage>1031</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kruggel</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>von Cramon</surname>, <given-names>D. Y.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Modeling the hemodynamic response in single-trial functional mri experiments</article-title>. <source>Magnetic Resonance in Medicine</source>, <volume>42</volume> (<issue>4</issue>), <fpage>787</fpage>–<lpage>797</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kruggel</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Zysset</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>von Cramon</surname>, <given-names>D. Y.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Nonlinear regression of functional mri data: an item recognition task study</article-title>. <source>NeuroImage</source>, <volume>12</volume> (<issue>2</issue>), <fpage>173</fpage>–<lpage>183</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuppens</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Verduyn</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Emotion dynamics</article-title>. <source>Current Opinion in Psychology</source>, <volume>17</volume>, <fpage>22</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuznetsova</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Brockhoff</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Christensen</surname>, <given-names>R. H.</given-names></string-name>, <etal>et al.</etal></person-group> (<year>2017</year>). <article-title>lmertest package: tests in linear mixed effects models</article-title>. <source>Journal of statistical software</source>, <volume>82</volume> (<issue>13</issue>), <fpage>1</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LaBar</surname>, <given-names>K. S.</given-names></string-name>, <string-name><surname>LeDoux</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Spencer</surname>, <given-names>D. D.</given-names></string-name>, &amp; <string-name><surname>Phelps</surname>, <given-names>E. A</given-names></string-name></person-group>. (<year>1995</year>). <article-title>Impaired fear conditioning following unilateral temporal lobectomy in humans</article-title>. <source>Journal of neuroscience</source>, <volume>15</volume> (<issue>10</issue>), <fpage>6846</fpage>–<lpage>6855</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lenth</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Singmann</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Love</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Buerkner</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Herve</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Emmeans: Estimated marginal means, aka least-squares means</article-title>. <source>R package version</source>, <volume>1</volume> (<issue>1</issue>), <fpage>3</fpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindquist</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Kober</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bliss-Moreau</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Barrett</surname>, <given-names>L. F</given-names></string-name></person-group>. (<year>2012</year>). <article-title>The brain basis of emotion: a meta-analytic review</article-title>. <source>Behavioral and brain sciences</source>, <volume>35</volume> (<issue>3</issue>), <fpage>121</fpage>–<lpage>143</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindquist</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Loh</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Atlas</surname>, <given-names>L. Y.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Modeling the hemodynamic response function in fmri: efficiency, bias and mis-modeling</article-title>. <source>Neuroimage</source>, <volume>45</volume> (<issue>1</issue>), <fpage>S187</fpage>–<lpage>S198</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>C. H.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Fu</surname>, <given-names>X</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Reexamining the neural network involved in perception of facial expression: A meta-analysis</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>, <volume>131</volume>, <fpage>179</fpage>–<lpage>191</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Jin</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Yao</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Separating 4d multi-task fmri data of multiple subjects by independent component analysis with projection</article-title>. <source>Magnetic Resonance Imaging</source>, <volume>31</volume> (<issue>1</issue>), <fpage>60</fpage>–<lpage>74</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meer</surname>, <given-names>J. N. v. d.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Sonkusare</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Cocchi</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Movie viewing elicits rich and reliable brain state dynamics</article-title>. <source>Nature communications</source>, <volume>11</volume> (<issue>1</issue>), <fpage>5004</fpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Méndez-Bértolo</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Moratti</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Toledano</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lopez-Sosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Martínez-Alvarez</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mah</surname>, <given-names>Y. H.</given-names></string-name>, <etal>…</etal> <string-name><surname>Strange</surname>, <given-names>B. A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A fast pathway for fear in human amygdala</article-title>. <source>Nature neuroscience</source>, <volume>19</volume> (<issue>8</issue>), <fpage>1041</fpage>–<lpage>1049</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Menon</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Luknowsky</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Gati</surname>, <given-names>J. S</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Mental chronometry using latency-resolved functional mri</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>95</volume> (<issue>18</issue>), <fpage>10902</fpage>–<lpage>10907</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minka</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2000</year>). <article-title>Automatic choice of dimensionality for pca</article-title>. <source>Advances in neural information processing systems</source>, <volume>13</volume> .</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morgenroth</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Vilaclara</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Muszynski</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gaviria</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Vuilleumier</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Van De Ville</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Probing neurodynamics of experienced emotions—a hitchhiker’s guide to film fmri</article-title>. <source>Social Cognitive and Affective Neuroscience</source>, <volume>18</volume> (<issue>1</issue>), <elocation-id>nsad063</elocation-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murphy</surname>, <given-names>F. C.</given-names></string-name>, <string-name><surname>Nimmo-Smith</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Lawrence</surname>, <given-names>A. D</given-names></string-name></person-group>. (<year>2003</year>). <article-title>Functional neuroanatomy of emotions: a meta-analysis</article-title>. <source>Cognitive, affective, &amp; behavioral neuroscience</source>, <volume>3</volume> (<issue>3</issue>), <fpage>207</fpage>–<lpage>233</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ochsner</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Bunge</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Gabrieli</surname>, <given-names>J. D</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Rethinking feelings: an fmri study of the cognitive regulation of emotion</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>14</volume> (<issue>8</issue>), <fpage>1215</fpage>–<lpage>1229</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ochsner</surname>, <given-names>K. N.</given-names></string-name>, &amp; <string-name><surname>Gross</surname>, <given-names>J. J</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Cognitive emotion regulation: Insights from social cognitive and affective neuroscience</article-title>. <source>Current directions in psychological science</source>, <volume>17</volume> (<issue>2</issue>), <fpage>153</fpage>–<lpage>158</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ogawa</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tank</surname>, <given-names>D. W.</given-names></string-name>, <string-name><surname>Menon</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ellermann</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>S. G.</given-names></string-name>, <string-name><surname>Merkle</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Ugurbil</surname>, <given-names>K</given-names></string-name></person-group>. (<year>1992</year>). <article-title>Intrinsic signal changes accompanying sensory stimulation: functional brain mapping with magnetic resonance imaging</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>89</volume> (<issue>13</issue>), <fpage>5951</fpage>–<lpage>5955</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olman</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Davachi</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Inati</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Distortion and signal loss in medial temporal lobe</article-title>. <source>PloS one</source>, <volume>4</volume> (<issue>12</issue>), <fpage>e8160</fpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palomero-Gallagher</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Amunts</surname>, <given-names>K</given-names></string-name></person-group>. (<year>2022</year>). <article-title>A short review on emotion processing: a lateralized network of neuronal networks</article-title>. <source>Brain Structure and Function</source>, <volume>227</volume> (<issue>2</issue>), <fpage>673</fpage>–<lpage>684</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Razlighi</surname>, <given-names>Q. R</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Optimal slice timing correction and its interaction with fmri parameters and artifacts</article-title>. <source>Medical image analysis</source>, <volume>35</volume>, <fpage>434</fpage>–<lpage>445</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pessoa</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2017</year>). <article-title>A network model of the emotional brain</article-title>. <source>Trends in cognitive sciences</source>, <volume>21</volume> (<issue>5</issue>), <fpage>357</fpage>–<lpage>371</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pessoa</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>McMenamin</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Dynamic networks in the emotional brain</article-title>. <source>The Neuroscientist</source>, <volume>23</volume> (<issue>4</issue>), <fpage>383</fpage>–<lpage>396</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Phan</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Wager</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>S. F.</given-names></string-name>, &amp; <string-name><surname>Liberzon</surname>, <given-names>I</given-names></string-name></person-group>. (<year>2002</year>). <article-title>Functional neuroanatomy of emotion: a meta-analysis of emotion activation studies in pet and fmri</article-title>. <source>Neuroimage</source>, <volume>16</volume> (<issue>2</issue>), <fpage>331</fpage>–<lpage>348</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riedel</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Yanes</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Ray</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Sutherland</surname>, <given-names>M. T.</given-names></string-name>, &amp; <string-name><surname>Laird</surname>, <given-names>A. R</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Dissociable meta-analytic brain networks contribute to coordinated emotional processing</article-title>. <source>Human brain mapping</source>, <volume>39</volume> (<issue>6</issue>), <fpage>2514</fpage>–<lpage>2531</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosen</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Roalf</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Ruparel</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Blake</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Seelaus</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Villa</surname>, <given-names>L. P.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2018</year>). <article-title>Quantitative assessment of structural image quality</article-title>. <source>Neuroimage</source>, <volume>169</volume>, <fpage>407</fpage>–<lpage>418</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saarimäki</surname>, <given-names>H</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Naturalistic stimuli in affective neuroimaging: A review</article-title>. <source>Frontiers in human neuroscience</source>, <volume>15</volume>, <fpage>675068</fpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Satpute</surname>, <given-names>A. B.</given-names></string-name>, &amp; <string-name><surname>Lindquist</surname>, <given-names>K. A</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The default mode network’s role in discrete emotion</article-title>. <source>Trends in cognitive sciences</source>, <volume>23</volume> (<issue>10</issue>), <fpage>851</fpage>–<lpage>864</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seeley</surname>, <given-names>W. W</given-names></string-name></person-group>. (<year>2019</year>). <article-title>The salience network: a neural system for perceiving and responding to homeostatic demands</article-title>. <source>Journal of Neuroscience</source>, <volume>39</volume> (<issue>50</issue>), <fpage>9878</fpage>–<lpage>9882</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sladky</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Tröstl</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cunnington</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Windischberger</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2011</year>). <article-title>Slice-timing effects and their correction in functional mri</article-title>. <source>Neuroimage</source>, <volume>58</volume> (<issue>2</issue>), <fpage>588</fpage>–<lpage>594</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Glahn</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>P. M.</given-names></string-name>, <string-name><surname>Mackay</surname>, <given-names>C. E.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2009</year>). <article-title>Correspondence of the brain’s functional architecture during activation and rest</article-title>. <source>Proceedings of the national academy of sciences</source>, <volume>106</volume> (<issue>31</issue>), <fpage>13040</fpage>–<lpage>13045</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sonkusare</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Guo</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Naturalistic stimuli in neuroscience: critically acclaimed</article-title>. <source>Trends in cognitive sciences</source>, <volume>23</volume> (<issue>8</issue>), <fpage>699</fpage>–<lpage>714</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vemuri</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Surampudi</surname>, <given-names>B. R</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Evidence of stimulus correlated empathy modes–group ica of fmri data</article-title>. <source>Brain and cognition</source>, <volume>94</volume>, <fpage>32</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visser</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jefferies</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Embleton</surname>, <given-names>K. V.</given-names></string-name>, &amp; <string-name><surname>Lambon Ralph</surname>, <given-names>M. A</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Both the middle temporal gyrus and the ventral anterior temporal area are crucial for multimodal semantic processing: distortion-corrected fmri evidence for a double gradient of information convergence in the temporal lobes</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>24</volume> (<issue>8</issue>), <fpage>1766</fpage>–<lpage>1778</lpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wallenwein</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>S. N.</given-names></string-name>, <string-name><surname>Hass</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Mier</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Cross-modal decoding of emotional expressions in fmri—cross-session and cross-sample replication</article-title>. <source>Imaging Neuroscience</source>, <volume>2</volume>, <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waugh</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Shing</surname>, <given-names>E. Z.</given-names></string-name>, &amp; <string-name><surname>Avery</surname>, <given-names>B. M</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Temporal dynamics of emotional processing in the brain</article-title>. <source>Emotion Review</source>, <volume>7</volume> (<issue>4</issue>), <fpage>323</fpage>–<lpage>329</lpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Westermann</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Spies</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Stahl</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Hesse</surname>, <given-names>F. W</given-names></string-name></person-group>. (<year>1996</year>). <article-title>Relative effectiveness and validity of mood induction procedures: A meta-analysis</article-title>. <source>European Journal of social psychology</source>, <volume>26</volume> (<issue>4</issue>), <fpage>557</fpage>–<lpage>580</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Windischberger</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cunnington</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Lamm</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lanzenberger</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Langenberger</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Deecke</surname>, <given-names>L.</given-names></string-name>, <etal>…</etal> <string-name><surname>Moser</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2008</year>). <article-title>Time-resolved analysis of fmri signal changes using brain activation movies</article-title>. <source>Journal of neuroscience methods</source>, <volume>169</volume> (<issue>1</issue>), <fpage>222</fpage>–<lpage>230</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>M.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2023</year>). <article-title>Functional connectivity profiles of the default mode and visual networks reflect temporal accumulative effects of sustained naturalistic emotional experience</article-title>. <source>NeuroImage</source>, <volume>269</volume>, <fpage>119941</fpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yeo</surname>, <given-names>B. T.</given-names></string-name>, <string-name><surname>Krienen</surname>, <given-names>F. M.</given-names></string-name>, <string-name><surname>Sepulcre</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sabuncu</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Lashkari</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hollinshead</surname>, <given-names>M.</given-names></string-name>, <etal>…</etal> <string-name><surname>others</surname></string-name></person-group> (<year>2011</year>). <article-title>The organization of the human cerebral cortex estimated by intrinsic functional connectivity</article-title>. <source>Journal of neurophysiology</source>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zemeckis</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Hanks</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wright</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sinise</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Williamson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Field</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Groom</surname>, <given-names>W.</given-names></string-name></person-group> (<year>1994</year>). <source>Forrest gump</source>. <publisher-name>Paramount Pictures Los Angeles</publisher-name>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Qi</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Geng</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kendrick</surname>, <given-names>K. M.</given-names></string-name>, <etal>…</etal> <string-name><surname>Becker</surname>, <given-names>B</given-names></string-name></person-group>. (<year>2021</year>). <article-title>A distributed fmri-based signature for the subjective experience of fear</article-title>. <source>Nature communications</source>, <volume>12</volume> (<issue>1</issue>), <fpage>6643</fpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106070.2.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Hu</surname>
<given-names>Xiaoqing</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02zhqgq86</institution-id>
<institution>University of Hong Kong</institution>
</institution-wrap>
<city>Hong Kong</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This manuscript provides <bold>important</bold> information on the neurodynamics of emotional processing while participants were watching movie clips. This work provides <bold>convincing</bold> results in deciphering the temporal-spatial dynamics of emotional processing. This work will be of interest to affective neuroscientists and fMRI researchers in general.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106070.2.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary and strengths:</p>
<p>In this manuscript, the authors endeavor to capture the dynamics of emotion-related brain networks. They employ slice-based fMRI combined with ICA on fMRI time series recorded while participants viewed a short movie clip. This approach allowed them to track the time course of four non-noise independent components at an effective 2s temporal resolution at the BOLD level. Notably, the authors report a temporal sequence from input to meaning, followed by response, and finally default mode networks, with significant overlap between stages. The use of ICA offers a data-driven method to identify large-scale networks involved in dynamic emotion processing. Overall, this paradigm and analytical strategy mark an important step forward in shifting affective neuroscience toward investigating temporal dynamics rather than relying solely on static network assessments.</p>
<p>(1) One of the main advantages highlighted is the improved temporal resolution offered by slice-based fMRI. However, the manuscript does not clearly explain how this method achieves a higher effective resolution, especially since the results still show a 2s temporal resolution-comparable to conventional methods. Clarification on this point would help readers understand the true benefit of the approach.</p>
<p>(2) While combining ICA with task fMRI is an innovative approach to study the spatiotemporal dynamics of emotion processing, task fMRI typically relies on modeling the hemodynamic response (e.g., using FIR or IR models) to mitigate noise and collinearity across adjacent trials. The current analysis uses unmodeled BOLD time series, which might risk suffering from these issues.</p>
<p>(3) The study's claims about emotion dynamics are derived from fMRI data, which are inherently affected by the hemodynamic delay. This delay means that the observed time courses may differ substantially from those obtained through electrophysiology or MEG studies. A discussion on how these fMRI-derived dynamics relate to-or complement-is critical for the field to understand the emotion dynamics.</p>
<p>(4) Although using ICA to differentiate emotion elements is a convenient approach to tell a story, it may also be misleading. For instance, the observed delayed onset and peak latency of the 'response network' might imply that emotional responses occur much later than other stages, which contradicts many established emotion theories. Given the involvement of large-scale brain regions in this network, the underlying reasons for this delay could be very complex.</p>
<p>Added after revision: In the response letter, the authors have provided clear responses to these comments and improved the manuscript.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.106070.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Janssen</surname>
<given-names>Niels</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9770-4458</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Elvira</surname>
<given-names>Uriel KA</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Janssen</surname>
<given-names>Joost</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7613-2067</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>van Erp</surname>
<given-names>Theo GM</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>In this manuscript, the authors endeavor to capture the dynamics of emotion-related brain networks. They employ slice-based fMRI combined with ICA on fMRI time series recorded while participants viewed a short movie clip. This approach allowed them to track the time course of four non-noise independent components at an effective 2s temporal resolution at the BOLD level. Notably, the authors report a temporal sequence from input to meaning, followed by response, and finally default mode networks, with significant overlap between stages. The use of ICA offers a data-driven method to identify large-scale networks involved in dynamic emotion processing. Overall, this paradigm and analytical strategy mark an important step forward in shifting affective neuroscience toward investigating temporal dynamics rather than relying solely on static network assessments</p>
<p>Strengths:</p>
<p>(1) One of the main advantages highlighted is the improved temporal resolution offered by slice-based fMRI. However, the manuscript does not clearly explain how this method achieves a higher effective resolution, especially since the results still show a 2s temporal resolution, comparable to conventional methods. Clarification on this point would help readers understand the true benefit of the approach.</p>
<p>(2) While combining ICA with task fMRI is an innovative approach to study the spatiotemporaldynamics of emotion processing, task fMRI typically relies on modeling the hemodynamic response (e.g., using FIR or IR models) to mitigate noise and collinearity across adjacent trials. The current analysis uses unmodeled BOLD time series, which might risk suffering from these issues.</p>
<p>(3) The study's claims about emotion dynamics are derived from fMRI data, which are inherently affected by the hemodynamic delay. This delay means that the observed time courses may differ substantially from those obtained through electrophysiology or MEG studies. A discussion on how these fMRI-derived dynamics relate to - or complement - is critical for the field to understand the emotion dynamics.</p>
<p>(4) Although using ICA to differentiate emotion elements is a convenient approach to tell a story, it may also be misleading. For instance, the observed delayed onset and peak latency of the 'response network' might imply that emotional responses occur much later than other stages, which contradicts many established emotion theories. Given the involvement of largescale brain regions in this network, the underlying reasons for this delay could be very complex.</p>
<p>Concerns and suggestions:</p>
<p>However, I have several concerns regarding the specific presentation of temporal dynamics in the current manuscript and offer the following suggestions.</p>
<p>(1) One selling point of this work regarding the advantages of testing temporal dynamics is the application of slice-based fMRI, which, in theory, should improve the temporal resolution of the fMRI time course. Improving fMRI temporal resolution is critical for a research project on this topic. The authors present a detailed schematic figure (Figure 2) to help readers understand it. However, I have difficulty understanding the benefits of this method in terms of temporal resolution.</p>
<p>(a) In Figure 2A, if we examine a specific voxel in slice 2, the slice acquisitions occur at 0.7s, 2.7s, and 4.7s, which implies a temporal resolution of 2s rather than 0.7s. I am unclear on how the temporal resolution could be 0.7s for this specific voxel. I would prefer that the authors clarify this point further, as it would benefit readers who are not familiar with this technology.</p>
</disp-quote>
<p>We very much appreciate these concerns as they highlight shortcomings in our explanation of the method. Please note that the main explanation of the method (and comparison with expected HRF and FIR based methods) is done in Janssen et al. (2018, NeuroImage; see further explanations in Janssen et al., 2020). However, to make the current paper more selfcontained, we provided further explanation of the Slice-Based method in Figure 2. With respect to the specific concern of the reviewer, in the hypothetical example used in Figure 2, the temporal resolution of the voxel on slice 2 is 0.7s because it combines the acquisitions from stimulus presentations across all trials. Specifically, given the specific study parameters as outlined in Figures 2A and B, slice 2 samples the state of the brain exactly 0s after stimulus presentation on trial 1 (red color), 0.7s after stimulus presentation on trial 3 (green color), and 1.3s after stimulus presentation on trial 2 (yellow color). Thus after combining data acquisitions across these three 3 stimuli presentations, slice 2 has sampled the state of the brain at timepoints that are multiples of 0.7s starting from stimulus onset. This is why we say that the theoretical maximum temporal resolution is equal to the TR divided by the number of slices (in the example 2/3 = 0.7s, in the actual experiment 3/39 = 0.08s). In the current study we used temporal binning across timepoints to reduce the temporal resolution (to 2 seconds) and improve the tSNR.</p>
<p>We have updated the legend of Figure 3 to more clearly explain this issue.</p>
<disp-quote content-type="editor-comment">
<p>(b) Even with the claim of an increased temporal resolution (0.7s), the actual data (Figure 3) still appears to have a 2s resolution. I wonder what specific benefit slice-based fMRI brings in terms of testing temporal dynamics, aside from correcting the temporal distortions that conventional fMRI exhibits.</p>
</disp-quote>
<p>This is a good point. In the current experiment, the TR was 3s, but we extracted the fMRI signal at 2s temporal resolution, which means an increment of 33%. In this study we did not directly compare the impact of different temporal resolutions on the efficacy of detection of network dynamics. Indeed, we agree with the reviewer that there remain many unanswered questions about the issue of temporal resolution of the extracted fMRI signal and the impact on the ability to detect fMRI network dynamics. We think that questions such as those posed by the reviewer should be addressed in future studies that are directly focused on this issue. We have updated our discussion section (page 21-22) to more clearly reflect this point of view.</p>
<disp-quote content-type="editor-comment">
<p>(2) In task-fMRI, the hemodynamic response is usually estimated using a specific model (e.g., FIR, IR model; see Lindquist et al., 2009). These models are effective at reducing noise and collinearity across adjacent trials. The current method appears to be conducted on unmodeled BOLD time series.</p>
<p>(a) I am wondering how the authors avoid the issues that are typically addressed by these HRF modeling approaches. For example, if we examine the baseline period (say, -4 to 0s relative to stimulus onset), the activation of most networks does not remain around zero, which could be due to delayed influences from the previous trial. This suggests that the current time course may not be completely accurate.</p>
</disp-quote>
<p>We thank the reviewer for highlighting this issue. Let us start by reiterating what we stated above: That there are many issues related to BOLD signal extraction and fMRI network discovery in task-based fMRI that remain poorly understood and should be addressed in future work. Such work should explore, for example, the impact of using a FIR vs Slice-based method on the discovery of networks in task-fMRI. These studies should also investigate the impact of different types of baselines and baseline durations on the extraction of the BOLD signal and network discovery. For the present purposes, our goal was not to introduce a new technique of fMRI signal extraction, but to show that the slice-based technique, in combination with ICA, can be used to study the brain’s networks dynamics in an emotional task. In other words, while we clearly appreciate the reviewer’s concerns and have several other studies underway that directly address these concerns, we believe that such concerns are better addressed in independent research. See our discussion on page 21-22 that addresses this issue.</p>
<disp-quote content-type="editor-comment">
<p>(b) A related question: if the authors take the spatial map of a certain network and apply a modeling approach to estimate a time series within that network, would the results be similar to the current ICA time series?</p>
</disp-quote>
<p>Interesting point. Typically in a modeling approach the expected HRF (e.g., the double gamma function) is fitted to the fMRI data. Importantly, this approach produces static maps of the fit between the expected HRF and the data. By contrast, model-free approaches such as FIR or slice-based methods extract the fMRI signal directly from the data without making apriori assumptions about the expected shape of the signal. These approaches do not produce static maps but instead are capable of extracting the whole-brain dynamics during the execution of a task (event-related dynamics). These data-driven approaches (FIR, SliceBased, etc) are therefore a necessary first step in the analyses of the dynamics of brain activity during a task. The subsequent step involves the analyses of these complex eventrelated brain dynamics. In the current paper we suggest that a straightforward way to do this is to use ICA which produces spatial maps of voxels with similar time courses, and hence, yields insights into the temporal dynamics of whole-brain fMRI networks. As we mentioned above, combining ICA with a high temporal resolution data-driven signal is new and there are many new avenues for research in this burgeoning new field.</p>
<disp-quote content-type="editor-comment">
<p>(3) Human emotion should be inherently fast to ensure survival, as shown in many electrophysiology and MEG studies. For example, the dynamics of a fearful face can occur within 100ms in subcortical regions (Méndez-Bértolo et al., 2016), and general valence and arousal effects can occur as early as 200ms (e.g., Grootswagers et al., 2020; Bo et al., 2022). In contrast, the time-to-peak or onset timing in the BOLD time series spans a much larger time range due to the hemodynamic delay. fMRI findings indeed add spatial precision to our understanding of the temporal dynamics of emotion, but could the authors comment on how the current temporal dynamics supplement those electrophysiology studies that operate on much finer temporal scales?</p>
</disp-quote>
<p>We really like this point. One way that EEG and fMRI are typically discussed is that these two approaches are said to be complementary. While EEG is able to provide information on temporal dynamics, but not spatial localization of brain activity, fMRI cannot provide information on the temporal dynamics, but can provide insights into spatial localization. Our study most directly challenges the latter part of this statement. We believe that by using tasks that highlight “slow” cognition, fMRI can be used to reveal not only spatial but also temporal information of brain activity. The movie task that we used presumably relies on a kind of “slow” cognition that takes place on longer time scales (e.g., the construction of the meaning of the scene). Our results show that with such tasks, whole-brain networks with different temporal dynamics can be separated by ICA, at odds with the claim that fMRI is only good for spatial information. One avenue of future research would be to attempt such “slow” tasks directly with EEG and try to find the electrical correlates of the networks detected in the current study.</p>
<p>We hope to have answered the concerns of the reviewer.</p>
<disp-quote content-type="editor-comment">
<p>(4) The response network shows activation as late as 15 to 20s, which is surprising. Could the authors discuss further why it takes so long for participants to generate an emotional response in the brain?</p>
</disp-quote>
<p>We thank the reviewer for this question. Our study design was such that there was an initial movie clip that lasted 12.5s, which was then followed by a two-alternative forced-choice decision task (including a button press, 2.5s), and finally followed by a 10s rest period. We extracted the fMRI signal across this entire 25s period (actually 28s because we also took into account some uncertainty in BOLD signal duration). Network discovery using ICA then showed various networks with distinct time courses (across the 25s period), including one network (IC2 response) that showed a peak around 21s (see Figure 3). Given the properties of the spatial map (eg., activity in primary motor areas, Figure 4), as well as the temporal properties of its timecourse (e.g., peak close to the response stage of the task), we interpreted this network as related to generating the manual response in the two-alternative forced-choice decision task. Further analyses showed that this aspect of the task (e.g., deciding the emotion of the character in the movie clip) was also sensitive to the emotional content of the earlier movie clip (Figure 6 and 7).</p>
<p>We have further clarified this aspect of our results (see pages 16-17). We thank the reviewer for pointing this out.</p>
<disp-quote content-type="editor-comment">
<p>(5) Related to 4. In many theories, the emotion processing stages-including perception, valuation, and response-are usually considered iterative processes (e.g., Gross, 2015), especially in real-world scenarios. The advantage of the current paradigm is that it incorporates more dynamic elements of emotional stimuli and is closer to reality. Therefore, one might expect some degree of dynamic fluctuation within the tested brain networks to reflect those potential iterative processes (input, meaning, response). However, we still do not observe much brain dynamics in the data. In Figure 5, after the initial onset, most network activations remain sustained for an extended period of time. Does this suggest that emotion processing is less dynamic in the brain than we thought, or could it be related to limitations in temporal resolution? It could also be that the dynamics of each individual trial differ, and averaging them eliminates these variations. I would like to hear the authors' comments on this topic.</p>
</disp-quote>
<p>We thank the reviewer for this interesting question. We are assuming the reviewer is referring to Figure 3 and not Figure 5. Indeed what Figure 3 shows is the average time course of each detected network across all subjects and trial types. This figure therefore does not directly show the difference in dynamics between the different emotions. However, as we show in further analyses that examine how emotion modulates specific aspects of the fMRI signal dynamics (time to peak, peak value, duration) of different networks, there are differences in the dynamics of these networks depending on the emotion (Figure 6 and 7). Thus, our results show that different emotions evoked by movie clips differ in their dynamics. Obviously, generalizing this to say that in general, different emotions have different brain dynamics is not straightforward and would require further study (probably using other tasks, and other emotions). We have updated the discussion section as well as the caption of Figure 3 to better explain this issue (see also comments by reviewer 2).</p>
<disp-quote content-type="editor-comment">
<p>(6) The activation of the default mode network (DMN), although relatively late, is very interesting. Generally, one would expect a deactivation of this network during ongoing external stimulation. Could this suggest that participants are mind-wandering during the later portion of the task?</p>
</disp-quote>
<p>Very good point. Indeed this is in line with our interpretation. The late activity of the default mode network could reflect some further processing of the previous emotional experience. More work is required to clarify this further in terms of reflective, mind-wandering or regulatory processing. We have updated our discussion section to better highlight this issue (see page 19).</p>
<p>We thank the reviewer for their really insightful comments and suggestions!</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This manuscript examined the neural correlates of the temporal-spatial dynamics of emotional processing while participants were watching short movie clips (each 12.5 s long) from the movie &quot;Forrest Gump&quot;. Participants not only watched each film clip, but also gave emotional responses, followed by a brief resting period. Employing fMRI to track the BOLD responses during these stages of emotional processing, the authors found four large-scale brain networks (labeled as IC0,1,2,4) were differentially involved in emotional processing. Overall, this work provides valuable information on the neurodynamics of emotional processing.</p>
<p>Strengths:</p>
<p>This work employs a naturalistic movie watching paradigm to elicit emotional experiences. The authors used a slice-based fMRI method to examine the temporal dynamics of BOLD responses. Compared to previous emotional research that uses static images, this work provides some new data and insights into how the brain supports emotional processing from a temporal dynamics view.</p>
</disp-quote>
<p>Thank you!</p>
<disp-quote content-type="editor-comment">
<p>Weaknesses:</p>
<p>Some major conclusions are unwarranted and do not have relevant evidence. For example, the authors seemed to interpret some neuroimaging results to be related to emotion regulation. However, there were no explicit instructions about emotional regulation, and there was no evidence suggesting participants regulated their emotions. How to best interpret the corresponding results thus requires caution.</p>
</disp-quote>
<p>We thank the reviewer for pointing this out. We have updated the limitations section of our Discussion section (page 20) to better qualify our interpretations.</p>
<disp-quote content-type="editor-comment">
<p>Relatedly, the authors argued that &quot;In turn, our findings underscore the utility of examining temporal metrics to capture subtle nuances of emotional processing that may remain undetectable using standard static analyses.&quot; While this sentence makes sense and is reasonable, it remains unclear how the results here support this argument. In particular, there were only three emotional categories: sad, happy, and fear. These three emotional categories are highly different from each other. Thus, how exactly the temporal metrics captured the &quot;subtle nuances of emotional processing&quot; shall be further elaborated.</p>
</disp-quote>
<p>This is an important point. We also discuss this limitation in the “limitations” section of our Discussion (page 20). We again thank the reviewer for pointing this out.</p>
<disp-quote content-type="editor-comment">
<p>The writing also contained many claims about the study's clinical utility. However, the authors did not develop their reasoning nor elaborate on the clinical relevance. While examining emotional processing certainly could have clinical relevance, please unpack the argument and provide more information on how the results obtained here can be used in clinical settings.</p>
</disp-quote>
<p>We very much appreciate this comment. Note that we did not intend to motivate our study directly from a clinical perspective (because we did not test our approach on a clinical population). Instead, our point is that some researchers (e.g., Kuppens &amp; Verduyn 2017; Waugh et al., 2015) have conceptualized emotional disorders frequently having a temporal component (e.g., dwelling abnormally long on negative thoughts) and that our technique could be used to examine if temporal dynamics of networks are affected in such disorders. However, as we pointed out, this should be verified in future work. We have updated our final paragraph (page 22) to more clearly highlight this issue. We thank the reviewer for pointing this out.</p>
<disp-quote content-type="editor-comment">
<p>Importantly, how are the temporal dynamics of BOLD responses and subjective feelings related? The authors showed that &quot;the time-to-peak differences in IC2 (&quot;response&quot;) align closely with response latency results, with sad trials showing faster response latencies and earlier peak times&quot;. Does this mean that people typically experience sad feelings faster than happy or fear? Yet this is inconsistent with ideas such that fear detection is often rapid, while sadness can be more sustained. Understandably, the study uses movie clips, which can be very different from previous work, mostly using static images (e.g., a fearful or a sad face). But the authors shall explicitly discuss what these temporal dynamics mean for subjective feelings.</p>
</disp-quote>
<p>Excellent point! Our results indeed showed that sad trials had faster reaction times compared to happy and fearful trials, and that this result was reflected in the extracted time-to-peak measures of the fMRI data (see Figure 8D). To us, this primarily demonstrates that, as shown in other studies (e.g., Menon et al., 1997), that gross differences detected in behavioral measures can be directly recovered from temporal measures in fMRI data, which is not trivial. However, we do not think we are allowed to make interpretations of the sort suggested by the reviewer (and to be clear: we do not make such interpretations in the paper). Specifically, the faster reaction times on sad trials likely reflect some audio/visual aspect of the movie clips that result in faster reaction times instead of a generalized temporal difference in the subjective experience of sad vs happy/fearful emotions. Presumably the speed with which emotional stimuli influence the brain depends on the context. Perhaps future studies that examine emotional responses while controlling for the audio/visual experience could shed further light on this issue. We have updated the discussion section to address the reviewer’s concern.</p>
<p>We thank the reviewer for the interesting points which have certainly improved our manuscript!</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>Minor:</p>
<p>(1) Please add the unit to the y-axis in Figure 7, if applicable.</p>
</disp-quote>
<p>Done. We have added units.</p>
<disp-quote content-type="editor-comment">
<p>(2) Adding a note in the legend of Figure 3 regarding the meaning of the amplitude of the timeseries would be helpful.</p>
</disp-quote>
<p>Done. We have added a sentence further explaining the meaning of the timecourse fluctuations.</p>
<p>Related references:</p>
<p>(1) Lindquist, M. A., Loh, J. M., Atlas, L. Y., &amp; Wager, T. D. (2009). Modeling the hemodynamic response function in fMRI: efficiency, bias, and mis-modeling. Neuroimage, 45(1), S187-S198.</p>
<p>(2) Méndez-Bértolo, C., Moratti, S., Toledano, R., Lopez-Sosa, F., Martínez-Alvarez, R., Mah, Y. H., ... &amp; Strange, B. A. (2016). A fast pathway for fear in human amygdala. Nature neuroscience, 19(8), 1041-1049.</p>
<p>(3) Bo, K., Cui, L., Yin, S., Hu, Z., Hong, X., Kim, S., ... &amp; Ding, M. (2022). Decoding the temporal dynamics of affective scene processing. NeuroImage, 261, 119532.</p>
<p>(4) Grootswagers, T., Kennedy, B. L., Most, S. B., &amp; Carlson, T. A. (2020). Neural signatures of dynamic emotion constructs in the human brain. Neuropsychologia, 145, 106535.</p>
<p>(5) Gross, J. J. (2015). The extended process model of emotion regulation: Elaborations, applications, and future directions. Psychological inquiry, 26(1), 130-137.</p>
</body>
</sub-article>
</article>