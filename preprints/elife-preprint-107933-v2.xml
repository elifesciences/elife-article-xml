<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107933</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107933</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107933.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories><title-group>
<article-title>Modality-Agnostic Decoding of Vision and Language from fMRI</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5609-6628</contrib-id>
<name>
<surname>Nikolaus</surname>
<given-names>Mitja</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
<email>mitja.nikolaus@cnrs.fr</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4521-1640</contrib-id>
<name>
<surname>Mozafari</surname>
<given-names>Milad</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Berry</surname>
<given-names>Isabelle</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Asher</surname>
<given-names>Nicholas</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7078-1055</contrib-id>
<name>
<surname>Reddy</surname>
<given-names>Leila</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>VanRullen</surname>
<given-names>Rufin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ahyrz84</institution-id><institution>Université de Toulouse, CNRS, CerCo</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution>Torus AI</institution>, <city>Toulouse</city>, <country country="FR">France</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ahyrz84</institution-id><institution>Université de Toulouse, IRIT</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country country="CN">China</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-09-03">
<day>03</day>
<month>09</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-12-03">
<day>03</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107933</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-06-14">
<day>14</day>
<month>06</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-06-08">
<day>08</day>
<month>06</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.06.08.658221"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-09-03">
<day>03</day>
<month>09</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107933.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.107933.1.sa3">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107933.1.sa2">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107933.1.sa1">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107933.1.sa0">Reviewer #3 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Nikolaus et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Nikolaus et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107933-v2.pdf"/>
<abstract>
<p>Humans perform tasks involving the manipulation of inputs regardless of how these signals are perceived by the brain, thanks to representations that are invariant to the stimulus modality. In this paper, we present modality-agnostic decoders that leverage such modality-invariant representations to predict which stimulus a subject is seeing, irrespective of the modality in which the stimulus is presented. Training these modality-agnostic decoders is made possible thanks to our new large-scale fMRI dataset SemReps-8K, released publicly along with this paper. It comprises 6 subjects watching both images and short text descriptions of such images, as well as conditions during which the subjects were imagining visual scenes. We find that modality-agnostic decoders can perform as well as modality-specific decoders, and even outperform them when decoding captions and mental imagery. Further, a searchlight analysis revealed that large areas of the brain contain modality-invariant representations. Such areas are also particularly suitable for decoding visual scenes from the mental imagery condition.</p>
</abstract>
<funding-group>
<award-group id="par-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id>
<institution>EC | European Research Council (ERC)</institution>
</institution-wrap>
</funding-source>
<award-id>Advanced grant GLoW (101096017)</award-id>
<principal-award-recipient>
<name>
<surname>VanRullen</surname>
<given-names>Rufin</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id>
<institution>Agence Nationale de la Recherche (ANR)</institution>
</institution-wrap>
</funding-source>
<award-id>AI-REPS (ANR-18-CE37-0007-01)</award-id>
<principal-award-recipient>
<name>
<surname>Reddy</surname>
<given-names>Leila</given-names>
</name>
<name>
<surname>VanRullen</surname>
<given-names>Rufin</given-names>
</name>
<name>
<surname>Asher</surname>
<given-names>Nicholas</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="par-5">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id>
<institution>Agence Nationale de la Recherche (ANR)</institution>
</institution-wrap>
</funding-source>
<award-id>ANITI (ANR-19-PI3A-0004)</award-id>
<principal-award-recipient>
<name>
<surname>Asher</surname>
<given-names>Nicholas</given-names>
</name>
<name>
<surname>Reddy</surname>
<given-names>Leila</given-names>
</name>
<name>
<surname>VanRullen</surname>
<given-names>Rufin</given-names>
</name>
</principal-award-recipient>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>- This version contains the updated manuscript after reviews
- major changes include that the main contribution of this paper to introduce modality-agnostic decoders is highlighted more clearly, and an updated terminology regarding modality-invariant representations</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Several regions in the human brain have developed a high degree of specialization for particular lower-level perceptive as well as higher-level cognitive functions (<xref ref-type="bibr" rid="c56">Kanwisher, 2010</xref>). For many higher-level functions, it is crucial to be able to manipulate inputs regardless of the modality in which a stimulus was perceived by the brain. Such manipulations can be performed thanks to representations that are abstracted away from particularities of specific modalities, and are therefore <italic>modality-invariant</italic>. A range of theories have been developed to explain how and where in the human brain such invariant representations are created (<xref ref-type="bibr" rid="c22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="c13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="c76">Martin, 2016</xref>; <xref ref-type="bibr" rid="c8">Barsalou, 2016</xref>; <xref ref-type="bibr" rid="c99">Ralph et al., 2017</xref>).</p>
<p>In this paper, we aim to develop modality-agnostic decoders that are specifically trained to leverage such modality-invariant representation patterns in the brain. In contrast to <italic>modality-specific</italic> decoders that are trained to be applied only in the modality that they were trained on, <italic>modalityagnostic</italic> decoders can be applied to decode brain patterns from multiple modalities, even without knowing a priori the modality the stimulus was presented in.</p>
<p>Regarding the terminology that will be used in the following, we use the terms modality-agnostic and modality-specific to refer to the way decoders are trained. The term modality-dependent describes patterns in brain activity that are completely unrelated for stimuli presented in varying modalities. In contrast, modality-invariant patterns do contain a significant degree of shared representational structure between modalities (<xref ref-type="bibr" rid="c74">Man et al., 2012</xref>). Evidence suggests that many regions in the brain contain mixtures of both types of information: A single region can show activation patterns with contributions of multiple modality-dependent and modality-invariant features (<xref ref-type="bibr" rid="c72">Liuzzi et al., 2020</xref>; <xref ref-type="bibr" rid="c29">Dirani and Pylkkänen, 2024</xref>). Modality-agnostic decoders are explicitly trained to leverage the shared information in modality-invariant patterns of the brain activity.</p>
<p>In order to build modality-agnostic decoders, large multimodal neuroimaging datasets with well-controlled stimuli across modalities are required. While large-scale datasets exist for vision (<xref ref-type="bibr" rid="c50">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="c20">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="c3">Allen et al., 2022</xref>), language (<xref ref-type="bibr" rid="c17">Brennan and Hale, 2019</xref>; <xref ref-type="bibr" rid="c84">Nastase et al., 2021</xref>; <xref ref-type="bibr" rid="c104">Schoffelen et al., 2019</xref>; <xref ref-type="bibr" rid="c119">Tang et al., 2023a</xref>), and video (naturalistic movies) (<xref ref-type="bibr" rid="c2">Aliko et al., 2020</xref>; <xref ref-type="bibr" rid="c87">Visconti di Oleggio Castello et al., 2020</xref>; <xref ref-type="bibr" rid="c15">Boyle et al., 2020</xref>), none of these contain a controlled set of equivalent stimuli that are presented separately in both modalities. For instance, the different modalities in movies (vision and language) are complementary but do not always carry the same semantics. Further, they are not presented separately but simultaneously, impeding a study of the respective activity pattern caused by each modality in isolation.</p>
<p>The analyses of this paper are based on <bold>SemReps-8K</bold>, a new large-scale multimodal fMRI dataset of 6 subjects each viewing more than 8,000 stimuli which are presented separately in one of two modalities, as images of visual scenes or as descriptive captions of such images. In addition, the dataset also contains 3 imagery conditions for each subject, where they had to imagine an visual scene based on a caption description they had received before the start of the fMRI experiment.</p>
<p>We exploit this new data to develop modality-agnostic decoders that are trained on brain imaging data from multiple modalities, which we demonstrate here for the case of vision and language. We find that modality-agnostic decoders trained on this dataset perform on par with their respective modality-specific counterparts for decoding images, despite the additional challenge of uncertainty about the stimulus modality. For decoding captions, the modality-agnostic decoders even outperform the respective modality-specific decoders (because the former, but not the latter, can leverage the additional training data from the other image modality).</p>
<p>Additionally, we use this novel kind of decoders for a searchlight analysis to localize regions with modality-invariant representations in the brain. Previous studies that aimed to localize modality-invariant patterns were based on limited and rather simple stimulus sets and did not always agree on the exact location and extent of such regions (e.g. <xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref>). We design a search-light analysis based on a combination of modality-agnostic decoders and cross-decoding. The results reveal that modality-invariant patterns can be found in a widespread left-lateralized network across the brain, encompassing virtually all regions that have been proposed previously.</p>
<p>Finally, we find that modality-agnostic decoders trained only on data with perceptual input also generalize to conditions during which the subjects were performing mental imagery, and outperform both kinds of modality-specific decoders on this task.</p>
<sec id="s2">
<title>Related Work</title>
<sec id="s2a">
<title>Modality-invariant representations</title>
<p>Decades of neuro-anatomical research (e.g. based on clinical lesions) and electrophysiology in non-human and human primates, as well as modern experiments leveraging recent brain imaging techniques have provided evidence that the activation patterns in certain brain areas are modality-dependent; for example, the occipital cortex responds predominantly to visual stimulation (<xref ref-type="bibr" rid="c36">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="c105">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="c45">Grill-Spector and Malach, 2004</xref>), and a commonly leftlateralized network responds to language processing tasks (<xref ref-type="bibr" rid="c132">Zola-Morgan, 1995</xref>; <xref ref-type="bibr" rid="c35">Fedorenko et al., 2010</xref>, <xref ref-type="bibr" rid="c34">2011</xref>; <xref ref-type="bibr" rid="c40">Friederici, 2017</xref>; <xref ref-type="bibr" rid="c16">Brennan, 2022</xref>).</p>
<p>More recent research has started to focus on higher-level regions that respond with <italic>modalityinvariant</italic> patterns, i.e., patterns that are abstracted away from any modality-dependent information. A modality-invariant region responds with similar patterns to input stimuli of the same meaning, even if they are presented in different modalities (e.g. the word “cat” and picture of a cat).</p>
<p>Several theories and frameworks on how the brain transforms inputs from different modalities into modality-invariant representations have been proposed (Here, we do not attempt to disambiguate whether information is modality-invariant or completely abstracted away from input modalities, described in the literature as modality-independent (<xref ref-type="bibr" rid="c29">Dirani and Pylkkänen, 2024</xref>), supramodal (<xref ref-type="bibr" rid="c103">Sanchez et al., 2020</xref>), transmodal (<xref ref-type="bibr" rid="c99">Ralph et al., 2017</xref>), amodal (<xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>), or abstract/conceptual (<xref ref-type="bibr" rid="c12">Binder, 2016</xref>) (see also distinctions made in <xref ref-type="bibr" rid="c8">Barsalou, 2016</xref>). For our goal of building modality-agnostic decoders, it is only important that information be represented in a way invariant to the stimulus modality. We will come back to this point in the Discussion section.). The <bold>convergence zones view</bold> proposes that modality-dependent information coming from sensory cortices is integrated in multiple convergence zones that are distributed across the cortex, predominantly in temporal and parietal lobes (<xref ref-type="bibr" rid="c22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="c123">Tranel et al., 1997</xref>; <xref ref-type="bibr" rid="c79">Meyer and Damasio, 2009</xref>). These convergence zones are organized hierarchically, learned associations are used to create abstractions from lower-level to higher-level feature representations (<xref ref-type="bibr" rid="c109">Simmons and Barsalou, 2003</xref>; <xref ref-type="bibr" rid="c79">Meyer and Damasio, 2009</xref>). A perceived stimulus first causes modality-dependent activity in the related low-level region (e.g. visual cortex), subsequently higher-level convergence zones serve as relays that cause associated activity in other regions of the brain (e.g. the language network) (<xref ref-type="bibr" rid="c79">Meyer and Damasio, 2009</xref>; <xref ref-type="bibr" rid="c58">Kiefer and Pulvermüller, 2012</xref>). According to <xref ref-type="bibr" rid="c12">Binder (2016)</xref>, the most high-level convergence zones can become so abstract that they are representing amodal symbols.</p>
<p>The <bold>GRAPES framework</bold> (Grounding Representations in Action, Perception, and Emotion Systems) also suggests that representations are distributed across temporal and parietal areas of the cortex. More specifically, they are hypothesized to be situated in areas connected to the perception and manipulation of the environment, as well as in the language system (<xref ref-type="bibr" rid="c75">Martin, 2009</xref>, <xref ref-type="bibr" rid="c76">2016</xref>). According to this theory, conceptual knowledge is organized in domains: For example, semantic information related to object form and object motion is represented within specific visual processing systems, regardless of the stimulus modality, and both for perception as well as imagination.</p>
<p>The <bold>hub-and-spoke theory</bold> states that cross-modal interactions are mediated by a single modalityinvariant hub, located in the anterior temporal lobes (<xref ref-type="bibr" rid="c102">Rogers et al., 2004</xref>; <xref ref-type="bibr" rid="c64">Lambon Ralph et al., 2006</xref>; <xref ref-type="bibr" rid="c90">Patterson and Lambon Ralph, 2016</xref>; <xref ref-type="bibr" rid="c99">Ralph et al., 2017</xref>). The hub contains a “continuous distributed representation space that expresses conceptual similarities among items even though its dimensions are not independently interpretable” (<xref ref-type="bibr" rid="c41">Frisby et al., 2023</xref>, p. 262). The spokes form the links between the hubs and the modality-dependent association cortices. Most importantly, semantic representations are not solely based in the hub, for a given concept all spokes that are linked to modalities in which the concept can be experienced do contribute to the semantic representation. This explains why selective damage to spokes can cause category-specific deficits (<xref ref-type="bibr" rid="c94">Pobric et al., 2010</xref>).</p>
<p>This is conceptually similar to some aspects of the <bold>Global Workspace Theory</bold>, which assumes both a multimodal convergence of inputs towards a specific (network of) region(s), and the possibility of flexibly recruiting unimodal regions into this Global Workspace (<xref ref-type="bibr" rid="c6">Baars, 1993</xref>, <xref ref-type="bibr" rid="c7">2005</xref>).</p>
<p>While these and other theories partly disagree on <italic>how</italic> modality-invariant information is represented in the brain, they agree that such information is distributed across the cortex, and possibly overlapping with the semantic network (<xref ref-type="bibr" rid="c13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="c50">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="c4">Andrews et al., 2014</xref>; <xref ref-type="bibr" rid="c133">Zwaan, 2016</xref>).</p>
</sec>
<sec id="s2b">
<title>Decoding of vision and language from fMRI</title>
<p>Early approaches of brain decoding focused on identifying and reconstructing limited sets of simple visual stimuli (<xref ref-type="bibr" rid="c48">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="c21">Cox and Savoy, 2003</xref>; <xref ref-type="bibr" rid="c57">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="c83">Naselaris et al., 2009</xref>; <xref ref-type="bibr" rid="c85">Nishimoto et al., 2011</xref>). Soon after, attempts to decode linguistic stimuli could identify single words and short paragraphs with the help of models trained to predict features extracted from word embeddings (<xref ref-type="bibr" rid="c93">Pereira et al., 2018</xref>).</p>
<p>More recently, large-scale open source fMRI datasets for both vision and language have become available (<xref ref-type="bibr" rid="c20">Chang et al., 2019</xref>; <xref ref-type="bibr" rid="c3">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="c104">Schoffelen et al., 2019</xref>; <xref ref-type="bibr" rid="c120">Tang et al., 2023b</xref>) and allowed for the training of decoding models for a larger range and more complex naturalistic stimuli with the help of features extracted from deep learning models. For example, modality-specific decoders for vision can be trained by mapping the brain activity of subjects viewing naturalistic images to feature representation spaces of computational models of the same modality (i.e. vision models) (<xref ref-type="bibr" rid="c106">Shen et al., 2019</xref>; <xref ref-type="bibr" rid="c10">Beliy et al., 2019</xref>; <xref ref-type="bibr" rid="c68">Lin et al., 2022</xref>; <xref ref-type="bibr" rid="c118">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="c89">Ozcelik and VanRullen, 2023</xref>). Moreover, a range of studies provided evidence that certain representations can transfer between vision and language by evaluating decoders in a modality that they were not trained on (<xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c74">Man et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c108">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref>). The performance in such <italic>cross-modal decoding</italic> evaluations always lags behind when compared to within-modality decoding. One explanation is that modality-specific decoders are not explicitly encouraged to pick up on modality-invariant features during training, and modality-dependent features do not transfer to other modalities (An exception is the case of modality-specific decoders that are trained using features from the other modality. Such decoders might also rely on modality-invariant features, however, these might be less easily decodable than the modality-dependent features.).</p>
<p>To address this limitation, we here propose to directly train <italic>modality-agnostic decoders</italic>, i.e. models that are exposed to multiple stimulus modalities during training in order to make it more likely that they are leveraging representations that are modality-invariant. Training this kind of decoder is enabled by the multimodal nature of our fMRI dataset: The stimuli are taken from COCO, a multimodal dataset of images with associated descriptive captions (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>). During the experiment the subjects are exposed to stimuli in both modalities (images and captions) in separate trials. The modality-agnostic decoders learn to map brain activity associated with a specific stimulus independent of its presentation modality (image or caption) to high-level features extracted from a deep learning model. After training, a single modality-agnostic decoder can be used to decode stimuli from multiple modalities, leveraging representations that are invariant to the stimulus modality.</p>
</sec>
<sec id="s2c">
<title>Decoding of mental imagery</title>
<p>Apart from decoding perceived stimuli, it is also possible to decode representations when subjects were performing mental imagery, without being exposed to any perceptual input.</p>
<p>Different theories on mental imagery processes emphasize either the role of the early visual areas (<xref ref-type="bibr" rid="c60">Kosslyn et al., 1999</xref>; <xref ref-type="bibr" rid="c91">Pearson, 2019</xref>) or the role of the high-level visual areas in the ventral temporal cortex and frontoparietal networks (<xref ref-type="bibr" rid="c113">Spagna et al., 2021</xref>; <xref ref-type="bibr" rid="c46">Hajhajate et al., 2022</xref>; <xref ref-type="bibr" rid="c71">Liu et al., 2025</xref>). There is evidence for both kinds of theories in the form of neuroimaging studies that used decoding to identify stimuli during mental imagery. Some of these found relevant patterns in the early visual cortex (<xref ref-type="bibr" rid="c1">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="c82">Naselaris et al., 2015</xref>), others highlighted the role of higherlevel areas in the ventral visual processing stream (<xref ref-type="bibr" rid="c117">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c100">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="c125">VanRullen and Reddy, 2019</xref>; <xref ref-type="bibr" rid="c14">Boccia et al., 2019</xref>) as well as the precuneus and the intraparietal sulcus (<xref ref-type="bibr" rid="c54">Johnson and Johnson, 2014</xref>). These discrepancies can possibly be explained by differences in experimental design: For example, the early visual cortex might only become involved if the task requires the imagination of high-resolution details, which are represented in lower levels of the visual processing hierarchy (<xref ref-type="bibr" rid="c61">Kosslyn and Thompson, 2003</xref>).</p>
<p>Crucially, it has been shown that decoders trained exclusively on trials <italic>with</italic> perceptual input can generalize to imagery trials (<xref ref-type="bibr" rid="c117">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c100">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="c54">Johnson and Johnson, 2014</xref>; <xref ref-type="bibr" rid="c82">Naselaris et al., 2015</xref>), providing evidence that representations formed during perception overlap to some degree with representations formed during mental imagery (<xref ref-type="bibr" rid="c28">Dijkstra et al., 2019</xref>).</p>
<p>In our study, we explored to what extent these findings hold true for more varied and complex stimuli. Following previous approaches, we use decoders trained exclusively on trials where subjects were viewing images and captions, and evaluated them on their ability to decode the imagery trials. We additionally hypothesized that mental imagery should be at least as conceptual as it is sensory and therefore primarily recruit modality-invariant representations. Consequently, modality-agnostic decoders should be ideally suited to decode mental imagery and outperform modality-specific decoders on that task.</p>
</sec>
<sec id="s2d">
<title>Localizing modality-invariant regions</title>
<p>The first evidence for the existence of modality-invariant regions came from observations of patients with lesions in particular cortical regions which lead to deficits in the retrieval and use of knowledge across modalities (<xref ref-type="bibr" rid="c128">Warrington and Shallice, 1984</xref>; <xref ref-type="bibr" rid="c127">Warrington and Mccarthy, 1987</xref>; <xref ref-type="bibr" rid="c42">Gainotti, 2000</xref>; <xref ref-type="bibr" rid="c23">Damasio et al., 2004</xref>). Semantic impairments across modalities have also been observed in patients with the neurodegenerative disorder semantic dementia (<xref ref-type="bibr" rid="c126">Warrington, 1975</xref>; <xref ref-type="bibr" rid="c112">Snowden et al., 1989</xref>; <xref ref-type="bibr" rid="c51">Jefferies et al., 2009</xref>).</p>
<p>In early work exploring the possible locations of modality-invariant regions in healthy subjects, brain activity was recorded using imaging techniques while they were presented with a range of concepts in two modalities (e.g. words and pictures). Regions that were active during semantic processing of stimuli in the first modality were compared to regions that were active during semantic processing of stimuli in the second modality. The conjunction of these regions was proposed to be modality-invariant (<xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c80">Moore and Price, 1999</xref>; <xref ref-type="bibr" rid="c18">Bright et al., 2004</xref>).</p>
<p>While this methodology allows for the identification of candidate regions in which semantic processing of multiple modalities occurs, it can not be used to probe the information represented in these regions. In order to compare the information content (i.e. multivariate patterns) of brain regions, researchers have developed Representational Similarity Analysis (RSA, <xref ref-type="bibr" rid="c63">Kriegeskorte et al., 2008</xref>) as well as encoding and decoding analyses (<xref ref-type="bibr" rid="c81">Naselaris et al., 2011</xref>). More specifically, RSA has been used to find modality-invariant regions by comparing activation patterns of a candidate region when subjects are viewing stimuli from different modalities (<xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>; <xref ref-type="bibr" rid="c73">Liuzzi et al., 2017</xref>). This comparison is performed in an indirect way, by measuring the correlation of dissimilarity matrices of activation patterns. In turn, cross-decoding analysis can be leveraged to identify modality-invariant regions by training a classifier to predict the category of a stimulus in a given modality, and then evaluating its performance to predict the category of stimuli that were presented in another modality (<xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c74">Man et al., 2012</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c108">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c55">Jung et al., 2018</xref>). However, all these studies relied on a predefined set of stimulus categories, and can therefore not be easily extended to more realistic and complex stimuli, as we perceive them in our everyday life.</p>
<p>We summarize candidates for modality-invariant regions that have been identified by previous studies in Appendix 4. This overview reveals substantial disagreement regarding the possible locations of modality-invariant patterns in the brain. For example, <xref ref-type="bibr" rid="c32">Fairhall and Caramazza (2013)</xref> found modality-invariant representations in the left ventral temporal cortex (fusiform, parahippocampal, and perirhinal cortex), middle and inferior temporal gyrus, angular gyrus, parts of the prefrontal cortex as well as the precuneus. <xref ref-type="bibr" rid="c107">Shinkareva et al. (2011)</xref> found a larger network of left-lateralized regions, including additionally the left superior temporal, inferior parietal, supramarginal, inferior and inferior occipital, precentral and postcentral gyrus, supplementary motor area, intraparietal sulcus, cuneus, posterior cingulum as well as the right fusiform gyrus and the superior parietal gyrus, paracentral lobule on both hemispheres. In contrast, <xref ref-type="bibr" rid="c55">Jung et al. (2018)</xref> found modality-invariant representations only in the right prefrontal cortex. These diverging results can probably be explained by the limited number of stimuli as well as the use of artificially constructed stimuli in certain studies.</p>
<p>Recent advances in machine learning have enabled another generation of fMRI analyses based on large-scale naturalistic datasets. Here, we present a new multimodal dataset of subjects viewing both images and text. Most importantly, the dataset contains a large number of naturalistic stimuli in the form of complex visual scenes and full sentence descriptions of the same type of complex scenes, instead of pictures of single objects and words as commonly used in previous studies. This data enables the development of modality-agnostic decoders that are explicitly trained to leverage features that are shared across modalities. Further, we use this data to localize modality-invariant regions in the brain by applying decoders in a multimodal searchlight analysis.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Methods</title>
<sec id="s3a">
<title>fMRI Experiment</title>
<p>Six subjects (3 female, age between 22 and 47 years, all right-handed and fluent English speakers) participated in the experiment after providing informed consent. The study was performed in accordance with French national ethical regulations (Comité de Protection des Personnes, ID 2019-A01920-57). We collected functional MRI data using a 3T Philips ACHIEVA scanner (gradient echo pulse sequence, TR=2s, TE=30ms, 46 slices with a 32-channel head coil, slice thickness=3mm with 0.2mm gap, in-plane voxel dimensions 3×3mm). At the start of each session, we further acquired high-resolution anatomical images for each subject (voxel size=1mm<sup>3</sup>, TR=8.13ms, TE=3.74ms, 170 sagittal slices).</p>
<p>Scanning was spanned over 10 sessions (except for sub-01: 11 sessions), each consisting of 13 to 16 runs during which the subjects were presented with 86 stimuli. Each run started and ended with an 8s fixation period. The stimulus type varied randomly inside each run between images and captions. Each stimulus was presented for 2.5 seconds at the center of the screen (visual angle: 14.6 degrees; captions were displayed in white on a dark gray background (font: “Consolas”), the inter-stimulus interval was 1s. Every 10 stimuli there was a fixation trial that lasted for 2.5s. Every 5min there was a longer fixation trial for 16s.</p>
<p>Subjects performed a one-back matching task: They were instructed to press a button whenever the stimulus matched the immediately preceding one (cf. <xref rid="fig1" ref-type="fig">Figure 1</xref> Panel A). In case the previous stimulus was of the same modality (e.g. two captions in a row), the subjects were instructed to press a button if the stimuli matched exactly. In the cross-modal case (e.g. an image followed by a caption), the button had to be pressed if the caption was a valid description of the image, and vice versa. Positive one-back trials occurred on average every 10 stimuli.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p><bold>Panel A:</bold> Setup of the main fMRI experiment. Subjects were seeing images and captions in random alternation. Whenever the current stimulus matched the previous stimulus, the subjects were instructed to press a button (one-back matching task). Images and captions for illustration only; actual size and stimuli as described in the text. <bold>Panel B:</bold> Number of distinct training stimuli (excluding trials that were one-back targets or during which the subject pressed the response button). The number of overlapping stimuli indicates how many stimuli were presented both as caption and as image. There was an additional set of 140 stimuli (70 images and 70 captions) used for testing. <bold>Panel C:</bold> Setup of the fMRI experiment for the imagery trials. Subjects were instructed to remember 3 image descriptions with corresponding indices (numbers 1 to 3). One of these indices was displayed during the instruction phase, followed by a fixation phase, and then the subjects were imagining the visual scene for 10s.</p></caption>
<graphic xlink:href="658221v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Images and captions were taken from the training and validation sets of the COCO dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>). This dataset contains 5 matching captions for each image, of which we only considered the shortest one in order to fit on the screen and to ensure a comparable length for all captions. Spelling errors were corrected manually. As our training set, a random subset of images and another random subset of captions were selected for each subject. All these stimuli were presented only a single time (Except for the case of one-back target trials, for which the second presentation was however excluded during the preprocessing of the data.). Information on the number of training stimuli for each subject is shown in <xref rid="fig1" ref-type="fig">Figure 1</xref> Panel B. Additionally, a shared subset of 140 stimuli (70 images and 70 captions) was presented repeatedly to each subject in order to reduce noise, serving as our test set (on average: 26 times, min: 22, max: 31; Contrary to the training stimuli which were randomly selected from the COCO dataset, the 70 test stimuli were chosen by hand to avoid including multiple scenes that could match the same semantic description. The 70 chosen images as well as their 70 corresponding captions constituted the test set.). These stimuli were inserted randomly between the training stimuli.</p>
<p>Note that for each stimulus presented to the subject (e.g. an image), we also have access to the corresponding stimulus in the other modality (the corresponding caption from the COCO dataset), allowing us to estimate model features based on both modalities (vision model features extracted from the image and language model features extracted from the corresponding caption) as well as multimodal features extracted from both the image and the caption.</p>
<p>In addition to these perceptual trials, there were 3 imagery trials for each subject (see also <xref rid="fig1" ref-type="fig">Figure 1</xref> Panel C). Prior to the first fMRI scanning session, each subject was presented with a set of 20 captions (manually selected to be diverse and easy to visualize) that were not part of the perceptual trials, and they selected 3 captions for which they felt comfortable imagining a corresponding image. Then, they learned a mapping of each caption to a number (1, 2, and 3) so that they could be instructed to perform mental imagery of a specific stimulus, without having to present them with the caption again. The imagery trials occurred every second run, either at the beginning or the end of the run, so that each of the 3 imagery conditions were repeated on average 26 times (min: 23, max: 29). At the start of the imagery trial, the imagery instruction number was presented for 2s, then there was a 1s fixation period followed by the actual imagery period during which a light gray box was depicted for 10s on a dark gray background (the same background that was also used for trials with perceptual input). The light gray box was meant to represent the area in which the mental image should be “projected”. At the end of the experiment, the subjects drew sketches of the images they had been imagining during the imagery trials.</p>
<p>We report dataset quality metrics on head motion and intersession alignment in Appendix 1.</p>
</sec>
<sec id="s3b">
<title>fMRI Preprocessing</title>
<p>Preprocessing of the fMRI data was performed using SPM12 (<xref ref-type="bibr" rid="c5">Ashburner et al., 2014</xref>) via nipype (<xref ref-type="bibr" rid="c44">Gorgolewski et al., 2011</xref>). We applied slice time correction and realignment for each subject. Each session was coregistered with an anatomical scan of the respective subject’s first session (downsampled to 2mm<sup>3</sup>). We created and applied explicit gray matter masks for each subject based on their anatomical scans using a maximally lenient threshold (probability&gt;0).</p>
<p>In order to obtain beta-values for each stimulus, for each subject we fit a GLM (using SPM12) on data from all sessions. We included regressors for train images, train captions, test images, test captions, imagery trials, fixations, blank screens, button presses, and one-back target trials. One-back target trials as well as trials in which the participant pressed the button were excluded in the calculation of all training and test stimulus betas. As output of these GLMs we obtained beta-values for each training and test caption and image as well as the imagery trials.</p>
<p>Finally, we transformed the volume-space data to surface space Freesurfer (<xref ref-type="bibr" rid="c39">Fischl, 2012</xref>). We used trilinear interpolation and the fsaverage template in the highest possible resolution (163,842 vertices on each hemisphere) as target.</p>
</sec>
<sec id="s3c">
<title>Modality-Agnostic Decoders</title>
<p>The multimodal nature of our dataset allowed for the training of modality-agnostic decoders. We trained decoders by fitting ridge regression models that take fMRI beta-values as input and predict latent representations extracted from a pretrained deep learning model. Further details on decoder training can be found in <xref rid="fig2" ref-type="fig">Figure 2</xref> as well as Appendix 2.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Training of modality-specific and modality-agnostic decoders.</title>
<p><bold>Panel A:</bold> Modality-specific decoders are trained on fMRI data of one modality (e.g. subjects viewing images) by mapping it to features extracted from the same stimuli. <bold>Panel B:</bold> Modality-agnostic decoders are trained jointly on fMRI data of both modalities (subjects viewing images and captions). <bold>Panel C:</bold> To train decoders, features can be either extracted unimodally from the corresponding images or captions, or by creating multimodal features based on both modalities. For example, to train a modality-agnostic decoder based on features from a unimodal language model, we map the fMRI data of subjects viewing captions to features extracted from the respective captions using this language model, as well as the fMRI data of subjects viewing images to features extracted by the language model from the corresponding captions. We can also train modality-specific decoders on features from another modality, for example by mapping fMRI data of subjects viewing images to features extracted from the corresponding captions using a language model (cf. crosses on orange bars in <xref ref-type="fig" rid="fig4">Figure 4</xref> or using multimodal features (cf. crosses on blue bars in <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p></caption>
<graphic xlink:href="658221v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>While modality-specific decoders are trained only on brain imaging data of a single modality, modality-agnostic decoders are trained on brain imaging data from multiple modalities and therefore allow for decoding of stimuli irrespective of their modality.</p>
<p>More specifically, in our case the modality-specific decoders are trained on fMRI beta-values from one stimulus modality, e.g. when subjects were watching images (cf. <xref rid="fig2" ref-type="fig">Figure 2</xref> panel A). Conversely, modality-agnostic decoders are trained jointly using fMRI data from both stimulus modalities (images and captions; cf. <xref rid="fig2" ref-type="fig">Figure 2</xref> panel B). For all decoders, the features that serve as regression targets can either be unimodal (e.g. extracted from images using a vision model) or multimodal (e.g. extracted from both stimulus modalities using a multimodal model, cf. <xref rid="fig2" ref-type="fig">Figure 2</xref> panel C).</p>
<p>We considered features extracted from a range of vision, language, and multimodal models: For vision features, we considered ResNet (<xref ref-type="bibr" rid="c49">He et al., 2016</xref>), ViT (<xref ref-type="bibr" rid="c30">Dosovitskiy et al., 2020</xref>), and DI-NOv2 (<xref ref-type="bibr" rid="c88">Oquab et al., 2023</xref>); for language features BERT (<xref ref-type="bibr" rid="c26">Devlin et al., 2019</xref>), GPT2 (<xref ref-type="bibr" rid="c98">Radford et al., 2019</xref>), Llama2 (<xref ref-type="bibr" rid="c122">Touvron et al., 2023</xref>), mistral and mixtral (<xref ref-type="bibr" rid="c53">Jiang et al., 2023</xref>). Regarding multimodal features, we extracted features from VisualBERT (<xref ref-type="bibr" rid="c67">Li et al., 2019</xref>), BridgeTower (<xref ref-type="bibr" rid="c130">Xu et al., 2023</xref>), ViLT (<xref ref-type="bibr" rid="c59">Kim et al., 2021</xref>), CLIP (<xref ref-type="bibr" rid="c97">Radford et al., 2021</xref>), ImageBind (<xref ref-type="bibr" rid="c43">Girdhar et al., 2023</xref>), Flava (<xref ref-type="bibr" rid="c110">Singh et al., 2022</xref>), Blip2 (<xref ref-type="bibr" rid="c66">Li et al., 2023</xref>), SigLip (<xref ref-type="bibr" rid="c131">Zhai et al., 2023</xref>), and Paligemma2 (<xref ref-type="bibr" rid="c115">Steiner et al., 2024</xref>). In order to estimate the effect of model training, we further extracted features from a randomly initialized ImageBind model as a baseline. Further details on feature extraction can be found in Appendix 2. All decoders were evaluated on the held-out test data (140 stimuli, 70 captions and 70 images) using pairwise accuracy calculated using cosine distance. Prior to calculating the pairwise accuracy, the model predictions for all stimuli were standardized to have mean of 0 and standard deviation of 1 (In the case of imagery decoding, the model predictions were standardized separately.). In the case of cross-modal decoding (e.g. mapping an image stimulus into the latent space of a language model), a trial was counted as correct if the caption corresponding to the image (according to the ground-truth in COCO) was closest.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> provides an overview on the evaluation metrics. A modality-specific decoder for images can be evaluated on its ability to decode images (Panel A, top) and in a cross-decoding setup for captions (Panel B, bottom). In the same way, we can compute the respective evaluation metrics for modality-specific decoders trained on captions. For the case of modality-agnostic decoders, we evaluate performance for decoding both images and captions using the same single decoder that is trained on both modalities (Panel C).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Evaluation of modality-specific and modality-agnostic decoders.</title>
<p>The matrices display cosine similarity scores between features extracted from the candidate stimuli and features predicted by the decoder. The evaluation metric is pairwise accuracy, which is calculated row-wise: For a given matrix row, we compare the similarity score of the target stimulus on the diagonal (in green) with the similarity scores of all other candidate stimuli (in red). <bold>Panel A:</bold> Within-modality decoding metrics of modality-specific decoders. To compute within-modality accuracy for image decoding, a modality-specific decoder trained on images is evaluated on all stimuli that were presented as images. To compute within-modality accuracy for caption decoding, a modality-specific decoder trained on captions is evaluated on all caption stimuli. <bold>Panel B:</bold> Cross-modality decoding metrics of modality-specific decoders. To compute cross-modality accuracy for image decoding, a modality-specific decoder trained on captions is evaluated on all stimuli that were presented as images. To compute cross-modality accuracy for caption decoding, a modality-specific decoder trained on images is evaluated on all caption stimuli. <bold>Panel C:</bold> Metrics for modality-agnostic decoders. To compute modality-agnostic accuracy for image decoding, a modality-agnostic decoder is evaluated on all stimuli that were presented as images. The same decoder is evaluated on caption stimuli to compute modality-agnostic accuracy for caption decoding. Here we show feature extraction based on unimodal features for modality-specific decoders and based on multimodal features for the modality-agnostic decoder, in practice the feature extraction can be unimodal or multimodal for any decoder type (see also <xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></caption>
<graphic xlink:href="658221v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<sec id="s4a">
<title>Modality-Agnostic Decoders</title>
<p>We first compared the performance of modality-specific and modality-agnostic decoders that are trained on the whole brain fMRI data based on different unimodal and multimodal features. The average pairwise accuracy scores are presented in <xref rid="fig4" ref-type="fig">Figure 4</xref>. <xref rid="fig5" ref-type="fig">Figure 5</xref> presents pairwise accuracy scores separately for decoding images and for decoding captions. Results for individual subjects can be found in Appendix 6.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Average decoding scores for modality-agnostic decoders (green), compared to modality-specific decoders trained on data from subjects viewing images (orange) or on data from subjects viewing captions (purple).</title>
<p>The metric is pairwise accuracy (see also <xref ref-type="fig" rid="fig3">Figure 3</xref>). Error bars indicate 95% confidence intervals calculated using bootstrapping. Chance performance is at 0.5.</p></caption>
<graphic xlink:href="658221v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Decoding accuracy for decoding images (top) and for decoding captions (bottom).</title>
<p>The orange bars in the top row indicate within-modality decoding scores for images, the purple bars in the bottom row indicate within-modality decoding scores for captions. The purple bars in the top row indicate cross-decoding scores for images, the orange bars in the bottom row indicate cross-decoding scores for captions (see also <xref ref-type="fig" rid="fig3">Figure 3</xref>). Error bars indicate 95% confidence intervals calculated using bootstrapping. Chance performance is at 0.5.</p></caption>
<graphic xlink:href="658221v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>When analyzing the average decoding accuracy (<xref rid="fig4" ref-type="fig">Figure 4</xref>), we find that modality-agnostic decoders perform better than modality-specific decoders. To support this observation, we performed two repeated measures ANOVAs (grouping the data by subject), once comparing the average accuracy values of modality-agnostic decoders (mean value: 81.86%) with those of modality-specific decoders trained on images (mean value: 78.15%), and once comparing modality-agnostic decoders to modality-specific decoders trained on captions (mean value: 72.52%). In both cases, the accuracy values for the two decoder types were significantly different. When comparing modality-agnostic decoders to modality-specific decoders trained on images: decoder_type : <italic>β</italic> = 0.037, <italic>SE</italic> = 0.004, <italic>p &lt;</italic> 1 ⋅ 10<sup>−18</sup>; and when comparing to modality-specific decoders trained on captions: decoder_type : <italic>β</italic> = 0.093, <italic>SE</italic> = 0.005, <italic>p &lt;</italic> 1 ⋅ 10<sup>−93</sup>. This high performance (which can be attributed to the large training dataset used by modality-agnostic decoders) is achieved despite the additional challenge of not knowing the modality of the stimulus the subject was seeing.</p>
<p>Further, we observed that modality-agnostic decoders based on the best multimodal features (imagebind: 85.71% ± 2.58%) do not perform substantially better than decoders based on the best language features (GPT2-large: 85.31% ± 2.83%) and only slightly better than decoders trained on the best vision features (Dino-giant: 82.02% ± 2.43%). This result suggests that high-performing modality-agnostic decoders do not necessarily need to rely on multimodal features; features extracted from language models can lead to equally high performance. When comparing the different architecture types of models for multimodal feature extraction (dual stream vs. single stream with early fusion vs. single stream with late fusion; cf. Panel C in <xref rid="fig2" ref-type="fig">Figure 2</xref>), we only observed a slight performance disadvantage for single-stream models with early fusion (Mean accuracy values for dual stream models: 85.04%; for single stream models with early fusion: 81.01%; for single stream models with late fusion 83.63%). We confirmed this observation with a repeated measures ANOVA (grouping the data by subject), comparing the decoding accuracy values of modalityagnostic decoders based on different families of multimodal features. The only significant effect was: model_family_single_stream_early_fusion : <italic>β</italic> = −0.04, <italic>SE</italic> = 0.011, <italic>p &lt;</italic> 1<italic>e</italic> − 3.</p>
<p>When analyzing the performance specifically for decoding images (<xref rid="fig5" ref-type="fig">Figure 5</xref>, top), we find that modality-agnostic decoders perform as well as the modality-specific decoders trained on images (green bars in the top row are at the same level as the orange bars in <xref rid="fig5" ref-type="fig">Figure 5</xref>) We found no statistically significant difference in their performances. A repeated measures ANOVA (grouping the data by subject), comparing the image decoding accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on images resulted in a p-value of <italic>p</italic> = 0.73 for the effect of the decoder_type.</p>
<p>Further, modality-agnostic decoders even outperform modality-specific decoders trained on captions for decoding captions (purple bars in the bottom row are lower than green bars in <xref rid="fig5" ref-type="fig">Figure 5</xref>). This was supported by a repeated measures ANOVA (grouping the data by subject), comparing the caption decoding accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on captions. The result was: decoder_type : <italic>β</italic> = 0.036, <italic>SE</italic> = 0.006, <italic>p &lt;</italic> 1 ⋅ 10<sup>−8</sup>. In other words, even if we know that a brain pattern was recorded in response to the subject reading a caption, we are more likely to decode it accurately if we choose to apply a decoder trained using both modalities, than if we apply the appropriate decoder, trained only on captions. Finally, we found that the cross-modal decoding performance for decoding visual stimuli (images) using decoders trained on captions (mean value: 73.31%) is higher than the cross-modal decoding performance in the other direction (mean value: 68.03%), corroborating similar results from <xref ref-type="bibr" rid="c119">Tang et al. (2023a)</xref> on movies and audio books (purple bars in top row are higher than orange bars in bottom row in <xref rid="fig5" ref-type="fig">Figure 5</xref>). We performed a repeated measures ANOVA (grouping the data by subject) to compare the pairwise accuracy values of both cross-decoding directions and found that the difference was significant: decoder_type : <italic>β</italic> = 0.053, <italic>SE</italic> = 0.004, <italic>p &lt;</italic> 1 ⋅ 10<sup>−31</sup> (the decoder_type variable is indicating whether the decoder is trained on images or captions).</p>
<sec id="s4a1">
<title>Qualitative Decoding Results</title>
<p>To obtain a better understanding of the decoding performance of the modality-agnostic decoders, we inspected the decoding results for 5 randomly selected test stimuli. We created a large candidate set of 41,118 stimuli by combining the test stimuli and the training stimuli from all subjects. For each stimulus, we ranked these candidate set stimuli based on their similarity to the predicted feature vector. As the test stimuli were shared among all subjects, we could average the prediction feature vectors across subjects to obtain the best decoding results.</p>
<p><xref rid="fig6" ref-type="fig">Figure 6</xref> presents the results for decoding images using a modality-agnostic decoder trained on ImageBind features. We display the target stimulus along with the top-5 ranked candidate stimuli. We can observe some clear success cases (the train in the first row, a person eating pizza in the last row) but also failure cases (the teddy bear decoded as pizza). For the other stimuli, some aspects such as the high-level semantic class (e.g. vehicle, animal, sports) are correctly decoded: For the cars on the highway (5th row), the top-ranked images depict trains, which are also vehicles. For the dog (2nd row), the top images contain cats of similar colors. The footballer in the 6th row is decoded as a tennis player, the horses of the 7th row are decoded as zebras. Regarding the giraffe (3rd row), the model appears to have picked up on the fact that there was a body of water depicted in the image.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Decoding examples for image decoding using a modality-agnostic decoder.</title>
<p>The first column shows the image the subject was seeing and the 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Note that these qualitative results are not directly comparable with previous work on retrieval or reconstruction using the NSD dataset (<xref ref-type="bibr" rid="c3">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="c68">Lin et al., 2022</xref>; <xref ref-type="bibr" rid="c118">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="c89">Ozcelik and VanRullen, 2023</xref>), as our data was collected on a 3T MRI scanner with lower signal-to-noise-ratio than NSD’s 7T MRI scanner.</p>
<p>The ranking results for decoding captions are depicted in <xref rid="fig7" ref-type="fig">Figure 7</xref>. The results are somewhat similar to the image decoding results, stimuli that were decoded successfully when presented as image such as the train are also decoded successfully when presented as caption; cases that were failures in the case of image decoding (e.g. the teddy bear) also fail here. However, the top-ranked stimuli for the dog (2nd row) do not always contain animals (the decoder seems to have picked up on the presence of a vehicle in the caption), and the horses of the 7th row do not get decoded correctly. The cars on the highway (5th row) get decoded more accurately than during image decoding.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Decoding examples for caption decoding using a modality-agnostic decoder.</title>
<p>For details see caption of <xref ref-type="fig" rid="fig6">Figure 6</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We additionally provide qualitative results for modality-specific decoders in Appendix 3. These results generally reflect the observations from the quantitative results: Modality-agnostic decoders perform similarly to modality-specific decoders evaluated in a within-modality decoding setup, but substantially better than modality-specific decoders when evaluated in cross-decoding setups.</p>
</sec>
</sec>
<sec id="s4b">
<title>Modality-Invariant Regions</title>
<p>Modality-agnostic decoders perform best when they can leverage modality-invariant representations. To provide insight into the spatial organization of such modality-invariant representations in the brain, we performed a surface-based searchlight analysis.</p>
<p>Modality-invariant regions should contain patterns that generalize between stimulus modalities. Therefore, such regions should allow for decoding of stimuli in both modalities using a decoder that is trained to pick up on modality-invariant features, i.e. the decoding performance for images and captions of a modality-agnostic decoder should both be above chance. However, as a modality-agnostic decoder is trained on stimuli from both modalities, it could have learned to leverage certain features to project stimuli from one modality and different features to project stimuli from the other modality. We added two conditions to control that the representations directly transfer between the modalities by additionally training two modality-specific decoders and evaluating them according to their cross-decoding performance, i.e. we require that their decoding performance in the modality they were not trained on is above chance. These four conditions are summarized at the top of <xref rid="fig8" ref-type="fig">Figure 8</xref>.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Searchlight method to identify modality-invariant ROIs.</title>
<p>The top plots show performance (pairwise accuracy averaged over subjects) of modality-agnostic decoders for decoding images (top left) and decoding captions (top right). In the second row, we display cross-decoding performances: On the left, modality-specific decoders trained on captions are evaluated on images. On the right, modality-specific decoders trained on images are evaluated on captions. We identified modality-invariant ROIs as clusters in which all 4 decoding accuracies are above chance by taking the minimum of the respective t-values at each location, then performed TFCE to calculate cluster values. The plot only shows left medial views of the brain to illustrate the method, different views of all resulting clusters are shown in <xref ref-type="fig" rid="fig9">Figure 9</xref>.</p></caption>
<graphic xlink:href="658221v2_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We used ImageBind features for these searchlight analyses as they led to the highest decoding performance when using the whole brain data. The decoders were trained based on the surface projection of the fMRI beta-values. For each vertex, we defined a searchlight with a fixed size by selecting the 750 closest vertices, corresponding to an average radius of ∼ 9.4mm (Details on how this size was selected are outlined in Appendix 5.).</p>
<p>We trained and evaluated a modality-agnostic decoder and modality-specific decoders for both modalities on the beta-values for each searchlight location and each subject, providing us with a decoding accuracy scores for each location on the cortex. Then we performed one-tailed t-tests to identify locations in which the decoding performance is above chance (<italic>acc &gt;</italic> 0.5). We aggregated all 4 comparisons by taking the minimum of the 4 t-values at each spatial location. Finally, we performed threshold-free cluster enhancement (TFCE, <xref ref-type="bibr" rid="c111">Smith and Nichols, 2009</xref>) to identify modalityinvariant ROIs (<xref rid="fig8" ref-type="fig">Figure 8, bottom</xref>; We used the default hyperparameters of <italic>h</italic> = 2 and <italic>e</italic> = 1 for surface-based TFCE (<xref ref-type="bibr" rid="c52">Jenkinson et al., 2012</xref>).).</p>
<p>To estimate the statistical significance of the resulting clusters we performed a permutation test, combined with a bootstrapping procedure to estimate a group-level null distribution (see also <xref ref-type="bibr" rid="c116">Stelzer et al., 2013</xref>). For each subject, we evaluated the decoders 100 times with shuffled labels to create per-subject chance-level results. Then, we randomly selected one of the 100 chance-level results for each of the 6 subjects and calculated group-level statistics (TFCE values) the exact same way as described in the preceding paragraph. We repeated this procedure 10,000 times resulting in 10,000 permuted group-level results. We ensured that every permutation was unique, i.e. no two permutations were based on the same combination of selected chance-level results. Based on this null distribution, we calculated p-values for each vertex by calculating the proportion of sampled permutations where the TFCE value was greater than the observed TFCE value. To control for multiple comparisons across space, we always considered the maximum TFCE score across vertices for each group-level permutation (<xref ref-type="bibr" rid="c111">Smith and Nichols, 2009</xref>).</p>
<p>The results of the surface-based searchlight analysis are presented in <xref rid="fig9" ref-type="fig">Figure 9</xref>. The analysis revealed that modality-invariant patterns are actually widespread across the brain, especially on the left hemisphere. Peak cluster values were found in the left supramarginal gyrus, inferior parietal gyrus and posterior superior temporal sulcus. Regions belonging to the precuneus, isthmus of the cingulate gyrus, parahippocampus, middle temporal gyrus, inferior temporal gyrus and fusiform gyrus also showed high cluster values.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Searchlight results for modality-invariant regions.</title>
<p>Maps thresholded at TFCE value of 1508, which is the significance threshold value for which <italic>p &lt;</italic> 10<sup>−4</sup> based on the permutation testing. Regions with highest cluster values are outlined and annotated based on the Desikan-Killiany atlas (<xref ref-type="bibr" rid="c24">Desikan et al., 2006</xref>).</p></caption>
<graphic xlink:href="658221v2_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s4c">
<title>Imagery Decoding</title>
<p>Finally, we evaluated the ability of decoders trained on the fMRI data with perceptual input to decode stimuli during the imagery conditions.</p>
<p>For a modality-agnostic decoder trained on the whole-brain data, the imagery pairwise decoding accuracy reaches 84.48% (averaged across subjects and model features) when using the 3 imagery stimuli as the candidate set (Note that we used the ground-truth caption and corresponding image from COCO in this candidate set, and not the sketches drawn by the subjects.). When the whole test set is added to the candidate set (in total: 73 stimuli), the average pairwise accuracy drops to 72.47%. This substantial drop in performance is most likely explained by the fact that the predicted features for the imagery trials were standardized using only 3 stimuli, and this transformation emphasized differences that enabled distinguishing the 3 imagery trials but do not generalize to the larger test set (We also attempted decoding without standardization of the predicted feature vectors, but this led to much lower performance.).</p>
<p>As expected, we also found that modality-agnostic decoders are better suited for imagery decoding than modality-specific decoders. We compared the imagery decoding accuracy of both decoder types taking into account the results for all features and all subjects. To this end, we performed two repeated measures ANOVAs (grouping the data by subject), once comparing the accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on images, and once comparing modality-agnostic decoders to modality-specific decoders trained on captions. The average decoding accuracies were 69.42% for a modality-specific decoder trained on images and 70.02% for a modality-specific decoder trained on captions (vs. 72.47% for a modality-agnostic decoder, as mentioned above). In both comparisons, the accuracy values for the two decoder types were significantly different (when comparing modality-agnostic decoders to modality-specific decoders trained on images: decoder_type : <italic>β</italic> = 0.03, <italic>SE</italic> = 0.011, <italic>p &lt;</italic> 0.01; and when comparing to modality-specific decoders trained on captions: decoder_type : <italic>β</italic> = 0.024, <italic>SE</italic> = 0.012, <italic>p &lt;</italic> 0.04).</p>
<p>Appendix 7 presents qualitative decoding results for the imagery trials for each subject as well as the sketches of the mental images drawn at the end of the experiment. As expected, the results are worse than those for perceived stimuli, but for several subjects it was possible to decode some major semantic concepts.</p>
<p>We further computed the imagery decoding accuracy during the searchlight analysis. <xref rid="fig10" ref-type="fig">Figure 10</xref> shows the result clusters for decoding imagery (using the whole test set + the 3 imagery trials as potential candidates). To assess statistical significance, we employed the same permutation testing procedure with bootstrapping as used for identifying modality-invariant regions (10,000 permutations based on random combinations of 100 chance distributions for each subject).</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Searchlight results for imagery decoding.</title>
<p>Maps thresholded at TFCE values that surpass the significance threshold of <italic>p &lt;</italic> 10<sup>−4</sup> based on a permutation test. Maps thresholded at TFCE value of 3897, which is the significance threshold value for which <italic>p &lt;</italic> 10<sup>−4</sup> based on the permutation testing. We used the pairwise accuracy for imagery decoding using the large candidate set of 73 stimuli. We outlined the same regions as in <xref ref-type="fig" rid="fig9">Figure 9</xref> to facilitate comparison.</p></caption>
<graphic xlink:href="658221v2_fig10.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We observe that many regions that were found to contain modality-invariant patterns (cf. <xref rid="fig9" ref-type="fig">Figure 9</xref>) are also regions in which decoding of mental imagery is possible.</p>
<p>One main difference is that the imagery decoding clusters appear to be less left-lateralized than the modality-invariant region clusters (peak cluster values can be found both on the right inferior parietal cortex and bilaterally in the precuneus). To estimate overlap of the regions allowing for imagery decoding and modality-invariant regions we calculated the correlation between the TFCE values that were used for identifying modality-invariant regions (<xref rid="fig9" ref-type="fig">Figure 9</xref>) and the TFCE values for imagery decoding (<xref rid="fig10" ref-type="fig">Figure 10</xref>). The Pearson correlation score for the left hemisphere is 0.41 (<italic>p &lt;</italic> 1<italic>e</italic> − 8), and for the right hemisphere 0.62 (<italic>p &lt;</italic> 1<italic>e</italic> − 8). Importantly, these correlation scores are substantially higher when compared to the correlation with decoding accuracy of modality-specific decoders: The correlation between the TFCE values for imagery decoding and TFCE values for image decoding of a modality-specific decoder trained on images is 0.28 on the left hemisphere and 0.40 on the right hemisphere. When using TFCE values based on the caption decoding accuracy of a modality-specific decoder trained on captions we obtain 0.19 on the left hemisphere and 0.45 on the right hemisphere.</p>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<p>In this work, we introduced modality-agnostic decoders, which are trained to decode stimuli from different modalities using a single model. Such modality-agnostic decoders are specifically trained to pick up on modality-invariant representations, enabling a performance increase over modality-specific decoders when decoding linguistic stimuli in the form of captions as well as when decoding mental imagery. Previous work on decoding with large-scale brain imaging datasets has largely focused on modality-specific cortices, such as the visual cortex for decoding natural scenes (<xref ref-type="bibr" rid="c106">Shen et al., 2019</xref>; <xref ref-type="bibr" rid="c10">Beliy et al., 2019</xref>; <xref ref-type="bibr" rid="c68">Lin et al., 2022</xref>; <xref ref-type="bibr" rid="c118">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="c89">Ozcelik and VanRullen, 2023</xref>). Our work demonstrates the advantages of leveraging higher-level representations that are shared between modalities, and which we found to be present in widespread areas of the human cortex. In the following, we will further discuss the nature and location of the modality-invariant representations in light of previous experimental and theoretical work.</p>
<p>The results of our searchlight analysis suggest that modality-invariant representations are found in temporal, parietal, and frontal regions. Peak cluster values are found on the border of the temporal and parietal cortices on the left hemisphere (cf. <xref rid="fig9" ref-type="fig">Figure 9</xref>). All areas with high cluster values confirm findings from previous studies: The left precuneus (<xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c95">Popham et al., 2021</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), posterior cingulate/ retrosplenial cortex (<xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), supramarginal gyrus (<xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>), inferior parietal cortex (<xref ref-type="bibr" rid="c74">Man et al., 2012</xref>; <xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c95">Popham et al., 2021</xref>; <xref ref-type="bibr" rid="c108">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), superior temporal sulcus (<xref ref-type="bibr" rid="c74">Man et al., 2012</xref>), middle temporal gyrus (<xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c25">Devereux et al., 2013</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), inferior temporal gyrus (<xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c108">Simanova et al., 2014</xref>; <xref ref-type="bibr" rid="c47">Handjaras et al., 2016</xref>), fusiform gyrus (<xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>; <xref ref-type="bibr" rid="c80">Moore and Price, 1999</xref>; <xref ref-type="bibr" rid="c18">Bright et al., 2004</xref>; <xref ref-type="bibr" rid="c107">Shinkareva et al., 2011</xref>; <xref ref-type="bibr" rid="c32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="c108">Simanova et al., 2014</xref>), and parahippocampus (<xref ref-type="bibr" rid="c124">Vandenberghe et al., 1996</xref>). However, previous studies have led to contradicting results regarding the locality of modality-invariant regions (they were identifying varying subsets of these regions; see also Appendix 4), probably due to the limited number and artificial nature of stimuli employed. Our method identified <italic>almost all</italic> of the previously proposed regions as regions with modality-invariant patterns, highlighting the advantage of our searchlight decoding approach and the large multimodal dataset in which subjects are viewing photographs of complex natural scenes and reading full English sentences (The left superior occipital gyrus was not identified in our study, but in previous studies by <xref ref-type="bibr" rid="c124">Vandenberghe et al. (1996)</xref>; <xref ref-type="bibr" rid="c107">Shinkareva et al. (2011)</xref>. However, we found that a major part of the left superior occipital <italic>sulcus</italic> represents modality-invariant information. Further, <xref ref-type="bibr" rid="c55">Jung et al. (2018)</xref> found modality-invariant patterns in the right superior frontal gyrus. One major difference between their study and ours is that they used auditory input as a second modality instead of text. Further work is required to investigate to what extent the modality-invariant regions identified in our work generalize to all modalities.).</p>
<p>According to a range of theories, modality-invariant representations are tightly linked to (lexical-) semantic representations (<xref ref-type="bibr" rid="c109">Simmons and Barsalou, 2003</xref>; <xref ref-type="bibr" rid="c13">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="c78">Meschke and Gallant, 2024</xref>). Most importantly, a range of studies that aimed to identify brain regions linked to semantic/conceptual representations by asking subjects to perform tasks that require semantic processing of words found evidence for such regions that overlap to a high degree with the regions identified in our study (<xref ref-type="bibr" rid="c37">Fernandino et al., 2016</xref>; <xref ref-type="bibr" rid="c77">Martin et al., 2018</xref>; <xref ref-type="bibr" rid="c19">Carota et al., 2021</xref>; <xref ref-type="bibr" rid="c38">Fernandino et al., 2022</xref>; <xref ref-type="bibr" rid="c121">Tong et al., 2022</xref>). A strong link between these systems could also explain our result that modality-agnostic decoders based on unimodal representations from language models are performing as well as decoders based on multimodal representations (cf. <xref rid="fig4" ref-type="fig">Figure 4</xref>), as well as the partial left-lateralization of the identified modality-invariant regions.</p>
<p>The fact that the presence of modality-invariant patterns is positively correlated with the imagery decoding performance in different locations provides further evidence that the identified patterns are truly modality-invariant (they are not only shared between vision and language, but also for mental imagery). We further found that decoders trained exclusively on data for which participants were exposed to perceptual input do generalize to imagery trials, confirming previous findings that were based on more limited stimulus sets (<xref ref-type="bibr" rid="c117">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="c100">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="c54">Johnson and Johnson, 2014</xref>; <xref ref-type="bibr" rid="c82">Naselaris et al., 2015</xref>). Moreover, we found that modality-agnostic decoders outperform modality-specific decoders in terms of imagery decoding, demonstrating another advantage of this new type of decoder explicitly trained to pick up modality-invariant patterns.</p>
<p>The findings of our searchlight analysis for imagery decoding suggest that mental imagery indeed involves a large network of regions across both hemispheres of the cerebral cortex. This includes high-level visual areas, parietal areas such as the precuneus and inferior parietal cortex and several frontal regions, but also parts of the early visual cortex. Results are highly similar on both hemispheres, highlighting the involvement of large-scale bilateral brain networks during mental imagery of complex scenes.</p>
<p>While there are lesion studies on hemispheric asymmetries that suggest that regions in the left hemisphere are crucial for mental imagery (<xref ref-type="bibr" rid="c33">Farah, 1984</xref>; <xref ref-type="bibr" rid="c9">Bartolomeo, 2002</xref>), a more recent review that additionally considers evidence from neuroimaging and direct cortical stimulation studies suggests that frontoparietal networks in both hemispheres are involved in mental imagery, and that lateralization patterns can be found in the temporal lobes (<xref ref-type="bibr" rid="c70">Liu et al., 2022</xref>). Such lateralization was found to depend on the nature of the imagined items, the imagination of objects and words involving the left inferior temporal cortex while the imagination of faces and people was found to be more right-lateralized and the imagination of complex scenes (as in our study) leads to significant activity in both hemispheres (<xref ref-type="bibr" rid="c86">O’Craven and Kanwisher, 2000</xref>; <xref ref-type="bibr" rid="c114">Steel et al., 2021</xref>; <xref ref-type="bibr" rid="c113">Spagna et al., 2021</xref>). Crucially, in more recent decoding studies, results were either observed bilaterally, or the analyses did not target hemispheric asymmetries (<xref ref-type="bibr" rid="c100">Reddy et al., 2010</xref>; <xref ref-type="bibr" rid="c65">Lee et al., 2012</xref>; But see <xref ref-type="bibr" rid="c117">Stokes et al. (2009)</xref> in which perception-to-imagery generalization of single letters was left-lateralized.). Our work shows for the first time results of a searchlight analysis of imagery decoding of complex visual scenes.</p>
<p>Above-chance decoding is possible on both hemispheres, with the highest decoding accuracies in the precuneus and the right inferior parietal cortex and the superior temporal sulcus. Future investigations with larger sets of imagined scenes could address the question of whether lateralization patterns depend on the nature of the imagined objects.</p>
<p>According to the results of our searchlight analysis, the anterior temporal lobes are not among the regions with the highest probability of being modality-invariant, contradicting the hypothesis of the hub-and-spoke theory that these areas are the major semantic hub in the brain. However, MRI signals from these regions have a lower signal-to-noise ratio with standard fMRI pulse sequences (<xref ref-type="bibr" rid="c27">Devlin et al., 2000</xref>; <xref ref-type="bibr" rid="c31">Embleton et al., 2010</xref>). A more targeted study with an adapted fMRI protocol would be required to shed light on the nature of patterns in these regions. More generally, the hub-and-spoke theory also puts emphasis on the role of spokes for the formation of conceptual representations (<xref ref-type="bibr" rid="c94">Pobric et al., 2010</xref>; <xref ref-type="bibr" rid="c99">Ralph et al., 2017</xref>). Future work could be aimed at testing the hub-and-spoke theory proposal that features in hierarchically lower level representation spaces of the spokes are not directly relatable to features in the representation space of the hubs: Object representations in the spokes are based on interpretable features (e.g. shape, color, affordances of an object) and get translated into another representational format that is representing conceptual similarities (but its dimensions do not directly map to interpretable features) in the semantic hub (<xref ref-type="bibr" rid="c41">Frisby et al., 2023</xref>). To test this hypothesis, modality-invariant representations in the anterior temporal lobes (measured with targeted fMRI pulse sequences) could be compared to representations in candidate regions for modality-specific spokes using RSA.</p>
<p>The modality-invariant regions we found in the searchlight analysis can also be seen as candidates for convergence zones, in which increasingly abstract representations are formed (<xref ref-type="bibr" rid="c22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="c123">Tranel et al., 1997</xref>; <xref ref-type="bibr" rid="c79">Meyer and Damasio, 2009</xref>). To obtain further insight into the hierarchical organization of these zones, future work could take advantage of the improved temporal resolution of other brain imaging techniques such as MEG to explore in which areas modality-invariant patterns are formed first, and how they are being transformed when spreading to higher-level areas of the brain (<xref ref-type="bibr" rid="c29">Dirani and Pylkkänen, 2024</xref>; <xref ref-type="bibr" rid="c11">Benchetrit et al., 2024</xref>).</p>
<p>In line with the GRAPES framework, (<xref ref-type="bibr" rid="c75">Martin, 2009</xref>, <xref ref-type="bibr" rid="c76">2016</xref>), we found that modality-invariant representations are distributed across temporal and parietal areas. To test the related hypothesis that conceptual information is organized in domains, we plan to use RSA to understand which kind of semantic information is represented in the different modality-invariant regions identified.</p>
<p>Finally, our results can be interpreted with respect to the Global Workspace Theory. All modality-invariant regions are good candidate regions for a global workspace. They could, however, also be part of modality-specific modules that get activated in a modality-invariant fashion through a “broadcast” operation as a stimulus is perceived consciously (<xref ref-type="bibr" rid="c6">Baars, 1993</xref>, <xref ref-type="bibr" rid="c7">2005</xref>). To distinguish these two cases, an experimental manipulation of attention could be used: according to Global Workspace Theory, attention is required for information to enter the workspace, but not for the workspace signals to reach other brain regions via broadcast. In the future, we plan to investigate how modality-invariant patterns are modulated by attention, by analyzing additional test sessions from the same subjects in which they were instructed in specific runs to pay attention to only one of the modalities (These sessions will be released as part of another future dataset and publication.).</p>
<p>To conclude, the results from our searchlight analysis so far are in line with all major theories on modality-invariant representations that were considered. As the dataset that this study was based on will be shared publicly, more targeted investigations can be performed by the research community in order to adjudicate between different theories.</p>
<sec id="s5a">
<title>Limitations</title>
<p>There are some important limitations of this study, which are mainly consequences of some design choices to enable large-scale data collection. First, the fMRI experiment relied on a one-back task, which required subjects to remember preceding stimuli, and therefore, brain activity in each trial could be influenced by the previous stimuli kept in working memory. Such unrelated activity could have impaired the performance of decoders, which were only trained to decode the presented stimulus. However, this limitation could also represent an opportunity for future work on modality-invariant representations in working memory, for example by training decoders to decode the preceding stimuli from such working memory activity. Second, the inclusion of the crossmodal one-back task could have incentivized subjects to perform visual imagery when reading the captions, or to recall descriptive words (and/or have inner speech) when viewing images. Such behavior might have facilitated the spread of modality-invariant information in the cortex. It remains an open question to what degree such widespread modality-invariant information is also present during isolated reading and visual inspection of images, without any cross-modal task.</p>
<p>Finally, it remains an open question whether the activation patterns in the modality-invariant regions identified in our study relate to abstract concepts or to lower-level features that are shared between the two modalities (see also Section Modality-invariant representations). Our current study did not aim to tackle this question, as it is not crucial for the goal of building modality-agnostic decoders. In the future, techniques such as RSA and encoding models based on carefully designed feature spaces could be used to explore the nature of the representations in more detail. One major challenge to overcome will be the spatial overlap of modality-dependent, modality-invariant and abstract information in brain representations (<xref ref-type="bibr" rid="c37">Fernandino et al., 2016</xref>; <xref ref-type="bibr" rid="c72">Liuzzi et al., 2020</xref>; <xref ref-type="bibr" rid="c29">Dirani and Pylkkänen, 2024</xref>). More generally, <xref ref-type="bibr" rid="c12">Binder (2016)</xref> puts the dichotomy between modality-invariant and abstract representations into question, considering that “there is no absolute demarcation between embodied/perceptual and abstract/conceptual representation in the brain.” (p. 1098). The author argues for a hierarchical system in which representational patterns become increasingly abstract, creating a continuum from actual experiential information up to higher-level conceptual information (see also <xref ref-type="bibr" rid="c4">Andrews et al., 2014</xref>).</p>
</sec>
</sec>
</body>
<back>
<sec id="das" sec-type="data-availability">
<title>Data availability</title>
<p>All fMRI data is available at <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds006798">https://openneuro.org/datasets/ds006798</ext-link> - Code for preprocessing and analyses is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mitjanikolaus/multimodal_decoding">https://github.com/mitjanikolaus/multimodal_decoding</ext-link></p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This research was funded by grants from the French Agence Nationale de la Recherche (ANR: AI-REPS grant number ANR-18-CE37-0007-01 and ANITI grant number ANR-19-PI3A-0004) as well as the European Union (ERC Advanced grant GLoW, 101096017). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p>
<p>We thank the Inserm/UPS UMR1214 Technical Platform for their help in setting up and for the acquisitions of the MRI sequences.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Albers</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kok</surname> <given-names>P</given-names></string-name>, <string-name><surname>Toni</surname> <given-names>I</given-names></string-name>, <string-name><surname>Dijkerman</surname> <given-names>HC</given-names></string-name>, <string-name><surname>de Lange</surname> <given-names>F.</given-names></string-name></person-group> <article-title>Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex</article-title>. <source>Current Biology</source>. <year>2013</year> <month>Aug</month>; <volume>23</volume>(<issue>15</issue>):<fpage>1427</fpage>–<lpage>1431</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aliko</surname> <given-names>S</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gheorghiu</surname> <given-names>F</given-names></string-name>, <string-name><surname>Meliss</surname> <given-names>S</given-names></string-name>, <string-name><surname>Skipper</surname> <given-names>JI</given-names></string-name></person-group>. <article-title>A naturalistic neuroimaging database for understanding the brain using ecological stimuli</article-title>. <source>Scientific Data</source>. <year>2020</year> <month>Oct</month>; <volume>7</volume>(<issue>1</issue>):<fpage>347</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-020-00680-2</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Allen</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>St-Yves</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Breedlove</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Prince</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Dowdle</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Nau</surname> <given-names>M</given-names></string-name>, <string-name><surname>Caron</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pestilli</surname> <given-names>F</given-names></string-name>, <string-name><surname>Charest</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hutchinson</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>K.</given-names></string-name></person-group> <article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title>. <source>Nature Neuroscience</source>. <year>2022</year>; <volume>25</volume>(<issue>1</issue>):<fpage>116</fpage>–<lpage>126</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-021-00962-x</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andrews</surname> <given-names>M</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vigliocco</surname> <given-names>G.</given-names></string-name></person-group> <article-title>Reconciling Embodied and Distributional Accounts of Meaning in Language</article-title>. <source>Topics in Cognitive Science</source>. <year>2014</year> <month>Jul</month>; <volume>6</volume>(<issue>3</issue>):<fpage>359</fpage>–<lpage>370</lpage>. doi: <pub-id pub-id-type="doi">10.1111/tops.12096</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Ashburner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Barnes</surname> <given-names>G</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Daunizeau</surname> <given-names>J</given-names></string-name>, <string-name><surname>Flandin</surname> <given-names>G</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kiebel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kilner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Litvak</surname> <given-names>V</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>R.</given-names></string-name></person-group> <source>SPM12</source>. <publisher-name>Wellcome Trust Centre for Neuroimaging</publisher-name>. <year>2014</year>;.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Baars</surname> <given-names>BJ</given-names></string-name></person-group>. <source>A cognitive theory of consciousness</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>1993</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Baars</surname> <given-names>BJ</given-names></string-name></person-group>. <chapter-title>Global workspace theory of consciousness: toward a cognitive neuroscience of human experience</chapter-title>. In: <source>Progress in Brain Research</source>, vol. <volume>150</volume> <publisher-name>Elsevier</publisher-name>; <year>2005</year>. p. <fpage>45</fpage>–<lpage>53</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0079-6123(05)50004-9</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barsalou</surname> <given-names>LW</given-names></string-name></person-group>. <article-title>On Staying Grounded and Avoiding Quixotic Dead Ends</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year>; <volume>23</volume>(<issue>4</issue>):<fpage>1122</fpage>–<lpage>1142</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bartolomeo</surname> <given-names>P.</given-names></string-name></person-group> <article-title>The Relationship Between Visual Perception and Visual Mental Imagery: A Reappraisal of the Neuropsychological Evidence</article-title>. <source>Cortex</source>. <year>2002</year> <month>Jan</month>; <volume>38</volume>(<issue>3</issue>):<fpage>357</fpage>–<lpage>378</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0010-9452(08)70665-8</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Beliy</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gaziv</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hoogi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Strappini</surname> <given-names>F</given-names></string-name>, <string-name><surname>Golan</surname> <given-names>T</given-names></string-name>, <string-name><surname>Irani</surname> <given-names>M.</given-names></string-name></person-group> <article-title>From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</article-title>. In: <conf-name>NeurIPS</conf-name>; <year>2019</year>. .</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Benchetrit</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Banville</surname> <given-names>H</given-names></string-name>, <string-name><surname>King</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Brain decoding: toward real-time reconstruction of visual perception</article-title>. In: <conf-name>The Twelfth International Conference on Learning Representations</conf-name>; <year>2024</year>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binder</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>In defense of abstract conceptual representations</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year> <month>Aug</month>; <volume>23</volume>(<issue>4</issue>):<fpage>1096</fpage>–<lpage>1108</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13423-015-0909-1</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Binder</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Desai</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Graves</surname> <given-names>WW</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name></person-group>. <article-title>Where Is the Semantic System? A Critical Review and Meta-Analysis of 120 Functional Neuroimaging Studies</article-title>. <source>Cerebral Cortex</source>. <year>2009</year>; <volume>19</volume>(<issue>12</issue>):<fpage>2767</fpage>–<lpage>2796</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boccia</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sulpizio</surname> <given-names>V</given-names></string-name>, <string-name><surname>Teghil</surname> <given-names>A</given-names></string-name>, <string-name><surname>Palermo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Piccardi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Galati</surname> <given-names>G</given-names></string-name>, <string-name><surname>Guariglia</surname> <given-names>C.</given-names></string-name></person-group> <article-title>The dynamic contribution of the high-level visual cortex to imagery and perception</article-title>. <source>Human Brain Mapping</source>. <year>2019</year> <month>Jan</month>; <volume>40</volume>(<issue>8</issue>):<fpage>2449</fpage>–<lpage>2463</lpage>. doi: <pub-id pub-id-type="doi">10.1002/hbm.24535</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Boyle</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Pinsard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Boukhdhir</surname> <given-names>A</given-names></string-name>, <string-name><surname>Belleville</surname> <given-names>S</given-names></string-name>, <string-name><surname>Brambatti</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cohen-Adad</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cyr</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fuente Rainville</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bellec</surname> <given-names>P.</given-names></string-name></person-group> <article-title>The Courtois project on neuronal modelling-first data release</article-title>. In: <conf-name>26th annual meeting of the organization for human brain mapping</conf-name>; <year>2020</year>. .</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Brennan</surname> <given-names>JR</given-names></string-name></person-group>. <source>Language and the brain: a slim guide to neurolinguistics</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2022</year>. <ext-link ext-link-type="uri" xlink:href="https://books.google.com/books?hl=en&amp;lr=&amp;id=cDRtEAAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=brennan+language++brain&amp;ots=9SYgp7FBJy&amp;sig=PTuVqNSsF7NlwWWNboXe-ruH7uM">https://books.google.com/books?hl=en&amp;lr=&amp;id=cDRtEAAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=brennan+language++brain&amp;ots=9SYgp7FBJy&amp;sig=PTuVqNSsF7NlwWWNboXe-ruH7uM</ext-link>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brennan</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Hale</surname> <given-names>JT</given-names></string-name></person-group>. <article-title>Hierarchical structure guides rapid linguistic predictions during naturalistic listening</article-title>. <source>PLOS One</source>. <year>2019</year> <month>Jan</month>; <volume>14</volume>(<issue>1</issue>):<fpage>e0207741</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0207741</pub-id>, publisher: Public Library of Science.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bright</surname> <given-names>P</given-names></string-name>, <string-name><surname>Moss</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name></person-group>. <article-title>Unitary vs multiple semantics: PET studies of word and picture processing</article-title>. <source>Brain and Language</source>. <year>2004</year>; <volume>89</volume>(<issue>3</issue>):<fpage>417</fpage>–<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carota</surname> <given-names>F</given-names></string-name>, <string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Pulvermüller</surname> <given-names>F</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Distinct fronto-temporal substrates of distributional and taxonomic similarity among words: evidence from RSA of BOLD signals</article-title>. <source>NeuroImage</source>. <year>2021</year>; <volume>224</volume>:<fpage>117408</fpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pyles</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Marcus</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tarr</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Aminoff</surname> <given-names>EM</given-names></string-name></person-group>. <article-title>BOLD5000, a public fMRI dataset while viewing 5000 visual images</article-title>. <source>Scientific Data</source>. <year>2019</year>; <volume>6</volume>(<issue>1</issue>):<fpage>49</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-019-0052-3</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cox</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Savoy</surname> <given-names>RL</given-names></string-name></person-group>. <article-title>Functional magnetic resonance imaging (fMRI) “brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex</article-title>. <source>NeuroImage</source>. <year>2003</year> <month>Jun</month>; <volume>19</volume>(<issue>2</issue>):<fpage>261</fpage>–<lpage>270</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00049-1</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>The Brain Binds Entities and Events by Multiregional Activation from Convergence Zones</article-title>. <source>Neural Computation</source>. <year>1989</year>; <volume>1</volume>(<issue>1</issue>):<fpage>123</fpage>–<lpage>132</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Damasio</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tranel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Grabowski</surname> <given-names>T</given-names></string-name>, <string-name><surname>Adolphs</surname> <given-names>R</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Neural systems behind word and concept retrieval</article-title>. <source>Cognition</source>. <year>2004</year> <month>May</month>; <volume>92</volume>(<issue>1–2</issue>):<fpage>179</fpage>–<lpage>229</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2002.07.001</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desikan</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Ségonne</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Quinn</surname> <given-names>BT</given-names></string-name>, <string-name><surname>Dickerson</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Blacker</surname> <given-names>D</given-names></string-name>, <string-name><surname>Buckner</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Maguire</surname> <given-names>RP</given-names></string-name>, <string-name><surname>Hyman</surname> <given-names>BT</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Killiany</surname> <given-names>RJ</given-names></string-name></person-group>. <article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title>. <source>NeuroImage</source>. <year>2006</year>; <volume>31</volume>(<issue>3</issue>):<fpage>968</fpage>–<lpage>980</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devereux</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Clarke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Marouchos</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name></person-group>. <article-title>Representational Similarity Analysis Reveals Commonalities and Differences in the Semantic Processing of Words and Objects</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>(<issue>48</issue>).</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Devlin</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>K</given-names></string-name>, <string-name><surname>Toutanova</surname> <given-names>K.</given-names></string-name></person-group> <article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title>. In: <conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name>, Volume <volume>1</volume> (Long and Short Papers) <conf-loc>Minneapolis, Minnesota</conf-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2019</year>. p. <fpage>4171</fpage>–<lpage>4186</lpage>. doi: <pub-id pub-id-type="doi">10.18653/v1/N19-1423</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devlin</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Russell</surname> <given-names>RP</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Price</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moss</surname> <given-names>HE</given-names></string-name>, <string-name><surname>Matthews</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Tyler</surname> <given-names>LK</given-names></string-name></person-group>. <article-title>Susceptibility-Induced Loss of Signal: Comparing PET and fMRI on a Semantic Task</article-title>. <source>NeuroImage</source>. <year>2000</year> <month>Jun</month>; <volume>11</volume>(<issue>6</issue>):<fpage>589</fpage>–<lpage>600</lpage>. doi: <pub-id pub-id-type="doi">10.1006/nimg.2000.0595</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dijkstra</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bosch</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Van Gerven</surname> <given-names>MAJ</given-names></string-name></person-group>. <article-title>Shared Neural Mechanisms of Visual Perception and Imagery</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2019</year> <month>May</month>; <volume>23</volume>(<issue>5</issue>):<fpage>423</fpage>–<lpage>434</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2019.02.004</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dirani</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pylkkänen</surname> <given-names>L.</given-names></string-name></person-group> <article-title>MEG Evidence That Modality-Independent Conceptual Representations Contain Semantic and Visual Features</article-title>. <source>Journal of Neuroscience</source>. <year>2024</year> <month>Jul</month>; <volume>44</volume>(<issue>27</issue>). doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0326-24.2024</pub-id>, publisher: Society for Neuroscience Section: Research Articles.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Dosovitskiy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weissenborn</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Unterthiner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Dehghani</surname> <given-names>M</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heigold</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gelly</surname> <given-names>S</given-names></string-name>, <string-name><surname>Uszkoreit</surname> <given-names>J</given-names></string-name>, <string-name><surname>Houlsby</surname> <given-names>N.</given-names></string-name></person-group> <article-title>An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale</article-title>. In: <conf-name>International Conference on Learning Representations</conf-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Embleton</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Haroon</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Morris</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name>, <string-name><surname>Parker</surname> <given-names>GJM</given-names></string-name></person-group>. <article-title>Distortion correction for diffusionweighted MRI tractography and fMRI in the temporal lobes</article-title>. <source>Human Brain Mapping</source>. <year>2010</year>; <volume>31</volume>(<issue>10</issue>):<fpage>1570</fpage>– <lpage>1587</lpage>. doi: <pub-id pub-id-type="doi">10.1002/hbm.20959</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fairhall</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Caramazza</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Brain Regions That Represent Amodal Conceptual Knowledge</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>; <volume>33</volume>(<issue>25</issue>):<fpage>10552</fpage>–<lpage>10558</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farah</surname> <given-names>MJ</given-names></string-name></person-group>. <article-title>The neurological basis of mental imagery: A componential analysis</article-title>. <source>Cognition</source>. <year>1984</year> <month>Dec</month>; <volume>18</volume>(<issue>1–3</issue>):<fpage>245</fpage>–<lpage>272</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0010-0277(84)90026-X</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>, <string-name><surname>Behr</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Functional specificity for high-level linguistic processing in the human brain</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year> <month>Sep</month>; <volume>108</volume>(<issue>39</issue>):<fpage>16428</fpage>–<lpage>16433</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id>, publisher: Proceedings of the National Academy of Sciences.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hsieh</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Nieto-Castañón</surname> <given-names>A</given-names></string-name>, <string-name><surname>Whitfield-Gabrieli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name></person-group> <article-title>New Method for fMRI Investigations of Language: Defining ROIs Functionally in Individual Subjects</article-title>. <source>Journal of Neurophysiology</source>. <year>2010</year> <month>Aug</month>; <volume>104</volume>(<issue>2</issue>):<fpage>1177</fpage>–<lpage>1194</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.00032.2010</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Felleman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name></person-group>. <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cerebral cortex</source> <year>1991</year> <month>Jan</month>; <volume>1</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>47</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandino</surname> <given-names>L</given-names></string-name>, <string-name><surname>Binder</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Desai</surname> <given-names>RH</given-names></string-name>, <string-name><surname>Pendl</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Humphries</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Gross</surname> <given-names>WL</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Seidenberg</surname> <given-names>MS</given-names></string-name></person-group>. <article-title>Concept Representation Reflects Multimodal Abstraction: A Framework for Embodied Semantics</article-title>. <source>Cerebral Cortex</source>. <year>2016</year>; <volume>26</volume>(<issue>5</issue>):<fpage>2018</fpage>–<lpage>2034</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandino</surname> <given-names>L</given-names></string-name>, <string-name><surname>Tong</surname> <given-names>JQ</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Humphries</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Binder</surname> <given-names>JR</given-names></string-name></person-group>. <article-title>Decoding the information structure underlying the neural representation of concepts</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2022</year>; <volume>119</volume>(<issue>6</issue>).</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fischl</surname> <given-names>B.</given-names></string-name></person-group> <source>FreeSurfer. NeuroImage</source>. <year>2012</year>; <volume>62</volume>(<issue>2</issue>):<fpage>774</fpage>–<lpage>781</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>, place: Netherlands Publisher: <publisher-name>Elsevier Science</publisher-name>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Friederici</surname> <given-names>AD</given-names></string-name></person-group>. <source>Language in Our Brain: The Origins of a Uniquely Human Capacity</source>. <publisher-name>The MIT Press</publisher-name>; <year>2017</year>. doi: <pub-id pub-id-type="doi">10.7551/mit-press/11173.001.0001</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frisby</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Halai</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Cox</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name></person-group>. <article-title>Decoding semantic representations in mind and brain</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2023</year> <month>Mar</month>; <volume>27</volume>(<issue>3</issue>):<fpage>258</fpage>–<lpage>281</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2022.12.006</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gainotti</surname> <given-names>G.</given-names></string-name></person-group> <article-title>What the locus of brain lesion tells us about the nature of the cognitive defect underlying category-specific disorders: a review</article-title>. <source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source>. <year>2000</year> <month>Sep</month>; <volume>36</volume>(<issue>4</issue>):<fpage>539</fpage>–<lpage>559</lpage>. doi: <pub-id pub-id-type="doi">10.1016/s0010-9452(08)70537-9</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Girdhar</surname> <given-names>R</given-names></string-name>, <string-name><surname>El-Nouby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>M</given-names></string-name>, <string-name><surname>Alwala</surname> <given-names>KV</given-names></string-name>, <string-name><surname>Joulin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Misra</surname> <given-names>I.</given-names></string-name></person-group> <article-title>ImageBind: One Embedding Space To Bind Them All</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2023</year>. p. <fpage>15180</fpage>–<lpage>15190</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gorgolewski</surname> <given-names>K</given-names></string-name>, <string-name><surname>Burns</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Madison</surname> <given-names>C</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>D</given-names></string-name>, <string-name><surname>Halchenko</surname> <given-names>YO</given-names></string-name>, <string-name><surname>Waskom</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Ghosh</surname> <given-names>SS</given-names></string-name></person-group>. <article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title>. <source>Frontiers in neuroinformatics</source>. <year>2011</year>; <volume>5</volume>:<fpage>12318</fpage>. Publisher: Frontiers.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name>, <string-name><surname>Malach</surname> <given-names>R.</given-names></string-name></person-group> <article-title>The Human Visual Cortex</article-title>. <source>Annual Review of Neuroscience</source>. <year>2004</year> <month>Jul</month>; <volume>27</volume>(<issue>1</issue>):<fpage>649</fpage>–<lpage>677</lpage>. doi: <pub-id pub-id-type="doi">10.1146/an-nurev.neuro.27.070203.144220</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hajhajate</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kaufmann</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Siuda-Krzywicka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P.</given-names></string-name></person-group> <article-title>The connectional anatomy of visual mental imagery: evidence from a patient with left occipito-temporal damage</article-title>. <source>Brain Structure &amp; Function</source>. <year>2022</year> <month>Dec</month>; <volume>227</volume>(<issue>9</issue>):<fpage>3075</fpage>–<lpage>3083</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00429-022-02505-x</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Handjaras</surname> <given-names>G</given-names></string-name>, <string-name><surname>Ricciardi</surname> <given-names>E</given-names></string-name>, <string-name><surname>Leo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lenci</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cecchetti</surname> <given-names>L</given-names></string-name>, <string-name><surname>Cosottini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Marotta</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pietrini</surname> <given-names>P.</given-names></string-name></person-group> <article-title>How concepts are encoded in the human brain: A modality independent, category-based cortical organization of semantic knowledge</article-title>. <source>NeuroImage</source>. <year>2016</year>; <volume>135</volume>:<fpage>232</fpage>–<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haxby</surname> <given-names>JV</given-names></string-name>, <string-name><surname>Gobbini</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Furey</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Ishai</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schouten</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Pietrini</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex</article-title>. <source>Science</source>. <year>2001</year> <month>Sep</month>; <volume>293</volume>(<issue>5539</issue>):<fpage>2425</fpage>–<lpage>2430</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.1063736</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Deep Residual Learning for Image Recognition</article-title>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2016</year>. p. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huth</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title>. <source>Neuron</source>. <year>2012</year> <month>Dec</month>; <volume>76</volume>(<issue>6</issue>):<fpage>1210</fpage>–<lpage>1224</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Jones</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name></person-group>. <article-title>Comprehension of concrete and abstract words in semantic dementia</article-title>. <source>Neuropsychology</source>. <year>2009</year> <month>Jul</month>; <volume>23</volume>(<issue>4</issue>):<fpage>492</fpage>. <pub-id pub-id-type="pmcid">PMC2801065</pub-id>, doi: <pub-id pub-id-type="doi">10.1037/a0015452</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beckmann</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>, <string-name><surname>Woolrich</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM</given-names></string-name></person-group>. <article-title>FSL</article-title>. <source>NeuroImage</source>. <year>2012</year> <month>Aug</month>; <volume>62</volume>(<issue>2</issue>):<fpage>782</fpage>–<lpage>790</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Jiang</surname> <given-names>AQ</given-names></string-name>, <string-name><surname>Sablayrolles</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mensch</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bamford</surname> <given-names>C</given-names></string-name>, <string-name><surname>Chaplot</surname> <given-names>DS</given-names></string-name>, <string-name><given-names>Casas</given-names> <surname>Ddl</surname></string-name>, <string-name><surname>Bressand</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lengyel</surname> <given-names>G</given-names></string-name>, <string-name><surname>Lample</surname> <given-names>G</given-names></string-name>, <string-name><surname>Saulnier</surname> <given-names>L</given-names></string-name>, <string-name><surname>Lavaud</surname> <given-names>LR</given-names></string-name>, <string-name><surname>Lachaux</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Stock</surname> <given-names>P</given-names></string-name>, <string-name><surname>Scao</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Lavril</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lacroix</surname> <given-names>T</given-names></string-name>, <string-name><surname>Sayed</surname> <given-names>WE</given-names></string-name></person-group>, <article-title>Mistral 7B</article-title>. <source>arXiv</source> <elocation-id>2310.06825</elocation-id>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2310.06825</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>MK</given-names></string-name></person-group>. <article-title>Decoding individual natural scene representations during perception and imagery</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2014</year> <month>Feb</month>; <volume>8</volume>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2014.00059</pub-id>, publisher: Frontiers.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Larsen</surname> <given-names>B</given-names></string-name>, <string-name><surname>Walther</surname> <given-names>DB</given-names></string-name></person-group>. <article-title>Modality-Independent Coding of Scene Categories in Prefrontal Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2018</year> <month>Jun</month>; <volume>38</volume>(<issue>26</issue>):<fpage>5969</fpage>–<lpage>5981</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Functional specificity in the human brain: A window into the functional architecture of the mind</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year> <month>Jun</month>; <volume>107</volume>(<issue>25</issue>):<fpage>11163</fpage>–<lpage>11170</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id>, publisher: Proceedings of the National Academy of Sciences.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source>. <year>2008</year> <month>Mar</month>; <volume>452</volume>(<issue>7185</issue>):<fpage>352</fpage>–<lpage>355</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nature06713</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiefer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pulvermüller</surname> <given-names>F.</given-names></string-name></person-group> <article-title>Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions</article-title>. <source>Cortex</source>. <year>2012</year> <month>Jul</month>; <volume>48</volume>(<issue>7</issue>):<fpage>805</fpage>–<lpage>825</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2011.04.006</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Kim</surname> <given-names>W</given-names></string-name>, <string-name><surname>Son</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>I.</given-names></string-name></person-group> <article-title>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</article-title>. <source>In: Proceedings of the 38th International Conference on Machine Learning PMLR</source>; <year>2021</year>. p. <fpage>5583</fpage>–<lpage>5594</lpage>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v139/kim21k.html">https://proceedings.mlr.press/v139/kim21k.html</ext-link>, ISSN: <issn>2640-3498</issn>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kosslyn</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Pascual-Leone</surname> <given-names>A</given-names></string-name>, <string-name><surname>Felician</surname> <given-names>O</given-names></string-name>, <string-name><surname>Camposano</surname> <given-names>S</given-names></string-name>, <string-name><surname>Keenan</surname> <given-names>JP</given-names></string-name>, <string-name><given-names>L W</given-names>, <surname>Thompson</surname></string-name>, <string-name><surname>Ganis</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sukel</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Alpert</surname> <given-names>NM</given-names></string-name></person-group>. <article-title>The Role of Area 17 in Visual Imagery: Convergent Evidence from PET and rTMS</article-title>. <source>Science</source>. <year>1999</year> <month>Apr</month>; <volume>284</volume>(<issue>5411</issue>):<fpage>167</fpage>–<lpage>170</lpage>. doi: <pub-id pub-id-type="doi">10.1126/sci-ence.284.5411.167</pub-id>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kosslyn</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>WL</given-names></string-name></person-group>. <article-title>When is early visual cortex activated during visual mental imagery?</article-title> <source>Psychologi-cal Bulletin</source>. <year>2003</year>; <volume>129</volume>(<issue>5</issue>):<fpage>723</fpage>–<lpage>746</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-2909.129.5.723</pub-id>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Krasnowska-Kieraś</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wróblewska</surname> <given-names>A.</given-names></string-name></person-group> <article-title>Empirical Linguistic Study of Sentence Embeddings</article-title>. In: <conf-name>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</conf-name> <publisher-loc>Florence, Italy</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2019</year>. p. <fpage>5729</fpage>–<lpage>5739</lpage>. doi: <pub-id pub-id-type="doi">10.18653/v1/P19-1573</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mur</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bandettini</surname> <given-names>PA</given-names></string-name></person-group>. <article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2008</year> <month>Nov</month>; <volume>2</volume>. doi: <pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id>, publisher: <publisher-name>Frontiers</publisher-name>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Lowe</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name></person-group>. <article-title>Neural basis of category-specific semantic deficits for living things: evidence from semantic dementia, HSVE and a neural network model</article-title>. <source>Brain</source>. <year>2006</year> <month>Nov</month>; <volume>130</volume>(<issue>4</issue>):<fpage>1127</fpage>–<lpage>1137</lpage>. doi: <pub-id pub-id-type="doi">10.1093/brain/awm025</pub-id>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname> <given-names>SH</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name></person-group>. <article-title>Disentangling visual imagery and perception of real-world objects</article-title>. <source>NeuroIm-age</source>. <year>2012</year> <month>Feb</month>; <volume>59</volume>(<issue>4</issue>):<fpage>4064</fpage>–<lpage>4073</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>J</given-names></string-name>, <string-name><surname>Li</surname> <given-names>D</given-names></string-name>, <string-name><surname>Savarese</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hoi</surname> <given-names>S.</given-names></string-name></person-group> <article-title>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</article-title>. <conf-name>International Conference on Machine Learning</conf-name>; <year>2023</year>. .</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Li</surname> <given-names>LH</given-names></string-name>, <string-name><surname>Yatskar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Yin</surname> <given-names>D</given-names></string-name>, <string-name><surname>Hsieh</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>KW</given-names></string-name></person-group>. <article-title>VisualBERT: A Simple and Performant Baseline for Vision and Language</article-title>. <source>arXiv</source>:<elocation-id>1908.03557</elocation-id>. <year>2019</year> <month>Aug</month>; <pub-id pub-id-type="doi">10.48550/arXiv.1908.03557</pub-id>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sprague</surname> <given-names>T</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>AK</given-names></string-name></person-group>. <article-title>Mind Reader: Reconstructing complex images from brain activities</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2022</year> <month>Dec</month>; <volume>35</volume>:<fpage>29624</fpage>–<lpage>29636</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Lin</surname> <given-names>TY</given-names></string-name>, <string-name><surname>Maire</surname> <given-names>M</given-names></string-name>, <string-name><surname>Belongie</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hays</surname> <given-names>J</given-names></string-name>, <string-name><surname>Perona</surname> <given-names>P</given-names></string-name>, <string-name><surname>Ramanan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dollár</surname> <given-names>P</given-names></string-name>, <string-name><surname>Zitnick</surname> <given-names>CL</given-names></string-name></person-group>. <article-title>Microsoft COCO: Common Objects in Context</article-title>. In: <conf-name>Computer Vision – ECCV 2014</conf-name>, vol. <volume>8693</volume>; <year>2014</year>. p. <fpage>740</fpage>–<lpage>755</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Spagna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Hemispheric asymmetries in visual mental imagery</article-title>. <source>Brain Structure &amp; Function</source>. <year>2022</year> <month>Mar</month>; <volume>227</volume>(<issue>2</issue>):<fpage>697</fpage>–<lpage>708</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s00429-021-02277-w</pub-id>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hajhajate</surname> <given-names>D</given-names></string-name>, <string-name><surname>Spagna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Visual mental imagery in typical imagers and in aphantasia: A millimeter-scale 7-T fMRI study</article-title>. <source>Cortex</source>. <year>2025</year> <month>Apr</month>; <volume>185</volume>:<fpage>113</fpage>–<lpage>132</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2025.01.013</pub-id>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liuzzi</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Aglinskas</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fairhall</surname> <given-names>SL</given-names></string-name></person-group>. <article-title>General and feature-based semantic representations in the semantic network</article-title>. <source>Scientific Reports</source>. <year>2020</year> <month>Jun</month>; <volume>10</volume>(<issue>1</issue>):<fpage>8931</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-020-65906-0</pub-id>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liuzzi</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Bruffaerts</surname> <given-names>R</given-names></string-name>, <string-name><surname>Peeters</surname> <given-names>R</given-names></string-name>, <string-name><surname>Adamczuk</surname> <given-names>K</given-names></string-name>, <string-name><surname>Keuleers</surname> <given-names>E</given-names></string-name>, <string-name><surname>De Deyne</surname> <given-names>S</given-names></string-name>, <string-name><surname>Storms</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dupont</surname> <given-names>P</given-names></string-name>, <string-name><surname>Vandenberghe</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Cross-modal representation of spoken and written word meaning in left pars triangularis</article-title>. <source>NeuroImage</source>. <year>2017</year> <month>Apr</month>; <volume>150</volume>:<fpage>292</fpage>–<lpage>307</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.032</pub-id>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Man</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kaplan</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>A</given-names></string-name>, <string-name><surname>Meyer</surname> <given-names>K.</given-names></string-name></person-group> <article-title>Sight and Sound Converge to Form Modality-Invariant Representations in Temporoparietal Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>; <volume>32</volume>(<issue>47</issue>):<fpage>16629</fpage>–<lpage>16636</lpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Martin</surname> <given-names>A.</given-names></string-name></person-group> <chapter-title>Circuits in Mind: The Neural Foundations for Object Concepts</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Gazzaniga</surname> <given-names>MS</given-names></string-name>, editor</person-group>. <source>The Cognitive Neurosciences</source>, <edition>4</edition> ed. <publisher-name>The MIT Press</publisher-name>; <year>2009</year>. doi: <pub-id pub-id-type="doi">10.7551/mitpress/8029.003.0091</pub-id>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname> <given-names>A.</given-names></string-name></person-group> <article-title>GRAPES—Grounding representations in action, perception, and emotion systems: How object properties and categories are represented in the human brain</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year>; <volume>23</volume>(<issue>4</issue>):<fpage>979</fpage>– <lpage>990</lpage>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Douglas</surname> <given-names>D</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>RN</given-names></string-name>, <string-name><surname>Man</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Barense</surname> <given-names>MD</given-names></string-name></person-group>. <article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title>. <source>eLife</source>. <year>2018</year>; <volume>7</volume>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Meschke</surname> <given-names>E</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Mapping Multimodal Conceptual Representations within the Lexical-Semantic Brain System</article-title>. In: <conf-name>CCN</conf-name>; <year>2024</year>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Meyer</surname> <given-names>K</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>A.</given-names></string-name></person-group> <chapter-title>Convergence and divergence in a neural architecture for recognition and memory</chapter-title>. <source>Trends in Neurosciences</source>. <year>2009</year> <month>Jul</month>; <volume>32</volume>(<issue>7</issue>):<fpage>376</fpage>–<lpage>382</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tins.2009.04.002</pub-id>, publisher: <publisher-name>Elsevier</publisher-name>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Price</surname> <given-names>CJ</given-names></string-name></person-group>. <article-title>Three Distinct Ventral Occipitotemporal Regions for Reading and Object Naming</article-title>. <source>NeuroImage</source>. <year>1999</year>; <volume>10</volume>(<issue>2</issue>):<fpage>181</fpage>–<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Encoding and decoding in fMRI</article-title>. <source>NeuroImage</source>. <year>2011</year> <month>May</month>; <volume>56</volume>(<issue>2</issue>):<fpage>400</fpage>–<lpage>410</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Olman</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Stansbury</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title>. <source>NeuroImage</source>. <year>2015</year> <month>Jan</month>; <volume>105</volume>:<fpage>215</fpage>–<lpage>228</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.018</pub-id>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Oliver</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Bayesian Reconstruction of Natural Images from Human Brain Activity</article-title>. <source>Neuron</source>. <year>2009</year> <month>Sep</month>; <volume>63</volume>(<issue>6</issue>):<fpage>902</fpage>–<lpage>915</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.006</pub-id>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nastase</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>YF</given-names></string-name>, <string-name><surname>Hillman</surname> <given-names>H</given-names></string-name>, <string-name><surname>Zadbood</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hasenfratz</surname> <given-names>L</given-names></string-name>, <string-name><surname>Keshavarzian</surname> <given-names>N</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Honey</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Yeshurun</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Regev</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nguyen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>CHC</given-names></string-name>, <string-name><surname>Baldassano</surname> <given-names>C</given-names></string-name>, <string-name><surname>Lositsky</surname> <given-names>O</given-names></string-name>, <string-name><surname>Simony</surname> <given-names>E</given-names></string-name>, <string-name><surname>Chow</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Leong</surname> <given-names>YC</given-names></string-name>, <string-name><surname>Brooks</surname> <given-names>PP</given-names></string-name>, <string-name><surname>Micciche</surname> <given-names>E</given-names></string-name>, <string-name><surname>Choe</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension</article-title>. <source>Scientific Data</source>. <year>2021</year> <month>Sep</month>; <volume>8</volume>(<issue>1</issue>):<fpage>250</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-021-01033-3</pub-id>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title>. <source>Current Biology</source>. <year>2011</year> <month>Oct</month>; <volume>21</volume>(<issue>19</issue>):<fpage>1641</fpage>–<lpage>1646</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2011.08.031</pub-id>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Craven</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Mental Imagery of Faces and Places Activates Corresponding Stimulus-Specific Brain Regions</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2000</year> <month>Nov</month>; <volume>12</volume>(<issue>6</issue>):<fpage>1013</fpage>–<lpage>1023</lpage>. doi: <pub-id pub-id-type="doi">10.1162/08989290051137549</pub-id>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visconti di Oleggio Castello</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chauhan</surname> <given-names>V</given-names></string-name>, <string-name><surname>Jiahui</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gobbini</surname> <given-names>MI</given-names></string-name></person-group>. <article-title>An fMRI dataset in response to “The Grand Budapest Hotel”, a socially-rich, naturalistic movie</article-title>. <source>Scientific Data</source>. <year>2020</year>; <volume>7</volume>(<issue>1</issue>):<fpage>383</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-020-00735-4</pub-id>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Oquab</surname> <given-names>M</given-names></string-name>, <string-name><surname>Darcet</surname> <given-names>T</given-names></string-name>, <string-name><surname>Moutakanni</surname> <given-names>T</given-names></string-name>, <string-name><surname>Vo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Szafraniec</surname> <given-names>M</given-names></string-name>, <string-name><surname>Khalidov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Fernandez</surname> <given-names>P</given-names></string-name>, <string-name><surname>Haziza</surname> <given-names>D</given-names></string-name>, <string-name><surname>Massa</surname> <given-names>F</given-names></string-name>, <string-name><surname>El-Nouby</surname> <given-names>A</given-names></string-name>, <string-name><surname>Assran</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ballas</surname> <given-names>N</given-names></string-name>, <string-name><surname>Galuba</surname> <given-names>W</given-names></string-name>, <string-name><surname>Howes</surname> <given-names>R</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>PY</given-names></string-name>, <string-name><surname>Li</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Misra</surname> <given-names>I</given-names></string-name>, <string-name><surname>Rabbat</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sharma</surname> <given-names>V</given-names></string-name>, <string-name><surname>Synnaeve</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>DINOv2: Learning Robust Visual Features without Supervision</article-title>. <source>arXiv</source> <elocation-id>2304.07193</elocation-id>; <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2304.07193</pub-id>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ozcelik</surname> <given-names>F</given-names></string-name>, <string-name><surname>VanRullen</surname> <given-names>R</given-names></string-name></person-group>, <article-title>Natural scene reconstruction from fMRI signals using generative latent diffusion</article-title>. <source>arXiv</source> <elocation-id>2303.05334</elocation-id>; <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2303.05334</pub-id>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Patterson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name></person-group>. <chapter-title>The Hub-and-Spoke Hypothesis of Semantic Memory</chapter-title>. In: <source>Neurobiology of Language</source> <publisher-name>Elsevier</publisher-name>; <year>2016</year>. p. <fpage>765</fpage>–<lpage>775</lpage>. doi: <pub-id pub-id-type="doi">10.1016/B978-0-12-407794-2.00061-4</pub-id>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pearson</surname> <given-names>J.</given-names></string-name></person-group> <article-title>The human imagination: the cognitive neuroscience of visual mental imagery</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2019</year> <month>Oct</month>; <volume>20</volume>(<issue>10</issue>):<fpage>624</fpage>–<lpage>634</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41583-019-0202-9</pub-id>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname> <given-names>O</given-names></string-name>, <string-name><surname>Blondel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Prettenhofer</surname> <given-names>P</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dubourg</surname> <given-names>V.</given-names></string-name></person-group> <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>the Journal of machine Learning research</source>. <year>2011</year>; <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname> <given-names>F</given-names></string-name>, <string-name><surname>Lou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pritchett</surname> <given-names>B</given-names></string-name>, <string-name><surname>Ritter</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fedorenko</surname> <given-names>E.</given-names></string-name></person-group> <article-title>Toward a universal decoder of linguistic meaning from brain activation</article-title>. <source>Nature Communications</source>. <year>2018</year> <month>Mar</month>; <volume>9</volume>(<issue>1</issue>):<fpage>963</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41467-018-03068-4</pub-id>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pobric</surname> <given-names>G</given-names></string-name>, <string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name></person-group>. <article-title>Category-Specific versus Category-General Semantic Impairment Induced by Transcranial Magnetic Stimulation</article-title>. <source>Current Biology</source>. <year>2010</year> <month>May</month>; <volume>20</volume>(<issue>10</issue>):<fpage>964</fpage>–<lpage>968</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2010.03.070</pub-id>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Popham</surname> <given-names>SF</given-names></string-name>, <string-name><surname>Huth</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Bilenko</surname> <given-names>NY</given-names></string-name>, <string-name><surname>Deniz</surname> <given-names>F</given-names></string-name>, <string-name><surname>Gao</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Nunez-Elizalde</surname> <given-names>AO</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name></person-group>. <article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title>. <source>Nature Neuroscience</source>. <year>2021</year>; <volume>24</volume>(<issue>11</issue>):<fpage>1628</fpage>– <lpage>1636</lpage>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Power</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Barnes</surname> <given-names>KA</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>AZ</given-names></string-name>, <string-name><surname>Schlaggar</surname> <given-names>BL</given-names></string-name>, <string-name><surname>Petersen</surname> <given-names>SE</given-names></string-name></person-group>. <article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title>. <source>NeuroImage</source>. <year>2012</year> <month>Feb</month>; <volume>59</volume>(<issue>3</issue>):<fpage>2142</fpage>–<lpage>2154</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Radford</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Hallacy</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ramesh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Goh</surname> <given-names>G</given-names></string-name>, <string-name><surname>Agarwal</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sastry</surname> <given-names>G</given-names></string-name>, <string-name><surname>Askell</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mishkin</surname> <given-names>P</given-names></string-name>, <string-name><surname>Clark</surname> <given-names>J</given-names></string-name>, <string-name><surname>Krueger</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I.</given-names></string-name></person-group> <article-title>Learning Transferable Visual Models From Natural Language Supervision</article-title>. <conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name>. <year>2021</year>; p. <fpage>16</fpage>.</mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radford</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Child</surname> <given-names>R</given-names></string-name>, <string-name><surname>Luan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Amodei</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I.</given-names></string-name></person-group> <article-title>Language models are unsupervised multitask learners</article-title>. <source>OpenAI blog</source>. <year>2019</year>; <volume>1</volume>(<issue>8</issue>):<fpage>9</fpage>.</mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralph</surname> <given-names>MAL</given-names></string-name>, <string-name><surname>Jefferies</surname> <given-names>E</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name></person-group>. <article-title>The neural and computational bases of semantic cognition</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2017</year>; <volume>18</volume>(<issue>1</issue>):<fpage>42</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reddy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Tsuchiya</surname> <given-names>N</given-names></string-name>, <string-name><surname>Serre</surname> <given-names>T.</given-names></string-name></person-group> <article-title>Reading the mind’s eye: decoding category information during mental imagery</article-title>. <source>NeuroImage</source>. <year>2010</year> <month>Apr</month>; <volume>50</volume>(<issue>2</issue>):<fpage>818</fpage>–<lpage>825</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Reimers</surname> <given-names>N</given-names></string-name>, <string-name><surname>Gurevych</surname> <given-names>I.</given-names></string-name></person-group> <article-title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</article-title>. In: <conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</conf-name> <conf-loc>Hong Kong, China</conf-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2019</year>. p. <fpage>3980</fpage>–<lpage>3990</lpage>. doi: <pub-id pub-id-type="doi">10.18653/v1/D19-1410</pub-id>.</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname> <given-names>TT</given-names></string-name>, <string-name><surname>Lambon Ralph</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Garrard</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bozeat</surname> <given-names>S</given-names></string-name>, <string-name><surname>McClelland</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Hodges</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>K.</given-names></string-name></person-group> <article-title>Structure and Deterioration of Semantic Memory: A Neuropsychological and Computational Investigation</article-title>. <source>Psychological Review</source>. <year>2004</year>; <volume>111</volume>(<issue>1</issue>):<fpage>205</fpage>–<lpage>235</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-295X.111.1.205</pub-id>.</mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sanchez</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hartmann</surname> <given-names>T</given-names></string-name>, <string-name><surname>Fuscà</surname> <given-names>M</given-names></string-name>, <string-name><surname>Demarchi</surname> <given-names>G</given-names></string-name>, <string-name><surname>Weisz</surname> <given-names>N.</given-names></string-name></person-group> <article-title>Decoding across sensory modalities reveals common supramodal signatures of conscious perception</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2020</year> <month>Mar</month>; <volume>117</volume>(<issue>13</issue>):<fpage>7437</fpage>–<lpage>7446</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1912584117</pub-id>.</mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>Lam</surname> <given-names>NHL</given-names></string-name>, <string-name><surname>Uddén</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hultén</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hagoort</surname> <given-names>P.</given-names></string-name></person-group> <article-title>A 204-subject multimodal neuroimaging dataset to study language processing</article-title>. <source>Scientific Data</source>. <year>2019</year>; <volume>6</volume>(<issue>1</issue>):<fpage>17</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-019-0020-y</pub-id>.</mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Reppas</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Kwong</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Belliveau</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Brady</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RBH</given-names></string-name></person-group>. <article-title>Borders of Multiple Visual Areas in Humans Revealed by Functional Magnetic Resonance Imaging</article-title>. <source>Science</source>. <year>1995</year> <month>May</month>; <volume>268</volume>(<issue>5212</issue>):<fpage>889</fpage>–<lpage>893</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.7754376</pub-id>.</mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname> <given-names>G</given-names></string-name>, <string-name><surname>Horikawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Majima</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kamitani</surname> <given-names>Y.</given-names></string-name></person-group> <article-title>Deep image reconstruction from human brain activity</article-title>. <source>PLOS Computational Biology</source>. <year>2019</year> <month>Jan</month>; <volume>15</volume>(<issue>1</issue>):<fpage>e1006633</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006633</pub-id>.</mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shinkareva</surname> <given-names>SV</given-names></string-name>, <string-name><surname>Malave</surname> <given-names>VL</given-names></string-name>, <string-name><surname>Mason</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Mitchell</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Just</surname> <given-names>MA</given-names></string-name></person-group>. <article-title>Commonality of neural representations of words and pictures</article-title>. <source>NeuroImage</source>. <year>2011</year>; <volume>54</volume>(<issue>3</issue>):<fpage>2418</fpage>–<lpage>2425</lpage>.</mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simanova</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hagoort</surname> <given-names>P</given-names></string-name>, <string-name><surname>Oostenveld</surname> <given-names>R</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MAJ</given-names></string-name></person-group>. <article-title>Modality-Independent Decoding of Semantic Information from the Human Brain</article-title>. <source>Cerebral Cortex</source>. <year>2014</year>; <volume>24</volume>(<issue>2</issue>):<fpage>426</fpage>–<lpage>434</lpage>.</mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simmons</surname> <given-names>WK</given-names></string-name>, <string-name><surname>Barsalou</surname> <given-names>LW</given-names></string-name></person-group>. <article-title>The similarity-in-topography principle: Reconciling theories of conceptual deficits</article-title>. <source>Cognitive Neuropsychology</source>. <year>2003</year>; <volume>20</volume>(<issue>3–6</issue>):<fpage>451</fpage>–<lpage>486</lpage>.</mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Singh</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hu</surname> <given-names>R</given-names></string-name>, <string-name><surname>Goswami</surname> <given-names>V</given-names></string-name>, <string-name><surname>Couairon</surname> <given-names>G</given-names></string-name>, <string-name><surname>Galuba</surname> <given-names>W</given-names></string-name>, <string-name><surname>Rohrbach</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kiela</surname> <given-names>D.</given-names></string-name></person-group> <article-title>FLAVA: A Foundational Language and Vision Alignment Model</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2022</year>. p. <fpage>15638</fpage>–<lpage>15650</lpage>.</mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>TE</given-names></string-name></person-group>. <article-title>Threshold-free cluster enhancement: Addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title>. <source>NeuroImage</source>. <year>2009</year>; <volume>44</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snowden</surname> <given-names>J</given-names></string-name>, <string-name><surname>Goulding</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Neary</surname> <given-names>D.</given-names></string-name></person-group> <article-title>Semantic dementia: A form of circumscribed cerebral atrophy</article-title>. <source>Behavioural Neurology</source>. <year>1989</year>; <volume>2</volume>(<issue>3</issue>):<fpage>124043</fpage>. doi: <pub-id pub-id-type="doi">10.1155/1989/124043</pub-id>.</mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spagna</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hajhajate</surname> <given-names>D</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bartolomeo</surname> <given-names>P.</given-names></string-name></person-group> <article-title>Visual mental imagery engages the left fusiform gyrus, but not the early visual cortex: A meta-analysis of neuroimaging evidence</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2021</year> <month>Mar</month>; <volume>122</volume>:<fpage>201</fpage>–<lpage>217</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.12.029</pub-id>.</mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Billings</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Silson</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Robertson</surname> <given-names>CE</given-names></string-name></person-group>. <article-title>A network linking scene perception and spatial memory systems in posterior cerebral cortex</article-title>. <source>Nature Communications</source>. <year>2021</year> <month>May</month>; <volume>12</volume>(<issue>1</issue>):<fpage>2632</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41467-021-22848-z</pub-id>, publisher: Nature Publishing Group.</mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Steiner</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Tschannen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Keysers</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Bitton</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Gritsenko</surname> <given-names>A</given-names></string-name>, <string-name><surname>Minderer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sherbondy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Long</surname> <given-names>S</given-names></string-name>, <string-name><surname>Qin</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ingle</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bugliarello</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kazemzadeh</surname> <given-names>S</given-names></string-name>, <string-name><surname>Mesnard</surname> <given-names>T</given-names></string-name>, <string-name><surname>Alabdulmohsin</surname> <given-names>I</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L</given-names></string-name>, <string-name><surname>Zhai</surname> <given-names>X</given-names></string-name></person-group>, <article-title>PaliGemma 2: A Family of Versatile VLMs for Transfer</article-title>. <source>arXiv</source> <elocation-id>2412.03555</elocation-id>; <year>2024</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2412.03555</pub-id>.</mixed-citation></ref>
<ref id="c116"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stelzer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>R.</given-names></string-name></person-group> <article-title>Statistical inference and multiple testing correction in classification-based multivoxel pattern analysis (MVPA): random permutations and cluster size control</article-title>. <source>NeuroImage</source>. <year>2013</year> <month>Jan</month>; <volume>65</volume>:<fpage>69</fpage>– <lpage>82</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.063</pub-id>.</mixed-citation></ref>
<ref id="c117"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>M</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cusack</surname> <given-names>R</given-names></string-name>, <string-name><surname>Duncan</surname> <given-names>J.</given-names></string-name></person-group> <article-title>Top-Down Activation of Shape-Specific Population Codes in Visual Cortex during Mental Imagery</article-title>. <source>The Journal of Neuroscience</source>. <year>2009</year> <month>Feb</month>; <volume>29</volume>(<issue>5</issue>):<fpage>1565</fpage>–<lpage>1572</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4657-08.2009</pub-id>.</mixed-citation></ref>
<ref id="c118"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Takagi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Nishimoto</surname> <given-names>S.</given-names></string-name></person-group> <article-title>High-Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity</article-title>. In: <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>; <year>2023</year>. p. <fpage>14453</fpage>–<lpage>14463</lpage>.</mixed-citation></ref>
<ref id="c119"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Du</surname> <given-names>M</given-names></string-name>, <string-name><surname>Vo</surname> <given-names>VA</given-names></string-name></person-group>. <article-title>Brain encoding models based on multimodal transformers can transfer across language and vision</article-title>. In: <conf-name>NeurIPS</conf-name>; <year>2023a</year>. .</mixed-citation></ref>
<ref id="c120"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname> <given-names>J</given-names></string-name>, <string-name><surname>LeBel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jain</surname> <given-names>S</given-names></string-name>, <string-name><surname>Huth</surname> <given-names>AG</given-names></string-name></person-group>. <article-title>Semantic reconstruction of continuous language from non-invasive brain recordings</article-title>. <source>Nature Neuroscience</source>. <year>2023b</year> <month>May</month>; <volume>26</volume>(<issue>5</issue>):<fpage>858</fpage>–<lpage>866</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-023-01304-9</pub-id>.</mixed-citation></ref>
<ref id="c121"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tong</surname> <given-names>J</given-names></string-name>, <string-name><surname>Binder</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Humphries</surname> <given-names>C</given-names></string-name>, <string-name><surname>Mazurchuk</surname> <given-names>S</given-names></string-name>, <string-name><surname>Conant</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Fernandino</surname> <given-names>L.</given-names></string-name></person-group> <article-title>A Distributed Network for Multimodal Experiential Representation of Concepts</article-title>. <source>Journal of Neuroscience</source>. <year>2022</year>; <volume>42</volume>(<issue>37</issue>):<fpage>7121</fpage>–<lpage>7130</lpage>.</mixed-citation></ref>
<ref id="c122"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Touvron</surname> <given-names>H</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Stone</surname> <given-names>K</given-names></string-name>, <string-name><surname>Albert</surname> <given-names>P</given-names></string-name>, <string-name><surname>Almahairi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Babaei</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Bashlykov</surname> <given-names>N</given-names></string-name>, <string-name><surname>Batra</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bhargava</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bhosale</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bikel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Blecher</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ferrer</surname> <given-names>CC</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cucurull</surname> <given-names>G</given-names></string-name>, <string-name><surname>Esiobu</surname> <given-names>D</given-names></string-name>, <string-name><surname>Fernandes</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Fuller</surname> <given-names>B</given-names></string-name>, <etal>et al.</etal></person-group>, <article-title>Llama 2: Open Foundation and Fine-Tuned Chat Models</article-title>. <source>arXiv</source>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2307.09288</pub-id>.</mixed-citation></ref>
<ref id="c123"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tranel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>H</given-names></string-name>, <string-name><surname>Damasio</surname> <given-names>AR</given-names></string-name></person-group>. <article-title>A neural basis for the retrieval of conceptual knowledge</article-title>. <source>Neuropsychologia</source>. <year>1997</year> <month>Oct</month>; <volume>35</volume>(<issue>10</issue>):<fpage>1319</fpage>–<lpage>1327</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0028-3932(97)00085-7</pub-id>.</mixed-citation></ref>
<ref id="c124"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vandenberghe</surname> <given-names>R</given-names></string-name>, <string-name><surname>Price</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wise</surname> <given-names>R</given-names></string-name>, <string-name><surname>Josephs</surname> <given-names>O</given-names></string-name>, <string-name><surname>Frackowiak</surname> <given-names>RS</given-names></string-name></person-group>. <article-title>Functional anatomy of a common semantic system for words and pictures</article-title>. <source>Nature</source>. <year>1996</year>; <volume>383</volume>(<issue>6597</issue>).</mixed-citation></ref>
<ref id="c125"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Reconstructing faces from fMRI patterns using deep generative neural networks</article-title>. <source>Communications Biology</source>. <year>2019</year> <month>May</month>; <volume>2</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s42003-019-0438-y</pub-id>.</mixed-citation></ref>
<ref id="c126"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warrington</surname> <given-names>EK</given-names></string-name></person-group>. <article-title>The Selective Impairment of Semantic Memory</article-title>. <source>Quarterly Journal of Experimental Psychology</source>. <year>1975</year> <month>Nov</month>; <volume>27</volume>(<issue>4</issue>):<fpage>635</fpage>–<lpage>657</lpage>. doi: <pub-id pub-id-type="doi">10.1080/14640747508400525</pub-id>.</mixed-citation></ref>
<ref id="c127"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warrington</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Mccarthy</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Categories of knowledge: Further fractionations and an attempted integration</article-title>. <source>Brain: a Journal of Neurology</source>. <year>1987</year>; .</mixed-citation></ref>
<ref id="c128"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Warrington</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Shallice</surname> <given-names>T.</given-names></string-name></person-group> <article-title>Category Specific Semantic Impairments</article-title>. <source>Brain: a Journal of Neurology</source>. <year>1984</year>; .</mixed-citation></ref>
<ref id="c129"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Wolf</surname> <given-names>T</given-names></string-name>, <string-name><surname>Debut</surname> <given-names>L</given-names></string-name>, <string-name><surname>Sanh</surname> <given-names>V</given-names></string-name>, <string-name><surname>Chaumond</surname> <given-names>J</given-names></string-name>, <string-name><surname>Delangue</surname> <given-names>C</given-names></string-name>, <string-name><surname>Moi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cistac</surname> <given-names>P</given-names></string-name>, <string-name><surname>Rault</surname> <given-names>T</given-names></string-name>, <string-name><surname>Louf</surname> <given-names>R</given-names></string-name>, <string-name><surname>Funtowicz</surname> <given-names>M.</given-names></string-name></person-group> <article-title>Transformers: State-of-the-art natural language processing</article-title>. In: <conf-name>Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</conf-name>; <year>2020</year>. p. <fpage>38</fpage>–<lpage>45</lpage>.</mixed-citation></ref>
<ref id="c130"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Xu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>C</given-names></string-name>, <string-name><surname>Rosenman</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lal</surname> <given-names>V</given-names></string-name>, <string-name><surname>Che</surname> <given-names>W</given-names></string-name>, <string-name><surname>Duan</surname> <given-names>N.</given-names></string-name></person-group> <article-title>BridgeTower: Building Bridges between Encoders in Vision-Language Representation Learning</article-title>. <conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name>. <year>2023</year> <month>Jun</month>; <volume>37</volume>(<issue>9</issue>):<fpage>10637</fpage>–<lpage>10647</lpage>. doi: <pub-id pub-id-type="doi">10.1609/aaai.v37i9.26263</pub-id>.</mixed-citation></ref>
<ref id="c131"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Zhai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Mustafa</surname> <given-names>B</given-names></string-name>, <string-name><surname>Kolesnikov</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beyer</surname> <given-names>L.</given-names></string-name></person-group> <article-title>Sigmoid Loss for Language Image Pre-Training</article-title>. In: <conf-name>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name> <publisher-loc>Paris, France</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2023</year>. p. <fpage>11941</fpage>–<lpage>11952</lpage>. doi: <pub-id pub-id-type="doi">10.1109/ICCV51070.2023.01100</pub-id>.</mixed-citation></ref>
<ref id="c132"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zola-Morgan</surname> <given-names>S.</given-names></string-name></person-group> <article-title>Localization of Brain Function: The Legacy of Franz Joseph Gall (1758-1828)</article-title>. <source>Annual Review of Neuroscience</source>. <year>1995</year> <month>Mar</month>; <volume>18</volume>(Volume 18, 1995):<fpage>359</fpage>–<lpage>383</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.002043</pub-id>, publisher: Annual Reviews.</mixed-citation></ref>
<ref id="c133"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zwaan</surname> <given-names>RA</given-names></string-name></person-group>. <article-title>Situation models, mental simulations, and abstract concepts in discourse comprehension</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year> <month>Aug</month>; <volume>23</volume>(<issue>4</issue>):<fpage>1028</fpage>–<lpage>1034</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13423-015-0864-x</pub-id>.</mixed-citation></ref>
<ref id="dataref1"><mixed-citation publication-type="data" specific-use="generated"><person-group person-group-type="author"><string-name><surname>Nikolaus</surname></string-name>, <string-name><surname>Mitja</surname></string-name>, <string-name><surname>Milad Mozafari</surname></string-name>, <string-name><surname>Isabelle Berry</surname></string-name>, <string-name><surname>Nicholas Asher</surname></string-name>, <string-name><surname>Leila Reddy</surname></string-name>, <string-name><surname>and Rullen VanRullen</surname></string-name></person-group> (<year iso-8601-date="2025">2025</year>) <article-title>SemReps-8K</article-title>. <source>OpenNeuro</source>. <pub-id pub-id-type="doi">10.18112/openneuro.ds006798</pub-id></mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<label>Appendix 1</label>
<sec id="s6">
<title>Dataset quality metrics</title>
<sec id="s6a">
<title>Head motion</title>
<p><xref rid="app1fig1" ref-type="fig">Figure 1</xref> and <xref rid="app1fig2" ref-type="fig">2</xref> present information on head motion for each subject. All head motion estimates are based on the realignment parameters calculated using SPM12. We found that head motion estimates are largely stable and do not vary extensively between sessions and subjects.</p>
<fig id="app1fig1" position="float" fig-type="figure">
<label>Appendix 1—figure 1</label>
<caption><title>Head motion estimates for each subject.</title>
<p>The plots show the realignment parameters as computed by SPM12 (spm_realign) for estimating within modality rigid body alignment. We multiplied the rotation parameters pitch, roll and yaw (originally in radian units) by 50 in order to allow interpretation in terms of millimeters of displacement for a circle of diameter 10 cm (which is approximately the mean distance from the cerebral cortex to the center of the head) (<xref ref-type="bibr" rid="c96">Power et al., 2012</xref>). The translucent error bands show 95% confidence intervals calculating using bootstrapping over all frames/runs from a session.</p></caption>
<graphic xlink:href="658221v2_figA1_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app1fig2" position="float" fig-type="figure">
<label>Appendix 1—figure 2.</label>
<caption><title>Framewise displacement for each subject.</title>
<p>The measure indicates how much the head changed position from one frame to the next. We calculated framewise displacement as the sum of the absolute values of the derivatives of the six realignment parameters. The translucent error bands show 95% confidence intervals calculating using bootstrapping over all frames/runs from a session.</p></caption>
<graphic xlink:href="658221v2_figA1_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s6b">
<title>Intersession alignment</title>
<p>During preprocessing, all functional MRI data was coregistered to the anatomical scan of the first session. To assess to which degree the alignment of functional data of different sessions varies with respect to the alignment of the first session, we estimated mutual information between the coregistered scans and the reference anatomical scan. During coregistration, our pipeline based on SPM12 created a mean image of the functional data for each session to estimate transformation parameters specific for each scanning session. We calculated mutual information between the coregistered mean images and the anatomical scan of the first session. Then, we normalized all values with respect to the mutual information of the first session (i.e. the normalized mutual information value for session 1 for all subjects is 1). The results are shown in <xref rid="app1fig3" ref-type="fig">Figure 3</xref>. The plot shows that the mutual information does not deviate strongly from the baseline (minimum value: 90%) and stays in an interval of ±5% for most subjects.</p>
<fig id="app1fig3" position="float" fig-type="figure">
<label>Appendix 1—figure 3.</label>
<caption><title>Mutual information between the anatomical scan of the first session and functional data for each session as an indicator of intersession alignment.</title>
<p>All values were normalized based on the mutual information from the functional data from the first session. The raw mutual information scores for the alignment during the first session are: subject 1: 0.49; subject 2: 0.55; subject 3: 0.55; subject 4: 0.53; subject 5: 0.57; subject 6: 0.57.</p></caption>
<graphic xlink:href="658221v2_figA1_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
</app>
<app id="app2">
<label>Appendix 2</label>
<sec id="s7">
<title>Feature extraction details</title>
<p>For each target stimulus (image or caption), our database also contained an equivalent stimulus in the other modality (caption or image). In this way, we could extract model features from the corresponding image for vision models, the corresponding caption for language models, and an multimodal representation of both image and caption for the multimodal models. We used publicly available pretrained models implemented in the HuggingFace Transformers library (<xref ref-type="bibr" rid="c129">Wolf et al., 2020</xref>) or from their respective authors’ repositories.</p>
<p>Model versions for unimodal models are as indicated in <xref rid="fig4" ref-type="fig">Figure 4</xref>. For multimodal models, the exact version for CLIP was <monospace>clip-vit-large-patch14</monospace>, for ViLT vilt-b32-mlm, for Visual-BERT <monospace>visualbert-nlvr2-coco-pre</monospace>, for Imagebind <monospace>imagebind_huge</monospace>, for Bridgetower <monospace>bridgetower-large-itm-mlm-itc</monospace>, for Flava flava-full, for SigLip siglip-so400m-patch14-384, and for Paligemma2 <monospace>paligemma2-3b-pt-224</monospace>.</p>
<p>We extracted language features from all models by averaging the outputs for each token, as this was established as common practice for the extraction of sentence embeddings from Transformer-based language models (e.g. <xref ref-type="bibr" rid="c62">Krasnowska-Kieraś and Wróblewska, 2019</xref>; <xref ref-type="bibr" rid="c101">Reimers and Gurevych, 2019</xref>).</p>
<p>For Transformer-based vision models, we compared representations extracted by averaging the outputs for each patch with representations extracted from [CLS] tokens in <xref rid="tblS1" ref-type="table">Table 1</xref>. We found that for almost all models, the mean features allow for higher decoding accuracies. For all experiments reported in the main paper we therefore only considered this method.</p>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Appendix 2—table 1.</label>
<caption><title>Feature comparison for vision models.</title>
<p>Pairwise accuracy for modality-agnostic decoders based on vision features extracted by averaging the last hidden states of all patches (“vision_features_mean”) compared to when using features extracted from [CLS] tokens (“vision_features_cls”). The method leading to the best decoding performance for each model is highlighted in bold.</p></caption>
<graphic xlink:href="658221v2_tblA2_1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>For multimodal models, we also compared a range of techniques for feature extraction. The results are shown in <xref rid="tblS2" ref-type="table">Table 2</xref>.</p>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Appendix 2—table 2.</label>
<caption><title>Feature comparison for multimodal models.</title>
<p>The features are either based on the [CLS] tokens from the fused representations (“fused_cls”), averaging over the fused tokens (“fused_mean”), or averaging over tokens from intermediate vision and language stream outputs (“avg”). For the last case, for some models the vision and language features can also be either based on &lt;monosapce&gt;[CLS] &lt;/monosapce&gt; or based on averaging over all tokens/patches. The method leading to the best decoding performance for each model is highlighted in bold.</p></caption>
<graphic xlink:href="658221v2_tblA2_2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<p>For dual-stream multimodal models, we averaged the vision and language features to create the final multimodal feature representation. For single-stream multimodal features we compared using representations extracted by averaging the outputs for each token with representations extracted from [CLS] token and found that the averaged output leads to better performance in almost all cases (see Table 2). Further, some single-stream models (Flava, Paligemma2 and BLIP2) allow for feature extraction based on intermediate vision and language representations in addition to a direct extraction of multimodal features (based on fused representations from the multimodal stream). We found that averaging features from these intermediate vision and language representations leads to better performance (see Table 2). For all results reported in the main paper we used the feature extraction method leading to best performance for each model.</p>
<sec id="s7a">
<title>Decoder training details</title>
<p>The decoders were linear ridge-regression models as implemented in the scikit-learn library (<xref ref-type="bibr" rid="c92">Pedregosa et al., 2011</xref>). All training data was standardized to have mean of 0 and standard deviation of 1. The test data was standardized using the mean and standard deviation of the training data. The regularization hyperparameter <italic>α</italic> was optimized using 5-fold cross validation on the training set (values considered: <italic>α</italic> ∈ {1<italic>e</italic>3, 1<italic>e</italic>4, 1<italic>e</italic>5, 1<italic>e</italic>6, 1<italic>e</italic>7}). Afterwards, a final model was trained using the best <italic>α</italic> on the whole training set.</p>
</sec>
</sec>
</app>
<app id="app3">
<label>Appendix 3</label>
<sec id="s8">
<title>Qualitative Decoding Results for Modality-Specific Decoders</title>
<p>In this section we present qualitative decoding results for modality-specific decoders for the same examples as presented in Section Qualitative Decoding Results for modality-agnostic decoders. The results show that modality-agnostic decoders are as good as modality-specific decoders when evaluated in a within-modality decoding setup (<xref rid="app3fig1" ref-type="fig">Figures 1</xref> and <xref rid="app3fig3" ref-type="fig">3</xref>) but sub-stantially better than modality-specific decoders when evaluated in a cross-decoding setup (<xref rid="app3fig2" ref-type="fig">Figures 2</xref> and <xref rid="app3fig4" ref-type="fig">4</xref>).</p>
<fig id="app3fig1" position="float" fig-type="figure">
<label>Appendix 3—figure 1.</label>
<caption><title>Decoding examples for image decoding using a modality-specific decoder trained on captions (cross-modality decoding).</title>
<p>The first column shows the image the subject was seeing and the 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA3_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app3fig2" position="float" fig-type="figure">
<label>Appendix 3—figure 2.</label>
<caption><title>Decoding examples for caption decoding using a modality-specific decoder trained on images (cross-modality decoding).</title>
<p>For details see caption of <xref ref-type="fig" rid="app3fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA3_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app3fig3" position="float" fig-type="figure">
<label>Appendix 3—figure 3.</label>
<caption><title>Decoding examples for caption decoding using a modality-specific decoder trained on captions (within-modality decoding).</title>
<p>For details see caption of <xref ref-type="app3fig1" rid="fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA3_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app3fig4" position="float" fig-type="figure">
<label>Appendix 3—figure 4.</label>
<caption><title>Decoding examples for image decoding using a modality-specific decoder trained on captions (cross-modality decoding).</title>
<p>For details see caption of <xref ref-type="fig" rid="app3fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA3_4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app id="app4">
<label>Appendix 4</label>
<sec id="s9">
<title>Candidates for Modality-invariant regions</title>
<p><xref rid="tblS3" ref-type="table">Table 1</xref> shows candidates for modality-invariant regions that were identified by previous work. We only considered fMRI experiments involving multiple stimulus modalities. Many other studies relied on unimodal stimuli and semantic tasks to identify modality-invariant regions, these are however less directly comparable to our setup.</p>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Appendix 4—table 1.</label>
<caption><title>Candidates for modality-invariant regions as identified by previous work.</title>
<p>All these regions were also found in our analysis, except for the 2 regions marked with an asterisk (*).</p></caption>
<graphic xlink:href="658221v2_tblA4_1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
<app id="app5">
<label>Appendix 5</label>
<sec id="s10">
<title>Searchlight size</title>
<p>In order to optimize the size of the searchlight for this analysis, we first ran a searchlight analysis with a fixed radius of 10mm using a modality-agnostic decoder on the data from the first subject (sub-01). Due to the shape of the cortex this leads of searchlights that contain varying numbers of vertices (on average: 897.4; max: 1580; min: 399). By observing the decoding scores as function of the number of vertices we find that performance peaks at 750 vertices (cf. <xref rid="tblS4" ref-type="table">Table 1</xref>). The final searchlight analyses was therefore performed with a searchlight of a fixed number of 750 vertices. The average radius with this number of vertices was 9.41mm (max: 13.65mm).</p>

<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Appendix 5—table 1.</label>
<caption><title>Average decoding scores (for images and captions) by number of vertices.</title>
<p>Scores calculated based on the results of a searchlight analysis with a radius of 10mm. The accuracy values were grouped into bins based on the number of vertices that the searchlight was based on.</p></caption>
<graphic xlink:href="658221v2_tblA5_1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
</app>
<app id="app6">
<label>Appendix 6</label>
<sec id="s11">
<title>Per-subject results</title>
<p>Results for individual subjects can be found in <xref rid="app6fig1" ref-type="fig">Figure 1</xref>. Among all subjects, we found similar converging results for decoding accuracies when comparing models, feature modalities, and modality-agnostic with modality-specific decoders.</p>
<fig id="app6fig1" position="float" fig-type="figure">
<label>Appendix 6—figure 1.</label>
<caption><title>Pairwise accuracy per subject.</title>
<p>For details refer to <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption>
<graphic xlink:href="658221v2_figA6_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
<app id="app7">
<label>Appendix 7</label>
<sec id="s12">
<title>Qualitative Imagery Decoding Results</title>
<p>The following <xref rid="app7fig1" ref-type="fig">figures 1</xref>-<xref rid="app7fig6" ref-type="fig">6</xref> present qualitative decoding results for the imagery conditions using a modality-agnostic decoder. We present the results separately for each subject as each subject chose an individual set of 3 stimuli to perform mental imagery on. In each plot, the leftmost column shows the caption that was used as initial instruction as well as the subject’s sketch that they drew at the end of the experiment.</p>
<p>We used the same large candidate set of 41K stimuli as for the qualitative decoding results for the other conditions (see also Section Qualitative Decoding Results).</p>
<p>Overall, we found that the decoding quality for imagery stimuli lags behind that for trials with perceived stimuli. This was expected and confirms the quantitative results reported in Section Imagery Decoding. Still, in several cases some of the concepts are decoded correctly (e.g. a women in the first row of <xref rid="app7fig1" ref-type="fig">Figure 1</xref>; winter sports in the second row of <xref rid="app7fig3" ref-type="fig">Figure 3</xref>; laptops/screens and multiple people in the third row of <xref rid="app7fig3" ref-type="fig">Figure 3</xref>).</p>
<fig id="app7fig1" position="float" fig-type="figure">
<label>Appendix 7—figure 1.</label>
<caption><title>Imagery decoding for Subject 1 using a modality-agnostic decoder.</title>
<p>The first column shows the caption that was used to stimulate imagery and on top of it the sketch of their mental image that the subjects draw at the end of the experiment. The 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA7_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app7fig2" position="float" fig-type="figure">
<label>Appendix 7—figure 2.</label>
<caption><title>Imagery decoding for Subject 2 using a modality-agnostic decoder.</title>
<p>For further details refer to the caption of <xref ref-type="fig" rid="app7fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA7_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app7fig3" position="float" fig-type="figure">
<label>Appendix 7—figure 3.</label>
<caption><title>Imagery decoding for Subject 3 using a modality-agnostic decoder.</title>
<p>For further details refer to the caption of <xref ref-type="fig" rid="app7fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA7_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app7fig4" position="float" fig-type="figure">
<label>Appendix 7—figure 4.</label>
<caption><title>Imagery decoding for Subject 4 using a modality-agnostic decoder.</title>
<p>For further details refer to the caption of <xref ref-type="fig" rid="app7fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA7_4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app7fig5" position="float" fig-type="figure">
<label>Appendix 7—figure 5.</label>
<caption><title>Imagery decoding for Subject 5 using a modality-agnostic decoder.</title>
<p>For further details refer to the caption of <xref ref-type="fig" rid="app7fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA7_5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="app7fig6" position="float" fig-type="figure">
<label>Appendix 7—figure 6.</label>
<caption><title>Imagery decoding for Subject 6 using a modality-agnostic decoder.</title>
<p>For further details refer to the caption of <xref ref-type="fig" rid="app7fig1">Figure 1</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="c69">Lin et al., 2014</xref>).</p></caption>
<graphic xlink:href="658221v2_figA7_6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Bi</surname>
<given-names>Yanchao</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution>
</institution-wrap>
<city>Beijing</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>The study introduces a <bold>valuable</bold> dataset for investigating the relationship between vision and language in the brain. The authors provide <bold>convincing</bold> evidence that decoders trained on brain responses to both images and captions outperform those trained on responses to a single modality. The dataset and decoder results will be of interest to communities studying brain and machine decoding.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This work presents a modality-agnostic decoder trained on a large fMRI dataset (SemReps-8K), in which subjects viewed natural images and corresponding captions. The decoder predicts stimulus content from brain activity irrespective of the input modality and performs on par with-or even outperforms-modality-specific decoders. Its success depends more on the diversity of brain data (multimodal vs. unimodal) than on whether the feature-extraction models are visual, linguistic, or multimodal. Particularly, the decoder shows strong performance in decoding imagery content. These results suggest that the modality-agnostic decoder effectively leverages shared brain information across image and caption tasks.</p>
<p>Strengths:</p>
<p>(1) The modality-agnostic decoder compellingly leverages multimodal brain information, improving decoding accuracy-particularly for non-sensory input such as captions-showing high methodological and application value.</p>
<p>(2) The dataset is a substantial and well-controlled contribution, with &gt;8,000 image-caption trials per subject and careful matching of stimuli across modalities-an essential resource for testing theories about different representational modalities.</p>
<p>Weakness:</p>
<p>In the searchlight analysis aimed at identifying modality-invariant representations, although the combined use of four decoding conditions represents a relatively strict approach, the underlying logic remains unclear. The modality-agnostic decoder has demonstrated strong sensitivity in decoding brain activity, as shown earlier in the paper, whereas the cross-decoding with modality-specific decoders is inherently more conservative. If, as the authors note, the modality-agnostic decoder might have learned to leverage different features to project stimuli from different modalities, then taking the union of conditions would seem more appropriate. Conversely, if the goal is to obtain a more conservative result, why not focus solely on the cross-decoding conditions? The relationships among the four decoding conditions are not clearly delineated, and the contrasts between them might themselves yield valuable insights. As it stands, however, the logic of the current approach is not straightforward.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors recorded brain responses while participants viewed images and captions. The images and captions were taken from the COCO dataset, so each image has a corresponding caption and each caption has a corresponding image. This enabled the authors to extract features from either the presented stimulus or the corresponding stimulus in the other modality. The authors trained linear decoders to take brain responses and predict stimulus features. &quot;Modality-specific&quot; decoders were trained on brain responses to either images or captions while &quot;modality-agnostic&quot; decoders were trained on brain responses to both stimulus modalities. The decoders were evaluated on brain responses while the participants viewed and imagined new stimuli, and prediction performance was quantified using pairwise accuracy. The authors reported the following results:</p>
<p>(1) Decoders trained on brain responses to both images and captions can predict new brain responses to either modality.</p>
<p>(2) Decoders trained on brain responses to both images and captions outperform decoders trained on brain responses to a single modality.</p>
<p>(3) Many cortical regions represent the same concepts in vision and language.</p>
<p>(4) Decoders trained on brain responses to both images and captions can decode brain responses to imagined scenes.</p>
<p>Strengths:</p>
<p>This is an interesting study that addresses important questions about modality-agnostic representations. Previous work has shown that decoders trained on brain responses to one modality can be used to decode brain responses to another modality. The authors build on these findings by collecting a new multimodal dataset and training decoders on brain responses to both modalities.</p>
<p>To my knowledge, SemReps-8K is the first dataset of brain responses to vision and language where each stimulus item has a corresponding stimulus item in the other modality. This means that brain responses to a stimulus item can be modeled using visual features of the image, linguistic features of the caption, or multimodal features derived from both the image and the caption. The authors also employed a multimodal one-back matching task which forces the participants to activate modality-agnostic representations. Overall, SemReps-8K is a valuable resource that will help researchers answer more questions about modality-agnostic representations.</p>
<p>The analyses are also very comprehensive. The authors trained decoders on brain responses to images, captions, and both modalities, and they tested the decoders on brain responses to images, caption, and imagined scenes. They extracted stimulus features using a range of visual, linguistic, and multimodal models. The modeling framework appears rigorous and the results offer new insights into the relationship between vision, language, and imagery. In particular, the authors found that decoders trained on brain responses to both images and captions were more effective at decoding brain responses to imagined scenes than decoders trained on brain responses to either modality in isolation. The authors also found that imagined scenes can be decoded from a broad network of cortical regions.</p>
<p>Weaknesses:</p>
<p>The characterization of &quot;modality-agnostic&quot; and &quot;modality-specific&quot; decoders seems a bit contradictory. There are three major choices when fitting a decoder: the modality of the training stimuli, the modality of the testing stimuli, and the model used to extract stimulus features. However, the authors characterize their decoders based on only the first choice-&quot;modality-specific&quot; decoders were trained on brain responses to either images or captions while &quot;modality-agnostic&quot; decoders were trained on brain responses to both stimulus modalities. I think that this leads to some instances where the conclusions are inconsistent with the methods and results.</p>
<p>First, the authors suggest that &quot;modality-specific decoders are not explicitly encouraged to pick up on modality-agnostic features during training&quot; (line 137) while &quot;modality-agnostic decoders may be more likely to leverage representations that are modality-agnostic&quot; (line 140). However, whether a decoder is required to learn modality-agnostic representations depends on both the training responses and the stimulus features. Consider the case where the stimuli are represented using linguistic features of the captions. When you train a &quot;modality-specific&quot; decoder on image responses, the decoder is forced to rely on modality-agnostic information that is shared between the image responses and the caption features. On the other hand, when you train a &quot;modality-agnostic&quot; decoder on both image responses and caption responses, the decoder has access to the modality-specific information that is shared by the caption responses and the caption features, so it is not explicitly required to learn modality-agnostic features. As a result, while the authors show that &quot;modality-agnostic&quot; decoders outperform &quot;modality-specific&quot; decoders in most conditions, I am not convinced that this is because they are forced to learn more modality-agnostic features.</p>
<p>Second, the authors claim that &quot;modality-specific decoders can be applied only in the modality that they were trained on&quot; while &quot;modality-agnostic decoders can be applied to decode stimuli from multiple modalities, even without knowing a priori the modality the stimulus was presented in&quot; (line 47). While &quot;modality-agnostic&quot; decoders do outperform &quot;modality-specific&quot; decoders in the cross-modality conditions, it is important to note that &quot;modality-specific&quot; decoders still perform better than expected by chance (figure 5). It is also important to note that knowing about the input modality still improves decoding performance even for &quot;modality-agnostic&quot; decoders, since it determines the optimal feature space-it is better to decode brain responses to images using decoders trained on image features, and it is better to decode brain responses to captions using decoders trained on caption features.</p>
<p>Comments on revised version:</p>
<p>The revised version benefits from clearer claims and more precise terminology (i.e. classifying the decoders as &quot;modality-agnostic&quot; or &quot;modality-specific&quot; while classifying the representations as &quot;modality-invariant&quot; or &quot;modality-dependent&quot;).</p>
<p>While the modality-agnostic decoders outperform the modality-specific decoders, I am still not convinced that this is because they are &quot;explicitly trained to leverage the shared information in modality-invariant patterns of the brain activity&quot;. On one hand, the high-level feature spaces may each contain some amount of modality-invariant information, so even modality-specific decoders can capture some modality-invariant information. On the other hand, I do not see how training the modality-agnostic decoders on responses to both modalities necessitates that they learn modality-invariant representations beyond those that are learned by the modality-specific decoders.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107933.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Nikolaus</surname>
<given-names>Mitja</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5609-6628</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Mozafari</surname>
<given-names>Milad</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4521-1640</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Berry</surname>
<given-names>Isabelle</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Asher</surname>
<given-names>Nicholas</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Reddy</surname>
<given-names>Leila</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7078-1055</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>VanRullen</surname>
<given-names>Rufin</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews</p>
<p>We would like to thank all reviewers for their constructive and in-depth reviews. Thanks to your feedback, we realized that the main objective of the paper was not presented clearly enough, and that our use of the same “modality-agnostic” terminology for both decoders and representations caused confusion. We addressed these two major points as outlined in the following.</p>
<p>In the revised manuscript, we highlight that the main contribution of this paper is to introduce modality-agnostic decoders. Apart from introducing this new decoder type, we put forward their advantages in comparison to modality-specific decoders in terms of decoding performance and analyze the modality-invariant representations (cf. updated terminology in the following paragraph) that these decoders rely on. The dataset that these analyses are based on is released as part of this paper, in the spirit of open science (but this dataset is only a secondary contribution for our paper).</p>
<p>Regarding the terminology, we clearly define modality-agnostic decoders as decoders that are trained on brain imaging data from subjects exposed to stimuli in multiple modalities. The decoder is not given any information on which modality a stimulus was presented in, and is therefore trained to operate in a modality-agnostic way. In contrast, modality-specific decoders are trained only on data from a single stimulus modality. These terms are explained in Figure 2. While these terms describe different ways of how decoders can be trained, there are also different ways to evaluate them afterwards (see also Figure 3); but obviously, this test-time evaluation does not change the nature of the decoder, i.e., there is no contradiction in applying a modality-specific decoder to brain data from a different modality.</p>
<p>Further, we identify representations that are relevant for modality-agnostic decoders using the searchlight analysis. We realized that our choice of using the same “modality-agnostic” term to describe these brain representations created unnecessary debate and confusion. In order to not conflate the terminology, in the updated manuscript we call these representations modality-invariant (and the opposite modality-dependent). Our methodology does not allow us to distinguish whether certain representations merely share representational structure to a certain degree, or are truly representations that abstract away from any modality-dependent information. However, in order to be useful for modality-agnostic decoding, a significant degree of shared representational structure is sufficient, and it is this property of brain representations that we now define as “modality-invariant”.</p>
<p>We updated the manuscript in line with this new terminology and focus: in particular, the first Related Work section on Modality-invariant brain representations, as well as the Introduction and Discussion.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>Summary:</p>
<p>The authors introduce a densely-sampled dataset where 6 participants viewed images and sentence descriptions derived from the MS Coco database over the course of 10 scanning sessions. The authors further showcase how image and sentence decoders can be used to predict which images or descriptions were seen, using pairwise decoding across a set of 120 test images. The authors find decodable information widely distributed across the brain, with a left-lateralized focus. The results further showed that modality-agnostic models generally outperformed modality-specific models, and that data based on captions was not explained better by caption-based models but by modality-agnostic models. Finally, the authors decoded imagined scenes.</p>
<p>Strengths:</p>
<p>(1) The dataset presents a potentially very valuable resource for investigating visual and semantic representations and their interplay.</p>
<p>(2) The introduction and discussion are very well written in the context of trying to understand the nature of multimodal representations and present a comprehensive and very useful review of the current literature on the topic.</p>
<p>Weaknesses:</p>
<p>(1) The paper is framed as presenting a dataset, yet most of it revolves around the presentation of findings in relation to what the authors call modality-agnostic representations, and in part around mental imagery. This makes it very difficult to assess the manuscript, whether the authors have achieved their aims, and whether the results support the conclusions.</p>
</disp-quote>
<p>Thanks for this insightful remark. The dataset release is only a secondary contribution of our study; this was not clear enough in the previous version. We updated the manuscript to make the main objective of the paper more clear, as outlined in our general response to the reviews (see above).</p>
<disp-quote content-type="editor-comment">
<p>(2) While the authors have presented a potential use case for such a dataset, there is currently far too little detail regarding data quality metrics expected from the introduction of similar datasets, including the absence of head-motion estimates, quality of intersession alignment, or noise ceilings of all individuals.</p>
</disp-quote>
<p>As already mentioned in the general response, the main focus of the paper is to introduce modality-agnostic decoders. The dataset is released in addition, this is why we did not focus on reporting extensive quality metrics in the original manuscript. To respond to your request, we updated the appendix of the manuscript to include a range of data quality metrics.</p>
<p>The updated appendix includes head motion estimates in the form of realignment parameters and framewise displacement, as well as a metric to assess the quality of intersession alignment. More detailed descriptions can be found in Appendix 1 of the updated manuscript.</p>
<p>Estimating noise ceilings based on repeated presentations of stimuli (as for example done in Allen et al. (2022)) requires multiple betas for each stimulus. All training stimuli were only presented once, so this could only be done for the test stimuli which were presented repeatedly. However, during our preprocessing procedure we directly calculated stimulus-specific betas based on data from all sessions using one single GLM, which means that we did not obtain separate betas for repeated presentations of the same stimulus. We will however share the raw data publicly, so that such noise ceilings can be calculated using an adapted preprocessing procedure if required.</p>
<p>Allen, E. J., St-Yves, G., Wu, Y., Breedlove, J. L., Prince, J. S., Dowdle, L. T., Nau, M., Caron, B., Pestilli, F., Charest, I., Hutchinson, J. B., Naselaris, T., &amp; Kay, K. (2022). A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience, 25(1), 116–126. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-021-00962-x">https://doi.org/10.1038/s41593-021-00962-x</ext-link></p>
<disp-quote content-type="editor-comment">
<p>(3) The exact methods and statistical analyses used are still opaque, making it hard for a reader to understand how the authors achieved their results. More detail in the manuscript would be helpful, specifically regarding the exact statistical procedures, what tests were performed across, or how data were pooled across participants.</p>
</disp-quote>
<p>In the updated manuscript, we improved the level of detail for the descriptions of statistical analyses wherever possible (see also our response to your “Recommendations for the authors”, Point 6).</p>
<p>Regarding data pooling across participants:</p>
<p>Figure 8 shows averaged results across all subjects (as indicated in the caption)</p>
<p>Regarding data pooling for the estimation of the significance threshold of the searchlight analysis for modality-invariant regions: We updated the manuscript to clarify that we performed a permutation test, combined with a bootstrapping procedure to estimate a group-level null distribution: “For each subject, we evaluated the decoders 100 times with shuffled labels to create per-subject chance-level results. Then, we randomly selected one of the 100 chance-level results for each of the 6 subjects and calculated group-level statistics (TFCE values) the exact same way as described in the preceding paragraph. We repeated this procedure 10,000 times resulting in 10,000 permuted group-level results.”</p>
<p>Additionally, we indicated that the same permutation testing methods were applied to assess the significance threshold for the imagery decoding searchlight maps (Figure 10).</p>
<disp-quote content-type="editor-comment">
<p>(4) Many findings (e.g., Figure 6) are still qualitative but could be supported by quantitative measures.</p>
</disp-quote>
<p>The Figures 6 and 7 are intentionally qualitative results to support the quantitative decoding results presented in Figures 4 and 5. (see also Reviewer 2 Comment 2)</p>
<p>Figures 4 and 5 show pairwise decoding accuracy as a quantitative measure for evaluation of the decoders. This metric is the main metric we used to compare different decoder types and features. Based on the finding that modality-agnostic decoders using imagebind features achieve the best score on this metric, we performed the additional qualitative analysis presented in Figures 6 and 7. (Note that we expanded the candidate set for the qualitative analysis in order to have a larger and more diverse set of images.)</p>
<disp-quote content-type="editor-comment">
<p>(5) Results are significant in regions that typically lack responses to visual stimuli, indicating potential bias in the classifier. This is relevant for the interpretation of the findings. A classification approach less sensitive to outliers (e.g., 70-way classification) could avoid this issue. Given the extreme collinearity of the experimental design, regressors in close temporal proximity will be highly similar, which could lead to leakage effects.</p>
</disp-quote>
<p>It is true that our searchlight analysis revealed significant activity in regions outside of the visual cortex. However, it is assumed that the processing of visual information does not stop at the border of the visual cortex. The integration of information such as the semantics of the image is progressively processed in other higher-level regions of the brain. Recent studies have shown that activity in large areas of the cortex (including many outside of the visual cortex) can be related to visual stimulation (Solomon et al. 2024; Raugel et al. 2025). Our work confirms this finding and we therefore do not see reason to believe that this is due to a bias in our decoders.</p>
<p>Further, you are suggesting that we could replace our regression approach with a 70-way classification. However, this is difficult using our fMRI data as we do not see a straightforward way to assign the training and testing stimuli with class labels (the two datasets consist of non-overlapping sets of naturalistic images).</p>
<p>To address your concerns regarding the collinearity of the experimental design and possible leakage effects, we trained and evaluated a decoder for one subject after running a “null-hypothesis” adapted preprocessing. More specifically, for all sessions, we shifted the functional data of all runs by one run (moving the data of the last run to the very front), but leaving the design matrices in place. Thereby, we destroyed the relationship of stimuli and brain activity but kept the original data and design with its collinearity (and possible biases). We preprocessed this adapted data for subject 1, and ran a whole-brain decoding using Imagebind features and verified that the decoding performance was at chance level:  Pairwise accuracy (captions): 0.43 | Pairwise accuracy (images): 0.47 | Pairwise accuracy (imagery): 0.50. This result provides evidence against the notion that potential collinearity or biases in our experimental design or evaluation procedure could have led to inflated results.</p>
<p>Raugel, J., Szafraniec, M., Vo, H.V., Couprie, C., Labatut, P., Bojanowski, P., Wyart, V. and King, J.R. (2025). Disentangling the Factors of Convergence between Brains and Computer Vision Models. arXiv preprint arXiv:2508.18226.</p>
<p>Solomon, S. H., Kay, K., &amp; Schapiro, A. C. (2024). Semantic plasticity across timescales in the human brain. bioRxiv, 2024-02.</p>
<disp-quote content-type="editor-comment">
<p>(6) The manuscript currently lacks a limitations section, specifically regarding the design of the experiment. This involves the use of the overly homogenous dataset Coco, which invites overfitting, the mixing of sentence descriptions and visual images, which invites imagery of previously seen content, and the use of a 1-back task, which can lead to carry-over effects to the subsequent trial.</p>
</disp-quote>
<p>Regarding the dataset CoCo: We agree that CoCo is somewhat homogenous, it is however much more diverse and naturalistic than the smaller datasets used in previous fMRI experiments with multimodal stimuli. Additionally, CoCo has been widely adopted as a benchmark dataset in the Machine Learning community, and features rich annotations for each image (e.g. object labels, segmentations, additional captions, people’s keypoints) facilitating many more future analyses based on our data.</p>
<p>Regarding the mixing of sentence descriptions and images: Subjects were not asked to visualize sentences and different techniques for the one-back tasks might have been used. Generally, we do not see it as problematic if subjects are performing visual imagery to some degree while reading sentences, and this might even be the case during normal reading as well. A more targeted experiment comparing reading with and without interleaved visual stimulation in the form of images and a one-back task would be required to assess this, but this was not the focus of our study. For now, it is true that we can not be sure that our results generalize to cases in which subjects are just reading and are less incentivized to perform mental imagery.</p>
<p>Regarding the use of a 1-back task: It was necessary to make some design choices in order to realize this large-scale data collection with approximately 10 hours of recording per subject. Specifically, the 1-back task was included in the experimental setup in order to assure continuous engagement of the participant during the rather long sessions of 1 hour. The subjects did indeed need to remember the previous stimulus to succeed at the 1-back task, which means that some brain activity during the presentation of a stimulus is likely to be related to the previous stimulus. We aimed to account for this confound during the preprocessing stage when fitting the GLM, which was fit to capture only the response to the presented image/caption, not the preceding one. Still, it might have picked up on some of the activity from preceding stimuli, causing some decrease of the final decoding performance.</p>
<p>We added a limitations section to the updated manuscript to discuss these important issues.</p>
<disp-quote content-type="editor-comment">
<p>(7) I would urge the authors to clarify whether the primary aim is the introduction of a dataset and showing the use of it, or whether it is the set of results presented. This includes the title of this manuscript. While the decoding approach is very interesting and potentially very valuable, I believe that the results in the current form are rather descriptive, and I'm wondering what specifically they add beyond what is known from other related work. This includes imagery-related results. This is completely fine! It just highlights that a stronger framing as a dataset is probably advantageous for improving the significance of this work.</p>
</disp-quote>
<p>Thanks a lot for pointing this out. Based on this comment and feedback from the other reviewers we restructured the abstract, introduction and discussion section of the paper to better reflect the primary aim. (cf. general response above).</p>
<p>You further mention that it is not clear what our results add beyond what is known from related work. We list the main contributions here:</p>
<p>A single modality-agnostic decoder can decode the semantics of visual and linguistic stimuli irrespective of the presentation modality with a performance that is not lagging behind modality-specific decoders.</p>
<p>Modality-agnostic decoders outperform modality-specific decoders for decoding captions and mental imagery.</p>
<p>Modality-invariant representations are widespread across the cortex (a range of previous work has suggested they were much more localized (Bright et al. 2004; Jung et al. 2018; Man et al. 2012; Simanova et al. 2014).</p>
<p>Regions that are useful for imagery are largely overlapping with modality-invariant regions</p>
<p>Bright, P., Moss, H., &amp; Tyler, L. K. (2004). Unitary vs multiple semantics: PET studies of word and picture processing. Brain and language, 89(3), 417-432.</p>
<p>Jung, Y., Larsen, B., &amp; Walther, D. B. (2018). Modality-Independent Coding of Scene Categories in Prefrontal Cortex. Journal of Neuroscience, 38(26), 5969–5981.</p>
<p>Liuzzi, A. G., Bruffaerts, R., Peeters, R., Adamczuk, K., Keuleers, E., De Deyne, S., Storms, G., Dupont, P., &amp; Vandenberghe, R. (2017). Cross-modal representation of spoken and written word meaning in left pars triangularis. NeuroImage, 150, 292–307. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.02.032">https://doi.org/10.1016/j.neuroimage.2017.02.032</ext-link></p>
<p>Man, K., Kaplan, J. T., Damasio, A., &amp; Meyer, K. (2012). Sight and Sound Converge to Form Modality-Invariant Representations in Temporoparietal Cortex. Journal of Neuroscience, 32(47), 16629–16636.</p>
<p>Simanova, I., Hagoort, P., Oostenveld, R., &amp; van Gerven, M. A. J. (2014). Modality-Independent Decoding of Semantic Information from the Human Brain. Cerebral Cortex, 24(2), 426–434.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>Summary:</p>
<p>This study introduces SemReps-8K, a large multimodal fMRI dataset collected while subjects viewed natural images and matched captions, and performed mental imagery based on textual cues. The authors aim to train modality-agnostic decoders--models that can predict neural representations independently of the input modality - and use these models to identify brain regions containing modality-agnostic information. They find that such decoders perform comparably or better than modality-specific decoders and generalize to imagery trials.</p>
<p>Strengths:</p>
<p>(1) The dataset is a substantial and well-controlled contribution, with &gt;8,000 image-caption trials per subject and careful matching of stimuli across modalities - an essential resource for testing theories of abstract and amodal representation.</p>
<p>(2) The authors systematically compare unimodal, multimodal, and cross-modal decoders using a wide range of deep learning models, demonstrating thoughtful experimental design and thorough benchmarking.</p>
<p>(3) Their decoding pipeline is rigorous, with informative performance metrics and whole-brain searchlight analyses, offering valuable insights into the cortical distribution of shared representations.</p>
<p>(4) Extension to mental imagery decoding is a strong addition, aligning with theoretical predictions about the overlap between perception and imagery.</p>
<p>Weaknesses:</p>
<p>While the decoding results are robust, several critical limitations prevent the current findings from conclusively demonstrating truly modality-agnostic representations:</p>
<p>(1) Shared decoding ≠ abstraction: Successful decoding across modalities does not necessarily imply abstraction or modality-agnostic coding. Participants may engage in modality-specific processes (e.g., visual imagery when reading, inner speech when viewing images) that produce overlapping neural patterns. The analyses do not clearly disambiguate shared representational structure from genuinely modality-independent representations. Furthermore, in Figure 5, the modality-agnostic encoder did not perform better than the modality-specific decoder trained on images (in decoding images), but outperformed the modality-specific decoder trained on captions (in decoding captions). This asymmetry contradicts the premise of a truly &quot;modality-agnostic&quot; encoder. Additionally, given the similar performance between modality-agnostic decoders based on multimodal versus unimodal features, it remains unclear why neural representations did not preferentially align with multimodal features if they were truly modality-independent.</p>
</disp-quote>
<p>We agree that successful modality-agnostic and cross-modal decoding does not necessarily imply that abstract patterns were decoded. In the updated manuscript, we therefore refer to these representations as modality-invariant (see also the updated terminology explained in the general response above).</p>
<p>If participants are performing mental imagery when reading, and this is allowing us to perform cross-decoding, then this means that modality-invariant representations are formed during this mental imagery process, i.e. that the representations formed during this form of mental imagery are compatible with representations during visual perception (or, in your words, produce overlapping neural patterns). While we can not know to what extent people were performing mental imagery while reading (or having inner speech while viewing images), our results demonstrate that their brain activity allows for decoding across modalities, which implies that modality-invariant representations are present.</p>
<p>It is true that our current analyses can not disambiguate modality-invariant representations (or, in your words, shared representational structure) from abstract representations (in your words, genuinely modality-independent representations). As the main goal of the paper was to build modality-agnostic decoders, and these only require what we call “modality-invariant” representations (see our updated terminology in the general reviewer response above), we leave this question open for future work. We do however discuss this important limitation in the Discussion section of the updated manuscript.</p>
<p>Regarding the asymmetry of decoding results when comparing modality-agnostic decoders with the two respective modality-specific decoders for captions and images: We do not believe that this asymmetry contradicts the premise of a modality-agnostic decoder. Multiple explanations for this result are possible: (1) The modality-specific decoder for images might benefit from the more readily decodable lower-level modality-dependent neural activity patterns in response to images, which are less useful for the modality-agnostic decoder because they are not useful for decoding caption trials. The modality-specific decoders for captions might not be able to pick up on low-level modality-dependent neural activity patterns as these might be less easily decodable.</p>
<p>The signal-to-noise ratio for caption trials might be lower than for image trials (cf. generally lower caption decoding performance), therefore the addition of training data (even if it is from another modality) improves the decoding performance for captions, but not for images (which might be at ceiling already).</p>
<p>Regarding the similar performance between modality-agnostic decoders based on multimodal versus unimodal features: Unimodal features are based on rather high-level features of the respective modality (e.g. last-layer features of a model trained for semantic image classification), which can be already modality-invariant to some degree. Additionally, as already mentioned before, in the updated manuscript we only require representations to be modality-invariant and not necessarily abstract.</p>
<disp-quote content-type="editor-comment">
<p>(2) The current analysis cannot definitively conclude that the decoder itself is modality-agnostic, making &quot;Qualitative Decoding Results&quot; difficult to interpret in this context. This section currently provides illustrative examples, but lacks systematic quantitative analyses.</p>
</disp-quote>
<p>The qualitative decoding results in Figures 6 and 7 present exemplary qualitative results for the quantitative results presented in Figures 4 and 5 (see also Reviewer 1 Comment 4).</p>
<p>Figures 4 and 5 show pairwise decoding accuracy as a quantitative measure for evaluation of the decoders. This metric is the main metric we used to compare different decoder types and features. Based on the finding that modality-agnostic decoders using imagebind features achieve the best score on this metric, we performed the additional qualitative analysis presented in Figures 6 and 7. (Note that we expanded the candidate set for the qualitative analysis in order to have a larger and more diverse set of images.)</p>
<disp-quote content-type="editor-comment">
<p>(3) The use of mental imagery as evidence for modality-agnostic decoding is problematic.</p>
</disp-quote>
<p>Imagery involves subjective, variable experiences and likely draws on semantic and perceptual networks in flexible ways. Strong decoding in imagery trials could reflect semantic overlap or task strategies rather than evidence of abstraction.</p>
<p>It is true that mental imagery does not necessarily rely on modality-agnostic representations. In the updated manuscript we revised our terminology and refer to the analyzed representations as modality-invariant, which we define as “representations that significantly overlap between modalities”.</p>
<disp-quote content-type="editor-comment">
<p>The manuscript presents a methodologically sophisticated and timely investigation into shared neural representations across modalities. However, the current evidence does not clearly distinguish between shared semantics, overlapping unimodal processes, and true modality-independent representations. A more cautious interpretation is warranted.</p>
<p>Nonetheless, the dataset and methodological framework represent a valuable resource for the field.</p>
</disp-quote>
<p>We fully agree with these observations, and updated our terminology as outlined in the general response.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public review):</bold></p>
<p>Summary:</p>
<p>The authors recorded brain responses while participants viewed images and captions. The images and captions were taken from the COCO dataset, so each image has a corresponding caption, and each caption has a corresponding image. This enabled the authors to extract features from either the presented stimulus or the corresponding stimulus in the other modality.</p>
<p>The authors trained linear decoders to take brain responses and predict stimulus features.</p>
<p>&quot;Modality-specific&quot; decoders were trained on brain responses to either images or captions, while &quot;modality-agnostic&quot; decoders were trained on brain responses to both stimulus modalities. The decoders were evaluated on brain responses while the participants viewed and imagined new stimuli, and prediction performance was quantified using pairwise accuracy. The authors reported the following results:</p>
<p>(1) Decoders trained on brain responses to both images and captions can predict new brain responses to either modality.</p>
<p>(2) Decoders trained on brain responses to both images and captions outperform decoders trained on brain responses to a single modality.</p>
<p>(3) Many cortical regions represent the same concepts in vision and language.</p>
<p>(4) Decoders trained on brain responses to both images and captions can decode brain responses to imagined scenes.</p>
<p>Strengths:</p>
<p>This is an interesting study that addresses important questions about modality-agnostic representations. Previous work has shown that decoders trained on brain responses to one modality can be used to decode brain responses to another modality. The authors build on these findings by collecting a new multimodal dataset and training decoders on brain responses to both modalities.</p>
<p>To my knowledge, SemReps-8K is the first dataset of brain responses to vision and language where each stimulus item has a corresponding stimulus item in the other modality. This means that brain responses to a stimulus item can be modeled using visual features of the image, linguistic features of the caption, or multimodal features derived from both the image and the caption. The authors also employed a multimodal one-back matching task, which forces the participants to activate modality-agnostic representations. Overall, SemReps-8K is a valuable resource that will help researchers answer more questions about modality-agnostic representations.</p>
<p>The analyses are also very comprehensive. The authors trained decoders on brain responses to images, captions, and both modalities, and they tested the decoders on brain responses to images, captions, and imagined scenes. They extracted stimulus features using a range of visual, linguistic, and multimodal models. The modeling framework appears rigorous, and the results offer new insights into the relationship between vision, language, and imagery. In particular, the authors found that decoders trained on brain responses to both images and captions were more effective at decoding brain responses to imagined scenes than decoders trained on brain responses to either modality in isolation. The authors also found that imagined scenes can be decoded from a broad network of cortical regions.</p>
<p>Weaknesses:</p>
<p>The characterization of &quot;modality-agnostic&quot; and &quot;modality-specific&quot; decoders seems a bit contradictory. There are three major choices when fitting a decoder: the modality of the training stimuli, the modality of the testing stimuli, and the model used to extract stimulus features. However, the authors characterize their decoders based on only the first choice-&quot;modality-specific&quot; decoders were trained on brain responses to either images or captions, while &quot;modality-agnostic&quot; decoders were trained on brain responses to both stimulus modalities. I think that this leads to some instances where the conclusions are inconsistent with the methods and results.</p>
</disp-quote>
<p>In our analysis setup, a decoder is entirely determined by two factors: (1) the modality of the stimuli that the subject was exposed to, and (2) the machine learning model used to extract stimulus features.</p>
<p>The modality of the testing stimuli defines whether we are evaluating the decoder in a within-modality or cross-modality setting, but is not an inherent characteristic of a trained decoder</p>
<disp-quote content-type="editor-comment">
<p>First, the authors suggest that &quot;modality-specific decoders are not explicitly encouraged to pick up on modality-agnostic features during training&quot; (line 137) while &quot;modality-agnostic decoders may be more likely to leverage representations that are modality-agnostic&quot; (line 140). However, whether a decoder is required to learn modality-agnostic representations depends on both the training responses and the stimulus features. Consider the case where the stimuli are represented using linguistic features of the captions. When you train a &quot;modality-specific&quot; decoder on image responses, the decoder is forced to rely on modality-agnostic information that is shared between the image responses and the caption features. On the other hand, when you train a &quot;modality-agnostic&quot; decoder on both image responses and caption responses, the decoder has access to the modality-specific information that is shared by the caption responses and the caption features, so it is not explicitly required to learn modality-agnostic features. As a result, while the authors show that &quot;modality-agnostic&quot; decoders outperform &quot;modality-specific&quot; decoders in most conditions, I am not convinced that this is because they are forced to learn more modality-agnostic features.</p>
</disp-quote>
<p>It is true that for example a modality-specific decoder trained on fmri data from images with stimulus features extracted from captions might also rely on modality-invariant features. We still call this decoder modality-specific, as it has been trained to decode brain activity recorded from a specific stimulus modality. In the updated manuscript we corrected the statement that “modality-specific decoders are not explicitly encouraged to pick up on modality-invariant features during training” to include the case of decoders trained on features from the other modality which might also rely on modality-invariant features.</p>
<p>It is true that a modality-agnostic decoder can also have access to modality-dependent information for captions and images. However, as it is trained jointly with both modalities and the modality-dependent features are not compatible, it is encouraged to rely on modality-invariant features. The result that modality-agnostic decoders are outperforming modality-specific decoders trained on captions for decoding captions confirms this, because if the decoder was only relying on modality-dependent features the addition of additional training data from another stimulus modality could not increase the performance. (Also, the lack of a performance drop compared to modality-specific decoders trained on images is only possible thanks to the reliance on modality-invariant features. If the decoder only relied on modality-dependent features the addition of data from another modality would equal an addition of noise to the training data which must result in a performance drop at test time.). We can not exclude the possibility that modality-agnostic decoders are also relying on modality-dependent features, but our results suggest that they are relying at least to some degree on modality-invariant features.</p>
<disp-quote content-type="editor-comment">
<p>Second, the authors claim that &quot;modality-specific decoders can be applied only in the modality that they were trained on, while &quot;modality-agnostic decoders can be applied to decode stimuli from multiple modalities, even without knowing a priori the modality the stimulus was presented in&quot; (line 47). While &quot;modality-agnostic&quot; decoders do outperform &quot;modality-specific&quot; decoders in the cross-modality conditions, it is important to note that &quot;modality-specific&quot; decoders still perform better than expected by chance (figure 5). It is also important to note that knowing about the input modality still improves decoding performance even for &quot;modality-agnostic&quot; decoders, since it determines the optimal feature space-it is better to decode brain responses to images using decoders trained on image features, and it is better to decode brain responses to captions using decoders trained on caption features.</p>
</disp-quote>
<p>Thanks for this important remark. We corrected this statement and now say that “modality-specific decoders that are trained to be applied only in the modality that they were trained on”, highlighting that their training process optimizes them for decoding in a specific modality. They can indeed be applied to the other modality at test time, this however results in a substantial performance drop.</p>
<p>It is true that knowing the input modality can improve performance even for modality-agnostic decoders. This can most likely be explained by the fact that in that case the decoder can leverage both, modality-invariant and modality-dependent features. We will not further focus on this result however as the main motivation to build modality-agnostic decoders is to be able to decode stimuli without knowing the stimulus modality a priori.</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations for the authors:</bold></p>
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>I will list additional recommendations below in no specific order:</p>
<p>(1) I find the term &quot;modality agnostic&quot; quite unusual, and I believe I haven't seen it used outside of the ML community. I would urge the authors to change the terminology to be more common, or at least very early explain why the term is much better suited than the range of existing terms. A modality agnostic representation implies that it is not committed to a specific modality, but it seems that a representation cannot be committed to something.</p>
</disp-quote>
<p>In the updated manuscript we now refer to the identified brain patterns as modality-invariant, which has previously been used in the literature (Man et al. 2012; Devereux et al. 2013; Patterson et al. 2016; Deniz et al. 2019, Nakai et al. 2021) (see also the general response on top and the Introduction and Related Work sections of the updated manuscript).</p>
<p>We continue to refer to the decoders as modality-agnostic, as this is a new type of decoder, and describes the fact that they are trained in a way that abstracts away from the modality of the stimuli. We chose this term as we are not aware of any work in which brain decoders were trained jointly on multiple stimulus modalities and in order not to risk contradictions/confusions with other definitions.</p>
<p>Deniz, F., Nunez-Elizalde, A. O., Huth, A. G., &amp; Gallant, J. L. (2019). The Representation of Semantic Information Across Human Cerebral Cortex During Listening Versus Reading Is Invariant to Stimulus Modality. Journal of Neuroscience, 39(39), 7722–7736. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0675-19.2019">https://doi.org/10.1523/JNEUROSCI.0675-19.2019</ext-link></p>
<p>Devereux, B. J., Clarke, A., Marouchos, A., &amp; Tyler, L. K. (2013). Representational Similarity Analysis Reveals Commonalities and Differences in the Semantic Processing of Words and Objects. The Journal of Neuroscience, 33(48).</p>
<p>Nakai, T., Yamaguchi, H. Q., &amp; Nishimoto, S. (2021). Convergence of Modality Invariance and Attention Selectivity in the Cortical Semantic Circuit. Cerebral Cortex, 31(10), 4825–4839. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhab125">https://doi.org/10.1093/cercor/bhab125</ext-link></p>
<p>Man, K., Kaplan, J. T., Damasio, A., &amp; Meyer, K. (2012). Sight and Sound Converge to Form Modality-Invariant Representations in Temporoparietal Cortex. Journal of Neuroscience, 32(47), 16629–16636.</p>
<p>Patterson, K., &amp; Lambon Ralph, M. A. (2016). The Hub-and-Spoke Hypothesis of Semantic Memory. In Neurobiology of Language (pp. 765–775). Elsevier. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/B978-0-12-407794-2.00061-4">https://doi.org/10.1016/B978-0-12-407794-2.00061-4</ext-link></p>
<disp-quote content-type="editor-comment">
<p>(2) The table in Figure 1B would benefit from also highlighting the number of stimuli that have overlapping captions and images.</p>
</disp-quote>
<p>The number of overlapping stimuli is rather small (153-211 stimuli depending on the subject). We added this information to Table 1B.</p>
<disp-quote content-type="editor-comment">
<p>(3) The authors wrote that training stimuli were presented only once, yet they used a one-back task. Did the authors also exclude the first presentation of these stimuli?</p>
</disp-quote>
<p>Thanks for pointing this out. It is indeed true that some training stimuli were presented more than once, but only for the case of one-back target trials. In these cases the second presentation of the stimulus was excluded, but not the first. As the subject can not be aware of the fact that the upcoming presentation is going to be a one-back target, the first presentation can not be affected by the presence of the subsequent repeated presentation. We updated the manuscript to clarify this issue.</p>
<disp-quote content-type="editor-comment">
<p>(4) Coco has roughly 80-90 categories, so many image captions will be extremely similar (e.g., &quot;a giraffe walking&quot;, &quot;a surfer on a wave&quot;, etc.). How can people keep these apart?</p>
</disp-quote>
<p>It is true that some captions and images are highly similar even though they are not matching in the dataset. This might result in several false button presses because the subjects identified an image-caption pair as matching when in fact it wasn't intended to. However, as there was no feedback given on the task performance, this issue should not have had a major influence on the brain activity of the participants.</p>
<disp-quote content-type="editor-comment">
<p>(5) Footnotes for statistics are quite unusual - could the authors integrate statistics into the text?</p>
</disp-quote>
<p>Thanks for this remark, in the updated manuscript all statistics are part of the main text.</p>
<disp-quote content-type="editor-comment">
<p>(6) It may be difficult to achieve the assumptions of a permutation test - exchangeability, which may bias statistical results. It is not uncommon for densely sampled datasets to use bootstrap sampling on the predictions of the test data to identify if a given percentile of that distribution crosses 0. The lowest p-value is given by the number of bootstrap samples (e.g., if all 10,000 bootstrap samples are above chance, then p &lt; 0.0001). This may turn out to be more effective.</p>
</disp-quote>
<p>Thanks for this comment. Our statistical procedure was in fact involving a bootstrapping procedure to generate a null distribution on the group-level. We updated the manuscript to describe this method in more detail. Here is the updated paragraph: “To estimate the statistical significance of the resulting clusters we performed a permutation test, combined with a bootstrapping procedure to estimate a group-level null distribution see also Stelzer et al., 2013). For each subject, we evaluated the decoders 100 times with shuffled labels to create per-subject chance-level results. Then, we randomly selected one of the 100 chance-level results for each of the 6 subjects and calculated group-level statistics (TFCE values) the exact same way as described in the preceding paragraph. We repeated this procedure 10,000 times resulting in 10,000 permuted group-level results. We ensured that every permutation was unique, i.e. no two permutations were based on the same combination of selected chance-level results. Based on this null distribution, we calculated p-values for each vertex by calculating the proportion of sampled permutations where the TFCE value was greater than the observed TFCE value. To control for multiple comparisons across space, we always considered the maximum TFCE score across vertices for each group-level permutation (Smith and Nichols, 2009).”</p>
<disp-quote content-type="editor-comment">
<p>(7) The authors present no statistical evidence for some of their claims (e.g., lines 335-337). It would be good if they could complement this in their description. Further, the visualization in Figure 4 is rather opaque. It would help if the authors could add a separate bar for the average modality-specific and modality-agnostic decoders or present results in a scatter plot, showing modality-specific on the x-axis and modality-agnostic on the y-axis and color-code the modality (i.e., making it two scatter colors, one for images, one for captions). All points will end up above the diagonal.</p>
</disp-quote>
<p>We updated the manuscript and added statistical evidence for the claims made:</p>
<p>We now report results for the claim that when considering the average decoding performance for images and captions, modality-agnostic decoders perform better than modality-specific decoders, irrespective of the features that the decoders were trained on.</p>
<p>Additionally, we report the average modality-agnostic and modality-specific decoding accuracies corresponding to Figure 4. For modality-agnostic decoders the average value is 81.86\%, for modality-specific decoders trained on images 78.15\%, and for modality-specific decoders trained on captions 72.52\%. We did not add a separate bar to Figure 4 as this would add additional information to a Figure which is already very dense in its information content (cf. Reviewers 2’s recommendations for the authors). We therefore believe it is more useful to report the average values in the text and provide results for a statistical test comparing the decoder types. A scatter plot would make it difficult to include detailed information on the features, which we believe is crucial.</p>
<p>We further provide statistical evidence for the observation regarding the directionality of cross-modal decoding.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>For achieving more evidence to support modality-agnostic representations in the brain, I suggest more thorough analyses, for example:</p>
<p>(1) Traditional searchlight RSA using different deep learning models. Through this approach, it might identify different brain areas that are sensitive to different formats of information (visual, text, multimodal); subsequently, compare the decoding performance using these ROIs.</p>
<p>(2) Build more dissociable decoders for information of different modality formats, if possible. While I do not have a concrete proposal, more targeted decoder designs might better dissociate representational formats (i.e., unimodal vs. modality-agnostic).</p>
<p>(3) A more detailed exploration of the &quot;qualitative decoding results&quot;--for example, quantitatively examining error types produced by modality-agnostic versus modality-specific decoders--would be informative for clarifying what specific content the decoder captures, potentially providing stronger evidence for modality-agnostic representations.</p>
</disp-quote>
<p>Thanks for these suggestions. As the main goal of the paper is to introduce modality-agnostic decoders (which should be more clear from the updated manuscript, see also the general response to reviews), we did not include alternative methods for identifying modality-invariant regions. Nonetheless, we agree that in order to obtain more in-depth insight into the nature of representations that were recorded, performing analyses with additional methods such as RSA, comparisons with more targeted decoder designs in terms of their target features will be indispensable, as well as more in-depth error type analyses. We leave these analyses as promising directions for future work.</p>
<disp-quote content-type="editor-comment">
<p>The writing could be further improved in the introduction and, accordingly, the discussion. The authors listed a series of theories about conceptual representations; however, they did not systematically explain the relationships and controversies between them, and it seems that they did not aim to address the issues raised by these theories anyway. Thus, the extraction of core ideas is suggested. The difference between &quot;modality-agnostic&quot; and terms like &quot;modality-independent,&quot; &quot;modality-invariant,&quot; &quot;abstract,&quot; &quot;amodal,&quot; or &quot;supramodal,&quot; and the necessity for a novel term should be articulated.</p>
</disp-quote>
<p>The updated manuscript includes an improved introduction and discussion section that highlight the main focus and contributions of the study.</p>
<p>We believe that a systematic comparison of theories on conceptual representations involving their relationships and controversies would require a dedicated review paper. Here, we focused on the aspects that are relevant for the study at hand (modality-invariant representations), for which we find that none of the considered theories can be rejected based on our results.</p>
<p>Regarding the terminology (modality-agnostic vs. modality-invariant, ..) please refer to the general response.</p>
<disp-quote content-type="editor-comment">
<p>The figures also have room to improve. For example, Figures 4 and 5 present dense bar plots comparing multiple decoding settings (e.g., modality-specific vs. modality-agnostic decoders, feature space, within-modal vs. cross-modal, etc.); while comprehensive, they would benefit from clearer labels or separated subplots to aid interpretation. All figures are recommended to be optimized for greater clarity and directness in future revisions.</p>
</disp-quote>
<p>Thanks for this remark. We agree that the figures are quite dense in information. However, splitting them up into subplots (e.g. separate subplots for different decoder types) would make it much less straightforward to compare the accuracy scores between conditions. As the main goal of these figures is to compare features and decoder types, we believe that it is useful to keep all information in the same plot.</p>
<p>You are also suggesting to improve the clarity of the labels. It is true that the top left legend of Figures 4 and 5 was mixing information about decoder type and broad classes of features  (vision/language/multimodal). To improve clarity, we updated the figures and clearly separated information on decoder type (the hue of different bars) and features (x-axis labels).  The broad classes of features (vision/language/multimodal) are distinguished by alternating light gray background colors and additional labels at the very bottom of the plots.</p>
<p>The new plots allow for easy performance comparison of the different decoder types and additionally provide information on confidence intervals for the performance of modality-specific decoders, which was not available in the previous figures.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations for the authors):</bold></p>
<p>(1) As discussed in the Public Review, I think the paper would greatly benefit from clearer terminology. Instead of describing the decoders as &quot;modality-agnostic&quot; and &quot;modality-specific&quot;, perhaps the authors could describe the decoding conditions based on the train and test modalities (e.g., &quot;image-to-image&quot;, &quot;caption-to-image&quot;, &quot;multimodal-to-image&quot;) or using the terminology from Figure 3 (e.g., &quot;within-modality&quot;, &quot;cross-modality&quot;, &quot;modality-agnostic&quot;).</p>
</disp-quote>
<p>We updated our terminology to be clearer and more accurate, as outlined in the general response. The terms modality-agnostic and modality-specific refer to the training conditions, and the test conditions are described in Figure 3 and are used throughout the paper.</p>
<disp-quote content-type="editor-comment">
<p>(2) Line 244: I think the multimodal one-back task is an important aspect of the dataset that is worth highlighting. It seems to be a relatively novel paradigm, and it might help ensure that the participants are activating modality-agnostic representations.</p>
</disp-quote>
<p>It is true that the multimodal one-back task could play an important role for the activation of modality-invariant representations. Future work could investigate to what degree the presence of widespread modality-invariant representations is dependent on such a paradigm.</p>
<disp-quote content-type="editor-comment">
<p>(3) Line 253: Could the authors elaborate on why they chose a random set of training stimuli for each participant? Is it to make the searchlight analyses more robust?</p>
</disp-quote>
<p>A random set of training stimuli was chosen in order to maximize the diversity of the training sets, i.e. to avoid bias based on a specific subsample of the CoCo dataset. Between-subject comparisons can still be made based on the test set which was shared for all subjects, with the limitation that performance differences due to individual differences or to the different training sets can not be disentangled. However, the main goal of the data collection was not to make between-subject comparisons based on common training sets, but rather to make group-level analyses based on a large and maximally diverse dataset.</p>
<disp-quote content-type="editor-comment">
<p>(4) Figure 4: Could the authors comment more on the patterns of decoding performance in Figure 5? For instance, it is interesting that ResNet is a better target than ViT, and BERT-base is a better target than BERT-large.</p>
</disp-quote>
<p>A multitude of factors influence the decoding performance, such as features dimensionality, model architecture, training data, and training objective(s) (Conwell et al. 2023; Raugel et al. 2025). Bert-base might be better than bert-large because the extracted features are of lower dimension. Resnet might be better than ViT because of its architecture (CNN vs. Transformer). To dive deeper into these differences further controlled analysis would be necessary, but this is not the focus of this paper. The main objective of the feature comparison was to provide a broad overview over visual/linguistic/multimodal feature spaces and to identify the most suitable features for modality-agnostic decoding.</p>
<p>Conwell, C., Prince, J. S., Kay, K. N., Alvarez, G. A., &amp; Konkle, T. (2023). What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines? (p. 2022.03.28.485868). bioRxiv. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2022.03.28.485868">https://doi.org/10.1101/2022.03.28.485868</ext-link></p>
<p>Raugel, J., Szafraniec, M., Vo, H.V., Couprie, C., Labatut, P., Bojanowski, P., Wyart, V. and King, J.R. (2025). Disentangling the Factors of Convergence between Brains and Computer Vision Models. arXiv preprint arXiv:2508.18226.</p>
<disp-quote content-type="editor-comment">
<p>(5) Figure 7: It is interesting that the modality-agnostic decoder predictions mostly appear traffic-related. Is there a possibility that the model always produces traffic-related predictions, making it trivially correct for the presented stimuli that are actually traffic-related? It could be helpful to include some examples where the decoder produces other types of predictions to dispel this concern.</p>
</disp-quote>
<p>The presented qualitative examples were randomly selected. To make sure that the decoder is not always predicting traffic-related content, we included 5 additional randomly selected examples in Figures 6 and 7 of the updated manuscript. In only one of the 5 new examples the decoder was predicting traffic-related content, and in this case the stimulus had actually been traffic-related (a bus).</p>
</body>
</sub-article>
</article>