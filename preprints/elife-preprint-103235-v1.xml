<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">103235</article-id>
<article-id pub-id-type="doi">10.7554/eLife.103235</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103235.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Selective attention and sensitivity to auditory disturbances in a virtually-real Classroom: Comparison of adults with and without AD(H)D</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Levy</surname>
<given-names>Orel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Libman Hackmon</surname>
<given-names>Shirley</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zvilichovsky</surname>
<given-names>Yair</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Korisky</surname>
<given-names>Adi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bidet-Caulet</surname>
<given-names>Aurelie</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schweitzer</surname>
<given-names>Julie B</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8831-3188</contrib-id>
<name>
<surname>Zion Golumbic</surname>
<given-names>Elana</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>elana.zion-golumbic@biu.ac.il</email>
</contrib>
<aff id="a1"><label>1</label><institution>The Gonda Brain Research Center, Bar Ilan University</institution>, Ramat Gan, <country>Israel</country></aff>
<aff id="a2"><label>2</label><institution>Aix Marseille Univ, Inserm, INS, Inst Neurosci Syst</institution>, Marseille, <country>France</country></aff>
<aff id="a3"><label>3</label><institution>Department of Psychiatry and Behavioral Sciences, University of California</institution>, Davis, Sacramento, CA, <country>U.S.A</country></aff>
<aff id="a4"><label>4</label><institution>MIND Institute, University of California</institution>, Davis, Sacramento, CA, <country>U.S.A</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<pub-date date-type="original-publication" iso-8601-date="2024-11-04">
<day>04</day>
<month>11</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP103235</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-09-16">
<day>16</day>
<month>09</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-09-08">
<day>08</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.04.17.590012"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Levy et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Levy et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-103235-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Many people, and particularly individuals with Attention Deficit (Hyperactivity) Disorder (AD(H)D), find it difficult to maintain attention during classroom learning. However, traditional paradigms used to evaluate attention do not capture the complexity and dynamic nature of real-life classrooms. Using a novel Virtual Reality platform, coupled with measurement of neural activity, eye-gaze and skin conductance, here we studied the neurophysiological manifestations of attention and distractibility, under realistic learning conditions. Individuals with AD(H)D exhibited higher neural responses to irrelevant sounds and reduced speech tracking of the teacher, relative to controls. Additional neurophysiological measures, such the power of alpha-oscillations and frequency of gaze-shifts away from the teacher, contributed to explaining variance in self-reported AD(H)D symptoms across the sample. These ecologically-valid findings provide critical insight into the neurophysiological mechanisms underlying individual differences in the capacity for sustained attention and the proneness to distraction and mind-wandering, experienced in real-life situations.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Results section has been re-structured, separating the different neurophysiological measures into three main classes: (1) Measures pertaining to processing/paying attention to the teacher (behavioral results, neural speech tracking, and eye-gaze to the teacher), (2) Measures reflecting response to the irrelevant event-sounds (ERPs, and transient changes in SC and gaze-shifts), and (3) Measures associated with 'general' aspects of attention/arousal (alpha/beta power and overall SC levels).</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Many people find it difficult to maintain attention to a frontal classroom lecture. Doing so requires listening, processing and comprehending the teacher’s speech over a prolonged period of time, while avoiding distraction from both irrelevant background sounds and internal mind-wandering(<xref ref-type="bibr" rid="c100">Smallwood et al., 2007</xref>; <xref ref-type="bibr" rid="c103">Szpunar et al., 2013</xref>; <xref ref-type="bibr" rid="c105">Thomson et al., 2015</xref>; <xref ref-type="bibr" rid="c30">Esterman and Rothlein, 2019</xref>). This task is supposedly even more difficult for individuals diagnosed with Attention-Deficit Disorder or Attention-Deficit and Hyperactivity Disorder (referred to jointly as: AD(H)D), which is often characterized by increase proneness to distraction and mind-wandering and poorer sustained attention (<xref ref-type="bibr" rid="c4">Avisar and Shalev, 2011</xref>; <xref ref-type="bibr" rid="c11">Berger and Cassuto, 2014</xref>; <xref ref-type="bibr" rid="c68">Merrill et al., 2022</xref>). Unsurprisingly perhaps, individuals are most likely to seek clinical help as a result of attentional difficulties experienced in school contexts (<xref ref-type="bibr" rid="c58">Litner, 2003</xref>), and there is a tight link between attention performance and learning outcomes (<xref ref-type="bibr" rid="c59">Loe and Feldman, 2007</xref>; <xref ref-type="bibr" rid="c36">Gray et al., 2017</xref>).</p>
<p>However, current clinical and scientific tools used for evaluating and quantifying the constructs of “distractibility” and “inattention,” are greatly removed from the real-life experience in organic classrooms and other ecological contexts. Commonly used approaches either rely on subjective self-report (e.g., rating scales), or are based on highly artificial computerized tests, such as Continuous Performance Tests (CPT). Unfortunately, in recent years there is a growing understanding that these tools lack sufficient sensitivity and specificity to reliably capture the type and severity of real-life attention challenges that people face outside the lab or the clinic (<xref ref-type="bibr" rid="c87">Rabiner et al., 2010</xref>; <xref ref-type="bibr" rid="c76">Narad et al., 2015</xref>; <xref ref-type="bibr" rid="c39">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="c74">Murray et al., 2018</xref>; <xref ref-type="bibr" rid="c73">Mulraney et al., 2022</xref>; <xref ref-type="bibr" rid="c45">Hobbiss and Lavie, 2024</xref>).</p>
<p>In attempt to create laboratory-based tasks that are more ecologically valid, and better capture the type of attentional challenges faced in real-life, recent years have seen increased use of Virtual Reality (VR) in cognitive research (<xref ref-type="bibr" rid="c81">Parsons, 2015</xref>; <xref ref-type="bibr" rid="c97">Seesjärvi et al., 2022</xref>). In particular, VR classrooms have been used to test different aspects of cognitive performance and distraction by ecologically valid stimuli. For example, it has been suggested that CPT tasks administered in VR-Classroom environments show greater sensitivity for AD(H)D classification relative to standard evaluations and can provide more refined measures of attention and distractibility (<xref ref-type="bibr" rid="c91">Rizzo et al., 2007</xref>; <xref ref-type="bibr" rid="c77">Neguț et al., 2017</xref>; <xref ref-type="bibr" rid="c21">Coleman et al., 2019</xref>; <xref ref-type="bibr" rid="c82">Parsons et al., 2019</xref>; <xref ref-type="bibr" rid="c102">Stokes et al., 2022</xref>). However, to date, most VR-classroom studies still use relatively non-ecological tasks, and rely primarily on indirect behavioral outcomes to assess aspects of attentional performance. To improve the utility of VR classrooms for understanding attention and distraction in real-life classroom learning, these platforms need to use genuine academic tasks and stimuli, to more faithfully simulate the classroom experience. Moreover, there is need to incorporate more objective metrics related to attention, such as measurements of neural activity and physiological responses, in order to obtain a more comprehensive picture of how listeners actually respond to stimuli and process the content of a lesson in a VR classroom. Here we address this challenge by introducing a highly realistic multimodal VR classroom platform, that simulates participating in a frontal lesson by an avatar-teacher with occasional disturbances from ecological sound-events (<xref ref-type="bibr" rid="c56">Levy et al., 2024</xref>). Our novel platform also integrates measurements of neurophysiological responses, including neural activity (using electroencephalography; EEG), eye-gaze and arousal levels, as reflected in changes in skin-conductance. These multi-level recordings allow us to investigate which neurophysiological metrics distinguish between students with and without a diagnosis of AD(H)D when they engage in realistic classroom learning, and which metrics reliably predict the severity of AD(H)D-symptoms.</p>
<p>We focus on several specific neural and physiological metrics, that have been associated with aspects of attention, distraction and information processing in more traditional attention research, and test their generalizability to this ecological context. These include: (1) Neural speech-tracking of the teacher’s lesson, which captures aspects of its sensory and linguistic processing, and is known to be reduced under conditions of inattention or distraction (<xref ref-type="bibr" rid="c23">Ding and Simon, 2012a</xref>; <xref ref-type="bibr" rid="c115">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="c48">Huang and Elhilali, 2020</xref>; <xref ref-type="bibr" rid="c46">Holtze et al., 2021</xref>; <xref ref-type="bibr" rid="c53">Kaufman and Zion-Golumbic, 2023</xref>). Accordingly, this metric might be expected to be reduced in individuals with AD(H)D and/or in conditions that contain external disturbances; (2) Neural Event-Related Potentials (ERPs), transient changes in SC and overt gaze-shifts following unexpected sound-events in the background of the classroom. These metrics are thought to reflect exogenous capture of attention, increases in arousal and potentially distraction by salient irrelevant stimuli (<xref ref-type="bibr" rid="c86">Posner, 1980</xref>; <xref ref-type="bibr" rid="c13">Bidet-Caulet et al., 2015</xref>; <xref ref-type="bibr" rid="c66">Masson and Bidet-Caulet, 2019</xref>); (3) The frequency of gaze-shifts away from the teacher, and time spent looking around the classroom, metrics associated with attention-shifts and distractibility(<xref ref-type="bibr" rid="c37">Grosbras et al., 2005</xref>; <xref ref-type="bibr" rid="c93">Schomaker et al., 2017</xref>), and are often heightened in individuals with AD(H)D (<xref ref-type="bibr" rid="c67">Mauriello et al., 2022</xref>; <xref ref-type="bibr" rid="c102">Stokes et al., 2022</xref>; <xref ref-type="bibr" rid="c98">Selaskowski et al., 2023</xref>); (4) The power of alpha and beta oscillations, which are often associated with increased mind-wandering or boredom (<xref ref-type="bibr" rid="c20">Clarke et al., 2001</xref>; <xref ref-type="bibr" rid="c14">Boudewyn and Carter, 2018</xref>; <xref ref-type="bibr" rid="c50">Jin et al., 2019</xref>), and some have suggested may be altered in individuals with AD(H)D (although use of spectral signatures as biomarkers for AD(H)D is highly controversial(<xref ref-type="bibr" rid="c35">Gloss et al., 2016</xref>)); (5) Continuous levels of arousal, as measured by skin-conductance (SC), which some propose are either heightened or reduced in individuals with AD(H)D relative to their control peers (<xref ref-type="bibr" rid="c99">Sergeant, 2000</xref>; <xref ref-type="bibr" rid="c9">Bellato et al., 2020</xref>).</p>
<p>This novel ecological approach and the rich repertoire of neurophysiological metrics measured here, afford unique insights into the mechanistic underpinnings of paying attention in class and the disruptive effects of background sounds, in both the neurotypical and AD(H)D population.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Fifty-four participants initially participated in the study; however, the data from five participants were excluded due to technical issues or their voluntary request to discontinue participation. The final sample consisted of forty-nine 49 participants (21 male, 28 female). Of these participants, 24 had a prior clinical diagnosis of attention deficit hyperactivity disorder (AD(H)D group), and 25 participants did not (control group). The two groups were matched for age, which ranged between 20 and 38 years (mean age of all participants: 26.9 ± 3.0; AD(H)D participants: 26.7 ± 3.7; control participants: 25.7 ± 4.4). The majority of participants were university students. Participants in the AD(H)D group were questioned about their medication intake regimen, and the experiment was scheduled at least 12 hours after their last dose of medication. None of the participants (in either group) reported taking any other medications on a regular basis. The experimental protocol was approved by the ethics committee at Bar-Ilan University, and all participants provided written informed consent prior to their involvement in the study and data collection procedures. Participants received financial compensation or course credit in exchange for their participation.</p>
</sec>
<sec id="s2b">
<title>ASRS questionnaire</title>
<p>All participants completed a self-report questionnaire assessing AD(H)D symptoms (ASRS-v1.1; Adler et al., 2006, Hebrew version: Zohar and Konfortes, 2010), which has been shown to have good psychometric properties high sensitivity for AD(H)D diagnosis (Kessler et al., 2005). Participants in the AD(H)D group were instructed to complete the questionnaire based on their experiences at a time when they are <underline>not</underline> taking medication. The questionnaire consists of 18 items, which are rated on a 5-point Likert scale ranging from 0 (never) to 4 (very often). ASRS scores for each participant were calculated as the total sum of marks across all items, as this has been shown to yield the highest sensitivity to AD(H)D diagnosis (Zohar and Konfortes, 2010). We did not try to categorize AD(H)D-subtypes (e.g., inattention, hyperactivity, combined) since these subtypes are less reliable in adulthood. As expected, the group diagnosed with AD(H)D exhibited significantly higher ASRS scores compared to the control group [t(47) = 8.49, p &lt; 0.001; see <xref rid="fig1" ref-type="fig">Figure 1</xref>]. This outcome validates the group allocation and demonstrates the consistency between AD(H)D diagnosis and self-reported AD(H)D symptoms.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Histogram (A) and bar graph (B) showing the distribution of ASRS scores in the AD(H)D and Control groups. Error bars represent standard error of the mean (SEM). This confirms group-allocation, within the tested sample.</p></caption>
<graphic xlink:href="590012v2_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2c">
<title>The VR Classroom: Design and Stimuli</title>
<p>The VR classroom platform was developed using the Unity game engine (unity.com; JavaScript and C# programming). Participants experienced the virtual environment through an HTC Vive ProEye VR headset that is equipped with an embedded eye-tracker (120 Hz binocular sampling rate). Audio was delivered through in-ear headphones (Etymotic ER1-14 with disposable foam eartips).</p>
<p>In the VR classroom (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), participants experienced sitting at a desk in the second row of a classroom, facing a male avatar-teacher, who was standing in front of a blackboard and a wall adorned with maps. Ten additional avatar-students occupied the remaining desks in the classroom. Although these student avatars didn’t engage in verbal communication during the experiment, their bodies were animated to simulate natural sitting movements. The avatar-teacher delivered a series of 30 mini-lessons on scientific or historical topics (mean duration 42.7 sec ± 5.1). The audio for these mini-lessons was taken from existing informational podcast, recorded by a male speaker. Speech audio was presented using 3D spatial audio (implemented in Unity) to be perceive as emitting from the teacher’s lips. In addition, the teacher’s lip movements were animated to follow the temporal envelope of the speech, and his body motions were animated to mimic natural talking gestures and to capture appropriate emphasis and prosodic cues.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p><bold>A</bold>. Illustration of the Virtual Classroom used in the study. Inset: a participant wearing the VR headset over the EEG cap. <bold>B.</bold> Example of the sequence of sound-events presented in Event-trials, which were emitted from a spatial location to the right or left of the participant. Event-trials contained 5 events, of both types (Artificial and Human non-verbal sounds), randomized across trials and separated by 3-7 seconds. <bold>C.</bold> Example of a multiple-choice comprehension question presented after each trial (English translation).</p></caption>
<graphic xlink:href="590012v2_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Besides the teacher’s lesson, short sounds (‘sound-events’) were presented occasionally in most trials, from a spatial location to the right or left of the participant (randomized), as if originating from one of the students sitting beside them in class (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). A total of 110 different sounds were used, taken from existing sound databases (freesfx.co.uk, findsounds.com, <ext-link ext-link-type="uri" xlink:href="http://freesound.org">freesound.org</ext-link>, and the IADS-E sound stimuli database (Yang et al., 2018), from two broad categories: Non-verbal human sounds (mainly coughs and throat clearings), and Artificial sounds (phone ringtones and digitally-generated ‘boing’ sounds). All sound-events were normalized to the same loudness level using the software Audacity and were trimmed to a duration of 300 milliseconds. In the VR classroom, the loudness of the teacher’s speech was set to a comfortable level for each participant, as determined in a training trial. The precise intensity level cannot be measured reliably, since it is affected by the specific positioning of the foam earphone inserts in the participants’ ear, but was roughly between 60-70 dB SPL. The sound-events were played at a loudness level of 50% relative to the teacher’s voice (-3dB SPL).</p>
</sec>
<sec id="s2d">
<title>Experimental procedure</title>
<p>The experiment was conducted in a sound attenuated room. After comfortably fitting the VR headset on the participant, the eye-tracker was calibrated and validated using a standard 9-point calibration procedure. Participants were then familiarized with the VR classroom scene, and performed two training trials – one without events-sounds and one with – for training and calibration purposes.</p>
<p>The experiment consisted of 30 trials. In each trial, the avatar teacher delivered a different mini-lesson and participants were instructed to pay attention to its content. They were not given any specific instructions regarding the other sounds or stimuli in the classroom. After each mini-lesson, participants answered four multiple-choice questions about its content (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Most trials (22) contained occasional background event-sound (Events condition; with 5 of both types sound-events per trial, separated by 3-7 seconds; <xref rid="fig2" ref-type="fig">Figure 2B</xref>), whereas 8 trials did not, serving as a Quiet baseline condition. In total, 104 sound-events were presented throughout the experiment (52 of each type). The order of mini-lecture presentation and their allocation to the Events or Quiet condition was randomized across participants.</p>
</sec>
<sec id="s2e">
<title>EEG, GSR and Eye-Gaze recordings</title>
<p>Neural activity was recorded using 64 active ag-agCl EEG electrodes (BioSemi) and was sampled at 1024Hz. Electrooculographic (EOG) signals were also included in this recording, measured by 3 additional electrodes located above the right eye and on the external side of both eyes.</p>
<p>The Skin Conductance Response (SC; Akash et al., 2018) was recorded using two passive Ni hon Kohden electrodes placed on the fingertips of the index and middle fingers of participants’ non-dominant hand. These signals were also sampled by the BioSemi system, and thus synchronized to EEG data.</p>
<p>Eye-gaze data was recorded using the eye tracker embedded in the VR headset, which provides information about the x-y-z coordinate of the gaze in 3D space at each time-point, as well as automatic blink-detection. We defined several a-priori regions of interest (ROIs) in the 3D classroom space, which included: ‘Teacher’, ‘Left Board’, ‘Right Board’, ‘Ceiling’, ‘Floor’, ‘Middle Window’, ‘Left Student’, ‘Right Student’. Note that the sound-events were emitted from the ROI labeled ‘Right Student’. Using a combined calculation of the participants’ head position and direction of the gaze vector, gaze data could be automatically classified to describe which ROI the participant looked at, at each point in time.</p>
</sec>
<sec id="s2f">
<title>Behavioral data analysis</title>
<p>To evaluate participants’ comprehension and memory retention, we calculated their accuracy on four multiple-choice questions following each trial (average correct responses). Between-group differences were assessed using unpaired t-tests, separately for the Quiet and Events conditions. These were evaluated separately (here and in all other analyses), given the imbalanced number of trials in each condition. The Bayes Factor (BF<sub>10</sub>) was also calculated for each pairwise comparison, to test the level of confidence for embracing or rejecting the null hypothesis (H0; BF<sub>10</sub> thresholds used throughout: BF<sub>10</sub> &lt; 1 indicates weak support of H0 and BF<sub>10</sub> &lt; 0.25 indicates moderate/strong support of H0; BF<sub>10</sub> &gt; 1 indicates weak support for rejecting H0 and BF<sub>10</sub> &gt; 2.5 indicates moderate/strong support for rejecting H0).</p>
</sec>
<sec id="s2g">
<title>EEG data analysis</title>
<p>The EEG data was preprocessed using MATLAB with the FieldTrip toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/">https://www.fieldtriptoolbox.org/</ext-link>). The raw EEG data was referenced to linked right and left mastoids, then band-pass filtered between 0.5 and 40 Hz, detrended and demeaned. Visual inspection was performed to identify and remove gross artifacts (excluding eye movements) and Independent Component Analysis (ICA) was then used to further remove components associated with horizontal or vertical eye movements and heartbeats. Remaining noisy electrodes, containing extensive high-frequency activity or DC drifts, were interpolated using adjacent electrodes, either on the entire dataset or on a per-trial basis, as needed.</p>
<p>Three types of analyses were applied to the clean EEG data: 1) Speech tracking analysis of the neural response to teacher’s speech, 2) Event related potentials (ERPs) analysis capturing the time-locked neural response to the Sound-events and 3) Spectral analysis, looking specifically at ongoing power in two canonical frequency bands previously associated with attention: the Alpha (6-15 Hz), and Beta (13–30 Hz) ranges.</p>
<sec id="s2g1">
<title>Speech Tracking response (TRF)</title>
<p>The clean EEG data was segmented into trials from the onset the teacher’s speech until the end of the trial. For this analysis, EEG data was further bandpass filtered between 0.8 and 20 Hz and downsampled to 100 Hz for computational efficiency. To estimate the neural response to the speech we performed speech-tracking analysis which estimates a linear Temporal Response Function (TRF) that best describes the relationship between features of the stimulus (S) and the neural response (R) in a given trial. Speech tracking analysis was conducted using the mTRF MATLAB toolbox (Crosse et al., 2016), using both an encoding and decoding approach. Since the Quiet condition only contained 8 trials (∼ 5 minutes of data in total), which is insufficient for deriving reliable TRFs (<xref ref-type="bibr" rid="c22">Crosse et al., 2021</xref>; <xref ref-type="bibr" rid="c70">Mesik and Wojtczak, 2023</xref>), speech-tracking analysis was limited only to the Events condition. We used a multivariate approach, and included the temporal envelopes of both the teacher’s speech and the sound-events as separate regressors (S) when modeling the neural response, so as to represent the entire soundscape they heard. For both regressors, we derived a broadband temporal envelope by filtering the audio through a narrow-band cochlear filterbank, extracting the narrowband envelopes using a Hilbert transform, and averaging across bands. The regressors were then downsampled to 100 Hz to match the EEG data and normalized using z-score.</p>
<p>In the <bold>encoding</bold> approach, TRFs are estimated for each EEG channel, with lags between the S and the R ranging from -150ms (baseline) to 450ms. The mTRF toolbox uses a ridge-regression approach for L2 regularization of the model, to reduce its complexity and ensure better generalization to new data. Based on previous research in by our group (Kaufman &amp; Zion Golumbic 2023), we chose to use a constant regularization ridge parameter (λ= 100) for all participants and conditions. We used a leave-one-out cross-validation procedure to assess the model’s predictive power, whereby in each iteration, all but one trials are used to train the model, and it is then applied to the left-out trial. The predictive power of the model is estimated as the Pearson’s correlation between the predicted neural responses and the actual neural responses, separately for each electrode, averages across all iterations. The statistical reliability of the model’s predictive power at each electrode is evaluated using a permutation test where the same procedure is repeated 100 times on shuffled data where the S from each trial is randomly paired with the R from a different trial. From each permutation we extract the highest predictive power across all electrodes, yielding a null-distribution of the maximal predictive power that could be obtained by chance. The predictive power in the real data is compared to this distribution, and electrodes with values falling in the top 5%<sup>tile</sup> of the null-distribution are considered to have a significant speech tracking response. Note that by choosing the maximal value across electrodes in each permutation, this procedure also corrects for multiple comparisons.</p>
<p>A complementary way to examine the neural representation of continuous speech is the <bold>decoding</bold> approach. In the decoding model a multidimensional transfer function is estimated using the neural data (R) from all electrodes as input in attempt to reconstruct the stimulus feature (S). For the decoding analysis we used time lags between S and R ranging from −400 to 0 msec (negative lags imply that the S precedes the R; here including a baseline would be detrimental to decoding). The decoding model was optimized using the same method and ridge parameter as the encoding model, using a regularization parameter of λ= 100 and a leave-one-out procedure to test the predictive power of the model. For the decoding approach, the predictive power is the Pearson’s correlation between the reconstructed S using the model and the actual S. Similar to the encoding approach, we assess the reliability of the model using a permutation test in which the same procedure is repeated 100 times on shuffled data where the S from each trial is randomly paired with the R from a different trial. This yields a null-distribution of predictive power values that could be obtained by chance, and the real model is considered reliable if its predictive power falls within the top 5%<sup>tile</sup> of the null-distribution.</p>
<p>Speech tracking analysis – using both encoding and decoding approaches – was performed separately for each Group (AD(H)D and Control). To test for between-group differences, we compared the models’ predictive power, using unpaired t-tests. We additionally calculated the Bayes Factor (BF<sub>10</sub>) for each comparison, to test the level of confidence for embracing or rejecting the null hypothesis.</p>
</sec>
<sec id="s2g2">
<title>Event Related-Response</title>
<p>To quantify neural responses to the sound-events, we estimated the time-locked Event-Related Potentials (ERPs). For this analysis, the clean EEG data was segmented into epochs ranging from -100 to 500ms around each event. The data was further low-passed filtered at 12Hz, as is common in ERP analysis and baseline corrected to the pre-stimulus period (-100 to 0 ms). ERPs were derived separately for each group (AD(H)D vs. Control) and Event-type (Artificial vs. Non-verbal human sounds). Visual inspection of the ERPs showed that they were dominated by two early components: the N1-component (75-200ms) and later P2-component (210-260ms), which primarily reflect auditory sensory responses, but are sometimes modulated by attention (<xref ref-type="bibr" rid="c44">Hillyard et al., 1973</xref>; <xref ref-type="bibr" rid="c112">Woldorff and Hillyard, 1991</xref>).</p>
<p>Statistical analysis of the ERPs focused on the two simple effects, using a Monte-Carlo spatio-temporal cluster-based permutation test, across all electrodes and time points. To test for difference in the ERPs between Groups, we used unpaired t-tests on the ERPs, averaged across Event-types). For this analysis, we had to set the cluster alpha-level to a very low level of p=10<sup>-5</sup>, in order to obtain separate clusters for the N1 and P2 responses. To test for difference in the ERPs generated for the two Event-types, we use paired t-tests on the ERPs of all participant, irrespective of their group. For this analysis, the cluster alpha-level was set to p=0.05. Last, to verify the main effects and in order to test for potential interactions between Group and Event-type, we extracted the peak-amplitudes of the N1 and P2 component from each participant, and performed a Mixed ANOVA with repeated measures, with the factors Group (AD(H)D vs. Control; between) and Event-Type (Artificial vs. Human non-verbal; within).</p>
</sec>
<sec id="s2g3">
<title>Spectral Analysis</title>
<p>The third analysis performed on the EEG data was a spectral analysis. This analysis was performed on the clean EEG data, segmented into full trials (same segmentation used for the speech-tracking analysis). We calculated the EEG spectral power density (PSD) of individual trials using multitaper fast-fourier transform (FFT) with Hanning tapers (method ‘mtmfft’ in the fieldtrip toolbox). The PSDs were averaged across trials for each participant, separately for each electrode and condition (Quiet and Events). We used the FOOOF algorithm (<xref ref-type="bibr" rid="c26">Donoghue et al., 2020</xref>) to decompose the PSD into periodic (oscillatory) and aperiodic components. The periodic portion of the PSD showed clear peaks in the Alpha (8-12 Hz) and Beta (15–25 Hz) ranges, suggesting reliable oscillatory activity (<xref rid="fig7" ref-type="fig">Figure 7</xref>). Since the peaks of Alpha and Beta activity can vary substantially across individuals, for each participant, we identified the frequency with the largest amplitude within each range, focusing on a cluster of occipital-parietal electrodes (for Alpha) or frontal-central electrodes (for Beta), where these responses were largest. These were used for statistical analyses of between-group differences. assessed using unpaired t-tests, separately for the Quiet and Events conditions, in each frequency band, and the Bayes Factor (BF<sub>10</sub>) was calculated for each pairwise comparison to test the level of confidence for embracing or rejecting the null hypothesis.</p>
</sec>
</sec>
<sec id="s2h">
<title>Skin Conductance data analysis</title>
<p>The Skin Conductance (SC) signal was analyzed using the Ledalab MATLAB toolbox (<xref ref-type="bibr" rid="c10">Benedek and Kaernbach, 2010</xref>) and custom-written scripts. The raw data was downsampled to 16Hz and inspected for any noticeable artifacts, which were corrected using linear interpolation. A continuous decomposition analysis (CDA) was employed to separate the tonic and phasic SC responses for each participant. We tested for between-group differences in <italic>global SC levels</italic> (tonic and phasic responses separately, averaged across full trials) using unpaired t-tests, separately for the Quiet and Events conditions and the Bayes Factor (BF<sub>10</sub>) was calculated for each pairwise comparison.</p>
<p>In addition, we analyzed <italic>event-related changes in SC</italic> following Sound-events. This analysis focused on a time window between 0 to 5 seconds after the onset of Sound-events. Since changes in SC are relatively slow and peak between 2-3 seconds after a stimulus, the mean response between 0-1 seconds was used as a baseline and was subtracted from the signal. Statistical analysis focused the time-window surrounding the mean response between 2-3 seconds, extracted for each participant. A Mixed ANOVA with repeated measures was used to test for main effects of Group (AD(H)D vs. Control; between factor), and Event-Type (Artificial vs. non-verbal Human; within factor) and the interaction between them.</p>
</sec>
<sec id="s2i">
<title>Eye tracking data analysis</title>
<p>Eye-tracking data was used to study participants’ spontaneous gaze dynamics in the VR classroom throughout the entire trial and also to examine whether background Sound-events elicited overt gaze shifts. Gaze-data preprocessing involved removing measurements around blinks (from -100ms to 200ms after) and any other data points marked as &quot;missing data&quot; by the eye-tracker. Gaze-data were analyzed using in-house written Python scripts, and closely followed recommended procedures for analysis of gaze-data in VR (<xref ref-type="bibr" rid="c1">Anderson et al., 2023</xref>). To assess gaze-patterns and detect gaze-shift, we calculated the Euclidean distance between adjacent data points, using the x, y, z coordinates in 3D space provided by the eye-tracker. ‘Fixations’ were defined as clusters of adjacent time-points within a radius of less than 0.01 distance from each other, lasting at least 80 ms. Each fixation was associated with a specific ROI in the VR classroom, indicating the object in the virtual space that the participant looked at. ‘Gaze-shifts’ were defined as transitions between two stable fixations.</p>
<p>To analyze spontaneous gaze-dynamics throughout an entire trial, we quantified the percent of each trial that participants spent looking at the teacher (target) versus all other ROIs, as well as the number of gaze-shifts away from the teacher per trial. Between-group differences in these measures of spontaneous gaze patterns were assessed using unpaired t-tests, separately for the Quiet and Events conditions. The Bayes Factor (BF<sub>10</sub>) was calculated for each pairwise comparison to test the level of confidence for embracing or rejecting the null hypothesis.</p>
<p>To determine whether background Sound-events elicited overt gaze shifts, we segmented the gaze-data into 2-second long epochs around each Event-sound. As a control, we randomly chose a similar number of 2-second long epochs from the Quiet-condition. We then counted how many epochs included at least one gaze-shift away from the teacher, and used a paired t-test to determine whether the frequency of gaze-shifts was higher following Sound-events relative to the Control epochs (regardless of Group). We also used a Mixed ANOVA to test for potential differences in the likelihood of gaze-shifts following the different event types (Artificial vs. Human non-verbal), and for the two Groups (AD(H)D vs. Control). For this analysis, the proportion of gaze shifts was normalized for each participant by subtracting the number of gaze shifts following Sound-events relative to the control epochs.</p>
</sec>
<sec id="s2j">
<title>Multivariate Analyses</title>
<p>Besides testing for effects of Group in each metric individually, we also performed multivariate analyses to examine the relationship between the various metrics collected here and their ability to predict ADHD diagnosis (binary) and/or the severity of AD(H)D symptoms.</p>
<p>This analysis focused on 10 key measures, based on the results of the univariate analyses (averaged across conditions, when relevant): 5 measures related to the response to the teacher’s speech: Behavioral performance, neural speech tracking decoding accuracy, the N1- and P2 peaks of the speech tracking response (TRF-N1; TRF-P2), and the average number of gaze-shifts away from the teacher; 3 measures related to responses to the sound events: Amplitude of the N1- and P2-responses to sound-events (ERP-N1; ERP-P2), phasic SC response to sound events (event-related SC); and 2 global measures associated with the listeners’ ‘state’: alpha- and beta-power.</p>
<p>First, we calculated Spearman’s correlation between all pairs of measures (fdr correction for multiple comparisons was applied). Next, we entered all factors into a multiple regression model and performed a dominance analysis to determine the relative importance of each measure for predicting whether an individual was in the ADHD or control group (logistic regression) or the severity of ADHD symptoms, as reflected by their ASRS scores (linear regression). Dominance analysis was performed as follows: For each of the 10 measures, we computed the difference in a model-fit index (AUC for logistic regression; R<sup>2</sup> for linear regression) between a full model containing all 10 predictors, and a model where the measure under consideration was held-out, as a means for assessing the unique contribution of each measure to overall model performance (dominance). We then sorted the measures based on their dominance and report the multiple regression model that explains the most amount of variance using the least number of variables. These analyses were conducted using the statistical software JASP (<xref ref-type="bibr" rid="c49">JASP Team, 2024</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Paying attention to the teacher</title>
<p>First, we tested whether there are differences between individuals with and without AD(H)D in metrics associated with focusing ones’ attention on the teacher - accuracy on answering comprehension questions, neural speech tracking of the teacher’s speech and the gaze-patterns of focusing overt attention towards the teacher.</p>
<p>Due to the difference in number of trials statistical testing of between-group differences were performed separately for the Events and Quiet conditions (when applicable), and comparisons between conditions are evaluated only qualitatively.</p>
<sec id="s3a1">
<title>Behavioral Accuracy</title>
<p>Participants demonstrated overall good performance on the comprehension task, achieving an average accuracy of 87.87% (±6.77% SEM). This serves as verification that participants followed the instructions, listened to teacher’s speech and understood the content of the mini-lessons. Performance levels were similarly good in both the Events and Quiet conditions, indicating that the presence of occasional sound-events did not disrupt overall understanding of the mini-lessons. No significant differences in performance were found between groups, in either condition [Events: t(47) = 1.052, p = 0.297, Cohen’s d = 0.30, BF<sub>10</sub> = 0.448 (weak support of H0); Quiet: t(47) = 1.004, p = 0.320, Cohen’s d = 0.28, BF<sub>10</sub> = 0.430 (weak support for H0); <xref rid="fig3" ref-type="fig">Figure 3</xref>].</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Accuracy on comprehension questions.</title><p>Shown separately for the AD(H)D and Control groups, in the Quiet and Events conditions. Bar graphs represent average accuracy levels across participants and error bars represent the SEM. No significant differences were found between groups in either condition (n.s).</p></caption>
<graphic xlink:href="590012v2_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3a2">
<title>Speech tracking of teacher’s speech</title>
<p>We conducted speech-tracking analysis of the neural response to the teacher’s speech, using both an encoding and decoding approach. As noted in the methods section, speech-tracking analysis could only be performed reliably in the Events condition, and we used a multivariate approach to account for variance in the neural activity due to the presence of event-sounds (see supplementary materials for comparison with univariate analysis).</p>
<p>The TRFs estimated in response to the teachers’ speech were similar across both groups showing the typical negative peak at ∼ 100ms (N1), followed by a positive peak ∼200ms (P2), and the model’s predictive power following the traditional mid-central topography associated with auditory responses (<xref rid="fig4" ref-type="fig">Figure 4A,B</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Speech-tracking of the teacher in the presence of sound-events.</title><p><bold>A.</bold> Topographical distribution of the predictive power values, estimated using the multivariate encoding model, in the ADHD and Control groups. No significant differences were observed between groups. <bold>B.</bold> Grand-average TRFs estimated for each Group in the Events condition. Shaded areas represent the SEM. <bold>C.</bold> Bar graphs showing the average reconstruction accuracy of the decoding model (Pearson’s r), in each Group. Asterisk (*) indicate a significant difference between ADHD and Control groups (p &lt; 0.05). Error bars represent the SEM.</p></caption>
<graphic xlink:href="590012v2_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>No significant between-group differences were found in the predictive power of the encoding model (averaged across electrodes), and Bayes analysis indicates moderate support for the null-hypothesis [t(47) = 0.25, p = 0.804, Cohen’s d = 0.071, BF<sub>10</sub> = 0.29]. However, when using a decoding model, which considers all electrodes in one model, we did find slightly lower reconstruction accuracy of the teacher’s speech in the AD(H)D group relative to controls [t(47) = -2.023, p = 0.048, Cohen’s d = -0.578, BF<sub>10</sub> = 1.47 (weak/moderate support for H1); <xref rid="fig4" ref-type="fig">Figure 4C</xref>].</p>
</sec>
<sec id="s3a3">
<title>Looking at the teacher</title>
<p>Analysis of gaze-patterns showed that, as expected, participants spent most of the time looking at the teacher (60.2% ± 20.9% SEM of each trial). However, we also note the large variability across participants, with some focusing almost exclusively on the teacher (near 100%) and others spending less than 40% of the trial looking at the teacher. When not looking at the teacher, the next most popular places to look at were the blackboards behind the teacher, to the right and to the left (<xref rid="fig5" ref-type="fig">Figure 5A</xref>). We tested whether the amount of time spent looking at the teacher and the number of gaze-shifts away from the teacher were modulated by Group (AD(H)D vs. Control) separately in each Condition (Quiet and Events). No significant differences were found, however BF analysis indicated that the null-hypotheses was not strongly supported [<italic>Percent gaze-time at teacher:</italic> Events Condition: t(47) = -0.899, p = 0.372, Cohen’s d = -0.257, BF<sub>10</sub> = 0.397 (weak support for H0); Quiet Condition: t(47) = -0.597, p = 0.553, Cohen’s d = -0.170, BF<sub>10</sub> = 0.33 (weak support for H0); <italic>Number of gaze-shifts away from teacher</italic>: Events Condition: t(47) = 1.265, p = 0.211, Cohen’s d = 0.361, BF<sub>10</sub> = 0.547 (weak support for H0); Quiet Condition: t(47) = 0.644, p = 0.522, Cohen’s d = 0.184, BF<sub>10</sub> = 0.338 (weak support for H0); <xref rid="fig5" ref-type="fig">Figure 5C</xref>].</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Eye-gaze results.</title><p><bold>A.</bold> Pie chart representing the average amount of time that participants spent looking at different areas within the VR classroom. <bold>B.</bold> Distribution of the proportion of gaze-time towards the teacher (top) and number of gaze-shifts performed away from the teacher (bottom), for all participants in the AD(H)D and Control groups. <bold>C.</bold> Bar graph represent the average number of gaze-shifts performed away from the teacher, shown separately for the AD(H)D and Control groups, and for the Quiet and Events conditions. No significant differences were found in any comparison (n.s).</p></caption>
<graphic xlink:href="590012v2_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3b">
<title>Responses to Irrelevant Sound-Events</title>
<p>Next, we tested whether neurophysiological responses to the event-sounds themselves differed between groups. The three event-related metrics tested here were: neural ERPs, transient changes in skin-conductance (SC) and gaze-shifts following event-sounds. In addition, we compared the response to the two different event-types presented here (Artificial vs. non-verbal Human sounds).</p>
<sec id="s3b1">
<title>Neural Event Related Potentials (ERPs)</title>
<p>Visual inspection of the neural ERPs showed three prominent centro-parietal components - a negative peak around 100ms, which corresponds to the early sensory N1; followed by two positive peaks around 240ms and 350ms, which likely corresponds to the P2 and P3 responses, respectively (<xref rid="fig6" ref-type="fig">Figure 6</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Event Related Potentials in response to sound-events.</title>
<p><bold>A.</bold> Grand-average ERP to sound-events, shown separately for the AD(H)D vs. Control groups. ERPs are averaged across the cluster of electrodes where significant differences were found (see panel B). The black horizontal line indicates the time-window where significant differences between groups were found (75-155ms). Shaded areas around the waveforms represent the SEM. <bold>B.</bold> Scalp topographies of the N1 and P2 responses in the AD(H)D and Control Groups, and the difference between them. Electrodes where significant differences between groups were found are marked in white (p&lt;0.05, cluster corrected). <bold>C.</bold> Grand-average ERP to Artificial and non-verbal human event-sound. ERPs are averaged across the cluster of electrodes where significant differences were found (see panel D). The black horizontal lines indicate the time-windows where significant differences between ERP to the two types of sound-events were found (67-178 and 187-291ms). Shaded areas around waveforms represent the SEM. <bold>D.</bold> Scalp topographies of the N1 and P2 responses to Artificial and Non-verbal human sounds and the difference between them. Electrode where significant differences were found are marked in white (p&lt;0.05, cluster corrected). <bold>E&amp;F.</bold> Box plots depicting the average N1 and P2 responses, separately for each group (AD(H)D vs. Control) and Event-type (Artificial vs. Non-verbal human). ** p &lt; 0.001; * p&lt;0.05.</p></caption>
<graphic xlink:href="590012v2_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Statistical analysis comparing ERPs in the AD(H)D vs. Control groups (collapsed across both Event-Types) showed that the early N1 response was significantly larger in the AD(H)D group (p&lt;0.05, cluster correction; <xref rid="fig6" ref-type="fig">Figure 6A</xref>&amp;B). However, no significant differences between groups were found for the later P2 response. When comparing ERPs to Artificial vs. non-verbal Human sounds we found that both the N1 and the P2 were significantly modulated by Event-type, with a larger N1 for the Artificial sounds and a larger P2 for the non-verbal Human sounds (both p&lt;0.002, cluster correction; <xref rid="fig6" ref-type="fig">Figure 6C&amp;D</xref>). A mixed-ANOVA performed on the amplitudes of each component confirmed these main effects, but did not reveal any significant interaction between Group and Event-type [N1: <italic>Group</italic> [F(1, 47) = 6.15, p = 0.044, η<sup>2</sup>= 0.11], <italic>Event type</italic> [F(1, 47) = 63.82, p &lt; 10<sup>-9</sup>, η<sup>2</sup> = 0.57], <italic>interaction</italic> [F(1, 47) = 0.389, p = 0.24, η<sup>2</sup> = 0.008]; P2: <italic>Group</italic> [F(1, 47) = 0.84, p = 0.36, η<sup>2</sup> = 0.017], <italic>Event type</italic> [F(1, 47) = 71.75, p &lt; 10<sup>-9</sup>, η<sup>2</sup> =0.6], <italic>interaction</italic> [F(1, 47) = 0.07, p=0.78, η<sup>2</sup>= 0.001]; P3: <italic>Group</italic> [F(1, 47) = 0.028, p = 0.876, η<sup>2</sup> = 0.0005], <italic>Event type</italic> [F(1, 47) = 0.184, p = 0.669, η<sup>2</sup> = 0.003], <italic>interaction</italic> [F(1, 47) = 0.166, p = 0.684, η<sup>2</sup> = 0.003]; <xref rid="fig6" ref-type="fig">Figure 6</xref>].</p>
</sec>
<sec id="s3b2">
<title>Event-Related increase in SC</title>
<p>Sound-events elicited clear event-related changes in the phasic SC response, which peaked 2-3 seconds after the sounds, and took another ∼1 second to return to baseline (<xref rid="fig7" ref-type="fig">Figure 7</xref>). This response is consistent with the well documented orienting-reflex, elicited following salient events ($ref).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Event-Related changes in Skin Conductance (SC).</title><p><bold>A&amp;B.</bold> Time course of event-related changes in phasic SC following the sound-events, shown separately for each group (A) and for the two sound-events types (B). Shaded areas around waveforms represent the SEM. Horizontal line represents the time-windows where significant differences were found. <bold>C.</bold> Average levels of event-related changes in phasic SC (peak between 2-3sec) shown for each group and for Artificial vs. Non-verbal human sound-events. Error bars represent the SEM. **p&lt;0.01.</p></caption>
<graphic xlink:href="590012v2_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>A Mixed ANOVA showed a significant main effects of Event type, with a higher response following artificial sounds relative to non-verbal human sounds [F(1, 47) = 7.88, p = 0.007, η<sup>2</sup> = 0.143], however there was no significant main effect of Group [F(1, 47) = 0.4, p = 0.53, η<sup>2</sup> = 0.008] or interaction between them [F(1, 47) = 0.02, p = 0.87, η<sup>2</sup> = 0.0005; <xref rid="fig7" ref-type="fig">Figure 7</xref>].</p>
</sec>
<sec id="s3b3">
<title>Event-Related gaze-shifts</title>
<p>We tested whether the sound-events triggered overt gaze-shifts away from the teacher, by quantifying the percentage of event-sound that were followed by at least one gaze-shift, and comparing this to control epochs taken from the Quiet condition. However, no significant differences were found, suggesting that the sound-events did not elicit more frequent overt gaze-shifts away from the teacher, relative to what might be observed when no sound-events are present [t(48) = 1.34, p = 0.18; <xref rid="fig8" ref-type="fig">Figure 8A</xref>]. We also tested whether the likelihood of performing event-related gaze-shifts was different for the two Event types (Artificial vs. non-verbal human sounds) or in the two Groups (AD(H)D vs. Control) but did not find any significant main effect or interactions [<italic>Group</italic>: F(47) = 0.18, p = 0.67, η<sup>2</sup> = 0.003; <italic>Condition</italic>: F(47) = 2.25, p = 0.14, η<sup>2</sup> = 0.045; <italic>Interaction</italic>: F(47) = 0.51, p = 0.47, η<sup>2</sup> = 0.01; <xref rid="fig8" ref-type="fig">Figure 8B</xref>].</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Event-related gaze shifts.</title><p>AD(H)D <bold>A.</bold> Bar graph showing the number of gaze-shifts performed in 2-sec epochs following event-sounds vs. control epochs, averaged across groups and sound types. No significant differences were found, suggesting that event-sounds were not more likely to elicit overt gaze-shifts. <bold>B.</bold> Bar graph showing the number of gaze-shifts performed in 2-sec epochs following each type of sound-event, separately for each group. No significant differences were found in any comparison. Error bars represent the SEM in all bar graphs.</p></caption>
<graphic xlink:href="590012v2_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
</sec>
<sec id="s3c">
<title>Neurophysiological metrics associated with overall attention and arousal</title>
<p>Besides analyzing neurophysiological metrics that can be directly associated with processing the teacher’s speech or with response to the sound-events, several additional neurophysiological measures have been associated more broadly with the listeners’ state of attention or arousal. These including spectral properties of the EEG (and specifically power in the alpha and beta range), the overall level of tonic and phasic skin-conductance, and spontaneous gaze-patterns. We tested whether these metrics differed between the two groups, separately in the Quiet condition and Events conditions.</p>
<sec id="s3c1">
<title>Spectral EEG features</title>
<p>Spectral analysis of the EEG focused on the two frequency bands for which clear peaks were detected in the periodic power-spectrum, after applying the FOOOF algorithm: Alpha (8-12Hz) and Beta power (18-25Hz; <xref rid="fig9" ref-type="fig">Figure 9A</xref>). Power in each frequency band was assessed for each participant at their personal peak within a pre-selected cluster of electrodes where each response was maximal (<xref rid="fig9" ref-type="fig">Figure 9</xref>). We tested whether power in either frequency differed significantly between Groups, using unpaired t-tests, separately for each Condition (Quiet and Event trials). However, none of the comparisons revealed any significant differences between groups, in either frequency band (<xref rid="fig9" ref-type="fig">Figure 9C</xref>) [<italic>Alpha Power</italic>: Events Condition: t(47) = -0.656, p = 0.514, Cohen’s d = -0.18, BF<sub>10</sub> = 0.34 (weak evidence for H0); Quiet Condition: t(47) = -0.394, p = 0.695, Cohen’s d = -0.11, BF<sub>10</sub> = 0.30 (moderate evidence for H0); <italic>Beta-power:</italic> Events Condition: t(47) = -1.01, p = 0.315, Cohen’s d = -0.29, BF<sub>10</sub> = 0.43 (weak evidence for H0); Quiet Condition: t(47) = -0.484, p = 0.630, Cohen’s d = -0.14, BF<sub>10</sub> = 0.314 (moderate evidence for H0)].</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Spectral EEG and Skin Conductance Analysis.</title>
<p><bold>A.</bold> Grand-Average Power spectral density (PSD) of the Periodic portion of the EEG signal (after applying the FOOOF algorithm), shows two clear peaks corresponding to the alpha (8-12Hz) and beta (15-25Hz) bands. Shaded areas around waveforms represent the SEM. <bold>B.</bold> Topographical distribution of the average alpha-power and beta-power peaks, with the clusters of electrodes used to detect personal peaks in each frequency band marked by white circles. <bold>C.</bold> Average alpha- and beta-power in Group (AD(H)D vs. Control) and condition (Quiet and Events). <bold>D.</bold> Phasic and <bold>E.</bold> tonic skin conductance levels in the same groups and conditions. No significant between-group difference was found in any comparison (n.s.). Bar graphs represent the mean values, and error bars represent the SEM.</p></caption>
<graphic xlink:href="590012v2_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s3c2">
<title>Global Skin Conductance Levels</title>
<p>No significant differences between the two Groups were observed for the global tonic or phasic SC metrics, in either the Events or Quiet condition [<bold><italic>Phasic SC</italic></bold> (<xref rid="fig9" ref-type="fig">Figure 9 D&amp;E</xref>): Events Condition: t(47) = -0.003, p = 0.99, Cohen’s d = -0.0009, BF<sub>10</sub> = 0.285 (moderate evidence for H0); Quiet Condition: t(47) = -0.51, p = 0.61, Cohen’s d = -0.146, BF<sub>10</sub> = 0.317 (moderate evidence for H0); <bold><italic>Tonic SC</italic></bold>: Events Condition: t(47) = -0.85, p = 0.398, Cohen’s d = -0.244, BF<sub>10</sub> = 0.383 (weak evidence for H0); Quiet Condition: t(47) = -1.65, p = 0.104, Cohen’s d = -0.476, BF<sub>10</sub> = 0.865 (weak evidence for H0)].</p>
</sec>
</sec>
<sec id="s3d">
<title>Multivariate Analyses</title>
<p>The univariate analyses testing for between-group differences separately for each metric were complemented with a multivariate regression analysis, aimed at understanding the relative contribution of each neurophysiological metric to explaining between-group effects. For brevity, we focused on 10 key measures as described in the Methods section.</p>
<p>The pairwise Spearman’s correlations between all measures are shown in <xref rid="fig10" ref-type="fig">Figure 10</xref>. None of the correlations were significant after correcting for multiple comparisons (fdr-correction), however three pairs did reach significance before correction (Speech-Decoding vs. ERP-P2: r = -0.33, p=0.021; TRF-N1 vs. event-related SC: r = -0.3, p=0.033; ERP-N1 vs. event-related SC: r = 0.29, p=0.04). Given these generally weak correlations, we consider these measures to be independent for the purpose of multiple regression analysis.</p>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Correlation matrix.</title>
<p>Heatmap of the pairwise spearman’s correlation coefficient between the different neurophysiological measures included in the multivariate analysis. Red-shading indicates negative correlation values and blue-shading indicate positive values. Asterisks* indicate correlation values that pass a non-corrected threshold for statistical significance, however none survived fdr-correction for multiple comparisons.</p></caption>
<graphic xlink:href="590012v2_fig10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To test the contribution of each measure for predicting whether an individual was in the ADHD or control group we performed a logistic multiple regression analysis. An ‘omnibus’ model containing all 10 measures achieved χ<sup>2</sup>=15.6 with an AUC of 0.807 for predicting group-allocation. We assessed the relative contribution (‘dominance’) of each measure by comparing the AUC of the omnibus model to 10 models with a single measure held-out. Table X shows ΔAUC contributed by each measure, indicating that the two most dominant measures were the ERP-N1 (positive contribution) and Speech Decoding (negative contribution), which were also the only two measures whose addition to the model reduced the AIC. This result is consistent with our univariate results, and indicates that these two measures contribute independently to between-group differences. Together, these two measures contributed an AUC of 0.15 to the omnibus model, which corresponds to half of the model’s fit (Δχ<sup>2</sup>=11.8, p=0.003 relative to a model with both measures held-out).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Result of the dominance analysis of a multivariate logistic regression, describing the contribution of each measure for predicting whether an individual was in the ADHD or control group.</title></caption>
<graphic xlink:href="590012v2_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
<p>Since ADHD is arguably not a binary category but symptoms of attentional difficulties vary on a continuum, we complemented the logistic regression analysis with a multivariate linear regression analysis, using ASRS scores as a continuous dependent measure, reflecting the severity of ADHD symptoms. An ‘omnibus’ model containing all 10 achieved F(10,38)=1.48 (p=0.18) with an R<sup>2</sup> of 0.28 for predicting ASRS scores. We assessed the relative contribution (‘dominance’) of each measure by comparing the R<sup>2</sup> in the omnibus model (containing all 10 measures) to 10 models with a single measure held-out. Table X shows ΔR<sup>2</sup> and ΔAIC contributed by each measure (smaller values are better). Interestingly, this analysis identified slightly different factors than the logistic regression analysis where individuals were grouped based on having a prior diagnosis of AD(H)D (<xref rid="tbl2" ref-type="table">Table 2</xref>). The most dominant measures, who contributed at least ΔR<sup>2</sup> &gt;0.3 the model, were the ERP-N1 (β = 0.3), Gaze-shifts (β = 0.288), Alpha power (β = 0.2) and ERP-P2 (β = -0.182). Although the omnibus model was not significant, (see above), a multivariate regression model that contained only the 4 most dominant measures did significantly predicted ASRS scores (R<sup>2</sup>=0.227, p = 0.021), and collectively contributed ΔR<sup>2</sup>=0.225 (p=0.032) to the omnibus model.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><title>Result of the dominance analysis of a multivariate linear regression, describing the contribution of each measure for predicting individual ASRS scores.</title></caption>
<graphic xlink:href="590012v2_tbl2.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>Recent years have seen a reckoning in the field of AD(H)D research, acknowledging that most current tools used for clinical assessments and research do not adequately capture the attentional challenges faced in real life (<xref ref-type="bibr" rid="c39">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="c5">Barkley, 2019</xref>; <xref ref-type="bibr" rid="c73">Mulraney et al., 2022</xref>; <xref ref-type="bibr" rid="c3">Arrondo et al., 2023</xref>; <xref ref-type="bibr" rid="c96">Schweitzer and Zion Golumbic, 2023</xref>). We offer a new approach for advancing attention-research, by utilizing VR to simulate real-life experiences together with comprehensive measurement of different neurophysiological responses. This combination affords a more objective and well-rounded description of how individuals deal with the plethora of stimuli and task demands of their environments. Here we focused on VR classroom learning, since this is the context where many individuals, and particularly those with AD(H)D, experience difficulties in sustaining attention and avoiding distraction. Specifically, we evaluated neurophysiological measures related to focusing attention on the teacher and responses to irrelevant background sound-events alongside metrics associated more generally with levels of attention/engagement. We found that individuals diagnosed with AD(H)D exhibited significantly larger N1 response to background sound-events and reduced neural tracking of the teacher’s speech, which were also the two most dominant factors contributing to a multivariate classifier that could significantly predict whether an individual was diagnosed with AD(H)D. When considering the severity of AD(H)D symptoms as a continuous variable, variance across individuals was best explained by the combined contribution of the N1 and P2 response to sound-events, amplitude of alpha-power oscillations and the frequency of gaze-shifts away from the teacher. Together, these findings emphasize the variety of attentional challenges that individuals with AD(H)D may experience in realistic scenarios and provide novel insights into the underlying neurophysiological mechanisms.</p>
<sec id="s4a">
<title>Neurophysiological responses in the VR classroom associated with symptoms of AD(H)D</title>
<p>The most robust finding in this study was larger N1 response to background sound-events in individuals with AD(H)D. This response showed a significant univariate between-group effect, and was the most dominant contributor to predicting group-classification (AD(H)D vs. control) as well as the severity of AD(H)D symptoms. Enhanced N1 responses are often associated with heightened sensory sensitivity (<xref ref-type="bibr" rid="c42">Heil, 1997</xref>; <xref ref-type="bibr" rid="c61">Lütkenhöner and Klein, 2007</xref>), perceptual predictability (<xref ref-type="bibr" rid="c19">Chennu et al., 2013</xref>; <xref ref-type="bibr" rid="c55">Lange, 2013</xref>; <xref ref-type="bibr" rid="c94">Schwartze and Kotz, 2015</xref>), and may reflect a transient detector mechanism of surprising events that can potentially (though not necessarily) trigger an attention shift (<xref ref-type="bibr" rid="c75">Näätänen and Picton, 1987</xref>; <xref ref-type="bibr" rid="c28">Escera et al., 1998</xref>, <xref ref-type="bibr" rid="c29">2003</xref>; <xref ref-type="bibr" rid="c12">Berti, 2013</xref>). Interestingly, most studies comparing auditory ERPs in individuals with and without AD(H)D, using more traditional paradigms (e.g. oddball), typically <italic>have not reported</italic> differences in early-sensory components such as the N1, but rather sometimes find group differences for later ERP responses (<xref ref-type="bibr" rid="c7">Barry et al., 2003</xref>; <xref ref-type="bibr" rid="c38">Gumenyuk et al., 2005</xref>; <xref ref-type="bibr" rid="c52">Kaiser et al., 2020</xref>; <xref ref-type="bibr" rid="c83">Peisch et al., 2021</xref>). As discussed below, we attribute these discrepancies to the non-ecological nature of most traditional paradigm. The current result is in line with increased sensory sensitivity to background events, which might be especially pertinent under ecological condition. In addition, we found that neural tracking of the teacher’s speech was reduced in the AD(H)D group, and this metric also emerged as a dominant predictor of AD(H)D diagnosis in multivariate between-group classification. Neural speech-tracking of continuous speech is known to be modulated by attention, and is reduced when speech is not attended (<xref ref-type="bibr" rid="c24">Ding and Simon, 2012b</xref>; <xref ref-type="bibr" rid="c69">Mesgarani and Chang, 2012</xref>; <xref ref-type="bibr" rid="c115">Zion Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="c84">Petersen et al., 2017</xref>; <xref ref-type="bibr" rid="c110">Vanthornhout et al., 2019</xref>; <xref ref-type="bibr" rid="c53">Kaufman and Zion-Golumbic, 2023</xref>). We are not aware of previous studies that directly investigated neural speech tracking in AD(H)D, however these results are consistent with reduced levels of sustained attention towards the teacher in this group.</p>
<p>Reduced sustained attention and higher proneness to distraction are, of course, considered key characteristics of AD(H)D, however are rarely demonstrated under realistic conditions, using ecological tasks and materials. The current results demonstrate how, by coupling the ecological validity and flexibility of VR with recordings of neural activity, we can substantially advance our mechanistic understanding of attention and distractibility and obtain objective metrics of stimulus processing in realistic scenarios. Interestingly, since the magnitude of the N1 to sound-events and speech-tracking of the teacher were not correlated with each other, these results do not support the notion of an inherent ‘tradeoff’ between paying attention to the teacher and responding to irrelevant stimuli. Rather, they may represent two independent ways in which individuals with AD(H)D differ from their peers in the manner they respond to and process stimuli in dynamic environments.</p>
<p>Although here only two measures showed reliable between-group differences, adopting a binary perspective of AD(H)D can be misleading, as it does not capture the vast variability within groups, as emphasized by the RDoC framework (<xref ref-type="bibr" rid="c72">Morris et al., 2022</xref>). This is evident from inspecting the distribution of ASRS scores in the current sample, which shows that although the two groups are clearly separable from each other, they are far from uniform in the severity of symptoms experienced. For this reason, we conducted a continuous multiple regression analysis, to identify which neurophysiological measured explained variance in self-reported AD(H)D symptoms. This analysis identified several additional factors that contribute to explaining variance in ASRS scores across individuals, primarily: the level of alpha-power and the frequency of gaze-shifts away from the teacher. This finding is consistent with the hypothesized role of these metrics in attention. Higher alpha power is associated with reduced levels of attention/arousal, and is consistently found to increase in conditions of boredom, tiredness or prolonged time on tasks (<xref ref-type="bibr" rid="c20">Clarke et al., 2001</xref>; <xref ref-type="bibr" rid="c25">Dockree et al., 2007</xref>; <xref ref-type="bibr" rid="c79">Palva and Palva, 2007</xref>; <xref ref-type="bibr" rid="c113">Wöstmann et al., 2017</xref>; <xref ref-type="bibr" rid="c14">Boudewyn and Carter, 2018</xref>; <xref ref-type="bibr" rid="c50">Jin et al., 2019</xref>; <xref ref-type="bibr" rid="c40">Haro et al., 2022</xref>). Several studies have also found higher baseline alpha power in individuals with AD(H)D vs. controls (<xref ref-type="bibr" rid="c7">Barry et al., 2003</xref>; <xref ref-type="bibr" rid="c51">Johnstone et al., 2013</xref>; <xref ref-type="bibr" rid="c15">Bozhilova et al., 2022</xref>; <xref ref-type="bibr" rid="c71">Michelini et al., 2022</xref>), however results are not always consistent(<xref ref-type="bibr" rid="c60">Loo and Makeig, 2012</xref>; <xref ref-type="bibr" rid="c51">Johnstone et al., 2013</xref>). Similarly, gaze-shifts are often used as a proxy for distraction, particularly under naturalistic conditions (<xref ref-type="bibr" rid="c104">’t Hart et al., 2009</xref>; <xref ref-type="bibr" rid="c32">Foulsham et al., 2011</xref>; <xref ref-type="bibr" rid="c90">Risko and Kingstone, 2011</xref>; <xref ref-type="bibr" rid="c89">Risko et al., 2012</xref>; <xref ref-type="bibr" rid="c47">Hoppe et al., 2018</xref>), and recent VR and real-life eye-tracking studies have found increased frequency and prolonged duration of gaze-shifts towards irrelevant locations in individuals with AD(H)D (<xref ref-type="bibr" rid="c16">Braga et al., 2016</xref>; <xref ref-type="bibr" rid="c107">Türkan et al., 2016</xref>; <xref ref-type="bibr" rid="c109">Vakil et al., 2019</xref>; <xref ref-type="bibr" rid="c67">Mauriello et al., 2022</xref>; <xref ref-type="bibr" rid="c102">Stokes et al., 2022</xref>). The current results extend these findings to more ecological contexts and indicate that individuals who exhibited higher alpha-power, performed more frequent gaze-shifts and had larger N1 response to background sound-events in our VR classroom were more likely to report experiencing attentional difficulties in real-life.</p>
<p>One open question regarding the comparison between the binary vs. the continuous regression models is why in the latter case the neural speech-decoding accuracy did not contribute to explaining ASRS scores, despite showing nice between-group effects. We also note that although the current results had significant explanatory power of variance in AD(H)D symptoms, they are far from exhaustive with substantial variance still unexplained by these neurophysiological measures. In this regard it is important to bear in mind that the measures of AD(H)D themselves (diagnosis and ASRS scores) are also not entirely reliable, given the diversity diagnostic practice (<xref ref-type="bibr" rid="c39">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="c5">Barkley, 2019</xref>; <xref ref-type="bibr" rid="c73">Mulraney et al., 2022</xref>; <xref ref-type="bibr" rid="c3">Arrondo et al., 2023</xref>; <xref ref-type="bibr" rid="c96">Schweitzer and Zion Golumbic, 2023</xref>) and the subjective nature of self-reports (which are prone to bias (e.g., recency, salience, and generalization effects (<xref ref-type="bibr" rid="c87">Rabiner et al., 2010</xref>; <xref ref-type="bibr" rid="c106">Toplak et al., 2013</xref>; <xref ref-type="bibr" rid="c17">Brevik et al., 2020</xref>)). What is clear, though, is that no single neurophysiological measure alone is sufficient for explaining differences between the individuals – whether through the lens of clinical diagnosis or through report of symptoms. These data emphasize the complex nature of attentional functioning, especially under realistic conditions, and that fact that “attentional challenges” may manifest in a variety of different ways, indexed though different neurophysiological responses. Moving forward, we believe that by measuring multiple aspects of neural and physiological responses, and studying these in diverse and ecologically valid conditions, will ultimately lead to distilling specific neurophysiological patterns that can reliably generalize across contexts.</p>
</sec>
<sec id="s4b">
<title>What about the P300?</title>
<p>Notably, the current results diverge from previous studies of distractibility in AD(H)D in several ways. In traditional ERP studies, distractibility by irrelevant stimuli is often associated with a family of late-latency positive neural responses (the P300 family), and specifically the P3a ERP component, which is typically observed at a latency of ∼250-400ms after hearing novel or surprising sounds (<xref ref-type="bibr" rid="c92">SanMiguel et al., 2010</xref>; <xref ref-type="bibr" rid="c111">Wetzel et al., 2013</xref>; <xref ref-type="bibr" rid="c66">Masson and Bidet-Caulet, 2019</xref>; <xref ref-type="bibr" rid="c8">Barry et al., 2020</xref>). Given this vast literature, we had assumed that background sound-events in our VR classroom would also elicit this response. Moreover, past studies have shown reduced P3a responses in individuals with AD(H)D (<xref ref-type="bibr" rid="c7">Barry et al., 2003</xref>; <xref ref-type="bibr" rid="c38">Gumenyuk et al., 2005</xref>; <xref ref-type="bibr" rid="c52">Kaiser et al., 2020</xref>; <xref ref-type="bibr" rid="c83">Peisch et al., 2021</xref>; <xref ref-type="bibr" rid="c54">Kwasa et al., 2023</xref>), leading us to expect a similar pattern here. However, here the early N1 response to sound-events was larger in the ADHD group but no differences were found for any of later responses. Moreover, visual inspection of the ERPs to Artificial and non-verbal Human sound-events illustrates that not all ecological sounds elicit the same time-course of responses in mid- to late time windows.</p>
<p>When comparing the current results from the VR classroom with more traditional ERP studies of distractibility, we must consider the vast differences in context and task requirements and the ‘functional status’ of background sound-events in these studies. The P3a is typically observed in tasks that require high levels of perceptual vigilance to discriminate between specific stimulus features, such as the pitch, duration or intensity of sounds presented in an oddball paradigm. In these tasks, so-called ‘novel’ or ‘distractor’ sounds/stimuli are presented as part of the sequence of stimuli that need to be discriminated, and these stimuli elicit a P3a (whereas target sounds elicit a somewhat similar P3b response (<xref ref-type="bibr" rid="c8">Barry et al., 2020</xref>)). Accordingly, although these ‘distractor’ stimuli do not require an explicit response, they are in fact within the focus of participants’ endogenous attention, rather than truly ‘background’ stimuli (<xref ref-type="bibr" rid="c63">Makov et al., 2023</xref>). This differs substantially from the role of the novel sounds-events in the VR Classroom task used here, where sound-events are clearly in the background– both spatially and semantically - and occur in parallel to an ongoing speech stimulus (the lecture) that participants are primarily paying attention to. In addition, the perceptual and cognitive demands of the VR Classroom – where participants are instructed to listen and understand the speech as they would in a real-life classroom - differ substantially from the highly vigilant demands of speeded-response tasks. These important differences in task and stimulus characteristic ultimately lead to a different treatment of novel sound-events by the brain, emphasizing the importance of considering context when comparing results across experimental designs (<xref ref-type="bibr" rid="c63">Makov et al., 2023</xref>; <xref ref-type="bibr" rid="c64">Mandal et al., 2024</xref>).</p>
</sec>
<sec id="s4c">
<title>Null results – insufficient sensitivity or true lack of differences?</title>
<p>Besides the lack of a reliable P300-like effect here, several additional measures that were evaluated here did not show any group-wise effects and did not contribute to explaining ASRS scores, despite previous literature suggesting that they might. Although null-results are difficult to interpret fully, we find it worthwhile to note these here and briefly discuss potential reasons for the lack of group-related effects. Perhaps the most surprising null-effect was the lack of differences in behavioral performance. Participants in both groups achieved similar behavioral performance in the Quiet and Events condition, suggesting that the presence of occasional sound-events ultimately did not ultimately impair their ability to understand the content of the mini-lectures (at least at the level probed by our comprehension questions). These results should be interpreted bearing in mind the details of the current task-design – we deliberately chose a task that would mimic the level of information processing required in real-life learning contexts (as opposed to speeded-response tasks used in traditional attention research that encourage failures of performance by creating perceptually ‘extreme’ conditions (<xref ref-type="bibr" rid="c39">Hall et al., 2016</xref>; <xref ref-type="bibr" rid="c5">Barkley, 2019</xref>; <xref ref-type="bibr" rid="c3">Arrondo et al., 2023</xref>). Moreover, the presence of occasional sound-events – while noticeable, was not design to be extremely adverse. Under these conditions, individuals with and without AD(H)D were able to maintain good performance despite the occasional acoustic disturbances (<xref ref-type="bibr" rid="c64">Mandal et al., 2024</xref>), indicating that neurophysiological indication of somewhat reduced attention do not necessarily translate to poorer behavioral outcomes. It is of course possible that larger behavioral effects would have emerged if task-demands or the severity of acoustic disturbances were more challenging.</p>
<p>Besides neural evoked-responses, background sound-events also elicited a transient increase in SC that is associated with an ‘orienting reflex’, reflecting transient changes in arousal/alertness (<xref ref-type="bibr" rid="c33">Frith and Allen, 1983</xref>; <xref ref-type="bibr" rid="c114">Zimmer and Richter, 2023</xref>). This response is considered obligatory and automatic, and does not require allocation of attention as it can be observed during sleep and other unconscious states (albeit the magnitude of these responses can vary as a function of attention). Although we had hypothesized that this response might be heightened in individuals with AD(H)D, as has been reported in some studies (<xref ref-type="bibr" rid="c99">Sergeant, 2000</xref>; <xref ref-type="bibr" rid="c9">Bellato et al., 2020</xref>) (similar to the effect on the N1 response), the current results do not support this. Indeed, there has been much debate regarding the relationship between the SC orienting reflex and early neural responses, with conflicting results reported across studies (<xref ref-type="bibr" rid="c57">Lim et al., 1996</xref>; <xref ref-type="bibr" rid="c62">MacDonald and Barry, 2020</xref>). Additional research is likely required to better characterize the relationship between neural and physiological responses to surprising events in the environment.</p>
<p>Last, another interesting null-result was the lack of overt-gaze shifts following sound-events. Gaze-shifts are often considered a proxy for distractibility and capture of attention (<xref ref-type="bibr" rid="c80">Parkhurst et al., 2002</xref>; <xref ref-type="bibr" rid="c34">Geisler and Cormack, 2011</xref>; <xref ref-type="bibr" rid="c78">Nissens et al., 2016</xref>), particularly in individuals with AD(H)D. Accordingly, we had expected to find that sound-event elicited overt shifts towards the spatial location from which they were emitted. However, this was not the case, and individuals (in both groups) did not perform more gaze-shifts following sound-events relative to their baseline tendencies. A similar result was recently reported by our group in a VR café scenario, where that hearing semantically-salient words in background speech (e.g., your name or semantic violations) did elicited neural and SC responses, but was not accompanied by gaze-shifts (<xref ref-type="bibr" rid="c18">Brown et al., 2023</xref>). These results might indicate that peripheral auditory stimuli are, generally, less effective than visual stimuli at capturing overt attention (<xref ref-type="bibr" rid="c108">Turoman and Vergauwe, 2023</xref>; <xref ref-type="bibr" rid="c64">Mandal et al., 2024</xref>), or that in ecological audiovisual settings individuals are less prone to move their eyes to search for the source of auditory disturbances, despite noticing them. These possibilities will be further investigated in follow-up VR studies, comparing the effects of irrelevant auditory and visual events.</p>
</sec>
<sec id="s4d">
<title>General considerations</title>
<p>Before concluding, we note several important caveats, pertaining to the specific sample tested here. First, as noted, classification of the AD(H)D group was based on their prior clinical diagnosis, which likely varied across participants in terms of diagnosis-approach and age of diagnosis. Although the two groups were distinguishable in their self-reported ASRS scores, a more reliable assessment of the severity of current AD(H)D symptoms might have been achieved if we had conducted a full clinical assessment of all participants, as per the acceptable clinical guidelines (American Psychiatric <xref ref-type="bibr" rid="c2">Association, 2022</xref>). Second, this study was conducted on adults, whereas the majority of AD(H)D research – and particularly all VR classroom studies - have focused on children(<xref ref-type="bibr" rid="c85">Pollak et al., 2009</xref>; <xref ref-type="bibr" rid="c77">Neguț et al., 2017</xref>; <xref ref-type="bibr" rid="c95">Schweitzer and Rizzo, 2022</xref>; <xref ref-type="bibr" rid="c97">Seesjärvi et al., 2022</xref>; <xref ref-type="bibr" rid="c102">Stokes et al., 2022</xref>). AD(H)D is primarily defined as a childhood condition and although it often persists into adulthood, with profound implications on professional and academics outcomes, how the nature of the deficit changes with age is still unknown (<xref ref-type="bibr" rid="c6">Barkley et al., 2008</xref>). Moreover, the adult cohort tested here included primarily university students, who do not represent all adults but rather are a sub-group with relatively high cognitive and learning abilities, who are accustomed to sitting in lectures for long periods of time (<xref ref-type="bibr" rid="c43">Henrich et al., 2010</xref>). Hence, it is possible that the lack of group-differences in some metrics in the current dataset (and particularly in their behavioral outcomes) is because these individuals have learned to deal with distraction and have developed appropriate strategies, which allow them to thrive in academic settings. This caveat highlights the importance of considering sampling techniques in group-comparison studies, which sometimes limits their interpretation to more circumscribed sub-samples (<xref ref-type="bibr" rid="c88">Rad et al., 2018</xref>). At the same time, even if the current findings pertain only to this highly-functioning academic subgroup of individuals with AD(H)D, they imply an impressive ability to adapt and cope with the naturally occurring distractions of realistic classrooms. Specifically, they suggest that withstanding distraction in real-life contexts may be improved through training and good habit-forming, even in individuals with AD(H)D. We hope to follow up on this research in the future, to deepen our understanding of the diversity within the AD(H)D and typical population and to better characterize attention and distractibility across different types of real-life scenarios.</p>
</sec>
<sec id="s4e">
<title>Conclusions</title>
<p>Paying attention in class requires both investing cognitive resources in processing the lesson itself (e.g., the teacher’s speech), and trying to minimize intrusions from background events. Here we found that both of these operations are less effective in individuals who are diagnosed with AD(H)D or who report more severe symptoms of difficulties in everyday attention. Specifically, these individuals exhibit heightened sensory response to irrelevant sounds, reduced speech tracking of the teacher, a tendency towards more frequent gaze-shifts away from the teacher, and somewhat increased alpha-power. Interestingly, since these metrics were not correlated with each other, they likely represent different ways through which inattention or distraction may manifest, rather than a general ‘biomarker’ of inattention. Counter to some prevalent notions, these data suggest that AD(H)D may not be characterized by a specific “neural marker” indicating attentional deficits, particularly when tested under ecological circumstances (<xref ref-type="bibr" rid="c5">Barkley, 2019</xref>; <xref ref-type="bibr" rid="c31">Faraone et al., 2021</xref>; <xref ref-type="bibr" rid="c101">Sonuga-Barke et al., 2023</xref>). Rather, they resonate with recent calls to revisit the parameters used for defining ‘attention-disorders’ and the need to adopt more nuanced and dimensional approach, as well as designing more ecologically-valid means of testing, in order to better characterize and understand the complex nature of real-life attentional challenges(<xref ref-type="bibr" rid="c65">Marcus and Barry, 2011</xref>; <xref ref-type="bibr" rid="c41">Heidbreder, 2015</xref>; <xref ref-type="bibr" rid="c72">Morris et al., 2022</xref>; <xref ref-type="bibr" rid="c27">Elahi et al., 2023</xref>).</p>
</sec>
</sec>
<sec id="d1e1580" sec-type="supplementary-material">
<title>Supporting information</title>
<supplementary-material id="d1e1597">
<label>Supplemental materials</label>
<media xlink:href="supplements/590012_file02.docx"/>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by the Israel Science Foundation (ISF grant # 2339/20 to EZG), the National Institute of Mental Health (NIMH grant R33MH110043 to JS) and the Binational Science Foundation (BSF grant # 2022024 to EZG &amp; JS).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname> <given-names>NC</given-names></string-name>, <string-name><surname>Bischof</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group> (<year>2023</year>) <article-title>Eye tracking in virtual reality</article-title>. <source>Curr Top Behav Neurosci</source> <volume>65</volume>:<fpage>73</fpage>–<lpage>100</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>American Psychiatric</given-names> <surname>Association</surname></string-name></person-group> (<year>2022</year>) <source>Diagnostic and Statistical Manual of Mental Disorders</source>. <edition>5th ed</edition> <publisher-loc>Arlington, VA</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arrondo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Mulraney</surname> <given-names>M</given-names></string-name>, <string-name><surname>Iturmendi-Sabater</surname> <given-names>I</given-names></string-name>, <string-name><surname>Musullulu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Gambra</surname> <given-names>L</given-names></string-name>, <string-name><surname>Niculcea</surname> <given-names>T</given-names></string-name>, <string-name><surname>Banaschewski</surname> <given-names>T</given-names></string-name>, <string-name><surname>Simonoff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Döpfner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hinshaw</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Coghill</surname> <given-names>D</given-names></string-name>, <string-name><surname>Cortese</surname> <given-names>S</given-names></string-name></person-group> (<year>2023</year>) <article-title>Systematic review and meta-analysis: clinical utility of continuous performance tests for the identification of attention-deficit/hyperactivity disorder</article-title>. <source>J Am Acad Child Adolesc Psychiatry</source> <volume>0</volume>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avisar</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shalev</surname> <given-names>L</given-names></string-name></person-group> (<year>2011</year>) <article-title>Sustained attention and behavioral characteristics associated with adhd in adults</article-title>. <source>Appl Neuropsychol</source> <volume>18</volume>:<fpage>107</fpage>–<lpage>116</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barkley</surname> <given-names>RA</given-names></string-name></person-group> (<year>2019</year>) <article-title>Neuropsychological testing is not useful in the diagnosis of adhd: stop it (or prove it)!</article-title> <source>ADHD Rep</source> <volume>27</volume>:<fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barkley</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>KR</given-names></string-name>, <string-name><surname>Fischer</surname> <given-names>M</given-names></string-name></person-group> (<year>2008</year>) <source>ADHD in adults: what the science says</source>. <publisher-loc>New York, USA</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barry</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Johnstone</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Clarke</surname> <given-names>AR</given-names></string-name></person-group> (<year>2003</year>) <article-title>A review of electrophysiology in attention-deficit/hyperactivity disorder: ii. event-related potentials</article-title>. <source>Clinical Neurophysiology</source> <volume>114</volume>:<fpage>184</fpage>–<lpage>198</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barry</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Steiner</surname> <given-names>GZ</given-names></string-name>, <string-name><surname>De Blasio</surname> <given-names>FM</given-names></string-name>, <string-name><surname>Fogarty</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Karamacoska</surname> <given-names>D</given-names></string-name>, <string-name><surname>MacDonald</surname> <given-names>B</given-names></string-name></person-group> (<year>2020</year>) <article-title>Components in the p300: don’t forget the novelty p3!</article-title> <source>Psychophysiology</source> <volume>57</volume>:<fpage>e13371</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellato</surname> <given-names>A</given-names></string-name>, <string-name><surname>Arora</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hollis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Groom</surname> <given-names>MJ</given-names></string-name></person-group> (<year>2020</year>) <article-title>Is autonomic nervous system function atypical in attention deficit hyperactivity disorder (adhd)? a systematic review of the evidence</article-title>. <source>Neurosci Biobehav Rev</source> <volume>108</volume>:<fpage>182</fpage>–<lpage>206</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benedek</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kaernbach</surname> <given-names>C</given-names></string-name></person-group> (<year>2010</year>) <article-title>A continuous measure of phasic electrodermal activity</article-title>. <source>J Neurosci Methods</source> <volume>190</volume>:<fpage>80</fpage>–<lpage>91</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berger</surname> <given-names>I</given-names></string-name>, <string-name><surname>Cassuto</surname> <given-names>H</given-names></string-name></person-group> (<year>2014</year>) <article-title>The effect of environmental distractors incorporation into a cpt on sustained attention and adhd diagnosis among adolescents</article-title>. <source>J Neurosci Methods</source> <volume>222</volume>:<fpage>62</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berti</surname> <given-names>S</given-names></string-name></person-group> (<year>2013</year>) <article-title>The role of auditory transient and deviance processing in distraction of task performance: a combined behavioral and event-related brain potential study</article-title>. <source>Front Hum Neurosci</source> <volume>7</volume>:<fpage>54615</fpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bidet-Caulet</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bottemanne</surname> <given-names>L</given-names></string-name>, <string-name><surname>Fonteneau</surname> <given-names>C</given-names></string-name>, <string-name><surname>Giard</surname> <given-names>M-H</given-names></string-name>, <string-name><surname>Bertrand</surname> <given-names>O</given-names></string-name></person-group> (<year>2015</year>) <article-title>Brain dynamics of distractibility: interaction between top-down and bottom-up mechanisms of auditory attention</article-title>. <source>Brain Topogr</source> <volume>28</volume>:<fpage>423</fpage>–<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boudewyn</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Carter</surname> <given-names>CS</given-names></string-name></person-group> (<year>2018</year>) <article-title>I must have missed that: alpha-band oscillations track attention to spoken language</article-title>. <source>Neuropsychologia</source> <volume>117</volume>:<fpage>148</fpage>–<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bozhilova</surname> <given-names>N</given-names></string-name>, <string-name><surname>Kuntsi</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rubia</surname> <given-names>K</given-names></string-name>, <string-name><surname>Asherson</surname> <given-names>P</given-names></string-name>, <string-name><surname>Michelini</surname> <given-names>G</given-names></string-name></person-group> (<year>2022</year>) <article-title>Event-related brain dynamics during mind wandering in attention-deficit/hyperactivity disorder: an experience-sampling approach</article-title>. <source>Neuroimage Clin</source> <volume>35</volume>:<fpage>103068</fpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braga</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Fu</surname> <given-names>RZ</given-names></string-name>, <string-name><surname>Seemungal</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Wise</surname> <given-names>RJS</given-names></string-name>, <string-name><surname>Leech</surname> <given-names>R</given-names></string-name></person-group> (<year>2016</year>) <article-title>Eye movements during auditory attention predict individual differences in dorsal attention network activity</article-title>. <source>Front Hum Neurosci</source> <volume>10</volume>:<fpage>164</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brevik</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Lundervold</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Haavik</surname> <given-names>J</given-names></string-name>, <string-name><surname>Posserud</surname> <given-names>MB</given-names></string-name></person-group> (<year>2020</year>) <article-title>Validity and accuracy of the adult attention-deficit/hyperactivity disorder (adhd) self-report scale (asrs) and the wender utah rating scale (wurs) symptom checklists in discriminating between adults with and without adhd</article-title>. <source>Brain Behav</source> <volume>10</volume>:<fpage>e01605</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>D</given-names></string-name>, <string-name><surname>Burgart</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zvilichovsky</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zion-Golumbic</surname> <given-names>E</given-names></string-name></person-group> (<year>2023</year>) <article-title>Neurophysiological evidence for semantic processing of irrelevant speech and own-name detection in a virtual café</article-title>. <source>Journal of Neuroscience</source> <volume>43</volume>:<fpage>5045</fpage>–<lpage>5056</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chennu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Noreika</surname> <given-names>V</given-names></string-name>, <string-name><surname>Gueorguiev</surname> <given-names>D</given-names></string-name>, <string-name><surname>Blenkmann</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kochen</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ibáñez</surname> <given-names>A</given-names></string-name>, <string-name><surname>Owen</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Bekinschtein</surname> <given-names>TA</given-names></string-name></person-group> (<year>2013</year>) <article-title>Expectation and attention in hierarchical auditory prediction</article-title>. <source>Journal of Neuroscience</source> <volume>33</volume>:<fpage>11194</fpage>–<lpage>11205</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clarke</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>McCarthy</surname> <given-names>R</given-names></string-name>, <string-name><surname>Selikowitz</surname> <given-names>M</given-names></string-name></person-group> (<year>2001</year>) <article-title>Excess beta activity in children with attention-deficit/hyperactivity disorder: an atypical electrophysiological group</article-title>. <source>Psychiatry Res</source> <volume>103</volume>:<fpage>205</fpage>–<lpage>218</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coleman</surname> <given-names>B</given-names></string-name>, <string-name><surname>Marion</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rizzo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Turnbull</surname> <given-names>J</given-names></string-name>, <string-name><surname>Nolty</surname> <given-names>A</given-names></string-name></person-group> (<year>2019</year>) <article-title>Virtual reality assessment of classroom – related attention: an ecologically relevant approach to evaluating the effectiveness of working memory training</article-title>. <source>Front Psychol</source> <volume>10</volume>:<fpage>1851</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crosse</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Zuk</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Di Liberto</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Nidiffer</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Molholm</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lalor</surname> <given-names>EC</given-names></string-name></person-group> (<year>2021</year>) <article-title>Linear modeling of neurophysiological responses to speech and other continuous stimuli: methodological considerations for applied research</article-title>. <source>Front Neurosci</source> <volume>15</volume>:<fpage>705621</fpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2012a</year>) <article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>109</volume>:<fpage>11854</fpage>–<lpage>11859</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name></person-group> (<year>2012b</year>) <article-title>Neural coding of continuous speech in auditory cortex during monaural and dichotic listening</article-title>. <source>J Neurophysiol</source> <volume>107</volume>:<fpage>78</fpage>–<lpage>89</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dockree</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Kelly</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Foxe</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Reilly</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Robertson</surname> <given-names>IH</given-names></string-name></person-group> (<year>2007</year>) <article-title>Optimal sustained attention is linked to the spectral content of background eeg activity: greater ongoing tonic alpha (approximately 10 hz) power supports successful phasic goal activation</article-title>. <source>Eur J Neurosci</source> <volume>25</volume>:<fpage>900</fpage>–<lpage>907</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Donoghue</surname> <given-names>T</given-names></string-name>, <string-name><surname>Haller</surname> <given-names>M</given-names></string-name>, <string-name><surname>Peterson</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Varma</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sebastian</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gao</surname> <given-names>R</given-names></string-name>, <string-name><surname>Noto</surname> <given-names>T</given-names></string-name>, <string-name><surname>Lara</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Wallis</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Knight</surname> <given-names>RT</given-names></string-name>, <string-name><surname>Shestyuk</surname> <given-names>A</given-names></string-name>, <string-name><surname>Voytek</surname> <given-names>B</given-names></string-name></person-group> (<year>2020</year>) <article-title>Parameterizing neural power spectra into periodic and aperiodic components</article-title>. <source>Nat Neurosci</source> <volume>23</volume>:<fpage>1655</fpage>–<lpage>1665</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elahi</surname> <given-names>H</given-names></string-name>, <string-name><surname>Iosif</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Mukherjee</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hinshaw</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Schweitzer</surname> <given-names>JB</given-names></string-name></person-group> (<year>2023</year>) <article-title>Using hot and cool measures to phenotype and predict functional outcomes across dimensions of adhd and typical development in adolescents</article-title>. <source>Res Child Adolesc Psychopathol</source>:<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Escera</surname> <given-names>C</given-names></string-name>, <string-name><surname>Alho</surname> <given-names>K</given-names></string-name>, <string-name><surname>Winkler</surname> <given-names>I</given-names></string-name>, <string-name><surname>Näätänen</surname> <given-names>R</given-names></string-name></person-group> (<year>1998</year>) <article-title>Neural mechanisms of involuntary attention to acoustic novelty and change</article-title>. <source>J Cogn Neurosci</source> <volume>10</volume>:<fpage>590</fpage>–<lpage>604</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Escera</surname> <given-names>C</given-names></string-name>, <string-name><surname>Yago</surname> <given-names>E</given-names></string-name>, <string-name><surname>Corral</surname> <given-names>M-J</given-names></string-name>, <string-name><surname>Corbera</surname> <given-names>S</given-names></string-name>, <string-name><surname>Nuñez</surname> <given-names>MI</given-names></string-name></person-group> (<year>2003</year>) <article-title>Attention capture by auditory significant stimuli: semantic analysis follows attention switching</article-title>. <source>European Journal of Neuroscience</source> <volume>18</volume>:<fpage>2408</fpage>–<lpage>2412</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esterman</surname> <given-names>M</given-names></string-name>, <string-name><surname>Rothlein</surname> <given-names>D</given-names></string-name></person-group> (<year>2019</year>) <article-title>Models of sustained attention</article-title>. <source>Curr Opin Psychol</source> <volume>29</volume>:<fpage>174</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faraone</surname> <given-names>S V.</given-names></string-name> <etal>et al.</etal></person-group> (<year>2021</year>) <article-title>The world federation of adhd international consensus statement: 208 evidence-based conclusions about the disorder</article-title>. <source>Neurosci Biobehav Rev</source> <volume>128</volume>:<fpage>789</fpage>–<lpage>818</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foulsham</surname> <given-names>T</given-names></string-name>, <string-name><surname>Walker</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group> (<year>2011</year>) <article-title>The where, what and when of gaze allocation in the lab and the natural environment</article-title>. <source>Vision Res</source> <volume>51</volume>:<fpage>1920</fpage>–<lpage>1931</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frith</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Allen</surname> <given-names>HA</given-names></string-name></person-group> (<year>1983</year>) <article-title>The skin conductance orienting response as an index of attention</article-title>. <source>Biol Psychol</source> <volume>17</volume>:<fpage>27</fpage>–<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Geisler</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Cormack</surname> <given-names>LK</given-names></string-name></person-group> (<year>2011</year>) <source>Models of overt attention</source>. <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gloss</surname> <given-names>D</given-names></string-name>, <string-name><surname>Varma</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Pringsheim</surname> <given-names>T</given-names></string-name>, <string-name><surname>Nuwer</surname> <given-names>MR</given-names></string-name></person-group> (<year>2016</year>) <article-title>Practice advisory: the utility of eeg theta/beta power ratio in adhd diagnosis</article-title>. <source>Neurology</source> <volume>87</volume>:<fpage>2375</fpage>–<lpage>2379</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gray</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Dueck</surname> <given-names>K</given-names></string-name>, <string-name><surname>Rogers</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tannock</surname> <given-names>R</given-names></string-name></person-group> (<year>2017</year>) <article-title>Qualitative review synthesis: the relationship between inattention and academic achievement</article-title>. <source>Educational Research</source> <volume>59</volume>:<fpage>17</fpage>–<lpage>35</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Grosbras</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Laird</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Paus</surname> <given-names>T</given-names></string-name></person-group> (<year>2005</year>) <chapter-title>Cortical regions involved in eye movements, shifts of attention, and gaze perception</chapter-title>. In: <source>Human Brain Mapping</source>. pp <fpage>140</fpage>–<lpage>154</lpage> <publisher-name>Wiley-Blackwell</publisher-name>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gumenyuk</surname> <given-names>V</given-names></string-name>, <string-name><surname>Korzyukov</surname> <given-names>O</given-names></string-name>, <string-name><surname>Escera</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hämäläinen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Huotilainen</surname> <given-names>M</given-names></string-name>, <string-name><surname>Häyrinen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Oksanen</surname> <given-names>H</given-names></string-name>, <string-name><surname>Näätänen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Von Wendt</surname> <given-names>L</given-names></string-name>, <string-name><surname>Alho</surname> <given-names>K</given-names></string-name></person-group> (<year>2005</year>) <article-title>Electrophysiological evidence of enhanced distractibility in adhd children</article-title>. <source>Neurosci Lett</source> <volume>374</volume>:<fpage>212</fpage>–<lpage>217</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hall</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Valentine</surname> <given-names>AZ</given-names></string-name>, <string-name><surname>Groom</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Walker</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Sayal</surname> <given-names>K</given-names></string-name>, <string-name><surname>Daley</surname> <given-names>D</given-names></string-name>, <string-name><surname>Hollis</surname> <given-names>C</given-names></string-name></person-group> (<year>2016</year>) <article-title>The clinical utility of the continuous performance test and objective measures of activity for diagnosing and monitoring adhd in children: a systematic review</article-title>. <source>Eur Child Adolesc Psychiatry</source> <volume>25</volume>:<fpage>677</fpage>–<lpage>699</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haro</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rao</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Quatieria</surname> <given-names>TF</given-names></string-name>, <string-name><surname>Smalt</surname> <given-names>CJ</given-names></string-name></person-group> (<year>2022</year>) <article-title>EEG alpha and pupil diameter reflect endogenous auditory attention switching and listening effort</article-title>. <source>European Journal of Neuroscience</source>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heidbreder</surname> <given-names>R</given-names></string-name></person-group> (<year>2015</year>) <article-title>ADHD symptomatology is best conceptualized as a spectrum: a dimensional versus unitary approach to diagnosis</article-title>. <source>ADHD Attention Deficit and Hyperactivity Disorders</source> 2015 7:4 <volume>7</volume>:<fpage>249</fpage>–<lpage>269</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heil</surname> <given-names>P</given-names></string-name></person-group> (<year>1997</year>) <article-title>Auditory cortical onset responses revisited. ii. response strength</article-title>. <source>J Neurophysiol</source> <volume>77</volume>:<fpage>2642</fpage>–<lpage>2660</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henrich</surname> <given-names>J</given-names></string-name>, <string-name><surname>Heine</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Norenzayan</surname> <given-names>A</given-names></string-name></person-group> (<year>2010</year>) <article-title>The weirdest people in the world?</article-title> <source>Behavioral and Brain Sciences</source> <volume>33</volume>:<fpage>61</fpage>–<lpage>83</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hillyard</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hink</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schwent</surname> <given-names>V</given-names></string-name>, <string-name><surname>Picton</surname> <given-names>T</given-names></string-name></person-group> (<year>1973</year>) <article-title>Electrical signs of selective attention in the human brain</article-title>. <source>Science</source> <volume>182</volume>:<fpage>177</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hobbiss</surname> <given-names>MH</given-names></string-name>, <string-name><surname>Lavie</surname> <given-names>N</given-names></string-name></person-group> (<year>2024</year>) <article-title>Sustained selective attention in adolescence: cognitive development and predictors of distractibility at school</article-title>. <source>J Exp Child Psychol</source> <volume>238</volume>:<fpage>105784</fpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holtze</surname> <given-names>B</given-names></string-name>, <string-name><surname>Jaeger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Debener</surname> <given-names>S</given-names></string-name>, <string-name><surname>Adiloğlu</surname> <given-names>K</given-names></string-name>, <string-name><surname>Mirkovic</surname> <given-names>B</given-names></string-name></person-group> (<year>2021</year>) <article-title>Are they calling my name? attention capture is reflected in the neural tracking of attended and ignored speech</article-title>. <source>Front Neurosci</source> <volume>15</volume>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoppe</surname> <given-names>S</given-names></string-name>, <string-name><surname>Loetscher</surname> <given-names>T</given-names></string-name>, <string-name><surname>Morey</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Bulling</surname> <given-names>A</given-names></string-name></person-group> (<year>2018</year>) <article-title>Eye movements during everyday behavior predict personality traits</article-title>. <source>Front Hum Neurosci</source> <volume>12</volume>:<fpage>105</fpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname> <given-names>N</given-names></string-name>, <string-name><surname>Elhilali</surname> <given-names>M</given-names></string-name></person-group> (<year>2020</year>) <article-title>Push-pull competition between bottom-up and top-down auditory attention to natural soundscapes</article-title>. <source>Elife</source> <volume>9</volume>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="software"><person-group person-group-type="author"><collab>JASP Team</collab></person-group> (<year>2024</year>) <source>JASP (version 0.19.0)</source>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jin</surname> <given-names>CY</given-names></string-name>, <string-name><surname>Borst</surname> <given-names>JP</given-names></string-name>, <string-name><given-names>Vugt MK</given-names> <surname>Van</surname></string-name></person-group> (<year>2019</year>) <article-title>Predicting task-general mind-wandering with eeg</article-title>. <source>Cogn Affect Behav Neurosci</source>:<fpage>1059</fpage>–<lpage>1073</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnstone</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Clarke</surname> <given-names>AR</given-names></string-name></person-group> (<year>2013</year>) <article-title>Ten years on: a follow-up review of erp research in attention-deficit/hyperactivity disorder</article-title>. <source>Clinical Neurophysiology</source> <volume>124</volume>:<fpage>644</fpage>–<lpage>657</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaiser</surname> <given-names>A</given-names></string-name>, <string-name><surname>Aggensteiner</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Baumeister</surname> <given-names>S</given-names></string-name>, <string-name><surname>Holz</surname> <given-names>NE</given-names></string-name>, <string-name><surname>Banaschewski</surname> <given-names>T</given-names></string-name>, <string-name><surname>Brandeis</surname> <given-names>D</given-names></string-name></person-group> (<year>2020</year>) <article-title>Earlier versus later cognitive event-related potentials (erps) in attention-deficit/hyperactivity disorder (adhd): a meta-analysis</article-title>. <source>Neurosci Biobehav Rev</source> <volume>112</volume>:<fpage>117</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaufman</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zion-Golumbic</surname> <given-names>E</given-names></string-name></person-group> (<year>2023</year>) <article-title>Listening to two speakers: capacity and tradeoffs in neural speech tracking during selective and distributed attention</article-title>. <source>Neuroimage</source>:<elocation-id>e119984</elocation-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kwasa</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Noyce</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Torres</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Richardson</surname> <given-names>BN</given-names></string-name>, <string-name><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></string-name></person-group> (<year>2023</year>) <article-title>Top-down auditory attention modulates neural responses more strongly in neurotypical than adhd young adults</article-title>. <source>Brain Res</source> <volume>1798</volume>:<fpage>148144</fpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lange</surname> <given-names>K</given-names></string-name></person-group> (<year>2013</year>) <article-title>The ups and downs of temporal orienting: a review of auditory temporal orienting studies and a model associating the heterogeneous findings on the auditory n1 with opposite effects of attention and prediction</article-title>. <source>Front Hum Neurosci</source> <volume>7</volume>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levy</surname> <given-names>O</given-names></string-name>, <string-name><surname>Korisky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zvilichovsky</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zion Golumbic</surname> <given-names>E</given-names></string-name></person-group> (<year>2024</year>) <article-title>The neurophysiological costs of learning in a noisy classroom: an ecological virtual reality study</article-title>. <source>The Journal of Cognitive Neuroscience</source> in press.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lim</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>E</given-names></string-name>, <string-name><surname>Sawant</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rennie</surname> <given-names>C</given-names></string-name>, <string-name><surname>Yiannikas</surname> <given-names>C</given-names></string-name></person-group> (<year>1996</year>) <article-title>The relationship between quantified eeg and skin conductance level</article-title>. <source>International Journal of Psychophysiology</source> <volume>21</volume>:<fpage>151</fpage>–<lpage>162</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Litner</surname> <given-names>B</given-names></string-name></person-group> (<year>2003</year>) <article-title>Teens with adhd: the challenge of high school</article-title>. <source>Child Youth Care Forum</source> <volume>32</volume>:<fpage>137</fpage>– <lpage>158</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loe</surname> <given-names>IM</given-names></string-name>, <string-name><surname>Feldman</surname> <given-names>HM</given-names></string-name></person-group> (<year>2007</year>) <article-title>Academic and educational outcomes of children with adhd</article-title>. <source>J Pediatr Psychol</source> <volume>32</volume>:<fpage>643</fpage>–<lpage>654</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loo</surname> <given-names>SK</given-names></string-name>, <string-name><surname>Makeig</surname> <given-names>S</given-names></string-name></person-group> (<year>2012</year>) <article-title>Clinical utility of eeg in attention-deficit/hyperactivity disorder: a research update</article-title>. <source>Neurotherapeutics</source> <volume>9</volume>:<fpage>569</fpage>–<lpage>587</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lütkenhöner</surname> <given-names>B</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>JS</given-names></string-name></person-group> (<year>2007</year>) <article-title>Auditory evoked field at threshold</article-title>. <source>Hear Res</source> <volume>228</volume>:<fpage>188</fpage>–<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>MacDonald</surname> <given-names>B</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>RJ</given-names></string-name></person-group> (<year>2020</year>) <article-title>Integration of three investigations of novelty, intensity, and significance in dishabituation paradigms: a study of the phasic orienting reflex</article-title>. <source>International Journal of Psychophysiology</source> <volume>147</volume>:<fpage>113</fpage>–<lpage>127</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Makov</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>D</given-names></string-name>, <string-name><surname>Har-shai Yahav</surname> <given-names>P</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Zion Golumbic</surname> <given-names>E</given-names></string-name></person-group> (<year>2023</year>) <article-title>“Unattended, distracting or irrelevant”: theoretical implications of terminological choices in auditory selective attention research</article-title>. <source>Cognition</source> <volume>231</volume>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mandal</surname> <given-names>A</given-names></string-name>, <string-name><surname>Liesefeld</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Liesefeld</surname> <given-names>HR</given-names></string-name></person-group> (<year>2024</year>) <article-title>The surprising robustness of visual search against concurrent auditory distraction</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>50</volume>:<fpage>99</fpage>–<lpage>118</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marcus</surname> <given-names>DK</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>TD</given-names></string-name></person-group> (<year>2011</year>) <article-title>Does attention-deficit/hyperactivity disorder have a dimensional latent structure? a taxometric analysis</article-title>. <source>J Abnorm Psychol</source> <volume>120</volume>:<fpage>427</fpage>–<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Masson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bidet-Caulet</surname> <given-names>A</given-names></string-name></person-group> (<year>2019</year>) <article-title>Fronto-central p3a to distracting sounds: an index of their arousing properties</article-title>. <source>Neuroimage</source> <volume>185</volume>:<fpage>164</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mauriello</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pham</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kumar</surname> <given-names>S</given-names></string-name>, <string-name><surname>Piguet</surname> <given-names>C</given-names></string-name>, <string-name><surname>Deiber</surname> <given-names>M-P</given-names></string-name>, <string-name><surname>Aubry</surname> <given-names>J-M</given-names></string-name>, <string-name><surname>Dayer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Perroud</surname> <given-names>N</given-names></string-name>, <string-name><surname>Berchio</surname> <given-names>C</given-names></string-name></person-group> (<year>2022</year>) <article-title>Dysfunctional temporal stages of eye-gaze perception in adults with adhd: a high-density eeg study</article-title>. <source>Biol Psychol</source> <volume>171</volume>:<fpage>108351</fpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merrill</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Raiker</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Mattfeld</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Macphee</surname> <given-names>FL</given-names></string-name>, <string-name><surname>Ramos</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>X</given-names></string-name>, <string-name><surname>Altszuler</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Schooler</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Coxe</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gnagy</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Greiner</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Coles</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Pelham</surname> <given-names>WE</given-names></string-name></person-group> (<year>2022</year>) <article-title>Mind-wandering and childhood adhd: experimental manipulations across laboratory and naturalistic settings</article-title>. <source>Res Child Adolesc Psychopathol</source> <volume>50</volume>:<fpage>1139</fpage>–<lpage>1149</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mesgarani</surname> <given-names>N</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>EF</given-names></string-name></person-group> (<year>2012</year>) <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source> <volume>485</volume>:<fpage>233</fpage>–<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mesik</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wojtczak</surname> <given-names>M</given-names></string-name></person-group> (<year>2023</year>) <article-title>The effects of data quantity on performance of temporal response function analyses of natural speech processing</article-title>. <source>Front Neurosci</source> <volume>16</volume>:<fpage>963629</fpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Michelini</surname> <given-names>G</given-names></string-name>, <string-name><surname>Salmastyan</surname> <given-names>G</given-names></string-name>, <string-name><surname>Vera</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Lenartowicz</surname> <given-names>A</given-names></string-name></person-group> (<year>2022</year>) <article-title>Event-related brain oscillations in attention-deficit/hyperactivity disorder (adhd): a systematic review and meta-analysis</article-title>. <source>International Journal of Psychophysiology</source> <volume>174</volume>:<fpage>29</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morris</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Sanislow</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Pacheco</surname> <given-names>J</given-names></string-name>, <string-name><surname>Vaidyanathan</surname> <given-names>U</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Cuthbert</surname> <given-names>BN</given-names></string-name></person-group> (<year>2022</year>) <article-title>Revisiting the seven pillars of rdoc</article-title>. <source>BMC Med</source> <volume>20</volume>:<fpage>220</fpage>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mulraney</surname> <given-names>M</given-names></string-name>, <string-name><surname>Arrondo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Musullulu</surname> <given-names>H</given-names></string-name>, <string-name><surname>Iturmendi-Sabater</surname> <given-names>I</given-names></string-name>, <string-name><surname>Cortese</surname> <given-names>S</given-names></string-name>, <string-name><surname>Westwood</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Donno</surname> <given-names>F</given-names></string-name>, <string-name><surname>Banaschewski</surname> <given-names>T</given-names></string-name>, <string-name><surname>Simonoff</surname> <given-names>E</given-names></string-name>, <string-name><surname>Zuddas</surname> <given-names>A</given-names></string-name>, <string-name><surname>Döpfner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hinshaw</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Coghill</surname> <given-names>D</given-names></string-name></person-group> (<year>2022</year>) <article-title>Systematic review and meta-analysis: screening tools for attention-deficit/hyperactivity disorder in children and adolescents</article-title>. <source>J Am Acad Child Adolesc Psychiatry</source> <volume>61</volume>:<fpage>982</fpage>–<lpage>996</lpage>.</mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Murray</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Booth</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ribeaud</surname> <given-names>D</given-names></string-name>, <string-name><surname>Eisner</surname> <given-names>M</given-names></string-name></person-group> (<year>2018</year>) <article-title>Disagreeing about development: an analysis of parent-teacher agreement in adhd symptom trajectories across the elementary school years</article-title>. <source>Int J Methods Psychiatr Res</source> <volume>27</volume>:<fpage>e1723</fpage>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Näätänen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Picton</surname> <given-names>T</given-names></string-name></person-group> (<year>1987</year>) <article-title>The n1 wave of the human electric and magnetic response to sound: a review and an analysis of the component structure</article-title>. <source>Psychophysiology</source> <volume>24</volume>:<fpage>375</fpage>–<lpage>425</lpage>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Narad</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Garner</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Peugh</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Tamm</surname> <given-names>L</given-names></string-name>, <string-name><surname>Antonini</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Kingery</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JO</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>JN</given-names></string-name></person-group> (<year>2015</year>) <article-title>Parent-teacher agreement on adhd symptoms across development</article-title>. <source>Psychol Assess</source> <volume>27</volume>:<fpage>239</fpage>–<lpage>248</lpage>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neguț</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jurma</surname> <given-names>AM</given-names></string-name>, <string-name><surname>David</surname> <given-names>D</given-names></string-name></person-group> (<year>2017</year>) <article-title>Virtual-reality-based attention assessment of adhd: clinicavr: classroom-cpt versus a traditional continuous performance test</article-title>. <source>Child Neuropsychology</source> <volume>23</volume>:<fpage>692</fpage>–<lpage>712</lpage>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nissens</surname> <given-names>T</given-names></string-name>, <string-name><surname>Failing</surname> <given-names>M</given-names></string-name>, <string-name><surname>Theeuwes</surname> <given-names>J</given-names></string-name></person-group> (<year>2016</year>) <article-title>People look at the object they fear: oculomotor capture by stimuli that signal threat</article-title>. <source>Cogn Emot</source> <pub-id pub-id-type="doi">10.1080/0269993120161248905</pub-id> <volume>31</volume>:<fpage>1707</fpage>–<lpage>1714</lpage>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Palva</surname> <given-names>S</given-names></string-name>, <string-name><surname>Palva</surname> <given-names>JM</given-names></string-name></person-group> (<year>2007</year>) <article-title>New vistas for α-frequency band oscillations</article-title>. <source>Trends Neurosci</source> <volume>30</volume>:<fpage>150</fpage>–<lpage>158</lpage>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parkhurst</surname> <given-names>D</given-names></string-name>, <string-name><surname>Law</surname> <given-names>K</given-names></string-name>, <string-name><surname>Niebur</surname> <given-names>E</given-names></string-name></person-group> (<year>2002</year>) <article-title>Modeling the role of salience in the allocation of overt visual attention</article-title>. <source>Vision Res</source> <volume>42</volume>:<fpage>107</fpage>–<lpage>123</lpage>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parsons</surname> <given-names>TD</given-names></string-name></person-group> (<year>2015</year>) <article-title>Virtual reality for enhanced ecological validity and experimental control in the clinical, affective and social neurosciences</article-title>. <source>Front Hum Neurosci</source> <volume>9</volume>:<fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parsons</surname> <given-names>TD</given-names></string-name>, <string-name><surname>Duffield</surname> <given-names>T</given-names></string-name>, <string-name><surname>Asbee</surname> <given-names>J</given-names></string-name></person-group> (<year>2019</year>) <article-title>A comparison of virtual reality classroom continuous performance tests to traditional continuous performance tests in delineating adhd: a meta-analysis</article-title>. <source>Neuropsychol Rev</source> <volume>29</volume>:<fpage>338</fpage>–<lpage>356</lpage>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peisch</surname> <given-names>V</given-names></string-name>, <string-name><surname>Rutter</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wilkinson</surname> <given-names>CL</given-names></string-name>, <string-name><surname>Arnett</surname> <given-names>AB</given-names></string-name></person-group> (<year>2021</year>) <article-title>Sensory processing and p300 event-related potential correlates of stimulant response in children with attention-deficit/hyperactivity disorder: a critical review</article-title>. <source>Clinical Neurophysiology</source> <volume>132</volume>:<fpage>953</fpage>–<lpage>966</lpage>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petersen</surname> <given-names>EB</given-names></string-name>, <string-name><surname>Wöstmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name>, <string-name><surname>Lunner</surname> <given-names>T</given-names></string-name></person-group> (<year>2017</year>) <article-title>Neural tracking of attended versus ignored speech is differentially affected by hearing loss</article-title>. <source>J Neurophysiol</source> <volume>117</volume>:<fpage>18</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pollak</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>PL</given-names></string-name>, <string-name><surname>Rizzo</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Weizer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Shriki</surname> <given-names>L</given-names></string-name>, <string-name><surname>Shalev</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Gross-Tsur</surname> <given-names>V</given-names></string-name></person-group> (<year>2009</year>) <article-title>The utility of a continuous performance test embedded in virtual reality in measuring adhd-related deficits</article-title>. <source>J Dev Behav Pediatr</source> <volume>30</volume>:<fpage>2</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Posner</surname> <given-names>MI</given-names></string-name></person-group> (<year>1980</year>) <article-title>Orienting of attention</article-title>. <source>Q J Exp Psychol</source> <volume>32</volume>:<fpage>3</fpage>–<lpage>25</lpage>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rabiner</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Murray</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hardy</surname> <given-names>K</given-names></string-name>, <string-name><surname>Skinner</surname> <given-names>A</given-names></string-name>, <string-name><surname>Underwood</surname> <given-names>M</given-names></string-name></person-group> (<year>2010</year>) <article-title>Instability in teacher ratings of children’s inattentive symptoms</article-title>. <source>Journal of Developmental &amp; Behavioral Pediatrics</source> <volume>31</volume>:<fpage>175</fpage>–<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rad</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Martingano</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Ginges</surname> <given-names>J</given-names></string-name></person-group> (<year>2018</year>) <article-title>Toward a psychology of homo sapiens: making psychological science more representative of the human population</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>115</volume>:<fpage>11401</fpage>–<lpage>11405</lpage>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Risko</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>N</given-names></string-name>, <string-name><surname>Sarwal</surname> <given-names>A</given-names></string-name>, <string-name><surname>Engelhardt</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group> (<year>2012</year>) <article-title>Everyday attention: variation in mind wandering and memory in a lecture</article-title>. <source>Appl Cogn Psychol</source> <volume>26</volume>:<fpage>234</fpage>–<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c90"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Risko</surname> <given-names>EF</given-names></string-name>, <string-name><surname>Kingstone</surname> <given-names>A</given-names></string-name></person-group> (<year>2011</year>) <article-title>Eyes wide shut: implied social presence, eye tracking and attention</article-title>. <source>Atten Percept Psychophys</source> <volume>73</volume>:<fpage>291</fpage>–<lpage>296</lpage>.</mixed-citation></ref>
<ref id="c91"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rizzo</surname> <given-names>ASA</given-names></string-name>, <string-name><surname>Bowerly</surname> <given-names>T</given-names></string-name>, <string-name><surname>Buckwalter</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Klimchuk</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mitura</surname> <given-names>R</given-names></string-name>, <string-name><surname>Parsons</surname> <given-names>TD</given-names></string-name>, <string-name><surname>Bowerly</surname> <given-names>T</given-names></string-name>, <string-name><surname>Buckwalter</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Rizzo</surname> <given-names>ASA</given-names></string-name>, <string-name><surname>Adams</surname> <given-names>R</given-names></string-name>, <string-name><surname>Finn</surname> <given-names>P</given-names></string-name>, <string-name><surname>Moes</surname> <given-names>E</given-names></string-name>, <string-name><surname>Flannery</surname> <given-names>K</given-names></string-name>, <string-name><surname>Rizzo</surname> <given-names>ASA</given-names></string-name></person-group> (<year>2007</year>) <article-title>A virtual reality scenario for all seasons: the virtual classroom</article-title>. <source>Child Neuropsychol</source> <volume>13</volume>:<fpage>35</fpage>–<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c92"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>SanMiguel</surname> <given-names>I</given-names></string-name>, <string-name><surname>Morgan</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>C</given-names></string-name>, <string-name><surname>Linden</surname> <given-names>D</given-names></string-name>, <string-name><surname>Escera</surname> <given-names>C</given-names></string-name></person-group> (<year>2010</year>) <article-title>On the functional significance of novelty-p3: facilitation by unexpected novel sounds</article-title>. <source>Biol Psychol</source> <volume>83</volume>:<fpage>143</fpage>–<lpage>152</lpage>.</mixed-citation></ref>
<ref id="c93"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schomaker</surname> <given-names>J</given-names></string-name>, <string-name><surname>Walper</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wittmann</surname> <given-names>BC</given-names></string-name>, <string-name><surname>Einhäuser</surname> <given-names>W</given-names></string-name></person-group> (<year>2017</year>) <article-title>Attention in natural scenes: affective-motivational factors guide gaze independently of visual salience</article-title>. <source>Vision Res</source> <volume>133</volume>:<fpage>161</fpage>–<lpage>175</lpage>.</mixed-citation></ref>
<ref id="c94"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwartze</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kotz</surname> <given-names>SA</given-names></string-name></person-group> (<year>2015</year>) <article-title>The timing of regular sequences: production, perception, and covariation</article-title>. <source>J Cogn Neurosci</source> <volume>27</volume>:<fpage>1697</fpage>–<lpage>1707</lpage>.</mixed-citation></ref>
<ref id="c95"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schweitzer</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Rizzo</surname> <given-names>A</given-names></string-name></person-group> (<year>2022</year>) <article-title>Virtual reality and adhd: clinical assessment and treatment in the metaverse</article-title>. <source>The ADHD Report</source> <pub-id pub-id-type="doi">10.1521/adhd20223031</pub-id> <volume>30</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c96"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schweitzer</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Zion Golumbic</surname> <given-names>E</given-names></string-name></person-group> (<year>2023</year>) <article-title>Editorial: the use of continuous performance tasks in attention-deficit/hyperactivity disorder diagnosis: a cautionary note</article-title>. <source>J Am Acad Child Adolesc Psychiatry</source> <volume>63</volume>.</mixed-citation></ref>
<ref id="c97"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seesjärvi</surname> <given-names>E</given-names></string-name>, <string-name><surname>Puhakka</surname> <given-names>J</given-names></string-name>, <string-name><surname>Aronen</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Lipsanen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mannerkoski</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hering</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zuber</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kliegel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Laine</surname> <given-names>M</given-names></string-name>, <string-name><surname>Salmi</surname> <given-names>J</given-names></string-name></person-group> (<year>2022</year>) <article-title>Quantifying adhd symptoms in open-ended everyday life contexts with a new virtual reality task</article-title>. <source>J Atten Disord</source> <volume>26</volume>:<fpage>1394</fpage>–<lpage>1411</lpage>.</mixed-citation></ref>
<ref id="c98"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Selaskowski</surname> <given-names>B</given-names></string-name>, <string-name><surname>Asché</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Wiebe</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kannen</surname> <given-names>K</given-names></string-name>, <string-name><surname>Aslan</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gerding</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Sanchez</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ettinger</surname> <given-names>U</given-names></string-name>, <string-name><surname>Kölle</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lux</surname> <given-names>S</given-names></string-name>, <string-name><surname>Philipsen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Braun</surname> <given-names>N</given-names></string-name></person-group> (<year>2023</year>) <article-title>Gaze-based attention refocusing training in virtual reality for adult attention-deficit/hyperactivity disorder</article-title>. <source>BMC Psychiatry</source> <volume>23</volume>:<fpage>74</fpage>.</mixed-citation></ref>
<ref id="c99"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sergeant</surname> <given-names>J</given-names></string-name></person-group> (<year>2000</year>) <chapter-title>The cognitive-energetic model: an empirical approach to attention-deficit hyperactivity disorder</chapter-title>. In: <source>Neuroscience and Biobehavioral Reviews</source>. pp <fpage>7</fpage>–<lpage>12</lpage> <publisher-name>Pergamon</publisher-name>.</mixed-citation></ref>
<ref id="c100"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smallwood</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fishman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Schooler</surname> <given-names>JW</given-names></string-name></person-group> (<year>2007</year>) <article-title>Counting the cost of an absent mind: mind wandering as an underrecognized influence on educational performance</article-title>. <source>Psychon Bull Rev</source> <volume>14</volume>:<fpage>230</fpage>–<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c101"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sonuga-Barke</surname> <given-names>EJS</given-names></string-name>, <string-name><surname>Becker</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Bölte</surname> <given-names>S</given-names></string-name>, <string-name><surname>Castellanos</surname> <given-names>FX</given-names></string-name>, <string-name><surname>Franke</surname> <given-names>B</given-names></string-name>, <string-name><surname>Newcorn</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Nigg</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Rohde</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Simonoff</surname> <given-names>E</given-names></string-name></person-group> (<year>2023</year>) <article-title>Annual research review: perspectives on progress in adhd science – from characterization to cause</article-title>. <source>Journal of Child Psychology and Psychiatry</source> <volume>64</volume>:<fpage>506</fpage>–<lpage>532</lpage>.</mixed-citation></ref>
<ref id="c102"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Rizzo</surname> <given-names>A</given-names></string-name>, <string-name><surname>Geng</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Schweitzer</surname> <given-names>JB</given-names></string-name></person-group> (<year>2022</year>) <article-title>Measuring attentional distraction in children with adhd using virtual reality technology with eye-tracking</article-title>. <source>Front Virtual Real</source> <volume>3</volume>.</mixed-citation></ref>
<ref id="c103"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szpunar</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Moulton</surname> <given-names>ST</given-names></string-name>, <string-name><surname>Schacter</surname> <given-names>DL</given-names></string-name></person-group> (<year>2013</year>) <article-title>Mind wandering and education: from the classroom to online learning</article-title>. <source>Front Psychol</source> <volume>4</volume>:<fpage>495</fpage>.</mixed-citation></ref>
<ref id="c104"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>’t Hart</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Vockeroth</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schumann</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bartl</surname> <given-names>K</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>E</given-names></string-name>, <string-name><surname>König</surname> <given-names>P</given-names></string-name>, <string-name><surname>Einhäuser</surname> <given-names>W</given-names></string-name></person-group> (<year>2009</year>) <article-title>Gaze allocation in natural stimuli: comparing free exploration to head-fixed viewing conditions</article-title>. <source>Vis cogn</source> <volume>17</volume>:<fpage>1132</fpage>–<lpage>1158</lpage>.</mixed-citation></ref>
<ref id="c105"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thomson</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Besner</surname> <given-names>D</given-names></string-name>, <string-name><surname>Smilek</surname> <given-names>D</given-names></string-name></person-group> (<year>2015</year>) <article-title>A resource-control account of sustained attention</article-title>. <source>Perspectives on Psychological Science</source> <volume>10</volume>:<fpage>82</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="c106"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Toplak</surname> <given-names>ME</given-names></string-name>, <string-name><surname>West</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Stanovich</surname> <given-names>KE</given-names></string-name></person-group> (<year>2013</year>) <article-title>Practitioner review: do performance-based measures and ratings of executive function assess the same construct?</article-title> <source>Journal of Child Psychology and Psychiatry</source> <volume>54</volume>:<fpage>131</fpage>–<lpage>143</lpage>.</mixed-citation></ref>
<ref id="c107"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Türkan</surname> <given-names>BN</given-names></string-name>, <string-name><surname>Amado</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ercan</surname> <given-names>ES</given-names></string-name>, <string-name><surname>Perçinel</surname> <given-names>I</given-names></string-name></person-group> (<year>2016</year>) <article-title>Comparison of change detection performance and visual search patterns among children with/without adhd: evidence from eye movements</article-title>. <source>Res Dev Disabil</source> <volume>49–50</volume>:<fpage>205</fpage>–<lpage>215</lpage>.</mixed-citation></ref>
<ref id="c108"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turoman</surname> <given-names>N</given-names></string-name>, <string-name><surname>Vergauwe</surname> <given-names>E</given-names></string-name></person-group> (<year>2023</year>) <article-title>The effect of multisensory distraction on working memory: a role for task relevance?</article-title> <source>J Exp Psychol Learn Mem Cogn</source>.</mixed-citation></ref>
<ref id="c109"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vakil</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mass</surname> <given-names>M</given-names></string-name>, <string-name><surname>Schiff</surname> <given-names>R</given-names></string-name></person-group> (<year>2019</year>) <article-title>Eye movement performance on the stroop test in adults with adhd</article-title>. <source>J Atten Disord</source> <volume>23</volume>:<fpage>1160</fpage>–<lpage>1169</lpage>.</mixed-citation></ref>
<ref id="c110"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vanthornhout</surname> <given-names>J</given-names></string-name>, <string-name><surname>Decruy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Francart</surname> <given-names>T</given-names></string-name></person-group> (<year>2019</year>) <article-title>Effect of task and attention on neural tracking of speech</article-title>. <source>Front Neurosci</source> <volume>13</volume>.</mixed-citation></ref>
<ref id="c111"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wetzel</surname> <given-names>N</given-names></string-name>, <string-name><surname>Schröger</surname> <given-names>E</given-names></string-name>, <string-name><surname>Widmann</surname> <given-names>A</given-names></string-name></person-group> (<year>2013</year>) <article-title>The dissociation between the p3a event-related potential and behavioral distraction</article-title>. <source>Psychophysiology</source> <volume>50</volume>:<fpage>920</fpage>–<lpage>930</lpage>.</mixed-citation></ref>
<ref id="c112"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woldorff</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Hillyard</surname> <given-names>SA</given-names></string-name></person-group> (<year>1991</year>) <article-title>Modulation of early auditory processing during selective listening to rapidly presented tones</article-title>. <source>Electroencephalogr Clin Neurophysiol</source> <volume>79</volume>:<fpage>170</fpage>–<lpage>191</lpage>.</mixed-citation></ref>
<ref id="c113"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wöstmann</surname> <given-names>M</given-names></string-name>, <string-name><surname>Lim</surname> <given-names>S-J</given-names></string-name>, <string-name><surname>Obleser</surname> <given-names>J</given-names></string-name></person-group> (<year>2017</year>) <article-title>The human neural alpha response to speech is a proxy of attentional control</article-title>. <source>Cerebral Cortex</source> <volume>27</volume>:<fpage>3307</fpage>–<lpage>3317</lpage>.</mixed-citation></ref>
<ref id="c114"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zimmer</surname> <given-names>H</given-names></string-name>, <string-name><surname>Richter</surname> <given-names>F</given-names></string-name></person-group> (<year>2023</year>) <article-title>Novelty detection and orienting: effects on skin conductance and heart rate</article-title>. <source>Psychol Res</source> <volume>87</volume>:<fpage>1101</fpage>–<lpage>1113</lpage>.</mixed-citation></ref>
<ref id="c115"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zion Golumbic</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Bickel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lakatos</surname> <given-names>P</given-names></string-name>, <string-name><surname>Schevon</surname> <given-names>CA</given-names></string-name>, <string-name><surname>McKhann</surname> <given-names>G</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>RR</given-names></string-name>, <string-name><surname>Emerson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Mehta</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name>, <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Schroeder</surname> <given-names>CE</given-names></string-name></person-group> (<year>2013</year>) <article-title>Mechanisms underlying selective neuronal tracking of attended speech at a cocktail party</article-title>. <source>Neuron</source> <volume>77</volume>:<fpage>980</fpage>–<lpage>991</lpage>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103235.1.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Ding</surname>
<given-names>Nai</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
<city>Hangzhou</city>
<country>China</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study investigates how AD(H)D affects attention using neural and physiological measures in a Virtual Reality (VR) environment. <bold>Solid</bold> evidence is provided that individuals diagnosed with AD(H)D differ from control participants in both the encoding of the target sound and the encoding of acoustic interference. The VR paradigm here can potentially bridge lab experiments and real-life experiments. However, the reviewers identified a few potential technical issues that will need to be verified and discussed.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103235.1.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This is an interesting study on AD(H)D. The authors combine a variety of neural and physiological metrics to study attention in a VR classroom setting. The manuscript is well written and the results are interesting, ranging from an effect of group (AD(H)D vs. control) on metrics such as envelope tracking, to multivariate regression analyses considering alpha-power, gaze, TRF, ERPs, and behaviour simultaneously. I find the first part of the results clear and strong. The multivariate analyses in Tables 1 and 2 are good ideas, but I think they would benefit from additional clarification. Overall, I think that the methodological approach is useful in itself. The rest is interesting in that it informs us on which metrics are sensitive to group effects and correlated with each other. I think this might be one interesting way forward. Indeed, much more work is needed to clarify how these results change with different stimuli and tasks. So, I see this as an interesting first step into a more naturalistic measurement of speech attention.</p>
<p>Strengths:</p>
<p>I praise the authors for this interesting attempt to tackle a challenging topic with naturalistic experiments and metrics. I think the results broadly make sense and they contribute to a complex literature that is far from being linear and cohesive.</p>
<p>Weaknesses:</p>
<p>Nonetheless, I have a few comments that I hope will help the authors improve the manuscript. Some aspects should be clearer, some methodological steps were unclear (missing details on filters), and others were carried out in a way that doesn't convince me and might be problematic (e.g., re-filtering). I also suggested areas where the authors might find some improvements, such as deriving distinct markers for the overall envelope reconstruction and its change over time, which could solve some of the issues reported in the discussion (e.g., the lack of correlation with TRF metrics).</p>
<p>I also have some concerns regarding reproducibility. Many details are imprecise or missing. And I did not find any comments on data and code sharing. A clarification would be appreciated on that point for sure.</p>
<p>There are some minor issues, typically caused by some imprecisions in the write-up. There are a few issues that could change things though (e.g., re-filtering; the worrying regularisation optimisation choices), and there I'll have to see the authors' reply to determine whether those are major issues or not. Figures should also be improved (e.g., Figure 4B is missing the ticks).</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103235.1.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>While selective attention is a crucial ability of human beings, previous studies on selective attention are primarily conducted in a strictly controlled context, leaving a notable gap in underlying the complexity and dynamic nature of selective attention in a naturalistic context. This issue is particularly important for classroom learning in individuals with ADHD, as selecting the target and ignoring the distractions are pretty difficult for them but are the prerequisites of effective learning. The authors of this study have addressed this challenge using a well-motivated study. I believe the findings of this study will be a nice addition to the fields of both cognitive neuroscience and educational neuroscience.</p>
<p>Strengths:</p>
<p>To achieve the purpose of setting up a naturalistic context, the authors have based their study on a novel Virtual Reality platform. This is clever as it is usually difficult to perform such a study in a real classroom. Moreover, various techniques such as brain imaging, eye-tracking, and physiological measurement are combined to collect multi-level data. They found that, different from the controls, individuals with ADHD had higher neural responses to the irrelevant rather than the target sounds, and reduced speech tracking of the teacher. Additionally, the power of alpha-oscillations and frequency of gaze shifts away from the teacher are found to be associated with ADHD symptoms. These results provide new insights into the mechanism of selective attention among ADHD populations.</p>
<p>Weaknesses:</p>
<p>It is worth noting that nowadays there have been some studies trying to do so in the real classroom, and thus the authors should acknowledge the difference between the virtual and real classroom context and foresee the potential future changes.</p>
<p>The approach of combining multi-level data has the advantage of obtaining reliable results, but also raises significant difficulty for the readers to understand the main results.</p>
<p>An appraisal of whether the authors achieved their aims, and whether the results support their conclusions.</p>
<p>As expected, individuals with ADHD showed anomalous patterns of neural responses, and eye-tracking patterns, compared to the controls. But there are also some similarities between groups such as the amount of time paying attention to teachers, etc. In general, their conclusions are supported.</p>
<p>A discussion of the likely impact of the work on the field, and the utility of the methods and data to the community, would highlight the contributions of the work.</p>
<p>The findings are an extension of previous efforts in understanding selective attention in the naturalistic context. The findings of this study are particularly helpful in inspiring teacher's practice and advancing the research of educational neuroscience. This study demonstrates, again, that it is important to understand the complexity of cognitive processes in the naturalistic context.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103235.1.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors conducted a well-designed experiment, incorporating VR classroom scenes and background sound events, with both control and ADHD participants. They employed multiple neurophysiological measures, such as EEG, eye movements, and skin conductance, to investigate the mechanistic underpinnings of paying attention in class and the disruptive effects of background noise.</p>
<p>The results revealed that individuals with ADHD exhibited heightened sensory responses to irrelevant sounds and reduced tracking of the teacher's speech. Overall, this manuscript presented an ecologically valid paradigm for assessing neurophysiological responses in both control and ADHD groups. The analyses were comprehensive and clear, making the study potentially valuable for the application of detecting attentional deficits.</p>
<p>Strengths:</p>
<p>• The VR learning paradigm is well-designed and ecologically valid.</p>
<p>• The neurophysiological metrics and analyses are comprehensive, and two physiological markers are identified capable of diagnosing ADHD.</p>
<p>• This research provides a valuable dataset that could serve as a benchmark for future studies on attention deficits.</p>
<p>Weaknesses:</p>
<p>• Several results are null results, i.e., no significant differences were found between ADHD and control populations.</p>
<p>• Although the paradigm is well-designed and ecologically valid, the specific contributions or insights from the results remain unclear.</p>
<p>• Lack of information regarding code and data availability.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.103235.1.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Levy</surname>
<given-names>Orel</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Libman Hackmon</surname>
<given-names>Shirley</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zvilichovsky</surname>
<given-names>Yair</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Korisky</surname>
<given-names>Adi</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bidet-Caulet</surname>
<given-names>Aurelie</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schweitzer</surname>
<given-names>Julie B</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zion Golumbic</surname>
<given-names>Elana</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8831-3188</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>We are glad that the reviewers found our work to be interesting and appreciate its contribution to enhancing ecological validity of attention research. We also agree that much more work is needed to solidify this approach, and that some of the results should be considered “exploratory” at this point, but appreciate the recognition of the novelty and scientific potential of the approach introduced here.</p>
<p>We will address the reviewers’ specific comments in a revised version of the paper, and highlight the main points here:</p>
<p>· We agree that the use of multiple different neurophysiological measures is both an advantage and a disadvantage, and that the abundance of results can make it difficult to tell a “simple” story. In our revision, we will make an effort to clarify what (in our opinion) are the most important results and provide readers with a more cohesive narrative.</p>
<p>· Important additional discussion points raised by the reviewers, which will be discussed in a revised version are a) the similarities and differences between virtual and real classrooms; b) the utility of the methods and data to the community and c) the implication of these results for educational neuroscience and ADHD research.</p>
<p>· In the revision, we will also clarify several methodological aspects of the data analysis, as per the reviewers’ requests.</p>
<p>· After final publication, the data will be made available for other researchers to use.</p>
</body>
</sub-article>
</article>