<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">95837</article-id>
<article-id pub-id-type="doi">10.7554/eLife.95837</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95837.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.1</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Neural representations of predicted events: Evidence from time-resolved EEG decoding</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2340-3422</contrib-id>
<name>
<surname>Li</surname>
<given-names>Ai-Su</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5849-7721</contrib-id>
<name>
<surname>Theeuwes</surname>
<given-names>Jan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0491-1317</contrib-id>
<name>
<surname>van Moorselaar</surname>
<given-names>Dirk</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Institute Brain and Behavior Amsterdam, Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam</institution>, Amsterdam, <country>the Netherlands</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, Soochow University</institution>, Suzhou, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>William James Center for Research, ISPA-Instituto Universitario</institution>, Lisbon, <country>Portugal</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Marquand</surname>
<given-names>Andre F</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Radboud University Nijmegen</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label>Correspondence should be addressed to: Ai-Su Li (<email>aisuli.psy@gmail.com</email>), Department of Psychology, Soochow University, WenJing Street 1, 215000 Suzhou, China.; Dirk van Moorselaar (<email>dirkvanmoorselaar@gmail.com</email>), Department of Experimental and Applied Psychology, Vrije Universiteit Amsterdam, Van der Boechorststraat 7-9, 1081 BT Amsterdam, The Netherlands.</corresp>
<fn id="n1"><p>Conflict of Interest: The authors declare no competing financial interests</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-04-29">
<day>29</day>
<month>04</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP95837</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-01-22">
<day>22</day>
<month>01</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-01-05">
<day>05</day>
<month>01</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.01.05.574347"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Li et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Li et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-95837-v1.pdf"/>
<abstract>
<title>Abstract</title><p>Through statistical learning, humans are able to extract temporal regularities, using the past to predict the future. Evidence suggests that learning relational structures makes it possible to anticipate the imminent future; yet, the neural dynamics of predicting the future and its time-course remain elusive. To examine whether future representations are denoted in a temporally discounted fashion, we used the high-temporal-resolution of electroencephalography (EEG). Observers were exposed to a fixed sequence of events at four unique spatial positions within the display. Using multivariate pattern analyses trained on independent pattern estimators, we were able to decode the spatial position of dots within full sequences, and within randomly intermixed partial sequences wherein only a single dot was presented. Crucially, within these partial sequences, subsequent spatial positions could be reliably decoded at their expected moment in time. These findings highlight the dynamic weight changes within the assumed spatial priority map and mark the first implementation of EEG to decode predicted, yet critically omitted events.</p>
</abstract>
<abstract abstract-type="teaser"><title>Impact statement</title>
<p>Utilizing high-temporal-resolution EEG, the dynamic weight changes of assumed spatial priority map were visualized by decoding the spatial position of expected, yet omitted, events at their expected moment in time.</p></abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>An essential aspect of our capacity to perceive and interact with the visual environment lies in our ability to discern recurrent patterns from past experiences and utilize them to forecast future occurrences, thereby guiding our actions. Critically, our ever-evolving surroundings, filled with abundant information, maintain structure and stability. For instance, through extensive practice in specific ball sports, we acquire the skill to predict the ball’s trajectory, enabling us to anticipate its future position and precisely intercept it when the need arises. At the behavioral level, the benefits of such sequence learning is demonstrated through the serial reaction time (SRT) task pioneered by <xref ref-type="bibr" rid="c49">Nissen and Bullemer (1987)</xref>, wherein a single stimulus is sequentially presented at various locations (e.g., 4). The critical finding is that, even without any explicit awareness, observers can track the current position of the target stimulus (using spatially compatible keys) faster when the target follows a particular repetitive sequence compared to a random sequence. Subsequent experiments demonstrated that this learning could not be attributed to response-response or stimulus-response associations (see <xref ref-type="bibr" rid="c56">Schwarb &amp; Schumacher, 2012</xref> for a review) as learning persisted even when the motor component was removed (<xref ref-type="bibr" rid="c53">Remillard, 2003</xref>). Thus, unintentionally, human observers learn perceptual regularities, leading to more efficient processing of expected sensory events.</p>
<p>In more recent years, it has become clear that such perceptual learning is not restricted to sequentially presented individual shapes and associated responses, as observers can also learn spatial associations across trials during visual search (e.g., <xref ref-type="bibr" rid="c3">Boettcher et al., 2022</xref>; <xref ref-type="bibr" rid="c39">Li et al., 2022</xref>, <xref ref-type="bibr" rid="c40">2023</xref>; <xref ref-type="bibr" rid="c41">Li &amp; Theeuwes, 2020</xref>; Li et al., under review; <xref ref-type="bibr" rid="c60">Toh et al., 2021</xref>). For example, in <xref ref-type="bibr" rid="c41">Li and Theeuwes (2020)</xref>, the target appeared with equal probability across all display locations, yet critically, two across-trial regularities regarding target locations were introduced so that the target presented at a specific location (e.g., left side of the display) was always followed by a target at the opposite display location. While most participants were unable to explicitly report these across-trial regularities, performance on predictable trials was nevertheless facilitated (i.e., faster RTs and higher accuracies) compared to unpredictable trials. This type of learning was classified as a form of statistical learning — a process by which observers extract statistical regularities in the environment from past experiences (see <xref ref-type="bibr" rid="c20">Frost et al., 2019</xref>; <xref ref-type="bibr" rid="c59">Theeuwes et al., 2022</xref> for a review). When observers optimize their visual search performance according to the learned regularities that exist in the display, it is often claimed that through statistical learning the weights within an assumed spatial priority map change such that attention becomes biased towards locations where the target appears with a higher probability (e.g., <xref ref-type="bibr" rid="c16">Fecteau &amp; Munoz, 2006</xref>; <xref ref-type="bibr" rid="c17">Ferrante et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Theeuwes et al., 2022</xref>; <xref ref-type="bibr" rid="c69">Zelinsky &amp; Bisley, 2015</xref>). Notably, in across-trial statistical learning the regularity is occurring across trials suggesting that local weight changes within the assumed priority map dynamically adapt on the basis of the trial-to-trial probabilities.</p>
<p>In this perspective the attentional priority landscape is not inherently a static representation constructed by previous selection episodes, but instead it operates as a dynamic system, wherein attentional priority undergoes continual adjustments based on anticipations triggered by the current event. This dynamic view on attentional priority, wherein the current state predicts the upcoming (future) states, is reminiscent of the successor representation framework (SR, <xref ref-type="bibr" rid="c8">Dayan, 1993</xref>). This framework postulates a predictive representation wherein the current state is represented in terms of its future (successor) states, in a temporally discounted fashion. While the SR framework was originally conceptualized in the context of hippocampal representations (e.g., <xref ref-type="bibr" rid="c8">Dayan, 1993</xref>; <xref ref-type="bibr" rid="c15">Fang et al., 2023</xref>; <xref ref-type="bibr" rid="c57">Stachenfeld et al., 2017</xref>), a recent functional magnetic resonance imaging (fMRI) study by <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref> showed that after visual sequence learning, not only the hippocampus, but also V1 receptive fields became tuned to respond to both the current input, as well as expected future events. The fact that reactivations of future sequence locations were also observed in early visual cortex suggests that the successor representations are a more ubiquitous coding schema than previously assumed.</p>
<p>The evidence above highlights that the relational structure, for instance that event A is usually followed by event B, that is learned through exposure during past experiences is not passively represented, but instead is actively used to anticipate the imminent future. Consistent with this, previous research has repeatedly shown that prior expectations influence neural activity in the visual cortex (e.g., <xref ref-type="bibr" rid="c19">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="c36">Kok et al., 2013</xref>; <xref ref-type="bibr" rid="c37">Kok et al., 2012</xref>). Yet, in the context of learned spatial associations the time course of these effects remains largely unexplored, and it thus also remains unclear how and when the current representation shapes future representations in the context of spatial associations. As <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref> employed fMRI as a way to measure brain activity, the temporal dynamics of the observed reactivations remain elusive. Therefore, in the current study, we relied on the high temporal resolution of electrophysiology (EEG) to investigate the temporal dynamics of anticipating future events. Previous research has demonstrated that the expected neural representation induced by probability cues on a trial-by-trial basis is already activated prior to actual stimulus onset (<xref ref-type="bibr" rid="c38">Kok et al., 2017</xref>). Here, we specifically set out to test whether stimulus presentation within a structured sequence also serves as a cue that triggers the retrieval of successor locations.</p>
<p>To test this, we adopted the sequence learning task of <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref>. Participants were exposed to an arbitrary spatiotemporal sequence of dots (see <xref rid="fig1" ref-type="fig">Figure 1</xref>) consisting of four items (A-B-C-D). Critically, after a training phase with repeated exposure to the sequence, we randomly intermixed full sequences with partial sequences, wherein only a single stimulus of the sequence appeared on screen at its expected moment in time (e.g., - B - -). Using time-resolved multivariate pattern analyses, we were able to test whether merely presenting one of the items in a learned sequence would trigger activation at the successor locations (e.g., C and D) and critically for the current study, if so, at what moment in time.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Stimuli and design in sequence blocks and pattern estimator blocks. (a) Illustration of sequence blocks containing full and partial sequence trials. In full sequence trials the white dot was presented for 100 ms at one of four positions successively in a fixed order with an inter-stimulus interval (ISI) of 200 ms. During partial sequence trials only one dot of the sequence (equal probability for each position in the sequence) was presented for 100 ms and all intervals remained the same as full sequence trials. (b) Illustration of target (i.e., gray dot detection) frame. (c) Graphical illustration of the eight possible dot locations, out of which only four were shown in full and partial sequence trials. (d) During random pattern estimator blocks, the white dot was presented at one of eight possible locations (as illustrated in <xref rid="fig1" ref-type="fig">Figure 1c</xref>) for 100 ms in random order with an ISI of 600 ms in long-ISI blocks and of 100 ms in short-ISI blocks.</p></caption>
<graphic xlink:href="574347v1_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<p>While recording EEG and gaze behavior, participants (<italic>N</italic> = 25) had to detect each time a white dot turned grey within a fixed four-position dot sequence (i.e., ABCD; <xref rid="fig1" ref-type="fig">Figure 1a/b</xref>). Unbeknownst to participants these rare events only occurred at the final sequence position. Critically, after an initial exposure phase to familiarize participants with the sequence order, partial sequences, wherein only a single stimulus of the sequence was presented at its expected moment in time (e.g., -B--), were randomly intermixed into these full sequences (<xref rid="fig1" ref-type="fig">Figure 1a</xref>; see Methods section for details). This design allowed us to establish whether, and critically if so when, presenting a single item of a learned sequence would trigger the retrieval of successor locations within that sequence (<xref ref-type="bibr" rid="c13">Ekman et al., 2023</xref>).</p>
<sec id="s2a">
<title>Within-block decoding: Increasing the interval between successive stimuli enhances performance</title>
<p>Before delving into the neural representations of specific positions within the sequence, including both present and absent dots, we initially ensured the robustness of decoding individual dot positions using data from independent pattern estimator blocks. Within these blocks participants were instructed to passively view a dot stimulus appearing at one of eight locations in random order (<xref rid="fig1" ref-type="fig">Figure 1c</xref>). Previous studies have successfully employed multivariate pattern analyses based on independent pattern estimators with either short (range: 0-200 ms; e.g., <xref ref-type="bibr" rid="c1">Alilovic et al., 2021</xref>; <xref ref-type="bibr" rid="c2">Blom et al., 2020</xref>; <xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>) or long (range: 433-1200 ms; e.g., <xref ref-type="bibr" rid="c30">Hogendoorn &amp; Burkitt, 2018</xref>; <xref ref-type="bibr" rid="c38">Kok et al., 2017</xref>) ISI’s in between successive stimuli. Although these approaches have their own benefits (see Methods for details), here for the first time we attempted to decode the spatial position of expected dots in the absence of visual input. Given that this approach is arguably more challenging due to the event of interest not being time-locked to an evoked response, we chose to employ both types of pattern estimators to establish whether one type is more suited than the other. As visualized in <xref rid="fig2" ref-type="fig">Figure 2a</xref>, we observed that classifiers trained on data from both pattern estimators could decode individual dot positions well above chance level. However, noteworthy distinctions emerged between the two distinct pattern estimator conditions. The long-ISI pattern estimator decoding demonstrated superior overall classification performance, with subsequent peaks in decoding occurring at later time points (the short-ISI pattern estimator displayed an initial peak at approximately 88 ms, while the long-ISI pattern estimator showed an initial peak at around 135 ms). Previous research has established that multiple stimuli within a sequence can be simultaneously represented, with each individual stimulus being decodable for approximately one second (<xref ref-type="bibr" rid="c4">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="c35">King &amp; Wyart, 2021</xref>). Arguably, the signal contamination stemming from preceding dots may also account for the observed differences between independent pattern estimators, with the sustained influence from preceding dots being more pronounced in the short-ISI pattern estimator blocks, where subsequent stimuli are temporally closer together compared to the long-ISI pattern estimator blocks. Critically, control analysis convincingly demonstrated that this above-chance classification could not be explained by systematic eye movements as the same analysis using the x, y coordinates measured by the eye tracker did not result in above-chance classification (e.g., <xref ref-type="bibr" rid="c5">Chota et al., 2023</xref>; <xref ref-type="bibr" rid="c33">Johnson et al., 2023</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Pattern estimator position (four dots from the sequence) decoding results. The decoder was trained and tested at the same time point with posterior electrodes (a), or gaze positions (b) as features. Horizontal dashed lines indicate chance level while vertical dashed lines indicate dot onset. Green and red solid lines represent long- and short-ISI pattern estimators, respectively. Shaded region represents standard error of the mean (SEM, same applied to subsequent figures). Corresponding color bars below the x-axis indicate clusters that significantly differed from chance while the black bar indicates clusters with a significant difference between long- and short-ISI pattern estimators (<italic>p</italic> &lt; .05). (c) Temporal generalization matrix, with posterior electrodes as features, showing classification performance as a function of training and test time point in long- and short-ISI blocks. Solid black lines indicate clusters that significantly different from chance (<italic>p</italic> &lt; 0.05). All corresponding results that discriminated eight positions can be observed in <xref rid="fig2" ref-type="fig">Figure 2</xref> – figure supplement 1.</p></caption>
<graphic xlink:href="574347v1_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In order to determine whether an individual event automatically triggers the retrieval of successive events and pinpoint the timing of this process, our study aimed to decode the spatial position of absent, yet anticipated dots. As noted, pattern estimator decoding displayed two successive peaks, and it was not immediately clear what cognitive processes these peaks represented, and which would be most applicable for position decoding within structured sequences. To gain a better understanding of the dynamics of position decoding, we therefore employed a generalization across time approach while applying the pattern classifiers (<xref ref-type="bibr" rid="c34">King &amp; Dehaene, 2014</xref>), wherein a classifier trained at a specific time point is tested on all time points. The resulting generalization across time matrix (with training time on y-axis; testing time on x-axis) can help identify periods during which a representation remains stable, i.e., generalizes across time. As depicted in <xref rid="fig2" ref-type="fig">Figure 2c</xref>, both pattern estimators exhibited little to no evidence of early off-diagonal decoding, which has been associated with the perceptual maintenance of early sensory representations (e.g., <xref ref-type="bibr" rid="c46">Marti &amp; Dehaene, 2017</xref>; <xref ref-type="bibr" rid="c48">Meijs et al., 2019</xref>; <xref ref-type="bibr" rid="c66">Weaver et al., 2019</xref>). Furthermore, in line with <xref ref-type="bibr" rid="c30">Hogendoorn and Burkitt (2018)</xref>, the temporal generalization matrix revealed no above-chance classification performance at the intersections of the distinct peaks in diagonal decoding, suggesting that these peaks represent sequential, distinct patterns of neural activation (<xref ref-type="bibr" rid="c34">King &amp; Dehaene, 2014</xref>).</p>
<p>Considering that our classifier likely tracked the dynamic transformation from representation to another over time, and we had no a priori prediction regarding the optimal moment to capture the processing of an anticipated, yet critically missing event, we decided not to restrain our cross-block classification (i.e., pattern estimator to structured sequence data classification) to a specific time window. Instead, we explored the entire time range using only the data from the long-ISI pattern estimator, as that one demonstrated superior decoding.</p>
</sec>
<sec id="s2b">
<title>Cross-block decoding: predecessor events in a sequence enhance decoding of the current event</title>
<p>Before examining the neural representation of anticipated, yet absent events, we first tested whether the data from the pattern estimator sequences could be used to decode present dot positions within the structured sequence. For this purpose, time courses of the pattern estimator blocks and the main task were aligned relative to dot onset and decoding was performed across time, with the model trained on pattern estimator data and tested on main task data, separate for full sequences (i.e., BCD) and partial sequences (e.g., -B--). Note that in this analysis, the first dot position (i.e., A), which does not allow to dissociate full from partial sequences, was not included. As shown in <xref rid="fig3" ref-type="fig">Figure 3a</xref>, cross-block classification also showed robust decoding of dot position, irrespective of whether the dot was embedded in a full sequence, or whether that dot was the only dot presented within the sequence (partial sequences). Notably, while clusters of significant decoding exhibited the same temporal dynamics, decoding was more pronounced approximately 200 ms following dot onset in the full sequence trials relative to partial sequence trials. Although speculative, we attribute this difference in decoding between two identical events to a more variable tuning of attention in the partial sequence relative to the full sequences. Whereas in the latter attentional shifts are yoked to a sequence of events that are fixed in time, attentional tuning arguably became more variable when dots were omitted from the sequence, and participants had to thus rely on internally generated representations. Consistent with this an exploratory analysis of the event-related waveforms elicited by posterior contralateral electrodes showed that especially the N1 component, which is linked to the orienting of attention to a task-relevant stimulus and modulated by the time in between successive events (<xref ref-type="bibr" rid="c44">Luck et al., 1990</xref>) was more pronounced in full than in partial sequences (<xref rid="fig3" ref-type="fig">Figure 3b</xref>). We then believe that this N1 modulation underlies our observed difference in decoding.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Position decoding and ERPs results for present dots. (a) Decoding of present dot positions (i.e., BCD) based on long-ISI pattern estimator (training and testing at the same time point) using posterior electrodes. (b) ERPs for the 8 posterior electrodes contralateral to each present dot position, collapsed over dots. The pink and blue lines represent full and partial sequence trials, respectively. Corresponding color bars below the x-axis indicate clusters that significantly differed from chance while black bars indicate clusters with a significant difference between full and partial sequence trials (<italic>p</italic> &lt; .05).</p></caption>
<graphic xlink:href="574347v1_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Cross-block decoding: Expected, but absent (immediate) successor events are represented at their expected moment in time</title>
<p>Having established that we could robustly decode dot positions within the structured sequences using independent pattern estimator data, we next examined whether those same spatial positions could be decoded when the anticipated dots were omitted from the sequence. For this purpose, following <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref>, we categorized omitted dots within partial sequences into two conditions: ‘predecessor’ and ‘successor’, based on their position in relation to the presented dot. For instance, in the partial sequence, -B--, A would be categorized as a predecessor position, while dots C and D would be designated as successor positions. As visualized in <xref rid="fig4" ref-type="fig">Figure 4</xref>, whereas decoding performance for predecessor locations was at chance level, despite the absence of evoked response classifiers were able to reliably classify successor locations, with above chance decoding emerging approximately around the time that the stimulus should have appeared. Counter to decoding of present dots however, which was largely restricted to the diagonal of temporal generalization matrix (<xref rid="fig4" ref-type="fig">Figure 4b</xref>), successor decoding was more diffuse (<xref rid="fig4" ref-type="fig">Figure 4c</xref>), arguably because the temporal predictions were not reset by actual stimulus presentation and hence somewhat variable. Next, we explored whether the observed successor decoding was observed independent of the temporal distance from the presented dot. As shown in <xref rid="fig5" ref-type="fig">Figure 5</xref>, whereas the successor representation immediately following the presented dot could be decoded reliably, there was little evidence that the subsequent successor representation was also evident in the ongoing EEG signal. Note that these results do not necessarily mean that only the immediate successor was actively represented, as EEG might simply not be sensitive enough to pick up on these weak and temporal diffuse representations. Indeed, the fMRI study by <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref> showed that while all successor representations were represented, activations decreased with increasing temporal distance from the present dot. Nevertheless, the current results convincingly demonstrate that anticipated events are actively represented at their expected moments in time, even without an evoked response elicited by that event.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Position decoding results of partial sequence trials. Temporal generalization of decoding absent predecessor (a), present (b), and absent successor (c) positions of partial sequence trials based on long-ISI pattern estimator using posterior electrodes. Solid black lines indicate clusters that significantly different from chance (<italic>p</italic> &lt; 0.05).</p></caption>
<graphic xlink:href="574347v1_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Successive position decoding results of partial sequence trials. Temporal generalization of decoding the first successor (a) and second successor (b) positions based on long-ISI pattern estimator using posterior electrodes. Solid black lines indicate clusters that significantly different from chance (<italic>p</italic> &lt; 0.05).</p></caption>
<graphic xlink:href="574347v1_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Recently, a large body of research has shown that people are able to learn from past experiences and apply what is learned to optimize selection (e.g., <xref ref-type="bibr" rid="c11">Duncan et al., 2023</xref>; <xref ref-type="bibr" rid="c17">Ferrante et al., 2018</xref>; <xref ref-type="bibr" rid="c21">Gao &amp; Theeuwes, 2020</xref>; <xref ref-type="bibr" rid="c32">Huang et al., 2022</xref>; <xref ref-type="bibr" rid="c43">Liesefeld &amp; Müller, 2021</xref>; <xref ref-type="bibr" rid="c64">van Moorselaar &amp; Theeuwes, 2023</xref>; <xref ref-type="bibr" rid="c65">Wang &amp; Theeuwes, 2018</xref>; <xref ref-type="bibr" rid="c68">Xu et al., 2022</xref>). It was assumed that through learning the weights within the assumed spatial priority map are up- and down regulated optimizing attentional selection. We have argued that in conditions in which one event predicts an upcoming event the weights within this spatial priority map need to be dynamically (from trial to trial) adjusted (<xref ref-type="bibr" rid="c39">Li et al., 2022</xref>; <xref ref-type="bibr" rid="c41">Li &amp; Theeuwes, 2020</xref>). The current study provides direct evidence for these dynamic weight changes within the priority map as the presentation of a particular stimulus generated neural patterns at the location where the next stimulus was expected to be presented. Participants learned an arbitrary spatial sequence consisting of 4 dots (A-B-C-D), and after learning, partial sequence trials, wherein only a single dot (e.g., - B - -) appeared at its expected moment in time, were intersected with full sequence trials. Using the posterior electrodes, we were able to decode the spatial position of expected, yet critically omitted stimuli within these partial sequences, visualizing the dynamic weight changes within the presumed spatial priority map on a trial-to-trial basis. Our findings are fully consistent with those of <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref> who used fMRI and showed reactivations of future sequence locations in both V1 and hippocampus.</p>
<p>The present findings contribute to an extensive body of literature demonstrating that learned associations are actively utilized to predict the imminent future (e.g., <xref ref-type="bibr" rid="c8">Dayan, 1993</xref>; <xref ref-type="bibr" rid="c13">Ekman et al., 2023</xref>; <xref ref-type="bibr" rid="c15">Fang et al., 2023</xref>; <xref ref-type="bibr" rid="c57">Stachenfeld et al., 2017</xref>). However, studies investigating how expectations induce anticipatory activity in early visual cortices (e.g., <xref ref-type="bibr" rid="c12">Ekman et al., 2017</xref>; <xref ref-type="bibr" rid="c22">Gavornik &amp; Bear, 2014</xref>; <xref ref-type="bibr" rid="c29">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="c37">Kok et al., 2012</xref>) have yet to clarify whether such anticipatory traces represent expected future states in a temporally discounted fashion or solely reflect the learned association between different stimuli without the added complexity of a temporal structure (e.g., <xref ref-type="bibr" rid="c10">Deuker et al., 2016</xref>; <xref ref-type="bibr" rid="c31">Horner et al., 2015</xref>; <xref ref-type="bibr" rid="c55">Rolls, 2016</xref>). While Ekman et al.’s observation that events in the close future were more prominently represented than events in the more distant future supports the former account, in that study, the limitations of the BOLD approach made it impossible to provide a detailed analysis of how these learned predictions evolve over time. In our study, leveraging the high temporal resolution of EEG in conjunction with multivariate analysis, we demonstrate that any stimulus in a fixed, yet passively viewed sequence triggers the internal representation of the immediately following stimulus in the sequence at its expected moment in time. Crucially, this above-chance classification of an expected yet omitted stimulus was achieved by an independent pattern estimator trained on the evoked signal elicited by stimuli presented in random order, thereby avoiding confounding effects related to prediction. This confirms not only that future representations are represented in a temporally discounted fashion but also that the predicted future recruits similar neural processes as when viewing those stimuli.</p>
<p>The finding that learned spatial associations could evoke sensory-like stimulus representations in the absence of sensory input aligns with prior studies demonstrating decoding of anticipated positions during circular object motion (i.e., clockwise or counterclockwise) around fixation. However, there is a crucial difference between previous studies using rotating sequences and the current one. Critically, in previous studies, the rotating stimulus either induced illusory motion (<xref ref-type="bibr" rid="c30">Hogendoorn &amp; Burkitt, 2018</xref>) or participants were explicitly instructed to mentally envisage the subsequent dot position each time a sound was played (<xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>). In contrast, our stimuli were not only presented in an arbitrary visual dot sequence, but this sequence also did not necessitate active planning to anticipate different future outcomes. Instead, the sequences were passively viewed, and therefore it is unlikely that the internal representations are the result of top-down attention; instead it is likely that they are formed automatically and incidentally via statistical learning (<xref ref-type="bibr" rid="c6">Christiansen, 2019</xref>; <xref ref-type="bibr" rid="c20">Frost et al., 2019</xref>). It is generally assumed that statistical learning operates through local weight changes in assumed spatial priority maps, enhancing the weights associated with locations likely to contain relevant information and downregulating those with a higher distractor probability (e.g., <xref ref-type="bibr" rid="c16">Fecteau &amp; Munoz, 2006</xref>; <xref ref-type="bibr" rid="c17">Ferrante et al., 2018</xref>; <xref ref-type="bibr" rid="c59">Theeuwes et al., 2022</xref>; <xref ref-type="bibr" rid="c69">Zelinsky &amp; Bisley, 2015</xref>). To date, studies examining local weight changes within the priority map have did so in paradigms where the learned locations, be it targets or distractors, were static across stretches of time (<xref ref-type="bibr" rid="c11">Duncan et al., 2023</xref>; <xref ref-type="bibr" rid="c18">Ferrante et al., 2023</xref>). By contrast, here the dot stimulus moved across space, necessitating a flexible priority landscape. At a behavioral level, we have previously shown that while learned attentional biases can be persistent, the priority landscape can also be flexibly adjusted across trials if needed, enhancing efficiency when a target at a specific location is consistently preceded by a target at the opposite display location (<xref ref-type="bibr" rid="c39">Li et al., 2022</xref>; <xref ref-type="bibr" rid="c41">Li &amp; Theeuwes, 2020</xref>). Although the current study did not require attentional selection for each individual event, attention systematically moved from one location to the next, over time, activating the expected event in response to the current event.</p>
<p>Although we observed robust generalization from the random pattern estimator to both present as well as omitted dots within the structured sequence, there were clear differences in the temporal classification dynamics. Whereas decoding of viewed dots was largely restricted to the diagonal, suggestive of a chain of distinct generators (<xref ref-type="bibr" rid="c34">King &amp; Dehaene, 2014</xref>), generalization to internally generated representations mostly relied on two training intervals (<xref rid="fig4" ref-type="fig">Figures 4c</xref> and <xref rid="fig5" ref-type="fig">5a</xref>): one starting at around 100 ms and the other starting at approximately 200 ms. In the context of spatial attention, this early interval is related to early sensory/perceptual processing (indexed by P1 and N1 ERP components, e.g., <xref ref-type="bibr" rid="c28">Hillyard &amp; Anllo-Vento, 1998</xref>), whereas the second interval correlates with post-perceptual processes such as attentional selection (indexed by N2pc ERPs component, e.g., <xref ref-type="bibr" rid="c45">Luck &amp; Hillyard, 1994</xref>). It thus appears that both sensory/perceptual and post-perceptual processes underpin the activation of internal representations for the anticipated yet omitted stimuli. The fact that these processes generalized to relatively large stretches in time in stimulus absent conditions is not surprising given that in contrast to evoked signals, internally generated processes are likely to be much more variable, resulting in more diffuse time-locked signals (see also <xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>). In line with this explanation, the decoding accuracy of present dots was consistently higher when the dot was embedded within a full sequence compared to the same dots presented in isolation. This effect coincided with a reduction in the N1 elicited by dots in partial sequences. It suggests that attention could be precisely tuned to each individual event in full sequences, while this attentional tuning became more variable when dots were omitted from the sequence, and participants had to rely on internally generated representations. The diffuse nature of these internally generated representations might also explain why, in contrast to the <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref>, we could only decode the immediate successor representation, but not the subsequent representations within the sequence. Although these representations may have been present in the signal, the jitter caused by increasing temporal uncertainty arguably attenuated the signal to a degree that prevented their retrieval by our pattern estimator.</p>
<p>It is generally assumed that the neural signals elicited by successive stimuli with longer ISIs are less contaminated than those with shorter ISIs. However, to date this assumption has not been empirically tested. In a recent study by <xref ref-type="bibr" rid="c24">Grootswagers et al. (2019)</xref>, hundreds of different images were presented at two different rates without an ISI (i.e., 20 Hz and 5 Hz). It was observed that decoding performance was higher and lasted longer in the 5-Hz condition compared to the 20-Hz condition, with no difference in the onset time between the two. Notably, the decoders captured category features rather than low-level features, as determined by the cross-validation procedure that should generalize to the test images. In comparison, the present study focused on decoding a specific low-level feature (i.e., position), making a comparison between two relatively slower frequencies. Consequently, our findings align with those of <xref ref-type="bibr" rid="c24">Grootswagers et al. (2019)</xref>, and also provide some suggestions regarding ISIs for future decoding studies, especially those aim at decoding low-level features like positions and orientations. While the benefit of short-ISI pattern estimator sequences is that they are less time consuming, if time permits it may be best practice to use a pattern estimator with longer ISI’s as these provide a better model of stimulus specific representations.</p>
<p>In conclusion, the present study successfully decoded the positions of omitted anticipated dots within a learned spatial sequence, visualizing the dynamic weight changes within the presumed spatial priority map on a trial-to-trial basis. We demonstrated that the internal spatial representations, shaped by prior experiences, involved both sensory/perceptual and post-perceptual processes. Our research highlights the brain’s remarkable ability to dynamically prioritize and internally generate representations for anticipated locations in a future-oriented manner, even in the absence of sensory inputs. These findings help us further understand how the brain predicts and optimizes responses to external stimuli in a constantly changing world.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Participants</title>
<p>The sample size was determined based on previous EEG studies decoding dot positions in a circular configuration (<xref ref-type="bibr" rid="c2">Blom et al., 2020</xref>; <xref ref-type="bibr" rid="c30">Hogendoorn &amp; Burkitt, 2018</xref>; <xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>). Twenty-five undergraduate students (22 females and 3 males; <italic>M</italic>age = 21.64 years, <italic>SD</italic>age = 2.36) recruited from research pool at the Vrije Universiteit Amsterdam participated in the study in exchange for course credit or monetary compensation. Two participants were replaced: one due to a poor connection to the earlobe channels, and the other one because of a recording error. All participants reported normal or corrected-to-normal visual acuity and gave informed consent prior to the start of the experiment. The study was approved by the Ethical Review Committee of the Faculty of Behavioral and Movement Sciences of the Vrije Universiteit Amsterdam.</p>
</sec>
<sec id="s4b">
<title>Apparatus</title>
<p>The experiment was created on a Dell Precision 3640 Windows 10 computer equipped with an NVIDIA Quadro P620 graphics card in OpenSesame (<xref ref-type="bibr" rid="c47">Mathôt et al., 2012</xref>) using PsychoPy (<xref ref-type="bibr" rid="c51">Peirce, 2007</xref>) functionality. Stimuli were presented on a 23.8-inch LED monitor (ASUS ROG Strix XG248Q; resolution: 1920 × 1080 pixels) at a refresh rate of 240 Hz. Participants were seated in a dimly lit room with their head positioned on a chin rest at a viewing distance of approximately 65 cm. Behavioral responses were collected via a standard keyboard. EEG signals were recorded using ActiView software (Biosemi, Amsterdam, Netherlands), and as common for Biosemi, two additional electrodes were used as reference and ground electrodes during recording. An Eyelink 1000 eye-tracker (SR Research, Ontario, Canada) sampling at 1000 Hz was used to on- and off-line monitor the gaze positions of both eyes during the experiment. Participants heard a low-volume beep sound from the speaker whenever their fixation deviation exceeded 1.5 degrees of visual angle (dva). Each time the participant moved away from the chinrest, the eye-tracker was recalibrated via a five-dot calibration procedure until the spatial error for each dot position was below 1 dva. Drift correction was applied before each block started.</p>
</sec>
<sec id="s4c">
<title>Stimuli and Design</title>
<p>The paradigm was modeled after <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref>. As illustrated in <xref rid="fig1" ref-type="fig">Figure 1</xref>, all stimuli were presented on a grey (RGB: 128/128/128) background. To elicit reliable and stable fixation, a combination of bull’s eye and hair cross (<xref ref-type="bibr" rid="c58">Thaler et al., 2013</xref>) was used as the central fixation marker (diameter: 0.7 dva; RGB: 0/0/0) which remained visible throughout the entire experiment. The stimulus was a white circular dot (diameter: 1.2 dva; RGB: 210/210/210), which could appear at any of eight possible locations (i.e., 22.5°, 67.5°, 112.5°, 157.5°, 202.5°, 247.5°, 292.5°, 337.5° of polar angle from the vertical line, see <xref rid="fig1" ref-type="fig">Figure 1b</xref>) at 6.5 dva away from fixation. This dot, when present, remained on screen for 100 ms.</p>
<p>Individual dots were presented in sequence in different block conditions, the <italic>main task</italic> and <italic>pattern estimator</italic> blocks. In the main task, participants viewed a sequence of four successively presented dots (A-B-C-D). As illustrated in <xref rid="fig1" ref-type="fig">Figure 1a</xref> (top row), following <xref ref-type="bibr" rid="c13">Ekman et al. (2023)</xref>, the sequence trajectory was created such that each quadrant was stimulated once per sequence and neighboring locations were thus never part of the same sequence. Specifically, the starting position of the first dot determined the sequence trajectory such that the second dot always appeared 180° away from the starting position; the third dot was presented 90° clockwise from the second position; the last dot appeared opposite from the third position. Each participant was assigned to one of eight possible sequences. Each dot was shown for 100 ms with an inter-stimulus interval (ISI) of 200 ms, and individual sequences were separated by a jittered inter-trial interval (ITI) between 500 and 750 ms. On a small subset of these sequences (i.e., 10% of full sequence trials, the last dot was rendered in grey (RGB: 150/150/150) rather than white and participants were instructed to detect these grey dot presentations by means of a speeded space bar press (timeout 700 ms), while holding fixation at all times. To ensure that these rare target trials were relatively evenly distributed across trials within each block, trial order was pseudo-randomized with the constraint that consecutive grey dots were separated by minimally five sequences. Each time a grey dot was not reported, the text “Be focused!” appeared in red for 500 ms at the center of the screen. Also, the ITI following all target present trials was extended by 300 ms to reduce the influence on the next trial. The sole purpose of this detection task was to keep participants engaged with the stimulus sequences and hence behavioral data were not analyzed, and epochs time-locked to grey dot onset were excluded from further analysis.</p>
<p>The main task was separated into four learning blocks and 12 main task blocks. Within each learning block, the full sequence was repeated 130 times (out of which 13 contained a grey dot at the final position). By contrast, in main task blocks, 88 full sequence trials (out of which 9 contained a grey dot at the final position), were intermixed with 44 partial sequences. These partial sequences were identical to full sequences, with the difference that only one of the four dots in the sequence (with equal probability) was actually displayed (see bottom of <xref rid="fig1" ref-type="fig">Figure 1a</xref>). The purpose of these partial sequences was to probe neural activations of expected events. Sequence order within these main blocks was pseudo-randomized with the extra constraint that a partial sequence trial must be preceded by a non-target full sequence and not followed by another partial sequence. Feedback containing the hit rate and average response times was given at the end of each block.</p>
<p>In addition to the main task blocks, we also ran separate pattern estimator blocks, which served as an independent source of training data for the multivariate decoding analyses. Within these blocks, successive sequences of dots at all 8 possible locations were shown in random order (see <xref rid="fig1" ref-type="fig">Figure 1c</xref>), with the restriction that a dot position was never repeated at the crossing between sequences. Participants were instructed to keep fixation, while passively viewing these randomly concatenated sequences.</p>
<p>EEG decoding based on independent pattern estimator blocks has been successfully applied, where the ISI in between successive stimuli was either short (range: 0-200 ms; e.g., <xref ref-type="bibr" rid="c1">Alilovic et al., 2021</xref>; <xref ref-type="bibr" rid="c2">Blom et al., 2020</xref>; <xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>) or long (range: 433-1200 ms; e.g., <xref ref-type="bibr" rid="c30">Hogendoorn &amp; Burkitt, 2018</xref>; <xref ref-type="bibr" rid="c38">Kok et al., 2017</xref>), with each approach having their own benefits. It has been demonstrated that even a simple flashing stimulus, as also used here, can evoke cortical response for ∼1 s (<xref ref-type="bibr" rid="c4">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="c35">King &amp; Wyart, 2021</xref>) and the advantage of pattern estimator with longer ISI’s is that the signal evoked by each individual stimulus is less contaminated by evoked responses from preceding stimuli. By contrast, the benefit of short-ISI pattern estimator, apart from being less time consuming, the high rate of stimulus presentations also makes them less susceptible to systematic eye movements. Since this is the first time, we have attempted to decode the spatial position of expected dots in the absence of visual input, which is arguably more challenging due to the event of interest not being time-locked to an evoked response, we chose to employ both type of pattern estimators to establish whether one type is more suited than the other. To balance the length of ISI on long-ISI blocks and the length of the entire experiment, we chose an ISI of 600 ms for long-ISI blocks and an ISI of 100 ms for short-ISI blocks.</p>
<p>The three long-ISI pattern estimator blocks (50 trials each, ∼4.7 min) were introduced at the start of the experiment such that the longer ISI’s were not contaminated by learned anticipations in response to structured sequences. Subsequently, participants completed four learning blocks (∼4 min/block) to introduce them to the structured sequence. Then, following previous work (<xref ref-type="bibr" rid="c38">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>) every short-ISI pattern estimator block (25 trials each, ∼0.7 min) was interleaved with two main blocks (i.e., intermixed full and partial sequences). In total there were thus 300 pattern estimator sequences (i.e., 8 unique positions in random order), divided into three long-ISI blocks and six short-ISI blocks. For each participant, the entire experiment including preparation lasted about three hours.</p>
</sec>
<sec id="s4d">
<title>Analysis scripts</title>
<p>All preprocessing and subsequent analyses were conducted in a Python environment, using custom-written scripts based on functions from MNE-python (<xref ref-type="bibr" rid="c23">Gramfort et al., 2013</xref>), PyGazeAnalyser (<xref ref-type="bibr" rid="c7">Dalmaijer et al., 2014</xref>) and scikit-learn (<xref ref-type="bibr" rid="c50">Pedregosa et al., 2011</xref>) modules. The scripts are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/dvanmoorselaar/DvM">https://github.com/dvanmoorselaar/DvM</ext-link>.</p>
</sec>
<sec id="s4e">
<title>EEG recording and preprocessing</title>
<p>EEG data were acquired at a sampling rate of 512 Hz, using a 64-electrode cap with electrodes arranged according to the international 10-10 system (Biosemi ActiveTwo system; and two external electrodes placed on earlobes (used as an offline reference). Horizontal and vertical electrooculogram (HEOG and VEOG) were recorded via external electrodes located ∼1 cm lateral to the outer canthi, and ∼2 cm above and below the right eye, respectively. Recordings were filtered online with a high-pass filter of 0.16 Hz and a low-pass filter of 100 Hz. As an initial preprocessing step, EEG data was referenced to the average signal recorded over the left and right earlobe, and subsequently a zero-phase shift finite impulse response (FIR) high-pass filter at 0.01 Hz (<xref ref-type="bibr" rid="c61">van Driel et al., 2021</xref>) was applied to remove slow drifts.</p>
<p>Before cleaning, malfunctioning electrodes (<italic>M</italic> = 0.6, <italic>SD</italic> = 0.91) marked during recording were temporarily removed. Continuous EEG signals was epoched from −100 ms relative to the onset of a full sequence until the end of that sequence. Given that we had sequences of various lengths this resulted in epochs of various lengths (−100 to 1200 ms for full and partial sequences; −100 to 1700 ms for short-ISI pattern estimator sequences; −100 to 5200 ms for long-ISI pattern estimator sequences), which were all preprocessed independently. All epochs were extended by 500 ms at the start and end of the epoch to control for filter artifacts during preprocessing. First, independent component analysis (ICA) as implemented in MNE (method=‘picard’) was fitted on 1-Hz high-pass filtered epochs to remove eye-blink components from the 0.01-Hz filtered epochs. Second, an automatic artifact-rejection procedure was used to detect electromyography (EMG) noise. Specifically, the epochs were band-pass filtered at 110−140 Hz and transformed into a z-score threshold per participant based on the within-subject variance of z-scores (<xref ref-type="bibr" rid="c9">de Vries et al., 2017</xref>). Epochs that exceeded the threshold within the time window of interest were flagged (<xref ref-type="bibr" rid="c11">Duncan et al., 2023</xref>). To decrease the number of false alarms, for each marked epoch, the five electrodes that contributed most to accumulated z-score were identified. Then these electrodes were interpolated one by one using spherical splines (<xref ref-type="bibr" rid="c52">Perrin et al., 1989</xref>), checking after each interpolation whether the data still exceeded the threshold. Epochs were only excluded if the z-score threshold was still exceeded after this iterative procedure, which resulted in an average rejection of 5.85% (<italic>SD</italic>: 6.44%) of full/partial sequences, 14.37% (<italic>SD</italic>: 11.25%) of long-ISI pattern estimator sequences, and 9.95% (<italic>SD</italic>: 12.39%) of short-ISI pattern estimator sequences. Then, temporarily removed malfunctioning electrodes were interpolated using spherical splines. As a final step, in all three individual datasets the epoched sequences were split into individual dot-based epochs by creating smaller epochs (i.e., −100 to 300 ms) relative to the onset of each individual dot, irrespective of whether that dot was present or absent.</p>
<p>To eliminate the influence of eye movements on EEG decoding, after preprocessing we also rejected dot-based epochs on the basis of deviations from fixation as detected by the eye-tracker. To this end, gaze position (i.e., x and y coordinates) were epoched relative to the onset of the sequence using the same windows used during EEG preprocessing. Missing data due to blinks detected by the eye-tracker were interpolated by padding 100 ms before and after the blink interval (according to table 2 in <xref ref-type="bibr" rid="c27">Hershman et al., 2018</xref>). To control for drifts in the eye- tracker data, eye epochs without a saccade in the 100-ms pre-trial window were shifted toward fixation. The epoched data was then down-sampled to 512 Hz and aligned to the corresponding EEG epochs by creating dot-based epochs. Each of these individual dot-based epochs were summarized by a single value indicating the maximum fixation deviation measured in continuous 40-ms intervals of data (<xref ref-type="bibr" rid="c63">van Moorselaar &amp; Slagter, 2019</xref>). If this value was above 1 dva, this epoch was marked for exclusion. In case of missing eye-tracker data, we used a sliding-window step method (length: 200 ms, step: 10 ms, threshold: 20 μV) on HEOG to detect eye movements. Combining these two methods, we excluded an average of 5.51% (<italic>SD</italic>: 5.1%) of sequence epochs, 4.59% (<italic>SD</italic>: 4.47%) of long-ISI pattern estimator epochs, 3.46% (<italic>SD</italic>: 4.33%) of short-ISI pattern estimator epochs.</p>
</sec>
<sec id="s4f">
<title>Decoding analyses</title>
<p>To investigate the position representation of both present and absent dots on the screen, we applied multivariate pattern analysis, using linear discriminant analysis (LDA) in combination with a 10-fold cross-validation scheme, with channel voltages from the 29 posterior electrodes (i.e., CPz, CP1, CP2, CP3, CP4, CP5, CP6, TP7, TP8, Pz, P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, POz, PO3, PO4, PO7, PO8, Oz, O1, O2, Iz) used as features (e.g., <xref ref-type="bibr" rid="c5">Chota et al., 2023</xref>; <xref ref-type="bibr" rid="c26">Hajonides et al., 2021</xref>; <xref ref-type="bibr" rid="c54">Robinson et al., 2020</xref>; <xref ref-type="bibr" rid="c67">Wolff et al., 2017</xref>) and the four dot positions of the full sequences as classes. Before classification EEG data was averaged across three epochs (i.e., matching dot positions), while ensuring that the number of observations per stimulus position that entered the analysis was balanced across classes so that training was not biased towards a specific class. Event averaged data was then baseline corrected using a −100 to 0 ms window and down-sampled to 128 Hz (to decrease the computational time of decoding analyses). To further increase signal-to-noise ratio (<xref ref-type="bibr" rid="c25">Grootswagers et al., 2017</xref>), the data were transformed per sample using principal component analysis (PCA) with 99% of the variance. Before applying PCA, the data was standardized. Note that while performing standardization and PCA on both the training and test data, the transformation statistics were estimated using the training data only. Next, within-block decoding was performed via a 10-fold cross validation procedure, such that at each time point, the classifier was trained on nine folds and tested on the remaining fold until each fold was tested once. Classifier performance was then averaged over folds and characterized via Area Under the Curve (AUC), where a value of 0.5 is considered chance classification.</p>
<p>These initial decoding analyses, which were performed separately for short- and long-ISI pattern estimators, served to establish whether the pattern estimator could reliably track the dot position on screen. To subsequently assess whether absent dots were represented, if at all, differently than present dots, in a next set of analysis, the classifiers, with the same parameters used during within-block pattern estimator decoding, were trained on all events from the pattern estimator sequences and tested on both present and absent dots in full and partial sequences. Considering that there was no dot on screen in the absent condition, in these analyses we adopted a temporal generalization approach, training and testing was performed at all combinations of time points, to assess whether and when the EEG signal contained information regarding anticipated, yet absent events (<xref ref-type="bibr" rid="c34">King &amp; Dehaene, 2014</xref>). In doing so, we again ensured that the number of observations in both the training and test set were balanced per stimulus position to avoid inflated above-chance performance resulting from a bias in the distribution of classes (<xref ref-type="bibr" rid="c14">Fahrenfort et al., 2018</xref>; <xref ref-type="bibr" rid="c62">van Moorselaar et al., 2020</xref>).</p>
</sec>
<sec id="s4g">
<title>Event-related potentials (ERPs)</title>
<p>To isolate stimulus-evoked ERPs components, separate contralateral waveforms were computed for full and partial sequence conditions, using a set of 8 posterior electrodes (i.e., P1, P3, P5, P7, P9, PO3, PO7, O1 or P2, P4, P6, P8, P10, PO4, PO8, O2) (e.g., <xref ref-type="bibr" rid="c26">Hajonides et al.,2021</xref>; <xref ref-type="bibr" rid="c44">Luck et al., 1990</xref>; <xref ref-type="bibr" rid="c45">Luck &amp; Hillyard, 1994</xref>). In doing so, the first dot position (i.e., A) was excluded as at that point in the sequence there is no difference yet between full and partial sequences. Epochs were baseline corrected using a 100 ms window preceding dot onset. Resulting waveforms were averaged across dot positions (i.e., BCD).</p>
</sec>
<sec id="s4h">
<title>Statistics</title>
<p>Decoding scores were evaluated across time via cluster-based permutation tests with one sampled two-sided t-tests with cluster correction (<italic>p</italic> = .05, 1024 iterations; chance level = 0.5) using MNE functionality (<xref ref-type="bibr" rid="c23">Gramfort et al., 2013</xref>). The same procedure, but with a paired sampled t-test, was used to compare the difference between short- and long-ISI pattern estimators across time with the time window of the long-ISI pattern estimator matched to the short-ISI pattern estimator window.</p>
</sec>
</sec>
<sec id="s5">
<title>Author contributions</title>
<p>Ai-Su Li, Conceptualization, Methodology, Validation, Formal analysis, Investigation, Resources, Data curation, Funding acquisition, Visualization, Project administration, Writing - original draft, Writing - review and editing; Jan Theeuwes, Conceptualization, Funding acquisition, Writing - original draft, Writing - review and editing; Dirk van Moorselaar, Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, Visualization, Supervision, Writing - original draft, Writing - review and editing.</p>
</sec>
<sec id="s6">
<title>Open practices statement</title>
<p>Data will be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AisuLi/dot_seq">https://github.com/AisuLi/dot_seq</ext-link>, and the experiment was not preregistered.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>AL was supported by National Key Research and Development Program of China (2022YFB4500601) and National Natural Science Foundation of China (32171049). DvM and JT were supported by a European Research Council (ERC) advanced grant 833029 –[LEARNATTEND].</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alilovic</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Graetz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>van Gaal</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Slagter</surname>, <given-names>H. A.</given-names></string-name> (<year>2021</year>). <article-title>Representational dynamics preceding conscious access</article-title>. <source>Neuroimage</source>, <volume>230</volume>, <fpage>117789</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117789</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Blom</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Feuerriegel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bode</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Hogendoorn</surname>, <given-names>H</given-names></string-name>. (<year>2020</year>). <article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>13</issue>), <fpage>7510</fpage>–<lpage>7515</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1917777117</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Boettcher</surname>, <given-names>S. E. P.</given-names></string-name>, <string-name><surname>Shalev</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Wolfe</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Nobre</surname>, <given-names>A. C</given-names></string-name>. (<year>2022</year>). <article-title>Right place, right time: Spatiotemporal predictions guide attention in dynamic visual search</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>151</volume>(<issue>2</issue>), <fpage>348</fpage>–<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000901</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Tovar</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Kriegeskorte</surname>, <given-names>N</given-names></string-name>. (<year>2013</year>). <article-title>Representational dynamics of object vision: The first 1000 ms</article-title>. <source>Journal of Vision</source>, <volume>13</volume>(<issue>10</issue>), <fpage>1</fpage>. <pub-id pub-id-type="doi">10.1167/13.10.1</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Chota</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gayet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kenemans</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name>, &amp; <string-name><surname>Van der Stigchel</surname>, <given-names>S.</given-names></string-name> (<year>2023</year>). <article-title>A matter of availability: sharper tuning for memorized than for perceived stimulus features</article-title>. <source>Cerebral Cortex</source>. <pub-id pub-id-type="doi">10.1093/cercor/bhad064</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Christiansen</surname>, <given-names>M. H</given-names></string-name>. (<year>2019</year>). <article-title>Implicit statistical learning: A tale of two literatures</article-title>. <source>Topics in Cognitive Science</source>, <volume>11</volume>(<issue>3</issue>), <fpage>468</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1111/tops.12332</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Dalmaijer</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Mathot</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Van der Stigchel</surname>, <given-names>S.</given-names></string-name> (<year>2014</year>). <article-title>PyGaze: An open-source, cross-platform toolbox for minimal-effort programming of eyetracking experiments</article-title>. <source>Behavior Research Methods</source>, <volume>46</volume>(<issue>4</issue>), <fpage>913</fpage>–<lpage>921</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-013-0422-2</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Dayan</surname>, <given-names>P</given-names></string-name>. (<year>1993</year>). <article-title>Improving generalization for temporal difference learning: The successor representation</article-title>. <source>Neural computation</source>, <volume>5</volume>(<issue>4</issue>), <fpage>613</fpage>–<lpage>624</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>de Vries</surname>, <given-names>I. E.</given-names></string-name>, <string-name><surname>van Driel</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name> (<year>2017</year>). <article-title>Posterior alpha EEG dynamics dissociate current from future goals in working memory-guided visual search</article-title>. <source>The Journal of Neuroscience</source>, <volume>37</volume>(<issue>6</issue>), <fpage>1591</fpage>–<lpage>1603</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2945-16.2016</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Deuker</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bellmund</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Navarro Schroder</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Doeller</surname>, <given-names>C. F</given-names></string-name>. (<year>2016</year>). <article-title>An event map of memory space in the hippocampus</article-title>. <source>Elife</source>, <volume>5</volume>. <pub-id pub-id-type="doi">10.7554/eLife.16534</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Duncan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name> (<year>2023</year>). <article-title>Pinging the brain to reveal the hidden attentional priority map using encephalography</article-title>. <source>Nature Communications</source>, <volume>14</volume>(<issue>1</issue>), <fpage>4749</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-023-40405-8</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Ekman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2017</year>). <article-title>Time-compressed preplay of anticipated events in human primary visual cortex</article-title>. <source>Nature Communications</source>, <volume>8</volume>, <fpage>15276</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms15276</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Ekman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kusch</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2023</year>). <article-title>Successor-like representation guides the prediction of future events in human visual cortex and hippocampus</article-title>. <source>Elife</source>, <volume>12</volume>, <fpage>e78904</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.78904</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Fahrenfort</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>van Driel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>van Gaal</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name> (<year>2018</year>). <article-title>From ERPs to MVPA using the Amsterdam Decoding and Modeling Toolbox (ADAM)</article-title>. <source>Frontiers in Neuroscience</source>, <volume>12</volume>, <fpage>368</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2018.00368</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Fang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Aronov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, &amp; <string-name><surname>Mackevicius</surname>, <given-names>E. L</given-names></string-name>. (<year>2023</year>). <article-title>Neural learning rules for generating flexible predictions and computing the successor representation</article-title>. <source>Elife</source>, <volume>12</volume>. <pub-id pub-id-type="doi">10.7554/eLife.80680</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Fecteau</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Munoz</surname>, <given-names>D. P</given-names></string-name>. (<year>2006</year>). <article-title>Salience, relevance, and firing: A priority map for target selection</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>10</volume>(<issue>8</issue>), <fpage>382</fpage>–<lpage>390</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2006.06.011</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Ferrante</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Patacca</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Di Caro</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Della Libera</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Santandrea</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Chelazzi</surname>, <given-names>L.</given-names></string-name> (<year>2018</year>). <article-title>Altering spatial priority maps via statistical learning of target selection and distractor filtering</article-title>. <source>Cortex</source>, <volume>102</volume>, <fpage>67</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2017.09.027</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Ferrante</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Zhigalov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hickey</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Jensen</surname>, <given-names>O</given-names></string-name>. (<year>2023</year>). <article-title>Statistical learning of distractor suppression downregulates prestimulus neural excitability in early visual cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>43</volume>(<issue>12</issue>), <fpage>2190</fpage>–<lpage>2198</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1703-22.2022</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Fiser</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mahringer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Oyibo</surname>, <given-names>H. K.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>A. V.</given-names></string-name>, <string-name><surname>Leinweber</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Keller</surname>, <given-names>G. B</given-names></string-name>. (<year>2016</year>). <article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>12</issue>), <fpage>1658</fpage>–<lpage>1664</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4385</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Frost</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Armstrong</surname>, <given-names>B. C.</given-names></string-name>, &amp; <string-name><surname>Christiansen</surname>, <given-names>M. H</given-names></string-name>. (<year>2019</year>). <article-title>Statistical learning research: A critical review and possible new directions</article-title>. <source>Psychological Bulletin</source>, <volume>145</volume>(<issue>12</issue>), <fpage>1128</fpage>–<lpage>1153</lpage>. <pub-id pub-id-type="doi">10.1037/bul0000210</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Gao</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2020</year>). <article-title>Independent effects of statistical learning and top-down attention. <italic>Attention, Perception</italic></article-title>, <source>&amp; Psychophysics</source>, <volume>82</volume>(<issue>8</issue>), <fpage>3895</fpage>–<lpage>3906</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-020-02115-x</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Gavornik</surname>, <given-names>J. P.</given-names></string-name>, &amp; <string-name><surname>Bear</surname>, <given-names>M. F</given-names></string-name>. (<year>2014</year>). <article-title>Learned spatiotemporal sequence recognition and prediction in primary visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>5</issue>), <fpage>732</fpage>–<lpage>737</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3683</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luessi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Larson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Engemann</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Strohmeier</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Brodbeck</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Goj</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brooks</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Parkkonen</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Hamalainen</surname>, <given-names>M</given-names></string-name>. (<year>2013</year>). <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Frontiers in Neuroscience</source>, <volume>7</volume>, <fpage>267</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name>. (<year>2019</year>). <article-title>The representational dynamics of visual objects in rapid serial visual processing streams</article-title>. <source>Neuroimage</source>, <volume>188</volume>, <fpage>668</fpage>–<lpage>679</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.12.046</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wardle</surname>, <given-names>S. G.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name>. (<year>2017</year>). <article-title>Decoding dynamic brain patterns from evoked responses: A tutorial on multivariate pattern analysis applied to time series neuroimaging data</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>29</volume>(<issue>4</issue>), <fpage>677</fpage>–<lpage>697</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Hajonides</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>van Ede</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Stokes</surname>, <given-names>M. G.</given-names></string-name> (<year>2021</year>). <article-title>Decoding visual colour from scalp electroencephalography measurements</article-title>. <source>Neuroimage</source>, <volume>237</volume>, <fpage>118030</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118030</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Hershman</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Henik</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Cohen</surname>, <given-names>N</given-names></string-name>. (<year>2018</year>). <article-title>A novel blink detection method based on pupillometry noise</article-title>. <source>Behavior Research Methods</source>, <volume>50</volume>(<issue>1</issue>), <fpage>107</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-017-1008-1</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name>, &amp; <string-name><surname>Anllo-Vento</surname>, <given-names>L</given-names></string-name>. (<year>1998</year>). <article-title>Event-related brain potentials in the study of visual selective attention</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>95</volume>(<issue>3</issue>), <fpage>781</fpage>–<lpage>787</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.95.3.781</pub-id></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Hindy</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Ng</surname>, <given-names>F. Y.</given-names></string-name>, &amp; <string-name><surname>Turk-Browne</surname>, <given-names>N. B</given-names></string-name>. (<year>2016</year>). <article-title>Linking pattern completion in the hippocampus to predictive coding in visual cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>5</issue>), <fpage>665</fpage>–<lpage>667</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4284</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Burkitt</surname>, <given-names>A. N</given-names></string-name>. (<year>2018</year>). <article-title>Predictive coding of visual object position ahead of moving objects revealed by time-resolved EEG decoding</article-title>. <source>Neuroimage</source>, <volume>171</volume>, <fpage>55</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.063</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Horner</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Bisby</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Bush</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>W. J.</given-names></string-name>, &amp; <string-name><surname>Burgess</surname>, <given-names>N</given-names></string-name>. (<year>2015</year>). <article-title>Evidence for holistic episodic recollection via hippocampal pattern completion</article-title>. <source>Nature Communications</source>, <volume>6</volume>, <fpage>7462</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms8462</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Huang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Donk</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Theeuwe</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>Proactive enhancement and suppression elicited by statistical regularities in visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>48</volume>(<issue>5</issue>), <fpage>443</fpage>–<lpage>457</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0001002</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Johnson</surname>, <given-names>P. A.</given-names></string-name>, <string-name><surname>Blom</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>van Gaal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Feuerriegel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bode</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name> (<year>2023</year>). <article-title>Position representations of moving objects align with real-time position in the early visual response</article-title>. <source>Elife</source>, <volume>12</volume>. <pub-id pub-id-type="doi">10.7554/eLife.82424</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>King</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Dehaene</surname>, <given-names>S</given-names></string-name>. (<year>2014</year>). <article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>18</volume>(<issue>4</issue>), <fpage>203</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>King</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Wyart</surname>, <given-names>V</given-names></string-name>. (<year>2021</year>). <article-title>The human brain encodes a chronicle of visual events at each instant of time through the multiplexing of traveling waves</article-title>. <source>The Journal of Neuroscience</source>, <volume>41</volume>(<issue>34</issue>), <fpage>7224</fpage>–<lpage>7233</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2098-20.2021</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Brouwer</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>van Gerven</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2013</year>). <article-title>Prior expectations bias sensory representations in visual cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>33</volume>(<issue>41</issue>), <fpage>16275</fpage>–<lpage>16284</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0742-13.2013</pub-id></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jehee</surname>, <given-names>J. F.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2012</year>). <article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title>. <source>Neuron</source>, <volume>75</volume>(<issue>2</issue>), <fpage>265</fpage>–<lpage>270</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2017</year>). <article-title>Prior expectations induce prestimulus sensory templates</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>114</volume>(<issue>39</issue>), <fpage>10473</fpage>–<lpage>10478</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1705652114</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Bogaerts</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2022</year>). <article-title>Statistical learning of across-trial regularities during serial search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>48</volume>(<issue>3</issue>), <fpage>262</fpage>–<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000987</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Bogaerts</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2023</year>). <article-title>No evidence for spatial suppression due to across-trial distractor learning in visual search. <italic>Attention</italic></article-title>, <source>Perception, and Psychophysics</source>, <volume>85</volume>(<issue>4</issue>), <fpage>1088</fpage>–<lpage>1105</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-023-02667-8</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2020</year>). <article-title>Statistical regularities across trials bias attentional selection</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>46</volume>(<issue>8</issue>), <fpage>860</fpage>–<lpage>870</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000753</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="other"><string-name><surname>Li</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name> (under review). <article-title>Attending is not enough: responding to targets is needed for across-trial statistical learning</article-title>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Liesefeld</surname>, <given-names>H. R.</given-names></string-name>, &amp; <string-name><surname>Müller</surname>, <given-names>H. J</given-names></string-name>. (<year>2021</year>). <article-title>Modulations of saliency signals at two hierarchical levels of priority computation revealed by spatial statistical distractor learning</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>150</volume>(<issue>4</issue>), <fpage>710</fpage>–<lpage>728</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000970</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Heinze</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>Mangun</surname>, <given-names>G. R.</given-names></string-name>, &amp; <string-name><surname>Hillyard</surname>, <given-names>S. A</given-names></string-name>. (<year>1990</year>). <article-title>Visual event-related potentials index focused attention within bilateral stimulus arrays. II. Functional dissociation of P1 and N1 components</article-title>. <source>Electroencephalography and Clinical Neurophysiology</source>, <volume>75</volume>(<issue>6</issue>), <fpage>528</fpage>–<lpage>542</lpage>. <pub-id pub-id-type="doi">10.1016/0013-4694(90)90139-b</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name>, &amp; <string-name><surname>Hillyard</surname>, <given-names>S. A</given-names></string-name>. (<year>1994</year>). <article-title>Spatial filtering during visual search: Evidence from human electrophysiology</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>20</volume>(<issue>5</issue>), <fpage>1000</fpage>–<lpage>1014</lpage>. <pub-id pub-id-type="doi">10.1037//0096-1523.20.5.1000</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Marti</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Dehaene</surname>, <given-names>S</given-names></string-name>. (<year>2017</year>). <article-title>Discrete and continuous mechanisms of temporal selection in rapid visual streams</article-title>. <source>Nature Communications</source>, <volume>8</volume>(<issue>1</issue>), <fpage>1955</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-017-02079-x</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Mathôt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schreij</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2012</year>). <article-title>OpenSesame: An open-source, graphical experiment builder for the social sciences</article-title>. <source>Behavior Research Methods</source>, <volume>44</volume>(<issue>2</issue>), <fpage>314</fpage>–<lpage>324</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-011-0168-7</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Meijs</surname>, <given-names>E. L.</given-names></string-name>, <string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Slagter</surname>, <given-names>H. A.</given-names></string-name>, <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name>, &amp; <string-name><surname>van Gaal</surname>, <given-names>S.</given-names></string-name> (<year>2019</year>). <article-title>Exploring the role of expectations and stimulus relevance on stimulus-specific neural representations and conscious report</article-title>. <source>Neuroscience of Consciousness</source>, <volume>2019</volume>(<fpage>1</fpage>), niz011. <pub-id pub-id-type="doi">10.1093/nc/niz011</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Nissen</surname>, <given-names>M. J.</given-names></string-name>, &amp; <string-name><surname>Bullemer</surname>, <given-names>P</given-names></string-name>. (<year>1987</year>). <article-title>Attentional requirements of learning: Evidence from performance measures</article-title>. <source>Cognitive Psychology</source>, <volume>19</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/0010-0285(87)90002-8</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Grisel</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Blondel</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Prettenhofer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Weiss</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Dubourg</surname>, <given-names>V</given-names></string-name>. (<year>2011</year>). <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source>, <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.1201.0490</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Peirce</surname>, <given-names>J. W</given-names></string-name>. (<year>2007</year>). <article-title>PsychoPy—Psychophysics software in Python</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>162</volume>(<issue>1-2</issue>), <fpage>8</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Perrin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Pernier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bertrand</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Echallier</surname>, <given-names>J. F</given-names></string-name>. (<year>1989</year>). <article-title>Spherical splines for scalp potential and current density mapping</article-title>. <source>Electroencephalography and Clinical Neurophysiology</source>, <volume>72</volume>(<issue>2</issue>), <fpage>184</fpage>–<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1016/0013-4694(89)90180-6</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Remillard</surname>, <given-names>G</given-names></string-name>. (<year>2003</year>). <article-title>Pure perceptual-based sequence learning. <italic>Journal of Experimental Psychology: Learning</italic></article-title>, <source>Memory, and Cognition</source>, <volume>29</volume>(<issue>4</issue>), <fpage>581</fpage>–<lpage>597</lpage>. <pub-id pub-id-type="doi">10.1037/0278-7393.29.4.581</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Robinson</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Shatek</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Gerboni</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Holcombe</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Carlson</surname>, <given-names>T. A</given-names></string-name>. (<year>2020</year>). <article-title>Overlapping neural representations for the position of visible and imagined objects. <italic>Neurons, Behavior</italic></article-title>, <source>Data Analysis, and Theory</source>, <volume>4</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.2010.09932</pub-id></mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Rolls</surname>, <given-names>E. T</given-names></string-name>. (<year>2016</year>). <article-title>Pattern separation, completion, and categorisation in the hippocampus and neocortex</article-title>. <source>Neurobiology of Learning and Memory</source>, <volume>129</volume>, <fpage>4</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/j.nlm.2015.07.008</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Schwarb</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Schumacher</surname>, <given-names>E. H</given-names></string-name>. (<year>2012</year>). <article-title>Generalized lessons about sequence learning from the study of the serial reaction time task</article-title>. <source>Advances in Cognitive Psychology</source>, <volume>8</volume>(<issue>2</issue>), <fpage>165</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.2478/v10053-008-0113-1</pub-id></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Stachenfeld</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Botvinick</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name><surname>Gershman</surname>, <given-names>S. J</given-names></string-name>. (<year>2017</year>). <article-title>The hippocampus as a predictive map</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>11</issue>), <fpage>1643</fpage>–<lpage>1653</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4650</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Thaler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Schütz</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Goodale</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Gegenfurtner</surname>, <given-names>K. R</given-names></string-name>. (<year>2013</year>). <article-title>What is the best fixation target? The effect of target shape on stability of fixational eye movements</article-title>. <source>Vision Research</source>, <volume>76</volume>, <fpage>31</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bogaerts</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name> (<year>2022</year>). <article-title>What to expect where and when: how statistical learning drives visual selection</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>26</volume>(<issue>10</issue>), <fpage>860</fpage>–<lpage>872</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2022.06.001</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Toh</surname>, <given-names>Y. N.</given-names></string-name>, <string-name><surname>Remington</surname>, <given-names>R. W.</given-names></string-name>, &amp; <string-name><surname>Lee</surname>, <given-names>V. G</given-names></string-name>. (<year>2021</year>). <article-title>Sequence learning is surprisingly fragile in visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>47</volume>(<issue>10</issue>), <fpage>1378</fpage>–<lpage>1394</lpage>. <pub-id pub-id-type="doi">10.1037/xhp0000952</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>van Driel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Olivers</surname>, <given-names>C. N. L.</given-names></string-name>, &amp; <string-name><surname>Fahrenfort</surname>, <given-names>J. J.</given-names></string-name> (<year>2021</year>). <article-title>High-pass filtering artifacts in multivariate classification of neural time series data</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>352</volume>, <fpage>109080</fpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2021.109080</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lampers</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Cordesius</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Slagter</surname>, <given-names>H. A.</given-names></string-name> (<year>2020</year>). <article-title>Neural mechanisms underlying expectation-dependent inhibition of distracting information</article-title>. <source>Elife</source>, <volume>9</volume>, <fpage>e61048</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.61048</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Slagter</surname>, <given-names>H. A.</given-names></string-name> (<year>2019</year>). <article-title>Learning what is irrelevant or relevant: Expectations facilitate distractor inhibition and target facilitation through distinct neural mechanisms</article-title>. <source>The Journal of Neuroscience</source>, <volume>39</volume>(<issue>35</issue>), <fpage>6953</fpage>–<lpage>6967</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0593-19.2019</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>van Moorselaar</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name> (<year>2023</year>). <article-title>Statistical Learning Within Objects</article-title>. <source>Psychological Science</source>, <volume>34</volume>(<issue>4</issue>), <fpage>501</fpage>–<lpage>511</lpage>. <pub-id pub-id-type="doi">10.1177/09567976231154804</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Theeuwes</surname>, <given-names>J</given-names></string-name>. (<year>2018</year>). <article-title>Statistical regularities modulate attentional capture</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>44</volume>(<issue>1</issue>), <fpage>13</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.3758/s13414-018-1562-3</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Weaver</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Fahrenfort</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Belopolsky</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>van Gaal</surname>, <given-names>S.</given-names></string-name> (<year>2019</year>). <article-title>Independent neural activity patterns for sensory- and confidence-based information maintenance during category-selective visual processing</article-title>. <source>eNeuro</source>, <volume>6</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1523/ENEURO.0268-18.2018</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Wolff</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Jochim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Akyurek</surname>, <given-names>E. G.</given-names></string-name>, &amp; <string-name><surname>Stokes</surname>, <given-names>M. G</given-names></string-name>. (<year>2017</year>). <article-title>Dynamic hidden states underlying working-memory-guided behavior</article-title>. <source>Nature Neuroscience</source>, <volume>20</volume>(<issue>6</issue>), <fpage>864</fpage>–<lpage>871</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4546</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Xu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Theeuwes</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Los</surname>, <given-names>S. A</given-names></string-name>. (<year>2022</year>). <article-title>Statistical learning of spatiotemporal regularities dynamically guides visual attention across space. <italic>Attention</italic></article-title>, <source>Perception, and Psychophysics</source>. <pub-id pub-id-type="doi">10.3758/s13414-022-02573-5</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Zelinsky</surname>, <given-names>G. J.</given-names></string-name>, &amp; <string-name><surname>Bisley</surname>, <given-names>J. W</given-names></string-name>. (<year>2015</year>). <article-title>The what, where, and why of priority maps and their interactions with visual working memory</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1339</volume>, <fpage>154</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1111/nyas.12606</pub-id></mixed-citation></ref>
</ref-list>
<sec id="s7">
<title>Supplementary materials</title>
<fig id="fig2s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2—figure supplement 1.</label>
<caption><p>Pattern estimator position (all eight possible positions) decoding results. The decoder was trained and tested at the same time point with posterior electrodes (a), or gaze positions (b) as features. Horizontal dashed lines indicate chance level while vertical dashed lines indicate dot onset. Green and red solid lines represent long- and short-ISI pattern estimators, respectively. Shaded region represents standard error of the mean (SEM). Corresponding color bars below the x-axis indicate clusters that significantly differed from chance while the black bar indicates clusters with a significant difference between long- and short-ISI pattern estimators (<italic>p</italic> &lt; .05). (c) Temporal generalization matrix, with posterior electrodes as features, showing classification performance as a function of training and test time point in long- and short-ISI blocks. Solid black lines indicate clusters that significantly different from chance (<italic>p</italic> &lt; 0.05).</p></caption>
<graphic xlink:href="574347v1_fig2s1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95837.1.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Press</surname>
<given-names>Clare</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University College London</institution>
</institution-wrap>
<city>London</city>
<country>United Kingdom</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study investigates how a predicted - but not presented - stimulus within a sequence is represented in the brain using time-resolved EEG decoding. The results demonstrate that when the predicted stimulus is omitted, it is still represented at the expected space and time, although at present they provide only <bold>incomplete</bold> support given some alternative explanations. The findings will have implications for researchers across domains who are interested in learning and perception.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95837.1.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Li and colleagues describe an experiment whereby sequences of dots in different locations were presented to participants while electroencephalography (EEG) was recorded. By presenting fixed sequences of dots in different locations repeatedly to participants, the authors assumed that participants had learned the sequences during the experiment. The authors also trained classifiers using event-related potential (ERP) data recorded from separate experimental blocks of dots presented in a random (i.e., unpredictable) order. Using these trained classifiers, the authors then assessed whether patterns of brain activity could be detected that resembled the neural response to a dot location that was expected, but not presented. They did this by presenting an additional set of sequences whereby only one of the dots in the learned sequence appeared, but not the other dots. They report that, in these sequences with omitted stimuli, patterns of EEG data resembled the visual response evoked by a dot location for stimuli that could be expected, but were not presented. Importantly, this only occurred for an omitted dot stimulus that would be expected to appear immediately after the dot that was presented in these partial sequences.</p>
<p>This exciting finding complements previous demonstrations of the ability to decode expected (but not presented) stimuli in Blom et al. (2020) and Robinson et al. (2020) that are cited in this manuscript. It suggests that the visual system is able to generate patterns of activity that resemble expected sensory events, approximately at times at which an observer would expect them.</p>
<p>Strengths:</p>
<p>The experiment was carefully designed and care was taken to rule out some confounding factors. For example, gaze location was tracked over time, and deviations from fixation were marked, in order to minimise the contributions of saccades to above-chance decoding of dot position. The use of a separate block of dots (with unpredictable locations) to train the classifiers was also useful in isolating visual responses evoked by each dot location independently of any expectations that might be formed during the experiment. A large amount of data was also collected from each participant, which is important when using classifiers to decode stimulus features from EEG data. This careful approach is commendable and draws on best practices from existing work.</p>
<p>Weaknesses:</p>
<p>While there was clear evidence of careful experiment design, there are some aspects of the data analysis and results that significantly limit the inferences that can be drawn from the data. Both issues raised here relate to the use of pre-stimulus baselines and associated problems. As these issues are somewhat technical and may not be familiar to many readers, I will try to unpack each line of reasoning below. Here, it should be noted that these problems are complex, and similar issues often go undetected even by highly experienced EEG researchers.</p>
<p>Relevant to both issues, the authors derived segments of EEG data relative to the time at which each dot was presented in the sequences (or would have appeared when the stimuli were omitted in the partial sequences). Segments were derived that spanned -100ms to 300ms relative to the actual or expected onset of the dot stimulus. The 300ms post-stimulus time period corresponds to the duration of each dot in the sequence (100ms) plus the inter-stimulus interval (ISI) that was 200ms in duration before the next dot appeared (or would be expected to appear in the partial sequences). Importantly, a pre-stimulus baseline was applied to each of these segments of data, meaning that the average amplitude at each electrode between -100ms and 0ms relative to (actual or expected) stimulus onset was subtracted from each segment of data (i.e., each epoch in common EEG terminology). While this type of baseline subtraction procedure is commonplace in EEG studies, in this study design it is likely to cause problematic effects that could plausibly lead to the patterns of results reported in this manuscript.</p>
<p>First of all, the authors compare event-related potentials (ERPs) evoked by dots in the full as compared to partial sequences, to test a hypothesis relating to attentional tuning. They reported ERP amplitude differences across these conditions, for epochs corresponding to when a dot was presented to a participant (i.e., excluding epochs time-locked to omitted dots). However, these ERP comparisons are complicated by the fact that, in the full sequences, dot presentations are preceded by the presentation of other dots in the sequence. This means that ERPs evoked by the preceding dots in the full sequences will overlap in time with the ERPs corresponding to the dots presented at the zero point in the derived epochs. Importantly, this overlap would not occur in the partial sequence conditions, where only one dot was presented in the sequence. This essentially makes any ERP comparisons between full and partial sequences very difficult to interpret, because it is unclear if ERP differences are simply a product of overlapping ERPs from previously presented dots in the full sequence conditions. For example, there are statistically significant differences observed even in the pre-stimulus baseline period for this ERP analysis, which likely reflects the contributions ERPs evoked by the preceding dots in the full sequences, which are absent in the partial sequences.</p>
<p>The problems with interpreting this data are also compounded by the use of pre-stimulus baselines as described above. Importantly, the use of pre-stimulus baselines relies on the assumption that the ERPs in the baseline period (here, the pre-stimulus period) do not systematically differ across the conditions that are compared (here, the full vs. partial sequences). This assumption is violated due to the overlapping ERPs issue described just above. Accordingly, the use of the pre-stimulus baseline subtraction can produce spurious effects in the time period after stimulus onset (for examples see Feuerriegel &amp; Bode, 2022, Neuroimage). This also makes it very difficult to meaningfully compare the ERPs following dot stimulus onset in these analyses.</p>
<p>The second issue relates to the use of pre-stimulus baselines and concerns the key finding reported in the paper: that EEG patterns corresponding to expected but omitted events can be decoded in the partial sequences. In the partial sequences, there are two critical epochs that were derived: One time-locked to the presentation of the dot, and another that was time-locked to 300ms after the dot was presented (i.e. when the next dot would be expected to appear). The latter epoch was used to test for representations of expected, but omitted, stimulus locations.</p>
<p>For the epochs in which the dots were presented, above-chance decoding can be observed spanning a training time range from around 100-300ms and a testing time range of a similar duration (see the plot in Figure 4b). This plot indicates that, during the time window of around 200-300ms following dot stimulus onset, the position of the dot can be decoded not only from trained classifiers using the same time windows spanning 200-300ms, but also using classifiers trained using earlier time windows of around 100-200ms.</p>
<p>This is important because the 200-300ms time period after dot onset in the partial sequences is the window used for pre-stimulus baseline subtraction when deriving epochs corresponding to the first successor representation (i.e., the first stimulus that might be expected to follow from the presented dot, but did not actually appear). In other words, the 200-300ms time window from dot onset corresponds to the -100 to 0 ms time window in the first successor epochs. Accordingly, the pattern that is indicative of the preceding, actually presented dot position would be subtracted from the EEG data used to test for the successor representation. Notably, the first successor condition would always be in another visual field quadrant (90-degree rotated or the opposite quadrant) as stated in the methods. In other words, the omitted stimulus would be expected to appear in the opposite vertical and/or horizontal visual hemifield as compared to the previously presented dot in these partial sequences.</p>
<p>This is relevant because ERPs tend to show reversed polarity across hemifields. For example, a stimulus presented in the right hemifield will have reversed polarity patterns at the same electrode as compared to an equivalent stimulus presented in the left hemifield (e.g., Supplementary Figure 3 in the comparable study of Blom et al., 2020). By subtracting the ERP patterns evoked by the presented dot in the partial sequences during the time period of 200-300ms (corresponding to the -100 to 0ms baseline window), this would be expected to bias patterns of EEG data in the first successor epochs to resemble stimulus positions in opposite hemifields. This could plausibly produce above-chance decoding accuracy in the time windows identified in Figure 5a, where the training time windows broadly correspond to the periods of above-chance decoding during 200-300ms from dot stimulus onset in Figure 4b.</p>
<p>In other words, the above-chance decoding of the first successor representation may plausibly be an artefact of the pre-stimulus baseline subtraction procedure used when deriving the epochs. This casts some doubt as to whether genuine successor representations were actually detected in the study. Additional tests for successor representations using ERP baselines prior to the presented dot in the partial sequences may be able to get around this, but such analyses were not presented, and the code and data were not accessible at the time of this review.</p>
<p>Although the study is designed well and a great amount of care was taken during the analysis stage, these issues with ERP overlap and baseline subtraction raise some doubts regarding the interpretability of the findings in relation to the analyses currently presented.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95837.1.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors investigated how predicted stimuli are represented in posterior regions of the brain by recording electroencephalography during a visual sequence learning task. After learning the spatial order in which four stimuli were presented within a fixed sequence, participants were shown partial sequences - i.e., sequences in which only one element of the sequence was presented. By examining the decoding accuracy of the omitted stimuli, the authors aimed to investigate whether anticipated stimuli are (pro)actively represented in the expected spatial location at the expected time.</p>
<p>Strengths and Weaknesses:</p>
<p>The study successfully replicated previous findings on omitted stimuli within a predicted sequence (Ekman et al., 2023), while providing novel information regarding the temporal dynamics of predictive representation. Nevertheless, this outcome is not entirely surprising, as similar temporal dynamics were observed in a previous study employing a different task (Kok et al., 2017). The high level of scientific rigor is evident, as demonstrated by the numerous control analyses. Additionally, the results are particularly intriguing in terms of discerning the nature of the prepared representation, spanning from early perceptual to late attentional representations. Unfortunately, this distinction is not investigated in detail, thus allowing for alternative interpretations of the results.</p>
<p>The connection between the findings and the literature on priority maps could benefit from further clarification. There is room for a clearer delineation of how much of the representation can be ascribed to a perceptual prediction mechanism versus an attentional (post-perceptual) spatial cueing process. Although the latter can be readily connected to the concept of a priority map guiding spatial attention, the relationship between the priority map and perceptual prediction remains somewhat ambiguous. Noteworthy, an explanation of the results in terms of spatial cueing does not necessarily require a perceptual predictive mechanism. The significant decoding of the location of the omitted stimulus might be attributed to the preceding stimulus orienting attention towards the following location. While this potential explanation was not explicitly addressed in the study, it presents an intriguing avenue for further investigation.</p>
<p>The study provides valuable insight into how omitted, yet predicted stimuli are represented in the brain and its dynamics. While the research is commendable, addressing the outlined limitations would enhance its impact in the field. Specifically, the spatial location decoding results do not disentangle between perceptual prediction (i.e., the features of the expected stimulus) and attentional processes (i.e., the cueing of the to-be-attended location),</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.95837.1.sa0</article-id>
<title-group>
<article-title>Reviewer #3 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors conducted a time-resolved EEG decoding study where they presented sequences of dot locations (4 locations onscreen) or single elements of those sequences, presented at the correct temporal epoch for if they had been presented in the full sequence. They were interested in examining whether presenting single items would activate representations of the anticipated following events that were never presented. Stimuli were presented for 100 ms and separated by 200 ms ISIs. They also had pattern estimation blocks with 600 ms ISIs. They found indeed, that anticipated events could be decoded at their correct moment in time, although future anticipated elements could not.</p>
<p>The decoding of presented dots was fairly confined to the diagonal of the decoding matrix (training time x testing time), suggesting little temporal generalisation. This was in contrast with successor representations which were temporally more diffuse. The subsequent successor could be decoded but not future successors.</p>
<p>Strengths:</p>
<p>I liked this paper. The design was simple and clean and the implications of the findings are clear. The authors achieved their aims with this design, with the results supporting the conclusions. The findings will be of interest to a range of researchers studying learning and perception mechanisms, as well as the more generic role of prediction in the brain.</p>
<p>Weaknesses:</p>
<p>The sample size is fairly low for an EEG study. The authors justify it according to a previous Hogendoorn study, but not according to effect sizes in that study and particular power values.</p>
<p>For understandable reasons, the long ISI blocks were presented before the main test blocks (I would have made the same decision) but there is the risk that participants then come to expect stimuli at larger temporal separations in the main blocks. I do wonder whether this is part of the reason for the greater temporal generalisation for anticipated event representations.</p>
<p>Additional context:</p>
<p>My memory of Ekman et al. 2017 is that single events (presented at position 1) elicited predictive activation of anticipated future events, but that there was a temporal compression. The present study appears to show no temporal compression but that the representations are activated at the correct moment in time. This seems like a potentially interesting difference and one with mechanistic implications for the field.</p>
</body>
</sub-article>
</article>