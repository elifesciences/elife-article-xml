<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98047</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98047</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98047.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Reconstructing Voice Identity from Noninvasive Auditory Cortex Recordings</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9918-8258</contrib-id>
<name>
<surname>Lamothe</surname>
<given-names>Charly</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<email>charlylmth@gmail.com</email>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8214-6278</contrib-id>
<name>
<surname>Thoret</surname>
<given-names>Etienne</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1137-8669</contrib-id>
<name>
<surname>Trapeau</surname>
<given-names>Régis</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7002-0486</contrib-id>
<name>
<surname>Giordano</surname>
<given-names>Bruno L</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1767-5330</contrib-id>
<name>
<surname>Sein</surname>
<given-names>Julien</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8410-0962</contrib-id>
<name>
<surname>Takerkart</surname>
<given-names>Sylvain</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2982-7127</contrib-id>
<name>
<surname>Ayache</surname>
<given-names>Stéphane</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3696-0321</contrib-id>
<name>
<surname>Artières</surname>
<given-names>Thierry</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">7</xref>
<email>thierry.artieres@lis-lab.fr</email>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7578-6365</contrib-id>
<name>
<surname>Belin</surname>
<given-names>Pascal</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">7</xref>
<email>pascal.belin@univ-amu.fr</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>La Timone Neuroscience Institute UMR 7289, CNRS, Aix-Marseille University</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Laboratoire d’Informatique et Systèmes UMR 7020, CNRS, Aix-Marseille University</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>Perception, Representation, Image, Sound, Music UMR 7061, CNRS</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Institute of Language Communication &amp; the Brain</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043hw6336</institution-id><institution>Centre IRM-INT@CERIMED</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/040baw385</institution-id><institution>École Centrale de Marseille</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3395-7234</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Shinn-Cunningham</surname>
<given-names>Barbara G</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution>
</institution-wrap>
<city>Pittsburgh</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>These authors jointly supervised this work: Thierry Artières, Pascal Belin</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-07-15">
<day>15</day>
<month>07</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-11-20">
<day>20</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98047</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-16">
<day>16</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-19">
<day>19</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.27.582302"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-07-15">
<day>15</day>
<month>07</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98047.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.98047.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98047.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98047.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98047.1.sa0">Reviewer #3 (Public Review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.98047.1.sa4">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Lamothe et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Lamothe et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98047-v2.pdf"/>
<abstract><p>The cerebral processing of voice information is known to engage, in human as well as non-human primates, “temporal voice areas” (TVAs) that respond preferentially to conspecific vocalizations. However, how voice information is represented by neuronal populations in these areas, particularly speaker identity information, remains poorly understood. Here, we used a deep neural network (DNN) to generate a high-level, small-dimension representational space for voice identity—the ‘voice latent space’ (VLS)—and examined its linear relation with cerebral activity via encoding, representational similarity, and decoding analyses. We find that the VLS maps onto fMRI measures of cerebral activity in response to tens of thousands of voice stimuli from hundreds of different speaker identities and better accounts for the representational geometry for speaker identity in the TVAs than in A1. Moreover, the VLS allowed TVA-based reconstructions of voice stimuli that preserved essential aspects of speaker identity as assessed by both machine classifiers and human listeners. These results indicate that the DNN-derived VLS provides high-level representations of voice identity information in the TVAs.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id>
<institution>European Research Council</institution>
</institution-wrap>
</funding-source>
<award-id award-id-type="doi">10.3030/788240</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Public Reviews in eLife, listed here: https://elifesciences.org/reviewed-preprints/98047/reviews#tab-content</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>The human voice carries speech, but is also an “auditory face” that carries much valuable information on the stable physical characteristics of the speaker (hereafter, ‘identity-related’; <xref ref-type="bibr" rid="c9">Belin et al., 2004</xref>, <xref ref-type="bibr" rid="c7">2011</xref>). The ability of listeners to extract identity-related information from a voice such as gender, age, or unique identity even in brief stimuli plays a crucial role in our social interactions. Yet, its neural bases remain poorly understood compared to those of speech processing. Studies over the past two decades have clearly established via complementary neuroimaging techniques that the cerebral processing of voice information involves a set of temporal voice areas (TVAs) in secondary auditory cortical regions of the human (fMRI: <xref ref-type="bibr" rid="c11">Belin et al., 2000</xref>, <xref ref-type="bibr" rid="c48">von Kriegstein et al., 2004</xref>, <xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>; EEG, MEG: <xref ref-type="bibr" rid="c21">Charest et al., 2009</xref>, <xref ref-type="bibr" rid="c15">Capilla et al., 2013</xref>, <xref ref-type="bibr" rid="c6">Barbero et al., 2021</xref>; Electrophysiology: <xref ref-type="bibr" rid="c69">Rupp et al., 2022</xref>, <xref ref-type="bibr" rid="c89">Zhang et al., 2021</xref>) as well as macaque (<xref ref-type="bibr" rid="c67">Petkov et al., 2008</xref>; <xref ref-type="bibr" rid="c14">Bodin et al., 2021</xref>) brain. The TVAs respond more strongly to sounds of voice – with or without speech (<xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>; <xref ref-type="bibr" rid="c69">Rupp et al., 2022</xref>; <xref ref-type="bibr" rid="c77">Trapeau et al., 2022</xref>)—and categorize voice apart from other sounds (<xref ref-type="bibr" rid="c14">Bodin et al., 2021</xref>) but the nature of the information encoded at these stages of cortical processing, especially with respect to speaker identity-related information, remains largely unknown (<xref ref-type="bibr" rid="c13">Blank et al., 2014</xref>; <xref ref-type="bibr" rid="c8">Belin et al., 2018</xref>).</p>
<p>In recent years, deep neural networks (DNNs) have emerged as a powerful tool for representing complex visual data, such as images (<xref ref-type="bibr" rid="c52">LeCun et al., 2015</xref>), videos (<xref ref-type="bibr" rid="c53">Liu et al., 2020</xref>) or audio (Chorowski et al., 2019). In the auditory domain, DNNs have been shown to provide valuable representations—so-called feature or latent spaces—for modeling the cerebral processing of sound (brain encoding) (speech: <xref ref-type="bibr" rid="c43">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="c58">Millet et al., 2022</xref>; <xref ref-type="bibr" rid="c78">Tuckute &amp; Feather, 2023</xref>; semantic content: <xref ref-type="bibr" rid="c16">Caucheteux et al., 2022</xref>; <xref ref-type="bibr" rid="c18">Caucheteux &amp; King, 2022</xref>; <xref ref-type="bibr" rid="c17">Caucheteux et al., 2023</xref>; <xref ref-type="bibr" rid="c31">Giordano et al., 2023</xref>; music: <xref ref-type="bibr" rid="c35">Güçlü et al., 2016</xref>), or reconstructing the stimuli heard by a participant (brain decoding) (<xref ref-type="bibr" rid="c3">Akbari et al., 2019</xref>). They have not yet been used to explain cerebral representations of identity-related information due in part to the focus on speech information (<xref ref-type="bibr" rid="c49">von Kriegstein et al., 2003</xref>).</p>
<p>Here, we addressed this challenge by training a ‘Variational autoencoder’ (VAE; <xref ref-type="bibr" rid="c45">Kingma &amp; Welling., 2014</xref>) DNN to reconstruct voice spectrograms from 182,000 250-ms voice samples from 405 different speaker identities in 8 different languages from the CommonVoice database (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>). Brief (250 ms) samples were used to emphasize speaker identity-related information in voice, already available after a few hundred milliseconds (<xref ref-type="bibr" rid="c74">Schweinberger et al., 1997</xref>; <xref ref-type="bibr" rid="c50">Lavan, 2023</xref>), over linguistic information unfolding over longer periods (word, &gt;350 ms; <xref ref-type="bibr" rid="c57">Mcallister et al., 1994</xref>). While 250 ms is admittedly short compared to standards of, e.g., computational speaker identification that typically uses 2-3 s samples, this short duration is sufficient to allow near-perfect gender classification and performance levels well above chance for speaker discrimination. This brief duration allowed the presentation of many more stimuli to our participants in the scanner while preserving acceptable behavioral and classifier performance levels.</p>
<p>State-of-the-art studies have primarily relied on task-optimized neural networks (i.e., DNN trained using supervised learning to classify a category from the input) to study sensory cortex processes (<xref ref-type="bibr" rid="c87">Yamins &amp; DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="c72">Schrimpf et al., 2018</xref>). They can reach high accuracies in brain encoding (<xref ref-type="bibr" rid="c44">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="c72">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="c37">Han et al., 2019</xref>). However, there is increasing evidence that unsupervised learning, such as that used for the VAE, also provides plausible computational models for investigating brain processing (<xref ref-type="bibr" rid="c39">Higgins et al., 2021</xref>; Zhuang et al., 2021; <xref ref-type="bibr" rid="c58">Millet et al., 2022</xref>; Orhan et al., 2022). Thus, latent spaces derived by a VAE, exploited within encoding, representational similarity, and decoding frameworks, offer a potentially promising tool for investigating the representations of voice stimuli in the secondary auditory cortex (<xref ref-type="bibr" rid="c61">Naselaris et al., 2011</xref>). Autoencoders learn to compress stimuli with high dimensionality into a lower-dimensional space that nonetheless allows reconstruction of the original stimuli via an inverse transformation learned by the second part of the network called the decoder.</p>
<p>We trained such a model to learn to compress spectrotemporal representations of voice samples into a <italic>voice latent space</italic> (VLS). In order to test whether VLS accounts well for cerebral activity in response to voice stimuli, we scanned three healthy volunteers using fMRI to measure an indirect index of their cerebral activity across 10+ hours of scanning each in response to ∼12,000 of the voice samples, denoted <italic>BrainVoice</italic> in the following (different from the ones used to train the DNN). The small number of participants does not allow for generalization at the general population level as in standard fMRI studies. However, it allows testing for replicability as in comparable studies involving 10+ hours of scanning per participant (<xref ref-type="bibr" rid="c80">VanRullen &amp; Reddy, 2019</xref>). Different stimulus sets were used across participants to provide a stringent test of replicability based on subject-level analyses. Stimuli consisted of randomly spliced 250-ms excerpts of speech samples from the CommonVoice database (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>) by 119 speakers in 8 languages.</p>
<p>We first asked how the VLS could account for the brain responses to speaker identities (encoding) measured in A1 and the TVAs in comparison with a linear autoencoder’s latent space (LIN). This approach was chosen to compare a representation learned linearly under similar conditions (same input data, learning algorithm, reconstruction objective and latent space size) with the VLS, which has non-linear transformations and a regularized latent space. For this, we used a general linear model (GLM) of fMRI responses to the speaker identities, resulting in one voxel activity map per speaker (Figure 2-S1). Then, we computed the average VLS coordinates of the fMRI voice stimuli for each speaker identity, which may be seen as a speaker representation in the VLS (see <italic>Identity-based and stimulus-based representations</italic> section). Next, we trained a linear voxel-based encoding model to predict the speaker voxel activity maps from the speaker’s VLS coordinates. As VAE achieves compression through a series of nonlinear transformations (<xref ref-type="bibr" rid="c83">Wetzel, 2017</xref>), we choose to contrast its results with a linear autoencoder’s latent space. This method has previously been applied to fMRI-based image reconstructions (<xref ref-type="bibr" rid="c23">Cowen et al., 2014</xref>; <xref ref-type="bibr" rid="c80">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c59">Mozafari et al., 2020</xref>).</p>
<p>The extent to which the VLS allows linearly predicting the fMRI recordings does not provide insight into the representational geometries, i.e., the differences between the patterns of cerebral activity for speaker identity. We addressed this question by using representational similarity analysis (RSA; <xref ref-type="bibr" rid="c47">Kriegeskorte et al., 2008</xref>) to test which model better accounts for the representational geometry for voice identities in the auditory cortex. Using RSA as a model comparison framework is relevant to examining the brain-model relationship from complementary angles (<xref ref-type="bibr" rid="c26">Diedrichsen &amp; Kriegeskorte, 2017</xref>; <xref ref-type="bibr" rid="c31">Giordano et al., 2023</xref>; <xref ref-type="bibr" rid="c78">Tuckute &amp; Feather, 2023</xref>). We built speaker x speaker representational dissimilarity matrices (RDMs) capturing pairwise differences in cerebral activity or model predictions between all pairs of speakers; then, we examined how well the LIN and VLS-derived RDMs correlated with the cerebral RDMs from A1 and the TVAs.</p>
<p>A robust test of the adequacy of models of brain activity, and a long-standing goal in computational neurosciences, is the reconstruction of a stimulus presented to a participant from the evoked brain responses. While reconstruction of visual stimuli (images, videos) from cerebral activity has been performed by a number of groups (<xref ref-type="bibr" rid="c80">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c59">Mozafari et al., 2020</xref>; <xref ref-type="bibr" rid="c51">Le et al., 2022</xref>; <xref ref-type="bibr" rid="c30">Gaziv et al., 2022</xref>; <xref ref-type="bibr" rid="c24">Dado et al., 2022</xref>; <xref ref-type="bibr" rid="c22">Chen et al., 2023</xref>), validating the DNN-derived representational spaces, comparable work in the auditory domain is scarce, almost exclusively concentrated on linguistic information (<xref ref-type="bibr" rid="c70">Santoro et al., 2017</xref>). <xref ref-type="bibr" rid="c3">Akbari et al. (2019)</xref> used a DNN to reconstruct speech stimuli based on ECoG recording of auditory cortex activity, an invasive method compared to techniques like fMRI. They obtained a good phonetic recognition rate but chance-level gender categorization performance from reconstructed spectrograms and no evaluation of speaker identity discrimination.</p>
<p>Here, we built on the linear relationship uncovered in our encoding analysis between the VLS and the fMRI recordings to invert it and try to predict VLS coordinates from the recorded fMRI data; then, using the decoder, we reconstructed the spectrograms of stimuli presented to the participants (<xref ref-type="bibr" rid="c85">Wu et al., 2006</xref>; <xref ref-type="bibr" rid="c61">Naselaris et al., 2011</xref>). The voice identity information available in the reconstructed stimuli was finally assessed by human listeners using both machine learning classifiers and behavioral tasks (<xref rid="fig4" ref-type="fig">Figure 4</xref>).</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Voice Information in the Voice Latent Space (VLS)</title>
<p><xref rid="fig1" ref-type="fig">Figure 1a</xref> shows the architecture of the VAE, with its encoder that reduces an input spectrogram to a highly compressed, 128-dimension <italic>voice latent space</italic> (VLS) representation and its decoder that reconstructs the spectrogram from this VLS representation. We selected this latent space size as it was the first value that produced satisfactory reconstructions. Points in the VLS correspond to voice samples with different identities and phonetic content. A line segment in the VLS contains points corresponding to perceptual interpolations between its two extremities (<xref rid="fig1" ref-type="fig">Figure 1b</xref>; Supplementary Audio 1). VLS coordinates of samples presented to the participants averaged by speaker identity suggest that a major organizational dimension of the latent space is voice gender (<xref rid="fig1" ref-type="fig">Figure 1b</xref>) (colored by age or language in Figure 1-S1).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>DNN-derived Voice Latent Space (VLS).</title>
<p><bold>a,</bold> Variational autoencoder (VAE) Architecture. Two networks learned complementary tasks. An encoder was trained using 182K voice samples to compress their spectrogram into a 128-dimension representation, the voice latent space (VLS), while a decoder learned the reverse mapping. The network was trained end-to-end by minimizing the difference between the original and reconstructed spectrograms. <bold>b,</bold> Distribution of the 405 speaker identities along the first 2 principal components of the VLS coordinates from all sounds, averaged by speaker identity. Each disk represents a speaker’s identity colored by gender. PC2 largely maps onto voice gender (ANOVAs on the first two components: PC1: F(1, 405)=0.10, p=.74; PC2: F(1, 405)=11.00, p&lt;.001). Large disks represent the average of all male (black) or female (gray) speaker coordinates, with their associated reconstructed spectrograms (note the flat fundamental frequency (f<sub>0</sub>) and formant frequencies contours caused by averaging). The bottom of the spectrograms illustrates an interpolation between stimuli of two different speaker identities: spectrograms at the extremes correspond to two original stimuli (A, B) and their VLS-reconstructed spectrograms (A’, B’). Intermediary spectrograms were reconstructed from linearly interpolated coordinates between those two points in the VLS (red line) (cf. Supplementary Audio 1). <bold>c,d e,</bold> Performance of linear classifiers at categorizing speaker gender (chance level: 50%), age (young/adult, chance level: 50%), or identity (119 identities, chance level: 0.84%) based on VLS or Linear model (LIN) coordinates. Error bars indicate the standard error of the mean (s.e.m) across 5 folds. All ps&lt;0.05. The horizontal black dashed lines indicate chance levels. *: p&lt;0.05.</p></caption>
<graphic xlink:href="582302v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>In order to probe the informational content of the VLS, linear classifiers were trained to categorize the voice stimuli from 405 speakers by gender (2 classes), age (2 classes: before/above 30 years old, the median age in our sample) or identity (119 classes, cf Methods) based on VLS coordinates, or their LIN features (<xref rid="fig1" ref-type="fig">Figure 1c,d,e</xref>; we aggregated the stimuli from the 3 participants; for each model, we computed the latent space of each stimulus and averaged the latent spaces by speaker identity, leading to 405 128-dimensional vectors. We then trained linear classifiers using a 5-fold cross-validation scheme, see <italic>Characterization of the autoencoder latent space</italic>). The decoding accuracy was significantly above chance level (Wilcoxon signed-rank test, all W=15, p=0.03125) for all classifications (LIN: gender (mean accuracy ± s.d.) = 97.64±1.77%; age: 64.39±4.54%; identity: 40.52±9.14%; VLS: gender: 98.59±1.19%; age: 67.31±4.86%; identity: 38.40±8.75%).</p>
<p>Thus, despite its low number of dimensions (each input spectrogram has 401×21=8421 parameters and is summarized in the VLS by a mere 128 dimensions), the VLS appears to meaningfully represent the different sources of voice information perceptually available in the vocal stimuli. This representational space, therefore, constitutes a relevant candidate for linearly modeling voice stimulus representations by the brain.</p>
</sec>
<sec id="s2b">
<title>Brain Encoding</title>
<p>For assessing generalization performances of decoding models and brain-based reconstruction, six test stimuli were repeated more often (60 times) for each participant to provide robust estimates of their induced cerebral activity (see Methods). We first modeled these responses to voice using a general linear model (GLM) (<xref ref-type="bibr" rid="c29">Friston et al., 1994</xref>) with several nuisance regressors as an initial denoising step (Figure 2-S2), then used a second GLM modeling cerebral responses to the different speaker identities (Figure 2-S1a), resulting in one voxel activity map per speaker (Figure 2-S1b). We independently localized in each participant several regions of interest (ROIs) on which subsequent analyses were focused: the anterior, middle and posterior TVAs in each hemisphere (individually localized via an independent ‘voice localizer scan’ and MNI coordinates provided in <xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>; Figure 2-S1c) as well as primary auditory cortex (A1) (using a probabilistic map in MNI space (<xref ref-type="bibr" rid="c65">Penhune et al., 1996</xref>; Figure 1-S2d).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Predicting brain activity from the VLS.</title>
<p><bold>a,</bold> Linear brain activity prediction from VLS for ∼135 speaker identities in the different ROIs. We first fit a GLM to predict the Blood Oxygenation Level-Dependant (BOLD) responses to each voice speaker identity. Then, using the trained encoder, we computed the average VLS coordinates of the voice stimuli presented to the participants based on speaker identity. Finally, we trained a linear voxel-based encoding model to predict the speaker voxel activity maps from the speaker VLS coordinates. The cube illustrates the linear relationship between the fMRI responses to speaker identity and the VLS coordinates. The left face of the cube represents the activity of the voxels for each speaker’s identity, with each line corresponding to one speaker. The right face displays the VLS coordinates for each speaker’s identity. The cube’s top face shows the encoding model’s weight vectors. <bold>b,</bold> Encoding results. For each region of interest, the model’s performance was assessed using the Pearson correlation score between the true and the predicted responses of each voxel on the held-out speaker identities. Pearson’s correlation coefficients were computed for each voxel on the speakers’ axis and then averaged across hemispheres and participants. Similar predictions were tested with the LIN features. Error bars indicate the standard error of the mean (s.e.m) across voxels. *p &lt; 0.05; **p &lt; 0.01; **p &lt; 0.001; ****p &lt; 0.0001. <bold>c,</bold> Venn diagrams of the number of voxels in each ROI with the LIN, the VLS, or both models. For each ROI and each voxel, we checked whether the test correlation was higher than the median of all participant correlations (intersection circle), and if not, which model (LIN or VLS) yielded the highest correlation (left or right circles).</p></caption>
<graphic xlink:href="582302v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We used a linear voxel-based encoding model to test whether VLS linearly maps onto cerebral responses to speaker identities measured with fMRI in the different ROIs. A regularized linear regression model (cf. Methods) was trained on a subset of the data (5-fold cross-validation scheme) to predict the voxel maps for each speaker identity. For each fold, the trained model was tested on the held-out speaker identities (<xref rid="fig2" ref-type="fig">Figure 2a</xref>). The model’s performance was assessed for each ROI using the Pearson correlation score between each voxel’s actual and predicted responses (<xref ref-type="bibr" rid="c71">Schrimpf et al., 2021</xref>). Similar predictions were tested with features derived from LIN, as well as with more recent DNN networks: Wav2Vec and HuBERT (cf. Methods; Fig.2S3). <xref rid="fig2" ref-type="fig">Figure 2b</xref> shows the distribution of correlation coefficients obtained for each of the ROIs for the 2 sets of features across voxels, hemispheres, and participants.</p>
<p>One-sample t-tests showed that the means of Fisher z-transformed coefficients for both LIN features and VLS were significantly higher than zero (LIN: A1 t(197)=7.25, p&lt;.0001, pTVA t(175)=4.49, p&lt;.0001, mTVA t(164)=9.12, p&lt;.0001 and aTVA t(147)=6.81, p&lt;.0001; VLS: A1 t(197)=4.76, p&lt;.0001, mTVA t(164)=10.12, p&lt;.0001 and aTVA t(147)=5.52, p&lt;.0001 but not pTVA t(175)=-1.60) (Supplementary Tables 2-3).</p>
<p>A mixed ANOVA performed on the Fisher z-transformed coefficients with Feature (VLS, LIN) and ROI (A1, pTVA, mTVA, aTVA) as factors showed a significant effect of Feature (F(3, 683)=56.65, p&lt;.0001), a significant effect of ROI (F(3, 683)=18.50, p&lt;.0001), and a moderate interaction Feature x ROI (F(3, 683)=5.25, p&lt;.01). Post-hoc comparisons revealed that the mean of correlation coefficients was higher for LIN than for VLS in A1 (t(197)=4.02, p&lt;.0001), pTVA (t(175)=6.64, p&lt;.0001), aTVA (t(147)=3.78, p&lt;.001) but not in mTVA (t(164)=0.58) (Supplementary Table 4); and that the voxel patterns are better predicted in mTVA than in A1 for both models (LIN: t(361)=2.36, p&lt;.05); VLS: t(361)=4.91, p&lt;.0001) (Supplementary Table 5). However, inspecting the distribution of model-voxel correlations, we found that both models account for different parts of the voice identity responses and differ across ROIs (<xref rid="fig2" ref-type="fig">Figure 2c</xref>).</p>
</sec>
<sec id="s2c">
<title>Representational Similarity Analysis (RSA)</title>
<p>For RSA, we built speaker x speaker representational dissimilarity matrices (RDMs), capturing for each ROI the dissimilarity in voxel space between each pair of speaker voxel maps (‘brain RDMs’; cf. Methods) using Pearson’s correlation (<xref ref-type="bibr" rid="c81">Walther et al., 2016</xref>). We compared these four bilateral brain RDMs (A1, aTVA, mTVA, pTVA) to two ‘model RDMs’ capturing speaker pairwise feature differences predicted by LIN and the VLS (<xref rid="fig3" ref-type="fig">Figure 3a</xref>) built using cosine distance (<xref ref-type="bibr" rid="c86">Xing et al., 2015</xref>; <xref ref-type="bibr" rid="c12">Bhattacharya et al., 2017</xref>; <xref ref-type="bibr" rid="c82">Wang et al., 2018</xref>). <xref rid="fig3" ref-type="fig">Figure 3b</xref> shows for each ROI the Spearman correlation coefficients between the brain RDMs and the two model RDMs for each participant and hemisphere (<xref ref-type="bibr" rid="c47">Kriegeskorte et al., 2008</xref>; <xref rid="fig3" ref-type="fig">Figure 3c</xref> for an example of brain-model correlation).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>The VLS better explains representational geometry for voice identities in the TVAs than the linear model.</title>
<p><bold>a,</bold> Representational dissimilarity matrices (RDMs) of pairwise speaker dissimilarities for ∼135 identities (arranged by gender, cf. sidebars), according to LIN and VLS. <bold>b,</bold> Spearman correlation coefficients between the brain RDMs for A1, the 3 TVAs, and the 2 model RDMs. Error bars indicate the standard error of the mean (s.e.m) across brain-model correlations. <bold>c,</bold> Example of brain-model RDM correlation in the TVAs. The VLS RDM and the brain RDM yielding one of the highest correlations (LaTVA) are shown in the insert.</p></caption>
<graphic xlink:href="582302v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To correct for multiple comparisons across models, these brain-model correlation coefficients were compared to zero using a ‘maximum statistics’ approach based on random permutations of the model RDMs’ rows and columns (<xref ref-type="bibr" rid="c55">Maris &amp; Oostenveld, 2007</xref>; cf. Methods; <xref rid="fig3" ref-type="fig">Figure 3b</xref>). For the LIN model, only one brain-model RDM correlation was significantly different from zero (one-tailed test, corrected for multiple comparisons across models): in mTVA, right hemisphere in S3 (p=.0500). For the VLS model, in contrast, 5 significant brain-model RDM correlations were observed in all four ROIs: in A1, right hemisphere in S3 (p=.0142); pTVA: right hemisphere in S3 (p=.0160); mTVA: left hemisphere in S3 (p=.007); aTVA: left hemispheres in S1 (p=.0417) and S3 (p=.0001) (Supplementary Table 6).</p>
<p>A two-way repeated-measures ANOVA with Feature (VLS, LIN) and ROI (A1, pTVA, mTVA, aTVA) as factors performed on the Fisher z-transformed correlation coefficients showed a tendency towards a significant effect of Feature (F(1, 2)=22.53, p=.04), and no ROI (F(3, 6)=1.79, p=.30) or interaction effects (F(3, 6)=1.94, p=.22). We compared the correlation coefficients between the VLS and LIN models within participants and hemispheres using one-tailed tests, based on the a priori hypothesis that the VLS models would exhibit greater brain-model correlations than the LIN models (cf. Methods). The results revealed two significant differences in one of the three participants, both favoring the VLS model (S3: right pTVA, p=.0366; left aTVA, p=.00175) (Supplementary Table 7).</p>
</sec>
<sec id="s2d">
<title>Decoding and Reconstruction</title>
<p>We finally inverted the brain-VLS relationship to predict linearly VLS coordinates based on fMRI measurements (<xref rid="fig4" ref-type="fig">Figure 4a</xref>; see ‘Brain decoding’ in Methods) and reconstructed via the trained decoder the spectrograms of 18 Test Stimuli (3 participants x 6 stimuli per participant; see <xref rid="fig4" ref-type="fig">Figure 4b</xref>, and Supplementary Audio 2; audio estimated from spectrogram through phase reconstruction).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Reconstructing voice identity from brain recordings.</title>
<p><bold>a,</bold> A linear voxel-based decoding model was used to predict the VLS coordinates of 18 Test Stimuli based on fMRI responses to ∼12,000 Train stimuli in the different ROIs. To reconstruct the audio stimuli from the brain recordings, the predicted VLS coordinates were then fed to the trained decoder to yield reconstructed spectrograms, synthesized into sound waveforms using the Griffin-Lim phase reconstruction algorithm (<xref ref-type="bibr" rid="c34">Griffin &amp; Lim, 1983</xref>). <bold>b,</bold> Reconstructed spectrograms of the stimuli presented to the participants. The left panels show the spectrogram of example original stimuli reconstructed from the VLS, and the right panels show brain-reconstructed spectrograms via LIN or VLS autoencoder-based representations, and SPEC, direct regression from the audio spectrograms (cf. Supplementary Audio 2).</p></caption>
<graphic xlink:href="582302v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We first assessed the nature of the reconstructed stimuli by using a DNN trained to categorize natural audio events (<xref ref-type="bibr" rid="c41">Howard et al., 2017</xref>): all reconstructed versions of the 18 Test Stimuli were categorized as ‘speech’ (1 class out of 521 - no ‘voice’ classes). To evaluate the preservation of voice identity information in the reconstructed voices, pre-trained linear classifiers were used to classify the speaker gender (2 classes), age (2 classes), and identity (17 classes; one identity was shared across participants) of the 18 reconstructed Test Stimuli. The mean of the accuracy distribution obtained across random classifier initializations (20 per ROI) used on the stimuli reconstructed from the induced brain activity was significantly above chance level for gender (LIN: pTVA (mean accuracy ± s.d.): 72.08±5.48, t(39)=25.15; VLS: A1: 61.11±2.15, t(39)=32.25; pTVA: 63.89±2.78, t(39)=31.22), age (LIN: pTVA: 54.58±4.14, t(39)=6.90; aTVA: 63.96±12.55, t(39)=6.94; VLS: pTVA: 65.00±7.26, t(39)=12.89; aTVA: 60.42±5.19, t(39)=12.54) and identity (LIN: A1: 9.20±9.23, t(39)=2.24; pTVA: 9.48±4.90, t(39)=4.59; aTVA: 9.41±6.28, t(39)=3.51; VLS: pTVA: 16.18±7.05, t(39)=9.11; aTVA: 8.23±4.70, t(39)=3.12) (<xref rid="fig5" ref-type="fig">Figure 5a-c</xref>; Supplementary Tables 8-10).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Behavioural and machine classification of the reconstructed stimuli.</title>
<p><bold>a,b,c,</bold> Decoding voice identity information in brain-reconstructed spectrograms. Performance of linear classifiers at categorizing speaker gender (chance level: 50%), age (chance level: 50%), and identity (17 identities, chance level: 5.88%). Error bars indicate s.e.m across 40 random classifier initializations per ROI (instance of classifiers; 2 hemispheres x 20 seeds). The horizontal black dashed line indicates the chance level. The blue and yellow dashed lines indicate the LIN and VLS ceiling levels, respectively. *p &lt; .05; **p &lt; .001, ***p &lt; .001; ****p &lt; .0001. <bold>d,e,f,</bold> Listener performance at categorizing speaker gender (chance level: 50%) and age (chance level: 50%), and at identity discrimination (2 forced choice task, chance level: 50%) in the brain-reconstructed stimuli. Error bars indicate s.e.m across participant scores. The horizontal black dashed line indicates the chance level, while the red, blue, and yellow dashed lines indicate the ceiling levels for the original stimuli, the LIN-reconstructed and the VLS-reconstructed, respectively. *p &lt; .05; **p &lt; .01; ***p &lt; .001, ***p &lt; .0001. <bold>g,</bold> Perceptual ratings of voice naturalness in the brain-reconstructed stimuli’ as assessed by human listeners, between 0 and 100 (zoomed between 5-80). *p &lt; .05, ****p &lt; .0001.</p></caption>
<graphic xlink:href="582302v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Two-way ANOVAs with Feature (VLS, LIN) and ROI (A1, pTVA, mTVA, aTVA) as factors performed on classification accuracy scores (gender, age, identity) revealed for gender classifications significant effects of Feature F(1, 312)=12.82, p&lt;.0005) and ROI (gender: F(3, 312)=245.06, p&lt;.0001; age: F(3, 312)=64.49, p&lt;.0001; identity: F(3, 312)=14.49, p&lt;.0001), as well as Feature x ROI interactions (gender: F(3, 312)=56.74, p&lt;.0001; age: F(3, 312)=4.31, p&lt;.001; identity: F(3, 312)=8.82, p&lt;.0001). Post-hoc paired t-tests indicated that the VLS was better than LIN in preserving gender, age and identity information in at least one TVA compared with A1 (gender: aTVA: t(39)=5.13, p&lt;.0001; age: pTVA: t(39)=9.78, p&lt;.0001; identity: pTVA: t(39)=4.01, p&lt;.0005) (all tests in Supplementary Table 11). Post-hoc two sample t-tests comparing ROIs revealed significant differences in all classifications, in particular with pTVA outperforming other ROIs in gender (LIN: pTVA vs A1: t(78)=22.40, p&lt;.0001; pTVA vs mTVA: t(78)=10.92, p&lt;.0001; pTVA vs aTVA: t(78)=31.47, p&lt;.0001; VLS: pTVA vs A1: t(78)=4.94, p&lt;.0001; pTVA vs mTVA: t(78)=13.96, p&lt;.0001; pTVA vs aTVA: t(78)=22.06, p&lt;.0001), age (LIN: pTVA vs A1: t(78)=7.26, p&lt;.0001; pTVA vs mTVA: t(78)=10.11, p&lt;.0001; VLS: pTVA vs A1: t(78)=5.71, p&lt;.0001; pTVA vs mTVA: t(78)=10.11, p&lt;.0001; pTVA vs aTVA: t(78)=3.21, p&lt;.005) and identity (LIN: pTVA vs mTVA: t(78)=2.27, p&lt;.05; VLS: pTVA vs A1: t(78)=6.45, p&lt;.0001; pTVA vs mTVA: t(78)=6.62, p&lt;.0001; pTVA vs aTVA: t(78)=5.85, p&lt;.0001) (Supplementary Table 12).</p>
<p>We further evaluated voice identity information in the reconstructed stimuli by testing human participants (n=13) in a series of 4 online experiments assessing the reconstructed stimuli on (i) naturalness judgment, (ii) gender categorization, (iii) age categorization, and (iv) speaker categorization (cf. Methods). The naturalness rating task showed that the VLS-reconstructed stimuli sounded more natural compared to LIN-reconstructed ones, as revealed by a two-way repeated-measures ANOVA (factors: Feature and ROI) with a strong effect of Feature (F(1, 12)=53.72, p&lt;.0001) and a small ROI x Feature interaction (F(3, 36)=5.36, p&lt;.005). Post-hoc paired t-tests confirmed the greater naturalness of VLS-reconstructed stimuli in both A1 and the TVAs (all ps&lt;.0001) (<xref rid="fig5" ref-type="fig">Figure 5g</xref>). For the gender task, one-sample t-tests showed that categorization of the reconstructed stimuli was only significantly above chance level for the VLS (A1: (mean accuracy ± s.d.) 55.77±10.84, t(25)=2.66, p&lt;.01; pTVA: 61.75±7.11, t(25)=8.26, p&lt;.0001; aTVA: 55.13±9.23, t(25)=2.78, p&lt;.01). Regarding the age and speaker categorizations, results also indicated that both the LIN- and VLS-reconstructed stimuli yielded above-chance performance in the TVAs (age: LIN: aTVA, 55.77±14.95, t(25)=1.93, p&lt;.05; VLS: aTVA, 63.14±11.82, t(25)=5.56, p&lt;.0001; identity: LIN: pTVA: 54.38±9.34, t(17)=1.93, p&lt;.05; VLS: pTVA: 63.33±6.75, t(17)=8.14, p&lt;.0001) (Supplementary Tables 13-15). Two-way repeated-measures ANOVAs revealed a significant effect of ROI for all categories (gender: F(3, 27)=5.90, p&lt;.05; age: F(3, 36)=14.25, p&lt;.0001; identity: F(3, 24)=38.85, p&lt;.0001), and a Feature effect for gender (F(1, 9)=43.61, p&lt;.0001) and identity (F(1, 8)=14.07, p&lt;.001), but not for age (F(1, 12)=4.01, p=0.07), as well as a ROI x Feature interaction for identity discrimination (F(3, 24)=3.52, p&lt;.05) (Supplementary Tables 16-17 for the model and ROI comparisons).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this study we examined to what extent the cerebral activity elicited by brief voice stimuli can be explained by machine-learned representational spaces, specifically focusing on identity-related information. We trained a linear model and a DNN model to reconstruct 100,000s of short voice samples from 100+ speakers, providing low-dimensional spaces (LIN and VLS), which we related to fMRI measures of cerebral response to thousands of experimental stimuli. We find: (i) that 128 dimensions are sufficient to explain a sizeable portion of the brain activity elicited by the voice samples and yield brain-based voice reconstructions that preserve identity-related information; (ii) that the DNN-derived VLS outperforms the LIN space, particularly in yielding more brain-like representational spaces and more naturalistic voice reconstructions; (iii) that different ROIs have different degrees of brain-model relationship, with marked differences between A1 and the a, m, and pTVAs.</p>
<p>Low-dimensional spaces generated by machine learning have been used to approximate cerebral face representations and reconstruct recognizable faces based on fMRI (<xref ref-type="bibr" rid="c80">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c24">Dado et al., 2022</xref>). In the auditory domain, however, they have mainly been used with a focus on linguistic (speech) information, ignoring identity-related information (but see <xref ref-type="bibr" rid="c3">Akbari et al., 2019</xref>). Here, we applied them to brief voice stimuli–with minimal linguistic content but already rich identity-related information–and found that as little as 128 dimensions account reasonably well for the complexity of cerebral responses to thousands of these voice samples as measured by fMRI (<xref rid="fig2" ref-type="fig">Figure 2</xref>). LIN and VLS both showed brain-like representational geometries, particularly the VLS in the aTVAs (<xref rid="fig3" ref-type="fig">Figure 3</xref>). They made possible what is, to our knowledge, the first fMRI-based voice reconstructions to preserve voice-related identity information such as gender, age, or even individual identity, as indicated by above-chance categorization or discrimination performance by both machine classifiers (<xref rid="fig5" ref-type="fig">Figure 5a-c</xref>) and human listeners (<xref rid="fig5" ref-type="fig">Figure 5d-f</xref>).</p>
<p>Estimation of fMRI responses (encoding) by LIN yielded correlations largely comparable to those by VLS (<xref rid="fig2" ref-type="fig">Figure 2b</xref>), although many voxels were only explained by one or the other space (<xref rid="fig2" ref-type="fig">Figure 2c</xref>). However, in the RSA, VLS yielded higher overall correlations with brain RDMs (<xref rid="fig3" ref-type="fig">Figure 3</xref>), suggesting a representational geometry closer to that instantiated in the brain than LIN. Further, VLS-reconstructed stimuli sounded more natural than the LIN-reconstructed ones (<xref rid="fig5" ref-type="fig">Figure 5g</xref>) and yielded both the best speaker discrimination by listeners (<xref rid="fig5" ref-type="fig">Figure 5f</xref>) and speaker classification by machine classifiers (<xref rid="fig5" ref-type="fig">Figure 5c</xref>). Unlike LIN, which was generated via linear transforms, VLS was obtained through a series of nonlinear transformations (<xref ref-type="bibr" rid="c83">Wetzel, 2017</xref>). The fact that the VLS outperforms LIN in decoding performance indicates that nonlinear transformation is required to better account for the brain representation of voices (<xref ref-type="bibr" rid="c61">Naselaris et al., 2011</xref>; <xref ref-type="bibr" rid="c23">Cowen et al., 2014</xref>; <xref ref-type="bibr" rid="c37">Han et al., 2019</xref>).</p>
<p>Comparisons between ROIs revealed important differences between A1 and the a, m, and pTVAs. For both LIN and VLS, fMRI signal (encoding) predictions were more accurate for the mTVAs than for A1, and for A1 than for the pTVAs (<xref rid="fig2" ref-type="fig">Figure 2b</xref>). The aTVAs yielded the highest correlations with the models in the RSA (<xref rid="fig3" ref-type="fig">Figure 3</xref>). Stimulus reconstructions (<xref rid="fig4" ref-type="fig">Figure 4</xref>) based on the TVAs also yielded better gender, age, and identity classification than those based on A1, with gender and identity best preserved in the pTVA-, and to a lesser extent, in the aTVA-based reconstructions (<xref rid="fig5" ref-type="fig">Figure 5</xref>). These results show that the a and pTVAs not only respond more strongly to vocal sounds than A1, but they also represent identity-related information in voice better than mTVA, which was previously anticipated in some neuroimaging studies (Gender: <xref ref-type="bibr" rid="c20">Charest et al., 2013</xref>; Identity: <xref ref-type="bibr" rid="c10">Belin &amp; Zatorre, 2003</xref>; <xref ref-type="bibr" rid="c54">Maguinness et al., 2018</xref>; <xref ref-type="bibr" rid="c68">Roswandowitz et al., 2018</xref>; <xref ref-type="bibr" rid="c2">Aglieri et al., 2021</xref>). Moreover, several recent studies, using intracranial recordings, either through ECoG electrode grids (<xref ref-type="bibr" rid="c89">Zhang et al., 2021</xref>) or sEEG recordings (<xref ref-type="bibr" rid="c69">Rupp et al., 2022</xref>), found evidence that supports the idea of a hierarchical organization of voice patches in the temporal lobe, where the information flow starts from the mTVA patches and moves in two directions: one from mTVA to the anterior TVA (aTVA) and the other one from mTVA to posterior TVA (pTVA).</p>
<p>Overall, we show that a DNN-derived representational space provides an interesting approximation of the cerebral representations of brief voice stimuli that can preserve identity-related information. We find it remarkable that such results could be obtained to explain sound representations despite the poor temporal resolution of fMRI. Future work combining more complex architectures to time-resolved measures of cerebral activity, such as magneto-encephalography (<xref ref-type="bibr" rid="c25">Défossez et al., 2023</xref>) or ECoG (<xref ref-type="bibr" rid="c62">Pasley et al., 2012</xref>), will likely yield better models of the cerebral representations of voice information.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Experimental procedure overview</title>
<p>Three participants attended 13 MRI sessions each. The first session was dedicated to acquire high-resolution structural data, as well as to identify the voice-selective areas of each participant using a ‘voice localizer’ based on different stimuli than those in the same experiment (<xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>; see below).</p>
<p>The next 12 sessions began with the acquisition of two fast structural scans for inter-session realignment purposes, followed by six functional runs, during which the main stimulus set of the experiment was presented. Each functional run lasted approximately 12 minutes, during which 240 experimental stimuli were presented in a rapid event-related design with a jittered inter-stimulus interval of 2.8-3.2 s.</p>
<p>Participants 1 and 2 attended all scanning sessions (72 functional runs in total); due to unforeseen personal health reasons, Participant 3’s participation was limited to 24 runs.</p>
<p>Participants were instructed to stay still in the scanner while listening to the stimuli. To maintain participants’ awareness during functional scanning, they were asked to press an MRI-compatible button each time they heard the same stimulus two times in a row, a rare event occurring 3% of the time (correct button hits (median accuracy ± s.d.): S1=96.67±7.10, S2=100.00±0.89, S3=95.00±3.68).</p>
<p>Scanning sessions were spaced apart by at least two days to avoid possible auditory fatigue due to the exposure to scanner noise. To ensure that participants’ hearing abilities did not vary across scanning sessions, hearing thresholds were measured before each session using a standard audiometric procedure (<xref ref-type="bibr" rid="c56">Martin &amp; Champlin, 2000</xref>; ISO 2004) and compared with the thresholds obtained prior to the first session.</p>
</sec>
<sec id="s4b">
<title>Participants</title>
<p>This study was part of the project ‘Réseaux du Langage’ and was promoted by the National Center for Scientific Research (CNRS). It has been given favorable approval by the local ethics committee (Comité de Protection des Personnes Sud-Méditerranée) on the date of 13th February 2019. The National Agency for Medicines (ANSM) has been informed of this study, which is registered under the number 2017-A03614-49. Three native French human speakers were scanned (all females, 26-33 years old). Participants gave written informed consent and received a compensation of 40€ per hour for their participation. All were right-handed and no one had a hearing disorder or neurological disease. All participants had normal hearing thresholds below 15 dB HL, for octave frequencies between 0.125 and 8 kHz.</p>
</sec>
<sec id="s4c">
<title>Stimuli</title>
<p>The auditory stimuli were divided into two sequences. One ‘voice localizer’ sequence to identify the voice-selective areas of each participant (<xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>) and ‘main voice stimuli’.</p>
<sec id="s4c1">
<title>Voice localizer stimuli</title>
<p>The voice localizer stimuli consisted of 96 complex sounds of 500ms grouped in four categories of human voice, macaque vocalizations, marmoset vocalizations, and complex non-vocal sounds (more details in <xref ref-type="bibr" rid="c14">Bodin et al., 2021</xref>).</p>
</sec>
<sec id="s4c2">
<title>Main voice stimuli</title>
<p>The main stimulus set consisted of brief human voice sounds sampled from the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>). Stimuli were organized into four main category levels: language (English, French, Spanish, Deutch, Polish, Portuguese, Russian, Chinese), gender (female/male), age (young/adult; young: teenagers and twenties; adult: thirties to sixties included) and identity (S1: 135 identities; S2: 142 identities; S3: 128 identities; ∼44 samples per identity). Throughout the manuscript, the term ‘gender’ rather than ‘sex’ was utilized in reference to the demographic information obtained from the participants of the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>), as it was the terminology employed in the survey (‘male/female/other’). Stimulus sets were different for each participant and the number of stimuli per set also varied slightly (number of unique stimuli: Participant 1, N=6150; Participant 2, N=6148; Participant 3, N=5123). For each participant, six stimuli were selected randomly among the sounds having a higher energy (as measured by the amplitude envelope reaching an arbitrary threshold, likely corresponding to vowels) and were repeated extensively (60 times), to improve the performance of the brain decoding (<xref ref-type="bibr" rid="c80">VanRullen &amp; Reddy, 2019</xref>; <xref ref-type="bibr" rid="c40">Horikawa &amp; Kamitani, 2017</xref>; <xref ref-type="bibr" rid="c19">Chang et al., 2019</xref>); these will be called the “repeated” stimuli hereafter, the remaining stimuli were presented twice. The third participant attended 5 BrainVoice sessions instead of 12, one BrainVoice session corresponding to 1030 stimuli (1024 unique stimuli and 6 ‘test’ stimuli). Specifically, 5270 stimuli were presented to the third participant instead of ∼12,000 for the two others. Among these 5270 stimuli, 5120 unique stimuli were presented once, as for the two other participants, 6 ‘test’ stimuli were presented 25 times (150 trials). The dataset was fully balanced, with an equal number of samples for each combination of language, gender, age, and identity. Furthermore, to minimize potential adaptation effects, the stimuli were also balanced within each run according to these categories, and identity was balanced across sessions.</p>
<p>All stimuli of the main set were resampled at 24414 Hz and adjusted in duration (250 ms). For each stimulus, a fade-in and a fade-out were applied with a 15 ms cosine ramp to their onset and offset, and were normalized by dividing the root mean square amplitude. During fMRI sessions, stimulus presentations were controlled using custom Matlab scripts (Mathworks, Natick, MA, USA) interfaced with an RM1 Mobile Processor (Tucker-David Technologies, Alachua, USA). The auditory stimuli were delivered pseudo-randomly through MRI-compatible earphones (S14, SensiMetrics, USA) with no filtering and at a comfortable sound pressure level of around 85 dB SPL that allowed for clear and intelligible listening.</p>
</sec>
</sec>
<sec id="s4d">
<title>Computational models</title>
<p>We used two computational models to learn representational space for voice signals: Linear Autoencoder (LIN) and Deep Variational Autoencoder (VAE; <xref ref-type="bibr" rid="c45">Kingma &amp; Welling., 2014</xref>). Both are encoder-decoder models that are learnt to reproduce at their output their input while going through a low-dimensional representation space usually called latent space (that we will call <italic>voice latent space</italic> since they are learnt on voice data). The autoencoders were trained on a dataset of 182K sounds from the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>), balanced in gender, language and identity to reduce the bias in the synthesis (<xref ref-type="bibr" rid="c36">Gutierrez et al., 2021</xref>). Both models operate on sounds which were represented as spectrograms that we describe below. These representations were tested in all the encoding/decoding and RSA analyses.</p>
</sec>
<sec id="s4e">
<title>Spectrograms</title>
<p>We used amplitude spectrograms as input for the models that we describe below. Short-term Fourier transforms of the waveform were computed using a sliding window of length 50 ms with a hop size of 12.5 ms (hence an overlap of 37.5 ms) and applying a Hamming window of size 800 samples before computing the Fourier transform of each slice. Only the magnitude of the spectrogram was kept and the phase of the complex representation was removed. At the end, a 250 ms sound is represented by a 21×401 matrix with 21 time steps and 401 frequency bins.</p>
<p>We used a custom code based on <italic>numpy</italic>. <italic>fft</italic> package (<xref ref-type="bibr" rid="c38">Harris et al., 2020</xref>). The size and the overlap between the sliding windows of the spectrogram were chosen to conform with the uncertainty principle between time and frequency resolution. The main constraint was to find a trade-off between accurate phase reconstruction with the Griffin &amp; Lim algorithm (1983) and a reasonable size of the spectrogram.</p>
</sec>
<sec id="s4f">
<title>Deep neural network</title>
<p>We designed a deep variational autoencoder (VAE; <xref ref-type="bibr" rid="c45">Kingma &amp; Welling, 2014</xref>) of 15 layers with an intermediate hidden representation of 128 neurons that we refer to as the <italic>voice latent space</italic> (VLS). In an autoencoder model, the two sub-network components, the <italic>Encoder</italic> and the <italic>Decoder</italic>, are jointly learned on complementary tasks (<xref rid="fig1" ref-type="fig">Figure 1a</xref>). The Encoder network (noted <italic>Enc</italic> hereafter; 7 layers) learns to map an input, <italic>s</italic> (a spectrogram of a sound), onto a (128-dimensional) <italic>voice latent space</italic> representation (<italic>z</italic>; in blue in the middle of <xref rid="fig1" ref-type="fig">Figure 1a</xref>), while the Decoder (noted <italic>Dec</italic> hereafter; 7 layers) aims at reconstructing the spectrogram <italic>s</italic> from <italic>z</italic>. The learning objective of the full model is to make the output spectrogram <italic>Dec</italic>(<italic>Enc</italic>(<italic>s</italic>)) as close as possible to the original one <italic>s</italic>. This reconstruction objective is defined as the L2 loss, ||<italic>Dec</italic>(<italic>Enc</italic>(<italic>s</italic>)) − <italic>s</italic>||². The parameters of the Encoder and of the Decoder are jointly learned using gradient descent to optimize the average L2 loss computed on the training set ∑<sub><italic>s</italic> ∈<italic>Training Set</italic></sub> ||<italic>Dec</italic>(<italic>Enc</italic>(<italic>s</italic>)) − <italic>s</italic>||². We trained this DNN on the Common Voice dataset (<xref ref-type="bibr" rid="c4">Ardila et al., 2020</xref>) according to VAE learning procedure (as explained in <xref ref-type="bibr" rid="c46">Kingma &amp; Welling., 2019</xref>) until convergence (network architecture and particularities of the training procedure are provided in Supplementary Table 1), using the PyTorch python package (<xref ref-type="bibr" rid="c63">Paszke et al., 2019</xref>). Before feeding the spectrograms to the autoencoder, we standardized each of the 401 frequency bands separately by centering all the data corresponding to each frequency band at every time step in all spectrograms, which involved removing their mean, and dividing by their standard deviation. This separate standardization of frequency bands resulted in a smaller reconstruction error compared to standardizing across all the bands.</p>
</sec>
<sec id="s4g">
<title>Linear autoencoder</title>
<p>We trained a linear autoencoder on the same dataset (described above) to serve as a linear baseline. Both the <italic>Encoder</italic> and the <italic>Decoder</italic> networks consisted of a single fully-connected layer, without any activation functions. Similar to the VAE, the latent space obtained from the <italic>Encoder</italic> was a 128-dimensional vector. The parameters of both the <italic>Encoder</italic> and of the <italic>Decoder</italic> were jointly learned using gradient descent to optimize the average L2 loss computed on the training set.</p>
</sec>
<sec id="s4h">
<title>Neuroimaging data acquisition</title>
<p>Participants were scanned using a 3 Tesla Prisma scanner (Siemens Healthcare, Erlangen, Germany) equipped with a 64-channel receiver head-coil. Their movements were monitored during the acquisition using the software FIRMM (<xref ref-type="bibr" rid="c27">Dosenbach et al., 2017</xref>). The whole-head high-resolution structural scan acquired during the first session was a T1-weighted multi-echo MPRAGE (MEMPRAGE) (TR = 2.5 s, TE = 2.53, 4.28, 6.07, 7.86 ms, TI=1000 ms flip angle: 8°, matrix size = 208 × 300 × 320; resolution 0.8 × 0.8 × 0.8 mm<sup>3</sup>, acquisition time: 8min22s). Lower resolution scans acquired during all other sessions were T1-weighted MPRAGE scans (TR = 2.3 s, TE = 2.88 ms, TI=900ms, flip angle: 9°, matrix size = 192 × 240 × 256; resolution 1 × 1 × 1 mm<sup>3</sup>, sparse sampling with 2.8 times undersampling and compressed sensing reconstruction, acquisition time: 2min37). Functional imaging was performed using an EPI sequence (multiband factor = 5, TR = 462 ms, TE = 31.2 ms, flip angle: 45°, matrix size = 84 × 84 × 35, resolution 2.5 × 2.5 × 2.5 mm<sup>3</sup>). Functional slices were oriented parallel to the lateral sulci with a z-axis coverage of 87.5 mm, allowing it to fully cover both the TVAs (<xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>) and the FVAs (Aglieri et al., 2018). The physiological signals (heart rate and respiration) were measured with Siemens’ external sensors.</p>
</sec>
<sec id="s4i">
<title>Pre-processing of neuroimaging data and general linear modeling</title>
<p>Tissue segmentation and brain extraction were performed on the structural scans using the default segmentation procedure of SPM 12 (<xref ref-type="bibr" rid="c5">Ashburner et al., 2012</xref>). The preprocessing of the BOLD responses involved correcting motion, registering inter-runs, detrending and smoothing the data. Each functional volume was realigned to a reference volume taken from a steady period in the session that was spatially the closest to the average of all sessions. Transformation matrices between anatomical and functional data were computed using boundary-based registration (FSL; <xref ref-type="bibr" rid="c75">Smith et al., 2004</xref>). The data were respectively detrended and smoothed using the <italic>nilearn</italic> functions <italic>clean</italic>_<italic>img</italic> and <italic>smooth</italic>_<italic>img</italic> (kernel size of 3mm) (<xref ref-type="bibr" rid="c1">Abraham et al., 2014</xref>), resulting in the matrix <italic>Y</italic> ∈ <italic>R</italic><sup><italic>S</italic> × <italic>V</italic></sup>, with <italic>S</italic> the number of scans and <italic>V</italic> the number of voxels.</p>
<p>A first general linear model (GLM) was fit to regress out the noise by predicting <italic>Y</italic> from a “denoised” design matrix composed of <italic>R</italic> = 38 regressors of nuisance (Figure 2-S2). These regressors of nuisance, also called covariates of no interest, included: 6 head motion parameters (3 variable for the translations, 3 variables for the rotations); 18 ‘RETROICOR’ regressors (<xref ref-type="bibr" rid="c32">Glover et al., 2000</xref>) using the <italic>TAPAS PhysIO</italic> package (<xref ref-type="bibr" rid="c42">Kasper et al., 2017</xref>) (with the hyperparameters set as specified in Snoek et al.) were computed from the physiological signals; 13 regressors modeling slow artifactual trends (sines and cosines, cut frequency of the high-pass filter = 0.01 Hz); and a confound-mean predictor. The design matrix was convolved with a hemodynamic response function (HRF) with a peak at 6 s and an undershoot at 16 s (<xref ref-type="bibr" rid="c33">Glover et al., 1999</xref>), we note the convolved design matrix as <italic>X</italic><sub><italic>d</italic></sub> ∈ <italic>R</italic><sup><italic>S</italic> × <italic>R</italic></sup>. The “denoised” GLM’s parameters <italic>β</italic><sub><italic>d</italic></sub> ∈ <italic>R</italic><sup><italic>R</italic> ×<italic>V</italic></sup> were optimized to minimize the amplitude of the residual <italic>β</italic><sub><italic>d</italic></sub> = <italic>argmin</italic><sub><italic>β</italic>∈<italic>R</italic></sub><sup><italic>R</italic> ×<italic>V</italic></sup> || <italic>Y</italic> − <italic>X</italic><sub><italic>d</italic></sub> <italic>β</italic> ||<sup>2</sup>. We used a lag-1 autoregressive model (ar(1)) to model the temporal structure of the noise (<xref ref-type="bibr" rid="c28">Friston et al., 2002</xref>). The <italic>denoised</italic> BOLD signal <italic>Y</italic><sub><italic>d</italic></sub> was then obtained from the original one according to <italic>Y</italic><sub><italic>d</italic></sub> = <italic>Y</italic> − (<italic>X</italic><sub><italic>d</italic></sub> <italic>β</italic><sub><italic>d</italic></sub>) ∈ <italic>R</italic><sup><italic>S</italic> ×<italic>V</italic></sup>.</p>
<p>A second “stimulus” GLM model was used to predict the denoised responses for each stimulus based on the denoised betas form the first GLM using a design matrix <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="582302v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (which was convolved with a hemodynamic response function, HRF as above) and a parameters matrix <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="582302v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>N</italic><sub><italic>S</italic></sub> stands for the number of stimuli. The last row (resp. column) of <italic>β</italic><sub><italic>s</italic></sub> (resp. <italic>X</italic><sub><italic>s</italic></sub>) stands for a silence condition. Again, <italic>β</italic><sub><italic>s</italic></sub> was learned to minimize the residual <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="582302v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Once learned, each of the first <italic>N</italic><sub><italic>s</italic></sub> line of <italic>β</italic><sub><italic>s</italic></sub> was corrected by subtracting the <italic>(N</italic><sub><italic>s</italic></sub><italic>+1)<sup>th</sup></italic> line, yielding the contrast maps for stimuli <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="582302v3_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We note hereafter <italic>β̃s</italic>[<italic>i</italic>, : ] ∈ <italic>R V</italic> the contrast map for a given stimulus, it is the <italic>i <sup>th</sup></italic> line of <italic>β̃</italic><sub><italic>s</italic></sub>.</p>
<p>A third “identity” GLM was fit to predict the BOLD responses of each voice speaker identity, using a design matrix <inline-formula id="inline-eqn-5"><inline-graphic xlink:href="582302v3_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and a design matrix <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="582302v3_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (which was again convolved with a hemodynamic response function, HRF) where <italic>N</italic><sub><italic>s</italic></sub> stands for the number of unique speakers. Again the last row/column in <italic>β</italic><sub><italic>i</italic></sub> and <italic>X</italic><sub><italic>i</italic></sub> stands for the silent condition. <italic>β</italic><sub><italic>i</italic></sub> is learned to minimize the residual <inline-formula id="inline-eqn-7"><inline-graphic xlink:href="582302v3_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (Figure 2-S1a). Again, the final speaker contrast maps were obtained by contrasting (i.e., subtracting) the regression coefficients in a row of <italic>β</italic><sub><italic>i</italic></sub> with the silence condition (last row; Figure 2-S1a), yielding <inline-formula id="inline-eqn-8"><inline-graphic xlink:href="582302v3_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Here the <italic>j<sup>th</sup></italic> row of <italic>β̃</italic><sub><italic>i</italic></sub>, <italic>β̃</italic><sub><italic>i</italic></sub>[<italic>j</italic>, : ] ∈ <italic>R</italic><sup><italic>V</italic></sup>, represents the amplitude of the BOLD response of the contrast map for speaker <italic>j</italic> (i.e. to all the stimuli from this speaker).</p>
<p>A fourth “localizer” GLM model was used to predict the denoised BOLD responses of each sound category from the <italic>Voice localizer stimuli</italic> presented above. The procedure was similar as described for the two previous GLM models. Once the GLM was learned, we contrasted the human voice category with the other sound categories in order to localize for each participant the posterior Temporal Voice Area (pTVA), medial Temporal Voice Area (mTVA) and anterior Temporal Voice Area (aTVA) in each hemisphere. The center of each TVA corresponded to the local maximum of the voice &gt; non-voice t-map whose coordinates were the closest to the TVAs reported in (<xref ref-type="bibr" rid="c66">Pernet et al., 2015</xref>). The analyses were carried on for each region of interest (ROI) of each hemisphere.</p>
<p>Additionally, we defined for each participant the primary auditory cortex (A1) as the maximum value of the probabilistic map (non-linearly registered to each participant functional space) of Heschl’s gyri provided with the MNI152 template (<xref ref-type="bibr" rid="c65">Penhune et al., 1996</xref>), intersected with the sound vs silence contrast map.</p>
</sec>
<sec id="s4j">
<title>Identity-based and stimulus-based representations</title>
<p>We performed analyses either at the stimulus level, e.g. predicting the neural activity of a participant listening to a given <italic>stimulus</italic> (<italic>β̃</italic><sub><italic>s</italic></sub>’s lines) from the <italic>voice latent space</italic> representation of this stimuli, or at the speaker identity level, e.g. predicting the average neural activity in response to stimuli of a given speaker <italic>identity</italic> (<italic>β̃</italic><sub><italic>i</italic></sub>’s lines) from this speaker’s <italic>voice latent space</italic> representation. The identity-based analyses were used for the characterization of the <italic>voice latent space</italic> (<xref rid="fig1" ref-type="fig">Figure 1</xref>), the brain encoding (<xref rid="fig2" ref-type="fig">Figure 2</xref>), and the representational similarity analysis (<xref rid="fig3" ref-type="fig">Figure 3</xref>), while the stimulus-based analyses were used for the brain decoding analyses (<xref rid="fig4" ref-type="fig">Figure 4</xref>, <xref rid="fig5" ref-type="fig">5</xref>).</p>
<p>We conducted stimulus-based analyses to examine the relationship between stimulus contrast maps in neural activity (<italic>β̃</italic><sub><italic>s</italic></sub>) and the encodings of individual stimulus spectrograms computed by the encoder of an autoencoder model (either linear or deep variational autoencoder) on the computational side. We will note <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="582302v3_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the encodings of stimuli by the LIN model and <inline-formula id="inline-eqn-10"><inline-graphic xlink:href="582302v3_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the encodings of stimuli computed by the VAE model. The encoding of the k<sup>th</sup> stimuli by one of these models is the k<sup>th</sup> row of the corresponding matrix and it is noted as <italic>z</italic><sub><italic>s</italic></sub><sup><italic>model</italic></sup>[<italic>k</italic>, : ].</p>
<p>For identity-based analyses we studied relationships between identity contrast maps in <italic>β̃</italic><sub><italic>i</italic></sub> on the neural activity side, and an encoding of speaker identity in the VLS implemented by an autoencoder model (LIN or VAE) on the computational side, e.g. we note <inline-formula id="inline-eqn-11"><inline-graphic xlink:href="582302v3_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula> the representation of speaker <italic>j</italic> as computed by the <italic>vae</italic> model. We chose to define a speaker identity-based representation as the average of a set of sample-based representations for stimuli from this speaker, e.g. <inline-formula id="inline-eqn-12"><inline-graphic xlink:href="582302v3_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> where <italic>S</italic><sub><italic>j</italic></sub> stands for the set of stimuli by speaker <italic>j</italic> and <italic>model</italic> stands for <italic>vae</italic> or <italic>lin</italic>. Averaging in the <italic>voice latent space</italic> is expected to be much more powerful and relevant than averaging in the input space spectrograms (<xref ref-type="bibr" rid="c80">VanRullen &amp; Reddy, 2019</xref>).</p>
</sec>
<sec id="s4k">
<title>Characterization of the autoencoder latent space</title>
<p>We characterized the organization of the <italic>voice latent space</italic> (VLS) and of the features computed by the linear autoencoder (LIN) by measuring through classification experiments the presence of information about speaker’s gender, age, and identity in the representations learned by these models.</p>
<p>We first computed the speaker’s identity <italic>voice latent space</italic> representations for each of the 405 speakers in the main voice dataset (135+142+128 see <italic>Stimuli</italic> section) as explained above.</p>
<p>Next we used these speakers’ <italic>voice latent space</italic> representation to investigate if the gender, age, identity were encoded in the VLS. To do so, we divided the data into separate train and test sets and learned classifiers to predict gender, age, or identity from the train set. The balanced (to avoid the small effects associated with unbalanced folds) accuracy of the classifiers was then evaluated on the test set. The higher the performance on the test set, the more confident we are that the information is encoded in the VLS. More specifically for each task (gender, age, identity), we trained a Logistic Regression classifier (linear regularized logistic regression; L2 penalty, tol=0.0001, fit_intercept=True, intercept_scaling=1, max_iter=100) using the scikit-learn python package (<xref ref-type="bibr" rid="c64">Pedregosa et al., 2018</xref>).</p>
<p>We performed 5 train-test splits with 80% of the data in the training and 20% in the test set. For each split we used 5-fold cross-validation on the training set to select the optimal value for the regularization hyperparameter C (searching between 10 values logarithmically spaced on the interval [−3, +3]). We then computed the generalization performance on the test set of the model trained on the full training set with the best hyperparameter value. Reported results were then averaged over 5 folds. Note that data were systematically normalized with a scaler fitted on the training set. We used a robust scaling strategy for these experiments (removing the median, then scaling to the quantile range; 25<sup>th</sup> quantile and 75<sup>th</sup> quantile) which occurs to be more relevant with a small training set.</p>
<p>To investigate how speaker identity information is encoded in the latent space representations of speakers’ voices, we computed speaker identity <italic>voice latent space</italic> representations by averaging 20 stimulus-based representations, in order to obtain a limited amount of data per identity that could be distributed across training and test datasets.</p>
<p>We tested whether the mean of the distribution of accuracy scores obtained for 5 folds was significantly above chance level using Wilcoxon signed-rank test.</p>
</sec>
<sec id="s4l">
<title>Brain encoding</title>
<p>We performed encoding experiments on identity-based representations for each of the three participants (<xref rid="fig2" ref-type="fig">Figure 2</xref>). For each participant we learn a regularized linear regression that predicts a speaker-based neural activity, e.g. the <italic>j<sup>th</sup></italic> speaker’s contrast map <italic>β̃</italic><sub><italic>i</italic></sub>[<italic>j</italic>] ∈ <italic>R</italic><sup><italic>V</italic></sup>, from this speaker’s voice latent space representation, that we note <italic>z</italic><sub><italic>i</italic></sub><sup><italic>model</italic></sup>[<italic>j</italic>] ∈ <italic>R</italic><sup>128</sup> (<xref rid="fig2" ref-type="fig">Figure 2a</xref>), where <italic>i</italic> is the voxel index. We carried out these regression analyses for each ROI (A1, pTVA, mTVA, aTVA) in each hemisphere and participant, independently.</p>
<p>The regression model parameters <inline-formula id="inline-eqn-13"><inline-graphic xlink:href="582302v3_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula> were learned according to:
<disp-formula id="disp-eqn-1">
<graphic xlink:href="582302v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>where <italic>λ</italic> is a hyperparameter tuning the optimal tradeoff between the data fit and the penalization terms above. We used the ridge regression with built-in cross-validation as implemented as <italic>RidgeCV</italic> in the scikit-learn library (<xref ref-type="bibr" rid="c64">Pedregosa et al., 2018</xref>).</p>
<p>The statistical significance of each result was assessed using the following procedure: We repeated the following experiment 20 times with different random seeds. Each time, we performed 5 train-test splits with 80% of the data in the training and 20% in the test set. For each split we used RidgeCV (relying on leave-one-out) on the training set to select the optimal value for the hyperparameter <italic>λ</italic> (searching between 10 values logarithmically spaced on the interval [10<sup>−1</sup>; 10<sup>8</sup>]). Following standard practice in machine learning, we then computed the generalization performance on the test set of the model trained on the full training set with the best hyperparameter value. Reported results are then averaged over 20 experiments. Note that here again with small training sets data were systematically normalized in each experiment using robust scaling.</p>
<p>The evaluation relied on the ‘brain score’-inspired procedure (<xref ref-type="bibr" rid="c72">Schrimpf et al., 2018</xref>) which evaluates the performance of the ridge regression with a Pearson’s correlation score. Correlations between measured neural activities <italic>β̃</italic><sub><italic>i</italic></sub> and predicted ones <inline-formula id="inline-eqn-14"><inline-graphic xlink:href="582302v3_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula> were computed for each voxel and averaged over repeated experiments (folds and seeds) yielding one correlation value for every voxel and for every setting. The significance of the results was assessed with one-sample t-tests for the Fisher z-transformed correlation scores (3 x participants x 2 hemispheres x V voxels). For each region of interest, the scores are reported across participants and hemispheres (<xref rid="fig2" ref-type="fig">Figure 2b</xref>). The exact same procedure was followed for the LIN modeling, and for the Wav2Vec and HuBERT models (cf. Fig2S3).</p>
<p>In order to determine which of the two feature spaces (VLS, LIN) and which of the two ROI (A1, TVAs) yielded the best prediction of neural activity, we compared the means of distributions of correlations coefficients using a mixed ANOVA performed on the Fisher z-transformed coefficients (dependent variable: correlation; between factor: ROI; repeated measurements: Feature; between-participant identifier: voxel).</p>
<p>For each ROI, we then used t-tests to perform post-hoc contrasts for the VLS-LIN difference in brain encoding performance (comparison tests in <xref rid="fig2" ref-type="fig">Figure 2b</xref>; Supplementary Table 4). We finally conducted two-sample t-tests between the brain encoding model’s scores trained to predict A1 and those trained to predict temporal voice areas to test the significance of the A1-TVAs difference (Supplementary Table 5).</p>
<p>The statistical tests were all performed using the <italic>pingouin</italic> python package (<xref ref-type="bibr" rid="c79">Vallat, 2018</xref>).</p>
</sec>
<sec id="s4m">
<title>Representational similarity analysis</title>
<p>The RSA analyses were carried out using the package <italic>rsatoolbox</italic> (<xref ref-type="bibr" rid="c73">Schütt et al., 2021</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox">https://github.com/rsagroup/rsatoolbox</ext-link>). For each participant, region of interest and hemisphere, we computed the cerebral Representational Dissimilarity Matrix (RDM) using Pearson’s correlation between the speaker identity-specific response patterns of the GLM estimates <italic>β̃</italic><sub><italic>i</italic></sub> (<xref ref-type="bibr" rid="c81">Walther et al., 2016</xref>) (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). The model RDMs were built using cosine distance (<xref ref-type="bibr" rid="c86">Xing et al., 2015</xref>; <xref ref-type="bibr" rid="c12">Bhattacharya et al., 2017</xref>; <xref ref-type="bibr" rid="c82">Wang et al., 2018</xref>), capturing speaker pairwise feature differences predicted by the computational models LIN and the VLS (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). For greater comparability with the rest of the analyses described here, the GLM estimates and the computational models’ features were first normalized using robust scaling. We computed the Spearman correlations coefficients between the brain RDMs for each ROI, and the two model’s RDMs (<xref rid="fig3" ref-type="fig">Figure 3b</xref>). We assessed the significance of these brain-model correlation coefficients within a permutation-based ‘maximum statistics’ framework for multiple comparison correction (one-tailed inference; N permutations = 10,000 for each test; permutation of rows and columns of distance matrices, see <xref ref-type="bibr" rid="c31">Giordano et al., 2023</xref> and <xref ref-type="bibr" rid="c55">Maris &amp; Oostenveld, 2007</xref>; see <xref rid="fig3" ref-type="fig">Figure 3b</xref>). We evaluated the VLS-LIN difference using a two-way repeated-measures ANOVA on the Fisher z-transformed Spearman correlation coefficients (dependent variable: correlation; within factors: ROI and Feature; participant identifier: participant hemisphere pair). The same permutation framework was also used to assess the significance of the difference between the RSA correlation for the VLS and LIN models.</p>
</sec>
<sec id="s4n">
<title>Brain decoding</title>
<p>Brain decoding was investigated at the stimulus level. The stimuli’s voice latent space representations <italic>z</italic><sub><italic>s</italic></sub><sup><italic>model</italic></sup> ∈ <italic>R</italic><sup><italic>N</italic> × 128</sup> and voice samples’ contrast maps <italic>β̃</italic><sub><italic>s</italic></sub> ∈ <italic>R</italic><sup><italic>N</italic> × <italic>V</italic></sup> were divided into train and test splits, normalized across voice samples using robust scaling, and then fit into the training set. For every participant and each ROI, we trained a <italic>L</italic><sub>2</sub>-regularized linear model <italic>W</italic> ∈ <italic>R</italic><sup><italic>V</italic> × 128</sup> to predict the voice samples’ latent vectors from the voice samples’ contrast maps (<xref rid="fig4" ref-type="fig">Figure 4a</xref>). The hyperparameter selection and optimization were made similarly as in the Brain encoding scheme. Training was performed on non-repeated stimuli (see Stimuli section). We then used the trained models to predict for each participant the 6 repeated stimuli that were the most presented. Waveforms were estimated starting from the reconstructed spectrograms using the Griffin-Lim phase reconstruction algorithm (<xref ref-type="bibr" rid="c34">Griffin &amp; Lim, 1983</xref>).</p>
<p>We then used classifier analyses to assess the presence of voice information (gender, age, speaker identity) in the reconstructed latent representations (i.e., the latent representation predicted from the brain activity of a participant listening to a specific stimulus) (<xref rid="fig5" ref-type="fig">Figure 5a, b, c</xref>). To this purpose, we first trained linear classifiers to categorize the training voice stimuli (participant 1, N = 6144; participant 2, N = 6142; participant 3, N = 5117; total, N = 17403) by gender (2 classes), age (2 classes) or identity (17 classes) based on VLS coordinates. Secondly, we used the previously trained classifiers to predict the identity information based on the VLS derived from the brain responses of the 18 Test voice stimuli (3 participants x 6 stimuli). We first tested using one-sample t-tests that the mean of the distribution of accuracy scores obtained across random classifier initializations of classifiers (2 hemispheres x 20 seeds = 40) was significantly above chance level, for each category, ROI and model. We then evaluated the difference in performance at preserving identity-related information depending on the model or ROI via two-way ANOVAs (dependent variable: accuracy; between factors: Feature and ROI). We performed post-hoc planned paired t-tests between each model pair to test the significance of the VLS-LIN difference. Two-sample t-tests were finally used to test the significance of the A1-TVAs difference.</p>
</sec>
<sec id="s4o">
<title>Listening tests</title>
<p>We recruited 13 participants through the online platform Prolific (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.co">www.prolific.co</ext-link>) for a series of online behavioral experiments. All participants reported having normal hearing. The purpose of these experiments was to evaluate how well voice identity information and naturalness are preserved in fMRI-based reconstructed voice excerpts. In the main session, participants carried out 4 tasks, in the following order: ‘speaker discrimination’ (∼120 min), ‘perceived naturalness’ (∼30 min), ‘gender categorization’ (∼30 min), ‘age categorization’ (∼30 min). The experiment lasted 3 hours and 35 minutes, and each participant was paid £48. 12 participants performed the speaker discrimination task, and all participants performed the other tasks.</p>
<p>Prior to the main experiment session, participants carried out a short loudness-change detection task to ensure that they wore headphones and that they were attentive and properly set up for the main experiment (<xref ref-type="bibr" rid="c84">Woods et al., 2017</xref>). On each of the 12 trials, participants heard 3 tones and were asked to identify which tone was the least loud by clicking on one of 3 response buttons: ‘First’, ‘Second’, or ‘Third’. Participants were admitted to the main experiment only if they achieved perfect performance in this task. We additionally refined the participant pool by excluding those who performed badly on the original stimuli, by retaining only the subjects whose performance was above the 25th percentile of accuracy. (gender and age categorizations: as all participants performed well (<xref rid="fig5" ref-type="fig">Figure 5d, e</xref>, red dotted lines); speaker discrimination: 9/12 participants performed above the threshold of 64%).</p>
<p>The next three tasks were each carried out on the same set of 342 experimental stimuli, each presented on a different trial: 18 original stimuli, 36 stimuli reconstructed directly from the LIN and the VLS models, and 18 stimuli x 2 models x 4 regions of interest x 2 hemispheres= 288 brain-reconstructed stimuli.</p>
<p>In the ‘perceived naturalness’ task, participants were asked to rate how natural the voice sounded on a scale ranging from ‘Not at all natural’ to ‘Highly natural’ (i.e., similar to a real recording), and were instructed to use the full range of the scale.</p>
<p>During the ‘gender categorization’ task, participants categorized the gender by clicking on a ‘Female’ or ‘Male’ button.</p>
<p>Finally, in the ‘age categorization’ task, participants categorized the age of the speaker by clicking on a ‘Younger’ or ‘Older’ button.</p>
<p>In the ‘speaker discrimination’ task, participants carried out 684 trials (342 experimental stimuli x 2) with short breaks in between. On each trial, they were presented with 2 short sound stimuli, one after the other, and participants had to indicate whether they were from the same speaker or not. The speech material was selected randomly and was different between two stimuli.</p>
<p>To evaluate the performance of the participants, we first conducted one-sample t-tests to examine whether the mean accuracy score calculated from their responses was significantly higher than the chance level for each model and ROI. Next, we used two-way repeated-measures ANOVAs to assess the variation in participants’ performances in identifying identity-related information (dependent variable: accuracy; between-participant factors: Feature and ROI). To determine the statistical significance of the VLS-LIN difference, we carried out post-hoc planned paired t-tests between each model pair. Finally, we employed two-sample t-tests to evaluate the statistical significance of the A1-TVAs difference.</p>
</sec>
</sec>

</body>
<back>
<sec id="s5" sec-type="data-availability">
<title>Data and code availability</title>
<p>The preprocessed data and codes are publicly available on Zenodo: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.15797934">https://doi.org/10.5281/zenodo.15797934</ext-link>.</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>We thank Bruno Nazarian for the design of an MRI-compatible button. We thank Jean-Luc Anton and Kepkee Loh for useful discussions. This work was funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement no. 788240). This work was performed in the Center IRM-INT@CERIMED (UMR 7289, AMU-CNRS), platform member of France Life Imaging network (grant ANR-11-INBS-0 0 06). This work, carried out within the Institute of Convergence ILCB (ANR-16-CONV-0002), has benefited from support from the French government (<italic>France 2030</italic>), managed by the French National Agency for Research (ANR) and the Excellence Initiative of Aix-Marseille University (A*MIDEX).</p>
</ack>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>Supplementary Information</label>
<media xlink:href="supplements/582302_file02.pdf"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abraham</surname>, <given-names>Alexandre</given-names></string-name>, <string-name><given-names>Fabian</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Eickenberg</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Gervais</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Mueller</surname></string-name>, <string-name><given-names>Jean</given-names> <surname>Kossaifi</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, and <string-name><given-names>Gael</given-names> <surname>Varoquaux</surname></string-name></person-group>. <year>2014</year>. “<article-title>Machine Learning for Neuroimaging with Scikit-Learn</article-title>.” <source>Frontiers in Neuroinformatics</source> <volume>8</volume>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aglieri</surname>, <given-names>Virginia</given-names></string-name>, <string-name><given-names>Bastien</given-names> <surname>Cagna</surname></string-name>, <string-name><given-names>Lionel</given-names> <surname>Velly</surname></string-name>, <string-name><given-names>Sylvain</given-names> <surname>Takerkart</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name></person-group>. <year>2021</year>. “<article-title>FMRI-Based Identity Classification Accuracy in Left Temporal and Frontal Regions Predicts Speaker Recognition Performance</article-title>.” <source>Scientific Reports</source> <volume>11</volume>(<issue>1</issue>):<fpage>489</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-020-79922-7</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akbari</surname>, <given-names>Hassan</given-names></string-name>, <string-name><given-names>Bahar</given-names> <surname>Khalighinejad</surname></string-name>, <string-name><given-names>Jose L.</given-names> <surname>Herrero</surname></string-name>, <string-name><given-names>Ashesh D.</given-names> <surname>Mehta</surname></string-name>, and <string-name><given-names>Nima</given-names> <surname>Mesgarani</surname></string-name></person-group>. <year>2019</year>. “<article-title>Towards Reconstructing Intelligible Speech from the Human Auditory Cortex</article-title>.” <source>Scientific Reports</source> <volume>9</volume>(<issue>1</issue>):<fpage>874</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-018-37359-z</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ardila</surname>, <given-names>Rosana</given-names></string-name>, <string-name><given-names>Megan</given-names> <surname>Branson</surname></string-name>, <string-name><given-names>Kelly</given-names> <surname>Davis</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Henretty</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Kohler</surname></string-name>, <string-name><given-names>Josh</given-names> <surname>Meyer</surname></string-name>, <string-name><given-names>Reuben</given-names> <surname>Morais</surname></string-name>, <string-name><given-names>Lindsay</given-names> <surname>Saunders</surname></string-name>, <string-name><given-names>Francis M.</given-names> <surname>Tyers</surname></string-name>, and <string-name><given-names>Gregor</given-names> <surname>Weber</surname></string-name></person-group>. <year>2020</year>. “<article-title>Common Voice: A Massively-Multilingual Speech Corpus</article-title>.” <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/arXiv.1912.06670</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ashburner</surname>, <given-names>John</given-names></string-name></person-group>. <year>2012</year>. “<article-title>SPM: A History</article-title>.” <source>NeuroImage</source> <volume>62</volume>(<issue>2</issue>):<fpage>791</fpage>–<lpage>800</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.025</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barbero</surname>, <given-names>Francesca M.</given-names></string-name>, <string-name><given-names>Roberta P.</given-names> <surname>Calce</surname></string-name>, <string-name><given-names>Siddharth</given-names> <surname>Talwar</surname></string-name>, <string-name><given-names>Bruno</given-names> <surname>Rossion</surname></string-name>, and <string-name><given-names>Olivier</given-names> <surname>Collignon</surname></string-name></person-group>. <year>2021</year>. “<article-title>Fast Periodic Auditory Stimulation Reveals a Robust Categorical Response to Voices in the Human Brain</article-title>.” <source>eNeuro</source> <volume>8</volume>(<issue>3</issue>). doi: <pub-id pub-id-type="doi">10.1523/ENEURO.0471-20.2021</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Patricia E. G.</given-names> <surname>Bestelmeyer</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Latinus</surname></string-name>, and <string-name><given-names>Rebecca</given-names> <surname>Watson</surname></string-name></person-group>. <year>2011</year>. “<article-title>Understanding Voice Perception: Understanding Voice Perception</article-title>.” <source>British Journal of Psychology</source> <volume>102</volume>(<issue>4</issue>):<fpage>711</fpage>–<lpage>25</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.2044-8295.2011.02041.x</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Clémentine</given-names> <surname>Bodin</surname></string-name>, and <string-name><given-names>Virginia</given-names> <surname>Aglieri</surname></string-name></person-group>. <year>2018</year>. “<article-title>A ‘Voice Patch’ System in the Primate Brain for Processing Vocal Information?</article-title>” <source>Hearing Research</source> <volume>366</volume>:<fpage>65</fpage>–<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.heares.2018.04.010</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Shirley</given-names> <surname>Fecteau</surname></string-name>, and <string-name><given-names>Catherine</given-names> <surname>Bédard</surname></string-name></person-group>. <year>2004</year>. “<article-title>Thinking the Voice: Neural Correlates of Voice Perception</article-title>.” <source>Trends in Cognitive Sciences</source> <volume>8</volume>(<issue>3</issue>):<fpage>129</fpage>–<lpage>35</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2004.01.008</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, and <string-name><given-names>Robert J.</given-names> <surname>Zatorre</surname></string-name></person-group>. <year>2003</year>. “<article-title>Adaptation to Speaker’s Voice in Right Anterior Temporal Lobe</article-title>:” <source>NeuroReport</source> <volume>14</volume>(<issue>16</issue>):<fpage>2105</fpage>–<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1097/00001756-200311140-00019</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belin</surname>, <given-names>Pascal</given-names></string-name>, <string-name><given-names>Robert J.</given-names> <surname>Zatorre</surname></string-name>, <string-name><given-names>Philippe</given-names> <surname>Lafaille</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Ahad</surname></string-name>, and <string-name><given-names>Bruce</given-names> <surname>Pike</surname></string-name></person-group>. <year>2000</year>. “<article-title>Voice-Selective Areas in Human Auditory Cortex</article-title>.” <source>Nature</source> <volume>403</volume>(<issue>6767</issue>):<fpage>309</fpage>–<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1038/35002078</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Bhattacharya</surname>, <given-names>Gautam</given-names></string-name>, <string-name><given-names>Jahangir</given-names> <surname>Alam</surname></string-name>, and <string-name><given-names>Patrick</given-names> <surname>Kenny</surname></string-name></person-group>. <year>2017</year>. “<article-title>Deep Speaker Embeddings for Short-Duration Speaker Verification</article-title>.” Pp. <fpage>1517</fpage>–<lpage>21</lpage> in <conf-name>Interspeech</conf-name> 2017. <publisher-name>ISCA</publisher-name> <pub-id pub-id-type="doi">10.21437/Interspeech.2017-1575</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blank</surname>, <given-names>Helen</given-names></string-name>, <string-name><given-names>Nuri</given-names> <surname>Wieland</surname></string-name>, and <string-name><given-names>Katharina</given-names> <surname>von Kriegstein</surname></string-name></person-group>. <year>2014</year>. “<article-title>Person Recognition and the Brain: Merging Evidence from Patients and Healthy Individuals</article-title>.” <source>Neuroscience &amp; Biobehavioral Reviews</source> <volume>47</volume>:<fpage>717</fpage>–<lpage>34</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.10.022</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bodin</surname>, <given-names>Clémentine</given-names></string-name>, <string-name><given-names>Régis</given-names> <surname>Trapeau</surname></string-name>, <string-name><given-names>Bruno</given-names> <surname>Nazarian</surname></string-name>, <string-name><given-names>Julien</given-names> <surname>Sein</surname></string-name>, <string-name><given-names>Xavier</given-names> <surname>Degiovanni</surname></string-name>, <string-name><given-names>Joël</given-names> <surname>Baurberg</surname></string-name>, <string-name><given-names>Emilie</given-names> <surname>Rapha</surname></string-name>, <string-name><given-names>Luc</given-names> <surname>Renaud</surname></string-name>, <string-name><given-names>Bruno L.</given-names> <surname>Giordano</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name></person-group>. <year>2021</year>. “<article-title>Functionally Homologous Representation of Vocalizations in the Auditory Cortex of Humans and Macaques</article-title>.” <source>Current Biology</source> <volume>S0960982221011477</volume>. doi: <pub-id pub-id-type="doi">10.1016/j.cub.2021.08.043</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Capilla</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Gross</surname></string-name></person-group>. <year>2013</year>. “<article-title>The Early Spatio-Temporal Correlates and Task Independence of Cerebral Voice Processing Studied with MEG</article-title>.” <source>Cerebral Cortex</source> <volume>23</volume>(<issue>6</issue>):<fpage>1388</fpage>–<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhs119</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caucheteux</surname>, <given-names>Charlotte</given-names></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name></person-group>. <year>2022</year>. “<article-title>Deep Language Algorithms Predict Semantic Comprehension from Brain Activity</article-title>.” <source>Scientific Reports</source> <volume>12</volume>(<issue>1</issue>):<fpage>16327</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-022-20460-9</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caucheteux</surname>, <given-names>Charlotte</given-names></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name></person-group>. <year>2023</year>. “<article-title>Evidence of a Predictive Coding Hierarchy in the Human Brain Listening to Speech</article-title>.” <source>Nature Human Behaviour</source> <fpage>1</fpage>–<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41562-022-01516-2</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caucheteux</surname>, <given-names>Charlotte</given-names></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name></person-group>. <year>2022</year>. “<article-title>Brains and Algorithms Partially Converge in Natural Language Processing</article-title>.” <source>Communications Biology</source> <volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s42003-022-03036-1</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>Nadine</given-names></string-name>, <string-name><given-names>John A.</given-names> <surname>Pyles</surname></string-name>, <string-name><given-names>Austin</given-names> <surname>Marcus</surname></string-name>, <string-name><given-names>Abhinav</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>Michael J.</given-names> <surname>Tarr</surname></string-name>, and <string-name><given-names>Elissa M.</given-names> <surname>Aminoff</surname></string-name></person-group>. <year>2019</year>. “<article-title>BOLD5000, a Public fMRI Dataset While Viewing 5000 Visual Images</article-title>.” <source>Scientific Data</source> <volume>6</volume>(<issue>1</issue>):<fpage>49</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-019-0052-3</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name>, <string-name><given-names>C.</given-names> <surname>Pernet</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Latinus</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Crabbe</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Belin</surname></string-name></person-group>. <year>2013</year>. “<article-title>Cerebral Processing of Voice Gender Studied Using a Continuous Carryover fMRI Design</article-title>.” <source>Cerebral Cortex</source> <volume>23</volume>(<issue>4</issue>):<fpage>958</fpage>–<lpage>66</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/bhs090</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Charest</surname>, <given-names>Ian</given-names></string-name>, <string-name><given-names>Cyril R.</given-names> <surname>Pernet</surname></string-name>, <string-name><given-names>Guillaume A.</given-names> <surname>Rousselet</surname></string-name>, <string-name><given-names>Ileana</given-names> <surname>Quiñones</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Latinus</surname></string-name>, <string-name><given-names>Sarah</given-names> <surname>Fillion-Bilodeau</surname></string-name>, <string-name><given-names>Jean-Pierre</given-names> <surname>Chartrand</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name></person-group>. <year>2009</year>. “<article-title>Electrophysiological Evidence for an Early Processing of Human Voices</article-title>.” <source>BMC Neuroscience</source> <volume>10</volume>(<issue>1</issue>):<fpage>127</fpage>. doi: <pub-id pub-id-type="doi">10.1186/1471-2202-10-127</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>Zijiao</given-names></string-name>, <string-name><given-names>Jiaxin</given-names> <surname>Qing</surname></string-name>, <string-name><given-names>Tiange</given-names> <surname>Xiang</surname></string-name>, <string-name><given-names>Wan Lin</given-names> <surname>Yue</surname></string-name>, and <string-name><given-names>Juan Helen</given-names> <surname>Zhou</surname></string-name></person-group>. <year>2023</year>. “<article-title>Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding</article-title>.” Pp. <fpage>22710</fpage>–<lpage>20</lpage> in <conf-name>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. <publisher-loc>Vancouver, BC, Canada</publisher-loc>: <publisher-name>IEEE</publisher-name>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cowen</surname>, <given-names>Alan S.</given-names></string-name>, <string-name><given-names>Marvin M.</given-names> <surname>Chun</surname></string-name>, and <string-name><given-names>Brice A.</given-names> <surname>Kuhl</surname></string-name></person-group>. <year>2014</year>. “<article-title>Neural Portraits of Perception: Reconstructing Face Images from Evoked Brain Activity</article-title>.” <source>NeuroImage</source> <volume>94</volume>:<fpage>12</fpage>–<lpage>22</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.018</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dado</surname>, <given-names>Thirza</given-names></string-name>, <string-name><given-names>Yağmur</given-names> <surname>Güçlütürk</surname></string-name>, <string-name><given-names>Luca</given-names> <surname>Ambrogioni</surname></string-name>, <string-name><given-names>Gabriëlle</given-names> <surname>Ras</surname></string-name>, <string-name><given-names>Sander</given-names> <surname>Bosch</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>van Gerven</surname></string-name>, and <string-name><given-names>Umut</given-names> <surname>Güçlü</surname></string-name></person-group>. <year>2022</year>. “<article-title>Hyperrealistic Neural Decoding for Reconstructing Faces from fMRI Activations via the GAN Latent Space</article-title>.” <source>Scientific Reports</source> <volume>12</volume>(<issue>1</issue>):<fpage>141</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-021-03938-w</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Défossez</surname>, <given-names>Alexandre</given-names></string-name>, <string-name><given-names>Charlotte</given-names> <surname>Caucheteux</surname></string-name>, <string-name><given-names>Jérémy</given-names> <surname>Rapin</surname></string-name>, <string-name><given-names>Ori</given-names> <surname>Kabeli</surname></string-name>, and <string-name><given-names>Jean-Rémi</given-names> <surname>King</surname></string-name></person-group>. <year>2023</year>. “<article-title>Decoding Speech Perception from Non-Invasive Brain Recordings</article-title>.” <source>Nature Machine Intelligence</source> <volume>5</volume>(<issue>10</issue>):<fpage>1097</fpage>–<lpage>1107</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s42256-023-00714-5</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diedrichsen</surname>, <given-names>Jörn</given-names></string-name>, and <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name></person-group>. <year>2017</year>. “<article-title>Representational Models: A Common Framework for Understanding Encoding, Pattern-Component, and Representational-Similarity Analysis</article-title>.” <source>PLOS Computational Biology</source> <volume>13</volume>(<issue>4</issue>):<fpage>e1005508</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005508</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dosenbach</surname>, <given-names>Nico U. F.</given-names></string-name>, <string-name><given-names>Jonathan M.</given-names> <surname>Koller</surname></string-name>, <string-name><given-names>Eric A.</given-names> <surname>Earl</surname></string-name>, <string-name><given-names>Oscar</given-names> <surname>Miranda-Dominguez</surname></string-name>, <string-name><given-names>Rachel L.</given-names> <surname>Klein</surname></string-name>, <string-name><given-names>Andrew N.</given-names> <surname>Van</surname></string-name>, <string-name><given-names>Abraham Z.</given-names> <surname>Snyder</surname></string-name>, <string-name><given-names>Bonnie J.</given-names> <surname>Nagel</surname></string-name>, <string-name><given-names>Joel T.</given-names> <surname>Nigg</surname></string-name>, <string-name><given-names>Annie L.</given-names> <surname>Nguyen</surname></string-name>, <string-name><given-names>Victoria</given-names> <surname>Wesevich</surname></string-name>, <string-name><given-names>Deanna J.</given-names> <surname>Greene</surname></string-name>, and <string-name><given-names>Damien A.</given-names> <surname>Fair</surname></string-name></person-group>. <year>2017</year>. “<article-title>Real-Time Motion Analytics during Brain MRI Improve Data Quality and Reduce Costs</article-title>.” <source>NeuroImage</source> <volume>161</volume>:<fpage>80</fpage>–<lpage>93</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.025</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><given-names>D. E.</given-names> <surname>Glaser</surname></string-name>, <string-name><given-names>R. N. A.</given-names> <surname>Henson</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kiebel</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Phillips</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Ashburner</surname></string-name></person-group>. <year>2002</year>. “<article-title>Classical and Bayesian Inference in Neuroimaging: Applications</article-title>.” <source>NeuroImage</source> <volume>16</volume>(<issue>2</issue>):<fpage>484</fpage>–<lpage>512</lpage>. doi: <pub-id pub-id-type="doi">10.1006/nimg.2002.1091</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><given-names>A. P.</given-names> <surname>Holmes</surname></string-name>, <string-name><given-names>K. J.</given-names> <surname>Worsley</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Poline</surname></string-name>, <string-name><given-names>C. D.</given-names> <surname>Frith</surname></string-name>, and <string-name><given-names>R. S. J.</given-names> <surname>Frackowiak</surname></string-name></person-group>. <year>1994</year>. “<article-title>Statistical Parametric Maps in Functional Imaging: A General Linear Approach</article-title>.” <source>Human Brain Mapping</source> <volume>2</volume>(<issue>4</issue>):<fpage>189</fpage>–<lpage>210</lpage>. doi: <pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gaziv</surname>, <given-names>Guy</given-names></string-name>, <string-name><given-names>Roman</given-names> <surname>Beliy</surname></string-name>, <string-name><given-names>Niv</given-names> <surname>Granot</surname></string-name>, <string-name><given-names>Assaf</given-names> <surname>Hoogi</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Strappini</surname></string-name>, <string-name><given-names>Tal</given-names> <surname>Golan</surname></string-name>, and <string-name><given-names>Michal</given-names> <surname>Irani</surname></string-name></person-group>. <year>2022</year>. “<article-title>Self-Supervised Natural Image Reconstruction and Large-Scale Semantic Classification from Brain Activity</article-title>.” <source>NeuroImage</source> <volume>254</volume>:<fpage>119121</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119121</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giordano</surname>, <given-names>Bruno L.</given-names></string-name>, <string-name><given-names>Michele</given-names> <surname>Esposito</surname></string-name>, <string-name><given-names>Giancarlo</given-names> <surname>Valente</surname></string-name>, and <string-name><given-names>Elia</given-names> <surname>Formisano</surname></string-name></person-group>. <year>2023</year>. “<article-title>Intermediate Acoustic-to-Semantic Representations Link Behavioral and Neural Responses to Natural Sounds</article-title>.” <source>Nature Neuroscience</source> <fpage>1</fpage>–<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, <string-name><given-names>T. Q.</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Ress</surname></string-name></person-group>. <year>2000</year>. “<article-title>Image-Based Method for Retrospective Correction of Physiological Motion Effects in fMRI: RETROICOR</article-title>.” <source>Magnetic Resonance in Medicine</source> <volume>44</volume>(<issue>1</issue>):<fpage>162</fpage>–<lpage>67</lpage>. doi: <pub-id pub-id-type="doi">10.1002/1522-2594(200007)44:1&lt;162::aid-mrm23&gt;3.0.co;2-e</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glover</surname>, <given-names>Gary H</given-names></string-name></person-group>. <year>1999</year>. “<article-title>Deconvolution of Impulse Response in Event-Related BOLD fMRI1</article-title>.” <source>NeuroImage</source> <volume>9</volume>(<issue>4</issue>):<fpage>416</fpage>–<lpage>29</lpage>. doi: <pub-id pub-id-type="doi">10.1006/nimg.1998.0419</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Griffin</surname>, <given-names>D.</given-names></string-name> and <string-name><given-names>Jae</given-names> <surname>Lim</surname></string-name></person-group>. <year>1983</year>. “<article-title>Signal Estimation from Modified Short-Time Fourier Transform</article-title>.” Pp. <fpage>804</fpage>–<lpage>7</lpage> in <conf-name>ICASSP ‘83. IEEE International Conference on Acoustics, Speech, and Signal Processing</conf-name>. Vol. <volume>8</volume>. <publisher-loc>Boston, MASS, USA</publisher-loc>: <publisher-name>Institute of Electrical and Electronics Engineers</publisher-name>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Güçlü</surname>, <given-names>Umut</given-names></string-name>, <string-name><given-names>Jordy</given-names> <surname>Thielen</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Hanke</surname></string-name>, <string-name><given-names>M. A. J.</given-names> <surname>van Gerven</surname></string-name>, and <string-name><given-names>Marcel A. J.</given-names> <surname>van Gerven</surname></string-name></person-group>. <year>2016</year>. “<article-title>Brains on Beats</article-title>.” In <conf-name>Proceedings of the International Conference on Neural Information Processing Systems</conf-name> <fpage>2101</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gutierrez</surname>, <given-names>Miren</given-names></string-name></person-group>. <year>2021</year>. “<article-title>Algorithmic Gender Bias and Audiovisual Data: A Research Agenda</article-title>.” <source>International Journal of Communication</source> <volume>15</volume>:<fpage>439</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname>, <given-names>Kuan</given-names></string-name>, <string-name><given-names>Haiguang</given-names> <surname>Wen</surname></string-name>, <string-name><given-names>Junxing</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>Kun-Han</given-names> <surname>Lu</surname></string-name>, <string-name><surname>Yizhen Zhang</surname>, <given-names>Di Fu</given-names></string-name>, and <string-name><given-names>Zhongming</given-names> <surname>Liu</surname></string-name></person-group>. <year>2019</year>. “<article-title>Variational Autoencoder: An Unsupervised Model for Encoding and Decoding fMRI Activity in Visual Cortex</article-title>.” <source>NeuroImage</source> <volume>198</volume>:<fpage>125</fpage>–<lpage>36</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.039</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname>, <given-names>Charles R.</given-names></string-name>, <string-name><given-names>K.</given-names> <surname>Jarrod Millman</surname></string-name>, <string-name><given-names>Stéfan J.</given-names> <surname>van der Walt</surname></string-name>, <string-name><given-names>Ralf</given-names> <surname>Gommers</surname></string-name>, <string-name><given-names>Pauli</given-names> <surname>Virtanen</surname></string-name>, <string-name><given-names>David</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>Eric</given-names> <surname>Wieser</surname></string-name>, <string-name><given-names>Julian</given-names> <surname>Taylor</surname></string-name>, <string-name><given-names>Sebastian</given-names> <surname>Berg</surname></string-name>, <string-name><given-names>Nathaniel J.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>Robert</given-names> <surname>Kern</surname></string-name>, <string-name><given-names>Matti</given-names> <surname>Picus</surname></string-name>, <string-name><given-names>Stephan</given-names> <surname>Hoyer</surname></string-name>, <string-name><given-names>Marten H.</given-names> <surname>van Kerkwijk</surname></string-name>, <string-name><given-names>Matthew</given-names> <surname>Brett</surname></string-name>, <string-name><given-names>Allan</given-names> <surname>Haldane</surname></string-name>, <string-name><given-names>Jaime Fernández</given-names> <surname>del Río</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Wiebe</surname></string-name>, <string-name><given-names>Pearu</given-names> <surname>Peterson</surname></string-name>, <string-name><given-names>Pierre Gérard-</given-names> <surname>Marchant</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Sheppard</surname></string-name>, <string-name><given-names>Tyler</given-names> <surname>Reddy</surname></string-name>, <string-name><given-names>Warren</given-names> <surname>Weckesser</surname></string-name>, <string-name><given-names>Hameer</given-names> <surname>Abbasi</surname></string-name>, <string-name><given-names>Christoph</given-names> <surname>Gohlke</surname></string-name>, and <string-name><given-names>Travis E.</given-names> <surname>Oliphant</surname></string-name></person-group>. <year>2020</year>. “<article-title>Array Programming with NumPy</article-title>.” <source>Nature</source> <volume>585</volume>(<issue>7825</issue>):<fpage>357</fpage>–<lpage>62</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Higgins</surname>, <given-names>Irina</given-names></string-name>, <string-name><given-names>Le</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>Victoria</given-names> <surname>Langston</surname></string-name>, <string-name><given-names>Demis</given-names> <surname>Hassabis</surname></string-name>, <string-name><given-names>Christopher</given-names> <surname>Summerfield</surname></string-name>, <string-name><given-names>Doris</given-names> <surname>Tsao</surname></string-name>, and <string-name><given-names>Matthew</given-names> <surname>Botvinick</surname></string-name></person-group>. <year>2021</year>. “<article-title>Unsupervised Deep Learning Identifies Semantic Disentanglement in Single Inferotemporal Face Patch Neurons</article-title>.” <source>Nature Communications</source> <volume>12</volume>(<issue>1</issue>):<fpage>6456</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41467-021-26751-5</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Horikawa</surname>, <given-names>Tomoyasu</given-names></string-name>, and <string-name><given-names>Yukiyasu</given-names> <surname>Kamitani</surname></string-name></person-group>. <year>2017</year>. “<article-title>Generic Decoding of Seen and Imagined Objects Using Hierarchical Visual Features</article-title>.” <source>Nature Communications</source> <volume>8</volume>(<issue>1</issue>):<fpage>15037</fpage>. doi: <pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Howard</surname>, <given-names>Andrew G.</given-names></string-name>, <string-name><given-names>Menglong</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>Bo</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Dmitry</given-names> <surname>Kalenichenko</surname></string-name>, <string-name><given-names>Weijun</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Tobias</given-names> <surname>Weyand</surname></string-name>, <string-name><given-names>Marco</given-names> <surname>Andreetto</surname></string-name>, and <string-name><given-names>Hartwig</given-names> <surname>Adam</surname></string-name></person-group>. <year>2017</year>. “<article-title>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</article-title>.” <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/arXiv.1704.04861</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kasper</surname>, <given-names>Lars</given-names></string-name>, <string-name><given-names>Steffen</given-names> <surname>Bollmann</surname></string-name>, <string-name><given-names>Andreea O.</given-names> <surname>Diaconescu</surname></string-name>, <string-name><given-names>Chloe</given-names> <surname>Hutton</surname></string-name>, <string-name><given-names>Jakob</given-names> <surname>Heinzle</surname></string-name>, <string-name><given-names>Sandra</given-names> <surname>Iglesias</surname></string-name>, <string-name><given-names>Tobias U.</given-names> <surname>Hauser</surname></string-name>, <string-name><given-names>Miriam</given-names> <surname>Sebold</surname></string-name>, <string-name><given-names>Zina-Mary</given-names> <surname>Manjaly</surname></string-name>, <string-name><given-names>Klaas P.</given-names> <surname>Pruessmann</surname></string-name>, and <string-name><given-names>Klaas E.</given-names> <surname>Stephan</surname></string-name></person-group>. <year>2017</year>. “<article-title>The PhysIO Toolbox for Modeling Physiological Noise in fMRI Data</article-title>.” <source>Journal of Neuroscience Methods</source> <volume>276</volume>:<fpage>56</fpage>–<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.10.019</pub-id>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kell</surname>, <given-names>Alexander J. E.</given-names></string-name>, <string-name><given-names>Daniel L. K.</given-names> <surname>Yamins</surname></string-name>, <string-name><given-names>Erica N.</given-names> <surname>Shook</surname></string-name>, <string-name><given-names>Sam V.</given-names> <surname>Norman-Haignere</surname></string-name>, and <string-name><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name></person-group>. <year>2018</year>. “<article-title>A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title>.” <source>Neuron</source> <volume>98</volume>(<issue>3</issue>):<fpage>630</fpage>–<lpage>644.e16.</lpage> doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname>, <given-names>Seyed-Mahdi</given-names></string-name>, and <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name></person-group>. <year>2014</year>. “<article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation” edited by J. Diedrichsen</article-title>. <source>PLoS Computational Biology</source> <volume>10</volume>(<issue>11</issue>):<fpage>e1003915</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>Diederik P.</given-names></string-name>, and <string-name><given-names>Max</given-names> <surname>Welling</surname></string-name></person-group>. <year>2014</year>. “<article-title>Auto-Encoding Variational Bayes</article-title>.” <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1312.6114</pub-id></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>Diederik P.</given-names></string-name>, and <string-name><given-names>Max</given-names> <surname>Welling</surname></string-name></person-group>. <year>2019</year>. “<article-title>An Introduction to Variational Autoencoders</article-title>.” <source>Foundations and Trends® in Machine Learning</source> <volume>12</volume>(<issue>4</issue>):<fpage>307</fpage>–<lpage>92</lpage>. doi: <pub-id pub-id-type="doi">10.1561/2200000056</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>Nikolaus</given-names></string-name></person-group>. <year>2008</year>. “<article-title>Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience</article-title>.” <source>Frontiers in Systems Neuroscience</source>. doi: <pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegstein</surname>, <given-names>Katharina V.</given-names></string-name>, and <string-name><given-names>Anne-Lise</given-names> <surname>Giraud</surname></string-name></person-group>. <year>2004</year>. “<article-title>Distinct Functional Substrates along the Right Superior Temporal Sulcus for the Processing of Voices</article-title>.” <source>NeuroImage</source> <volume>22</volume>(<issue>2</issue>):<fpage>948</fpage>–<lpage>55</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.02.020</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>von Kriegstein</surname>, <given-names>Katharina</given-names></string-name>, <string-name><given-names>Evelyn</given-names> <surname>Eger</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Kleinschmidt</surname></string-name>, and <string-name><given-names>Anne Lise</given-names> <surname>Giraud</surname></string-name></person-group>. <year>2003</year>. “<article-title>Modulation of Neural Responses to Speech by Directing Attention to Voices or Verbal Content</article-title>.” <source>Cognitive Brain Research</source> <volume>17</volume>(<issue>1</issue>):<fpage>48</fpage>–<lpage>55</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0926-6410(03)00079-X</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lavan</surname>, <given-names>Nadine</given-names></string-name></person-group>. <year>2023</year>. “<article-title>The Time Course of Person Perception From Voices: A Behavioral Study</article-title>.” <source>Psychological Science</source> <volume>34</volume>(<issue>7</issue>):<fpage>771</fpage>–<lpage>83</lpage>. doi: <pub-id pub-id-type="doi">10.1177/09567976231161565</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le</surname>, <given-names>Lynn</given-names></string-name>, <string-name><given-names>Luca</given-names> <surname>Ambrogioni</surname></string-name>, <string-name><given-names>Katja</given-names> <surname>Seeliger</surname></string-name>, <string-name><given-names>Yağmur</given-names> <surname>Güçlütürk</surname></string-name>, <string-name><given-names>Marcel</given-names> <surname>van Gerven</surname></string-name>, and <string-name><given-names>Umut</given-names> <surname>Güçlü</surname></string-name></person-group>. <year>2022</year>. “<article-title>Brain2Pix: Fully Convolutional Naturalistic Video Frame Reconstruction from Brain Activity</article-title>.” <source>Frontiers in Neuroscience</source> <volume>16</volume>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>, <given-names>Yann</given-names></string-name>, <string-name><given-names>Yoshua</given-names> <surname>Bengio</surname></string-name>, and <string-name><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name></person-group>. <year>2015</year>. “<article-title>Deep Learning</article-title>.” <source>Nature</source> <volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>Dong</given-names></string-name>, <string-name><given-names>Yue</given-names> <surname>Li</surname></string-name>, <string-name><given-names>Jianping</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Houqiang</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>Feng</given-names> <surname>Wu</surname></string-name></person-group>. <year>2020</year>. “<article-title>Deep Learning-Based Video Coding: A Review and a Case Study</article-title>.” <source>ACM Computing Surveys</source> <volume>53</volume>(<issue>1</issue>):<fpage>11:1</fpage>–<lpage>11:35</lpage>. doi: <pub-id pub-id-type="doi">10.1145/3368405</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maguinness</surname>, <given-names>Corrina</given-names></string-name>, <string-name><given-names>Claudia</given-names> <surname>Roswandowitz</surname></string-name>, and <string-name><given-names>Katharina</given-names> <surname>von Kriegstein</surname></string-name></person-group>. <year>2018</year>. “<article-title>Understanding the Mechanisms of Familiar Voice-Identity Recognition in the Human Brain</article-title>.” <source>Neuropsychologia</source> <volume>116</volume>:<fpage>179</fpage>–<lpage>93</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.03.039</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maris</surname>, <given-names>Eric</given-names></string-name>, and <string-name><given-names>Robert</given-names> <surname>Oostenveld</surname></string-name></person-group>. <year>2007</year>. “<article-title>Nonparametric Statistical Testing of EEG- and MEG-Data</article-title>.” <source>Journal of Neuroscience Methods</source> <volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname>, <given-names>F. N.</given-names></string-name>, and <string-name><given-names>C. A.</given-names> <surname>Champlin</surname></string-name></person-group>. <year>2000</year>. “<article-title>Reconsidering the Limits of Normal Hearing</article-title>.” <source>Journal of the American Academy of Audiology</source> <volume>11</volume>(<issue>2</issue>):<fpage>64</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mcallister</surname>, <given-names>Jan</given-names></string-name>, <string-name><given-names>Anne</given-names> <surname>Potts</surname></string-name>, <string-name><given-names>Kathryn</given-names> <surname>Mason</surname></string-name>, and <string-name><given-names>Geoffrey</given-names> <surname>Marchant</surname></string-name></person-group>. <year>1994</year>. “<article-title>Word Duration in Monologue and Dialogue Speech</article-title>.” <source>Language and Speech</source> <volume>37</volume>(<issue>4</issue>):<fpage>393</fpage>–<lpage>405</lpage>. doi: <pub-id pub-id-type="doi">10.1177/002383099403700404</pub-id>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Millet</surname>, <given-names>Juliette</given-names></string-name>, <string-name><given-names>Charlotte</given-names> <surname>Caucheteux</surname></string-name>, <string-name><given-names>Pierre</given-names> <surname>Orhan</surname></string-name>, <string-name><given-names>Yves</given-names> <surname>Boubenec</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Ewan</given-names> <surname>Dunbar</surname></string-name>, <string-name><given-names>Christophe</given-names> <surname>Pallier</surname></string-name>, and <string-name><given-names>Jean-Remi</given-names> <surname>King</surname></string-name></person-group>. <year>2022</year>. <article-title>Toward a Realistic Model of Speech Processing in the Brain with Self-Supervised Learning</article-title>. <source>arXiv</source>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2206.01685</pub-id>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Mozafari</surname>, <given-names>Milad</given-names></string-name>, <string-name><given-names>Leila</given-names> <surname>Reddy</surname></string-name>, and <string-name><given-names>Rufin</given-names> <surname>VanRullen</surname></string-name></person-group>. <year>2020</year>. “<article-title>Reconstructing Natural Scenes from fMRI Patterns Using BigBiGAN</article-title>.” <conf-name>2020 International Joint Conference on Neural Networks (IJCNN)</conf-name> <fpage>1</fpage>–<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1109/IJCNN48605.2020.9206960</pub-id>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nagrani</surname>, <given-names>Arsha</given-names></string-name>, <string-name><given-names>Joon Son</given-names> <surname>Chung</surname></string-name>, and <string-name><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name></person-group>. <year>2017</year>. “<article-title>VoxCeleb: A Large-Scale Speaker Identification Dataset</article-title>.” Pp. <fpage>2616</fpage>–<lpage>20</lpage> in <source>Interspeech</source> 2017.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naselaris</surname>, <given-names>Thomas</given-names></string-name>, <string-name><given-names>Kendrick N.</given-names> <surname>Kay</surname></string-name>, <string-name><given-names>Shinji</given-names> <surname>Nishimoto</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name></person-group>. <year>2011</year>. “<article-title>Encoding and Decoding in fMRI</article-title>.” <source>NeuroImage</source> <volume>56</volume>(<issue>2</issue>):<fpage>400</fpage>–<lpage>410</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasley</surname>, <given-names>Brian N.</given-names></string-name>, <string-name><given-names>Stephen V.</given-names> <surname>David</surname></string-name>, <string-name><given-names>Nima</given-names> <surname>Mesgarani</surname></string-name>, <string-name><given-names>Adeen</given-names> <surname>Flinker</surname></string-name>, <string-name><given-names>Shihab A.</given-names> <surname>Shamma</surname></string-name>, <string-name><given-names>Nathan E.</given-names> <surname>Crone</surname></string-name>, <string-name><given-names>Robert T.</given-names> <surname>Knight</surname></string-name>, and <string-name><given-names>Edward F.</given-names> <surname>Chang</surname></string-name></person-group>. <year>2012</year>. “<article-title>Reconstructing Speech from Human Auditory Cortex</article-title>.” <source>PLOS Biology</source> <volume>10</volume>(<issue>1</issue>):<fpage>e1001251</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.1001251</pub-id>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Paszke</surname>, <given-names>Adam</given-names></string-name>, <string-name><given-names>Sam</given-names> <surname>Gross</surname></string-name>, <string-name><given-names>Francisco</given-names> <surname>Massa</surname></string-name>, <string-name><given-names>Adam</given-names> <surname>Lerer</surname></string-name>, <string-name><given-names>James</given-names> <surname>Bradbury</surname></string-name>, <string-name><given-names>Gregory</given-names> <surname>Chanan</surname></string-name>, <string-name><given-names>Trevor</given-names> <surname>Killeen</surname></string-name>, <string-name><given-names>Zeming</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>Natalia</given-names> <surname>Gimelshein</surname></string-name>, <string-name><given-names>Luca</given-names> <surname>Antiga</surname></string-name>, <string-name><given-names>Alban</given-names> <surname>Desmaison</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Köpf</surname></string-name>, <string-name><given-names>Edward</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Zach</given-names> <surname>DeVito</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Raison</surname></string-name>, <string-name><given-names>Alykhan</given-names> <surname>Tejani</surname></string-name>, <string-name><given-names>Sasank</given-names> <surname>Chilamkurthy</surname></string-name>, <string-name><given-names>Benoit</given-names> <surname>Steiner</surname></string-name>, <string-name><given-names>Lu</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>Junjie</given-names> <surname>Bai</surname></string-name>, and <string-name><given-names>Soumith</given-names> <surname>Chintala</surname></string-name></person-group>. <year>2019</year>. “<article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>.” <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="software"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>, <given-names>Fabian</given-names></string-name>, <string-name><given-names>Gaël</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>Bertrand</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>Olivier</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>Mathieu</given-names> <surname>Blondel</surname></string-name>, <string-name><given-names>Andreas</given-names> <surname>Müller</surname></string-name>, <string-name><given-names>Joel</given-names> <surname>Nothman</surname></string-name>, <string-name><given-names>Gilles</given-names> <surname>Louppe</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>Prettenhofer</surname></string-name>, <string-name><given-names>Ron</given-names> <surname>Weiss</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Dubourg</surname></string-name>, <string-name><given-names>Jake</given-names> <surname>Vanderplas</surname></string-name>, <string-name><given-names>Alexandre</given-names> <surname>Passos</surname></string-name>, <string-name><given-names>David</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>Matthieu</given-names> <surname>Brucher</surname></string-name>, <string-name><given-names>Matthieu</given-names> <surname>Perrot</surname></string-name>, and <string-name><given-names>Édouard</given-names> <surname>Duchesnay</surname></string-name></person-group>. <year>2018</year>. “<data-title>Scikit-Learn: Machine Learning in Python</data-title>.” <publisher-name>Scikit-Learn</publisher-name></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Penhune</surname>, <given-names>V. B.</given-names></string-name>, <string-name><given-names>R. J.</given-names> <surname>Zatorre</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>MacDonald</surname></string-name>, and <string-name><given-names>A. C.</given-names> <surname>Evans</surname></string-name></person-group>. <year>1996</year>. “<article-title>Interhemispheric Anatomical Differences in Human Primary Auditory Cortex: Probabilistic Mapping and Volume Measurement from Magnetic Resonance Scans</article-title>.” <source>Cerebral Cortex (New York, N.Y.: 1991)</source> <volume>6</volume>(<issue>5</issue>):<fpage>661</fpage>–<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/6.5.661</pub-id>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pernet</surname>, <given-names>Cyril R.</given-names></string-name>, <string-name><given-names>Phil</given-names> <surname>McAleer</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Latinus</surname></string-name>, <string-name><given-names>Krzysztof J.</given-names> <surname>Gorgolewski</surname></string-name>, <string-name><given-names>Ian</given-names> <surname>Charest</surname></string-name>, <string-name><given-names>Patricia E. G.</given-names> <surname>Bestelmeyer</surname></string-name>, <string-name><given-names>Rebecca H.</given-names> <surname>Watson</surname></string-name>, <string-name><given-names>David</given-names> <surname>Fleming</surname></string-name>, <string-name><given-names>Frances</given-names> <surname>Crabbe</surname></string-name>, <string-name><given-names>Mitchell</given-names> <surname>Valdes-Sosa</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name></person-group>. <year>2015</year>. “<article-title>The Human Voice Areas: Spatial Organization and Inter-Individual Variability in Temporal and Extra-Temporal Cortices</article-title>.” <source>NeuroImage</source> <volume>119</volume>:<fpage>164</fpage>–<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.050</pub-id>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petkov</surname>, <given-names>Christopher I.</given-names></string-name>, <string-name><given-names>Christoph</given-names> <surname>Kayser</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Steudel</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Whittingstall</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Augath</surname></string-name>, and <string-name><given-names>Nikos K.</given-names> <surname>Logothetis</surname></string-name></person-group>. <year>2008</year>. “<article-title>A Voice Region in the Monkey Brain</article-title>.” <source>Nature Neuroscience</source> <volume>11</volume>(<issue>3</issue>):<fpage>367</fpage>–<lpage>74</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn2043</pub-id>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roswandowitz</surname>, <given-names>Claudia</given-names></string-name>, <string-name><given-names>Claudia</given-names> <surname>Kappes</surname></string-name>, <string-name><given-names>Hellmuth</given-names> <surname>Obrig</surname></string-name>, and <string-name><given-names>Katharina</given-names> <surname>von Kriegstein</surname></string-name></person-group>. <year>2018</year>. “<article-title>Obligatory and Facultative Brain Regions for Voice-Identity Recognition</article-title>.” <source>Brain</source> <volume>141</volume>(<issue>1</issue>):<fpage>234</fpage>–<lpage>47</lpage>. doi: <pub-id pub-id-type="doi">10.1093/brain/awx313</pub-id>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rupp</surname>, <given-names>Kyle</given-names></string-name>, <string-name><given-names>Jasmine L.</given-names> <surname>Hect</surname></string-name>, <string-name><given-names>Madison</given-names> <surname>Remick</surname></string-name>, <string-name><given-names>Avniel</given-names> <surname>Ghuman</surname></string-name>, <string-name><given-names>Bharath</given-names> <surname>Chandrasekaran</surname></string-name>, <string-name><given-names>Lori L.</given-names> <surname>Holt</surname></string-name>, and <string-name><given-names>Taylor J.</given-names> <surname>Abel</surname></string-name></person-group>. <year>2022</year>. “<article-title>Neural Responses in Human Superior Temporal Cortex Support Coding of Voice Representations</article-title>.” <source>PLOS Biology</source> <volume>20</volume>(<issue>7</issue>):<fpage>e3001675</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3001675</pub-id>.</mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santoro</surname>, <given-names>Roberta</given-names></string-name>, <string-name><given-names>Michelle</given-names> <surname>Moerel</surname></string-name>, <string-name><given-names>Federico</given-names> <surname>De Martino</surname></string-name>, <string-name><given-names>Giancarlo</given-names> <surname>Valente</surname></string-name>, <string-name><given-names>Kamil</given-names> <surname>Ugurbil</surname></string-name>, <string-name><given-names>Essa</given-names> <surname>Yacoub</surname></string-name>, and <string-name><given-names>Elia</given-names> <surname>Formisano</surname></string-name></person-group>. <year>2017</year>. “<article-title>Reconstructing the Spectrotemporal Modulations of Real-Life Sounds from fMRI Response Patterns</article-title>.” <source>Proceedings of the National Academy of Sciences</source> <volume>114</volume>(<issue>18</issue>):<fpage>4799</fpage>–<lpage>4804</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.1617622114</pub-id>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>Martin</given-names></string-name>, <string-name><given-names>Idan Asher</given-names> <surname>Blank</surname></string-name>, <string-name><given-names>Greta</given-names> <surname>Tuckute</surname></string-name>, <string-name><given-names>Carina</given-names> <surname>Kauf</surname></string-name>, <string-name><given-names>Eghbal A.</given-names> <surname>Hosseini</surname></string-name>, <string-name><given-names>Nancy</given-names> <surname>Kanwisher</surname></string-name>, <string-name><given-names>Joshua B.</given-names> <surname>Tenenbaum</surname></string-name>, and <string-name><given-names>Evelina</given-names> <surname>Fedorenko</surname></string-name></person-group>. <year>2021</year>. “<article-title>The Neural Architecture of Language: Integrative Modeling Converges on Predictive Processing</article-title>.” <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>(<issue>45</issue>):<fpage>e2105646118</fpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id>.</mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>Martin</given-names></string-name>, <string-name><given-names>Jonas</given-names> <surname>Kubilius</surname></string-name>, <string-name><given-names>Ha</given-names> <surname>Hong</surname></string-name>, <string-name><given-names>Najib J.</given-names> <surname>Majaj</surname></string-name>, <string-name><given-names>Rishi</given-names> <surname>Rajalingham</surname></string-name>, <string-name><given-names>Elias B.</given-names> <surname>Issa</surname></string-name>, <string-name><given-names>Kohitij</given-names> <surname>Kar</surname></string-name>, <string-name><given-names>Pouya</given-names> <surname>Bashivan</surname></string-name>, <string-name><given-names>Jonathan</given-names> <surname>Prescott-Roy</surname></string-name>, <string-name><given-names>Franziska</given-names> <surname>Geiger</surname></string-name>, <string-name><given-names>Kailyn</given-names> <surname>Schmidt</surname></string-name>, <string-name><given-names>Daniel L. K.</given-names> <surname>Yamins</surname></string-name>, and <string-name><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name></person-group>. <year>2018</year>. <article-title>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like? preprint</article-title>. <source>Neuroscience</source>. doi: <pub-id pub-id-type="doi">10.1101/407007</pub-id>.</mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Schütt</surname>, <given-names>Heiko H.</given-names></string-name>, <string-name><given-names>Alexander D.</given-names> <surname>Kipnis</surname></string-name>, <string-name><given-names>Jörn</given-names> <surname>Diedrichsen</surname></string-name>, and <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name></person-group>. <year>2021</year>. “<article-title>Statistical Inference on Representational Geometries</article-title>.” <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/arXiv.2112.09200</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schweinberger</surname>, <given-names>S. R.</given-names></string-name>, <string-name><given-names>A.</given-names> <surname>Herholz</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Sommer</surname></string-name></person-group>. <year>1997</year>. “<article-title>Recognizing Famous Voices: Influence of Stimulus Duration and Different Types of Retrieval Cues</article-title>.” <source>Journal of Speech, Language, and Hearing Research: JSLHR</source> <volume>40</volume>(<issue>2</issue>):<fpage>453</fpage>–<lpage>63</lpage>. doi: <pub-id pub-id-type="doi">10.1044/jslhr.4002.453</pub-id>.</mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>Stephen M.</given-names></string-name>, <string-name><given-names>Mark</given-names> <surname>Jenkinson</surname></string-name>, <string-name><given-names>Mark W.</given-names> <surname>Woolrich</surname></string-name>, <string-name><given-names>Christian F.</given-names> <surname>Beckmann</surname></string-name>, <string-name><given-names>Timothy E. J.</given-names> <surname>Behrens</surname></string-name>, <string-name><given-names>Heidi</given-names> <surname>Johansen-Berg</surname></string-name>, <string-name><given-names>Peter R.</given-names> <surname>Bannister</surname></string-name>, <string-name><given-names>Marilena</given-names> <surname>De Luca</surname></string-name>, <string-name><given-names>Ivana</given-names> <surname>Drobnjak</surname></string-name>, <string-name><given-names>David E.</given-names> <surname>Flitney</surname></string-name>, <string-name><given-names>Rami K.</given-names> <surname>Niazy</surname></string-name>, <string-name><given-names>James</given-names> <surname>Saunders</surname></string-name>, <string-name><given-names>John</given-names> <surname>Vickers</surname></string-name>, <string-name><given-names>Yongyue</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Nicola</given-names> <surname>De Stefano</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Michael Brady</surname></string-name>, and <string-name><given-names>Paul M.</given-names> <surname>Matthews</surname></string-name></person-group>. <year>2004</year>. “<article-title>Advances in Functional and Structural MR Image Analysis and Implementation as FSL</article-title>.” <source>NeuroImage</source> <volume>23</volume>:<fpage>S208</fpage>–<lpage>19</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Snoek</surname>, <given-names>Lukas</given-names></string-name>, <string-name><given-names>Maite M.</given-names> <surname>van der Miesen</surname></string-name>, <string-name><given-names>Tinka</given-names> <surname>Beemsterboer</surname></string-name>, <string-name><given-names>Andries</given-names> <surname>van der Leij</surname></string-name>, <string-name><given-names>Annemarie</given-names> <surname>Eigenhuis</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Steven Scholte</surname></string-name></person-group>. <year>2021</year>. “<article-title>The Amsterdam Open MRI Collection, a Set of Multimodal MRI Datasets for Individual Difference Analyses</article-title>.” <source>Scientific Data</source> <volume>8</volume>(<issue>1</issue>):<fpage>85</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41597-021-00870-6</pub-id>.</mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trapeau</surname>, <given-names>Régis</given-names></string-name>, <string-name><given-names>Etienne</given-names> <surname>Thoret</surname></string-name>, and <string-name><given-names>Pascal</given-names> <surname>Belin</surname></string-name></person-group>. <year>2022</year>. “<article-title>The Temporal Voice Areas Are Not ‘Just’ Speech Areas</article-title>.” <source>Frontiers in Neuroscience</source> <volume>16</volume>:<fpage>1075288</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2022.1075288</pub-id>.</mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tuckute</surname>, <given-names>Greta</given-names></string-name>, <string-name><given-names>Jenelle</given-names> <surname>Feather</surname></string-name>, <string-name><given-names>Dana</given-names> <surname>Boebinger</surname></string-name>, and <string-name><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name></person-group>. <year>2023</year>. “<article-title>Many but Not All Deep Neural Network Audio Models Capture Brain Responses and Exhibit Correspondence between Model Stages and Brain Regions</article-title>.” <source>PLOS Biology</source> <volume>21</volume>(<issue>12</issue>):<fpage>e3002366</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pbio.3002366</pub-id>.</mixed-citation></ref>
<ref id="c79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vallat</surname>, <given-names>Raphael</given-names></string-name></person-group>. <year>2018</year>. “<article-title>Pingouin: Statistics in Python</article-title>.” <source>Journal of Open Source Software</source> <volume>3</volume>(<issue>31</issue>):<fpage>1026</fpage>. doi: <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>.</mixed-citation></ref>
<ref id="c80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname>, <given-names>Rufin</given-names></string-name>, and <string-name><given-names>Leila</given-names> <surname>Reddy</surname></string-name></person-group>. <year>2019</year>. “<article-title>Reconstructing Faces from fMRI Patterns Using Deep Generative Neural Networks</article-title>.” <source>Communications Biology</source> <volume>2</volume>(<issue>1</issue>):<fpage>193</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s42003-019-0438-y</pub-id>.</mixed-citation></ref>
<ref id="c81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walther</surname>, <given-names>Alexander</given-names></string-name>, <string-name><given-names>Hamed</given-names> <surname>Nili</surname></string-name>, <string-name><given-names>Naveed</given-names> <surname>Ejaz</surname></string-name>, <string-name><given-names>Arjen</given-names> <surname>Alink</surname></string-name>, <string-name><given-names>Nikolaus</given-names> <surname>Kriegeskorte</surname></string-name>, and <string-name><given-names>Jörn</given-names> <surname>Diedrichsen</surname></string-name></person-group>. <year>2016</year>. “<article-title>Reliability of Dissimilarity Measures for Multi-Voxel Pattern Analysis</article-title>.” <source>NeuroImage</source> <volume>137</volume>:<fpage>188</fpage>–<lpage>200</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id>.</mixed-citation></ref>
<ref id="c82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Xiaosha</given-names></string-name>, <string-name><given-names>Yangwen</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>Yuwei</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>Jiacai</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Zhenhua</given-names> <surname>Ling</surname></string-name>, and <string-name><given-names>Yanchao</given-names> <surname>Bi</surname></string-name></person-group>. <year>2018</year>. “<article-title>Representational Similarity Analysis Reveals Task-Dependent Semantic Influence of the Visual Word Form Area</article-title>.” <source>Scientific Reports</source> <volume>8</volume>(<issue>1</issue>):<fpage>3047</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-018-21062-0</pub-id>.</mixed-citation></ref>
<ref id="c83"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wetzel</surname>, <given-names>Sebastian J</given-names></string-name></person-group>. <year>2017</year>. “<article-title>Unsupervised Learning of Phase Transitions: From Principal Component Analysis to Variational Autoencoders</article-title>.” <source>Physical Review E</source> <volume>96</volume>(<issue>2</issue>):<fpage>022140</fpage>. doi: <pub-id pub-id-type="doi">10.1103/PhysRevE.96.022140</pub-id>.</mixed-citation></ref>
<ref id="c84"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woods</surname>, <given-names>Kevin J. P.</given-names></string-name>, <string-name><given-names>Max H.</given-names> <surname>Siegel</surname></string-name>, <string-name><given-names>James</given-names> <surname>Traer</surname></string-name>, and <string-name><given-names>Josh H.</given-names> <surname>McDermott</surname></string-name></person-group>. <year>2017</year>. “<article-title>Headphone Screening to Facilitate Web-Based Auditory Experiments</article-title>.” <source>Attention, Perception, &amp; Psychophysics</source> <volume>79</volume>(<issue>7</issue>):<fpage>2064</fpage>–<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13414-017-1361-2</pub-id>.</mixed-citation></ref>
<ref id="c85"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>Michael C. K.</given-names></string-name>, <string-name><given-names>Stephen V.</given-names> <surname>David</surname></string-name>, and <string-name><given-names>Jack L.</given-names> <surname>Gallant</surname></string-name></person-group>. <year>2006</year>. “<article-title>Complete Functional Characterization Of Sensory Neurons By System Identification</article-title>.” <source>Annual Review of Neuroscience</source> <volume>29</volume>(<issue>1</issue>):<fpage>477</fpage>–<lpage>505</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113024</pub-id>.</mixed-citation></ref>
<ref id="c86"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Xing</surname>, <given-names>Chao</given-names></string-name>, <string-name><given-names>Dong</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Chao</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>Yiye</given-names> <surname>Lin</surname></string-name></person-group>. <year>2015</year>. “<article-title>Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</article-title>.” Pp. <fpage>1006</fpage>–<lpage>11</lpage> in <conf-name>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name>. <publisher-loc>Denver, Colorado</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>.</mixed-citation></ref>
<ref id="c87"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>Daniel L. K.</given-names></string-name>, and <string-name><given-names>James J.</given-names> <surname>DiCarlo</surname></string-name></person-group>. <year>2016</year>. “<article-title>Using Goal-Driven Deep Learning Models to Understand Sensory Cortex</article-title>.” <source>Nature Neuroscience</source> <volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>–<lpage>65</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn.4244</pub-id>.</mixed-citation></ref>
<ref id="c88"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zäske</surname>, <given-names>Romi</given-names></string-name>, <string-name><given-names>Marie-Christin</given-names> <surname>Perlich</surname></string-name>, and <string-name><given-names>Stefan R.</given-names> <surname>Schweinberger</surname></string-name></person-group>. <year>2016</year>. “<article-title>To Hear or Not to Hear: Voice Processing under Visual Load</article-title>.” <source>Attention, Perception, &amp; Psychophysics</source> <volume>78</volume>(<issue>5</issue>):<fpage>1488</fpage>–<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13414-016-1119-2</pub-id>.</mixed-citation></ref>
<ref id="c89"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>Yang</given-names></string-name>, <string-name><given-names>Yue</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Juan</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Wenjing</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Zhipei</given-names> <surname>Ling</surname></string-name>, <string-name><given-names>Bo</given-names> <surname>Hong</surname></string-name>, and <string-name><given-names>Xiaoqin</given-names> <surname>Wang</surname></string-name></person-group>. <year>2021</year>. “<article-title>Hierarchical Cortical Networks of ‘Voice Patches’ for Processing Voices in Human Brain</article-title>.” <source>Proceedings of the National Academy of Sciences</source> <volume>118</volume>(<issue>52</issue>):<fpage>e2113887118</fpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.2113887118</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Martin</surname>
<given-names>Andrea E</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3395-7234</contrib-id>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution>
</institution-wrap>
<city>Nijmegen</city>
<country>Netherlands</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Solid</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This study used deep neural networks (DNN) to reconstruct voice information (viz., speaker identity), from fMRI responses in the auditory cortex and temporal voice areas, and assessed the representational content in these areas with decoding. A DNN-derived feature space approximated the neural representation of speaker identity-related information. The findings are <bold>valuable</bold> and the approach <bold>solid</bold>, yielding insight into how a specific model architecture can be used to relate the latent spaces of neural data and auditory stimuli to each other.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this study, the authors trained a variational autoencoder (VAE) to create a high-dimensional &quot;voice latent space&quot; (VLS) using extensive voice samples, and analyzed how this space corresponds to brain activity through fMRI studies focusing on the temporal voice areas (TVAs). Their analyses included encoding and decoding techniques, as well as representational similarity analysis (RSA), which showed that the VLS could effectively map onto and predict brain activity patterns, allowing for the reconstruction of voice stimuli that preserve key aspects of speaker identity.</p>
<p>Strengths:</p>
<p>This paper is well-written and easy to follow. Most of the methods and results were clearly described. The authors combined a variety of analytical methods in neuroimaging studies, including encoding, decoding, and RSA. In addition to commonly used DNN encoding analysis, the authors performed DNN decoding and resynthesized the stimuli using VAE decoders. Furthermore, in addition to machine learning classifiers, the authors also included human behavioral tests to evaluate the reconstruction performance.</p>
<p>Weaknesses:</p>
<p>This manuscript presents a variational autoencoder (VAE) model to study voice identity representations from brain activity. While the model's ability to preserve speaker identity is expected due to its reconstruction objective, its broader utility remains unclear. Specifically, the VAE is not benchmarked against state-of-the-art speech models such as Wav2Vec2, HuBERT, or Whisper, which have demonstrated strong performance on standard speech tasks and alignment with cortical responses. Without comparisons on downstream tasks like automatic speech recognition (ASR) or phoneme classification, it is difficult to assess the relevance or advantages of the VLS representation.</p>
<p>Furthermore, the neural basis of the observed correlations between VLS and brain activity is not well characterized. It remains unclear whether the VLS aligns with high-level abstract identity representations or lower-level acoustic features like pitch. Prior studies (e.g., Tang et al., Science 2017; Feng et al., NeuroImage 2021) have shown both types of coding in STG. The experimental design also does not clarify whether speech content was controlled across speakers, raising concerns about confounding acoustic-phonetic features. For example, PC2 in Figure 1b appears to reflect absolute pitch height, suggesting that identity decoding may partly rely on simpler acoustic cues. A more detailed analysis of the representational content of VLS would strengthen the conclusions.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Lamothe et al. collected fMRI responses to many voice stimuli in 3 subjects. The authors trained two different autoencoders on voice audio samples and predicted latent space embeddings from the fMRI responses, allowing the voice spectrograms to be reconstructed. The degree to which reconstructions from different auditory ROIs correctly represented speaker identity, gender or age was assessed by machine classification and human listener evaluations. Complementing this, the representational content was also assessed using representational similarity analysis. The results broadly concur with the notion that temporal voice areas are sensitive to different types of categorical voice information.</p>
<p>Strengths:</p>
<p>The single-subject approach that allow thousands of responses to unique stimuli to be recorded and analyzed is powerful. The idea of using this approach to probe cortical voice representations is strong and the experiment is technically solid.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>In this manuscript, Lamothe et al. sought to identify the neural substrates of voice identity in the human brain by correlating fMRI recordings with the latent space of a variational autoencoder (VAE) trained on voice spectrograms. They used encoding and decoding models, and showed that the &quot;voice&quot; latent space (VLS) of the VAE performs, in general, (slightly) better than a linear autoencoder's latent space. Additionally, they showed dissociations in the encoding of voice identity across the temporal voice areas.</p>
<p>Strengths:</p>
<p>The geometry of the neural representations of voice identity has not been studied so far. Previous studies on the content of speech and faces in vision suggest that such geometry could exist. This study demonstrates this point systematically, leveraging a specifically trained variational autoencoder.</p>
<p>The size of the voice dataset and the length of the fMRI recordings ensure that the findings are robust.</p>
<p>Comments on revisions:</p>
<p>The authors addressed my previous recommendations.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98047.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Lamothe</surname>
<given-names>Charly</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9918-8258</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Thoret</surname>
<given-names>Etienne</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8214-6278</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Trapeau</surname>
<given-names>Régis</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1137-8669</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Giordano</surname>
<given-names>Bruno L</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7002-0486</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Sein</surname>
<given-names>Julien</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1767-5330</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Takerkart</surname>
<given-names>Sylvain</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8410-0962</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Ayache</surname>
<given-names>Stéphane</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2982-7127</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Artières</surname>
<given-names>Thierry</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3696-0321</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Belin</surname>
<given-names>Pascal</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7578-6365</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public Review):</bold></p>
<p>Summary:</p>
<p>In this study, the authors trained a variational autoencoder (VAE) to create a high-dimensional &quot;voice latent space&quot; (VLS) using extensive voice samples, and analyzed how this space corresponds to brain activity through fMRI studies focusing on the temporal voice areas (TVAs). Their analyses included encoding and decoding techniques, as well as representational similarity analysis (RSA), which showed that the VLS could effectively map onto and predict brain activity patterns, allowing for the reconstruction of voice stimuli that preserve key aspects of speaker identity.</p>
<p>Strengths:</p>
<p>This paper is well-written and easy to follow. Most of the methods and results were clearly described. The authors combined a variety of analytical methods in neuroimaging studies, including encoding, decoding, and RSA. In addition to commonly used DNN encoding analysis, the authors performed DNN decoding and resynthesized the stimuli using VAE decoders. Furthermore, in addition to machine learning classifiers, the authors also included human behavioral tests to evaluate the reconstruction performance.</p>
<p>Weaknesses:</p>
<p>This manuscript presents a variational autoencoder (VAE) to evaluate voice identity representations from brain recordings. However, the study's scope is limited by testing only one model, leaving unclear how generalizable or impactful the findings are. The preservation of identity-related information in the voice latent space (VLS) is expected, given the VAE model's design to reconstruct original vocal stimuli. Nonetheless, the study lacks a deeper investigation into what specific aspects of auditory coding these latent dimensions represent. The results in Figure 1c-e merely tested a very limited set of speech features. Moreover, there is no analysis of how these features and the whole VAE model perform in standard speech tasks like speech recognition or phoneme recognition. It is not clear what kind of computations the VAE model presented in this work is capable of. Inclusion of comparisons with state-of-the-art unsupervised or self-supervised speech models known for their alignment with auditory cortical responses, such as Wav2Vec2, HuBERT, and Whisper, would strengthen the validation of the VAE model and provide insights into its relative capabilities and limitations.</p>
<p>The claim that the VLS outperforms a linear model (LIN) in decoding tasks does not significantly advance our understanding of the underlying brain representations. Given the complexity of auditory processing, it is unsurprising that a nonlinear model would outperform a simpler linear counterpart. The study could be improved by incorporating a comparative analysis with alternative models that differ in architecture, computational strategies, or training methods. Such comparisons could elucidate specific features or capabilities of the VLS, offering a more nuanced understanding of its effectiveness and the computational principles it embodies. This approach would allow the authors to test specific hypotheses about how different aspects of the model contribute to its performance, providing a clearer picture of the shared coding in VLS and the brain.</p>
<p>The manuscript overlooks some crucial alternative explanations for the discriminant representation of vocal identity. For instance, the discriminant representation of vocal identity can be either a higher-level abstract representation or a lower-level coding of pitch height. Prior studies using fMRI and ECoG have identified both types of representation within the superior temporal gyrus (STG) (e.g., Tang et al., Science 2017; Feng et al., NeuroImage 2021). Additionally, the methodology does not clarify whether the stimuli from different speakers contained identical speech content. If the speech content varied across speakers, the approach of averaging trials to obtain a mean vector for each speaker-the &quot;identity-based analysis&quot;-may not adequately control for confounding acoustic-phonetic features. Notably, the principal component 2 (PC2) in Figure 1b appears to correlate with absolute pitch height, suggesting that some aspects of the model's effectiveness might be attributed to simpler acoustic properties rather than complex identity-specific information.</p>
<p>Methodologically, there are issues that warrant attention. In characterizing the autoencoder latent space, the authors initialized logistic regression classifiers 100 times and calculated the tstatistics using degrees of freedom (df) of 99. Given that logistic regression is a convex optimization problem typically converging to a global optimum, these multiple initializations of the classifier were likely not entirely independent. Consequently, the reported degrees of freedom and the effect size estimates might not accurately reflect the true variability and independence of the classifier outcomes. A more careful evaluation of these aspects is necessary to ensure the statistical robustness of the results.</p>
</disp-quote>
<p>We thank Reviewer #1 for their thoughtful and constructive comments. Below, we address the key points raised:</p>
<p>New comparitive models. We agree there are still many open questions on the structure of the VLS and the specific aspects of auditory coding that its latent dimensions represent. The features tested in Figure 1c-e are not speech features, but aspects related to speaker identity: age, gender and unique identity. Nevertheless we agree the VLS could be compared to recent speech models (not available when we started this project): we have now included comparisons with Wav2Vec and HuBERT in the encoding section (new Figure 2-S3). The comparison of encoding results based on LIN, the VLS, Wav2Vec and HuBERT (new Fig2S3) indicates no clear superiority of one model over the others; rather, different sets of voxels are better explained by the different models. Interestingly all four models yielded best encoding results for the m and a TVA, indicating some consistency across models.</p>
<p>On decoding directly from spectrograms. We have now added decoding results obtained directly from spectrograms, as requested in the private review. These are presented in the revised Figure 4, and allow for comparison with the LIN- and VLS-based reconstructions. As noted, spectrogram-based reconstructions sounded less vocal-like and faithful to the original, confirming that the latent spaces capture more abstract and cerebral-like voice representations.</p>
<p>On the number and length of stimuli. The rationale for using a large number of brief, randomly spliced speech excerpts from different languages was to extract identity features independent of specific linguistic cues. Indeed, the PC2 could very well correlate with pitch; we were not able to extract reliable f0 information from the thousands of brief stimuli, many of which are largely inharmonic (e.g., fricatives), such that this assumption could not be tested empirically. But it would be relevant that the weight of PC2 correlates with pitch: although the average fundamental frequency of phonation is not a linguistic cue, it is a major acoustical feature differentiating speaker identities.</p>
<p>Statistics correction.  To address the issue of potential dependence between multiple runs of logistic regression, we replaced our previous analysis with a Wilcoxon signedrank test comparing decoding accuracies to chance. The results remain significant across classifications, and the revised figure and text reflect this change.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public Review):</bold></p>
<p>Summary:</p>
<p>Lamothe et al. collected fMRI responses to many voice stimuli in 3 subjects. The authors trained two different autoencoders on voice audio samples and predicted latent space embeddings from the fMRI responses, allowing the voice spectrograms to be reconstructed. The degree to which reconstructions from different auditory ROIs correctly represented speaker identity, gender, or age was assessed by machine classification and human listener evaluations. Complementing this, the representational content was also assessed using representational similarity analysis. The results broadly concur with the notion that temporal voice areas are sensitive to different types of categorical voice information.</p>
<p>Strengths:</p>
<p>The single-subject approach that allows thousands of responses to unique stimuli to be recorded and analyzed is powerful. The idea of using this approach to probe cortical voice representations is strong and the experiment is technically solid.</p>
<p>Weaknesses:</p>
<p>The paper could benefit from more discussion of the assumptions behind the reconstruction analyses and the conclusions it allows. The authors write that reconstruction of a stimulus from brain responses represents 'a robust test of the adequacy of models of brain activity' (L138). I concur that stimulus reconstruction is useful for evaluating the nature of representations, but the notion that they can test the adequacy of the specific autoencoder presented here as a model of brain activity should be discussed at more length. Natural sounds are correlated in many feature dimensions and can therefore be summarized in several ways, and similar information can be read out from different model representations. Models trained to reconstruct natural stimuli can exploit many correlated features and it is quite possible that very different models based on different features can be used for similar reconstructions. Reconstructability does not by itself imply that the model is an accurate brain model. Non-linear networks trained on natural stimuli are arguably not tested in the same rigorous manner as models built to explicitly account for computations (they can generate predictions and experiments can be designed to test those predictions). While it is true that there is increasing evidence that neural network embeddings can predict brain data well, it is still a matter of debate whether good predictability by itself qualifies DNNs as 'plausible computational models for investigating brain processes' (L72). This concern is amplified in the context of decoding and naturalistic stimuli where many correlated features can be represented in many ways. It is unclear how much the results hinge on the specificities of the specific autoencoder architectures used. For instance, it would be useful to know the motivations for why the specific VAE used here should constitute a good model for probing neural voice representations.</p>
<p>Relatedly, it is not clear how VAEs as generative models are motivated as computational models of voice representations in the brain. The task of voice areas in the brain is not to generate voice stimuli but to discriminate and extract information. The task of reconstructing an input spectrogram is perhaps useful for probing information content, but discriminative models, e.g., trained on the task of discriminating voices, would seem more obvious candidates. Why not include discriminatively trained models for comparison?</p>
<p>The autoencoder learns a mapping from latent space to well-formed voice spectrograms. Regularized regression then learns a mapping between this latent space and activity space. All reconstructions might sound 'natural', which simply means that the autoencoder works. It would be good to have a stronger test of how close the reconstructions are to the original stimulus. For instance, is the reconstruction the closest stimulus to the original in latent space coordinates out of using the experimental stimuli, or where does it rank? How do small changes in beta amplitudes impact the reconstruction? The effective dimensionality of the activity space could be estimated, e.g. by PCA of the voice samples' contrast maps, and it could then be estimated how the main directions in the activity space map to differences in latent space. It would be good to get a better grasp of the granularity of information that can be decoded/ reconstructed.</p>
<p>What can we make of the apparent trend that LIN is higher than VLS for identity classification (at least VLS does not outperform LIN)? A general argument of the paper seems to be that VLS is a better model of voice representations compared to LIN as a 'control' model. Then we would expect VLS to perform better on identity classification. The age and gender of a voice can likely be classified from many acoustic features that may not require dedicated voice processing.</p>
<p>The RDM results reported are significant only for some subjects and in some ROIs. This presumably means that results are not significant in the other subjects. Yet, the authors assert general conclusions (e.g. the VLS better explains RDM in TVA than LIN). An assumption typically made in single-subject studies (with large amounts of data in individual subjects) is that the effects observed and reported in papers are robust in individual subjects. More than one subject is usually included to hint that this is the case. This is an intriguing approach. However, reports of effects that are statistically significant in some subjects and some ROIs are difficult to interpret. This, in my view, runs contrary to the logic and leverage of the single-subject approach. Reporting results that are only significant in 1 out of 3 subjects and inferring general conclusions from this seems less convincing.</p>
<p>The first main finding is stated as being that '128 dimensions are sufficient to explain a sizeable portion of the brain activity' (L379). What qualifies this? From my understanding, only models of that dimensionality were tested. They explain a sizeable portion of brain activity, but it is difficult to follow what 'sizable' is without baseline models that estimate a prediction floor and ceiling. For instance, would autoencoders that reconstruct any spectrogram (not just voice) also predict a sizable portion of the measured activity? What happens to reconstruction results as the dimensionality is varied?</p>
<p>A second main finding is stated as being that the 'VLS outperforms the LIN space' (L381). It seems correct that the VAE yields more natural-sounding reconstructions, but this is a technical feature of the chosen autoencoding approach. That the VLS yields a 'more brain-like representational space' I assume refers to the RDM results where the RDM correlations were mainly significant in one subject. For classification, the performance of features from the reconstructions (age/ gender/ identity) gives results that seem more mixed, and it seems difficult to draw a general conclusion about the VLS being better. It is not clear that this general claim is well supported.</p>
<p>It is not clear why the RDM was not formed based on the 'stimulus GLM' betas. The 'identity GLM' is already biased towards identity and it would be stronger to show associations at the stimulus level.</p>
<p>Multiple comparisons were performed across ROIs, models, subjects, and features in the classification analyses, but it is not clear how correction for these multiple comparisons was implemented in the statistical tests on classification accuracies.</p>
<p>Risks of overfitting and bias are a recurrent challenge in stimulus reconstruction with fMRI. It would be good with more control analyses to ensure that this was not the case. For instance, how were the repeated test stimuli presented? Were they intermingled with the other stimuli used for training or presented in separate runs? If intermingled, then the training and test data would have been preprocessed together, which could compromise the test set. The reconstructions could be performed on responses from independent runs, preprocessed separately, as a control. This should include all preprocessing, for instance, estimating stimulus/identity GLMs on separately processed run pairs rather than across all runs. Also, it would be good to avoid detrending before GLM denoising (or at least testing its effects) as these can interact.</p>
</disp-quote>
<p>We appreciate Reviewer #2’s careful reading and numerous suggestions for improving clarity and presentation. We have implemented the suggested text edits, corrected ambiguities, and clarified methodological details throughout the manuscript. In particular, we have toned down several sentences that we agree were making strong claims (L72, L118, L378, L380-381).</p>
<p>Clarifications, corrections and additional information:</p>
<p>We streamlined the introduction by reducing overly specific details and better framing the VLS concept before presenting specifics.</p>
<p>Clarified the motivation for the age classification split and corrected several inaccuracies and ambiguities in the methods, including the hearing thresholds, balancing of category levels, and stimulus energy selection procedure.</p>
<p>Provided additional information on the temporal structure of runs and experimental stimuli selection.</p>
<p>Corrected the description of technical issues affecting one participant and ensured all acronyms are properly defined in the text and figure legends.</p>
<p>Confirmed that audiograms were performed repeatedly to monitor hearing thresholds and clarified our use of robust scaling and normalization procedures.</p>
<p>Regarding the test of RDM correlations, we clarified in the text that multiple comparisons were corrected using a permutation-based framework.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Public Review):</bold></p>
<p>Summary:</p>
<p>In this manuscript, Lamothe et al. sought to identify the neural substrates of voice identity in the human brain by correlating fMRI recordings with the latent space of a variational autoencoder (VAE) trained on voice spectrograms. They used encoding and decoding models, and showed that the &quot;voice&quot; latent space (VLS) of the VAE performs, in general, (slightly) better than a linear autoencoder's latent space. Additionally, they showed dissociations in the encoding of voice identity across the temporal voice areas.</p>
<p>Strengths:</p>
<p>The geometry of the neural representations of voice identity has not been studied so far. Previous studies on the content of speech and faces in vision suggest that such geometry could exist. This study demonstrates this point systematically, leveraging a specifically trained variational autoencoder.</p>
<p>The size of the voice dataset and the length of the fMRI recordings ensure that the findings are robust.</p>
<p>Weaknesses:</p>
<p>Overall, the VLS is often only marginally better than the linear model across analysis, raising the question of whether the observed performance improvements are due to the higher number of parameters trained in the VAE, rather than the non-linearity itself. A fair comparison would necessitate that the number of parameters be maintained consistently across both models, at least as an additional verification step.</p>
<p>The encoding and RSM results are quite different. This is unexpected, as similar embedding geometries between the VLS and the brain activations should be reflected by higher correlation values of the encoding model.</p>
<p>The consistency across participants is not particularly high, for instance, S1 seemed to have demonstrated excellent performances, while S2 showed poor performance.</p>
<p>An important control analysis would be to compare the decoding results with those obtained by a decoder operating directly on the latent spaces, in order to further highlight the interest of the non-linear transformations of the decoder model. Currently, it is unclear whether the non-linearity of the decoder improves the decoding performance, considering the poor resemblance between the VLS and brain-reconstructed spectrograms.</p>
</disp-quote>
<p>We thank Reviewer #3 for their comments. In response:</p>
<p>Code and preprocessed data are now available as indicated in the revised manuscript.</p>
<p>While we appreciate the suggestion to display supplementary analyses as boxplots split by hemisphere, we opted to retain the current format as we do not have hypotheses regarding hemispheric lateralization, and the small sample size per hemisphere would preclude robust conclusions.</p>
<p>Confirmed that the identities in Figure 3a are indeed ordered by age and have clarified this in the legend.</p>
<p>The higher variance observed in correlations for the aTVA in Figure 3b reflects the small number of data points (3 participants × 2 hemispheres), and this is now explained.</p>
<p>Regarding the cerebral encoding of gender and age, we acknowledge this interesting pattern. Prior work (e.g., Charest et al., 2013) found overlapping processing regions for voice gender without clear subregional differences in the TVAs. Evidence on voice age encoding remains sparse, and we highlight this novel finding in our discussion.</p>
<p>We again thank the reviewers for their insightful comments, which have greatly improved the quality and clarity of our work.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p>
<p>(1) A set of recent advances have shown that embeddings of unsupervised/self-supervised speech models aligned to auditory responses to speech in the temporal cortex (e.g. Wav2Vec2: Millet et al NeurIPS 2022; HuBERT: Li et al. Nat Neurosci 2023; Whisper: Goldstein et al.bioRxiv 2023). These models are known to preserve a variety of speech information (phonetics, linguistic information, emotions, speaker identity, etc) and perform well in a variety of downstream tasks. These other models should be evaluated or at least discussed in the study.</p>
</disp-quote>
<p>We fully agree - the pace of progress in this area of voice technology has been incredible. Many of these models were not yet available at the time this work started so we could not use them in our comparison with cerebral representations.</p>
<p>We have now implemented Reviewer #1’s suggestion and evaluated Wav2Vec and HuBERT. The results are presented in supplementary Figure 2-S3. Correlations between activity predicted by the model and the real activity were globally comparable with those obtained with the LIN and VLS models. Interestingly both HuBERT and Wav2Vec yielded highest correlations in the mTVA, and to a lesser extent, the aTVA, as the LIN and VLS models.</p>
<disp-quote content-type="editor-comment">
<p>(2) The test statistics of the results in Fig 1c-e need to be revised. Given that logistic regression is a convex optimization problem typically converging to a global optimum, these multiple initializations of the classifier were likely not entirely independent. Consequently, the reported degrees of freedom and the effect size estimates might not accurately reflect the true variability and independence of the classifier outcomes. A more careful evaluation of these aspects is necessary to ensure the statistical robustness of the results.</p>
</disp-quote>
<p>We thank Reviewer #1 for pointing out this important issue regarding the potential dependence between multiple runs of the logistic regression model. To address this concern, we have revised our analyses and used a Wilcoxon signed-rank test to compare the decoding accuracy to chance level. The results showed that the accuracy was significantly above chance for all classifications (Wilcoxon signed-rank test, all W=15, p=0.03125). We updated Figure 1c-e and the corresponding text (L154-L155) to reflect the revised analysis. Because the focus of this section is to probe the informational content of the autoencoder’s latent spaces, and since there are only 5 decoding accuracy values per model, we dropped the inter-model statistical test.</p>
<disp-quote content-type="editor-comment">
<p>(3) In Line 198, the authors discuss the number of dimensions used in their models. To provide a comprehensive comparison, it would be informative to include direct decoding results from the original spectrograms alongside those from the VLS and LIN models. Given the vast diversity in vocal speech characteristics, it is plausible that the speaker identities might correlate with specific speech-related features also represented in both the auditory cortex and the VLS. Therefore, a clearer understanding of the original distribution of voice identities in the untransformed auditory space would be beneficial. This addition would help ascertain the extent to which transformations applied by the VLS or LIN models might be capturing or obscuring relevant auditory information.</p>
</disp-quote>
<p>We have now implemented Reviewer #1’s suggestion. The graphs on the right panel b of revised Figure 4 now show decoding results obtained from the regression performed directly on the spectrograms, rather than on representations of them, for our two example test stimuli. They can be listened to and compared to the LIN- and VLS-based reconstructions in Supplementary Audio 2. Compared to the LIN and VLS, the SPEC-based reconstructions sounded much less vocal or similar to the original, indicating that the latent spaces indeed capture more abstract voice representations, more similar to cerebral ones.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p>
<p>L31: 'in voice' &gt; consider rewording (from a voice?).</p>
<p>L33: consider splitting sentence (after interactions).</p>
<p>L39: 'brain' after parentheses.</p>
<p>L45-: certainly DNNs 'as a powerful tool' extend to audio (not just image and video) beyond their use in brain models.</p>
<p>L52: listened to / heard.</p>
<p>L63: use second/s consistently.</p>
<p>L64: the reference to Figure 5D is maybe a bit confusing here in the introduction.</p>
</disp-quote>
<p>We thank Reviewer #2 for these recommendations, which we have implemented.</p>
<disp-quote content-type="editor-comment">
<p>L79-88: this section is formulated in a way that is too detailed for the introduction text (confusing to read). Consider a more general introduction to the VLS concept here and the details of this study later.</p>
<p>L99-: again, I think the experimental details are best saved for later. It's good to provide a feel for the analysis pipeline here, but some of the details provided (number of averages, denoising, preprocessing), are anyway too unspecific to allow the reader to fully follow the analysis.</p>
</disp-quote>
<p>Again, thank you for these suggestions for improving readability: we have modified the text accordingly.</p>
<disp-quote content-type="editor-comment">
<p>L159: what was the motivation for classifying age as a 2-class classification problem? Rather than more classes or continuous prediction? How did you choose the age split?</p>
</disp-quote>
<p>The motivation for the 2 age classes was to align on the gender classification task for better comparison. The cutoff (30 years) was not driven by any scientific consideration, but by practical ones, based on the median age in our stimulus set. This is now clarified in the manuscript (L149).</p>
<disp-quote content-type="editor-comment">
<p>L263: Is the test of RDM correlation&gt;0 corrected for multiple comparisons across ROIs, subjects, and models?</p>
</disp-quote>
<p>The test of RDM correlation&gt;0 was indeed corrected for multiple comparisons for models using the permutation-based ‘maximum statistics’ framework for multiple comparison correction (described in Giordano et al., 2023 and Maris &amp; Oostenveld, 2007). This framework was applied for each ROI and subject. It was described in the Methods (L745) but not clearly enough in the text—we thank Reviewer #2 and clarified it in the text (L246, L260-L261).</p>
<disp-quote content-type="editor-comment">
<p>L379: 'these stimuli' - weren't the experimental stimuli different from those used to train the V/AE?</p>
</disp-quote>
<p>We thank Reviewer #2 for spotting this issue. Indeed, the experimental stimuli are different from those used to train the models. We corrected the text to reflect this distinction (L84-L85).</p>
<disp-quote content-type="editor-comment">
<p>L443: what are 'technical issues' that prevented subject 3 from participating in 48 runs??</p>
</disp-quote>
<p>We thank Reviewer #2 for pointing out the ambiguity in our previous statement. Participant 3 actually experienced personal health concerns that prevented them from completing the whole number of runs. We corrected this to provide a more accurate description (L442-L443).</p>
<disp-quote content-type="editor-comment">
<p>L444: participants were instructed to 'stay in the scanner'!? Do you mean 'stay still', or something?</p>
</disp-quote>
<p>We thank the Reviewer for spotting this forgotten word. We have corrected the passage (L444).</p>
<disp-quote content-type="editor-comment">
<p>L463: Hearing thresholds of 15 dB: do you mean that all had thresholds lower than 15 dB at all frequencies and at all repeated audiogram measurements?</p>
</disp-quote>
<p>We thank Reviewer #2 for spotting this error: we meant thresholds below 15dB HL. This has been corrected (L463). Indeed participants were submitted to several audiograms between fMRI sessions, to ensure no hearing loss could be caused by the scanner noise in these repeated sessions.</p>
<disp-quote content-type="editor-comment">
<p>L472: were the 4 category levels balanced across the dataset (in number of occurrences of each category combination)?</p>
</disp-quote>
<p>The dataset was fully balanced, with an equal number of samples for each combination of language, gender, age, and identity. Furthermore, to minimize potential adaptation effects, the stimuli were also balanced within each run according to these categories, and identity was balanced across sessions. We made this clearer in Main voice stimuli (L492-L496).</p>
<disp-quote content-type="editor-comment">
<p>L482: the test stimuli were selected as having high energy by the amplitude envelope. It is unclear what this means (how is the envelope extracted, what feature of it is used to measure 'high energy'?)</p>
</disp-quote>
<p>The selection of sounds with high energy was based on analyzing the amplitude envelope of each signal, which was extracted using the Hilbert transform and then filtered to refine the envelope. This envelope, which represents the signal's intensity over time, was used to measure the energy of each stimulus, and those that exceeded an arbitrary threshold were selected. From this pool of high-energy stimuli, likely including vowels, we selected six stimuli to be repeated during the scanning session, then reconstructed via decoding. This has been clarified in the text (L483-L484).</p>
<disp-quote content-type="editor-comment">
<p>L500 was the audio filtered to account for the transfer function of the Sensimetrics headphones?</p>
</disp-quote>
<p>We did not perform any filtering, as the transfer function of the Sensimetrics is already very satisfactory as is. This has been clarified in the text (L503).</p>
<disp-quote content-type="editor-comment">
<p>L500: what does 'comfortable level' correspond to and was it set per session (i.e. did it vary across sessions)?</p>
</disp-quote>
<p>By comfortable we mean around 85 dB SPL. The audio settings were kept similar across sessions. This has been added to the text (L504).</p>
<disp-quote content-type="editor-comment">
<p>L526- does the normalization imply that the reconstructed spectrograms are normalized? Were the reconstructions then scaled to undo the normalization before inversion?</p>
</disp-quote>
<p>The paragraph on spectrogram standardization was not well placed inducing confusion. We have placed this paragraph in its more suitable location, in the Deep learning section (L545L550)</p>
<disp-quote content-type="editor-comment">
<p>L606: does the identity GLM model the denoised betas from the first GLM or simply the BOLD data? The text indicates the latter, but I suspect the former.</p>
</disp-quote>
<p>Indeed: this has been clarified (L601-L602).</p>
<disp-quote content-type="editor-comment">
<p>L704: could you unpack this a bit more? It is not easy to see why you specify the summing in the objective. Shouldn't this just be the ridge objective for a given voxel/ROI? Then you could just state it in matrix notation.</p>
</disp-quote>
<p>Thanks for pointing this out: we kept the formula unchanged but clarified the text, in particular specified that the voxel id is the ith index (L695).</p>
<disp-quote content-type="editor-comment">
<p>L716: you used robust scaling for the classifications in latent space but haven't mentioned scaling here. Are we to assume that the same applies?</p>
</disp-quote>
<p>Indeed we also used robust scaling here, this is now made clear (L710-L711).</p>
<disp-quote content-type="editor-comment">
<p>L720: Pearson correlation as a performance metric and its variance will depend on the choice of test/train split sizes. Can you show that the results generalize beyond your specific choices? Maybe the report explained variance as well to get a better idea of performance.</p>
</disp-quote>
<p>We used a standard 80/20 split. We think it is beyond the scope of this study to examine the different possible choices of splits, and prefer not to spend additional time on this point which we think is relatively minor.</p>
<disp-quote content-type="editor-comment">
<p>Could you specify (somewhere) the stimulus timing in a run? ISI and stimulus duration are mentioned in different places, but it would be nice to have a summary of the temporal structure of runs.</p>
</disp-quote>
<p>This is now clarified at the beginning of the Methods section (L437-441)</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p>
<p>Code and data are not currently available.</p>
</disp-quote>
<p>Code and preprocessed data are now available (L826-827).</p>
<disp-quote content-type="editor-comment">
<p>In the supplementary material, it would be beneficial to present the different analyses as boxplots, as in the main text, but with the ROIs in the left and right hemispheres separated, to better show potential hemispheric effect. Although this information is available in the Supplementary Tables, it is currently quite tedious to access it.</p>
</disp-quote>
<p>Although we provide the complete data split by hemisphere in the Tables, we do not believe it is relevant to illustrate left/right differences, as we do not have any hypotheses regarding hemispheric lateralization–and we would be underpowered in any case to test them with only three points by hemisphere.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 3a, it might be beneficial to order the identities by age for each gender in order to more clearly illustrate the structure of the RDMs,</p>
</disp-quote>
<p>The identities are indeed already ordered by increasing age: we now make this clear.</p>
<disp-quote content-type="editor-comment">
<p>In Figure 3b, the variance for the correlations for the aTVA is higher than in other regions, why?</p>
</disp-quote>
<p>Please note that the error bar indicates variance across only 6 data points (3 subjects x 2 hemispheres) such that some fluctuations are to be expected.</p>
<disp-quote content-type="editor-comment">
<p>Please make sure that all acronyms are defined, and that they are redefined in the figure legends.</p>
</disp-quote>
<p>This has been done.</p>
<disp-quote content-type="editor-comment">
<p>Gender and age are primarily encoded by different brain regions (Figure 5, pTVA vs aTVA). How does this finding compare with existing literature?</p>
</disp-quote>
<p>This interesting finding was not expected. The cerebral processing of voice gender has been investigated by several groups including ours (Charest et al., 2013, Cerebral Cortex). Using an fMRI-adaptation design optimized using a continuous carry-over protocol and voice gender continua generated by morphing, we found that regions dealing with acoustical differences between voices of varying gender largely overlapped with the TVAs, without clear differentiation between the different subparts. Evidence for the role of the different TVAs in voice age processing remains scarce.</p>
</body>
</sub-article>
</article>