<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">107259</article-id>
<article-id pub-id-type="doi">10.7554/eLife.107259</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107259.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.4</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Genetics and Genomics</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Computational and Systems Biology</subject>
</subj-group>
</article-categories><title-group>
<article-title>JAX Animal Behavior System (JABS): A genetics informed, end-to-end advanced behavioral phenotyping platform for the laboratory mouse</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6651-5224</contrib-id>
<name>
<surname>Choudhary</surname>
<given-names>Anshul</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Geuther</surname>
<given-names>Brian Q</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Sproule</surname>
<given-names>Thomas J</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Beane</surname>
<given-names>Glen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kohar</surname>
<given-names>Vivek</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Trapszo</surname>
<given-names>Jarek</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6643-7465</contrib-id>
<name>
<surname>Kumar</surname>
<given-names>Vivek</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<email>vivek.kumar@jax.org</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021sy4w91</institution-id><institution>The Jackson Laboratory</institution></institution-wrap>, <city>Bar Harbor</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Koo</surname>
<given-names>Peter</given-names>
</name>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8722-0038</contrib-id><role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Cold Spring Harbor Laboratory</institution>
</institution-wrap>
<city>Cold Spring Harbor</city>
<country country="US">United States</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Wassum</surname>
<given-names>Kate M</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California, Los Angeles</institution>
</institution-wrap>
<city>Los Angeles</city>
<country country="US">United States</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>*</label><p>Contributed Equally</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2025-07-23">
<day>23</day>
<month>07</month>
<year>2025</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-11-14">
<day>14</day>
<month>11</month>
<year>2025</year>
</pub-date>
<volume>14</volume>
<elocation-id>RP107259</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-05-01">
<day>01</day>
<month>05</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-01-19">
<day>19</day>
<month>01</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.01.13.476229"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2025-07-23">
<day>23</day>
<month>07</month>
<year>2025</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107259.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.107259.1.sa2">eLife Assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107259.1.sa1">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.107259.1.sa0">Reviewer #2 (Public review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2025, Choudhary et al</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>Choudhary et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-107259-v2.pdf"/>
<abstract><p>Automated detection of complex animal behavior remains a challenge in neuroscience. Developments in computer vision have greatly advanced automated behavior detection and allow high-throughput preclinical and mechanistic studies. An integrated hardware and software solution is necessary to facilitate the adoption of these advances in the field of behavioral neurogenetics, particularly for non-computational laboratories. We have published a series of papers using an open field arena to annotate complex behaviors such as grooming, posture, and gait as well as higher-level constructs such as biological age and pain. Here, we present our, integrated rodent phenotyping platform, JAX Animal Behavior System (JABS), to the community for data acquisition, machine learning-based behavior annotation and classification, classifier sharing, and genetic analysis. The JABS Data Acquisition Module (JABS-DA) enables uniform data collection with its combination of 3D hardware designs and software for real-time monitoring and video data collection. JABS-Active Learning Module (JABS-AL) allows behavior annotation, classifier training, and validation. We introduce a novel graph-based framework (<italic>ethograph</italic>) that enables efficient boutwise comparison of JABS-AL classifiers. JABS-Analysis and Integration Module (JABS-AI), a web application, facilitates users to deploy and share any classifier that has been trained on JABS, reducing the effort required for behavior annotation. It supports the inference and sharing of the trained JABS classifiers and downstream genetic analyses (heritability and genetic correlation) on three curated datasets spanning 168 mouse strains that we are publicly releasing alongside this study. This enables the use of genetics as a guide to proper behavior classifier selection. This open-source tool is an ecosystem that allows the neuroscience and genetics community for shared advanced behavior analysis and reduces the barrier to entry into this new field.</p>
</abstract>
<funding-group>
<award-group id="funding-1">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00fq5cm18</institution-id>
<institution>National Institute on Drug Abuse</institution>
</institution-wrap>
</funding-source>
<award-id>DA041668</award-id>
</award-group>
<award-group id="funding-1a">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/00fq5cm18</institution-id>
<institution>National Institute on Drug Abuse</institution>
</institution-wrap>
</funding-source>
<award-id>DA048634</award-id>
</award-group>
<award-group id="funding-2">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/049v75w11</institution-id>
<institution>National Institute on Aging</institution>
</institution-wrap>
</funding-source>
<award-id>AG078530</award-id>
</award-group>
<award-group id="funding-3">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
</funding-source>
<award-id>MH138309</award-id>
</award-group>
</funding-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>We have added more analysis based on reviewer's comments. We have also added appropriate text.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<label>1</label><title>Introduction</title>
<p>Behavioral analysis in animal models seeks to link complex and dynamic behaviors with underlying genetic and neural circuit functions [<xref ref-type="bibr" rid="c1">1</xref>]. In the context of disease, altered genetic circuits shape altered neural circuits, which in turn produces altered behaviors. The primary purpose of behavior analysis in the animal is to understand the mechanisms of disease and to seek novel therapeutics to improve human health. The laboratory mouse has been at the forefront of these discoveries. However, linking altered genetic circuits to functional changes in neural circuits and ultimately behavior is challenging. These challenges are broad, however one major hurdle has always been a behavior quantification task itself. Animal behavior quantification has rapidly advanced in the past few years with the application of machine learning to the problem of behavior annotation and with the adoption of computational ethology approaches to behavioral neurogenetics [<xref ref-type="bibr" rid="c2">2</xref>–<xref ref-type="bibr" rid="c6">6</xref>].</p>
<p>These advances are mainly due to breakthroughs in the statistical learning and computer science fields which have been adopted and extended for biological applications, and have made the task of behavior annotation at high resolution scalable and objective, with increased accuracy. [<xref ref-type="bibr" rid="c7">7</xref>]. Although significant advances have been made in the annotation of animal behavior using machine vision, a major challenge remains in the democratization of these technologies. As a simple example, many labs adopt their existing apparatus to generate an intermediate representation of the animal for tasks such as tracking. These are often segmentation masks or keypoints. Each lab generally trains a custom model for these, which, depending on the complexity of the task, can require large amounts of human annotated training data. Many do not validate or even report the performance of their models, which are taken at face value to work. This is a large data labeling burden that is repeated by individual labs. The next step of extracting behaviors from these intermediate representations is even more challenging. The process entails creating features from intermediate representations followed by heuristics or classifiers to determine when a behavior of interest occurs. Behaviorists often disagree on behavior definition, even within labs, and therefore these behavior classifiers are incredibly valuable. They encode a behaviorist’s expertise in the form of mathematical weights. Since labs start with niche behavior apparatus and intermediate representations, the process of feature extraction, classification, the logic of assigning behaviors stays within a lab. That is, it is challenging for labs to share classifiers, because they only work in their hardware setup. This paradigm is not sustainable and prohibits the application of engineering principles to biology. The paradigm described above, combined with the fact that a high level of expertise is needed for proper use and interpretation of machine learning methods, can be a challenge to the reproducibility and replicability of scientific discoveries and ultimately therapeutic discoveries.</p>
<p>With this in mind, we present two complementary systems that are designed for behavior characterization in rodent models. The first platform, called JAX Animal Behavior System (JABS), consists of video collection hardware and software, a behavior labeling and active learning app, and an online database for sharing classifiers. This is an open field system which we have used in over 6 papers [<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c13">13</xref>]. Adoption of JABS will allow laboratories to bypass the need for creating segmentation or pose estimation models for routine open-field tasks. In addition, existing models for frailty [<xref ref-type="bibr" rid="c14">14</xref>], nociception [<xref ref-type="bibr" rid="c13">13</xref>], seizures [<xref ref-type="bibr" rid="c15">15</xref>], and others can be adopted. The second, called Digital InVivo System (DIV Sys) is hardened and scalable home cage monitoring system (see Robertson et. al.). Both end-to-end systems are designed to enable community members to leverage others’ work and to extend the capabilities of the system. We hope that these platforms will be adopted and extended by the community.</p>
<p>JABS adds to a list of commercial and open source animal tracking platforms. JABS covers hardware, behavior prediction, a shared resource for classifiers, and genetic association studies. Were not aware of another system that encompasses all these components. Commercial packages such as EthoVision XT and HomeCage Scan give users a ready-made camera-plus-software solution that automatically tracks each mouse and reports simple measures such as distance traveled or time spent in preset zones, but they do not provide open hardware designs, editable behavior classifiers, or any genetics workflow. At the open-source end, there are greater than 100 projects cataloged on OpenBehavior and summarized in recent reviews (Luxem et al., 2023 [<xref ref-type="bibr" rid="c4">4</xref>]; Ik &amp; Ünal 2023 [<xref ref-type="bibr" rid="c16">16</xref>]) that usually cover only one link in the chainDIY rigs, pose-tracking libraries (e.g., DeepLabCut [<xref ref-type="bibr" rid="c17">17</xref>], SLEAP [<xref ref-type="bibr" rid="c18">18</xref>]) or supervised and unsupervised behaviour-classifier pipelines (e.g., SimBA [<xref ref-type="bibr" rid="c19">19</xref>], MARS [<xref ref-type="bibr" rid="c20">20</xref>], JAABA [<xref ref-type="bibr" rid="c21">21</xref>], B-SOiD [<xref ref-type="bibr" rid="c22">22</xref>], DeepEthogram [<xref ref-type="bibr" rid="c23">23</xref>]). JABS provides an open source ecosystem that integrates all four: (i) top-down arena hardware with parts list and assembly guide; (ii) an active-learning GUI that produces shareable classifiers; (iii) a public web service that enables sharing of the trained classifier and applies any up-loaded classifier to a large and diverse strain survey; and (iv) built-in heritability, genetic-correlation and GWAS reporting.</p>
</sec>
<sec id="s2">
<label>2</label><title>Results</title>
<p>JABS is an integrated platform developed in our lab over the past five years with pose and segmentation models as intermediate representations. Our lab has previously used computer-vision methods to track visually diverse mice under different environmental conditions [<xref ref-type="bibr" rid="c8">8</xref>], infer pose for gait analysis and posture analysis [<xref ref-type="bibr" rid="c10">10</xref>], and detect complex behaviors like grooming [<xref ref-type="bibr" rid="c9">9</xref>]. We have also used computervision derived features to predict complex constructs, such as health and frailty and pain [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. These models have been trained and validated on genetically diverse mouse strains for high-quality foundational metrics [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c10">10</xref>]. JABS hardware and software has been used to characterize complex behaviors such as grooming, gait, posture, as well as complex states such as frailty, pain, and intensity of seizures [<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref>]. JAX has made components of JABS including ML models are free to use for non-commercial purposes.</p>
<p>The process and various components of JABS are illustrated <xref rid="fig1" ref-type="fig">Figure 1A</xref>. Briefly, our system comprises of three components encompassing five different processes, namely, i) data acquisition, ii) behavior annotation, iii) classifier training, iv) behavior characterization, and v) data integration. The first component (JABS-DA module) is the custom designed standardized data acquisition hardware and software that provides a controlled environment, optimized video storage, and live monitoring capabilities. The second component (JABS-AL module) is a python based GUI active learning app for behavior annotation and training classifiers using the annotated data. One can then use the trained classifiers for predicting whether behavior happens or not in the unlabeled frames. The last component of JABS is analysis and integration module (JABS-AI), a web application that provides an inter-active user interface to browse through the strain survey results from different classifiers, download existing classifiers and related training data. The app can also be used to classify various behaviors in user submitted videos (pose files) using the classifiers available in the database. Furthermore, researchers have the option to contribute their custom classifiers, trained through the JABS-AL app. These user-generated classifiers can be submitted to perform predictions within our extensive strain survey dataset, coupled with comprehensive genetic analysis, including assessments of heritability and genetic correlations. Next, we discuss the individual components of JABS in detail.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>JABS data acquisition module (JABS-DA).</title>
<p>The JABS-DA consists of hardware and software for video data acquisition and processing. (A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B) Detailed example of JABS data acquisition, including a picture of the monitoring hardware, architecture of the real-time monitoring app, and screenshots from videos taken during daytime and nighttime. The open field arena is shown from the outside (left), and a screenshot of video data is shown on the right. JABS-DA blocks visible light to the camera and only collects data using IR illumination, which produces uniform data during day and night. The JABS-DA computer hardware and software (middle) allow streaming of video data from edge devices, which enables remote welfare checks and web based experiment setup and monitoring. Data compression is handled on these edge devices.</p></caption>
<graphic xlink:href="476229v4_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<sec id="s2a">
<label>2.1</label><title>JABS data acquisition - Hardware and Software</title>
<p>We use a standardized hardware setup for high quality data collection and optimized storage (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The end result is uniform video data across day and night. Complete details of the software and hardware, including 3D designs used for data collection, are available on our Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/JABS-data-pipeline/tree/main">https://github.com/KumarLabJax/JABS-data-pipeline/tree/main</ext-link>). We also provide a step-by-step assembly guide (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/JABS-data-pipeline/blob/main/Multi-day%20setup%20PowerPoint%20V3.pptx">https://github.com/KumarLabJax/JABS-data-pipeline/blob/main/Multi-day%20setup%20PowerPoint%20V3.pptx</ext-link>).</p>
<p>We have organized the animal habitat design into three groups of specifications. The first group of specifications are requirements necessary for enabling compatibility with our machine learning algorithms. The second group describes components that can be modified as long as they produce data that adheres to the first group. The third group describes components that do not affect compatibility with our machine learning algorithms. While we distinguish between abstract requirements in group 1 and specific hardware in group 2 that meet those requirements, we recommend that users of our algorithms use our specific hardware in group 2 to ensure compatibility.</p>
<p>The design elements that are critical to match specifications in order to re-use machine learning algorithms include (1a) the camera viewpoint, (1b) minimum camera resolution and frame rate, (1c) field of view and imaging distance, (1d) image quality and (1e) the general appearance of the habitat (cage or enclosure). The design elements that are flexible but impact the compatibility are (2a) camera model, (2b) compute interface for capturing frames, (2c) lighting conditions, (2d) strains and ages of mice and (2e) animal bedding contents and quantity. Design elements that have no impact on compatibility are (3a) height of habitat walls to prevent mice from escaping, (3b) animal husbandry concerns, (3c) mounting hardware, (3d) technician ergonomic considerations and (3e) electrical connection hardware and management.</p>
<sec id="s2a1">
<label>2.1.1</label><title>Group 1 specifications</title>
<p>Generally speaking, multi-view imaging will provide the most accurate pose models but requires increased resources on both hardware setup as well as processing of data. Our system operates on a top-down camera viewpoint. This specification enables flexibility and allows more diverse downstream hardware and ease of construction. Top-down provides the advantage of flexibility for materials, since the floor doesnt need to be transparent. Additionally lighting and potential reflection with the bottom-up perspective. Since the paws are not occluded from the bottom-up perspective, models should have improved paw keypoint precision allowing the model to observe more subtle behaviors. However, the appearance of the arena floor will change over time as the mice defecate and urinate. Care must be taken to clean the arena between recordings to ensure transparency is maintained. This doesnt impact top-down imaging that much but will occlude or distort from the bottom-up perspective. Additionally, the inclusion of bedding for longer recordings, which is required by IACUC, will essentially render bottom-up imaging useless because the bedding will completely obscure the mouse.</p>
<p>Our algorithms are trained using data originating from 800×800 pixel resolution image data and 30 frames per second temporal resolution. This resolution was selected to strike a balance between resolution of the data and size of data produced. While imaging at higher spatial and temporal resolution is possible and sometimes necessary for certain behaviors, these values were selected for general mouse behavior such as grooming, gait, posture, and social interactions. We train and test our developed algorithms against the spatial resolution. We note that these are minimum requirements, and down-sampling higher resolution and frame rate data still allows our algorithms to be applied.</p>
<p>Similar to the pixel resolution, we also specify the field of view and imaging distance for the acquired images in real-world coordinates. These are necessary to achieve similar camera perspectives on imaged mice. Cameras must be mounted at a working distance of approximately 100cm above the floor of the arena. Additionally, the field of view of the arena should allow for between 5 − 15% of the pixels to view the walls (field of view between 55cm and 60cm). Having the camera a far distance away from the arena floor reduces the effect of both perspective distortion and barrel distortion. We selected values such that our custom camera calibrations are not necessary, as any error introduced by these distortions are typically less than 1%.</p>
<p>Additionally, image quality is important for meeting valid criteria for enabling the use of machine learning algorithms. Carefully adjusting a variety of parameters of hardware and software values in order to achieve similar sharpness and overall quality of the image is important. While we cannot provide an exact number or metric to meet this quality, users of our algorithms should strive for equal or better quality that exists within our training data. One of the most overlooked aspect of image quality in behavioral recordings is image compression. We recommend against using typical software-default video compression algorithms and instead recommend using either defaults outlined in the software we use or recording uncompressed video data. Using software-defaults will introduce compression artifacts into the video and will affect algorithm performance. For example, most video recording software will default to having a constant keyframe rate, typically around every 30 frames. This will artificially create a measurable signal in video features at the keyframe rate.</p>
<p>Finally, the general appearance of the cage should be visually similar to the variety of training data used in training the machine learning algorithms. Documentation on this for each individual algorithm for assessing the limitations are published [<xref ref-type="bibr" rid="c8">8</xref>–<xref ref-type="bibr" rid="c11">11</xref>]. While our group strives for the most general visual diversities in mice behavioral assays, we still need to acknowledge that any machine learning algorithms should always be validated on new datasets that they are applied to. Generally our machine learning algorithms earlier in the entire processing pipeline, such as pose estimation, are trained on more diverse datasets than algorithms later in the pipeline, such as pain and frailty predictions.</p>
</sec>
<sec id="s2a2">
<label>2.1.2</label><title>Group 2 specifications</title>
<p>In order to achieve compliant imaging data for use with our machine learning algorithms, we specify the hardware we use. While the hardware and software mentioned in this section is modifiable, we recommend that careful consideration is taken such that changes still produce complaint video data.</p>
<p>We modified a standard open field arena that has been used for high-throughput behavioral screens [<xref ref-type="bibr" rid="c24">24</xref>]. The animal environment floor is 52 cm square with 92 cm high walls to prevent animals escaping and to limit environmental effects. The floor was cut from a 6mm sheet of Celtec (Scranton, PA) Expanded PVC Sheet, Celtec 700, White, Satin / Smooth, Digital Print Gradesquare and the walls from 6mm thick Celtec Expanded PVC Sheet, Celtec 700, Gray, (6 mm x 48 in x 96 in), Satin / Smooth, Digital Print Grade. All non-moving seams were bonded with adhesive from the same manufacturer. We used a Basler (Highland, IL) acA1300-75gm camera with a Tamron (Commack, NY) 12VM412ASIR 1/2&quot; 4-12mm F/1.2 Infrared Manual C-Mount Lens. Additionally, to control for lighting conditions, we mounted a Hoya (Wattana, Bangkok) IR-80 (800nm), 50.8mm Sq., 2.5mm Thick, Colored Glass Longpass Filter in front of the lens using a 3D printed mount. Our cameras are mounted 105 +/- 5 cm above the habitat floor and powered the camera using the power over ethernet (PoE) option with a TRENDnet (Torrance, CA) Gigabit Power Over Ethernet Plus Injector. For IR lighting, we used 6 10 inch segments of LED infrared light strips (LightingWill DC12V SMD5050 300LEDs IR InfraRed 940nm Tri-chip White PCB Flexible LED Strips 60LEDs 14.4W Per Meter) mounted on 16-inch plastic around the camera. We used 940nm LED after testing 850nm LED which produced a marked red hue. The light sections were coupled with the manufactured connectors and powered from an 120vac:12vdc power adapter.</p>
<p>For image capture, we connected the camera to an nVidia (Santa Clara, CA) Jetson AGX Xavier development kit embedded computer. To store the images, we connected a local four-terabyte (4TB) USB connected hard drive (Toshiba (Tokyo, Japan) Canvio Basics 4TB Portable External Hard Drive USB 3.0) to the embedded device. When writing compressed videos to disk, our software both applies optimized de-noising filters as well as selecting low compression settings for the codec. While most other systems rely on the codec for compression, we rely on applying more specific de-noising to remove unwanted information instead of risking visual artifacts in important areas of the image. We utilize the free ffmpeg library for handling this filtering and compression steps with the specific settings available in our shared C++ recording software. Complete parts list and assembly steps are described in (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/JABS-data-pipeline">https://github.com/KumarLabJax/JABS-data-pipeline</ext-link>)</p>
</sec>
<sec id="s2a3">
<label>2.1.3</label><title>Group 3 specifications</title>
<p>Finally, here we present hardware and software that can be modified without risk of affecting video compliance. For natural light, we used a F&amp;V (Netherlands) fully dimmable R-300SE Daylight LED ring light powered by a 120vac:12vdc power adapter. These lights are adjustable to meet the visible lighting needs of specific assays without affecting the visual appearance of the data. To keep the animals nourished, we installed water bottles and a food hopper external to the animal environment. These were placed on the outside of the arena on a removable panel. The panel can be customized as needed for experiments without the need to replace/modify the entire arena. To suspend the camera and lights, we used a wire shelf from our solution for technician ergonomics.</p>
<p>To raise the animal cage to an ergonomic height, we used the 24-inch by 24-inch option of the Metro (Wilkes-Barre, PA) Super Erecta wire shelving system with three shelves. As mentioned in the earlier paragraph, the topmost shelf was used to suspend the camera and lights. We also hinged one wall, turning it into a door, to allow easier animal access. Communication between the electronic devices was interconnected with CAT5 cables and a network switch and a powered USB hub was used between the USB connected hard drive and the nVidia compute device. We used a digital timer for the visible LED light, a 120v power strip to consolidate the power, and a universal power source (battery backup) between the chamber and facility power.</p>
<p>For ease of use and reduction of environmental noise, we also include a software for remote monitoring and welfare check. The software consists of three main components: a recording client implemented in C++, a control server implemented with the Flask Python framework, and a web-based user interface implemented with Angular (<xref rid="fig1" ref-type="fig">Figure 1</xref>). The recording client runs locally on each Nvidia Jetson Xavier computer and communicates with the server using the Microsoft C++ REST SDK to provide centralized monitoring and control of distributed recording devices. The recording client captures raw frames from the camera and encodes video using the ffmpeg library. In addition to saving encoded video on the local hard drive, the recording client can optionally send video over the RTMP protocol to a NGINX server configured with the nginx-rtmp plug-in. The web interface communicates with the control server, which relays recording start and stop commands to individual recording devices, enabling the user to remotely control various aspects of recording in addition to viewing the live stream from the NGINX streaming server using the HTTP Live Streaming (HLS) protocol (<xref rid="fig2" ref-type="fig">Figure 2</xref>). Each JABS-DA unit has its own edge device (Nvidia Jetson). Each system (which we define as multiple JABS-DA areas associated with one lab/group) can have multiple recording devices (arenas). The system requires only 1 control portal (RPi computer) and can handle as many recording devices as needed (Nvidia computer w/ camera associated with each JABS-DA arena). To collect data, 1 additional computer is needed to visit the web control portal and initiate a recording session. Since this is a web portal, users can use any computer or a tablet. The recording devices are not strictly synchronized but can be controlled in a unified manner.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>JABS data acquisition module (JABS-DA)</title>
<p>consists of a web-based control system for recording and monitoring experiments. (A) JABS pipeline. (B-E) Screenshots from Angular web client that allows monitoring of multiple JABS Acquisition units in multiple physical locations can be seen on one screen (B). Dashboard view allows monitoring of all JABS units and their status, Device Status provided detailed data on individual devices (C) Recording session dashboard allows initiation of new experiments (D), and remote welfare view allows live video to be streamed from each unit (E).</p></caption>
<graphic xlink:href="476229v4_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label><title>Environment checks</title>
<p>To evaluate the suitability of JABS-DA for long-term housing of mice, we conducted a series of experiments comparing environmental conditions and animal health outcomes within these arenas to those observed in standard JAX housing cages. Our goal was to provide data for the JAX Institutional Animal Care and Use Committee (IACUC) to confirm health and welfare of animals over time in these apparatus. These data can be used for Institutional ACUC protocols by others. We compare our data with established guidelines from the Guide for the Care and Use of Laboratory Animals (the Guide) [<xref ref-type="bibr" rid="c25">25</xref>]. Our experiments were performed in one room at The Jackson Laboratory, Bar Harbor, Maine (JAX) with temperature and humidity set to 70-74<italic>°</italic>F (<sup>∼</sup>21-23<italic>°</italic>C) and 40-50%, respectively.</p>
<p>One concern related to use of the JABS arena in long-term experiments was that the 90 cm height of the walls without lower air openings might result in inadequate air flow and build up of toxic gases. To address this, we compared environmental parameters in JABS arenas with that of a standard JAX housing cage. Two JABS arenas were observed with 12 male C57BL/6J mice 12-16 weeks old in each for a 14-day period. At the same time, one wean cage containing 10 male C57BL/6J age-matched mice was observed on a conventional rack for matching air flow in the same room. We used a #2 Wean Cage (30.80 x 30.80 x 14.29 cm) from Thoren (Hazleton, Pennsylvania) with 727.8 cm<sup>2</sup> floor space, which is a common housing container for mice and is approved at JAX to house 10 animals. This commercial cage has a floor area that is <sup>∼</sup>1/4 that of the JABS arena. The ceiling height in the wean cage ranges 5-14 cm due to the sloped metal wire cover that contains food and water. The JABS arena, by contrast, has no ceiling. Food, water, bedding type and depth and light level were all matched in the arenas and wean cage. Bedding (1:1 ratio of aspen chip/shavings mix and Alpha-Dri) was left unchanged for the full two-week period to minimize interaction with mice in JABS arenas as much as possible. To determine if forced air flow was needed for an acceptable arena environment, one of the two arenas and the wean cage were exposed to normal room air flow, while the second arena had a 6-inch quiet electric fan mounted above for increased circulation. The fan was pointed to blow air up to draw air out of the arena instead of actively blowing air towards the mice.</p>
<p>We monitored CO<sub>2</sub> and ammonia, common housing gases [<xref ref-type="bibr" rid="c25">25</xref>]. CO<sub>2</sub> was measured with an Amprobe CO<sub>2</sub> meter daily, excluding weekends and holidays, in both arenas and the wean cage. CO<sub>2</sub> was recorded in the room’s center before and after each arena and wean cage measurement as a control. For higher levels, CO<sub>2</sub> is shown as a range due to oscillation. Ammonia was tested with Sensidyne Ammonia Gas Detector Tubes (5-260 ppm) in the arena without a fan and the wean cage on days 0, 2, 4, 7, and 14, with samples taken near the floor and waste accumulation areas. Temperature and humidity data loggers (MadgeTech RHTEMP1000IS) were placed on the floor in each arena and the wean cage for the experiment’s duration. An environment monitor (Hobo, U12-012, Onset) was mounted on the wall for room background data. Body weight was measured daily, excluding weekends and holidays. Grain and water were weighed at the start and end of each experiment to check consumption.</p>
<p>We observed daily room background CO<sub>2</sub> levels of 454 to 502 ppm throughout the 14-day experiment. These are very close to expected outdoors levels and indicative of a high air exchange rate [<xref ref-type="bibr" rid="c26">26</xref>]. JABS arena CO<sub>2</sub> levels varied from a low of 511 ppm on day 1 to an oscillating range of 630 to 1565 ppm on day 14. The JAX standard wean cage experienced an oscillating range of 2320 to 2830 ppm on day 0 climbing to an oscillating range of 3650 to 4370 ppm on day 14. The wean cage CO<sub>2</sub> values approximately match those from another published study of maximum grouped mice in similar static housing [<xref ref-type="bibr" rid="c27">27</xref>]. Indoor CO<sub>2</sub> is often evaluated as level above background [<xref ref-type="bibr" rid="c26">26</xref>]. We observe a maximum JABS arena CO<sub>2</sub> level above background of 1082 ppm. This is 3.8 fold lower than the maximum observed CO<sub>2</sub> levels in the wean cage (4121 ppm) (<xref rid="figs1" ref-type="fig">Figure S1B</xref>, arena with fan excluded from graph for clarity).</p>
<p>Ammonia levels in the JABS arena were below 5 ppm on days 0, 2, 4, and 7, rising to 18 ppm on day 14. In the wean cage, levels were &lt;5 ppm on days 0 and 2, rose to 52 ppm on day 4, and remained at <sup>∼</sup>230ppm on days 7 and 14. Initial concerns about high JABS arena walls hindering airflow were alleviated as CO<sub>2</sub> and ammonia levels indicated better air exchange than standard housing. NIOSH’s recommended maximum ammonia exposure for humans is 25 ppm over 8 hours, with a similar recommendation for mice [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. Ammonia levels are mainly influenced by air changes per hour (ACH) [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. JAX animal rooms have <sup>∼</sup>10 ACH and PIV cages have <sup>∼</sup>55-65 ACH. Ammonia levels were consistently 10-50 times lower in the JABS arena compared to the control static wean cage and remained well within recommended limits (<xref rid="figs1" ref-type="fig">Figure S1C</xref>). Future JABS arena observations must consider the impact of ammonia on behavior [<xref ref-type="bibr" rid="c31">31</xref>]. Mice used in JABS experiments come from PIV housing, where ammonia levels are expected to be similar to those in the JABS arena, minimizing behavioral impact [<xref ref-type="bibr" rid="c30">30</xref>].</p>
<p>Temperatures in all locations (room background, two JABS arenas and one wean cage) remained in a range of 22-26<italic>°</italic>C throughout the experiment. Variance in room background readings suggest temperature fluctuations are more due to innate room conditions (such as environmental controls) than anything else. We find that arena structure does not adversely affect control of the temperature to which mice are exposed (<xref rid="figs1" ref-type="fig">Figure S1D</xref>).</p>
<p>The probes which measured temperature also measured humidity. The room probe, mounted on a wall 1 foot above the floor in the 8×8 feet room, recorded consistent background humidity of 45 ±5% (<xref rid="figs1" ref-type="fig">Figure S1E</xref>, green line). Housing probes in the bedding of each chambercentered in JABS arenas and along a wall in the smaller wean cagerecorded 55-60% humidity in the JABS arenas, except for occasional spikes not correlated with background changes, likely due to mouse urination (<xref rid="figs1" ref-type="fig">Figure S1E</xref>, blue and black lines). In contrast, wean cage humidity rose from 55-60% to above 75% within 12 hours and continued climbing to 97.5% by day 14 (<xref rid="figs1" ref-type="fig">Figure S1E</xref>, red line). Higher humidity in the micro-environments was due to mouse urination and limited air flow (Guide [<xref ref-type="bibr" rid="c25">25</xref>]). The JABS arenas maintained a drier environment because they had a higher bedding to mouse ratio (3.2 times more per mouse) and better air circulation compared to the wean cage (<xref rid="figs1" ref-type="fig">Figure S1E</xref>).</p>
<p>Weight is often used as a marker for health though body condition score is used as a more reliable indicator of serious concerns [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c33">33</xref>]. Mice in JABS arenas lost weight compared to those in the wean cage and this was initially a cause of concern. However, mice in JABS arenas maintained a healthy appearance and normal body condition score throughout the experiment. Other measurements demonstrating normal parameters and other control experiments not shown additionally led us to believe the weight differences are because JABS arena mice are active while wean cage mice, with more limited movement available, are sedentary. Mice started the experiment at 25-33 grams body weight. The lowest average recorded during the experiment was 95.6% of the start value, for mice in the JABS arena without a fan on day 9. The lowest individual recorded was 85.8% of start value at 23.6 grams on day 14, also in the arena without a fan (<xref rid="figs1" ref-type="fig">Figure S1F</xref>).</p>
<p>Per mouse grain usage was comparable between the JABS arena and the wean cage and in an expected range [<xref ref-type="bibr" rid="c34">34</xref>] (<xref rid="figs1" ref-type="fig">Figure S1G</xref>). Per mouse water usage was comparable between the JABS arena and the wean cage and in the expected range [<xref ref-type="bibr" rid="c35">35</xref>]. Somewhat higher water use in the arena could be indicative of higher activity requiring more hydration (<xref rid="figs1" ref-type="fig">Figure S1H</xref>). Since only one JABS arena and one wean cage were tested, error bars are not available to aid in interpretation.</p>
<p>Three mice from one arena and three from a wean cage were necropsied immediately following 14 days in the JABS arena or control wean cage to determine if any environmental conditions, such as possible low air flow in arenas potentially leading to a buildup of heavy unhealthy gases like ammonia or CO<sub>2</sub>, were detrimental to mouse health. Nasal cavity, eyes, trachea, and lungs were collected from each mouse. They were H&amp;E stained and analyzed by a qualified pathologist. No signs of pathology were observed in any of the tissue samples collected (<xref rid="figs2" ref-type="fig">Figure S2</xref>).</p>
<p>Based on these environmental and histological analysis, we conclude that the JABS arena is comparable and in many respects better than a standard wean cage. Lack of holes near the floor do not create a build up of ammonia or CO<sub>2</sub>. Mice ate and drank at normal levels. We initially observed a slight decrease in body weight, which is increased in the next few days. We hypothesize that this could be due to the novel environment and the increase in space for movement, leading to more active mice.</p>
</sec>
<sec id="s2c">
<label>2.3</label><title>JABS-AL: An active learning module for behavior classifier training</title>
<p>In the section, we first present an overview on behavior annotation and classifier training using JABS-AL module which utilizes our python-based, open-source graphical user interface (GUI) application which has been developed to be compatible with Mac, Linux and Windows operating systems. We then evaluate the utility and accuracy of JABS trained classifiers through two complementary approaches. In the first approach, we benchmark the performance of JABS classifiers against a previous neural network based approach [<xref ref-type="bibr" rid="c9">9</xref>], providing us a comparison of the performance of the two approaches on the same dataset. In the second approach, we studied how classifiers for the same behavior trained by two different human annotators in the lab compare with each other in terms of behavior identification, allowing us to assess the inherent variability among expert annotators.</p>
<sec id="s2c1">
<label>2.3.1</label><title>Behavior annotation and classifier training</title>
<p>There are two prominent approaches in the literature for training behavioral classifiers. The first approach trains the classifiers using the raw video files, as previously demonstrated to identify grooming behavior through the use of a deep neural network [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c23">23</xref>]. The second approach involves first extracting pose keypoints in each frame using deep neural networks, which serves as inputs for machine learning classifiers [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c36">36</xref>]. Previously, we utilized a deep neural network based classifier to extract poses and used the keypoints to study gait behavior [<xref ref-type="bibr" rid="c10">10</xref>]. Pose based approach offers the flexibility to use the identified poses for training classifiers for multiple behaviors and we used this approach for JABS. Additionally, the extracted keypoints can also be used to generate quantifiable and interpretable features that can be used to study various aspects of animal behavior such as gait and posture. In addition to the raw video file, JABS annotation and classification active learning module requires pose files from our previously established neural network for pose estimation as an input to train the classifiers. Note that the raw videos are needed only for annotating behaviors, and one can predict the behaviors using only the pose files.</p>
<p>We have developed an easy to use open source python GUI software to annotate behaviors in videos, as shown in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>. This tool allows users to easily annotate behaviors in video recordings through mouse/trackpad or keyboard shortcuts, as well as the option to leave frames unlabeled for ambigious cases. The GUI provides statistics of the total number of frames as well as the number of frames and bouts annotated for a particular behavior. The annotations are displayed below the video as an ethogram (<xref rid="fig3" ref-type="fig">Fig. 3B</xref>).The user can annotate multiple behaviors for the same video. Once minimum number of frames (100) and videos (2) have been annotated, the user can train a classifier using either of the tree-based methods such as Random Forest (RF)/Gradient Boost/XGBoost (XGB) [<xref ref-type="bibr" rid="c37">37</xref>–<xref ref-type="bibr" rid="c40">40</xref>] and check the classifiers accuracy with k-fold cross-validation, selecting a value of k that balances computational efficiency and accuracy. The input features for these classifiers are derived from the animal’s pose, which must be estimated prior to using the GUI. This is accomplished using our separate pose tracking pipeline (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/mouse-tracking-runtime">https://github.com/KumarLabJax/mouse-tracking-runtime</ext-link>), which employs an HRNet-based neural network [<xref ref-type="bibr" rid="c10">10</xref>] to identify the location of twelve keypoints in each video frame. This pipeline is described in greater detail in the Methods section. We then compute a number of informative features like distance between various keypoints, linear and angular velocity between keypoints, etc. that are used as input for these classifiers. We also incorporate temporal information from the videos by computing window features that include information from <italic>w</italic> (window size) frames on each side of the current frame. A complete list of base features currently included in JABS is provided in the supplementary information (<xref rid="tbls4" ref-type="table">Table S4</xref>). The weights of different features used by the trained classifiers improve the interpretability of the classifiers.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>JABS-AL is a behavior annotation and classification module that allows training classifiers with sparse labels.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B) Screenshot of the python based open source GUI application used for annotating multiple videos frame by frame. One can annotate multiple mouse and for multiple behaviors. The labeled data is used for training classifiers using either random forest or gradient boosting methods. Adjustable window size (number of frames on the left and right of the current frame) to include features from a window of frames around the current frame. The labels and predicted labels are displayed at the bottom. (C) A sample workflow for training a typical classifier. Multiple experts can sparsely label videos to train multiple classifiers for the same behavior. These classifiers can be compared and experts can consult to iterate through the training process</p></caption>
<graphic xlink:href="476229v4_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Typically, to arrive at an optimal classifier for a behavior, we start by training multiple classifiers using annotated data from different human experts for the same set of videos and then evaluating performance of each classifier against a separate set of test videos as depicted in <xref rid="fig3" ref-type="fig">Fig. 3C</xref>. However, a core challenge is that different researchers may &quot;see&quot; and define the same behavior differently. This leads to low inter-rater reliability, where two people annotating the same video produce different labels. Disagreements often arise from two primary sources: the granularity of the labels and their reliance on subjective interpretation versus objective, physical descriptions. These are often referred to as labeling style due to annotator encoding their intuition about the behavior [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. Even when there is high interannotator agreement, explainable machine learning methods have shown that labelers rely on different features for labeling the same behavior [<xref ref-type="bibr" rid="c19">19</xref>]. Added frustration results when experts have to redefine and relabel videos to increase interannotator agreement. Researchers often try to break down complex behaviors into heuristic rules for the labelers to follow, which almost always leads to conflict between these rules and intuitive understanding of the complex behavior. These predetermined rules never capture the full repertoire of a complex behavior and often ask the labeler to violate their intuition.</p>
<p>In JABS we advocate that each expert sparsely labels frames from a video library to create a behavior classifier. These are essentially expert specific classifiers that encode the intuition or labeling style of that behaviorist. These classifiers are inferred on a set of test videos that have been set aside (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). Each expert’s classifiers are inferred from these test videos. We can then compare interannotator agreement using these inferred sense labeled videos. Experts can confer among themselves to reach a consensus on whether they agree on the inference of each classifier, potentially altering their definition of the behavior and altering their labeling style (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). JABS allows the experts to reiterate through the labeling process to arrive at a consensus. The process gives expert labelers an opportunity to agree on a behavior definition but does not operationalize the behavior through rules. In the end there may be multiple expert-specific classifiers for the same behavior, and end users must decide which classifiers they prefer to use. In this paper, we propose the use of broad-sense heritability to prioritize classifiers for the same behavior derived from different expert’s labels.</p>
<p>After classifiers are finalized, we often ask experts to densely label (label every frame) a another small set of test videos, which are used for final validation. We find that creating the dense label dataset at the end allows the labelers to have a clear understanding and intuitive definition of the behavior, whereas creating the dense label in the beginning of a project leads to frequent shift in behavior definition as the expert sees more instances of a behavior, particularly edge cages. The detailed user guide, along with a video tutorial on how to install and run the JABS Active Learning app, is available online (<ext-link ext-link-type="uri" xlink:href="https://jabs-tutorial.readthedocs.io/en/latest/JABS_user_guide.html">https://jabs-tutorial.readthedocs.io/en/latest/JABS_user_guide.html</ext-link>).</p>
</sec>
<sec id="s2c2">
<label>2.3.2</label><title>Benchmarking JABS classifier using grooming behavior</title>
<p>Previously, a CNN based grooming behavior classifier trained on raw videos attained human level accuracy [<xref ref-type="bibr" rid="c9">9</xref>]. We re-purpose this large training dataset as a benchmark for estimating learning capacity of pose-based classifiers. For context, we report JABS classifier performance against both JAABA and CNN in [<xref ref-type="bibr" rid="c9">9</xref>]. Further, we evaluate how the performance of the classifier varies with the choice of machine learning algorithm, window size (<italic>w</italic>) of the features and the amount of training data. For the choice of machine learning algorithm, we utilize two popular tree-based methods, namely Random Forest (RF) and XGBoost (XGB). Briefly, the dataset contains 1,253 video segments, and we held out 153 video clips for validation (this is the same validation set used in [<xref ref-type="bibr" rid="c9">9</xref>]) and the rest are used to train the classifier. This split results in similar distributions of frame-level classifications between training and validation sets. More details of the dataset are available in Table-S1. We trained multiple classifiers by varying the amount of annotated data, window size, and machine learning algorithm. Our best accuracy from the neural network based approach for this dataset was 0.937 and the best classifier from JABS using all the annotated data, a window size (<italic>w</italic>) of 60 frames, and XGB machine learning algorithm achieved a comparable per-frame accuracy score of 0.9364. We noticed that with the same set of features, XGB typically achieved better accuracy than RF method across different window sizes and training data size. The results for these benchmark tests are shown in <xref rid="fig4" ref-type="fig">Figure 4B-D</xref>. Our tests with different window sizes show that grooming performance increases as we increase the window size, reaches a maximum (around 60 frames) and then degrades for large window sizes (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). Because grooming typically lasts for few seconds, classifiers using features within nearby frames will perform better as they incorporate optimal temporal information and including features from too few or too many frames will decrease the performance. We also investigate the impact of the amount of labeled data on the performance of JABS classifiers, as it can help to optimize the annotation process, ultimately reducing the time and resources required to train the model. To do this, we trained the XGB and RF classifiers using a subset of the full dataset (about 20 hours) consisting of 10, 20, 50, 100, 500 and 1100 training videos. These correspond to approximately 1.3%, 2.2%, 4.4%, 8.5%, 46.1% and 100% out of a total of 2181790 frames. As expected, the performance of JABS improves as we include more labeled data. However, the results demonstrate that a high degree of accuracy, approaching 85%, can be attained through the utilization of only 10 videos of training data, as evidenced by the corresponding area under the receiver operating characteristic curve (AUROC) of approximately 0.94, as depicted in <xref rid="fig4" ref-type="fig">Figure 4C-E</xref>. Additionally, it was found that the true positive rate (TPR) experienced a minimal decrease of about 1% when the training data was reduced from 100% to 50%, while maintaining a false positive rate (FPR) of 5% (<xref rid="fig4" ref-type="fig">Fig. 4F</xref>). To assess the generalizability of our grooming classifier across genetic diversity, we trained the model on videos spanning 60 genetically diverse mouse strains (n = 1,100 videos) and evaluated performance on an independent test set comprising 51 genetically diverse strains (n = 153 videos). Per-strain analysis revealed robust and uniform performance across the majority of genetic backgrounds (median F1 = 0.94, IQR = 0.8990.956). We observed only modest performance declines in albino strains, attributed to reduced visual contrast under infrared illumination conditions.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>JABS Benchmarks: Selecting hyper-parameters and benchmarking JABS classifiers using grooming dataset.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. Using feature window size, type of classification algorithm and the number of training videos as our benchmarking parameters: (B) Accuracy of JABS classifiers trained using different window size (in frames) features. Each boxplot shows the range of accuracy values for different number of training videos and type of classification algorithms. (C, D) The effect of increasing the training data size on Accuracy and AUROC score of the JABS classifiers. (E) ROC curves for the JABS classifier trained with the window size of 60, XGB algorithm and varying training data size. (F) True positive rate at 5% false positive rate corresponding to the JABS classifier from panel (E) as the amount of training data is changed. (G) Comparing the performance of JABS based classifiers with a 3D Convolutional neural network (CNN) and JAABA based classifiers for different training data sizes. JAABA and CNN results were adopted from [<xref ref-type="bibr" rid="c9">9</xref>].</p></caption>
<graphic xlink:href="476229v4_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In the rapidly evolving field of automated quantification of animal behavior, two predominant methodologies have been established for learning behavior: using raw video data and using a reduced representation (abstraction) of the animal with certain keypoints, from which informed features are calculated [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c19">19</xref>–<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c41">41</xref>]. To understand the trade-offs and strengths of each approach, we evaluate the performance of different classifiers that employ these methodologies when utilizing varying amounts of training data, as depicted in <xref rid="fig4" ref-type="fig">Fig. 4G</xref>. Interestingly, our findings demonstrate that utilizing keypoint-based low dimensional representation of animal behavior, as employed by JABS and JAABA [<xref ref-type="bibr" rid="c21">21</xref>] methodologies, leads to superior performance when compared to using high dimensional raw video data as employed by 3D CNNs, particularly when the availability of training data is limited. However, as the quantity of training data increases, the performance of both approaches tend to converge.</p>
<p>Therefore, by distilling the essence of a video into a series of key poses, JABS is able to effectively learn and generalize, even with smaller training sets. It has been shown to have a learning capacity on par with deep neural networks, as demonstrated by per-frame accuracy using the same benchmark data-set. Further, achieving 85% accuracy with just 1.4% of the labeled data, suggests that researchers can strike a balance between labeling efforts and desired accuracy by carefully selecting the amount of training data.</p>
</sec>
</sec>
<sec id="s2d">
<label>2.4</label><title>JABS analysis and integration module</title>
<p>In supervised machine learning, the accuracy and reliability of a trained classifier depends heavily on the quality of labeled data. Further, it has been observed that labeling of the same behavior by different human experts introduces variability among annotations due to variety of factors, including personal biases, subjectivity, and individual differences in understanding what constitutes a behavior [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>]. Therefore, it is critical to accurately capture the inter-annotator variability before selecting classifiers for downstream predictions. To capture this variability, we employ both frame based and bout based comparison and demonstrate that bout-based comparison gives a better estimate of inter-annotator agreement.</p>
<sec id="s2d1">
<label>2.4.1</label><title>Frame and bout-wise classifier comparison of inter-annotator variability</title>
<p>In order to test inter-annotator variability, we generated a set of single mouse behavior classifiers for two simple behaviors, left and right turn. We inferred behavior from all four classifies on a large set of videos and compared the two pairs of classifiers from each annotator (<xref rid="fig5" ref-type="fig">Figures 5</xref>, 6). The classifiers for all behaviors achieved good accuracy and F1 scores (<xref rid="tbls5" ref-type="table">Table S5</xref>). Further, the classifiers for the same behavior trained with different human annotations resulted in inter-annotator variability in predictions. This inter-annotator variability can be associated with (a) subjective differences of behavior definition among human labelers (b) varying level of annotator’s expertise, and (c) training with-in and across labs. We investigated the source of this variability and sought to determine the best method to mediate its effects. To capture this effect, we first visualized the predictions made by two classifiers trained for the same behaviors (left and right turn) but with different human annotators: annotator-1 (A1) &amp; annotator-2 (A2). <xref rid="fig5" ref-type="fig">Figure 5B,C</xref> shows two sample ethograms corresponding to the predictions made by A1 &amp; A2 for the left turn behavior. These ethograms show high level of concordance between the two annotators. However, upon closer examination, we observed that the percentage of left or right turn behavior predicted (for all the videos) by A2 was higher than A1 (see <xref rid="fig5" ref-type="fig">Figure 5D,G</xref>). The confusion matrix (shown in <xref rid="fig5" ref-type="fig">Figure 5E,H</xref>) quantifies the level of agreement between predictions made by annotators A1 and A2 for left and right turn behavior. However, since this behavioral task is heavily class-imbalanced (the number of frames with no-behavior is much more than that of behavior), accuracy can be misleading, as the classifier can achieve high accuracy by simply predicting majority class (not behavior) for all the frames. To address this imbalance, we calculate Cohen’s kappa (κ) metric [<xref ref-type="bibr" rid="c44">44</xref>] which is a commonly used measure of inter-annotator agreement accounting for the class imbalance. Mathematically, it is defined as <inline-formula><inline-graphic xlink:href="476229v4_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> where <italic>p<sub>o</sub></italic> is the observed agreement between annotators and <italic>p<sub>e</sub></italic> is the expected agreement due to random chance. A κ score of 0 indicates that the agreement is no better than chance, and a score of 1 indicates perfect agreement, regardless of high/low accuracy. Finally, we visualize the frame-wise comparison of the two annotators showcasing the percentage of frames where the annotators agree and disagree on the occurence of a behavior as shown in <xref rid="fig5" ref-type="fig">Figure 5F,I</xref>. The venn diagram clearly highlights the discrepancy between high accuracy resulting from class imbalance (<xref rid="fig5" ref-type="fig">Figure 5E,H</xref>) and significant mismatch between % of predicted behavior (<xref rid="fig5" ref-type="fig">Figure 5D</xref>, G), with annotator A2 account for majority of discrepancy by predicting more frames as turning behavior compared to annotator A1.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title><bold>Frame based comparison of classifiers from different annotators but trained for the same behavior</bold>.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B, C) Two sample ethograms for the left turn behavior showing variation in behavior inference for two different annotators. (D, G) Kernel density estimate (KDE) of the percentage of frames predicted to be a left turn and a right turn respectively, by each annotator across all the videos. The major discrepancy between the two annotators is that A-2 systematically predicts larger number of frames as behavior compared to A-1. (E, H) Confusion matrix showing the agreement between predictions of two classifiers over all the videos in the strain survey for left and right turn behavior. (F, I) Venn diagram capturing the frame-wise behavior agreement between the two annotators for left and right turn behavior.</p></caption>
<graphic xlink:href="476229v4_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We observed in the ethogram (<xref rid="fig5" ref-type="fig">Figure 5B,C</xref>) that although many of the same bouts are captured by both A1 and A2, most of the frame discrepancies seem to be in the beginnings and ends of the bout. A2 seems to predict longer bouts than A1 (<xref rid="fig5" ref-type="fig">Figure 5D</xref>). Between two humans labeling the same behavior, there are unavoidable and sometimes substantial discrepancies in the exact frames of the behavior labeled even when trained in the same lab [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c43">43</xref>]. To most behaviorists, detecting the same bouts of behavior is more important than the exact starting and ending frame of these bouts– as again, there are human-level discrepancies in this as well. Therefore, we used a bout-based comparison rather than a frame-based comparison to evaluate the performance of the classifiers.</p>
<p>For the bout-based comparison, we looked at how much overlap there was between the bouts of a behavior predicted by annotators A1 and A2, taking inspiration from the machine learning image-recognition and action-detection fields, where an overlap of pixels of the bounding box and ground truth label box called the intersection over union (IoU) [<xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c46">46</xref>]. We developed a graph-based approach called an <italic>ethograph</italic> to represent the bouts of behavior recorded in the ethograms of annotators A1 and A2. Concretely, we define the ethograph for two annotators as a bipartite graph <italic>G</italic> = (<italic>U,V, E</italic>), where <italic>U</italic>, <italic>V</italic> are two disjoint sets corresponding to bouts predicted by each annotator and <italic>E</italic> represents the edges that connect each element in set <italic>U</italic> to an element in set <italic>V</italic> capturing the overlap in time between the bouts. Further, the vertices of an ethograph represents bouts with vertex color encoding for the annotator and vertex size proportional to the duration of the bout. Further, the edges (<italic>E</italic>) of the graph (<italic>G</italic>) represents the temporal overlap between the bouts (corresponding to different annotators) with the thickness of the edge proportional to the amount of bout overlap. <xref rid="fig6" ref-type="fig">Figure 6B,C</xref> shows the ethograms and their associated ethograph for the left turn behavior as predicted by annotators A1 and A2. In contrast to traditional frame-based ethograms, which simply display the sequential list of frames in which a behavior is observed, the ethograph allows for a more intuitive and visual representation of the temporal overlap between the bouts corresponding to different annotators (or even behaviors). This can be especially useful in identifying patterns and trends that may not be immediately apparent from comparing ethograms. By coloring the vertices and edges based on the annotator, it becomes easy to see which behaviors are consistently identified by both the annotators and which are more subjective and open to interpretation. Moreover, we can easily compute the bout-based agreement between the two annotators as the fraction of edges having thickness greater than some fixed threshold (see <xref rid="fig6" ref-type="fig">figure 6F</xref> for mathematical definition) which essentially means the fraction of bouts having overlap greater than a chosen overlap threshold. The bout agreement between two annotators for the left and right turn at a threshold of 0.5 is shown as a Venn diagram in <xref rid="fig6" ref-type="fig">Figure 6D,E</xref> along with the density distribution of bout length. The agreement between two annotators with bout-based measure was certainly much better than that with frame-based comparison (see <xref rid="fig5" ref-type="fig">figure 5F</xref>,I).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Bout based comparison of classifier predictions from different annotators but trained for the same behavior.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B) Ethogram depicting frame-wise left turn predictions for annotators A1 (red) and A2 (blue). (C) Ethograph corresponding to the ethogram in panel (B) capturing the bout level information as a bipartite network. The nodes represent bouts with node size &amp; color proportional to the bout length &amp; annotator respectively. Edge weights captures the fraction of bout overlap between two bouts predicted by different annotators for the same behavior. Edge weight and node size with zero value indicate missed bouts by an annotator. These have been given a small positive value for visualization purposes only. (D-E) Bout length distribution of annotators A1 &amp; A2 for left and right turn behavior. (F) The mathematical definition of the average bout agreement between two annotators, where <italic>w</italic>(<italic>u, v</italic>) represents weight between nodes <italic>u</italic> and <italic>v</italic> (<italic>u</italic> ⊂ <italic>U</italic>, <italic>v</italic> ⊂ <italic>V</italic>) in the ethograph <italic>G</italic> (<italic>U,V, E</italic>) and <italic>w</italic><sup>∗</sup> is the bout overlap threshold (0.5 fixed for our study). (G) overview of the workflow for stitching and filtering at the bout level. (H, I) Hyper-parameters tuning to find optimal filtering and stitching thresholds. (J) Sample ethogram and its corresponding ethograph before and after applying stitching and filtering. (K) Inter-annotator agreement in frame wise predictions underestimates the agreement whereas the bout wise comparison post filtering and stitching captures the overall agreement in a more biologically meaningful way.</p></caption>
<graphic xlink:href="476229v4_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The predictions coming out of a classifier contained many short bouts (1-3 frames) of behavior that signal false positive bouts as they are much shorter than a typical bout of annotated behavior. Moreover, certain bouts of behavior were split by very short bouts (1-3 frames) of not-behavior signalling the presence of false negative bouts that results in fragmentation of a bout of behavior (see <xref rid="fig6" ref-type="fig">figure 6</xref>). To address this issue, we proposed a stitching and filtering step on the predictions coming out of classifier. First, we stitched those bouts whose distance to the neighboring bout is less than certain fixed threshold. This stitched the fragmented bouts as illustrated in <xref rid="fig6" ref-type="fig">Figure 6G</xref>. We then applied bout filtering which removed bouts of a length below a fixed threshold. To decide the optimal values of stitching and bout filtering thresholds, a hyper-parameter scan was performed for each behavior. <xref rid="fig6" ref-type="fig">Figure 6H,I</xref> presents the results from hyper-parameter scan over stitching and bout filtering thresholds when the value of percentage bout overlap is fixed at 25%, 50% and 75% for left (H) and right turn (I). <xref rid="fig6" ref-type="fig">Figure 6J</xref> captures the effect of applying bout filtering and stitching to a portion of an ethogram corresponding to the predictions made by A1 &amp; A2 for the left turn behavior. The effect was clearly discernible when looking at the changes in ethograph, particularly with bouts (nodes) having multiple overlaps (edges) reducing to single overlap (edge) per bout.</p>
<p>In summary, when comparing classifiers, it’s important to consider the inherent variability of human annotators. Frame-wise comparison penalizes this natural variability, making it a sub-optimal measure of agreement. On the other hand, bout-wise comparison takes this variability into account, making it a more biologically meaningful measure of agreement between classifiers. In addition to using bout-wise comparison, applying techniques like stitching and filtering can further improve agreement by reducing false and fragmented bouts in classifier predictions. By considering these factors, we can better understand the inter-annotator variability and design more effective guidelines for behavior annotation.</p>
</sec>
<sec id="s2d2">
<label>2.4.2</label><title>Compilation of Strain Survey Datasets</title>
<p>In the present study, we have curated and are releasing three comprehensive datasets to the public, namely JABS600, JABS1200 and JABS-BxD, that encapsulates behavioral data derived from approximately 168 unique mouse strains, ensuring a balanced representation with a nearly equable distribution between female and male sex. The JABS600 dataset includes a total of 598 videos corresponding to 60 strains, approximately balanced with five males and five females per strain. On the other hand, the JABS1200 dataset contains 1139 videos corresponding to 60 strains, representing approximately nine males and nine females represented per strain. Finally, the JABS-BxD dataset includes a total of 1083 videos corresponding to 108 BxD strains that are derived from a cross between C57BL/6J mice (B6) and DBA/2J mice (D2). The duration of each video is approximately one hour, furnishing a substantial repository of behavioral data, which is invaluable for large-scale automated analysis of behavioral patterns. Furthermore, each video is supplemented with a corresponding keypoint file comprising of 12 keypoints per frame, which is instrumental in extracting specific behavioral features. Additional information about the dataset is given in <xref rid="figs2" ref-type="fig">supplemental figure 2</xref>. In line with our dedication to scientific openness and collaboration, we have made these datasets - encompassing both the video recordings and the keypoint files - available for public access(<ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/">https://dataverse.harvard.edu/</ext-link> dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FSAPNJG), making it easier for fellow researchers across labs to leverage our findings, replicate our experiments, and advance the field of automated behavior quantification.</p>
</sec>
<sec id="s2d3">
<label>2.4.3</label><title>Strain Survey of Multiple Behaviors</title>
<p>One of the advantages of a standardized data acquisition system such as JABS is that data can be repurposed. For instance, a classifier trained by another lab could be inferred on videos generated by another lab. We trained a set of behavior classifiers using JABS active learning system and then inferred them on a previously published strain survey dataset [<xref ref-type="bibr" rid="c8">8</xref>]. The training dataset was composed of multiple human-annotated short videos (around 10 minutes each), we trained classifiers for left turn, right turn, grooming, rearing supported, rearing unsupported, scratch and escape as examples. These can easily be extended to other behaviors. To capture the effect of genotype on the behavior, we subsampled the original strain survey data set to 600 one-hour open field videos representing 60 different strains with 5 female and 5 male for each strain and make predictions using the trained classifiers. Further, we define 3 aggregate phenotype associated with each behavior namely the total duration of the behavior (in minutes) for the first 5, 20 and 55 minutes of the one-hour video [<xref ref-type="bibr" rid="c9">9</xref>], to capture the dynamic changes in behavior over time. The results are shown in <xref rid="fig7" ref-type="fig">Figure 7B</xref>, where the heatmap shows the Z-scores for the total duration of the behavior in 5, 20 and 55 minutes (|Z-score| <italic>&gt;</italic> 1 thresholding is applied for easier visualization). The red and blue colored entries for a particular phenotype represents strains exhibiting the behavior that is more than one standard deviation above and below the mean of the phenotype respectively. Such data can have multiple utility. First, any user of JABS can conduct a rich analysis with little effort to yield biological insight. Such data can be used to refine classifiers by adding edge cases to training data. In addition, downstream genetic analysis suchs as heritability quantification and GWAS analysis are possible with this data [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>]. In our analysis, we observed a high number of escape attempts in C58/J mice. This strain has been shown previously to have high number of repetitive behaviors, perhaps even a strain for the study of autism features [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c48">48</xref>] (<xref rid="fig7" ref-type="fig">Figure 7</xref> Bottom panel). We find that other strains such as I/LnJ, C57/L, and MOLF/EiJ show increased levels of escape behaviors, thus increasing potential strains that could be used to model this behavior.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>JABS-AI (Analysis and Integration) module: Strain-level behavioral phenotyping across genetically diverse mouse populations.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B) Heatmap showing Z-transformed behavioral scores for aggregate phenotypes measured at three time points (5, 20, and 55 minutes) across the JABS600 strain survey. Each column represents a genetically distinct mouse strain, and each row corresponds to a specific behavioral measure including locomotion (turning left/right), exploration (rearing), self-directed behaviors (grooming, scratching), and escape responses. Color intensity indicates deviation from the population mean, with red representing increased behavioral expression and blue representing decreased expression relative to the strain average. Z-score thresholding (|Z-score| <italic>&gt;</italic> 1) was applied to all behavioral measures, with escape behaviors displayed separately using modified thresholding parameters to preserve detection of outlier strains exhibiting rare but phenotypically important escape responses. Behavioral measures are stratified by time point (T5, T20, T55) to capture temporal dynamics of phenotypic variation across genetic backgrounds.</p></caption>
<graphic xlink:href="476229v4_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>In addition to phenotypic diversity due to genotype, we explored sexual dimorphism in our dataset with these new classifiers. We examined the impact of sex on the aggregated phenotype in various strains using a univariate approach. To test for the statistical significance of the effect of sex, we utilized a nonparametric rank test and correcting for multiple testing using false discovery rate (fdr) using Benajamini-Hochberg method. The LOD scores and effect sizes are presented in <xref rid="figs7" ref-type="fig">Figure S7B</xref>, with the left panel showing the strength of evidence against the null hypothesis of the non-sex effect. The right panel presents a representation of the direction and magnitude of the effect size with the color and size of circle represents the direction and magnitude of the effect, respectively. The strains highlighted in pink exhibit a significant sex effect for at least one of the aggregated phenotypes. It is important to note that we are generally underpowered with five animals of each sex. However, we find that a high proportion of phenotypes show a sex effect.</p>
</sec>
</sec>
<sec id="s2e">
<label>2.5</label><title>JABS-AI (Analysis and Integration) module: Heritability, genetic correlation, and GWAS analysis</title>
<p>Next, one of our goals is to understand the genetic architecture that governs the complex behavioral phenotypes. The quantitative-genetic analysis described in this section provides the backbone for the JABS-AI web platform that follows in <xref rid="s2f" ref-type="sec">section 2.6</xref>. The web interface lets any user compute behavioral predictions and downstream genetic analysis for newly uploaded classifiers.</p>
<p>To facilitate this, we utilized the data derived from 49 inbred strains along with 11 F1 hybrid strains, to perform a genome-wide association study (GWAS). It was deemed necessary to exclude the six wild derived strains due to their pronounced divergence, which carried the risk of distorting the outcomes of our mouse GWAS. We first carry out power analysis for both the strain survey datasets (JABS600, JABS1200) using simulation algorithm as proposed by Genome-wide Efficient Mixed Model Association (GEMMA) software. GEMMA, a useful tool for this type of analysis, accounts for population structure and genetic relatedness between individuals, making it ideal for our inbred and hybrid strains. The power analysis as shown in <xref rid="fig8" ref-type="fig">Fig. 8A</xref> revealed that we had sufficient statistical power to detect genetic associations. Notably, the JASB1200 dataset demonstrated higher power in detecting these associations compared to the JABS600 dataset. With JABS1200 established as our dataset of choice for conducting the GWAS, we moved forward with assessing each of the 72 phenotypes for their potential association with genotype. We employed the GEMMA software for this purpose, giving particular emphasis to the Wald test p-value in our analysis. These 72 phenotypes have been derived from eight basic classifiers, which include turn left and turn right (each assessed by two different annotators), grooming, scratching, supported rearing, and unsupported rearing. Each of these classifiers has been further categorized into three bout-based measures: average bout length, total duration, and total number of bouts. These bout-based measures were then dissected into three time-based measures (5 minutes, 20 minutes, and 55 minutes) to provide a comprehensive analysis. We tested a substantial number of SNPs (211,077) which necessitated accounting for the inherent correlations among SNP genotypes. To establish an empirical threshold for the p-values, we shuffled the values of one normally distributed phenotype (<italic>TL</italic>_<italic>T</italic> 20_<italic>duration</italic>) randomly and identified the smallest p-value from each permutation. This rigorous process allowed us to set a p-value threshold of 1.9e-05 that reflects a corrected p-value of 0.05. We first report the heritability estimates for phenotypes corresponding to 55 minutes of observed behavior as shown in <xref rid="fig8" ref-type="fig">Fig. 8B</xref>. Most of the phenotypes have heritability in the range (0.2-0.8) with bout length based phenotypes having lower heritability relative to bout number or bout duration based phenotypes. Next, to further shed light on the pleiotropic action of genes, we estimate the genetic correlations across these phenotypes using the bivariate linear mixed model implemented in GEMMA. We plot the genomic restricted maximum likelihood (GREML) estimates of bivariate genetic correlations in <xref rid="fig8" ref-type="fig">Fig. 8C</xref>. The magnitude of the genetic correlation estimate provides an estimate of genetic overlap (common genetic loci) between two traits, whereas the sign determines the direction of the effects of the overlap on the two traits, i.e., a negative sign corresponds to the effect in the opposite direction on the two traits and vice versa. We hypothesize that for a given behavior, the bout-based measures within the behavior share common genetic effects and affect the traits in the same direction. Indeed, we find estimates of genetic correlations that are positive between the number of bouts (nBouts) and duration of the behavior (duration), the average length of each bout (avgLen) and duration of the behavior (duration), and the number of bouts (nBouts) and duration of the behavior (duration) for all behaviors except the turn right behavior (A1_TR) by annotator 1. We find positive estimates of genetic correlations between two annotators (A1, A2) for the turn left or right behaviors since we expect the genetic architecture underlying the same behavior from two annotators to overlap maximally.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>JABS-AI (Analysis and Integration) module: Large-scale GWAS investigation of different mouse behaviors utilizing the JABS1200 dataset</title>
<p>: (A) Statistical power comparison between two datasets (JABS600 vs JABS1200) at the genome-wide significance threshold of 2.4e-07. The y axis shows how power varies with SNP effect size (x axis) (B) Aggregate (55 min) phenotypes’ heritability (PVE) estimates. (C) Lower Triangular Matrix Representation of Genotypic Correlation Among all of the 55 minute aggregate phenotypes using a bi-variate linear mixed model, (D) Linkage disequilibrium (LD) blocks size, along with the mean genotype correlations for SNPs at varying genomic distances. (E) Aggregated GWAS results graphically represented via a comprehensive Manhattan plot. Peak SNP clusters, extracted from (F), determine color differentiation; SNPs within the same LD block are color-coordinated to match their peak SNP. Each SNP is assigned the minimum p-value derived from all phenotypes. (F) An inclusive heatmap exhibiting all the significant peak SNPs for each phenotype. Each row, representing an SNP, is color-coordinated according to the allocated cluster within the k-means clustering. The color scheme originating from the k-means cluster is also applied in panel E of this analysis.</p></caption>
<graphic xlink:href="476229v4_fig8.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>We adopted a specific approach to identify quantitative trait loci (QTL): we started with the SNP that exhibited the lowest p-value across the genome and designated it as a locus. We then grouped together adjacent SNPs showing a significant level of correlation in their genotypes (<italic>r</italic><sup>2</sup> ≥ 0.2), employing a greedy strategy. We continued this process, moving on to the next SNP with the lowest p-value until we allocated all significant SNPs to a QTL. Given the inherent genetic structure of inbred mouse strains, large linkage disequilibrium (LD) blocks are expected, as represented in <xref rid="fig8" ref-type="fig">Fig. 8D</xref>.</p>
<p>Additionally, we observe pleiotropy with certain loci displaying significant associations with multiple phenotypes, an anticipated occurrence given the correlation among many of our phenotypes and the potential for individual traits to be influenced by similar genetic loci. To get a clearer picture of the pleiotropic structure apparent in our GWAS findings, we constructed a heatmap (<xref rid="fig8" ref-type="fig">fig. 8F</xref>) of significant QTL across all phenotypes and employed <italic>kmeans</italic> clustering to identify QTL sets governing phenotype groups. The phenotypes are grouped into 6 categories namely: grooming bout length, grooming bout number and total time, rearing supported, rearing unsupported, turn bout length, turn bout number and total time. We uncovered seven unique clusters of QTLs (A-G), each regulating a different combination of these phenotype subgroups (<xref rid="fig8" ref-type="fig">fig. 8F</xref>). Clusters B and G notably held pleiotropic QTLs that influenced overall turn and rearing behaviors, respectively. Yet within cluster F, we identified distinct QTL sets - one that steered grooming behavior, and another, non-overlapping set that determined turn length. This distinction signifies the existence of distinct genetic underpinnings for these different behaviors even within the same cluster. Finally, we color the associated SNPs in the Manhattan plot (<xref rid="fig8" ref-type="fig">fig. 8E</xref>) showing QTLs associated with all phenotypes.</p>
</sec>
<sec id="s2f" sec-type="data-availability">
<label>2.6</label><title>Data Integration: A web application for classifier sharing and downstream genetic analysis</title>
<p>In conjunction with the release of the curated datasets and the JABS active learning GUI app (JABS-AL), we have developed and launched a web-based application, JABS-AI (analysis and integration), aimed at streamlining the sharing and utilization of classifiers. Through this platform, users can view, download and rate the classifiers for various behaviors that have been developed and trained in our laboratory as shown in <xref rid="fig9" ref-type="fig">Figure 9B</xref>. In addition, it provides an insight into their heritability scores and offers a feature to examine the pair-wise genetic correlations amongst different phenotypes. An added functionality of this web application is that it allows users to upload their own classifiers (trained using JABS-AL GUI app) for any specific behavior. Upon uploading, the application automatically executes the classifier on a dataset of the user’s choosing from our strain survey datasets. It conducts an automated analysis of behavior and genetics, and subsequently dispatches the results to the user’s designated email address within a few hours (see <xref rid="fig9" ref-type="fig">figure 9A</xref>). The results coming out of webapp would contain following downstream analysis on the dataset selected by the user in the app:
<list list-type="order">
<list-item><p>Density plots of predicted behavior (similar to <xref rid="fig5" ref-type="fig">Figure 5D,G</xref>).</p></list-item>
<list-item><p>Strain-behavioral phenotype heatmap (similar to <xref rid="fig7" ref-type="fig">Figure 7</xref>).</p></list-item>
<list-item><p>Heritability and genetic correlation of behavioral phenotypes (similar to <xref rid="fig8" ref-type="fig">Figure 8B,C</xref>).</p></list-item>
</list>
</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>JABS-AI (Analysis and Integration): A web application for sharing the JABS classifiers and automated downstream genetic analysis.</title>
<p> (A) Illustrates the fundamental workflow of the web application, beginning with the user employing a classifier trained via the JABS active learning application.The user subsequently deposits this classifier into our web application, which performs comprehensive automated analyses, encompassing both behavioral and genetic aspects, on dataset selected by the user from our curated strain survey collection (JABS600, JABS1200) accessible via a dropdown menu. The outcome of these analyses, encapsulating detailed behavioral patterns and genetic correlations, are then dispatched to the user’s designated email address within a short timeframe. (B) Screenshot of the webapp highlighting the tabular presentation of the repository of classifiers developed in our laboratory, complete with pertinent metadata such as the date of creation, training hyperparameters, and user ratings. When any two classifiers are selected, the application offers the option to analyze the genetic correlations between the phenotypes corresponding to the selected classifiers, in conjunction with their heritability scores.</p></caption>
<graphic xlink:href="476229v4_fig9.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>This web application serves as a facilitative tool aimed at fostering collaboration among researchers and streamlining the advancement of automated behavior quantification studies by providing a platform for the efficient sharing and analysis of behavioral classifiers.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label><title>Discussion</title>
<p>Democratization of machine vision methods for advanced behavior quantification remains a challenge. Often, tracking and behavior classifiers are not transferable between laboratories. This limits the reuse of prior work with each laboratory essentially starting from scratch with advanced behavior quantification. JABS and the companion DIV Sys are designed to overcome these limitations. JABS components include video data acquisition, behavior annotation, classifier sharing, and genetic analysis. By adopting the JABS-DA laboratories can use our pose estimation and segmentation models that work across 62 mouse strains of varying coat colors and sizes. This greatly eases the barrier to entry for advanced behavior quantification. The next steps of creating behavior classifiers are carried out using JABS-AL, an active learning system modeled after JAABA [<xref ref-type="bibr" rid="c21">21</xref>]. We benchmarked JABS-AL using grooming data set and demonstrate that it reaches very good performance with 10% of the data needed for a 3D-CNN for action detection. Once constructed, behavior classifiers can be shared through JABS-AI, a cloud based tool. Labs can create their own behavior classifiers using JABS-AL or download an existing one for use from another lab from JABS-AI in order to annotate single behaviors. The power of JABS-AI is also in the embedded strain survey data. A deposited classifier is inferred on one of three datasets and heritability and genetic correlation results are returned.</p>
<p>A key decision point is the adoption of common apparatus to create a uniform visual look of the video data across laboratories. This enables cross application of foundational models and exchange of behavior classifiers across labs. We realize that this may be challenging for some labs with limited space and budget. Indeed, JABS has a large footprint with a 2×2×6 feet (W x L x H) space requirement, costs several thousand dollars for components, and requires some computational expertise to set up and operate. Laboratories must balance this cost with the labor and time costs of adopting an existing set-up for advanced behavior analysis. In lieu of adopting a common apparatus, efforts are being made to build foundational models that can handle diverse environments and even animals [<xref ref-type="bibr" rid="c49">49</xref>]. While similar datasets are common in human pose estimation, the development of equivalent datasets for animals is still underway. However, such foundational models are not yet available. Even when they are available, the initial abstraction step is simple compared to the later step of behavior classification. For instance, in our gait and posture paper, producing a pose estimation model that generalizes to diverse mouse strains took approximately 6 months. It was an iterative hard example mining task. However, the process of deriving gait and posture from keypoints and its genetic validation took over 1.5 years. By simply adopting JABS, laboratories gain access to both the pose model and validated gait/posture algorithms.</p>
<p>While our pose estimation model was not specifically trained on tethered animals, published research demonstrates that keypoint detection models maintain robust performance despite the presence of headstages and recording equipment. Once accurate pose coordinates are extracted, the downstream behavior classification pipeline operates independently of the pose estimation method and would remain fully functional. We recommend users validate pose estimation accuracy in their specific experimental setup, as the behavior classification component itself is agnostic to the source of pose coordinates.</p>
<p>Another added benefit to JABS is the ability to apply novel behavior classifiers to large-scale genetically diverse datasets collected at JAX through JABS-AI. We have modeled this after existing platforms such as GeneNetwork and DO QTL. Currently, we provide heritability estimates of any classifier that is deposited. Users can also select behaviors to genetic correlation studies. Thus, even if two behaviors are different, they may measure the same genetic architecture. Although the current version of JABS-AI does not offer GWAS analysis due to compute restrictions, the method can be easily extended for such analysis. It is also feasible to link animal behaviors to human traits through PheWAS analysis [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. This would provide even more detailed information for users about the genetic regulators of complex behaviors. The current data sets, JABS600, JABS1200, and JABS-BxD consist young wild type animals. We have collected datasets in aging populations with various frailty statuses and animals that display nocifensive behaviors. These could also be integrated into JABS-AI for preclinical behaviors independent of genetic analysis.</p>
<p>While JABS is designed for individual behavior annotation, a common task in behavioral neurogenetics to determine an internal state, e.g. anxiety or social state. Often these are accomplished by measuring a single behavior. A more powerful approach is the application of behavior indices to predict certain states. These indices can be constructed using multiple behaviors and even other covariates. For example, we trained a model to predict frailty using data from over 600 JABS-DA open field tests from C57BL/6J mice of varying age and frailty. We used 34 features from JABS to derive frailty. Similarly, in companion work, we derived pain scale with 82 features from JABS. These indices were constructed with almost 1000 animals and can readily be transferred to other labs that collect data using the JABS system. This is incredibly powerful and allows labs to leverage each other’s models by using a common platform. Similarly, for pain states, we have tested multiple strains and built a pain intensity models that can be utilized. We believe a true advantage of advanced phenotyping using video data is the ability to reuse and extract more information from existing data. This essentially allows us to use less animals, a core 3R principle.</p>
<p>Here, we describe JABS as a single animal open-field assay that lasts from minutes to a few hours. However, we have designed the JABS arena for long-term housing of animals with a food hopper and lixit. This was the primary reason we worked with JAX-IACUC to certify JABS-DA for long-term monitoring with key required environment measures. By blocking visible light and imaging using IR LED illumination, we obtain uniform data at night or day. We routinely collect video data with three mice over several days. The models for tracking, instancing, and identity maintenance need to evolve. We plan to extend JAB-AI with classifiers for social interactions and homeostatic behaviors. Thus, future iterations of JABS will develop and share multi-animal behavior analysis.</p>
<p>Even when data acquisition is standardized, another fundamental source of variability can enter the system when different human experts within or across different labs, annotate the same videos for the same behavior. This type of variability can arise due to variety of factors, including differences in training, personal biases, and individual interpretation of behavior. As behaviors become more complex, we expect behaviorists to show more disagreement. These disagreements could be as simple as varying understanding of the starts and stops of the behaviors. Or more fundamentally differing opinion on the behavior such as between aggression and play. In a previous study, we asked 5 humans to annotate grooming behavior and found that the agreement ranged from 86% to 91%. In this case we simply compared on a frame wise basis labels from annotators who were asked to label every frame in the same set of videos. In most cases, such comparisons are infeasible. A more realistic comparison is provided in this manuscript. Two annotators built their own classifiers for left and right turn behaviors. Then we compared the predictions from each set of classifiers on JABS-DA video. This is akin to two different laboratories that may deposit classifiers for the same behavior. JABS-AI does not save primary the training data from each annotator (lab), and simply uses the trained classifiers to infer on a new set of videos (e.g. JABS600). From these we can compare the overlap between the two classifiers’ inferences. We do not make assumptions about which classifier is the ground truth and simply compare both classifiers.</p>
<p>In <xref rid="s2d1" ref-type="sec">section 2.4.1</xref>, we demonstrate that even for simple behaviors like left and right turn, there is a significant amount of disagreement between predictions coming from classifiers trained by two expert annotators within the lab for the same behavior. One of the most commonly used statistical measures to quantify the inter-annotator variability is Cohen’s kappa which assesses the level of agreement between the annotators taking into account the possibility of agreement by chance. The Cohen’s kappa statistic works well for frame-wise comparison but is ill-defined for bout-wise comparison as unlike frames, bouts are not conserved. In order to overcome this limitation, we have introduced a new approach based on graph theory, called the <italic>ethograph</italic>. This network approach allows us to define measures that quantify the agreement between two annotators when comparing bouts of behavior among different annotators. By comparing the entire sequences of frames, the ethograph reduces subjectivity and allows for a more holistic and consistent interpretation of behaviors. This makes it well-suited for bout-wise comparison, and may provide a more accurate estimate of inter-annotator agreement than the frame-based kappa statistic.</p>
<p>Even though frame wise comparison shows the overlap performance is poor (κ = 0.64 and 0.65 for Left and Right Turn, respectively), each classifier does a good job of identifying tuning behaviors. The turn behaviors are short in length of frames, and the two annotators differ in defining the starts and stops of the behaviors. One annotator labels just the core turn behavior, and the other starts labeling turning behavior a few frames earlier and ends later. Classifiers from both annotators generally find the same bouts of turning which we could visualize in the ethograph. We explored bout-wise accuracy metrics as an alternative to frame-wise metrics. We also explore post-processing predictions using hyperparameters for filter and stitch. By adding these, we observe much higher agreement between the classifiers from two annotators for the same behaviors (overlap increases from 49% to 61%). It is important that users clearly define the behavior as best as possible, and document the filter and stitch parameters.</p>
<p>JABS users, when confronted with multiple classifiers for the same behavior in JABS-AI, must prioritize use of one classifier. JABS-AI offers a genetic solution to this challenge - by prioritizing the classfier that is more heritable. Heritability is an estimate of variance explained by genetics and can act as a discriminator in this situation. We also calculate genetic correlation with allows users to determine which underlying genetic construct is being measured. For instance, both left and right turn are highly genetically correlated and, therefore for the purposes of genetics, there is simply turn behavior. However, for certain unilateral models such as brain lesions, stroke, optogenetic stimulation, or injury, the ability to distinguish left and right turns can be critical.</p>
<sec id="s3a">
<label>3.1</label><title>Supervised vs unsupervised behavior segmentation</title>
<p>Unsupervised methods like Keypoint-MoSeq [<xref ref-type="bibr" rid="c51">51</xref>], VAME [<xref ref-type="bibr" rid="c52">52</xref>], B-SOiD [<xref ref-type="bibr" rid="c22">22</xref>], and MotionMapper [<xref ref-type="bibr" rid="c53">53</xref>] which prioritize motif discovery from unlabeled data but may yield less precise alignments with expert annotations, as evidenced by lower F1 scores in comparative evaluations [<xref ref-type="bibr" rid="c54">54</xref>]. Supervised approaches (like ours), by contrast, employs fully supervised classifiers to deliver frame-accurate, behavior-specific scores that align directly with experimental hypotheses. Ultimately, a pragmatic hybrid strategy, starting with unsupervised pilots to identify motifs and transitioning to supervised fine-tuning with minimal labels, can minimize annotation burdens and enhance both discovery and precision in ethological studies.</p>
</sec>
<sec id="s3b">
<label>3.2</label><title>Future directions and challenges</title>
<p>We see several areas of improvement in JABS in the future. First, the success of such a platform depends on community adoption. As such, JAX has made JABS free for noncommercial use, and we have listed all parts and software used to make JABS. We realize that many laboratories may not have the computational or fabrication resources to construct JABS and that commercial suppliers who can provide a turnkey system are needed for JABS-DA. JABS-AL and JABS-AI require fewer, though still significant, resources to support.</p>
<p>JABS-AI currently does not support upload of training videos due to resource limitations. This prevents other users from interrogating the primary training labels. It also prevents users from down-loading and labeling new behaviors or modifying classifiers that have been uploaded. Future versions could support sharing of complete training data instead of the classifier only.</p>
<p>Furthermore, since the classifiers are trained on few densely labeled short video recordings and then further make predictions on a large strain survey consisting of multiple strains of mice, there is some variability in predictions purely due to out-of-distribution strains in the strain survey. Therefore, the inter-annotator variability in predictions on the new set of strains of mice can be attributed to both the variability in the human labeling and genetic variability in the strain survey. Calculating the heritability scores might help in this scenario by providing us a quantitative measure of the extent to which the inter-annotator variability is due to genetic factors versus interpretation by the human labelers.</p>
<sec id="s3b1">
<label>3.2.1</label><title>Rodent Homes and Hotels</title>
<p>Finally, JABS and DIV Sys are complementary systems that enable behavioral monitoring across multiple scales and resolutions. DIV Sys facilitates long-term observation of home-cage behaviors, whereas JABS offers high-resolution tracking of gait, posture, and other discrete actions [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>]. The larger space in JABS can potentially accommodate additional tasks designed to probe specific neural circuits [<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>], and neural recordings can be collected from instrumented mice in this same arena. We see these as &quot;ethological tasks&quot; that can be performed continuously over long-periods of time in order to interrogate neural and genetic circuits in customizable environments – &quot;hotels&quot;. Examples include mazes and other tasks that neurobehavior researchers have been developing. These assays can be validated using genetic or pharmacological models on a shared platform such as JABS. These two platforms provide a dual approach: continuous surveillance of mice in their home-cage environments (via DIV Sys) alongside targeted assessments of particular behaviors in a dedicated hotel arena (via JABS). This combined paradigm presents a powerful framework to link genetic and neural changes to complex behaviors. Indeed, elucidating how altered behaviors result from altered neural circuits and altered genetic pathways remains a central challenge in computational ethology one that platforms such as JABS and DIV Sys are poised to address.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<label>4</label><title>Materials and methods</title>
<sec id="s4a">
<label>4.1</label><title>Animals</title>
<p>All animals were obtained from The Jackson Laboratory production colonies or bred in a room adjacent to the testing room as previously described [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c24">24</xref>]. All behavioral tests were performed in accordance with approved protocols from The Jackson Laboratory Institutional Animal Care and Use Committee guidelines.</p>
</sec>
<sec id="s4b">
<label>4.2</label><title>Datasets</title>
<p>To facilitate reproducibility and community-driven discovery, we have made three comprehensive datasets publicly available. Each dataset contains multiple open-field arena (OFA) videos (one hour recording per video) and corresponding pose-estimation keypoint files.</p>
<sec id="s4b1">
<title>JABS600</title>
<p>This dataset comprises approximately 600 videos from 62 genetically diverse mouse strains, with sexes balanced within each strain. It serves as a broad survey for initial explorations of behavioral phenotypes across a wide genetic landscape. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10">https://doi.org/10</ext-link>. 7910/DVN/SAPNJG</p>
</sec>
<sec id="s4b2">
<title>JABS1200</title>
<p>An extension of the first dataset, JABS1200 contains nearly 1,200 videos from the same 62 strains, effectively doubling the sample size per strain. This increased depth provides greater statistical power for detecting significant associations in Genome-Wide Association Studies (GWAS). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7910/DVN/SAPNJG">https://doi.org/10.7910/DVN/SAPNJG</ext-link></p>
</sec>
<sec id="s4b3">
<title>JABS-BxD</title>
<p>This collection includes over 1,000 videos from 108 BxD recombinant inbred mouse strains, which are derived from a cross between the C57BL/6J (B6) and DBA/2J (D2) parental strains. With approximately five males and five females per strain, this dataset is structured to support high-resolution genetic mapping of behavioral traits. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7910/DVN/RQYI04">https://doi.org/10.7910/DVN/RQYI04</ext-link></p>
</sec>
<sec id="s4b4">
<title>JABS behavioral classifier example projects</title>
<p>Complete training resources for behavioral classification, comprising experimental video data, extracted pose coordinates, and corresponding behavioral annotations, are made publicly available. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.16697332">https://doi.org/10.5281/zenodo.16697332</ext-link></p>
</sec>
</sec>
<sec id="s4c">
<label>4.3</label><title>JABS workflow guide</title>
<p>The JAX Animal Behavior System (JABS) provides an integrated, four-stage pipeline guiding users from standardized data acquisition to novel genetic discovery. This workflow utilizes three core software modules (JABS-DA, JABS-AL, JABS-AI) and a standalone pose estimation engine, as detailed in the summary below.</p>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="476229v4_utbl1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s4d">
<label>4.4</label><title>Pose tracking pipeline</title>
<p>We have previously published our single mouse pose model here [<xref ref-type="bibr" rid="c10">10</xref>], with the training data and trained models available at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/6380163">https://zenodo.org/records/6380163</ext-link>. We have also released our pose tracking pipeline, which includes single and multimouse tracking and classifier prediction at <ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/mouse-tracking-runtime">https://github.com/KumarLabJax/mouse-tracking-runtime</ext-link>. In addition to our inhouse tracking pipeline, we have made our pose format accessible via SLEAP-io conversion hooks (<ext-link ext-link-type="uri" xlink:href="https://github.com/talmolab/sleap-io">https://github.com/talmolab/sleap-io</ext-link>).</p>
</sec>
<sec id="s4e">
<label>4.5</label><title>Grooming benchmarking study</title>
<p>The Convolutional neural network (CNN) applied to the grooming benchmark dataset follows a typical feature encoder structure except using 3D convolution and pooling layers instead of 2D. The final layer was used as the output probabilities for not grooming and grooming predictions for each frame. The exact architecture and the training details are described in detail here [<xref ref-type="bibr" rid="c9">9</xref>]. Furthermore, the JABS grooming classifier has been trained with both XGBoost [<xref ref-type="bibr" rid="c37">37</xref>] and Random Forest model [<xref ref-type="bibr" rid="c38">38</xref>] with their default hyper-parameters in the XGBoost and scikit-learn library respectively.</p>
<p>To assess classifier performance on frame-by-frame labeling of animal behavior, we report five standard metrics:</p>
<sec id="s4e1">
<title>Accuracy</title>
<p>Percentage of all video frames correctly labeled as either target behavior or not.
<disp-formula>
<graphic xlink:href="476229v4_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4e2">
<title>F1 score</title>
<p>Harmonic mean of precision (fraction of predicted target frames that are correct) and recall (fraction of actual target frames detected).
<disp-formula>
<graphic xlink:href="476229v4_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4e3">
<title>AUROC</title>
<p>Area under the receiver-operating-characteristic curve; measures how well the classifier separates frames containing the target behavior from all others, independent of the decision threshold (k).
<disp-formula>
<graphic xlink:href="476229v4_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4e4">
<title>True Positive Rate (TPR)</title>
<p>Proportion of frames with the target behavior that the classifier correctly labels.
<disp-formula>
<graphic xlink:href="476229v4_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4e5">
<title>False Positive Rate (FPR)</title>
<p>Proportion of frames without the target behavior that are incorrectly labeled as target by the classifier.
<disp-formula>
<graphic xlink:href="476229v4_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4e6">
<title>TPR@5 % FPR</title>
<p>In practical applications, it is important to capture as many true instances of the behavior as possible while avoiding excessive false positives, which reduce trust in the predictions and increase validation effort. Therefore, we also report the true positive rate (TPR) achieved when the classifier is tuned to allow at most 5% false positive rate (FPR).
<disp-formula>
<graphic xlink:href="476229v4_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
</sec>
<sec id="s4f">
<label>4.6</label><title>Downstream analysis on strain survey</title>
<sec id="s4f1">
<label>4.6.1</label><title>Aggregate phenotypes</title>
<p>For each classifier, we construct nine aggregate phenotypes corresponding to the three metrics (total duration, number of bouts and average bout length) and three time bins (first 5, 20 and 55 minutes). For instance, a phenotype named “turn_left_T55&quot; (in <xref rid="fig7" ref-type="fig">Figure 7</xref>) represents the total duration of left turn behavior in the first 55 minutes of the video averaged across all videos. For more details, refer to <xref rid="tbls2" ref-type="table">supplementary tables S2</xref> and <xref rid="tbls3" ref-type="table">S3</xref>.</p>
</sec>
<sec id="s4f2">
<label>4.6.2</label><title>Z-score normalization of behavioral phenotypes</title>
<p>To facilitate comparison across strains and behaviors, each phenotype was standardized using z-score normalization. This process transforms the raw phenotype values into units of standard deviation relative to the mean across all strains. Specifically, for each phenotype <italic>x</italic>, the z-score <italic>z</italic> is calculated as
<disp-formula>
<graphic xlink:href="476229v4_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where µ is the mean and σ is the standard deviation of the phenotype values across all strains. This normalization centers the data around zero and scales it so that the spread reflects variability within the dataset. In our heatmap (<xref rid="fig7" ref-type="fig">Figure 7</xref>), colors correspond to z-scores, emphasizing how much a strains behavior deviates above (red) or below (blue) average. To improve clarity, we highlight entries with |<italic>z</italic>| <italic>&gt;</italic> 1, indicating behaviors differing from the mean by more than one standard deviation.</p>
</sec>
<sec id="s4f3">
<label>4.6.3</label><title>Genetic analysis</title>
<p>The aggregate behavioral phenotypes were analyzed to study the genetic associations of strain-specific behaviors. Genotype data for the different mouse strains were obtained from the Mouse Phenome Database (<ext-link ext-link-type="uri" xlink:href="https://phenome.jax.org/genotypes">https://phenome.jax.org/genotypes</ext-link>). We used genotypes derived from the Mouse Diversity Array (MDA), where di-allelic genotypes were inferred from parental genomes. Quality control filters retained SNPs with a minor allele frequency (MAF) of at least 10% and a maximum of 5% missing data.</p>
<p>Genome-wide association studies (GWAS) were conducted using the R package mousegwas, as previously described in [<xref ref-type="bibr" rid="c9">9</xref>]. Classical laboratory mouse strains were included in the analysis, excluding wild-derived strains. Associations were computed using the linear mixed model (LMM) method implemented in GEMMA. To reduce confounding from linkage disequilibrium near tested markers, a Leave One Chromosome Out (LOCO) approach was applied: for each chromosome under test, the kinship matrix was calculated using SNPs from all other chromosomes.</p>
<p>To control for multiple testing and establish an appropriate genome-wide significance threshold, we performed permutation-based empirical calibration. Specifically, phenotype values from a randomly sampled continuous trait were shuffled across individuals multiple times, and the minimum p-value from each permutation was recorded. This approach yielded a p-value threshold of approximately 1.9<italic>e</italic> − 05, corresponding to a family-wise error rate of 0.05 after correcting for the number of tests performed.</p>
<p>SNP-based heritability for each behavioral phenotype was also estimated using mousegwas. For each phenotype, a genetic relatedness matrix (GRM) was constructed from quality-controlled SNPs (filtered for MAF and missingness). The analysis was performed on the JABS1200 dataset, comprising 1,139 individuals and 211,070 SNPs. Fixed-effect covariates included sex, weight, and coat color of the animals.</p>
<p>The GWAS execution was wrapped in an R package called mouseGWAS available on github: <ext-link ext-link-type="uri" xlink:href="https://github.com/TheJacksonLaboratory/mousegwas">https://github.com/TheJacksonLaboratory/mousegwas</ext-link>.</p>
<p>For genetic correlation analyses, we applied GEMMA’s bivariate LMM (using this pipeline: <ext-link ext-link-type="uri" xlink:href="https://github.com/gautam-sabnis/genetic_correlation">https://github.com/gautam-sabnis/genetic_correlation</ext-link>) to each pair of phenotypes.</p>
</sec>
<sec id="s4f4">
<title>Power analysis simulation for GWAS</title>
<p>To assess the statistical power of our GWAS design, we followed the simulation-based approach described in the original GEMMA paper [<xref ref-type="bibr" rid="c57">57</xref>]. Briefly, we first filtered SNPs from the GEMMA output (for a randomly selected behavioral phenotype) to retain only those with nominal association (<italic>p &lt;</italic> 0.05), sorted them by genomic position, and then selected a fixed number of evenly spaced SNPs as &quot;causal.&quot; For each causal SNP, we assigned an effect size r<underline>equired to explai</underline>n a specified proportion of phenotypic variance (PVE), calculated as <inline-formula><inline-graphic xlink:href="476229v4_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where AF is the SNP allele frequency. We simulated new phenotypes by adding the effect of each causal SNP to the original phenotype, re-ran GWAS using GEMMA, and defined power as the proportion of simulated causal SNPs detected above the genome-wide significance threshold (e.g., Bonferroni-corrected <italic>p &lt;</italic> 0.05).</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<label>5</label><title>Acknowledgements</title>
<p>We thank members of the Kumar Lab for helpful advice and Leinani Hession for training behavior classifiers. Michelle Foskett (Process Quality Control) and Rosalinda Doty (Diagnostic and Pathology Services) help with environment and pathology data. This work was funded by The Jackson Laboratory Directors Innovation Fund, National Institute of Health DA041668 (NIDA), DA048634 (NIDA), MH138309 (NIMH), and AG078530 (NIA). All code and training data will be available at <ext-link ext-link-type="uri" xlink:href="https://Kumarlab.org">Kumarlab.org</ext-link> and Kumar Lab Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax">https://github.com/KumarLabJax</ext-link>).</p>
</ack>
<app-group>
<app id="s5">
<label>6</label><title>Supplementary material</title>
<sec id="s5a">
<label>6.1</label><title>Grooming benchmark dataset</title>
<table-wrap id="tbls1" orientation="portrait" position="anchor">
<label>Table S1.</label>
<caption><title>Data used for grooming benchmark.</title>
<p>Number of videos (first column), and number of annotated frames (second and third column).</p></caption>
<graphic xlink:href="476229v4_tbls1.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<table-wrap id="tbls2" orientation="portrait" position="anchor">
<label>Table S2.</label>
<caption><title>Summary of framewise behavioral phenotypes and their definitions.</title>
<p>Each value corresponds to the total duration (in seconds) of the indicated behavior during the specified time window, averaged across all analyzed videos.</p></caption>
<graphic xlink:href="476229v4_tbls2.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
</sec>
<sec id="s5b">
<label>6.2</label><title>Quantifying strain survey dataset imbalance</title>
<p>Strain Imbalance (SI):
<disp-formula id="eqn1">
<graphic xlink:href="476229v4_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Gender Imbalance (GI) for each strain <italic>i</italic>:
<disp-formula id="eqn2">
<graphic xlink:href="476229v4_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula></p>
<fig id="figs1" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S1.</label>
<caption><title>JABS data acquisition module: Environmental parameters in the arena.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B) Carbon dioxide concentrations and (C) ammonia concentrations were both much higher in the standard wean cage than in the JABS arena. Carbon dioxide was also compared to room background levels. (D) Temperature and (E) humidity measured at floor level in JABS arenas and a standard wean cage compared to room background across a 14 day period. (F) Average body weight as percent of start weight in each JABS arena and wean cage across the 14 day period. (G) Food and (H) water consumption shown as grams per mouse per day for one JABS arena and one wean cage for a 14 day period.</p></caption>
<graphic xlink:href="476229v4_figs1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs2" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S2.</label>
<caption><title>Representative hematoxylin and eosin (H&amp;E) stained tissue sections from mice after spending 14 days in the JABS arena or control wean cage.</title>
<p>Tissues selected for examination (eye, lung, trachea and nasal passages) are those expected to be most affected if the mice lived in a space with inadequate air flow. All tissues appeared normal.</p></caption>
<graphic xlink:href="476229v4_figs2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The Average Gender Imbalance (AGI) can be calculated as the mean of the Gender Imbalance (GI) for all strains:
<disp-formula id="eqn3">
<graphic xlink:href="476229v4_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<list list-type="order">
<list-item><p><italic>n</italic> is the number of strains</p></list-item>
<list-item><p><inline-formula><inline-graphic xlink:href="476229v4_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the number of male samples for strain <italic>i</italic></p></list-item>
<list-item><p><inline-formula><inline-graphic xlink:href="476229v4_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is the number of female samples for strain i.</p></list-item>
</list>
</sec>
<sec id="s5c">
<label>6.3</label><title>List of features for JABS</title>
<table-wrap id="tbls4" orientation="portrait" position="anchor">
<label>Table S4.</label>
<caption><title>List of JABS features.</title></caption>
<graphic xlink:href="476229v4_tbls4.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="476229v4_tbls4a.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="476229v4_tbls4b.tif" mimetype="image" mime-subtype="tiff"/>
<graphic xlink:href="476229v4_tbls4c.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figs3" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S3.</label>
<caption><title>Per strain F1 score and accuracy for Grooming dataset.</title></caption>
<graphic xlink:href="476229v4_figs3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs4" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S4.</label>
<caption><title>F1 score and accuracy for mice with different coat colors in the grooming dataset.</title></caption>
<graphic xlink:href="476229v4_figs4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls3" orientation="portrait" position="anchor">
<label>Table S3.</label>
<caption><title>Behavioral phenotypes annotated by different annotators (A1, A2).</title>
<p> Each phenotype measures a specific metric related to bouts of the indicated behavior during the first 55 minutes of the video averaged across all the analyzed videos.</p></caption>
<graphic xlink:href="476229v4_tbls3.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figs5" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S5.</label>
<caption><title>JABS 600 Strain Distribution by Sex.</title></caption>
<graphic xlink:href="476229v4_figs5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="figs6" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S6.</label>
<caption><title>JABS 1200 Strain Distribution by Sex.</title></caption>
<graphic xlink:href="476229v4_figs6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<table-wrap id="tbls5" orientation="portrait" position="anchor">
<label>Table S5.</label>
<caption><title>Classifiers trained by JABS with their respective window sizes and F1 scores.</title></caption>
<graphic xlink:href="476229v4_tbls5.tif" mimetype="image" mime-subtype="tiff"/>
</table-wrap>
<fig id="figs7" position="anchor" orientation="portrait" fig-type="figure">
<label>Figure S7.</label>
<caption><title>JABS behavior characterization module: Univariate analysis captures the combined effect of sex and strain on the aggregate phenotypes using JABS600 dataset.</title>
<p>(A) JABS pipeline highlighting individual steps towards automated behavioral quantification. (B) The LOD scores (−<italic>log</italic><sub>10</sub>(<italic>q<sub>value</sub></italic>)) and effect sizes are shown at left and right panels, respectively. In the left panel, the number of *s represents the strength of evidence against the null hypothesis of no sex effect, while + represents a suggestive effect. In the right panel, the color (red for female and blue for male) and area of the circle (area being proportional to the size of the effect) represent the direction and magnitude of the effect size. Strains with a sex difference in at least one of the aggregated phenotypes are colored pink.</p></caption>
<graphic xlink:href="476229v4_figs7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gomez-Marin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Paton</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Kampff</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>R. M.</given-names></string-name> &amp; <string-name><surname>Mainen</surname>, <given-names>Z. F</given-names></string-name></person-group>. <article-title>Big behavioral data: psychology, ethology and the foundations of neuroscience</article-title>. <source>Nature neuroscience</source> <volume>17</volume>, <fpage>1455</fpage>–<lpage>1462</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Datta</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Leifer</surname>, <given-names>A</given-names></string-name></person-group>. <article-title>Computational neuroethology: a call to action</article-title>. <source>Neuron</source> <volume>104</volume>, <fpage>11</fpage>–<lpage>24</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Murthy</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Quantifying behavior to understand the brain</article-title>. <source>Nature neuroscience</source>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luxem</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Open-source tools for behavioral video analysis: Setup, methods, and best practices</article-title>. <source>eLife</source> <volume>12</volume>, <elocation-id>e79305</elocation-id> (<year>2023</year>). <pub-id pub-id-type="doi">10.7554/eLife.79305</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname>, <given-names>D. J.</given-names></string-name> &amp; <string-name><surname>Perona</surname>, <given-names>P</given-names></string-name></person-group>. <article-title>Toward a science of computational ethology</article-title>. <source>Neuron</source> <volume>84</volume>, <fpage>18</fpage>–<lpage>31</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lauer</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Mathis</surname>, <given-names>M. W</given-names></string-name></person-group>. <article-title>A primer on motion capture with deep learning: principles, pitfalls, and perspectives</article-title>. <source>Neuron</source> <volume>108</volume>, <fpage>44</fpage>–<lpage>65</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname>, <given-names>J. D.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>A new era in quantification of animal social behaviors. en</article-title>. <source>Neurosci. Biobehav. Rev</source>. <volume>157</volume>, <fpage>105528</fpage> (<month>Feb.</month> <year>2024</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geuther</surname>, <given-names>B. Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Robust mouse tracking in complex environments using neural networks</article-title>. <source>Communications biology</source> <volume>2</volume>, <fpage>124</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geuther</surname>, <given-names>B. Q.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Action detection using a neural network elucidates the genetics of mouse grooming behavior</article-title>. <source>eLife</source> <volume>10</volume>, <elocation-id>e63207</elocation-id> (<year>2021</year>). <pub-id pub-id-type="doi">10.7554/eLife.63207</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sheppard</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Stride-level analysis of mouse open field behavior using deep-learning-based pose estimation</article-title>. <source>Cell Reports</source> <volume>38</volume>, <fpage>110231</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Hession</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Sabnis</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Churchill</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>V.</given-names></string-name></person-group> <article-title>A machine vision based frailty index for mice</article-title>. <source>bioRxiv</source> (<year>2021</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guzman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Geuther</surname>, <given-names>B. Q.</given-names></string-name>, <string-name><surname>Sabnis</surname>, <given-names>G. S.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>Highly accurate and precise determination of mouse mass using computer vision</article-title>. <source>Patterns</source> (<year>2023</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sabnis</surname>, <given-names>G. S.</given-names></string-name>, <string-name><surname>Hession</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Beierle</surname>, <given-names>J. A.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>A high-throughput machine vision-based univariate scale for pain and analgesia in mice</article-title>. <source>bioRxiv</source> (<year>2022</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hession</surname>, <given-names>L. E.</given-names></string-name>, <string-name><surname>Sabnis</surname>, <given-names>G. S.</given-names></string-name>, <string-name><surname>Churchill</surname>, <given-names>G. A.</given-names></string-name> &amp; <string-name><surname>Kumar</surname>, <given-names>V</given-names></string-name></person-group>. <article-title>A machine-vision-based frailty index for mice</article-title>. <source>Nature aging</source> <volume>2</volume>, <fpage>756</fpage>–<lpage>766</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Sabnis</surname>, <given-names>G.</given-names></string-name>, <etal>et al.</etal></person-group> <article-title>Visual detection of seizures in mice using supervised machine learning</article-title>. <source>bioRxiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Isik</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Unal</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Open-source software for automated rodent behavioral analysis</article-title>. <source>Frontiers in Neuroscience</source> <volume>17</volume>, <fpage>1149027</fpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature neuroscience</source> <volume>21</volume>, <fpage>1281</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title>. <source>Nature methods</source> <volume>19</volume>, <fpage>486</fpage>–<lpage>495</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodwin</surname>, <given-names>N. L.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Simple Behavioral Analysis (SimBA) as a platform for explainable machine learning in behavioral neuroscience</article-title>. <source>Nature Neuroscience</source> <volume>27</volume>, <fpage>1411</fpage>–<lpage>1424</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segalin</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The Mouse Action Recognition System (MARS) software pipeline for automated analysis of social behaviors in mice</article-title>. <source>eLife</source> <volume>10</volume> (<year>2021</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kabra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Robie</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Rivera-Alba</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Branson</surname>, <given-names>K</given-names></string-name></person-group>. <article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title>. <source>Nature methods</source> <volume>10</volume>, <fpage>64</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hsu</surname>, <given-names>A. I.</given-names></string-name> &amp; <string-name><surname>Yttri</surname>, <given-names>E. A</given-names></string-name></person-group>. <article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title>. <source>Nature communications</source> <volume>12</volume>, <fpage>5188</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bohnslav</surname>, <given-names>J. P.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels</article-title>. <source>eLife</source> <volume>10</volume>, <elocation-id>e63377</elocation-id> (<year>2021</year>). <pub-id pub-id-type="doi">10.7554/eLife.63377</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Second-generation high-throughput forward genetic screen in mice to isolate subtle behavioral mutants</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>108</volume>, <fpage>15557</fpage>–<lpage>15564</lpage>. ISSN: 0027-8424 (<year>2011</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>National Research Council</collab></person-group> <source>Guide for the care and use of laboratory animals</source> <publisher-loc>Washington DC</publisher-loc>:<publisher-name>The National Academies Press</publisher-name> (<year>2011</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Myatt</surname>, <given-names>T. A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A study of indoor carbon dioxide levels and sick leave among office workers</article-title>. <source>Environmental Health</source> <volume>1</volume>, <fpage>1</fpage>–<lpage>10</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mexas</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Brice</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Caro</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Hillanbrand</surname>, <given-names>T. S.</given-names></string-name> &amp; <string-name><surname>Gaertner</surname>, <given-names>D. J</given-names></string-name></person-group>. <article-title>Nasal histopathology and intracage ammonia levels in female groups and breeding mice housed in static isolation cages</article-title>. <source>Journal of the American Association for Laboratory Animal Science</source> <volume>54</volume>, <fpage>478</fpage>–<lpage>486</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="report"><person-group person-group-type="author"><string-name><surname>Fawcett</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Rose</surname>, <given-names>M</given-names></string-name></person-group>. <source>Guidelines for the housing of mice in scientific institutions</source>. <publisher-name>Animal Welfare Unit, NSW Department of Primary Industries</publisher-name>. <publisher-loc>West Pennant Hills</publisher-loc>, <fpage>1</fpage>–<lpage>43</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gamble</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Clough</surname>, <given-names>G</given-names></string-name></person-group>. <article-title>Ammonia build-up in animal boxes and its effect on rat tracheal epithelium</article-title>. <source>Laboratory Animals</source> <volume>10</volume>, <fpage>93</fpage>–<lpage>104</lpage> (<year>1976</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferrecchia</surname>, <given-names>C. E.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Van Andel</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Intracage ammonia levels in static and individually ventilated cages housing C57BL/6 mice on 4 bedding substrates</article-title>. <source>Journal of the American Association for Laboratory Animal Science</source> <volume>53</volume>, <fpage>146</fpage>–<lpage>151</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tepper</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Weiss</surname>, <given-names>B.</given-names></string-name> &amp; <string-name><surname>Wood</surname>, <given-names>R. W</given-names></string-name></person-group>. <article-title>Alterations in behavior produced by inhaled ozone or ammonia</article-title>. <source>Fundamental and Applied Toxicology</source> <volume>5</volume>, <fpage>1110</fpage>–<lpage>1118</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Easterly</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Foltz</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>Paulus</surname>, <given-names>M. J</given-names></string-name></person-group>. <article-title>Body condition scoring: comparing newly trained scorers and micro-computed tomography imaging</article-title>. <source>Lab Animal-new York</source><italic>-</italic> <volume>30</volume>, <fpage>46</fpage>–<lpage>49</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hickman</surname>, <given-names>D. L.</given-names></string-name> &amp; <string-name><surname>Swan</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Use of a body condition score technique to assess health status in a rat model of polycystic kidney disease</article-title>. <source>Journal of the American Association for Laboratory Animal Science</source> <volume>49</volume>, <fpage>155</fpage>–<lpage>159</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lovasz</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Marks</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>B. K.</given-names></string-name> &amp; <string-name><surname>Saunders</surname>, <given-names>K. E</given-names></string-name></person-group>. <article-title>Effects on Mouse Food Consumption After Exposure to Bedding from Sick Mice or Healthy Mice</article-title>. <source>Journal of the American Association for Laboratory Animal Science</source> <volume>59</volume>, <fpage>687</fpage>–<lpage>694</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Green</surname>, <given-names>E. L.</given-names></string-name></person-group> <source>Biology of the laboratory mouse</source> <publisher-loc>New York</publisher-loc>:<publisher-name>Dover publications</publisher-name> (<year>1966</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biderman</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Lightning Pose: improved animal pose estimation via semi-supervised learning, Bayesian ensembling and cloud-native open-source tools</article-title>. <source>Nature methods</source> <volume>21</volume>, <fpage>1316</fpage>–<lpage>1328</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>T.</given-names></string-name> &amp; <string-name><surname>Guestrin</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Xgboost: A scalable tree boosting system</article-title> in <conf-name>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</conf-name> (<year>2016</year>), <fpage>785</fpage>–<lpage>794</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ho</surname>, <given-names>T. K.</given-names></string-name></person-group> <article-title>Random decision forests</article-title> in <conf-name>Proceedings of 3rd international conference on document analysis and recognition</conf-name> <volume>1</volume> (<year>1995</year>), <fpage>278</fpage>–<lpage>282</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname>, <given-names>L.</given-names></string-name></person-group> <article-title>Random forests</article-title>. <source>Machine learning</source> <volume>45</volume>, <fpage>5</fpage>–<lpage>32</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname>, <given-names>J. H</given-names></string-name></person-group>. <article-title>Greedy function approximation: a gradient boosting machine</article-title>. <source>Annals of statistics</source>, <fpage>1189</fpage>–<lpage>1232</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mathis</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nature Neuroscience</source> <volume>21</volume> (Sept. <year>2018</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaufman</surname>, <given-names>A. B.</given-names></string-name> &amp; <string-name><surname>Rosenthal</surname>, <given-names>R</given-names></string-name></person-group>. <article-title>Can you believe my eyes? The importance of interobserver reliability statistics in observations of animal behaviour</article-title>. <source>Animal Behaviour</source> <volume>78</volume>, <fpage>1487</fpage>–<lpage>1491</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Tjandrasuwita</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chaudhuri</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Yue</surname>, <given-names>Y</given-names></string-name></person-group>. <article-title>Interpreting expert annotation differences in animal behavior</article-title>. <source>arXiv</source>:<fpage>2106.06114</fpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McHugh</surname>, <given-names>M. L</given-names></string-name></person-group>. <article-title>Interrater reliability: the kappa statistic</article-title>. <source>Biochemia medica</source> <volume>22</volume>, <fpage>276</fpage>–<lpage>282</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Feichtenhofer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Malik</surname>, <given-names>J.</given-names></string-name> &amp; <string-name><surname>He</surname>, <given-names>K.</given-names></string-name></person-group> <article-title>Slowfast networks for video recognition</article-title> in <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name> (<year>2019</year>), <fpage>6202</fpage>–<lpage>6211</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Kalogeiton</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Weinzaepfel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Schmid</surname>, <given-names>C.</given-names></string-name></person-group> <article-title>Action tubelet detector for spatiotemporal action localization</article-title> in <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name> (<year>2017</year>), <fpage>4405</fpage>–<lpage>4413</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ryan</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Young</surname>, <given-names>N. B.</given-names></string-name>, <string-name><surname>Crawley</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Bodfish</surname>, <given-names>J. W.</given-names></string-name> &amp; <string-name><surname>Moy</surname>, <given-names>S. S</given-names></string-name></person-group>. <article-title>Social deficits, stereotypy and early emergence of repetitive behavior in the C58/J inbred mouse strain</article-title>. <source>Behavioural Brain Research</source> <volume>208</volume>, <fpage>178</fpage>–<lpage>188</lpage>. ISSN: 0166-4328. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0166432809007086">https://www.sciencedirect.com/science/article/pii/S0166432809007086</ext-link> (<year>2010</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blick</surname>, <given-names>M. G.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Novel object exploration in the C58/J mouse model of autistic-like behavior</article-title>. <source>Behavioural Brain Research</source> <volume>282</volume>, <fpage>54</fpage>–<lpage>60</lpage>. ISSN: 0166-4328. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0166432814008249">https://www.sciencedirect.com/science/article/pii/S0166432814008249</ext-link> (<year>2015</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ye</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>SuperAnimal pretrained pose estimation models for behavioral analysis</article-title>. <source>Nature communications</source> <volume>15</volume>, <fpage>5165</fpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pendergrass</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>The use of phenome-wide association studies (PheWAS) for exploration of novel genotype-phenotype relationships and pleiotropy discovery</article-title>. <source>Genetic epidemiology</source> <volume>35</volume>, <fpage>410</fpage>–<lpage>422</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weinreb</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics</article-title>. <source>Nature Methods</source> <volume>21</volume>, <fpage>1329</fpage>–<lpage>1339</lpage> (<year>2024</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luxem</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>Identifying behavioral structure from deep variational embeddings of animal motion</article-title>. <source>Communications Biology</source> <volume>5</volume>, <fpage>1267</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Berman</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Bialek</surname>, <given-names>W.</given-names></string-name> &amp; <string-name><surname>Shaevitz</surname>, <given-names>J. W</given-names></string-name></person-group>. <article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title>. <source>Journal of The Royal Society Interface</source> <volume>11</volume>, <fpage>20140672</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Blau</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal></person-group> <article-title>A study of animal action segmentation algorithms across supervised, unsupervised, and semi-supervised learning paradigms</article-title>. <source>ArXiv</source> (<year>2024</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Rosenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Meister</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Mice in a labyrinth: Rapid learning, sudden insight, and efficient exploration</article-title>. <source>bioRxiv</source> DOI:<pub-id pub-id-type="doi">10.1101/2021.01.14.426746v1</pub-id> (<year>2021</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arakawa</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Blanchard</surname>, <given-names>D. C.</given-names></string-name> &amp; <string-name><surname>Blanchard</surname>, <given-names>R. J</given-names></string-name></person-group>. <article-title>Colony formation of C57BL/6J mice in visible burrow system: identification of eusocial behaviors in a background strain for genetic animal models of autism</article-title>. <source>Behavioural brain research</source> <volume>176</volume>, <fpage>27</fpage>–<lpage>39</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>X.</given-names></string-name> &amp; <string-name><surname>Stephens</surname>, <given-names>M</given-names></string-name></person-group>. <article-title>Genome-wide efficient mixed-model analysis for association studies</article-title>. <source>Nature genetics</source> <volume>44</volume>, <fpage>821</fpage>–<lpage>824</lpage> (<year>2012</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107259.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Koo</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8722-0038</contrib-id>
<aff>
<institution-wrap>
<institution>Cold Spring Harbor Laboratory</institution>
</institution-wrap>
<city>Cold Spring Harbor</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> study presents JABS, an open-source platform that integrates hardware and user-friendly software for standardized mouse behavioral phenotyping. The work has practical implications for improving reproducibility and accessibility in behavioral neuroscience, especially for linking behavior to genetics across diverse mouse strains. The strength of evidence is <bold>convincing</bold>, with comprehensive validation of the platform's components and enthusiastic reviewer support.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107259.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript provides an open-source tool including hardware and software, and dataset to facilitate and standardize behavioral classification in laboratory mice. The hardware for behavioral phenotyping was extensively tested for safety. The software is GUI based facilitating the usage of this tool across the community of investigators that do not have a programming background. The behavioral classification tool is highly accurate, and the authors deposited a large dataset of annotations and pose tracking for many strains of mice. This tool has great potential for behavioral scientists that use mice across many fields, however there are many missing details that currently limit the impact of this tool and publication.</p>
<p>Strengths:</p>
<p>Software-hardware integration for facilitating cross-lab adaptation of the tool and minimizing the need to annotate new data for behavioral classification.</p>
<p>Data from many strains of mice was included in the classification and genetic analyses in this manuscript.</p>
<p>Large dataset annotated was deposited for the use of the community</p>
<p>GUI based software tool decreases barriers of usage across users with limited coding experience.</p>
<p>Weaknesses:</p>
<p>The GUI requires pose tracking for classification but, the software provided in JABS does not do pose tracking, so users must do pose tracking using a separate tool. The pose tracking quality directly impacts the classification quality, given that it is used for the feature calculation</p>
<p>Comments on revisions:</p>
<p>The authors addressed all my concerns.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107259.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This manuscript presents the JAX Animal Behavior System (JABS), an integrated mouse phenotyping platform that includes modules for data acquisition, behavior annotation, and behavior classifier training and sharing. The manuscript provides details and validation for each module, demonstrating JABS as a useful open-source behavior analysis tool that removes barriers to adopting these analysis techniques by the community. In particular, with the JABS-AI module users can download and deploy previously trained classifiers on their own data, or annotate their own data and train their own classifiers. The JABS-AI module also allows users to deploy their classifiers on the JAX strain survey dataset and receive an automated behavior and genetic report.</p>
<p>Strengths:</p>
<p>(1) The JABS platform addresses the critical issue of reproducibility in mouse behavior studies by providing an end-to-end system from rig setup to downstream behavioral and genetic analyses. Each step has clear guidelines, and the GUIs are an excellent way to encourage best practices for data storage, annotation, and model training. Such a platform is especially helpful for labs without prior experience in this type of analysis.</p>
<p>(2) A notable strength of the JABS platform is its reuse of large amounts of previously collected data at JAX Labs, condensing this into pretrained pose estimation models and behavioral classifiers. JABS-AI also provides access to the strain survey dataset through automated classifier analyses, allowing large-scale genetic screening based on simple behavioral classifiers. This has the potential to accelerate research for many labs by identifying particular strains of interest.</p>
<p>(3) The ethograph analysis will be a useful way to compare annotators/classifiers beyond the JABS platform.</p>
<p>Weaknesses:</p>
<p>(1) The manuscript contains many assertions that lack references in both the Introduction and Discussion. For example, in the Discussion, the assertion &quot;published research demonstrates that keypoint detection models maintain robust performance despite the presence of headstages and recording equipment&quot; lacks reference.</p>
<p>(2) The provided GUIs lower the barrier to entry for labs that are just starting to collect and analyze mouse open field behavior data. However, users must run pose estimation themselves outside of the provided GUIs, which introduces a key bottleneck in the processing pipeline, especially for users without strong programming skills. The authors have provided pretrained pose estimation models and an example pipeline, which is certainly to be commended, but I believe the impact of these tools could be greatly magnified by an additional pose estimation GUI (just for running inference, not for labeling/training).</p>
<p>(3) While the manuscript does a good job of laying out best practices, there is an opportunity to further improve reproducibility for users of the platform. The software seems likely to perform well with perfect setups that adhere to the JABS criteria, but it is very likely there will be users with suboptimal setups - poorly constructed rigs, insufficient camera quality, etc. It is important, in these cases, to give users feedback at each stage of the pipeline so they can understand if they have succeeded or not. Quality control (QC) metrics should be computed for raw video data (is the video too dark/bright? are there the expected number of frames? etc.), pose estimation outputs (do the tracked points maintain a reasonable skeleton structure; do they actually move around the arena?), and classifier outputs (what is the incidence rate of 1-3 frame behaviors? a high value could indicate issues). In cases where QC metrics are difficult to define (they are basically always difficult to define), diagnostic figures showing snippets of raw data or simple summary statistics (heatmaps of mouse location in the open field) could be utilized to allow users to catch glaring errors before proceeding to the next stage of the pipeline, or to remove data from their analyses if they observe critical issues.</p>
<p>Comments on revisions:</p>
<p>I thank the authors for taking the time to address my comments. They have provided a lot of important context in their responses. My only remaining recommendation is to incorporate more of this text into the manuscript itself, as this context will also be interesting/important for readers (and potential users) to consider. Specifically:</p>
<p>the quality control/user feedback features that have already been implemented (these are extremely important, and unfortunately, not standard practice in many labs)</p>
<p>top-down vs bottom-up imaging trade-offs (you make very good points!)</p>
<p>video compression, spatial and temporal resolution trade-offs</p>
<p>more detail on why the authors chose pose-based rather than pixel-based classifiers</p>
<p>I believe the proposed system can be extremely useful for behavioral neuroscientists, especially since the top-down freely moving mouse paradigm is one of the most ubiquitous in the field. Many labs have reinvented the wheel here, and as a field it makes sense to coalesce around a set of pipelines and best practices to accelerate the science we all want to do. I make the above recommendation with this in mind: bringing together (properly referenced) observations and experiences of the authors themselves, as well as others in the field, provides a valuable resource for the community. Obviously, the main thrust of the manuscript should be about the tools themselves; it should not turn into a review paper, so I'm just suggesting some additional sentences/references sprinkled throughout as motivation for why the authors made the choices that they did.</p>
<p>Intro typo: &quot;one link in the chainDIY rigs&quot;</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.107259.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Choudhary</surname>
<given-names>Anshul</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6651-5224</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Geuther</surname>
<given-names>Brian Q</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sproule</surname>
<given-names>Thomas J</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Beane</surname>
<given-names>Glen</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kohar</surname>
<given-names>Vivek</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Trapszo</surname>
<given-names>Jarek</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kumar</surname>
<given-names>Vivek</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6643-7465</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Public review):</bold></p>
<p>(1) The authors only report the quality of the classification considering the number of videos used for training, but not considering the number of mice represented or the mouse strain. Therefore, it is unclear if the classification model works equally well in data from all the mouse strains tested, and how many mice are represented in the classifier dataset and validation.</p>
</disp-quote>
<p>We agree that strain-level performance is critical for assessing generalizability. In the revision we now report per-strain accuracy and F1 for the grooming classifier, which was trained on videos spanning 60 genetically diverse strains (n = 1100 videos) and evaluated on the test set videos spanning 51 genetically diverse strains (n=153 videos). Performance is uniform across most strains (median F1 = 0.94, IQR = 0.899–0.956), with only modest declines in albino lines that lack contrast under infrared illumination; this limitation and potential remedies are discussed in the text. The new per-strain metrics are presented in the Supplementary figure (corresponding to Figure 4).</p>
<disp-quote content-type="editor-comment">
<p>(2) The GUI requires pose tracking for classification, but the software provided in JABS does not do pose tracking, so users must do pose tracking using a separate tool. Currently, there is no guidance on the pose tracking recommendations and requirements for usage in JABS. The pose tracking quality directly impacts the classification quality, given that it is used for the feature calculation; therefore, this aspect of the data processing should be more carefully considered and described.</p>
</disp-quote>
<p>We have added a section to the methods describing how to use the pose estimation models used in JABS. The reviewer is correct that pose tracking quality will impact classification quality. We recommend that classifiers should only be re-used on pose files generated by the same pose models used in the behavior classifier training dataset. We hope that the combination of sharing classifier training data and making a more unified framework for developing and comparing classifiers will get us closer to having foundational behavior classification models that work in many environments. We also would like to emphasize that deviating from using our pose model will also likely hinder re-using our shared large datasets in JABS-AI (JABS1200, JABS600, JABS-BxD).</p>
<disp-quote content-type="editor-comment">
<p>(3) Many statistical and methodological details are not described in the manuscript, limiting the interpretability of the data presented in Figures 4,7-8. There is no clear methods section describing many of the methods used and equations for the metrics used. As an example, there are no details of the CNN used to benchmark the JABS classifier in Figure 4, and no details of the methods used for the metrics reported in Figure 8.</p>
</disp-quote>
<p>We thank the reviewer for bringing this to our attention. We have added a methods section to the manuscript to address this concern. Specifically, we now provide: (1) improved citation visibility of the source of CNN experiments such that the reader can locate the architecture information, (2) mathematical formulations for all performance metrics (precision, recall, F1, …) with explicit equations;  (3) detailed statistical procedures including permutation testing methods, power analysis and multiple testing corrections used throughout Figures 7-8. These additions facilitate reproducibility and proper interpretation of all quantitative results presented in the manuscript.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Public review):</bold></p>
<p>(1) The manuscript as written lacks much-needed context in multiple areas: what are the commercially available solutions, and how do they compare to JABS (at least in terms of features offered, not necessarily performance)? What are other open-source options?</p>
</disp-quote>
<p>JABS adds to a list of commercial and open source animal tracking platforms. There are several reviews and resources that cover these technologies. JABS covers hardware, behavior prediction, a shared resource for classifiers, and genetic association studies. We’re not aware of another system that encompasses all these components. Commercial packages such as EthoVision XT and HomeCage Scan give users a ready-made camera-plus-software solution that automatically tracks each mouse and reports simple measures such as distance travelled or time spent in preset zones, but they do not provide open hardware designs, editable behavior classifiers, or any genetics workflow. At the open-source end, the &gt;100 projects catalogued on OpenBehavior and summarised in recent reviews (Luxem et al., 2023; Işık &amp; Ünal 2023) usually cover only one link in the chain—DIY rigs, pose-tracking libraries (e.g., DeepLabCut, SLEAP) or supervised and unsupervised behaviour-classifier pipelines (e.g., SimBA, MARS, JAABA, B-SOiD, DeepEthogram). JABS provides an open source ecosystem that integrates all four: (i) top-down arena hardware with parts list and assembly guide; (ii) an active-learning GUI that produces shareable classifiers; (iii) a public web service that enables sharing of the trained classifier and applies any uploaded classifier to a large and diverse strain survey; and (iv) built-in heritability, genetic-correlation and GWAS reporting. We have added a concise paragraph in the Discussion that cites these resources and makes this end-to-end distinction explicit.</p>
<disp-quote content-type="editor-comment">
<p>(2) How does the supervised behavioral classification approach relate to the burgeoning field of unsupervised behavioral clustering (e.g., Keypoint-MoSeq, VAME, B-SOiD)?</p>
</disp-quote>
<p>The reviewer raises an important point about the rapidly evolving landscape of automated behavioral analysis, where both supervised and unsupervised approaches offer complementary strengths for different experimental contexts. Unsupervised methods like Keypoint-MoSeq , VAME , and B-SOiD , which prioritize motif discovery from unlabeled data but may yield less precise alignments with expert annotations, as evidenced by lower F1 scores in comparative evaluations. Supervised approaches (like ours), by contrast, employ fully supervised classifiers to deliver frame-accurate, behavior-specific scores that align directly with experimental hypotheses. Ultimately, a pragmatic hybrid strategy, starting with unsupervised pilots to identify motifs and transitioning to supervised fine-tuning with minimal labels, can minimize annotation burdens and enhance both discovery and precision in ethological studies. This has been added in the discussion section of the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(3) What kind of studies will this combination of open field + pose estimation + supervised classifier be suitable for? What kind of studies is it unsuited for? These are all relevant questions that potential users of this platform will be interested in.</p>
</disp-quote>
<p>This approach is suitable for a wide array of neuroscience, genetics, pharmacology, preclinical, and ethology studies. We have published in the domains of action detection for complex behaviors such as grooming, gait and posture, frailty, nociception, and sleep. We feel these tools are indispensable for modern behavior analysis.</p>
<disp-quote content-type="editor-comment">
<p>(4) Throughout the manuscript, I often find it unclear what is supported by the software/GUI and what is not. For example, does the GUI support uploading videos and running pose estimation, or does this need to be done separately? How many of the analyses in Figures 4-6 are accessible within the GUI?</p>
</disp-quote>
<p>We have now clarified these. The JABS framework comprises two distinct GUI applications with complementary functionalities. The JABS-AL (active learning) desktop application handles video upload, behavioral annotation, classifier training, and inference -- it does not perform pose estimation, which must be completed separately using our pose tracking pipeline (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/mouse-tracking-runtime">https://github.com/KumarLabJax/mouse-tracking-runtime</ext-link>). If a user does not want to use our pose tracking pipeline, we have provided conversions through SLEAP to convert to our JABS pose format.  The web-based GUI enables classifier sharing and cloud-based inference on our curated datasets (JABS600, JABS1200) and downstream behavioral statistics and genetic analyses (Figures 4-6). The JABS-AL application also supports CLI (command line interface) operation for batch processing.  We have clarified these distinctions and provided a comprehensive workflow diagram in the revised Methods section.</p>
<disp-quote content-type="editor-comment">
<p>(5) While the manuscript does a good job of laying out best practices, there is an opportunity to further improve reproducibility for users of the platform. The software seems likely to perform well with perfect setups that adhere to the JABS criteria, but it is very likely that there will be users with suboptimal setups - poorly constructed rigs, insufficient camera quality, etc. It is important, in these cases, to give users feedback at each stage of the pipeline so they can understand if they have succeeded or not. Quality control (QC) metrics should be computed for raw video data (is the video too dark/bright? are there the expected number of frames? etc.), pose estimation outputs (do the tracked points maintain a reasonable skeleton structure; do they actually move around the arena?), and classifier outputs (what is the incidence rate of 1-3 frame behaviors? a high value could indicate issues). In cases where QC metrics are difficult to define (they are basically always difficult to define), diagnostic figures showing snippets of raw data or simple summary statistics (heatmaps of mouse location in the open field) could be utilized to allow users to catch glaring errors before proceeding to the next stage of the pipeline, or to remove data from their analyses if they observe critical issues.</p>
</disp-quote>
<p>These are excellent suggestions that align with our vision for improving user experience and data quality assessment. We recognize the critical importance of providing users with comprehensive feedback at each stage of the pipeline to ensure optimal performance across diverse experimental setups. Currently, we provide end-users with tools and recommendations to inspect their own data quality. In our released datasets (Strain Survey OFA and BXD OFA), we provide video-level quality summaries for coverage of our pose estimation models.</p>
<p>For behavior classification quality control, we employ two primary strategies to ensure proper operation: (a) outlier manual validation and (b) leveraging known characteristics about behaviors. For each behavior that we predict on datasets, we manually inspect the highest and lowest expressions of this behavior to ensure that the new dataset we applied it to maintains sufficient similarity. For specific behavior classifiers, we utilize known behavioral characteristics to identify potentially compromised predictions. As the reviewer suggested, high incidence rates of 1-3 frame bouts for behaviors that typically last multiple seconds would indicate performance issues.</p>
<p>We currently maintain in-house post-processing scripts that handle quality control according to our specific use cases. Future releases of JABS will incorporate generalized versions of these scripts, integrating comprehensive QC capabilities directly into the platform. This will provide users with automated feedback on video quality, pose estimation accuracy, and classifier performance, along with diagnostic visualizations such as movement heatmaps and behavioral summary statistics.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1 (Recommendations for the authors):</bold></p>
<p>(1) A weakness of this tool is that it requires pose tracking, but the manuscript does not detail how pose tracking should be done and whether users should expect that the data deposited will help their pose tracking models. There is no specification on how to generate pose tracking that will be compatible with JABS. The classification quality is directly linked to the quality of the pose tracking. The authors should provide more details of the requirements of the pose tracking (skeleton used) and what pose tracking tools are compatible with JABS. In the user website link, I found no such information. Ideally, JABS would be integrated with the pose tracking tool into a single pipeline. If that is not possible, then the utility of this tool relies on more clarity on which pose tracking tools are compatible with JABS.</p>
</disp-quote>
<p>The JABS ecosystem was deliberately designed with modularity in mind, separating the pose estimation pipeline from the active learning and classification app (JABS-AL) to offer greater flexibility and scalability for users working across diverse experimental setups. Our pose estimation pipeline is documented in detail within the new Methods subsection, outlining the steps to obtain JABS-compatible keypoints with our recommended runtime (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/mouse-tracking-runtime">https://github.com/KumarLabJax/mouse-tracking-runtime</ext-link>) and frozen inference models (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/deep-hrnet-mouse">https://github.com/KumarLabJax/deep-hrnet-mouse</ext-link>). This pipeline is an independent component within the broader JABS workflow, generating skeletonized keypoint data that are then fed into the JABS-AL application for behavior annotation and classifier training.</p>
<p>By maintaining this separation, users have the option to use their preferred pose tracking tools— such as SLEAP —while ensuring compatibility through provided conversion utilities to the JABS skeleton format. These details, including usage instructions and compatibility guidance, are now thoroughly explained in the newly added pose estimation subsection of our Methods section. This modular design approach ensures that users benefit from best-in-class tracking while retaining the full power and reproducibility of our active learning pipeline.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors should justify why JAABA was chosen to benchmark their classifier. This tool was published in 2013, and there have been other classification tools (e.g., SIMBA) published since then.</p>
</disp-quote>
<p>We appreciate the reviewer’s suggestion regarding SIMBA. However, our comparisons to JAABA and a CNN are based on results from prior work (Geuther, Brian Q., et al. &quot;Action detection using a neural network elucidates the genetics of mouse grooming behavior.&quot; Elife 10 (2021): e63207.), where both were used to benchmark performance on our publicly released dataset. In this study, we introduce JABS as a new approach and compare it against those established baselines. While SIMBA may indeed offer competitive performance, we believe the responsibility to demonstrate this lies with SIMBA’s authors, especially given the availability of our dataset for benchmarking.</p>
<disp-quote content-type="editor-comment">
<p>(3) I had a lot of trouble understanding the elements of the data calculated in JABS vs outside of JABS. This should be clarified in the manuscript.</p>
<p>(a) For example, it was not intuitive that pose tracking was required and had to be done separately from the JABS pipeline. The diagrams and figures should more clearly indicate that.</p>
<p>(b) In section 2.5, are any of those metrics calculated by JABS? Another software GEMMA, but no citation is provided for this tool. This created ambiguity regarding whether this is an analysis that is separate from JABS or integrated into the pipeline.</p>
</disp-quote>
<p>We acknowledge the confusion regarding the delineation between JABS components and external tools, and we have comprehensively addressed this throughout the manuscript. The JABS ecosystem consists of three integrated modules: JABS-DA (data acquisition), JABS-AL (active learning for behavior annotation and classifier training), and JABS-AI (analysis and integration via web application). Pose estimation, while developed by our laboratory, operates as a preprocessing pipeline that generates the keypoint coordinates required for subsequent JABS classifier training and annotation workflows. We have now added a dedicated Methods subsection that explicitly maps each analytical step to its corresponding software component, clearly distinguishing between core JABS modules and external tools (such as GEMMA for genetic analysis). Additionally, we have provided proper citations and code repositories for all external pipelines to ensure complete transparency regarding the computational workflow and enable full reproducibility of our analyses.</p>
<disp-quote content-type="editor-comment">
<p>(4) There needs to be clearer explanations of all metrics, methods, and transformations of the data reported.</p>
<p>(a) There is very little information about the architecture of the classification model that JABS uses.</p>
<p>(b) There are no details on the CNN used for comparing and benchmarking the classifier in JABS.</p>
<p>(c) Unclear how the z-scoring of the behavioral data in Figure 7 was implemented.</p>
<p>(d) There is currently no information on how the metrics in Figure 8 are calculated.</p>
</disp-quote>
<p>We have added a comprehensive Methods section that not only addresses the specific concerns raised above but provides complete methodological transparency throughout our study. This expanded section includes detailed descriptions of all computational architectures (including the JABS classifier and grooming benchmark models and metrics), statistical procedures and data transformations (including the z-scoring methodology for Figure 7), downstream genetic analysis (including all measures presented in Figure 8), and preprocessing pipelines.</p>
<disp-quote content-type="editor-comment">
<p>(5) The authors talk about their datasets having visual diversity, but without seeing examples, it is hard to know what they mean by this visual diversity. Ideally, the manuscript would have a supplementary figure with a representation of the variety of setups and visual diversity represented in the datasets used to train the model. This is important so that readers can quickly assess from reading the manuscript if the pre-trained classifier models could be used with the experimental data they have collected.</p>
</disp-quote>
<p>The visual diversity of our training datasets has been comprehensively documented in our previous tracking work (<ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s42003-019-0362-1">https://www.nature.com/articles/s42003-019-0362-1</ext-link>), which systematically demonstrates tracking performance across mice with diverse coat colors (black, agouti, albino, gray, brown, nude, piebald), body sizes including obese mice, and challenging recording conditions with dynamic lighting and complex environments. Notably, Figure 3B in that publication specifically illustrates the robustness across coat colors and body shapes that characterize the visual diversity in our current classifier training data. To address the reviewer's concern and enable readers to quickly assess the applicability of our pre-trained models to their experimental data, we have now added this reference to the manuscript to ground our claims of visual diversity in published evidence.</p>
<disp-quote content-type="editor-comment">
<p>(6) All figures have a lot of acronyms used that are not defined in the figure legend. This makes the figures really hard to follow. The figure legends for Figures 1,2, 7, and 9 did not have sufficient information for me to comprehend the figure shown.</p>
</disp-quote>
<p>We have fixed this in the manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(7) In the introduction, the authors talk about compression artifacts that can be introduced in camera software defaults. This is very vague without specific examples.</p>
</disp-quote>
<p>This is a complex topic that balances the size and quality of video data and is beyond the scope of this paper. We have carefully optimized this parameter and given the user a balanced solution. A more detailed blog post on compression artifacts can be found at our lab’s webpage (<ext-link ext-link-type="uri" xlink:href="https://www.kumarlab.org/2018/11/06/brians-video-compression-tests/">https://www.kumarlab.org/2018/11/06/brians-video-compression-tests/</ext-link>). We have also added a comment about keyframes shifting temporal features in the main manuscript.</p>
<disp-quote content-type="editor-comment">
<p>(8) More visuals of the inside of the apparatus should be included as supplementary figures. For example, to see the IR LEDs surrounding the camera.</p>
</disp-quote>
<p>We have shared data from JABS as part of several papers including the tracking paper (Geuther et al 2019), grooming, gait and posture, mouse mass. We have also released entire datasets that as part of this paper (JABS1800, JABS-BXD). We also have step by step assembly guide that shows the location of the lights/cameras and other parts (see Methods, JABS workflow guide, and this PowerPoint file in the GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/KumarLabJax/JABS-datapipeline/blob/main/Multi-day%20setup%20PowerPoint%20V3.pptx">https://github.com/KumarLabJax/JABS-datapipeline/blob/main/Multi-day%20setup%20PowerPoint%20V3.pptx</ext-link>).</p>
<disp-quote content-type="editor-comment">
<p>(9) Figure 2 suggests that you could have multiple data acquisition systems simultaneously. Do each require a separate computer? And then these are not synchronized data across all boxes?</p>
</disp-quote>
<p>Each JABS-DA unit has its own edge device (Nvidia Jetson). Each system (which we define as multiple JABS-DA areas associated with one lab/group) can have multiple recording devices (arenas). The system requires only 1 control portal (RPi computer) and can handle as many recording devices as needed (Nvidia computer w/ camera associated with each JABS-DA arena). To collect data, 1 additional computer is needed to visit the web control portal and initiate a recording session. Since this is a web portal, users can use any computer or a tablet. The recording devices are not strictly synchronized but can be controlled in a unified manner.</p>
<disp-quote content-type="editor-comment">
<p>(10) The list of parts on GitHub seems incomplete; many part names are not there.</p>
</disp-quote>
<p>We thank referee for bringing this to our attention. We have updated the GitHub repository (and its README) which now links out to the design files.</p>
<disp-quote content-type="editor-comment">
<p>(11) The authors should consider adding guidance on how tethers and headstages are expected to impact the use of JABS, as many labs would be doing behavioral experiments combined with brain measurements.</p>
</disp-quote>
<p>While our pose estimation model was not specifically trained on tethered animals, published research demonstrates that keypoint detection models maintain robust performance despite the presence of headstages and recording equipment. Once accurate pose coordinates are extracted, the downstream behavior classification pipeline operates independently of the pose estimation method and would remain fully functional. We recommend users validate pose estimation accuracy in their specific experimental setup, as the behavior classification component itself is agnostic to the source of pose coordinates.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2 (Recommendations for the authors):</bold></p>
<p>(1) &quot;Using software-defaults will introduce compression artifacts into the video and will affect algorithm performance.&quot; Can this be quantified? I imagine most of the performance hit comes from a decrease in pose estimation quality. How does a decrease in pose estimation quality translate to action segmentation? Providing guidelines to potential users (e.g., showing plots of video compression vs classifier performance) would provide valuable information for anyone looking to use this system (and could save many labs countless hours replicating this experiment themselves). A relevant reference for the effect of compression on pose estimation is Mathis, Warren 2018 (bioRxiv): On the inference speed and video-compression robustness of DeepLabCut.</p>
</disp-quote>
<p>Since our behavior classification approach depends on features derived from keypoint, changes in keypoint accuracy will affect behavior segmentation accuracy. We agree that it is important to try and understand this further, particularly with the shared bioRxiv paper investigating the effect of compression on pose estimation accuracy. Measuring the effect of compression on keypoint and behavior classification is a complex task to evaluate concisely, given the number of potential variables to inspect. To list a few variables that should be investigated are: discrete cosine transform quality (Mathis, Warren experiment), Frame Size (Mathis, Warren experiment), Keyframe Interval (new, unique to video data), inter-frame settings (new, unique to video data), behavior of interest, Pose models with compression-augmentation used in training ( <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1506.08316?">https://arxiv.org/pdf/1506.08316?</ext-link>) and type of CNN used (under active development). The simplest recommendation that we can make at this time is that we know compression will affect behavior predictions and that users should be cautious about using our shared classifiers on compressed video data. To show that we are dedicated in sharing these results as we run those experiments, in a related work ( CV4Animals conference accepted paper (<ext-link ext-link-type="uri" xlink:href="https://www.cv4animals.com/">https://www.cv4animals.com/</ext-link>) and can be downloaded here <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/file/d/1UNQIgCUOqXQh3vcJbM4QuQrq02HudBLD/view">https://drive.google.com/file/d/1UNQIgCUOqXQh3vcJbM4QuQrq02HudBLD/view</ext-link>) we have already begun to inspect how changing some factors affect behavior segmentation performance. In this work, we investigate the robustness of behavior classification across multiple behaviors using different keypoint subsets. Our findings in this work is that classifiers are relatively stable across different keypoint subsets. We are actively working on follow-up effort to investigate the effect of keypoint noise, CNN model architecture, and other factors we've listed above on behavior segmentation tasks.</p>
<disp-quote content-type="editor-comment">
<p>(2) The analysis of inter-annotator variability is very interesting. I'm curious how these differences compare to two other types of variability:</p>
<p>(a) intra-annotator variability; I think this is actually hard to quantify with the presented annotation workflow. If a given annotator re-annotated a set of videos, but using different sparse subsets of the data, it is not possible to disentangle annotator variability versus the effect of training models on different subsets of data. This can only be rigorously quantified if all frames are labeled in each video.</p>
</disp-quote>
<p>We propose an alternative approach to behavior classifier development in the text associated with Figure 3C. We do not advocate for high inter-annotator agreement since individual behavior experts have differing labeling style (an intuitive understanding of the behavior). Rather, we allow multiple classifiers for the same behavior and allow the end user to prioritize classifiers based on heritability of the behavior from a classifier.</p>
<disp-quote content-type="editor-comment">
<p>(b) In lieu of this, I'd be curious to see the variability in model outputs trained on data from a single annotator, but using different random seeds or train/val splits of the data. This analysis would provide useful null distributions for each annotator and allow for more rigorous statistical arguments about inter-annotator variability.</p>
</disp-quote>
<p>JABS allows the user to use multiple classifiers (random forest, XGBoost). We do not expect the user to carry out hyperparameter tuning or other forms of optimization. We find that the major increase in performance comes from optimizing the size of the window features and folds of cross validation. However, future versions of JABS-AL could enable a complete hyper-parameter scan across seeds and data splits to obtain a null distribution for each annotator.</p>
<disp-quote content-type="editor-comment">
<p>(c) I appreciate the open-sourcing of the video/pose datasets. The authors might also consider publicly releasing their pose estimation and classifier training datasets (i.e., data plus annotations) for use by method developers.</p>
</disp-quote>
<p>We thank the referee for acknowledging our commitment to open data sharing practices. Building upon our previously released strain survey dataset, we have now also made our complete classifier training resources publicly available, including the experimental videos, extracted pose coordinates, and behavioral annotations. The repository link has been added to the manuscript to ensure full reproducibility and facilitate community adoption of our methods.</p>
<disp-quote content-type="editor-comment">
<p>(3) More thorough discussion on the limitations of the top-down vs bottom-up camera viewpoint; are there particular scientific questions that are much better suited to bottomup videos (e.g., questions about paw tremors, etc.).</p>
</disp-quote>
<p>Top-down imaging, bottom-up, and multi-view imaging have a variety of pros and cons. Generally speaking, multi-view imaging will provide the most accurate pose models but requires increased resources on both hardware setup as well as processing of data. Top-down provides the advantage of flexibility for materials, since the floor doesn’t need to be transparent. Additionally lighting and potential reflection with the bottom-up perspective. Since the paws are not occluded from the bottom-up perspective, models should have improved paw keypoint precision allowing the model to observe more subtle behaviors. However, the appearance of the arena floor will change over time as the mice defecate and urinate. Care must be taken to clean the arena between recordings to ensure transparency is maintained. This doesn’t impact top-down imaging that much but will occlude or distort from the bottom-up perspective. Additionally, the inclusion of bedding for longer recordings, which is required by IACUC, will essentially render bottom-up imaging useless because the bedding will completely obscure the mouse. Overall, while bottomup may provide a precision benefit that will greatly enhance subtle motion, top-down imaging is overall more robust for obtaining consistent imaging across large experiments for longer periods of time.</p>
<disp-quote content-type="editor-comment">
<p>(4) More thorough discussion on what kind of experiments would warrant higher spatial or temporal resolution (e.g., investigating slight tremors in a mouse model of neurodegenerative disease might require this greater resolution).</p>
</disp-quote>
<p>This is an important topic that deserves its own perspective guide. We try to capture some of this in the paper on specifications. However, we only scratch the surface. Overall, there are tradeoffs between frame rate, resolution, color/monochrome, and compression. Labs have collected data at hundreds of frames per second to capture the kinetics of reflexive behavior for pain (AbdoosSaboor lab) or whisking behavior. Labs have also collected data a low 2.5 frames per second for tracking activity or centroid tracking (see Kumar et al PNAS). The data collection specifications are largely dependent on the behaviors being captured. Our rule of thumb is the Nyquist Limit, which states that the data capture rate needs to be twice that of the frequency of the event. For example, certain syntaxes of grooming occur at 7Hz and we need 14FPS to capture this data. JABS collects data at 30FPS, which is a good compromise between data load and behavior rate. We use 800x800 pixel resolution which is a good compromise to capture animal body parts while limiting data size. Thank you for providing the feedback that the field needs guidance on this topic. We will work on creating such guidance documents for video data acquisition parameters to capture animal behavior data for the community as a separate publication.</p>
<disp-quote content-type="editor-comment">
<p>(5) References</p>
<p>(a) Should add the following ref when JAABA/MARS are referenced: Goodwin et al.2024, Nat Neuro (SimBA)</p>
<p>(b) Could also add Bohnslav et al. 2021, eLife (DeepEthogram).</p>
<p>(c) The SuperAnimal DLC paper (Ye et al. 2024, Nature Comms) is relevant to the introduction/discussion as well.</p>
</disp-quote>
<p>We thank the referee for the suggestions. We have added these references.</p>
<disp-quote content-type="editor-comment">
<p>(6) Section 2.2:</p>
<p>While I appreciate the thoroughness with which the authors investigated environmental differences in the JABS arena vs standard wean cage, this section is quite long and eventually distracted me from the overall flow of the exposition; might be worth considering putting some of the more technical details in the methods/appendix.</p>
</disp-quote>
<p>These are important data for adopters of JABS to gain IACUC approval in their home institution. These committees require evidence that any new animal housing environment has been shown to be safe for the animals. In the development of JABS, we spent a significant amount of time addressing the JAX veterinary and IACUC concerns. Therefore, we propose that these data deserve to be in the main text.</p>
<disp-quote content-type="editor-comment">
<p>(7) Section 2.3.1:</p>
<p>(a) Should again add the DeepEthogram reference here</p>
<p>(b) Should reference some pose estimation papers: DeepLabCut, SLEAP, Lightning Pose.</p>
</disp-quote>
<p>We thank the referee for the suggestions. We have added these references.</p>
<disp-quote content-type="editor-comment">
<p>(c) &quot;Pose based approach offers the flexibility to use the identified poses for training classifiers for multiple behaviors&quot; - I'm not sure I understand why this wouldn't be possible with the pixel-based approach. Is the concern about the speed of model training? If so, please make this clearer.</p>
</disp-quote>
<p>The advantage lies not just in training speed, but in the transferability and generalization of the learned representations. Pose-based approaches create structured, low-dimensional latent embeddings that capture behaviorally relevant features which can be readily repurposed across different behavioral classification tasks, whereas pixel-based methods require retraining the entire feature extraction pipeline for each new behavior. Recent work demonstrates that pose-based models achieve greater data efficiency when fine-tuned for new tasks compared to pixel-based transfer learning approaches [1], and latent behavioral representations can be partitioned into interpretable subspaces that generalize across different experimental contexts [2]. While pixel-based approaches can achieve higher accuracy on specific tasks, they suffer from the &quot;curse of dimensionality&quot; (requiring thousands of pixels vs. 12 pose coordinates per frame) and lack the semantic structure that makes pose-based features inherently reusable for downstream behavioral analysis.</p>
<p>(1) Ye, Shaokai, et al. &quot;SuperAnimal pretrained pose estimation models for behavioral analysis.&quot; Nature communications 15.1 (2024): 5165.</p>
<p>(2) Whiteway, Matthew R., et al. &quot;Partitioning variability in animal behavioral videos using semi-supervised variational autoencoders.&quot; PLoS computational biology 17.9 (2021): e1009439.</p>
<disp-quote content-type="editor-comment">
<p>(d) The pose estimation portion of the pipeline needs more detail. Do users use a pretrained network, or do they need to label their own frames and train their own pose estimator? If the former, does that pre-trained network ship with the software? Is it easy to run inference on new videos from a GUI or scripts? How accurate is it in compliant setups built outside of JAX? How long does it take to process videos?</p>
</disp-quote>
<p>We have added the guidance on pose estimation in the manuscript (section “2.3.1 Behavior annotation and classifier training” and in the methods section titled “Pose tracking pipeline”)</p>
<disp-quote content-type="editor-comment">
<p>(e) The final paragraph describing how to arrive at an optimal classifier is a bit confusing - is this the process that is facilitated by the app, or is this merely a recommendation for best practices? If this is the process the app requires, is it indeed true that multiple annotators are required? While obviously good practice, I imagine there will be many labs that just want a single person to annotate, at least in the beginning prototyping stages. Will the app allow training a model with just a single annotator?</p>
</disp-quote>
<p>We have clarified this in the text.</p>
<disp-quote content-type="editor-comment">
<p>(8) Section 2.5:</p>
<p>(a) This section contained a lot of technical details that I found confusing/opaque, and didn't add much to my overall understanding of the system; sec 2.6 did a good job of clarifying why 2.5 is important. It might be worth motivating 2.5 by including the content of 2.6 first, and moving some of the details of 2.5 to the method/appendix.</p>
</disp-quote>
<p>We moved some of the technical details in section 2.5 to the methods section titled “Genetic analysis”. Furthermore, we have added few statements to motivate the need of genetic analysis and how the webapp can facilitate this (which is introduced in the section 2.6)</p>
<disp-quote content-type="editor-comment">
<p>(9) Minor corrections:</p>
<p>(a) Bottom of first page, &quot;always been behavior quantification task&quot; missing &quot;a&quot;.</p>
<p>(b) &quot;Type&quot; column in Table S2 is undocumented and unused (i.e., all values are the same); consider removing.</p>
<p>(c) Figure 4B, x-axis: add units.</p>
<p>(d) Page 8/9: all panel references to Figure S1 are off by one</p>
</disp-quote>
<p>We have fixed them in the updated manuscript.</p>
</body>
</sub-article>
</article>