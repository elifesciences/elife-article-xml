<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">97612</article-id>
<article-id pub-id-type="doi">10.7554/eLife.97612</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97612.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.2</article-version>
</article-version-alternatives>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dynamic reinforcement learning reveals time-dependent shifts in strategy during reward learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4681-0538</contrib-id>
<name>
<surname>Venditto</surname>
<given-names>Sarah Jo C</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3465-2512</contrib-id>
<name>
<surname>Miller</surname>
<given-names>Kevin J</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4201-561X</contrib-id>
<name>
<surname>Brody</surname>
<given-names>Carlos D</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5029-1430</contrib-id>
<name>
<surname>Daw</surname>
<given-names>Nathaniel D</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">*</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Princeton University</institution></aff>
<aff id="a2"><label>2</label><institution>Howard Hughes Medical Institute</institution></aff>
<aff id="a3"><label>3</label><institution>Google DeepMind</institution></aff>
<aff id="a4"><label>4</label><institution>University College London</institution></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Uchida</surname>
<given-names>Naoshige</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Harvard University</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Frank</surname>
<given-names>Michael J</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Brown University</institution>
</institution-wrap>
<city>Providence</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>*</label><bold>For correspondence:</bold> <email>sjvenditto@gmail.com</email> (SJCV); <email>brody@princeton.edu</email> (CDB); <email>ndaw@princeton.edu</email> (NDD)</corresp>
<fn id="n1" fn-type="equal"><label>†</label><p>These authors contributed equally to this work</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-05-10">
<day>10</day>
<month>05</month>
<year>2024</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP97612</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-03-28">
<day>28</day>
<month>03</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-03-05">
<day>05</day>
<month>03</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.28.582617"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Venditto et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Venditto et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-97612-v1.pdf"/>
<abstract>
<title>Abstract</title>
<p>Different brain systems have been hypothesized to subserve multiple “experts” that compete to generate behavior. In reinforcement learning, two general processes, one model-free (MF) and one model-based (MB), are often modeled as a mixture of agents (MoA) and hypothesized to capture differences between automaticity vs. deliberation. However, shifts in strategy cannot be captured by a static MoA. To investigate such dynamics, we present the mixture-of-agents hidden Markov model (MoA-HMM), which simultaneously learns inferred action values from a set of agents and the temporal dynamics of underlying “hidden” states that capture shifts in agent contributions over time. Applying this model to a multi-step,reward-guided task in rats reveals a progression of within-session strategies: a shift from initial MB exploration to MB exploitation, and finally to reduced engagement. The inferred states predict changes in both response time and OFC neural encoding during the task, suggesting that these states are capturing real shifts in dynamics.</p>
</abstract>
</article-meta>
<notes>
<notes notes-type="competing-interest-statement">
<title>Competing Interest Statement</title><p>The authors have declared no competing interest.</p></notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Additional task information in figure 1. Fixed symbols in figure 2 for consistency with text. Updated references and additional paragraph in discussion.</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Behavior is rarely static. Time-varying factors, both internal and external, can influence the way in which humans and animals make decisions. Different ways of choosing an action can be attributed to using different strategies. One prominent perspective on such strategy heterogeneity is that the brain contains relatively independent, separable circuits that are conceptualized as supporting distinct strategies, each potentially competing for control. For instance, instrumental behavior appears to be supported by separable circuits for automatic versus deliberative control (<bold><italic><xref ref-type="bibr" rid="c33">Killcross and Coutureau, 2003</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c16">Daw et al., 2005</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c26">Gremel and Costa, 2013</xref></italic></bold>), and perhaps also different systems mediate the balance between such instrumental exploitation vs. different exploration strategies (<bold><italic><xref ref-type="bibr" rid="c17">Daw et al., 2006</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c7">Blanchard and Gershman, 2018</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c30">Hogeveen et al., 2022</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c11">Cinotti et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c12">Cockburn et al., 2022</xref></italic></bold>). The relative contribution of different strategies to behavior is often characterized via a “mixture-of-agents” (MoA) model that describes behavior as a weighted contribution of multiple “agents” that implement different strategies (<bold><italic><xref ref-type="bibr" rid="c10">Camerer and Ho, 1999</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c15">Daw et al., 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c40">Miller et al., 2017</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c35">Krueger et al., 2017</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c23">Gershman, 2018</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c54">Wilson et al., 2021</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c12">Cockburn et al., 2022</xref></italic></bold>). It is hypothesized that the brain acts in the same way, weighing advice from separable circuits on how best to act (<bold><italic><xref ref-type="bibr" rid="c45">O’Doherty et al., 2021</xref></italic></bold>). The agent with the biggest weight can be thought of as the dominant strategy. Henceforth, we will broaden our use of the word “strategy” to now refer to a given weighting over the underlying individual agents.</p>
<p>Although one key goal of this type of modeling is to characterize how humans and animals switch between different strategies (e.g., the formation of habits with excessive training), in prac-tice MoA models are usually fit to behavior assuming a single, fixed weighting over agents. Such models will therefore fail to capture changes in strategy over time, limiting our ability to associate behavioral and neural factors underlying variable strategies, and potentially missing contributions of a lesser used strategy all-together. For example, competing systems of automation and deliberation are often formalized and measured through a reinforcement learning (RL) framework as a mixture of model-free (MF) and model-based (MB) control (<bold><italic><xref ref-type="bibr" rid="c16">Daw et al., 2005</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c52">Sutton and Barto, 2020</xref></italic></bold>). A multi-step reward-guided task known as the “two-step task” was developed to differentiate these strategies in humans (<bold><italic><xref ref-type="bibr" rid="c15">Daw et al., 2011</xref></italic></bold>) and has since been used in rodents (<bold><italic><xref ref-type="bibr" rid="c40">Miller et al., 2017</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c29">Hasz and Redish, 2018</xref></italic></bold>; <bold><italic>Groman et al., 2019b</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c2">Akam et al., 2021</xref></italic></bold>). Many versions of the task show choices are well fit by a mixture of both MB and MF behavior (<bold><italic><xref ref-type="bibr" rid="c15">Daw et al., 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c29">Hasz and Redish, 2018</xref></italic></bold>; <bold><italic>Groman et al., 2019b</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c2">Akam et al., 2021</xref></italic></bold>), whereas other studies show behavior dominated by MB planning (<bold><italic><xref ref-type="bibr" rid="c40">Miller et al., 2017</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c22">Feher da Silva and Hare, 2020</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c41">Miller et al., 2022</xref></italic></bold>). The lack of evidence for MF influences in these studies is somewhat puzzling, for instance because MF learning is predicted by classic models of the dopamine system (<bold><italic><xref ref-type="bibr" rid="c50">Schultz et al., 1997</xref></italic></bold>).</p>
<p>Apart from a few studies which build in some specific hypothesized change rule for strategy weighting (but do not, accordingly, measure such change in an unbiased way) (<bold><italic><xref ref-type="bibr" rid="c37">Lee et al., 2014</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c34">Kool et al., 2016</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c20">Ebitz et al., 2018</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c14">Costa et al., 2019</xref></italic></bold>), these studies neglect the dynamic representation of strategy. This leaves open the possibility that characterizing the dynamics of the agent mixture could reveal time-dependent trade-offs between strategies. For example, it might help clarify the behavioral discrepancies between different two-step tasks by revealing bouts of where MF control is the dominant agent, in animals whose behavior is otherwise dominated by a MB agent. It might also identify dynamic shifts in the contributions from other less well understood agents, such as the novelty preference agent. This in turn might help to clarify its function, e.g. (as we show here) for exploration.</p>
<p>To answer these questions, we present an elaborated model that not only measures the contribution of separable agents, but also characterizes how this contribution changes over time. A recent class of models known as GLM-HMMs (<bold><italic><xref ref-type="bibr" rid="c9">Calhoun et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c4">Ashwood et al., 2022</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c8">Bolkan et al., 2022</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c46">Oostland et al., 2022</xref></italic></bold>) seeks to measure time-dependent shifts in strategy by nesting a generalized linear model (GLM), which maps perceptual variables to choices, under a hidden Markov model (HMM), which models an internal “hidden” state that switches between different GLM instantiations, producing a dynamic weighting over perceptual variables. This model has been successful at capturing how the dependence of actions on measurable variables – such as choice history, reward, or a task stimulus – can vary dynamically throughout the task. This can infer, for example, periods of engagement and disengagement (<bold><italic><xref ref-type="bibr" rid="c3">Ashwood et al., 2020</xref></italic></bold>), behavioral differences from neural inhibition (<bold><italic><xref ref-type="bibr" rid="c8">Bolkan et al., 2022</xref></italic></bold>), or speed of task learning with respect to an experimental stimulus (<bold><italic><xref ref-type="bibr" rid="c46">Oostland et al., 2022</xref></italic></bold>). However, GLM-HMMs, as originally specified, model dynamics over directly observable variables in the GLM, and do not incorporate learning rules that themselves generate internal variables (such as action values inferred from models of reinforcement learning) that intervene to explain observed data.</p>
<p>Here we replace the GLM stage of the GLM-HMM with a MoA to produce the mixture-of-agents hidden Markov model (MoA-HMM). Extending both MoA models and GLM-HMMs, the MoA-HMM characterizes the weighted influence of both inferred learning rules and (optionally) observed variables on subsequent actions, and how this weighting over agents can dynamically shift over time. We apply this approach to reanalyze data from rats performing the two-step task (<bold><italic><xref ref-type="bibr" rid="c40">Miller et al., 2017</xref></italic></bold>) to examine whether there is evidence for strategy shifting, and if so — making no assumptions other than the HMM dynamics — how it proceeds. Using MB and MF learning rules to capture MB versus MF control alongside differentiating between exploratory and exploitative behavior, we show that the MoA-HMM reveals discernible shifts in the dominant agent throughout behavioral sessions. These shifts reveal a time-dependent trade-off between exploration and exploitation as well as drifting engagement in the task. We further show that these shifts in strategy provide a new lens to analyze behavioral features and neural data that were not used to train the model: the model-inferred strategy captures, out of sample, significant changes in both reaction times and neural encoding.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Reinforcement learning in the rat two-step task</title>
<p>We study rat behavior and neural recordings in a multi-step, reward-guided decision-making task known as the rat two-step task. Originally described by <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>), this task is an adaptation of a similar task in humans (<bold><italic><xref ref-type="bibr" rid="c15">Daw et al., 2011</xref></italic></bold>) that was designed to study the relative contribution of model-based and model-free strategies to choices. In the current version of the task, the rat is placed in an environment with two choice states (step one) that are separated in space and time from two outcome states (step two), in addition to initiation states for each step (<bold><xref rid="fig1" ref-type="fig">Figure 1A</xref></bold>). Each state is characterized by its own nose port, into which the rat has been trained to only poke when lit (yellow sun). The rat initiates the trial by entering the top center port (<bold>i</bold>), after which it is presented with a free decision between the two choice states (<bold>ii</bold>). Once a choice is made, the animal initiates the second step (<bold>iv</bold>) and is presented with a single outcome port (<bold>v</bold>) that it can enter for a chance at receiving liquid reward (<bold>vi</bold>). The available outcome port is dependent on a set, probabilistic link between choices in step one and outcomes in step two, which can be represented as a transition matrix – the environment’s action-outcome model. Here one choice has an 80% probability of leading to one outcome (its <italic>common</italic> transition) and a 20% chance of leading to the opposite outcome (its <italic>rare</italic> transition), with flipped probabilities for the opposite choice. Either outcome state has a chance of giving reward; however, one outcome will always have a greater chance than the other (80% vs 20%), where the better outcome state flips unpredictably throughout the session (<bold>vii</bold>). The goal of the rat, who’s been incentivized for liquid reward, is to track the better outcome port to maximize reward, which they do successfully (<bold><xref rid="fig1" ref-type="fig">Figure 1B</xref></bold>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Rat Two-Step Task Description and Behavior</title>
<p><bold>A</bold> Schematic of single trial during the rat two step task, where yellow suns indicate active port. The rat initiates a trial by poking into the top-center port (<bold>i</bold>), after which the rat is prompted to chose between the adjacent side ports (<bold>ii</bold>). After making a choice choice, the rat initiates the second step (<bold>iii</bold>). During the second step, the probabilistic transition determines the outcome probability (<bold>iv</bold>), and the rat is instructed to enter the active outcome port (<bold>v</bold>). Each outcome port has some probability of delivering liquid reward, <italic>P</italic><sub><italic>r</italic></sub>, at one port and the inverse probability, 1 − <italic>P</italic><sub><italic>r</italic></sub>, at the other port (<bold>vi</bold>). Throughout the session, the reward probability <italic>P</italic><sub><italic>r</italic></sub> will reverse unpredictably (<bold>vii</bold>). <bold>B</bold> Trial-history regressions fit to data simulated by each individual RL agent, each with learning rate 0.5 (5000 trials split between 20 sessions for each simulation). Each agent demonstrates differential effects of trial type on choices, which decays across past trials. (<bold>i</bold>) The model-based reward (MBr) agent tends to repeat choices (positive weight) following common-rewarded (blue-solid) and rare-omission (red-dashed) trials, and switches choices (negative weight) following rare-rewarded (blue-dashed) and common-omission (red-solid) trials, showing an effect by both transition and reward. (<bold>ii</bold>) Model-based choice (MBc) agent with positive weight tends to repeat choices following common transitions (solid) and switch away from choices following rare transition trials (dashed), capturing transition-dependent outcome port perseveration. (<bold>iii</bold>) Model-free reward (MFr) agent tends to repeat choices after rewarded (blue) trials and switch away from choices following omission trials (red), ignoring the observed transition. (<bold>iv</bold>) Positive model-free choice (MFc) captures a choice perseveration, tending to repeat choices regardless of observed reward or transition. <bold>C</bold> Trial-history regressions fit to animal behavioral data (n=20 rats), where dark lines are mean regression values across all rats and shaded error bars are 95% confidence intervals around the mean. Rats look most similar to the MBr agent, but show additional modulation that can be accounted for by the influence of other agents.</p></caption>
<graphic xlink:href="582617v2_fig1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>The strength of the two-step task is in its separation of choice and outcome states and their transition structure, which allows model-free (MF) and model-based (MB) strategies to be distinguished by the way rewards and outcomes influence subsequent choices (<bold><xref rid="fig1" ref-type="fig">Figure 1C</xref></bold>). The key insight is that the stochastic state transitions (common vs. rare) decouple each action’s reward history (which should drive MF learning) from the reward histories associated with its predominant outcome state (which could, alternatively, be used for MB evaluation of the action’s value in terms of where it is likely to lead).</p>
<p>We can summarize MB and MF reward-learning strategies via two learning rules (or agents) that differ in whether they take account of transition in learning from reward. We also acccompany them by two more agents that capture reward-independent choice autocorrelation (perseveration or alternation), either MF (i.e., over actions) or MB (i.e., over predominant destination states). These learning rules (based on the formulation from <bold><italic><xref ref-type="bibr" rid="c31">Ito and Doya, 2009</xref></italic></bold>, extended to the two-step task, see <bold><italic><xref ref-type="bibr" rid="c47">Park et al. (2017</xref></italic></bold>); <bold><italic>Groman et al. (2019a</italic></bold>)) have some algebraic differences from variants used in some previous studies (<bold><italic><xref ref-type="bibr" rid="c15">Daw et al., 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c40">Miller et al., 2017</xref></italic></bold>; see Methods), but capture the same logic. In addition to the four RL agents, an intercept or “bias” agent is included that captures the tendency towards left-ward or right-ward choices.</p>
<p>To demonstrate how reward and transition differently affect each agent, we adopt a trial-history regression that maps the influence of each trial type (common-reward, common-omission, rarereward, and rare-omission) at varying trial lags (<bold><italic><xref ref-type="bibr" rid="c42">Miller et al., 2016</xref></italic></bold>). A positive weight indicates that choices tend to repeat following that trial type, and a negative weight indicate that choices tend to switch following that trial type. Specifically, the first agent, model-based reward learning (MBr), captures the effect of both reward and transition on future choices (<bold><xref rid="fig1" ref-type="fig">Figure 1Ai</xref></bold>). The key feature is that a rare transition, relative to a common one, reverses the effect of a reward or omission on MB evaluation. This is because of the symmetry of the transition matrix: the reward received after a rare transition is at an outcome port which is most often reached by the other choice. Thus, rewards following common transitions and omissions following rare transitions will positively weight the chosen action, and omissions following a common transition and a reward following a rare transition will negatively influence the chosen action. The second agent, model-based choice learning (MBc), captures a history-dependent effect of the sampled outcome port on choices (<bold><xref rid="fig1" ref-type="fig">Figure 1Aii</xref></bold>. Put simply, it captures a “common-stay, rare-switch” pattern of choices during the two-step task, leading to <italic>outcome perseveration</italic> or (if negative) <italic>alternation</italic>. That is, like MFc, this agent tends to repeat or switch choices regardless of reward; however, it does so by choosing or avoiding the action commonly associated with the obtained outcome port rather than the action itself. This means that (if negative) it promotes switching choices so as to sample previously unsampled outcomes, like a MB (transition-sensitive) form of directed exploration. This comes from repeating choices after “rare” transitions in attempt to sample the “common” outcome, and switching choices after “common” transitions in attempt to sample the opposite “common” outcome. These agents, then, can capture <italic>MB exploitation</italic> from a positive influence of MBr and <italic>MB exploration</italic> from a negative influence of MBc.</p>
<p>The third agent, Model-free reward learning (MFr) integrates choice and reward information by positively influencing choices that lead to reward and negatively influencing choices that lead to omissions (<bold><xref rid="fig1" ref-type="fig">Figure 1Aiii</xref></bold>), ignoring the transition (outcome state) that was experienced. The forth agent, Model-free choice learning (MFc), uses only choice information and captures a history-dependent tendency to repeat or avoid choices on subsequent trials, i.e., <italic>choice perseveration</italic> or <italic>alternation</italic> (<bold><xref rid="fig1" ref-type="fig">Figure 1Aiv</xref></bold>). In other words, depending on the sign of the agent’s mixture coefficient, each choice will favor or disfavor the same choice on future trials regardless of the reward received. Notably, a negative coefficient will drive a tendency to choose the action not previously sampled, akin to directed exploration in action space. Put together, these agents can capture <italic>MF exploitation</italic> from a positive influence of MFr and <italic>MF exploration</italic> from a negative influence of MFc.</p>
<p>The RL agents used here differ from those previously used to model the rat two-step task (<bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>, <xref rid="c41" ref-type="bibr">2022</xref>)). The main elaboration is the inclusion of MBc to capture history-dependent outcome perseveration/MB exploration. This generalizes and clarifies the role of the “novelty preference” (NP) agent used in the earlier studies, which similarly captured “common-stay, rare-switch” behavior (albeit in the opposite direction, i.e. “rare-stay/common-switch”), but lacked dependence on more than the preceding trial. Although it significantly contributed to behavior previously, its role was not thoroughly explored. The updated learning rules used here not only provide a better fit to behavior than previous versions of the MoA, (<xref rid="fig5_1" ref-type="fig">Figure 5 – figure supplement 1</xref>), but also the role of the MBc agent is further clarified especially, as we’ll see later on, when its dynamics become apparent. For further detail on specific value update rules for the new and original agents used for comparison, refer to the methods section.</p>
</sec>
<sec id="s2b">
<title>MoA-HMM framework</title>
<p>Before assessing how an MoA-HMM can describe behavior, it’s important to first understand how an static MoA model can generate decisions. Imagine a set of agents, <italic>A</italic> ∈ <italic>{agents}</italic>, and a set of choices, <italic>y</italic> ∈ <italic>{choices}</italic>, where each agent has its own value for each choice <italic>Q</italic><sub><italic>A</italic></sub>(<italic>y</italic>) that is updated at the end of every trial. Using the term “actor” to refer to overall action selection, a mixture-of-agents model dictates that the probability of a selecting choice <italic>y</italic><sub><italic>t</italic></sub> on some trial <italic>t</italic> arises from a softmax over net valuations for each action, computed as the weighted sum of each agent’s values
<disp-formula id="eqn1">
<graphic xlink:href="582617v2_eqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>In the context of the two-step task (and similar bandit-style reward learning tasks more generally), an actor takes some choice <italic>y</italic> that leads to some outcome state <italic>o</italic> and reward <italic>r</italic>. For demonstration, we assume an actor’s behavior is described by an MoA with two agents, a model-free choice learning rule (MFc) and a model-based reward learning rule (MBr), <italic>A</italic> ∈ <italic>{MBr, MF c}</italic> (<bold><xref rid="fig2" ref-type="fig">Figure 2A</xref></bold>). This might reflect competition between goal-directed and habitual strategies (<bold><italic><xref ref-type="bibr" rid="c44">Miller et al., 2019</xref></italic></bold>). On the first trial, each agent’s values are initialized (e.g., uniform across choices), <bold>Q</bold><sub>1</sub> = [<italic>Q</italic><sub><italic>MBr</italic></sub>, <italic>Q</italic><sub><italic>MF c</italic></sub>] (i)A net value is computed as a weighted sum of <bold>Q</bold><sub>1</sub> given static agent weights <bold><italic>β</italic></bold> = [<italic>β</italic><sub><italic>MBr</italic></sub>, <italic>β</italic><sub><italic>MF c</italic></sub>] to produce a probability distribution over choices <italic>p</italic>(<italic>y</italic><sub>1</sub>) (<bold><xref ref-type="disp-formula" rid="eqn1">eq. 1</xref></bold>) from which the actor’s choice is drawn (ii)After a choice is made <bold>(iii)</bold>, some outcome <italic>o</italic><sub>1</sub> and reward <italic>r</italic><sub>1</sub> is observed by the actor <bold>(iv)</bold>, and all observed variables may be used to update each agent’s choice values <bold>(v)</bold>. These values are fed into the next trial, and the process repeats until the final trial.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Model diagrams during a generic reward learning task.</title>
<p><bold>A</bold> Diagram of a static MoA model. At the start of a session, values for each agent, <bold>Q</bold><sub>1</sub> = [<italic>Q</italic><sub><italic>MBr</italic></sub>, <italic>Q</italic><sub><italic>MF c</italic></sub>], are initialized and fed into a MoA described by agent weights <bold><italic>β</italic></bold> = [<italic>β</italic><sub><italic>MBr</italic></sub>, <italic>β</italic><sub><italic>MF c</italic></sub>]. This weighted sum is pushed through a softmax to produce a probability distribution <italic>P</italic> (<italic>c</italic><sub>1</sub>) from which to draw choice <italic>c</italic><sub>1</sub>. An outcome port <italic>o</italic><sub>1</sub> and reward <italic>r</italic><sub>1</sub> are observed, and all task variables are used to update agent values according to their respective update rules (see methods). These updated values are used by the MoA on the next trial, and the process repeats until the session terminates. <bold>B</bold> Diagram of a MoA-HMM model. Values are initialized and updated in the same manner as A, but now the MoA used to generate choice probability is dependent on hidden state, where each hidden state MoA has its own set of agent weights <bold><italic>β</italic></bold><sup><italic>z</italic></sup> = [<bold><italic>β</italic></bold><sup>1</sup>, <bold><italic>β</italic></bold><sup>2</sup>, <bold><italic>β</italic></bold><sup>3</sup>]. The state at the beginning of the session is determined by an initial state probability distribution, and the conditional probability of each subsequent hidden state is governed by a transition matrix, each row corresponding to the previous state and each column corresponding to the upcoming state.</p></caption>
<graphic xlink:href="582617v2_fig2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>Next consider an HMM in isolation. An HMM produces a stochastic trajectory of hidden states that are each associated with different probabilistic patterns over observable data. Thus, it produces discrete changes in observation statistics, reflecting the underlying, temporal transition dynamics of hidden states. When paired with an MoA, these hidden states determine the set of agent weights being used to combine agent strategies at each time step (the <italic>β</italic>s in <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>), and thereby imply dynamics in how these sets of weights trade off over time. We assume that the MoA-HMM uses the same choice values and learning rules as before, but the weighting over agent values is now dependent on hidden state <italic>z, β</italic><sub><italic>A</italic></sub>(<italic>z</italic>). Thus, at time point <italic>t</italic> given hidden state <italic>z</italic><sub><italic>t</italic></sub>, <xref ref-type="disp-formula" rid="eqn1">equation 1</xref> updates to
<disp-formula id="eqn2">
<graphic xlink:href="582617v2_eqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p><bold><xref rid="fig2" ref-type="fig">Figure 2B</xref></bold> demonstrates how an MoA-HMM with three hidden states affects how choices are generated from two RL agents in an MoA model. Notably, in the case of three hidden states, we now have three sets of weights <bold><italic>β</italic></bold><sup><italic>z</italic></sup> = [<bold><italic>β</italic></bold><sup>1</sup>, <bold><italic>β</italic></bold><sup>2</sup>, <bold><italic>β</italic></bold><sup>3</sup>], where the set is selected according to the active hidden state. The active hidden state at any given time is, in turn, influenced by two distributions governing the HMM dynamics: 1) the initial state distribution, <italic>p</italic>(<italic>z</italic><sub>1</sub>), determines the state at the beginning of a sequence <bold>(i)</bold>; and 2) the conditional probability of a subsequent state given the current state <italic>p</italic>(<italic>z</italic><sub><italic>t</italic></sub>|<italic>z</italic><sub><italic>t</italic>−1</sub>), i.e. a state transition matrix <bold>(ii)</bold>, iteratively determines each subsequent state in the sequence. For more details on the fitting procedure, see Methods.</p>
<p>How might an MoA-HMM improve our understanding of RL behavior during a reward-learning task? As stated previously, RL behavior is often modeled using a weighted mixture of model-free and model-based RL rules. This simple mixture model would fail fully to capture a situation in which one learning rule dominates during some blocks of trials and the other dominates during other blocks of trials. For example, consider an actor during the two-step task whose behavior can be described by three states, one that is more goal-directed in which MBr dominates with a slower learning rate, a second that is more habitual in which MFc dominates with a faster learning rate, and a third producing a heavy side bias (<bold><xref rid="fig3" ref-type="fig">Figure 3i,ii</xref></bold>). Assume this actor also has a non-uniform initial-state probability (<bold>iii</bold>) and asymmetric transition dynamics depicted in (<bold>iv</bold>). Accordingly, over the course of a session, the net values used to select choices would be primarily dominated by one of the agents dependent on which hidden state is active (<bold><xref rid="fig3" ref-type="fig">Figure 3B</xref></bold>), allowing for variability, clustered in time, in how these values contribute to choice selection. Fitting a single-state MoA model to data generated from this actor would describe behavior as a mix of MBr, MFc, and bias strategies (<bold><xref rid="fig3" ref-type="fig">Figure 3Ci</xref></bold>) and might even fail to recover the MBr and MFc learning rates (<bold>ii</bold>). This single-state MoA would be unable to capture the dynamic influence of each of the agents with the underlying hidden state (<bold><xref rid="fig3" ref-type="fig">Figure 3D</xref></bold>. A 3-state MoA-HMM, however, is able to recover the original agent weights and learning rates (<bold><xref rid="fig3" ref-type="fig">Figure 3E</xref></bold>) as well as approximate the underlying hidden state and dynamic influence of agents given hidden state (<bold><xref rid="fig3" ref-type="fig">Figure 3F</xref></bold>). Beyond this didactic example, 3-state MoA-HMMs can be well-recovered over a wide range of parameters even when including all four RL agents (<xref rid="fig3_1" ref-type="fig">Figure 3 – figure supplements 1</xref> and <xref rid="fig3_2" ref-type="fig">2</xref>). To test if it can reveal switching RL strategies in real data, we apply the MoA-HMM to rats behaving in two-step task.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Example 3-state MoA-HMM with single-state and 3-state model recovery.</title>
<p><bold>A</bold> 3-state MoA-HMM parameters used to generate example data during the two-step task. Three agents are included: MBr, MFc, and Bias. (<bold>i</bold>) Agent weights for each hidden state. The first state (blue) is dominated by the MBr agent, the second state (orange) is dominated by the MFc agent, and the third state (green) is dominated by the Bias agent. (<bold>ii</bold>) Learning rates for MBr and MFc agents. (<bold>iii</bold>) Non-uniform initial state probability. (<bold>iv</bold>) Asymmetric transition probability matrix. <bold>B</bold> Effective (weighted) agent values during an example session. The dominant value in the weighted sum (black line) changes depending on the active hidden state (background color). <bold>C</bold> Recovered parameters by a single-state MoA. (<bold>i</bold>) Recovered agent weights erroneously identify behavior as a mixture of the three agents. (<bold>ii</bold>) Agent learning rates recovered by the single-state MoA. <bold>D</bold> Inferred values from recovered single-state MoA. The weighting on each agent is fixed across the session. <bold>E</bold> Recovered 3-state MoA-HMM parameters. (<bold>i</bold>) Each generative state, with a single dominating strategy per state, is recaptured. (<bold>ii</bold>) Learning rates are similarly recovered. <bold>F</bold> Inferred values from recovered 3-state MoA-HMM vary similarly to the generative values. The recovered hidden state is also closely approximated.<bold><xref rid="fig3_1" ref-type="fig">Figure 3—figure supplement 1</xref></bold>. Simulated 3-state MoA-HMM parameter recovery for the five agents used in behavioral fits: model-based reward, model-based choice, model-free reward, model-free choice, and bias. Each simulation contained 5000 trials evenly split between 20 sessions. Parameters for each state are pooled. <bold><xref rid="fig3_2" ref-type="fig">Figure 3—figure supplement 2</xref></bold>. 3-state MoA-HMM parameter recovery from data simulated from each rats’ behavioral model fit. Each rat’s model (n=20) was used to generate 5 independent data sets, where each data set contained the same number of trials and sessions as the corresponding rat’s behavioral data set used to fit the behavioral model, giving a total of 100 simulations.</p></caption>
<graphic xlink:href="582617v2_fig3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
<sec id="s2c">
<title>Dynamic reinforcement learning behavior is better captured by the MoA-HMM</title>
<p>We return to using a MoA model with all four RL agents alongside a “bias” agent. As stated previously, the rat two-step task specifically isolates model-based planning in rats. Alongside the choice plot in <xref rid="fig1" ref-type="fig">Figure 1C</xref>, this can be further demonstrated by fitting a single-state MoA (to each rat’s data separately) and looking at the difference in normalized likelihood (cross validated across sessions) when removing one of the agents (<bold><xref rid="fig4" ref-type="fig">Figure 4B</xref></bold>, gray). The normalized likelihood, computed as <italic>exp</italic>(<italic>L</italic>/<italic>N</italic>) where <italic>L</italic> is the cross-validated log likelihood of the model and <italic>N</italic> is the total number of trials, represents the proportion of the model likelihood attributed to the selected choice, where 50% is the model performing at chance level. We see the largest reduction in normalized likelihood when removing MBr (median of -3.10% across rats, p&lt;1E-5 from a non-parametric signed rank test (SR)) and a non-significant change when removing MFr (median of -0.03%, p=0.08 SR), consistent with results from <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>). We also see significant decrease in likelihood when removing MBc (median of -0.34%, p&lt;1E-4 SR), MFc (median of -1.23%, p&lt;1E-5 SR), and Bias (median of -0.30%, p&lt;1E-5 SR) agents.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Changes in model fit quality across rats.</title>
<p><bold>A</bold> Change in normalized, cross-validated likelihood from the full model when removing one of the agents, computed for both a single-state MoA (gray, left) and a 3-state MoA-HMM (green, right). All agents, excluding MFr for a single-state MoA, show a significant (across rats) decrease in likelihood. <bold>B</bold> (left) Normalized, cross-validated likelihood of a single-state MoA. (right) Increase in normalized, cross-validated likelihood when adding additional hidden states into the MoA-HMM, relative to one fewer states. Significant increases are observed through 4 states. <bold>A-B</bold> Each colored dot is an individual rat, black dots correspond to the median across rats, and error bars are bootstrapped 95% confidence intervals around the median. *:p&lt;0.02, **:p&lt;0.001, ***:p&lt;1E-4</p>
<p><bold><xref rid="fig4_1" ref-type="fig">Figure 4—figure supplement 1</xref></bold>. New RL learning rules significantly improve fit to behavior and capture much of the variance explained by the Novelty Preverence and Perseveration agents of the original model from <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>)</p></caption>
<graphic xlink:href="582617v2_fig4.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>What this model does not reveal, however, is any underlying dynamics to the dominant strategy. By adding additional hidden states, we observe a significant increase in normalized, cross-validated likelihood, suggesting that additional states capture dynamics in the behavior unaccounted for by a single-state MoA. (<bold><xref rid="fig4" ref-type="fig">Figure 4C</xref></bold>). At two hidden states (orange), we see an average jump of 0.77% (median, p&lt;1E-5 SR). The fit is still better with three hidden states (green), seeing an median of 1.15% jump in quality vs the one-state model, which is also significantly greater than with two states (median increase of 0.33%, p&lt;1E-4 SR). Significant improvements can be seen until four states (purple, median of 1.37% vs one state; median increase of 0.18% vs three states, p&lt;1E-4 SR), after which we see non-significant gains from five or more states relative to the predecessor model. This suggests that four hidden states optimally describe behavior. However, our chief goal in what follows is to explore a descriptive model. Examination revealed that four states often highlight idiosyncrasies between animals (<xref rid="fig5_3" ref-type="fig">Figure 5 – figure supplement 3</xref>), making it more difficult to draw general conclusions. Therefore, in the remaining analyses we restrict our model to three states to facilitate interpretability and comparison across animals.</p>
<p>Using a three-state model, we find that the state dynamics absorb some of the variance when either the MBr or MFc are removed (<bold><xref rid="fig4" ref-type="fig">Figure 4B</xref></bold>, green, MBr: -2.03%, p&lt;1E-4 SR; MFc: -0.35%, p&lt;0.001 SR). Intriguingly, the removal of MBc and Bias both show on average a greater decrease in model fit likelihood (MBc: -0.62%, p&lt;1E-4 SR; Bias: -0.52%, p&lt;1E-4 SR) as well as MFr showing a slightly greater, now significant decrease in likelihood (-0.06%, p=0.015 SR), possibly suggesting a greater role of these agents when dynamics are included.</p>
</sec>
<sec id="s2d">
<title>Time-varying hidden states reveal new dynamics of behavior</title>
<p>Fitting a three-state MoA-HMM to each rat reveals interesting properties not seen in a single-state MoA. <bold><xref rid="fig5" ref-type="fig">Figure 5A</xref></bold> shows an example fit on one of the animals, where we summarize the fit by displaying the agent weights split by hidden state (<bold>i</bold>), the initial state probability (<bold>ii</bold>), and the state transition probability matrix (<bold>iii</bold>). Additionally, we show the average likelihood of each state across sessions (<bold>iv</bold>), which displays the dynamics of when each state is inferred to be active within a session, averaged over sessions. From this example, we can identify some prominent features that are common to many of the rats. Since the ordering of states in the model is arbitrary, we used some of these features to identify a canonical ordering for the states across rats to allow group-level comparison.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Example and summary 3-state MoA-HMMs fit to rats in the two-step task using a population prior (see methods). The states are manually ordered based on three properties: the first state is chosen as the one with the highest initial state probability (blue diamond), the remaining two states are then ordered second and third (orange and green diamonds, respectively) in order of the weight they give to MBr (orange higher). <bold>A</bold> Example 3-state MoA-HMM on a single rat. (<bold>i</bold>) Agent weights split by hidden state. (<bold>ii</bold>) Initial state probability. (<bold>iii</bold>) State transition probability matrix. (<bold>iv</bold>) The expected hidden state calculated from the forward-backward algorithm averaged (mean) across sessions, with error bars as 95% confidence intervals around the mean. <bold>B</bold> Summary of all 3-state MoA-HMM fits across the population of 20 rats. States were sorted in the same way as (A). (<bold>i</bold>) Agent weights split by hidden state, where individual dots represent a single rat, light grey lines connect weights within a rat, bars represent the median weight over rats, and error bars are bootstrapped 95% confidence intervals around the median. Distribution of initial state probabilities, with each dot as an individual rat. (<bold>iii</bold>) Distribution of state transition probabilities, with each panel representing a single (<italic>t</italic> − 1) → (<italic>t</italic>) transition. (<bold>iv</bold>) Session-averaged expected state computed as in (Aiv), where light lines are average probabilities for individual rats and the dark solid lines are the population mean with 95% confidence intervals around the mean. *:p&lt;0.05, ***:p&lt;E-4</p>
<p><bold><xref rid="fig5_1" ref-type="fig">Figure 5—figure supplement 1</xref></bold>. (<bold>A</bold>) Learning rates fit for each agent (<bold>i</bold>) corresponding to the example rat shown in <xref rid="fig5" ref-type="fig">Figure 5A</xref> and (<bold>ii</bold>) summarizing each learning rate over the population of rats. Each dot is an individual rat, bars represent the median, and errorbars are bootstrapped 95% confidence intervals around the median. (<bold>B</bold>) Three example sessions showing the inferred state likelihood on each trial from the example rat shown in <xref rid="fig5" ref-type="fig">Figure 5A</xref>. (<bold>C</bold>) Cross correlation between left choices and reward probabilities for the common outcome port given that choice (gray). Left choices are highly correlated to left-outcome reward blocks, with the peak correlation at a slight lag (vertical dashed line) indicating the trial at which the rat detects the reward probability flip. To test whether the latent states track reward flips, the cross correlation is also shown between left-outcome reward probability and the likelihood of each state: initial state (blue), the remaining state with a more rightward choice bias (orange), and the remaining state with a more leftward bias (green). These correspond directly to states 1-3 in the example rat (<bold>i</bold>) whose model is shown in <xref rid="fig5" ref-type="fig">Figure 5A</xref>. while other rats had states 2 and 3 assigned according to their individual choice biases.</p>
<p><bold><xref rid="fig5_2" ref-type="fig">Figure 5—figure supplement 2</xref></bold>. A second 3-state MoA-HMM example and comparison to a GLM-HMM. States identified by a MoA-HMM and GLM-HMM are highly similar. <bold>A</bold> Example 3-state MoA-HMM model parameters with (<bold>i</bold>) agent weights split by state, (<bold>ii</bold>) each agent’s learning rate, the initial state probability, and (<bold>iv</bold>) the state transition matrix. <bold>B</bold> 3-state GLM-HMM fit to the same rat as (A). (<bold>i</bold>) GLM-HMM regression weights for each state. Each state is described by four types of regressors indicating four possible trial types – common-reward, common-omission, rare-reward and rare-omission – and choice direction for up to 5 previous trials, giving 20 parameters per state. Each state additionally had a bias term (<bold>ii</bold>), leading to a total of 63 model weights for a 3-state model. (<bold>iii</bold>) GLM-HMM initial state probabilities also identify a prominent initial state. (<bold>iv</bold>) GLM-HMM transition matrix closely matches MoA-HMM. <bold>C</bold> Expected state probabilities for MoA-HMM (<bold>i</bold>) averaged across all sessions and (<bold>ii</bold>) highlighted example sessions. <bold>D</bold> Expected state probabilities for GLM-HMM (<bold>i</bold>) averaged across all sessions and (<bold>ii</bold>) highlighted example sessions. Temporal structure is highly similar to MoA-HMM. <bold>E</bold> Cross-correlation between the expected state probabilities inferred from the MoA-HMM and GLM-HMM (i.e. panels Cii and Dii) across all sessions. Each dot is an individual rat, black circles are medians and error bars are 95% confidence intervals around the median.</p>
<p><bold><xref rid="fig5_3" ref-type="fig">Figure 5—figure supplement 3</xref></bold>. Two example 4-state MoA-HMM fits corresponding to 3 state fits from (<bold>A</bold>) <xref rid="fig5" ref-type="fig">Figure 5A</xref> and (<bold>B</bold>) <xref rid="fig5" ref-type="fig">Figure 5</xref>-figure supplement 2. States are ordered according to the initial state probability (Aii and Bii) and the transition probabilities to most-likely states that follow (Aiii and Biii). Initial states are generally consistent with the 3-state fits, and the way the remaining two states split into three states is more idiosyncratic. For example, (<bold>A</bold>) suggests state 3 from the smaller model is split into two states (iv) that differ by bias (i), while (<bold>B</bold>) suggest the additional state 4 draws from both the smaller model’s states 2 and 3 (iv), and the state with largest MBr state no longer directly follows the initial state (i).</p></caption>
<graphic xlink:href="582617v2_fig5.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p>First, the initial state probability <bold>(ii)</bold> reveals that a single state dominates at the beginning of the session (blue diamond), which we’ve used to label the first state (blue). Comparing the agent weights (<bold>i</bold>) during this initial state to the remaining states, we notice that one weight in particular, MBc, is more negative than in the remaining two states. Recall that a negative weight for MBc captures a preference to visit outcome states not recently visited, which we identified with MB exploration. That this arises most strongly at the beginning of the session is consistent with this interpretation, since the reward location is initially unknown. After the initial reward location is identified, exploratory behavior is less beneficial; with only two outcome states, there is only one alternate outcome once the other becomes less rewarding. This is corroborated by the decreasing magnitude/increasing value of MBc in the later two states.</p>
<p>Since we want to identify changes in planning behavior, we selected the second state (orange) based on having the highest MBr weight (orange diamond) among the remaining states. In our example rat, this state also has a higher MBr weight than the initial state, and it is most likely to follow the initial state in the transition matrix (<bold>iii,iv</bold>). Together with the reduced weight on MBc, this suggests a shift toward increased MB exploitation after an initial exploratory phase.</p>
<p>The remaining state is labeled green, and by definition, has a lower MBr weight than state two (green diamond). Additionally, this shows the most positive weight on MBc as well as the most positive weight on MFc, suggesting a stronger influence of perseveration (in both actions and outcomes) during this state. This state tends to occur towards the end of sessions. It is possible that later in the session, the rat is sated on water and is less motivated by reward-seeking. This is accompanied by increase in less cognitively-demanding strategies such as perseveration, a pattern captured by the reduced magnitude on MBr and more positive weights on MBc and MFc.</p>
<p>Many of these properties hold across our population of rats (<bold><xref rid="fig5" ref-type="fig">Figure 5B</xref></bold>), coloring the states according to the criteria described above. One state (<bold>ii</bold>) tends to occur with high probability initially with, on average, the most negative weight on MBc (<bold>i</bold>, median of -0.74 in state 1; vs 0.04 in state 2, p&lt;0.001 SR; vs 0.32 in state 3, p&lt;0.001 SR across rats). The second state, again selected by having the larger MBr weight of the remaining two, follows the initial state on average (<bold>iii</bold>, state 1 to 2 transition median: 0.037; vs state 1 to 3: 0.001, p=1E-4 SR). Alongside the increase in MBc, the second state shows a significant increase in MBr from the first state (state 1 median: 0.78; vs state 2: 1.47, p&lt;1E-5 SR). The remaining state, defined by a decrease in MBr, is also on average most active at the end of the session (<bold>iv</bold>). It sees the most-positive MBc (median of 0.32; vs 0.04 in state 2, p=0.03 SR) and MFc (median of 0.35; vs 0.15 in state 1, p&lt;1E-4 SR), although MFc is not significantly greater than state 2 (median of 0.25, p=0.35 SR). Together with the lower MBr and higher MBc, however, the relative increase in MFc from the beginning of the session is consistent with the idea of reduced engagement and motivation from reward.</p>
<p>Contrary to our initial expectation, we did not see any significant changes in MFr between states across our population of rats (state 1 median: 0.10; state 2 median: 0.06; state 3 median: 0.07). This further supports that rats are not substantially employing a MFr-like strategy during this version of the task, even during smaller subsets of trials. Thus, these data reveal no evidence of a tradeoff between MBr and MFr strategies in this setting, though they do suggest one between alternation/perseveration strategies in action vs outcome space (i.e., MBc vs. MFc).</p>
<p>To verify that these state dynamics are not an artifact of the particular agents as we have selected, we compared these results to states fit by a GLM-HMM using the same more generic, model-agnostic trial-history regressors as in <xref rid="fig1" ref-type="fig">Figure 1</xref> (<xref rid="fig5_2" ref-type="fig">Figure 5 – figure supplement 2</xref>). Strikingly, the hidden state dynamics (i.e. initial state and transition matrix) are extraordinarily similar, and the expected state likelihoods from both models are highly correlated. The hidden states themselves reveal the same effects by reward and transition across both models; however, the MoA-HMM provides a more streamlined description of this behavior with far fewer parameters and more interpretable learning rules.</p>
</sec>
<sec id="s2e">
<title>Strategy shifts significantly predict changes in behavioral and neural metrics</title>
<sec id="s2e1">
<title>Changes in strategy correlate to changes in response time</title>
<p>So far we have suggested an interpretation that might explain the progression of states we see across rats: an initial exploration state, a middle exploitation state, and an ending state with reduced engagement. To investigate this interpretation, we next looked to how the states relate, out of sample, to other measurements. If the relative influence of learning rules is changing between states, signifying a shift in strategy, we reasoned that the response times may also change as different strategies may require different amounts of computation. For example, it is often thought that more model-based strategies elicit slower response times than model-free strategies, as model-based strategies are more computationally intensive.</p>
<p>Our response time of interest is when the animal is most likely to engage in the learning rule updates we’re modeling; during the rat two-step task, this likely occurs during the inter-trial interval (ITI) period following outcome receipt and prior to initiating the next trial (<bold><italic><xref ref-type="bibr" rid="c41">Miller et al., 2022</xref></italic></bold>) (<bold><xref rid="fig6" ref-type="fig">Figure6i</xref></bold>). To understand whether latent state is related to ITI duration, we must also account for other variables that may also affect this response time. Thus, we account for the presence of reward due to time spent drinking. Additionally, since the progression of states is correlated with time within the session, we wish to ensure that the latent state per se is informative beyond any effects on response time that can simply be explained by the passage of time. To investigate this, we first transformed ITI duration to ITI rate (<bold><xref rid="fig6" ref-type="fig">Figure 6ii</xref></bold>) to constrain long ITIs and reveal a more linear relationship between ITI duration and time. We then fit a linear multilevel model to each rat with state identity, reward, and linear and nonlinear effects of time via a third degree polynomial (<italic>time</italic> + <italic>time</italic><sup>2</sup> + <italic>time</italic><sup>3</sup>, together referred to as “time”) as predictors, as well as interactions between reward and both state and time, while capturing variance in all predictors due to session-level random effects.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Changes in inter-trial interval (ITI) duration predicted by latent state</title>
<p><bold>A</bold> Task diagram of response time of interest: ITI duration. (<bold>i</bold>) The ITI duration is measured as the difference in time (Δ<italic>time</italic>) between outcome port entry during step 2 of trial <italic>t</italic> and initiation port entry during step one of the next trial <italic>t</italic> + 1. (<bold>ii</bold>) Due to a large right tail from the longest ITIs, we instead model the inverse of ITI duration, or ITI rate, in our multilevel linear regression model. <bold>B</bold> Example prediction from a multilevel linear regression with fixed effects of reward, time, state, time × reward, and state × reward, and random effects from session on every predictor. (<bold>i, top</bold>) Scatter plot of ITI rate by time in session (in minutes) within an example session, colored by whether the ITI followed a reward (gray) or omission (gold). Overlaid lines are the model-predicted rate split for reward (dashed, black) and omission (solid, dark gold) trials. Discrete shifts in ITI rate align with shifts in inferred state (background color), with the highest rate during state 2. (<bold>i, bottom</bold>) Components of the prediction in the top panel due to time predictors (blue) and state predictors (red) split for reward (dashed) and omission (solid) trials. The state component, and its interaction with reward, capture the discrete shift observed in the top panel. <bold>ii</bold> Grouped dot plot of ITI rate split by inferred state, after subtracting changes in rate due to time (time component from i, bottom panel). State 2 shows a significantly higher rate than state 3 for ITIs following both rewarded and omission trials. <bold>C</bold> Coefficient of partial determination (CPD, or partial R-squared) for both time and state decouples the variance explained by each group of predictors. Time predicts significantly more than state, but state still significantly increases the variance explained by the model. **:p&lt;0.01, ***:p&lt;1E-4</p></caption>
<graphic xlink:href="582617v2_fig6.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold> shows an example regression predicted on one of the sessions. Strikingly discrete shifts in response times are visible, well aligned to the inferred state (<bold><xref rid="fig6" ref-type="fig">Figure 6Bi</xref></bold> top panel) and captured by the predictive component due to state identity (<bold><xref rid="fig6" ref-type="fig">Figure 6Bi</xref></bold> bottom panel). After subtracting out drifts in ITI rate due to time, we find significant differences in ITI rate split by inferred state (<bold><xref rid="fig6" ref-type="fig">Figure 6Bii</xref></bold>) in this example session, where state 2 sees a significantly higher rate (or shorter ITI duration) than state 3 following both rewarded and omission trials. This effect holds across rats, where fitting a multilevel model to each rat reveals a significant increase in ITI rate following omission (0.026 ITIs/s, p&lt;0.001 SR) and reward (0.006 ITIs/s, p=0.008 SR), where the effect after omission is significantly greater (p&lt;0.001 SR). Since state 2 has the highest MBr weight, this may seem to contradict the expectation that MB strategies have slower response times than MF strategies. However, these states to not reflect a trade-off between MB and MF strategies; the fastest response time here is associated with the exploitative state, while slower response times are associated with the exploration and reduced engagement states. To formally disentangle the separable contributions of time and state, we use the coefficient of partial determination (CPD, or partial R-squared) to measure the explanatory power of each predictor. We find that while time contributes more to the model (median CPD=12%, p&lt;1E-5 SR, perhaps not surprisingly because it is, conservatively, modeled with a very flexible third-order form), the state identity still significantly explains an additional 2% of variance on average (median, p&lt;1E-5 SR) (<bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold>).</p>
<p>Hidden states capture additional variance of neural encoding in orbitofronal cortex As a second test that the states inferred from the fit of the model to choice behavior reflect meaningful computations, we investigated whether their dynamics correlate with changes in neural encoding. As a high-level test of this possibility, we apply the MoA-HMM to four animals that performed the two-step task while simultaneously having activity recorded in orbitofrontal cortex (OFC), data first published by <bold><italic><xref ref-type="bibr" rid="c41">Miller et al. (2022</xref></italic></bold>). Using a single-state MoA model to estimate action and outcome values, <bold><italic>Miller et al</italic></bold>. found that OFC neurons primarily encoded the value of the most recently experienced outcome (or expected outcome value), used for learning about action values on subsequent trials, during the ITI. Additionally, significant modulation to reward was observed in OFC neurons. Both reward and expected outcome value are relevant for learning between trials; therefore, our next analyses will focus on whether state modulates the encoding of these variables during the ITI.</p>
<p><bold><xref rid="fig7" ref-type="fig">Figure 7A</xref></bold> looks first at OFC modulation to expected outcome value. For more direct comparison to results from <bold><italic>Miller et al</italic></bold>., and since MBr is the largest contributor to behavior, expected outcome value is defined here as the value of the experienced outcome port predicted solely by the MBr agent prior to value updating following reward receipt. To look at a single unit’s response to expected outcome value, we binned trials by outcome value into terciles of outcome value. <bold><xref rid="fig7" ref-type="fig">Figure 7A</xref></bold> shows an example unit especially responsive to outcome value. Splitting the average firing rate by the inferred latent state reveals the highest modulation to outcome value during the second state (middle panel). To test the prevalence of this effect across all units, we measured the population CPDs of relevant behavioral variables in a Poisson regression model predicting the spike counts of each neuron computed separately for trials associated with each state. Specifically, our model included main effects of choice, outcome, transition (or interaction between choice and outcome), reward, next choice, expected outcome value, and next chosen value (see Methods for additional detail). Looking at the CPD of expected outcome value split by state (<bold><xref rid="fig7" ref-type="fig">Figure 7B</xref></bold>) reveals that the trend from the example neuron is consistent across the population of OFC units, where state 2 shows the greatest CPD. This is consistent with the idea that the second state generally has the highest weight on, and therefore the strongest influence by, the MBr agent.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Orbitofrontal cortex (OFC) neural encoding modulated by state.</title>
<p><bold>A</bold> Modulation of model-based (MBr) outcome value encoding by state. (<bold>i</bold>) PSTHs of an example OFC unit time-locked to outcome for each state, where outcome time is measured as the time at which the rat pokes the second-step outcome port. To differentiate between levels of expected outcome value, trials were split into terciles by value. Shaded areas are 95% confidence intervals around the mean. The greatest modulation to outcome value is seen in state 2. (<bold>ii</bold>) Average firing rate within the outcome window (defined from 1 second before until 3 seconds after outcome port entry) for each trial with a high expected outcome value throughout the session. Background shading corresponds to inferred state. Shifts in response magnitude align with shifts in inferred state. <bold>B</bold> Population CPD of expected outcome value throughout a trial, centered around the ITI, computed for each state. Dots above each panel correspond to time points where the true CPD was greater than 95% of CPDs computed from circularly permuted datasets. Similar to the example unit in (i), state 2 contains the greatest modulation by outcome value, and no significant outcome value encoding is measured in state 3. <bold>C</bold> Modulation of reward/omission encoding by state. (<bold>i</bold>) PSTHs of an example OFC unit time-locked to outcome for each state. To differentiate between responses to rewards and omissions, each PSTH is split by trials following omissions (dashed) and trials following rewards (solid). Shaded areas correspond to 95% confidence intervals around the mean. The first unit contains the highest response to reward in state 1 and the lowest in state 3. (<bold>ii</bold>) Average firing rate within the outcome window (defined from 1 second before and 3 seconds after outcome port entry) for each rewarded trial throughout the session. Background shading corresponds to inferred state. Shifts in response magnitude align with shifts in inferred state above a slow negative drift across the session. <bold>D</bold> The CPD of reward across all units throughout a trial, centered around the ITI, computed for each state. Similar to the example unit in (i), state 1 contains the greatest modulation by reward.</p>
<p><bold><xref rid="fig7_1" ref-type="fig">Figure 7—figure supplement 1</xref></bold>. Population CPD computed around the inter-trial interval (ITI) reveals significant encoding of state (red) even after accounting for time (blue). CPDs were measured as the median CPD across all units, with errorbars corresponding to bootstrapped 95% confidence intervals.</p>
<p><bold><xref rid="fig7_2" ref-type="fig">Figure 7—figure supplement 2</xref></bold>. Example and summary 3-state MoA-HMM fits for rats with electrophysiological recordings in OFC.</p></caption>
<graphic xlink:href="582617v2_fig7.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<p><bold><xref rid="fig7" ref-type="fig">Figure 7C</xref></bold> looks next at OFC modulation to reward or omission. A second example unit (<bold>i</bold>) shows differentiable firing response to these events. Splitting response by state reveals that the contrast in firing rate between reward and omissions is highest during the first state (blue) and nearly absent during the third state (green). Similarly across the population using the same regression model as above, we see the greatest reward CPD during state 1 (<bold><xref rid="fig7" ref-type="fig">Figure 7D</xref></bold>). This progression may reflect a change in reward modulation coincident with motivation and thirst/satiety. The animal will be thirstiest at the beginning of a session, and therefore most motivated by reward; the modulation of neural encoding by reward, then, follows accordingly.</p>
<p>Suggestively, the modulation of the reward effect can also be seen between states 2 and 3 – state 2, on average, sees a higher modulation to reward that lasts significantly longer than modulation in state 3. Although state 2 generally occurred in the middle of the session for the behavioral rats, states 2 and 3 trade off more competitively in the model fits of the rats with physiological recordings (<xref rid="fig7_2" ref-type="fig">Figure 7 – figure supplement 2</xref>), making it less likely that the difference between these two states is simply due to further passage of time. Indeed, the example unit in <xref rid="fig7" ref-type="fig">Figure 7Bii</xref> is a session that ends in state 2, coinciding with an uptick in response to reward. This suggests that reward response magnitude may be further modulated by how model-based or how engaged the animal is in the task, as captured by the choice model and dissociable from the simple passage of time.</p>
<p>Nevertheless, we wished to formally investigate the that state effects captured in <xref rid="fig7" ref-type="fig">Figure 7A-B</xref> are not simply due to the passage of time. To dissociate significant effects of these correlated factors, time and state, we similarly constructed a Poisson regression model containing main effects of behavioral variables – choice, outcome, transition, reward, next choice, expected outcome value, and next chosen value – alongside state and time. Notably, the model also included interaction terms between each behavioral variable with state and time, as well as additional interactions with reward, as a more thorough representation of possible effects being encoded by the population. Each variable’s CPD, then, was computed by removing the main effect and all interaction terms containing that variable (see Methods for a full specification of factors).</p>
<p><bold><xref rid="fig7_1" ref-type="fig">Figure 7 – figure supplement 1</xref></bold> highlights the average CPD of state (red) and time (blue) across all units. Similar to the CPDs computed for ITI rate, time explains a greater proportion of variance than state. However, we still observe a significant CPD for state, suggesting that state identity explains additional variance not captured by (even rather elaborate nonlinear efects of) time alone. Hidden states detected by the MoA-HMM, therefore, seem to provide useful boundaries to epoch electrophysiological data, opening a door for further analysis and contrast between identified behavioral states.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Different regions and subcircuits of the brain are thought to subserve different decision strategies — ways to learn about or evaluate candidate actions. Thus, deciding what to do may involve weighting information from multiple such systems, where the relative contribution of certain learning rules can vary based on internal motivations and/or external environmental factors. Dynamics of this sort are often thought to explain phenomena such as the formation of habits with overtraining and a shift between exploratory and exploitative choice modes. However, MoA models traditionally used descriptively to measure the influence of separable learning rules, especially in RL, are unable actually to capture such dynamics, but instead characterize all choices as arising from an IID mixture of agents. In addition to failing to capture key phenomena like habit formation endogenously in the models, this limitation may mask interesting, lesser-used strategies and limit interpretation of underlying neural correlates. Conversely, another family of models (the GLM-HMM) that attempts to account for time-dependent changes in behavior model only observable environmental factors, and therefore cannot capture changes in terms of more abstract learning rules thought to be used by the brain. Here we introduce the MoA-HMM, combining mixture modeling of abstract learning rules with temporal dynamics that can capture discrete changes in these mixtures throughout behavior. We apply this model to rats performing a multi-step, reward-learning task and examine the contribution of various reinforcement learning rules. We successfully capture shifts in strategy corresponding to differing contributions of the included RL rules, reflecting a within-session progression between exploration, exploitation, and reduced engagement in the task. Additionally, these shifts in strategies significantly predict variables not used to fit the model; e.g., response time is fastest during the exploitation state, even after accounting for drifts due to time. These shifts also capture changes in neural encoding, notably showing the highest response to model-based value during exploitation.</p>
<p>The two-step task has been historically used to measure the trade-off between MB and MF reward learning strategies as a proxy for the trade-off between goal-directed and habitual behavior. The rat version of the task used here, however, has not shown any significant involvement of MF reward learning in choice behavior. Accordingly, studies using this version of the task have primarily focused on its application to studying goal-directed behavior via MB planning (<bold><italic><xref ref-type="bibr" rid="c40">Miller et al., 2017</xref>, <xref ref-type="bibr" rid="c41">2022</xref></italic></bold>). The dynamic shifts in strategy uncovered here, however, suggest that behavior still may reflect a trade-off between goal-directed (outcome-state focused) and habitual (choice-focused) strategies, just not via the traditional MB/MF distinction. Instead, if we identify habitual behavior by choice-level perseveration as proposed by <bold><italic><xref ref-type="bibr" rid="c44">Miller et al. (2019</xref></italic></bold>), then our results show emergence of habitual control toward the end of sessions with a rise in MFc. Interestingly, the rise of MFc isn’t coupled with an opposing decay of MBr (i.e., goal-directed exploitation). Instead, we see increasing MFc accompanied by increasing (i.e., less negative) MBc, which likely reflects a decline in goal-directed exploration. This pattern further suggests that the goals vs. habits trade-off is not simply mapped to MBr/MFr as traditionally modeled. Instead, goal-directed contributions to behavior further subdivide into distinct MB exploratory and exploitative behaviors.</p>
<p>The nature of the MB/MF trade-off could be further explored by applying the MoA-HMM to other versions of the two-step task that do demonstrate significant contributions of of MBr and MFr agents in the single-state MoA model (<bold><italic><xref ref-type="bibr" rid="c15">Daw et al., 2011</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c24">Gillan et al., 2016</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c29">Hasz and Redish, 2018</xref></italic></bold>; <bold><italic>Groman et al., 2019b</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c2">Akam et al., 2021</xref></italic></bold>). This may uncover new properties of strategy switching when both MBr and MFr learning rules explain significant variance, especially in humans. One practical issue, and opportunity for future research, is that human datasets contain many fewer choices per subject than the rat dataset studied here. Thus to address these data, it would likely be necessary to develop a fully hierarchical MoA-HMM to pool data effectively over subjects.</p>
<p>Patterns of switching, in these and other cases, may also shed light on controversy about the interpretation of behaviors explained by mixtures of MBr and MFr agents in this task (<bold><italic><xref ref-type="bibr" rid="c1">Akam et al., 2015</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c49">Russek et al., 2017</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c22">Feher da Silva and Hare, 2020</xref></italic></bold>), e.g., whether they really reflect two separable contributions vs. some third hybrid, or an altogether different switching strategy. Indeed, it has been suggested that seemingly MB rat behavior during various rodent versions of the two-step task could instead reflect MF belief updates that switch between two latent states, one corresponding to each rewarded side. Such switching could be behaviorally indistinguishable from MBr in single-state MoAs for reasons similar to the example we show in <xref rid="fig3" ref-type="fig">Figure 3C</xref> (<bold><italic><xref ref-type="bibr" rid="c1">Akam et al., 2015</xref></italic></bold>). The MoA-HMM might instead be expected to capture this pattern via two strategies with opposing biases that track reward block flips. However, this is not what we observe in the animals. Even when aligning states 2 and 3 by signs in the bias, state dynamics do not follow reward block shifts (<xref rid="fig5_1" ref-type="fig">Figure 5 – figure supplement 1</xref>). While this does not necessarily rule out the interpretation of MF behavior put forward by <bold><italic><xref ref-type="bibr" rid="c1">Akam et al. (2015</xref></italic></bold>), the emergent role of MBc describing changes in MB exploration throughout sessions provides additional evidence that the animals are MB in the sense of using transition information to inform decisions.</p>
<p>The emergence of putative exploratory and exploitative states presents a new opportunity for using this task to study the explore-exploit trade-off. First, the predominance of the MBc agent at the start of each session strongly suggests an interpretation for the function of the previously described “novelty preference” agent in directed exploration. In this sense, MBc (with a negative mixture coefficient) is analogous to including an “information bonus” for directed exploration, similar to models used to describe exploratory behavior previously (<bold><italic><xref ref-type="bibr" rid="c55">Wilson et al., 2014</xref>, 2021</italic></bold>). Such a bonus, in turn, captures the qualitative features of more elaborate, exact decision-theoretic analyses of of the value of exploration (<bold><italic><xref ref-type="bibr" rid="c25">Gittins, 1979</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c18">Dayan and Daw, 2008</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c5">Averbeck, 2015</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c14">Costa et al., 2019</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c30">Hogeveen et al., 2022</xref></italic></bold>). In a different way, our results echo another recent study (<bold><italic><xref ref-type="bibr" rid="c20">Ebitz et al., 2018</xref></italic></bold>) which argued that primate behavior and neural responses were well-described by a (more purpose-built) HMM that switched between discrete exploratory and exploitative choice regimes.</p>
<p>Importantly, unlike single-step “bandit” tasks used in these and other studies of exploration, the two-step task is sequential. This means that effective exploration requires not simply choosing actions about which one is directly uncertain (as with MFc in the present task) but instead choosing actions that <italic>lead to future states</italic> (here, outcome ports) whose rewards are uncertain; this is precisely what distinguishes MBc as model-based. In this sense, the two-step task allows for — and our result of negative MBc but no negative MFc shows to our knowledge the first evidence in animal or human behavior for — what is known in RL as “deep exploration” (<bold><italic><xref ref-type="bibr" rid="c53">Wilson et al., 2020</xref></italic></bold>), i.e., behavior targeted at reaching future states for the purposes of exploring them.</p>
<p>Following the GLM-HMM, HMMs have been utilized recently in reinforcement learning tasks to estimate strategy switching (<bold><italic><xref ref-type="bibr" rid="c36">Le et al., 2023</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c38">Li et al., 2024</xref></italic></bold>). However, these approaches are notably different than the MoA-HMM. Specifically, the blockHMM designed by <bold><italic>Le et al</italic></bold>. captures differences in choice-switching behavior during a reward reversal task, and is therefore limited in application against tasks that do not contain reward reversals. Furthermore, the MoA-HMM uses different cognitive models to directly infer strategy switching, where <bold><italic>Le et al</italic></bold>. decodes underlying algorithms post hoc to states identified by the blockHMM, where decoding is limited to mixtures of only two algorithms (model-free and inference-based learning) relevant to reward reversal tasks. Conversely, dynamic noise estimation by <bold><italic>Li et al</italic></bold>. extends more broadly to various tasks and cognitive models. However, this method only assumes two behavioral states, engaged or disengaged, in order to improve modeling of the “engaged” state by separating out noise from the “disengaged” state. On the other hand, the MoA-HMM can extend to an arbitrary number of states and does not assume the existence of a “disengaged” state. Furthermore, instead of attributing strategy switching to “noisy” deviations from a single cognitive model, the MoA-HMM can instead identify competing cognitive models that may underlie strategy switches.</p>
<p>As a descriptive model, the MoA-HMM assumes discrete rather than continuous strategy shifts. It is possible that our results reflect a discrete approximation to a more continuous process. Our goal is to provide a quantitative description of significant changes to behavior, which may still offer a useful window into continuously changing strategies. A way to more explicitly assess discrete vs. continuous dynamics is to compare the fit of the MoA-HMM to that of a behavioral model with continuously drifting latent states, such as Psytrack (<bold><italic><xref ref-type="bibr" rid="c48">Roy et al., 2021</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c3">Ashwood et al., 2020</xref></italic></bold>), which could be adapted (much like GLM-HMM was extended to MoA-HMM) to infer continuous shifts in learning rule mixture weights.</p>
<p>The appearance of switching dynamics might also arise if there is a single underlying agent whose continuous dynamics (e.g. algebraic value learning rules) do not match the MoA agents. In this case, HMM states might capture residual variance due to this mismatch. In this case also, the current model would still be useful descriptively (e.g., to help design better underlying agents that endogenize the dynamics described by the HMM). Anecdotally, we were not able to capture switching in the present dataset by exploring agent variants, and at least some of the dynamics we observe (e.g. motivational shifts) seem unlikely to reflect learning per se. Nevertheless, an approach to address this possibility in future work would be to pair the MoA-HMM with more flexible agent models that seek to infer the continuous learning rules being used (<bold><italic><xref ref-type="bibr" rid="c21">Eckstein and Collins, 2021</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c43">Miller et al., 2023</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c32">Ji-An et al., 2023</xref></italic></bold>; <bold><italic><xref ref-type="bibr" rid="c13">Correa et al., 2024</xref></italic></bold>).</p>
<p>Finally, there are many promising applications of the MoA-HMM that have yet to be explored. For example, to test whether separable circuits support different strategies, temporally-specific perturbations targeting these circuits could test for isolated effects on the agent or agents describing said strategy. Additionally, finding a significant relationship between MoA-HMM states with behavioral response times and neural encoding alone cannot causally link the observed effects with inferred states. However, it does suggest that the inclusion of these additional metrics may provide a richer description of changing strategy that modeling choice alone cannot capture. Indeed, models combining both behavioral and neural data have found success in capturing behavioral and neural patterns that could not be captures by one modality alone (<bold><italic><xref ref-type="bibr" rid="c51">Shahar et al., 2019</xref></italic></bold>; <bold><italic>De- Pasquale et al., 2022</italic></bold>; <bold><italic><xref ref-type="bibr" rid="c39">Luo et al., 2023</xref></italic></bold>). Furthermore, while the MoA-HMM has been applied here in the context of RL, the framework can be applied more generally to arbitrary learning rules and tasks, and would apply particular naturally to rule-switching or task-switching settings.</p>
</sec>
<sec id="s4">
<title>Methods and Materials</title>
<sec id="s4a">
<title>Subjects and Behavior</title>
<p>The subjects used in this study were drawn from two published data sets. For behavioral analyses, we used 20 rats first published by <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>); for analyses paired with electrophysiological recordings, we used 4 rats first published by <bold><italic><xref ref-type="bibr" rid="c41">Miller et al. (2022</xref></italic></bold>). All subjects from both sources underwent the same training procedure. Briefly, rats were water deprived and trained daily on the two-step task to gain their daily allotment of water (<xref rid="fig1" ref-type="fig">Figure 1A</xref>). Behavioral training took place in custom behavioral chambers (<xref rid="fig1" ref-type="fig">Figure 1A</xref>, right inset), and each behavioral session lasted 2-3 hours during which the animal completed hundreds of trials. Before a rat would train specifically in the two-step task, they would undergo multiple training phases to 1) acclimate them to receiving water reward from successfully poking into the back-lit outcome port, 2) train them on the transition structure of the task via fully-instructed trials (which is fixed within rat and counterbalanced across rats), and 3) introduce free choice trials and reward reversals, where reversals only occurred after a performance threshold was reached. Within this stage, the reward reversal probabilities were walked down from 100%/0% to the final 80%/20% probability set. Once the rat was consistently triggering multiple reversals per session, rats were advanced to the full version of the task where reward reversals could trigger regardless of the performance of the animal. Notably, some rats still have a small percentage of forced-choice trials during the full task (up to 20%) to reduce strong side biases. For further details on the training pipeline, see <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>).</p>
</sec>
<sec id="s4b">
<title>Mixture-of-Agents Model</title>
<p>The mixture-of-agents model used here was taken and extended from <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>). This model describes behavior as arising from a mixture of multiple agents, <italic>A</italic> ∈ <italic>{</italic>agents<italic>}</italic>, each with their own method to calculate an action value <italic>Q</italic><sub><italic>A</italic></sub>(<italic>y</italic>) for each choice <italic>y</italic>. The MoA uses a weighted sum of each agents’ values to calculate choice likelihood for each <italic>y</italic> according to the softmax function,
<disp-formula id="ueqn1">
<graphic xlink:href="582617v2_ueqn1.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>β</italic><sub><italic>A</italic></sub> is the weighting parameter (or inverse temperature) on agent <italic>A</italic>. Importantly, forced-choice trials are <italic>included</italic> in updated action values <italic>Q</italic><sub><italic>A</italic></sub>(<italic>y</italic>), since the animal can still observe rewards during these trials; however, forced-choice trials are <italic>excluded</italic> when calculating total choice likelihood to infer the weighting parameters (fitting procedure outlined in Mixture-of-Agents Hidden Markov Model section).</p>
</sec>
<sec id="s4c">
<title>Model Agents</title>
<p>We use five agents in the present manuscript. Four of these agents are separable reinforcement learning rules to capture model-based and model-free influences of of reward and choice, derived from learning rules introduced by <bold><italic>Ito and Doya</italic></bold> (<bold><italic>2009</italic></bold>).</p>
</sec>
<sec id="s4d">
<title>Model-Based Reward Learning</title>
<p>The model-based reward (MBr) agent captures planning as the interaction of reward and transition on choice; however, the task transition probabilities, which are assumed to be known by the animals, have been simplified to definite transitions, such that common transition choices are updated directly from reward experienced at the second step. After each trial <italic>t</italic>, the action values for each <italic>y, Q</italic><sub>MBr</sub>(<italic>y</italic>), are updated according to
<disp-formula id="eqn3">
<graphic xlink:href="582617v2_eqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic><sub><bold>MBr</bold></sub> is a decay rate/learning rate on the influence of previous action value, constrained between [0, 1], and <italic>r</italic><sub><italic>t</italic></sub> is an integer representing the reward experienced on trial <italic>t</italic>, taking a value of [1, −1] for rewards and omissions, respectively. With the transition requirement on <italic>y</italic>, this learning rule will apply the reward value to chosen action (<italic>y</italic> = <italic>y</italic><sub><italic>t</italic></sub>) and decay the non-chosen action (<italic>y</italic> ≠ <italic>y</italic><sub><italic>t</italic></sub>) following common transitions, or apply the reward value to the non-chosen action (<italic>y</italic> ≠ <italic>y</italic><sub><italic>t</italic></sub>) and decay the chosen action (<italic>y</italic> = <italic>y</italic><sub><italic>t</italic></sub>) following rare transitions. Notably, <italic>α</italic><sub>MBr</sub> is not applied as a learning rate on reward <italic>r</italic><sub><italic>t</italic></sub> to reduce interaction with the weighting parameter <italic>β</italic><sub>MBr</sub> during model fitting, which is similarly applied in the following RL agents.</p>
</sec>
<sec id="s4e">
<title>Model-Based Choice Learning</title>
<p>Model-based choice (MBc, also model-based perseveration or outcome perseveration) captures the weighted influence of past transitions on choice, describing a “common-stay, rare-switch” strategy. After each trial <italic>t</italic>, action values <italic>Q</italic><sub>MBc</sub> are updated as
<disp-formula id="ueqn2">
<graphic xlink:href="582617v2_ueqn2.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
 where <italic>α</italic><sub>MBc</sub> is a decay rate, similarly constrained between [0, 1], which is unique from the decay rate on the previous agent, MBr. With a similar transition requirement on choice, this agent will always add 1 to chosen actions (<italic>y</italic> = <italic>y</italic><sub><italic>t</italic></sub>) following common transitions or non-chosen actions (<italic>y</italic> ≠ <italic>y</italic><sub><italic>t</italic></sub>) following rare transitions, regardless of reward value. This amounts to reinforcing experienced outcomes by reinforcing the common transition to that outcome.</p>
</sec>
<sec id="s4f">
<title>Model-Free Reward Learning</title>
<p>Model-free reward learning (MFr) is equivalent to a temporal difference learning rule, specifically TD(1), where the choice values are updated directly from experienced reward at outcome time. Namely, action values <italic>Q</italic><sub>MBr</sub> are updated according to
<disp-formula id="ueqn3">
<graphic xlink:href="582617v2_ueqn3.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic><sub>MFr</sub> is a decay rate constrained between [0, 1], distinct from both model-based decay rates. With transition no longer a contributing factor, this agent will always apply the experienced reward value <italic>r</italic><sub><italic>t</italic></sub> to the chosen action (<italic>y</italic> = <italic>y</italic><sub><italic>t</italic></sub>) and always decay the non-chosen action (<italic>y</italic> ≠ <italic>y</italic><sub><italic>t</italic></sub>) .</p>
</sec>
<sec id="s4g">
<title>Model-Free Choice Learning</title>
<p>The model-free choice agent (MFc, also model-free perseveration or choice perseveration) captures the animal’s tendency to repeat choices on subsequent trials. Specifically, action values <italic>Q</italic><sub>MFc</sub> are updated as
<disp-formula id="ueqn4">
<graphic xlink:href="582617v2_ueqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>α</italic><sub>MFr</sub> is a decay rate, also constrained between [0, 1], unique from the decay rate on MFr. This agent will always reinforce chosen actions (<italic>y</italic> = <italic>y</italic><sub><italic>t</italic></sub>) regardless of outcome or reward.</p>
</sec>
<sec id="s4h">
<title>Bias</title>
<p>The bias (or intercept) agent captures the animal’s overall tendency towards left or right choices, which has a set value <italic>Q</italic><sub>bias</sub> of
<disp-formula id="ueqn5">
<graphic xlink:href="582617v2_ueqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
that stays constant across trials.</p>
<p><xref ref-type="bibr" rid="c40">Miller et al. 2017</xref> Agents (move to supplement?)</p>
<p>The agents used in the original MoA model used for the rat two step task are described in <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>). In <xref rid="fig4_1" ref-type="fig">figure 4–figure supplement 1</xref>, we show that our extended learning rules show an improvement in model likelihood over these original agents. We include a brief description of these four agents below, and additionally a fifth model-free agent used in the original manuscript (but not in the reduced model) that we included for fairness in model comparison.</p>
</sec>
<sec id="s4i">
<title>Model-Based Temporal Difference Learning</title>
<p>Computationally similar to MBr, this agent is equated to a planning strategy, which uses knowledge of the transition structure between actions and outcomes to update action values. Specifically, the value of action <italic>y, Q</italic><sub>MB</sub>(<italic>y</italic>), is updated as the weighted sum:
<disp-formula id="ueqn6">
<graphic xlink:href="582617v2_ueqn6.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>R</italic><sub>MB</sub>(<italic>o</italic>) is the value of the outcome <italic>o</italic>, and <italic>T</italic> (<italic>y, o</italic>) is the transition probability from action <italic>y</italic> to outcome <italic>o</italic>, which is assumed to be known. The value of outcome <italic>o</italic> is updated using a TD learning rule as follows:
<disp-formula id="ueqn7">
<graphic xlink:href="582617v2_ueqn7.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
<italic>r</italic><sub><italic>t</italic></sub> is a constant indicating reward observed on trial <italic>t</italic> which takes the value [−1, 1] for omission and rewarded trials, respectively, and <italic>α</italic><sub>MB</sub> is the learning rate which takes a value between 0 and 1. Reward value is used to learn about the experienced outcome <italic>o</italic> = <italic>o</italic><sub><italic>t</italic></sub>, and the non-experienced outcome <italic>o</italic> ≠ <italic>o</italic><sub><italic>t</italic></sub> is decayed.</p>
</sec>
<sec id="s4j">
<title>Novelty Preference</title>
<p>Using the same terminology and definition as <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>), this agent captures the tendency to stay following a rare transition or switch following an common transition. The chosen action value <italic>Q</italic><sub>NP</sub>(<italic>y</italic><sub><italic>t</italic></sub>) and non-chosen action value <italic>Q</italic><sub>NP</sub>(<italic>y</italic> ≠ <italic>y</italic><sub><italic>t</italic></sub>) are updated as:
<disp-formula id="ueqn8">
<graphic xlink:href="582617v2_ueqn8.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>This agent is similar to MBc, except that it is defined in the opposite direction (i.e. “rare-stay/common-switch”) and has no learning rate to capture trial history effects.</p>
</sec>
<sec id="s4k">
<title>Model-free Temporal Difference Learning</title>
<p>This agent is nearly identical to MFr, where action values are updates as:
<disp-formula id="ueqn9">
<graphic xlink:href="582617v2_ueqn9.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Unlike MFr, however, the learning rate <italic>α</italic><sub>MF</sub> is applied to reward value <italic>r</italic><sub><italic>t</italic></sub> for consistency in definition with the model-based TD learning agent.</p>
</sec>
<sec id="s4l">
<title>Perseveration</title>
<p>The perseveration agent captures the animal’s tendency to repeat choices on subsequent trials. its value is update according to
<disp-formula id="ueqn10">
<graphic xlink:href="582617v2_ueqn10.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>This agent is similar to MFc, except that is has no learning rate to capture trial-history effects.</p>
</sec>
<sec id="s4m">
<title>Mixture-of-Agents Hidden Markov Model</title>
<p>The introduction of a hidden Markov model allows for the weighted influence of each agent to change discretely over time. Specifically, it assumes that behavior is described by multiple hidden states, each with their own set of agent weights <bold><italic>β</italic></bold><sup><italic>z</italic></sup>, of which one set will win out on any given trial to drive choices. Our MoA choice likelihood is updated, then, to be dependent on latent state such that:
<disp-formula id="eqn4">
<graphic xlink:href="582617v2_eqn4.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where the weighted influence of each agent <inline-formula><inline-graphic xlink:href="582617v2_inline1.gif" mimetype="image" mime-subtype="gif"/></inline-formula> is additionally dependent on latent state. We assume that all latent states use a shared value <italic>Q</italic><sub><italic>A</italic></sub>(<italic>y</italic>) and learning rates <italic>α</italic><sub>MB<italic>r</italic></sub>, <italic>α</italic><sub>MB<italic>c</italic></sub>, <italic>α</italic><sub>MF<italic>r</italic></sub>, and <italic>α</italic><sub>MFc</sub>.</p>
<p>Two additional parameters are introduced from the HMM to describe the temporal dynamics of the hidden states: the initial state probability <bold><italic>π</italic></bold> and the hidden state transition matrix <bold>P</bold>, making the full set of model parameters <italic>θ</italic> = <italic>{</italic><bold><italic>β</italic></bold>, <italic>α</italic><sub>MB<italic>r</italic></sub>, <italic>α</italic><sub>MB<italic>c</italic></sub>, <italic>α</italic><sub>MF<italic>r</italic></sub>, <italic>α</italic><sub>MF<italic>c</italic></sub>, <bold><italic>π</italic>, P</bold><italic>}</italic> where <bold><italic>β</italic></bold> is the full (nAgents x nStates) weight matrix. Details on the inference procedure for each of these parameters follow.</p>
<p>Initial state probability <bold><italic>π</italic></bold> is typically defined as the hidden state probability distribution on the first trial, <italic>p</italic>(<italic>z</italic><sub>1</sub>). However, with multiple sessions with only a single session a day, we treat the first trial within a session as an “initial” trial. Initial state probability <bold><italic>π</italic></bold> is estimated, then, from all trials that initiate a session. As stated previously, forced trials are excluded from choice likelihood estimation. For sessions that start with a forced trial, the first free choice trial is considered the initial trial of that session. We can break up the set of all trials <italic>T</italic> into subsets of trials belonging to each session <italic>s</italic> ∈ <italic>S</italic>, for all sessions <italic>S</italic>, such that <inline-formula><inline-graphic xlink:href="582617v2_inline2.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, where <italic>i</italic> = 1 is the first trial and <italic>i</italic> = <italic>Ts</italic> is the last trial in session <italic>s</italic>; therefore, the subset of all initial trials is denoted <inline-formula><inline-graphic xlink:href="582617v2_inline3.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Elements of <bold><italic>π</italic></bold> are defined as the normalized average probability
<disp-formula id="ueqn11">
<graphic xlink:href="582617v2_ueqn11.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The transition matrix is estimated from all trials excluding those that initiate a session, denoted as <inline-formula><inline-graphic xlink:href="582617v2_inline4.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Elements of <bold>P</bold> are <inline-formula><inline-graphic xlink:href="582617v2_inline5.gif" mimetype="image" mime-subtype="gif"/></inline-formula></p>
<p>We fit the MoA-HMM using expectation maximization (EM), which finds the posterior distribution of the hidden state variables, <italic>p</italic>(<italic>z</italic> | <italic>D, θ</italic><sup><italic>old</italic></sup>), in the expectation (E) step to evaluate and optimize the expected data log-likelihood, <italic>𝓁 𝓁</italic> (<italic>θ, θ</italic><sup><italic>old</italic></sup>) = <sup><italic>∑</italic></sup><italic>z p</italic>(<italic>z</italic> | <italic>D, θ</italic><sup><italic>old</italic></sup>) ln <italic>p</italic>(<italic>D, z</italic> | <italic>θ</italic>), in the maximization (M) step. Implementation closely follows <bold><italic>Bishop</italic></bold> (<bold><italic>2006</italic></bold>), which we briefly describe to highlight differences.</p>
<p>We introduce the following notation: <italic>γ</italic><sub><italic>tj</italic></sub> = <italic>p</italic>(<italic>z</italic><sub><italic>t</italic></sub> = <italic>j</italic> | <italic>D, θ</italic><sup><italic>old</italic></sup>), the marginal posterior probability of hidden state <italic>z</italic><sub><italic>t</italic></sub> = <italic>j</italic> at trial <italic>t</italic>; and <italic>ξ</italic><sub><italic>tij</italic></sub> = <italic>p</italic>(<italic>z</italic><sub><italic>t</italic>−1</sub> = <italic>i, z</italic><sub><italic>t</italic></sub> = <italic>j</italic> | <italic>D, θ</italic><sup><italic>old</italic></sup>), the joint posterior distribution of two successive latent variables, <italic>z</italic><sub><italic>t</italic>−1</sub> = <italic>i</italic> and <italic>z</italic><sub><italic>t</italic></sub> = <italic>j</italic> at trial <italic>t</italic>. With these definitions, our expected data log-likelihood becomes:
<disp-formula id="ueqn12">
<graphic xlink:href="582617v2_ueqn12.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
 where <italic>T</italic> is the total number of trials and <italic>Z</italic> the total number of hidden states. The E step will evaluate <italic>γ</italic> and <italic>ξ</italic> using existing parameter values <italic>θ</italic>; Optimization of <italic>θ</italic> during the M step is then done by treating <italic>γ</italic> and <italic>ξ</italic> as constants.</p>
</sec>
<sec id="s4n">
<title>E Step</title>
<p><italic>γ</italic> and <italic>ξ</italic> are estimated using the forward-backward algorithm as described by <bold><italic>Bishop</italic></bold> (<bold><italic>2006</italic></bold>). Briefly, the forward pass is calculated recursively within sessions from <italic>t</italic> = 2 to <italic>t</italic> = <italic>T</italic><sub><italic>s</italic></sub> as:
<disp-formula id="ueqn13">
<graphic xlink:href="582617v2_ueqn13.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Where <italic>c</italic><sub><italic>t</italic></sub> is a normalization factor such that <inline-formula><inline-graphic xlink:href="582617v2_inline6.gif" mimetype="image" mime-subtype="gif"/></inline-formula>, which also gives the marginal choice likelihood <inline-formula><inline-graphic xlink:href="582617v2_inline7.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. Notably in our case, a is initialized at the beginning of every session as:
<disp-formula id="ueqn14">
<graphic xlink:href="582617v2_ueqn14.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The backward pass is calculated backward-recursively within sessions from <italic>t</italic> = <italic>T</italic><sub><italic>s</italic></sub> − 1 to <italic>t</italic> = 1 as:
<disp-formula id="ueqn15">
<graphic xlink:href="582617v2_ueqn15.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
Where <italic>c</italic><sub><italic>t</italic></sub> is the same normalization factor calculated from the forward pass. Similarly, <italic>b</italic> is “initialized” at the end of each session as <inline-formula><inline-graphic xlink:href="582617v2_inline8.gif" mimetype="image" mime-subtype="gif"/></inline-formula> <italic>γ</italic> is calculated as:
<disp-formula id="eqn5">
<graphic xlink:href="582617v2_eqn5.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Importantly, the value of <italic>γ</italic>(<italic>z</italic><sub><italic>t</italic></sub>) gives us the expected likelihood of state <italic>z</italic> on trial <italic>t</italic>. Finally, <italic>ξ</italic> is calculated as:
<disp-formula id="ueqn16">
<graphic xlink:href="582617v2_ueqn16.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4o">
<title>M Step</title>
<p>Since <bold><italic>π</italic></bold> and <bold>P</bold> do not depend on hidden state <italic>z</italic>, they can be maximized using Lagrange multipliers:
<disp-formula id="ueqn17">
<graphic xlink:href="582617v2_ueqn17.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>A notable difference is the calculation of <bold><italic>π</italic></bold> includes all initial session trials as defined before, while <bold>P</bold> excludes these trials.</p>
<p>Dependence on agent parameters and agent weights <italic>θ</italic><sub><italic>A</italic></sub> = <italic>{</italic><bold><italic>β</italic></bold>, <italic>α</italic><sub>MB<italic>r</italic></sub>, <italic>α</italic><sub>MB<italic>c</italic></sub>, <italic>α</italic><sub>MF<italic>r</italic></sub>, <italic>α</italic><sub>MF<italic>c</italic></sub> <italic>}</italic> are limited to a single term in our data log-likelihood:
<disp-formula id="ueqn18">
<graphic xlink:href="582617v2_ueqn18.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
the negative of which can be optimized using gradient descent. Here we use the L-BFGS algorithm implemented by julia’s Optim package, making use of automatic differentiation to supply the gradient.</p>
<sec id="s4o1">
<title>Fitting Procedure</title>
<p>Parameter values were randomly initialized using the following distributions: the agent weights <bold><italic>β</italic></bold> from a normal distribution <italic>N</italic>(0, 0.1); transition matrix rows from a Dirichlet distribution, where shaping parameters were taken as the corresponding row from a <italic>Z</italic> x <italic>Z</italic> matrix of 1s with 5s along the diagonal; the initial state probability from a Dirichlet distribution with all shaping parameters = 1; the learning rates from Beta distribution with <italic>α</italic> = <italic>β</italic> = 5. In addition, a loose prior on <bold><italic>β</italic></bold>, denoted <italic>p</italic>(<bold><italic>β</italic></bold>), was included to penalize large magnitudes; explicitly, the prior was a normal distribution with mean 0 and standard deviation 10. Including a parameter prior updates the data log-likelihood used in the M Step to
<disp-formula id="ueqn19">
<graphic xlink:href="582617v2_ueqn19.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>p</italic>(<italic>θ</italic>) = <italic>p</italic>(<bold><italic>β</italic></bold>) = Normal(0, 10) with only a prior on <bold><italic>β</italic></bold>. EM was terminated if the difference in the maximum a posteriori (MAP) likelihood, <italic>p</italic>(<italic>θ</italic>|<italic>Y</italic>) ∝ <italic>p</italic>(<italic>Y</italic>)<italic>p</italic>(<italic>θ</italic>), fell below 1 × 10<sup>−5</sup>, where <italic>p</italic>(<italic>Y</italic>) is the marginal choice likelihood computed from the forward pass during the E step, and <italic>p</italic>(<italic>θ</italic>) is the parameter prior, which at this point <italic>p</italic>(<italic>θ</italic>) = <italic>p</italic>(<bold><italic>β</italic></bold>) = Normal(0, 10). Alternatively, if the difference in all parameter estimates on subsequent iterations fell below 1 × 10<sup>−5</sup>, or if the number of iterations exceeded 300, EM would be terminated.</p>
</sec>
<sec id="s4o2">
<title>Estimation of Population Prior</title>
<p>The best model was selected from 20 fits with random initializations for each rat using only a loose prior on the agent weights. The best fits were sorted according to their initial state probability for the first state, and by <italic>β</italic><sub>MBr</sub> magnitude for the remaining two states, and used to estimate prior distributions on each of the model parameters. The model was refit using the updated prior:
<disp-formula id="ueqn20">
<graphic xlink:href="582617v2_ueqn20.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The prior on <italic>β</italic> was a Normal distribution for each state, with mean <inline-formula><inline-graphic xlink:href="582617v2_inline9.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="582617v2_inline10.gif" mimetype="image" mime-subtype="gif"/></inline-formula> for each agent <italic>A</italic> and state <italic>z</italic> approximated from the sorted population parameters.</p>
<p>The prior on <bold><italic>π</italic></bold> and each row of <bold>P</bold> are Dirichlet distributions, where the vector elements of <italic>α</italic><sub>π</sub> and <inline-formula><inline-graphic xlink:href="582617v2_inline11.gif" mimetype="image" mime-subtype="gif"/></inline-formula> are estimated from population parameters using the method of moments:
<disp-formula id="ueqn21">
<graphic xlink:href="582617v2_ueqn21.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
where <italic>j</italic> can be any element of row <italic>P</italic><sub><italic>z</italic></sub>. To ensure all shaping parameters were at least 1, 1 was added to each index of <italic>α</italic> if any index was less than 1, effectively smoothing the prior. To account for the prior, closed form updates to <bold><italic>π</italic></bold> and <bold>P</bold> were updated as:
<disp-formula id="ueqn22">
<graphic xlink:href="582617v2_ueqn22.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The prior on the learning rates was taken as a Beta distribution, with shaping parameters <inline-formula><inline-graphic xlink:href="582617v2_inline12.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582617v2_inline13.gif" mimetype="image" mime-subtype="gif"/></inline-formula> estimated for each RL agent also using the method of moments:
<disp-formula id="ueqn23">
<graphic xlink:href="582617v2_ueqn23.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Similarly as before, if either <inline-formula><inline-graphic xlink:href="582617v2_inline14.gif" mimetype="image" mime-subtype="gif"/></inline-formula> or <inline-formula><inline-graphic xlink:href="582617v2_inline15.gif" mimetype="image" mime-subtype="gif"/></inline-formula> were estimated as a value less than 1, 1 was added to both <inline-formula><inline-graphic xlink:href="582617v2_inline16.gif" mimetype="image" mime-subtype="gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582617v2_inline17.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. To reduce the number of iterations and for further bias towards the prior, making up for the fact that the hierarchical estimates of the prior aren’t being inferred alongside each model, the model was refit for each animal with a single initialization at the population mean.</p>
</sec>
</sec>
<sec id="s4p">
<title>Data simulation and parameter recovery</title>
<p>To test the model’s ability to recover inferred parameter sets, we generated two-step behavioral data from a given MoA-HMM. Given a number of trials evenly split between a number of sessions, reward probabilities were first generated for left and right outcomes using task statistics: on the first trial, a either left or right was randomly chosen and assigned an 80% reward probability, and the opposite side assigned a 20% reward probability. A trial flip counter, initialized to 0 at the start of every session, was used to track the number of trials following a reward probability flip. After the trial flip counter reached 10 trials, a random number was drawn on each trial to determine whether the reward probabilities would flip sides, with flips occurring at a 5% probability. The trial flip counter was reset to 0 following a successful flip, and reward probabilities were generated in this manner throughout the set of simulated trials and sessions.</p>
<p>After generating reward probabilities, choices were generated from probability distribution given by <xref ref-type="disp-formula" rid="eqn4">equation 4</xref> using a given MoA-HMM parameter set. Similarly to model inference, choice values were initialized uniformly at the start of every session. After a choice was drawn, a common or rare transition was randomly chosen based on an 80%/20% transition probability to determine the outcome side, and reward was drawn from the predetermined reward probabilities for the given outcome. The selected choice, outcome, and reward are used to update choice values, and this process is repeated for the specified number of trials and sessions.</p>
<p>The MoA-HMM was then fit to the simulated data set using the model fitting procedure outline above, finding the best model fit out of at least 5 random initializations. In order to match recovered HMM states to the simulated HMM states, we computed the expected state likelihood <italic>γ</italic> (<xref ref-type="disp-formula" rid="eqn5">equation 5</xref>) for each state across the simulated data set for both the simulated model, <italic>γ</italic><sub>sim</sub>, and the recovered model, <italic>γ</italic><sub>fit</sub>, and we paired states based on the maximum correlation between <italic>γ</italic><sub>sim</sub> and <italic>γ</italic><sub>fit</sub>. If multiple states in <italic>γ</italic><sub>fit</sub> were maximally correlated to the same state in <italic>γ</italic><sub>sim</sub>, the state with the highest correlation won out, and the losing state was paired with its next highest correlated state. This process was repeated until all states have unique pairs.</p>
<p>Parameter recovery was performed separately on two sets of models: 1) models with randomly drawn parameter sets, and 2) models inferred from the rats’ behavior. For the first set, model parameters were randomly drawn from user-defined distributions. Specifically, agent weights <italic>β</italic> for all agents were drawn from a Normal distribution with <italic>μ</italic><sub><italic>β</italic></sub> = 0 and <italic>σ</italic><sub><italic>β</italic></sub> = 2. Learning rates <italic>α</italic> for relevant agents were drawn from a Beta distribution with shaping parameters <italic>α</italic><sub><italic>α</italic></sub> = 5 and <italic>β</italic><sub><italic>α</italic></sub> = 5. Initial state probabilities π were drawn from a Dirichlet distribution with shaping parameter <italic>α</italic><sub>π</sub> = [1, 1, 1], and each row of the state transition matrix <italic>P</italic> was also drawn from a Dirichlet distribution using the corresponding row of a shaping parameter matrix with 1 on all off-diagonals and 10 along the diagonal. 200 models were initialized under this framework, and data for each model was generated for 5000 trials evenly split between 20 sessions. Each model was recovered with no priors on any of the parameters except for a loose prior on the agent weights, Normal(0, 10).</p>
<p>Due to the interaction between different model parameters (e.g. a small <italic>β</italic> weight will affect the recoverability of the agent’s learning rate <italic>α</italic>), a number of “failures” can be seen in recovery of the first model group (<xref rid="fig3_1" ref-type="fig">Figure 3 – figure supplement 1</xref>). Instead of finding restrictions on the randomization of model parameters to eliminate failures, we performed parameter recovery on a second group of models defined by those fit to real data (post-prior) to ensure confidence in our ability to recover these models specifically (<xref rid="fig3_2" ref-type="fig">Figure 3 – figure supplement 2</xref>). Data generated by these models were set to match the trial and session statistics of the rat’s behavioral data from which the model was inferred as a further check on the recoverability from the size of the data set. 5 independent data sets were generated from each of the 20 inferred models, giving a total of 100 recovered sets. Similar to the fitting procedure on each rat, the prior distributions computed from the pre-prior model fits were used when fitting the model to each simulated data set.</p>
</sec>
<sec id="s4q">
<title>Cross-validation and model comparisons</title>
<p>Cross-validation was used for all procedures comparing models with different parameter sets. Either 3- or 6-fold cross validation was used, where entire sessions were held out together to preserve within-session temporal structure. Sessions for each fold were selected in stride, such that sessions were evenly spaced throughout the data set (i.e. every third session belonged to the same set in 3-fold CV). The cross-validated likelihood, then, was computed by fitting the model to all data not in the held-out set, and then running the forward pass of the E-step to compute the marginal choice log-likelihood on the held-out set, ln <italic>p</italic>(<italic>Y</italic><sub><italic>set</italic></sub>). After being computed for each fold, each log-likelihood was added and normalized by the number of trials, and exponentiated back into likelihood space, giving the model-fit likelihood:
<disp-formula id="ueqn24">
<graphic xlink:href="582617v2_ueqn24.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
</sec>
<sec id="s4r">
<title>GLM-HMM parameter description</title>
<p>The GLM used in the GLM-HMM version of the model used here was taken from <bold><italic><xref ref-type="bibr" rid="c40">Miller et al. (2017</xref></italic></bold>). It consists of trial-history predictors that indicate what trial type was experienced at multiple delays. Specifically, there is a common-reward predictor that takes a value of 1 if the trial was a left-choice rewarded trial with a common transition, −1 for a right-choice rewarded trial with a common transition, and 0 otherwise. This pattern follows for the remaining three trial types: common-omission, rare-reward, and rare-omission. Each of these four predictors are measured at variable delays, i.e. one trial back up to five trials back; including a bias term, this gives a total of 21 predictors. This model was fit similarly to the MoA-HMM, with each predictor treated as an “agent” with no agent hyperparameters to infer.</p>
</sec>
<sec id="s4s">
<title>Multilevel linear regression of response times</title>
<p>Multilevel linear regression was used to understand the influence of reward, state, and time on ITI duration. ITI duration was first pseudo-linearized by transforming it into a rate, or iti_rate = <inline-formula><inline-graphic xlink:href="582617v2_inline18.gif" mimetype="image" mime-subtype="gif"/></inline-formula>. The relationship between these variables can be summarized by the following formula:
<disp-formula id="ueqn25">
<graphic xlink:href="582617v2_ueqn25.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>“Time” consists of three terms, time = <italic>t</italic> + <italic>t</italic><sup>2</sup> + <italic>t</italic><sup>3</sup>, where <italic>t</italic> is the initiation time of the trial, minmaxed normalized across the session such that 0 corresponds to the beginning of the session and 1 corresponds to the end of the session. Time is described by a third-degree polynomial in this way to capture the possibility of asymmetric, non-linear effects of time throughout the session.</p>
<p>“State” consists of two terms, state = <italic>z</italic><sub>1</sub> + <italic>z</italic><sub>2</sub>, where <italic>z</italic><sub>1</sub> and <italic>z</italic><sub>2</sub> are non-overlapping boolean vectors. <italic>z</italic><sub>1</sub> = 1 indicates state 1 is the most-likely inferred state, and <italic>z</italic><sub>2</sub> = 1 indicates state 2 is the most-likely inferred state. A third boolean vector for state 3 is implicit with the intercept and excluded. In other words, “state” is a one-hot matrix corresponding to hidden state, where the column corresponding to the third state is dropped.</p>
<p>“Rew” is a boolean vector, where 1 corresponds to trials following reward and 0 corresponds to trials following omissions. The interaction between each time and state term with reward was also included in the model</p>
<p>The regression was fit individually for each rat, with random effects of session on each of the included terms to account for variability across sessions. Regressions were fit using the MixedModels package in julia.</p>
</sec>
<sec id="s4t">
<title>Coefficient of partial determination</title>
<p>The coefficient of partial determination (CPD) measures the additional variance explained by a heldout regressor by accounting for variance absorbed by correlated regressors. The CPD is defined as the proportional increase in the residual sum of squares (RSS) between the difference between the held-out model (RSS<sub>−<italic>x</italic></sub>) and the full model (RSS<sub>full</sub>).
<disp-formula id="ueqn26">
<graphic xlink:href="582617v2_ueqn26.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>The CPD can take a negative value if the RSS of the held-out model is less than the RSS of the full model.</p>
</sec>
<sec id="s4u">
<title>Non-parametric statistics</title>
<p>Since the distribution of behavioral and model metrics were non-normal and calculated at the level of rats, we performed non-parametric statistics to determine significant differences between groups. Specifically, we used a Wilcoxon signed-rank test on matched samples. These tests were performed using the SignedRankTest function in the HypothesisTests package in julia.</p>
</sec>
<sec id="s4v">
<title>Electrophysiological recordings and PSTH computation</title>
<p>Electrophysiological data was recorded from OFC in four rats, described by <bold><italic><xref ref-type="bibr" rid="c41">Miller et al. (2022</xref></italic></bold>). Briefly, 32-channel microwire arrays (Tucker-David Technologies) were surgically implanted to unilaterally target OFC (coordinates: 3.1-4.2 mm anterior, 2.4-4.2 mm lateral relative to bregma; lowered 5.2 mm from bregma/ 4.2 mm below the brain surface). After recovery, recording sessions were acquired in a behavioral chamber outfitted with a Neuralynx recording system. Spiking data were detected from 600-600Hz bandpass filtered voltage traces using a threshold of 30<italic>μ</italic>V and manually clustered.</p>
<p>Peristimulus time histograms (PSTHs) were computed for examples units to show changes in firing rate at outcome time due to differing task variables relative to each inferred state. Firing rates on each trial were computed in 250<italic>ms</italic> bins spanning 1 second before and 3 seconds after the time of outcome port entry, then smoothed using a Gaussian filter with kernel standard deviation of 1. In both cases, trials were initially split into three groups corresponding to the inferred state, measured as the state with the highest likelihood given by <xref ref-type="disp-formula" rid="eqn5">equation 5</xref>. For the first example, trials were further split by outcome value inferred by the MBr agent. The outcome value inferred by the MBr agent corresponds to the value described by <xref ref-type="disp-formula" rid="eqn3">equation 3</xref> relative to the experienced outcome <italic>before</italic> the reward update has occurred. Specifically, on common trials, the outcome value of trial <italic>t</italic> is equivalent to the chosen value of trial <italic>t, Q</italic><sub>MBr</sub>(<italic>y</italic> = <italic>y</italic><sub><italic>t</italic></sub>), before the reward update; on rare trials, the outcome value of trial <italic>t</italic> is equivalent to the non-chosen value on trial <italic>t, Q</italic><sub>MBr</sub>(<italic>y</italic> ≠ <italic>y</italic><sub><italic>t</italic></sub>), before the reward update. Trials were split into three groups defined by low (−2 to −0.5), medium (−0.5 to 0.5), and high (0.5 to 2) outcome values, giving 9 total trial groups in the first example. For the second example, trials were further split into two groups corresponding to rewarded and omission trials, giving a total of 6 trial groups. PSTHs were computed as the mean firing rate within each trial group, and error bars computed as 95% confidence intervals around the mean.</p>
</sec>
<sec id="s4w">
<title>Poisson regression of neural activity and population CPD</title>
<p>Two Poisson regression models were used to predict spiking activity and calculate the CPD of constituent predictors. A simpler model was fit when calculating the CPD separately for each state only including main effects of each predictor and a single interaction between choice and outcome for transition, which can by summarized by the formula:
<disp-formula id="ueqn27">
<graphic xlink:href="582617v2_ueqn27.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>Spike count was measured in 250<italic>ms</italic> time bins centered around task events: −2<italic>s</italic> before to 2<italic>s</italic> after second step initiation (16 bins), −2<italic>s</italic> before to 4<italic>s</italic> after the outcome port entry (24 bins), −4<italic>s</italic> before to 2<italic>s</italic> after the next trial initiation (24 bins), and −2<italic>s</italic> before to 2<italic>s</italic> after the next trial’s choice port entry. This regression was fit separately for each unit in each time bin, and the population CPD for each time bin <italic>i</italic> was computed as the aggregated CPD across all units <italic>u</italic> within that bin:
<disp-formula id="ueqn28">
<graphic xlink:href="582617v2_ueqn28.gif" mimetype="image" mime-subtype="gif"/>
</disp-formula>
</p>
<p>For this first model, to ensure that effects from the included predictors are not due to trial-by-trial co-fluctuations in neural activity, trial identities were circularly permuted and the CPD recomputed for each permuted session. A predictor’s CPD was considered significant in a particular time bin if the CPD of true session was greater than 95% of permuted session CPDs. The average CPD of all permuted sessions was subtracted from the true CPD to give a meaningful comparison to 0.</p>
<p>The second model, used to measure the separable effects of state and time, adds complexity by including not only main effects of time and state, but also interactios of time and state with every other predictor. Additional interactions of reward with outcome and next choice were also included.</p>
<disp-formula id="ueqn29">
<graphic xlink:href="582617v2_ueqn29.png" mimetype="image" mime-subtype="png"/>
</disp-formula>
<p>In the same way as the above model used to predict behavioral response times, the “time” predictor consists of three terms, time = <italic>t</italic> + <italic>t</italic><sup>3</sup> + <italic>t</italic><sup>3</sup>, and the “state” predictor consists of two terms, state = <italic>z</italic><sub>1</sub> + <italic>z</italic><sub>2</sub>. Therefore, all interactions listed above are repeated for each term in the “time” and “state” predictors. In order to calculate the CPD of each predictor, the main effect and all interaction terms were held out when fitting the reduced model. Explicitly for each predictor,</p>
<disp-formula id="ueqn30">
<graphic xlink:href="582617v2_ueqn30.png" mimetype="image" mime-subtype="png"/>
</disp-formula>
<p>Since the effects of time and state <italic>are</italic> dependent on trial-by-trial co-fluctuations, permuting the trial identities does not provide a meaningful control; the inclusion of these interactions, therefore, is intended to bolster the description of possible time and state effects. Instead of reporting the aggregated population CPD of state and time, the median and 95% confidence intervals were computed across all units to measure significant variance explained by time and state. All Poisson regression models were fit using Scikit-Learn in python.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Code availability</title>
<p>The MoA-HMM and figure code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Brody-Lab/MixtureAgentsModels">https://github.com/Brody-Lab/MixtureAgentsModels</ext-link></p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Iris Stone and Jonathan Pillow for helpful discussions about GLM-HMMs, and Kim Stachenfeld for feedback on the manuscript. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1656466, a Research Innovation Award from Princeton Neuroscience Institute, and NIMH R01MH121093.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Akam</surname> <given-names>T</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title>Simple Plans or Sophisticated Habits? State, Transition and Learning Interactions in the Two-Step Task</article-title>. <source>PLOS Computational Biology</source>. <year>2015</year> <month>Dec</month>; <volume>11</volume>(<issue>12</issue>):<fpage>e1004648</fpage>. doi: <pub-id pub-id-type="doi">10.1371/jour-nal.pcbi.1004648</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Akam</surname> <given-names>T</given-names></string-name>, <string-name><surname>Rodrigues-Vaz</surname> <given-names>I</given-names></string-name>, <string-name><surname>Marcelo</surname> <given-names>I</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Pereira</surname> <given-names>M</given-names></string-name>, <string-name><surname>Oliveira</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>RM</given-names></string-name>. <article-title>The Anterior Cingulate Cortex Predicts Future States to Mediate Model-Based Action Selection</article-title>. <source>Neuron</source>. <year>2021</year> <month>Jan</month>; <volume>109</volume>(<issue>1</issue>):<fpage>149</fpage>–<lpage>163</lpage>.e7. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2020.10.013</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Ashwood</surname> <given-names>ZC</given-names></string-name>, <string-name><surname>Roy</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Bak</surname> <given-names>JH</given-names></string-name>. <article-title>Inferring Learning Rules from Animal Decision-Making</article-title>. <source>NeurIPS</source>. <year>2020</year>; p. <fpage>12</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Ashwood</surname> <given-names>ZC</given-names></string-name>, <string-name><surname>Roy</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Stone</surname> <given-names>IR</given-names></string-name>, <string-name><surname>Urai</surname> <given-names>AE</given-names></string-name>, <string-name><surname>Churchland</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>. <article-title>Mice Alternate between Discrete Strategies during Perceptual Decision-Making</article-title>. <source>Nature Neuroscience</source>. <year>2022</year> <month>Feb</month>; <volume>25</volume>(<issue>2</issue>):<fpage>201</fpage>–<lpage>212</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-021-01007-z</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Averbeck</surname> <given-names>BB</given-names></string-name>. <article-title>Theory of Choice in Bandit, Information Sampling and Foraging Tasks</article-title>. <source>PLOS Computational Biology</source>. <year>2015</year> <month>Mar</month>; <volume>11</volume>(<issue>3</issue>):<fpage>e1004164</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004164</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="book"><string-name><surname>Bishop</surname> <given-names>CM</given-names></string-name>. <chapter-title>Pattern Recognition and Machine Learning</chapter-title>. <source>Information Science and Statistics</source>, <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Blanchard</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>. <article-title>Pure Correlates of Exploration and Exploitation in the Human Brain</article-title>. <source>Cognitive, Affective, &amp; Behavioral Neuroscience</source>. <year>2018</year> <month>Feb</month>; <volume>18</volume>(<issue>1</issue>):<fpage>117</fpage>–<lpage>126</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13415-017-0556-2</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Bolkan</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Stone</surname> <given-names>IR</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ashwood</surname> <given-names>ZC</given-names></string-name>, <string-name><surname>Iravedra Garcia</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Herman</surname> <given-names>AL</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bandi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cox</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zimmerman</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Cho</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Engelhard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Witten</surname> <given-names>IB</given-names></string-name>. <article-title>Opponent Control of Behavior by Dorsomedial Striatal Pathways Depends on Task Demands and Internal State</article-title>. <source>Nature Neuroscience</source>. <year>2022</year> <month>Mar</month>; <volume>25</volume>(<issue>3</issue>):<fpage>345</fpage>–<lpage>357</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41593-022-01021-9</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Calhoun</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Murthy</surname> <given-names>M.</given-names></string-name> <article-title>Unsupervised Identification of the Internal States That Shape Natural Behavior</article-title>. <source>Nature Neuroscience</source>. <year>2019</year>; <volume>22</volume>:<fpage>28</fpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Camerer</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ho</surname> <given-names>TH</given-names></string-name>. <article-title>Experienced-Weighted Attraction Learning in Normal Form Games</article-title>. <source>Econometrica</source>. <year>1999</year>; <volume>67</volume>(<issue>4</issue>):<fpage>827</fpage>–<lpage>874</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Cinotti</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fresno</surname> <given-names>V</given-names></string-name>, <string-name><surname>Aklil</surname> <given-names>N</given-names></string-name>, <string-name><surname>Coutureau</surname> <given-names>E</given-names></string-name>, <string-name><surname>Girard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Marchand</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Khamassi</surname> <given-names>M.</given-names></string-name> <article-title>Dopamine Blockade Impairs the Exploration-Exploitation Trade-off in Rats</article-title>. <source>Scientific Reports</source>. <year>2019</year> <month>May</month>; <volume>9</volume>(<issue>1</issue>):<fpage>6770</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-019-43245-z</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Cockburn</surname> <given-names>J</given-names></string-name>, <string-name><surname>Man</surname> <given-names>V</given-names></string-name>, <string-name><surname>Cunningham</surname> <given-names>WA</given-names></string-name>, <string-name><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>. <article-title>Novelty and Uncertainty Regulate the Balance between Exploration and Exploitation through Distinct Mechanisms in the Human Brain</article-title>. <source>Neuron</source>. <year>2022</year> <month>Aug</month>; <volume>110</volume>(<issue>16</issue>):<fpage>2691</fpage>–<lpage>2702</lpage>.e8. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2022.05.025</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="other"><string-name><surname>Correa</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Griffiths</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <article-title>Program-Based Strategy Induction for Reinforcement Learning</article-title>. <source>arXiv</source>; <year>2024</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2402.16668</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Costa</surname> <given-names>VD</given-names></string-name>, <string-name><surname>Mitz</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Averbeck</surname> <given-names>BB</given-names></string-name>. <article-title>Subcortical Substrates of Explore-Exploit Decisions in Primates</article-title>. <source>Neuron</source>. <year>2019</year> <month>Aug</month>; <volume>103</volume>(<issue>3</issue>):<fpage>533</fpage>–<lpage>545</lpage>.e5. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.017</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Seymour</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name>. <article-title>Model-Based Influences on Humans’ Choices and Striatal Prediction Errors</article-title>. <source>Neuron</source>. <year>2011</year> <month>Mar</month>; <volume>69</volume>(<issue>6</issue>):<fpage>1204</fpage>–<lpage>1215</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Niv</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P.</given-names></string-name> <article-title>Uncertainty-Based Competition between Prefrontal and Dorsolateral Striatal Systems for Behavioral Control</article-title>. <source>Nature Neuroscience</source>. <year>2005</year> <month>Dec</month>; <volume>8</volume>(<issue>12</issue>):<fpage>1704</fpage>–<lpage>1711</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn1560</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Seymour</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name>. <article-title>Cortical Substrates for Exploratory Decisions in Humans</article-title>. <source>Nature</source>. <year>2006</year> <month>Jun</month>; <volume>441</volume>(<issue>7095</issue>):<fpage>876</fpage>–<lpage>879</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nature04766</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title>Decision Theory, Reinforcement Learning, and the Brain</article-title>. <source>Cognitive, Affective, &amp; Behavioral Neuroscience</source>. <year>2008</year> <month>Dec</month>; <volume>8</volume>(<issue>4</issue>):<fpage>429</fpage>–<lpage>453</lpage>. doi: <pub-id pub-id-type="doi">10.3758/CABN.8.4.429</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="other"><string-name><surname>DePasquale</surname> <given-names>B</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>, <article-title>Neural Population Dynamics Underlying Evidence Accumulation in Multiple Rat Brain Regions</article-title>. <source>bioRxiv</source>; <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1101/2021.10.28.465122</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Ebitz</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Albarran</surname> <given-names>E</given-names></string-name>, <string-name><surname>Moore</surname> <given-names>T.</given-names></string-name> <article-title>Exploration Disrupts Choice-Predictive Signals and Alters Dynamics in Prefrontal Cortex</article-title>. <source>Neuron</source>. <year>2018</year> <month>Jan</month>; <volume>97</volume>(<issue>2</issue>):<fpage>450</fpage>–<lpage>461</lpage>.e9. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.007</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Eckstein</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>AG</given-names></string-name>. <article-title>How the Mind Creates Structure: Hierarchical Learning of Action Sequences</article-title>. <source>Proceedings of the Annual Meeting of the Cognitive Science Society</source>. <year>2021</year>; <volume>43</volume>(<issue>43</issue>).</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Feher da Silva</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hare</surname> <given-names>TA</given-names></string-name>. <article-title>Humans Primarily Use Model-Based Inference in the Two-Stage Task</article-title>. <source>Nature Human Behaviour</source>. <year>2020</year> <month>Oct</month>; <volume>4</volume>(<issue>10</issue>):<fpage>1053</fpage>–<lpage>1066</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41562-020-0905-y</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>. <article-title>Deconstructing the Human Algorithms for Exploration</article-title>. <source>Cognition</source>. <year>2018</year> <month>Apr</month>; <volume>173</volume>:<fpage>34</fpage>–<lpage>42</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cognition.2017.12.014</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Gillan</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Kosinski</surname> <given-names>M</given-names></string-name>, <string-name><surname>Whelan</surname> <given-names>R</given-names></string-name>, <string-name><surname>Phelps</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title>Characterizing a Psychiatric Symptom Dimension Related to Deficits in Goal-Directed Control</article-title>. <source>eLife</source>. <year>2016</year> <month>Mar</month>; <volume>5</volume>:<fpage>e11305</fpage>. doi: <pub-id pub-id-type="doi">10.7554/eLife.11305</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Gittins</surname> <given-names>JC</given-names></string-name>. <article-title>Bandit Processes and Dynamic Allocation Indices</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1979</year>; <volume>41</volume>(<issue>2</issue>):<fpage>148</fpage>–<lpage>177</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Gremel</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>RM</given-names></string-name>. <article-title>Orbitofrontal and Striatal Circuits Dynamically Encode the Shift between Goal-Directed and Habitual Actions</article-title>. <source>Nature Communications</source>. <year>2013</year> <month>Aug</month>; <volume>4</volume>(<issue>1</issue>):<fpage>2264</fpage>. doi: <pub-id pub-id-type="doi">10.1038/ncomms3264</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Groman</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Massi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mathias</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Curry</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name>, <string-name><surname>Taylor</surname> <given-names>JR</given-names></string-name>. <article-title>Neurochemical and Behavioral Dissections of Decision-Making in a Rodent Multistage Task</article-title>. <source>Journal of Neuroscience</source>. <year>2019</year> <month>Jan</month>; <volume>39</volume>(<issue>2</issue>):<fpage>295</fpage>–<lpage>306</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2219-18.2018</pub-id>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Groman</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Massi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Mathias</surname> <given-names>SR</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name>, <string-name><surname>Taylor</surname> <given-names>JR</given-names></string-name>. <article-title>Model-Free and Model-Based Influences in Addiction-Related Behaviors</article-title>. <source>Biological Psychiatry</source>. <year>2019</year> <month>Jun</month>; <volume>85</volume>(<issue>11</issue>):<fpage>936</fpage>–<lpage>945</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsych.2018.12.017</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Hasz</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD</given-names></string-name>. <article-title>Deliberation and Procedural Automation on a Two-Step Task for Rats</article-title>. <source>Frontiers in Integrative Neuroscience</source>. <year>2018</year> <month>Aug</month>; <volume>12</volume>:<fpage>30</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fnint.2018.00030</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Hogeveen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mullins</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Romero</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Eversole</surname> <given-names>E</given-names></string-name>, <string-name><surname>Rogge-Obando</surname> <given-names>K</given-names></string-name>, <string-name><surname>Mayer</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>VD</given-names></string-name>. <article-title>The Neurocomputational Bases of Explore-Exploit Decision-Making</article-title>. <source>Neuron</source>. <year>2022</year> <month>Jun</month>; <volume>110</volume>(<issue>11</issue>):<fpage>1869</fpage>–<lpage>1879</lpage>.e5. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.014</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Ito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Doya</surname> <given-names>K.</given-names></string-name> <article-title>Validation of Decision-Making Models and Analysis of Decision Variables in the Rat Basal Ganglia</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year> <month>Aug</month>; <volume>29</volume>(<issue>31</issue>):<fpage>9861</fpage>–<lpage>9874</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.6157-08.2009</pub-id>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="other"><string-name><surname>Ji-An</surname> <given-names>L</given-names></string-name>, <string-name><surname>Benna</surname> <given-names>MK</given-names></string-name>, <string-name><surname>Mattar</surname> <given-names>MG</given-names></string-name>, <article-title>Automatic Discovery of Cognitive Strategies with Tiny Recurrent Neural Networks</article-title>. <source>bioRxiv</source>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.04.12.536629</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Killcross</surname> <given-names>S</given-names></string-name>, <string-name><surname>Coutureau</surname> <given-names>E.</given-names></string-name> <article-title>Coordination of Actions and Habits in the Medial Prefrontal Cortex of Rats</article-title>. <source>Cerebral Cortex</source>. <year>2003</year> <month>Apr</month>; <volume>13</volume>(<issue>4</issue>):<fpage>400</fpage>–<lpage>408</lpage>. doi: <pub-id pub-id-type="doi">10.1093/cercor/13.4.400</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Kool</surname> <given-names>W</given-names></string-name>, <string-name><surname>Cushman</surname> <given-names>FA</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>. <article-title>When Does Model-Based Control Pay Off?</article-title> <source>PLOS Computational Biology</source>. <year>2016</year> <month>Aug</month>; <volume>12</volume>(<issue>8</issue>):<fpage>e1005090</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005090</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Krueger</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name>. <article-title>Strategies for Exploration in the Domain of Losses</article-title>. <source>Judgment and Decision Making</source>. <year>2017</year> <month>Mar</month>; <volume>12</volume>(<issue>2</issue>):<fpage>104</fpage>–<lpage>117</lpage>. doi: <pub-id pub-id-type="doi">10.1017/S1930297500005659</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Le</surname> <given-names>NM</given-names></string-name>, <string-name><surname>Yildirim</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sugihara</surname> <given-names>H</given-names></string-name>, <string-name><surname>Jazayeri</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sur</surname> <given-names>M.</given-names></string-name> <article-title>Mixtures of Strategies Underlie Rodent Behavior during Reversal Learning</article-title>. <source>PLOS Computational Biology</source>. <year>2023</year> <month>Sep</month>; <volume>19</volume>(<issue>9</issue>):<fpage>e1011430</fpage>. doi: <pub-id pub-id-type="doi">10.1371/jour-nal.pcbi.1011430</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Lee</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Shimojo</surname> <given-names>S</given-names></string-name>, <string-name><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>. <article-title>Neural Computations Underlying Arbitration between Model-Based and Model-free Learning</article-title>. <source>Neuron</source>. <year>2014</year> <month>Feb</month>; <volume>81</volume>(<issue>3</issue>):<fpage>687</fpage>–<lpage>699</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.028</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>C</given-names></string-name>, <string-name><surname>Li</surname> <given-names>L</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>AGE</given-names></string-name>. <article-title>Dynamic Noise Estimation: A Generalized Method for Modeling Noise Fluctuations in Decision-Making</article-title>. <source>Journal of Mathematical Psychology</source>. <year>2024</year> <month>Apr</month>; <volume>119</volume>:<fpage>102842</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jmp.2024.102842</pub-id>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="other"><string-name><surname>Luo</surname> <given-names>TZ</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>TD</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bondy</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Kopec</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Elliot</surname> <given-names>VA</given-names></string-name>, <string-name><surname>DePasquale</surname> <given-names>B</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>, <article-title>Transitions in Dynamical Regime and Neural Mode Underlie Perceptual Decision-Making</article-title>. <source>bioRxiv</source>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.10.15.562427</pub-id>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>. <article-title>Dorsal Hippocampus Contributes to Model-Based Planning</article-title>. <source>Nature Neuroscience</source>. <year>2017</year> <month>Sep</month>; <volume>20</volume>(<issue>9</issue>):<fpage>1269</fpage>–<lpage>1276</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn.4613</pub-id>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>. <article-title>Value Representations in the Rodent Orbitofrontal Cortex Drive Learning, Not Choice</article-title>. <source>eLife</source>. <year>2022</year> <month>Aug</month>; <volume>11</volume>:<fpage>e64575</fpage>. doi: <pub-id pub-id-type="doi">10.7554/eLife.64575</pub-id>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="other"><string-name><surname>Miller</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>. <article-title>Identifying Model-Based and Model-Free Patterns in Behavior on Multi-Step Tasks</article-title>. <source>Neuroscience</source>; <year>2016</year>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="other"><string-name><surname>Miller</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Eckstein</surname> <given-names>M</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Kurth-Nelson</surname> <given-names>Z</given-names></string-name>, <article-title>Cognitive Model Discovery via Disentangled RNNs</article-title>. <source>bioRxiv</source>; <year>2023</year>. doi: <pub-id pub-id-type="doi">10.1101/2023.06.23.546250</pub-id>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Miller</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Shenhav</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ludvig</surname> <given-names>EA</given-names></string-name>. <article-title>Habits without Values</article-title>. <source>Psychological Review</source>. <year>2019</year> <month>Mar</month>; <volume>126</volume>(<issue>2</issue>):<fpage>292</fpage>–<lpage>311</lpage>. doi: <pub-id pub-id-type="doi">10.1037/rev0000120</pub-id>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>O’Doherty</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Tadayonnejad</surname> <given-names>R</given-names></string-name>, <string-name><surname>Cockburn</surname> <given-names>J</given-names></string-name>, <string-name><surname>Iigaya</surname> <given-names>K</given-names></string-name>, <string-name><surname>Charpentier</surname> <given-names>CJ</given-names></string-name>. <article-title>Why and How the Brain Weights Contributions from a Mixture of Experts</article-title>. <source>Neuroscience &amp; Biobehavioral Reviews</source>. <year>2021</year> <month>Apr</month>; <volume>123</volume>:<fpage>14</fpage>–<lpage>23</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.10.022</pub-id>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="other"><string-name><surname>Oostland</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kislin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Venditto</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Deverett</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>SSH</given-names></string-name>, <article-title>Cerebellar Acceleration of Learning in an Evidence-Accumulation Task</article-title>. <source>bioRxiv</source>; <year>2022</year>. doi: <pub-id pub-id-type="doi">10.1101/2021.12.23.474034</pub-id>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Park</surname> <given-names>H</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name>, <string-name><surname>Chey</surname> <given-names>J.</given-names></string-name> <article-title>Stress Enhances Model-Free Reinforcement Learning Only after Negative Outcome</article-title>. <source>PLOS ONE</source>. <year>2017</year> <month>Jul</month>; <volume>12</volume>(<issue>7</issue>):<fpage>e0180588</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0180588</pub-id>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Roy</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Bak</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Akrami</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>JW</given-names></string-name>. <article-title>Extracting the Dynamics of Behavior in Sensory Decision-Making Experiments</article-title>. <source>Neuron</source>. <year>2021</year> <month>Feb</month>; <volume>109</volume>(<issue>4</issue>):<fpage>597</fpage>–<lpage>610</lpage>.e6. doi: <pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.004</pub-id>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Russek</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Momennejad</surname> <given-names>I</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Gershman</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title>Predictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms</article-title>. <source>PLOS Computational Biology</source>. <year>2017</year> <month>Sep</month>; <volume>13</volume>(<issue>9</issue>):<fpage>e1005768</fpage>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005768</pub-id>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Schultz</surname> <given-names>W</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Montague</surname> <given-names>PR</given-names></string-name>. <article-title>A Neural Substrate of Prediction and Reward</article-title>. <source>Science</source>. <year>1997</year> <month>Mar</month>; <volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>1599</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Shahar</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hauser</surname> <given-names>TU</given-names></string-name>, <string-name><surname>Moutoussis</surname> <given-names>M</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>R</given-names></string-name>, <string-name><surname>Keramati</surname> <given-names>M</given-names></string-name>, <string-name><surname>Consortium</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name>. <article-title>Improving the Reliability of Model-Based Decision-Making Estimates in the Two-Stage Decision Task with Reaction-Times and Drift-Diffusion Modeling</article-title>. <source>PLOS Computational Biology</source>. <year>2019</year> <month>Feb</month>; <volume>15</volume>(<issue>2</issue>):<fpage>e1006803</fpage>. doi: <pub-id pub-id-type="doi">10.1371/jour-nal.pcbi.1006803</pub-id>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="book"><string-name><surname>Sutton</surname> <given-names>RS</given-names></string-name>, <string-name><surname>Barto</surname> <given-names>A.</given-names></string-name> <chapter-title>Reinforcement Learning: An Introduction</chapter-title>. <edition>Second edition</edition> ed. <source>Adaptive Computation and Machine Learning</source>, <publisher-loc>Cambridge, Massachusetts London, England</publisher-loc>: <publisher-name>The MIT Press</publisher-name>; <year>2020</year>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="other"><string-name><surname>Wilson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sadeghiyeh</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name>, <article-title>Deep Exploration as a Unifying Account of Explore-Exploit Behavior</article-title>. <source>OSF</source>; <year>2020</year>. doi: <pub-id pub-id-type="doi">10.31234/osf.io/uj85c</pub-id>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Bonawitz</surname> <given-names>E</given-names></string-name>, <string-name><surname>Costa</surname> <given-names>VD</given-names></string-name>, <string-name><surname>Ebitz</surname> <given-names>RB</given-names></string-name>. <article-title>Balancing Exploration and Exploitation with Information and Randomization</article-title>. <source>Current Opinion in Behavioral Sciences</source>. <year>2021</year> <month>Apr</month>; <volume>38</volume>:<fpage>49</fpage>–<lpage>56</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cobeha.2020.10.001</pub-id>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Geana</surname> <given-names>A</given-names></string-name>, <string-name><surname>White</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Ludvig</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>JD</given-names></string-name>. <article-title>Humans Use Directed and Random Exploration to Solve the Explore–Exploit Dilemma</article-title>. <source>Journal of experimental psychology General</source>. <year>2014</year> <month>Dec</month>; <volume>143</volume>(<issue>6</issue>):<fpage>2074</fpage>–<lpage>2081</lpage>. doi: <pub-id pub-id-type="doi">10.1037/a0038199</pub-id>.</mixed-citation></ref>
</ref-list>
<sec>
<fig id="fig3_1" position="float" fig-type="figure">
<label>Figure 3–figure supplement 1.</label>
<caption><p>Simulated 3-state MoA-HMM parameter recovery for the five agents used in behavioral fits: model-based reward, model-based choice, model-free reward, model-free choice, and bias. Each simulation contained 5000 trials evenly split between 20 sessions. Parameters for each state are pooled.</p></caption>
<graphic xlink:href="582617v2_fig3_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig3_2" position="float" fig-type="figure">
<label>Figure 3–figure supplement 2.</label>
<caption><p>3-state MoA-HMM parameter recovery from data simulated from each rats’ behavioral model fit. Each rat’s model (n=20) was used to generate 5 independent data sets, where each data set contained the same number of trials and sessions as the corresponding rat’s behavioral data set used to fit the behavioral model, giving a total of 100 simulations.</p></caption>
<graphic xlink:href="582617v2_fig3_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig4_1" position="float" fig-type="figure">
<label>Figure 4–figure supplement 1.</label>
<caption><p>New RL learning rules significantly improve fit to behavior and capture much of the variance explained by the Novelty Preverence and Perseveration agents of the original model from <italic><xref ref-type="bibr" rid="c40">Miller et al. (2017)</xref></italic></p></caption>
<graphic xlink:href="582617v2_fig4_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5_1" position="float" fig-type="figure">
<label>Figure 5–figure supplement 1.</label>
<caption><p><bold>(A)</bold> Learning rates fit for each agent (i) corresponding to the example rat shown in Figure 5A and (ii) summarizing each learning rate over the population of rats. Each dot is an individual rat, bars represent the median, and errorbars are bootstrapped 95% confidence intervals around the median. <bold>(B)</bold> Three example sessions showing the inferred state likelihood on each trial from the example rat shown in Figure 5A. <bold>(C)</bold> Cross correlation between left choices and reward probabilities for the common outcome port given that choice (gray). Left choices are highly correlated to left-outcome reward blocks, with the peak correlation at a slight lag (vertical dashed line) indicating the trial at which the rat detects the reward probability flip. To test whether the latent states track reward flips, the cross correlation is also shown between left-outcome reward probability and the likelihood of each state: initial state (blue), the remaining state with a more rightward choice bias (orange), and the remaining state with a more leftward bias (green). These correspond directly to states 1-3 in the example rat (i) whose model is shown in Figure 5A. while other rats had states 2 and 3 assigned according to their individual choice biases.</p></caption>
<graphic xlink:href="582617v2_fig5_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5_2" position="float" fig-type="figure">
<label>Figure 5–figure supplement 2.</label>
<caption><p>A second 3-state MoA-HMM example and comparison to a GLMHMM. States identified by a MoA-HMM and GLM-HMM are highly similar. <bold>A</bold> Example 3-state MoAHMM model parameters with (i) agent weights split by state, (ii) each agent’s learning rate, (iii) the initial state probability, and (iv) the state transition matrix. <bold>B</bold> 3-state GLM-HMM fit to the same rat as (A). (i) GLM-HMM regression weights for each state. Each state is described by four types of regressors indicating four possible trial types – common-reward, common-omission, rare-reward and rare-omission – and choice direction for up to 5 previous trials, giving 20 parameters per state. Each state additionally had a bias term (ii), leading to a total of 63 model weights for a 3-state model. (iii) GLM-HMM initial state probabilities also identify a prominent initial state. (iv) GLM-HMM transition matrix closely matches MoA-HMM. <bold>C</bold> Expected state probabilities for MoA-HMM (i) averaged across all sessions and (ii) highlighted example sessions. <bold>D</bold> Expected state probabilities for GLMHMM (i) averaged across all sessions and (ii) highlighted example sessions. Temporal structure is highly similar to MoA-HMM. <bold>E</bold> Cross-correlation between the expected state probabilities inferred from the MoA-HMM and GLM-HMM (i.e. panels Cii and Dii) across all sessions. Each dot is an individual rat, black circles are medians and error bars are 95% confidence intervals around the median.</p></caption>
<graphic xlink:href="582617v2_fig5_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig5_3" position="float" fig-type="figure">
<label>Figure 5–figure supplement 3.</label>
<caption><p>Two example 4-state MoA-HMM fits corresponding to 3 state fits from <bold>(A)</bold> Figure 5A and <bold>(B)</bold> <xref rid="fig5_2" ref-type="fig">Figure 5-figure supplement 2</xref>. States are ordered according to the initial state probability (Aii and Bii) and the transition probabilities to most-likely states that follow (Aiii and Biii). Initial states are generally consistent with the 3-state fits, and the way the remaining two states split into three states is more idiosyncratic. For example, <bold>(A)</bold> suggests state 3 from the smaller model is split into two states (iv) that differ by bias (i), while <bold>(B)</bold> suggest the additional state 4 draws from both the smaller model’s states 2 and 3 (iv), and the state with largest MBr state no longer directly follows the initial state (i).</p></caption>
<graphic xlink:href="582617v2_fig5_3.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig7_1" position="float" fig-type="figure">
<label>Figure 7–figure supplement 1.</label>
<caption><p>Population CPD computed around the inter-trial interval (ITI) reveals significant encoding of state (red) even after accounting for time (blue). CPDs were measured as the median CPD across all units, with errorbars corresponding to bootstrapped 95% confidence intervals.</p></caption>
<graphic xlink:href="582617v2_fig7_1.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
<fig id="fig7_2" position="float" fig-type="figure">
<label>Figure 7–figure supplement 2.</label>
<caption><p>Example and summary 3-state MoA-HMM fits for rats with electrophysiological recordings in OFC.</p></caption>
<graphic xlink:href="582617v2_fig7_2.tif" mimetype="image" mime-subtype="tiff"/>
</fig>
</sec>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97612.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Uchida</surname>
<given-names>Naoshige</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>Harvard University</institution>
</institution-wrap>
<city>Cambridge</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Important</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>important</bold> work by Veneditto and colleagues developed a new modeling approach, called a mixture-of-agent hidden Markov model (MoA-HMM), in which choice behaviors are modeled as transitions between discrete states defined by different weighting of several reinforcement learning and decision strategies. The authors apply this approach to their previous data collected from rats performing the two-step task, and show that this method provides better fits to the data than previous methods, and predicts fluctuations in neural and other behavioral data. The reviewers found this study to be overall <bold>convincing</bold>, and the method is of general interest to the field.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97612.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Motivated by the existence of different behavioral strategies (e.g. model-based vs. model-free), and potentially different neural circuits that underlie them, Venditto et al. introduce a new approach for inferring which strategies animals are using from data. In particular, they extend the mixture of agents (MoA) framework to accommodate the possibility that the weighting among different strategies might change over time. These temporal dynamics are introduced via a hidden Markov model (HMM), i.e. with discrete state transitions. These state transition probabilities and initial state probabilities are fit simultaneously along with the MoA parameters, which include decay/learning rate and mixture weightings, using the EM algorithm. The authors test their model on data from Miller et al., 2017, 2022, arguing that this formulation leads to (1) better fits and (2) improved interpretability over their original model, which did not include the HMM portion. Lastly, they claim that certain aspects of OFC firing are modulated by the internal state as identified by the MoA-HMM.</p>
<p>Strengths:</p>
<p>The paper is very well written and easy to follow, especially for one with a significant modeling component. Furthermore, the authors do an excellent job explaining and then disentangling many threads that are often knotted together in discussions of animal behavior and RL: model-free vs. model-based choice, outcome vs. choice-focused, exploration vs. exploitation, bias, preservation. Each of these concepts is quantified by particular parameters of their models. Model recovery (Fig. 3) is mostly convincing and licenses their fits to animal behavior later (although see below). While the specific claims made about behavior and neural activity are not especially surprising (e.g. the animals begin a session, in which rare vs. common transitions are not yet known, in a more exploratory mode), the MoA-HMM framework seems broadly applicable to other tasks in the field and useful for the purpose of quantification here.</p>
<p>Weaknesses:</p>
<p>The authors sometimes seem to equivocate on to what extent they view their model as a neural (as opposed to merely behavioral) description. For example, they introduce their paper by citing work that views heterogeneity in strategy as the result of &quot;relatively independent, separable circuits that are conceptualized as supporting distinct strategies, each potentially competing for control.&quot; The HMM, of course, also relates to internal states of the animal. Therefore, the reader might come away with the impression that the MoA-HMM is literally trying to model dynamic, competing controllers in the brain (e.g. basal ganglia vs. frontal cortex), as opposed to giving a descriptive account of their emergent behavior. If the former is really the intended interpretation, the authors should say more about how they think the weighting/arbitration mechanism between alternative strategies is implemented, and how it can be modulated over time. If not, they should make this clearer.</p>
<p>Second, while the authors demonstrate that model recovery recapitulates the weight dynamics and action values (Fig. 3), the actual parameters that are recovered are less precise (Fig. 3 Supplement 1). The authors should comment on how this might affect their later inferences from behavioral data. Furthermore, it would be better to quantify using the R^2 score between simulated and recovered, rather than the Pearson correlation (r), which doesn't enforce unity slope and zero intercept (i.e. the line that is plotted), and so will tend to exaggerate the strength of parameter recovery.</p>
<p>Finally, the authors are very aware of the difficulties associated with long-timescale (minutes) correlations with neural activity, including both satiety and electrode drift, so they do attempt to control for this using a third-order polynomial as a time regressor as well as interaction terms (Fig. 7 Supplement 1). However, on net there does not appear to be any significant difference between the permutation-corrected CPDs computed for states 2 and 3 across all neurons (Fig. 7D). This stands in contrast to the claim that &quot;the modulation of the reward effect can also be seen between states 2 and 3 - state 2, on average, sees a higher modulation to reward that lasts significantly longer than modulation in state 3,&quot; which might be true for the neuron in Fig. 7C, but is never quantified. Thus, while I am convinced state modulation exists for model-based (MBr) outcome value (Fig. 7A-B), I'm not convinced that these more gradual shifts can be isolated by the MoA-HMM model, which is important to keep in mind for anyone looking to apply this model to their own data.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.97612.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public Review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This is an interesting and well-performed study that develops a new modeling approach (MoA-HMM) to simultaneously characterize reinforcement learning parameters of different RL agents, as well as latent behavioral states that differ in the relative contributions of those agents to the animal's choices. They performed this study in rats trained to perform the two-step task. While the major advance of the paper is developing and rigorously validating this novel technical approach, there are also a number of interesting conceptual advances. For instance, humans performing the two-step task are thought to exhibit a trade-off between model-free and model-based strategies. However, the MoA-HMM did not reveal such a trade-off in the rats, but instead suggested a trade-off between model-based exploratory vs. exploitative strategies. Additionally, the firing rates of neurons in the orbitofrontal cortex (OFC) reflected latent behavioral states estimated from the HMM, suggesting that (1) characterizing dynamic behavioral strategies might help elucidate neural dynamics supporting behavior, and (2) OFC might reflect the contributions of one or a subset of RL agents that are preferentially active or engaged in particular states identified by the HMM.</p>
<p>Strengths:</p>
<p>The claims were generally well-supported by the data. The model was validated rigorously and was used to generate and test novel predictions about behavior and neural activity in OFC. The approach is likely to be generally useful for characterizing dynamic behavioral strategies.</p>
<p>Weaknesses:</p>
<p>There were a lot of typos and some figures were mis-referenced in the text and figure legends.</p>
</body>
</sub-article>
</article>