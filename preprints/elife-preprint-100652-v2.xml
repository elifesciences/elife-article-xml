<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">100652</article-id>
<article-id pub-id-type="doi">10.7554/eLife.100652</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.100652.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Robust variability of grid cell properties within individual grid modules enhances encoding of local space</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4147-2026</contrib-id>
<name>
<surname>Redman</surname>
<given-names>William T</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
    <email>wredman@ucsb.edu</email>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-6698-476X</contrib-id>
<name>
<surname>Acosta-Mendoza</surname>
<given-names>Santiago</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">†</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2761-477X</contrib-id>
<name>
<surname>Wei</surname>
<given-names>Xue-Xin</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5366-8501</contrib-id>
<name>
<surname>Goard</surname>
<given-names>Michael J</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
    <aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>Interdepartmental Graduate Program in Dynamical Neuroscience, University of California, Santa Barbara</institution></institution-wrap>, <city>Santa Barbara</city>, <country country="US">United States</country></aff>
    <aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/029pp9z10</institution-id><institution>Intelligent Systems Center, Johns Hopkins University Applied Physics Lab</institution></institution-wrap>, <city>North Laurel</city>, <country country="US">United States</country></aff>
    <aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Department of Neuroscience, The University of Texas at Austin</institution></institution-wrap>, <city>Austin</city>, <country country="US">United States</country></aff>
    <aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Department of Psychology, The University of Texas at Austin</institution></institution-wrap>, <city>Austin</city>, <country country="US">United States</country></aff>
    <aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Center for Perceptual Systems, The University of Texas at Austin</institution></institution-wrap>, <city>Austin</city>, <country country="US">United States</country></aff>
    <aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Center for Theoretical and Computational Neuroscience, The University of Texas at Austin</institution></institution-wrap>, <city>Austin</city>, <country country="US">United States</country></aff>
    <aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>Department of Psychological and Brain Sciences, University of California, Santa Barbara</institution></institution-wrap>, <city>Santa Barbara</city>, <country country="US">United States</country></aff>
    <aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>Department of Molecular, Cellular, and Developmental Biology, University of California, Santa Barbara</institution></institution-wrap>, <city>Santa Barbara</city>, <country country="US">United States</country></aff>
    <aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>Neuroscience Research Institute, University of California, Santa Barbara</institution></institution-wrap>, <city>Santa Barbara</city>, <country country="US">United States</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Peyrache</surname>
<given-names>Adrien</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>McGill University</institution>
</institution-wrap>
<city>Montreal</city>
<country>Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country>Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>†</label><p>Equal contribution</p></fn>
<fn fn-type="coi-statement"><p>Competing Interest Statement: The authors have declared no competing interest.</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-09-13">
<day>13</day>
<month>09</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-02-04">
<day>04</day>
<month>02</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP100652</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-07-01">
<day>01</day>
<month>07</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-06-13">
<day>13</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.02.27.582373"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-09-13">
<day>13</day>
<month>09</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.100652.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.100652.1.sa4">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100652.1.sa3">Reviewer #1 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100652.1.sa2">Reviewer #2 (Public review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.100652.1.sa1">Reviewer #3 (Public review):</self-uri>
<self-uri content-type="author-comment" xlink:href="https://doi.org/10.7554/eLife.100652.1.sa0">Author response:</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, Redman et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Redman et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-100652-v2.pdf"/>
<abstract>
<title>Abstract</title>
<p>Although grid cells are one of the most well studied functional classes of neurons in the mammalian brain, whether there is a single orientation and spacing value per grid module has not been carefully tested. We analyze a recent large-scale recording of medial entorhinal cortex to characterize the presence and degree of heterogeneity of grid properties within individual modules. We find evidence for small, but robust, variability and hypothesize that this property of the grid code could enhance the encoding of local spatial information. Performing analysis on synthetic populations of grid cells, where we have complete control over the amount heterogeneity in grid properties, we demonstrate that grid property variability of a similar magnitude to the analyzed data leads to significantly decreased decoding error. This holds even when restricted to activity from a single module. Our results highlight how the heterogeneity of the neural response properties may benefit coding and opens new directions for theoretical and experimental analysis of grid cells.</p>
</abstract>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>In this revised manuscript, we make a number of major improvements in response to reviewer comments, including:
1) Analysis of an recurrent neural network (RNN) model to determine whether grid property variability prevents path integration or toroidal attractor dynamics.
2) Additional analysis of changes to within-module variability over time
3) Further control analyses to rule out additional sources of grid property variability
4) Revised the text to avoid overstating previous theoretical positions
In addition to these major changes, there are a number of minor changes to address specific reviewer comments.
</p></fn>
</fn-group>
</notes>
</front>
<body>
    <sec id="s1">
        <title>Introduction</title>
   <p>The discovery of grid cells in medial entorhinal cortex (MEC) [<xref ref-type="bibr" rid="c1">1</xref>] has led to considerable experimental and computational work aimed at identifying their origin [<xref ref-type="bibr" rid="c2">2</xref>–<xref ref-type="bibr" rid="c12">12</xref>] and their function [<xref ref-type="bibr" rid="c13">13</xref>–<xref ref-type="bibr" rid="c21">21</xref>]. The organization of grid cells into discrete modules [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>], with grid properties (grid spacing and orientation) clustered within module, but not between modules, has fundamentally shaped this research. For instance, the increasing size and spacing of grid modules along the dorsal-ventral axis of MEC, by discontinuous jumps of a near constant ratio, has been argued to be optimal for encoding local spatial information [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>] when grid cell activity across all modules is integrated together [<xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c28">28</xref>]. This is despite the fact that the individual neurons in hippocampus, a downstream target of the MEC, receive inputs from only a portion of the dorsal-ventral axis [<xref ref-type="bibr" rid="c29">29</xref>]. The modularity of the grid system has also been proposed to simplify the wiring necessary for generating continuous attractor dynamics [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c16">16</xref>], a computational mechanism theorized to underlie grid cells function that enjoys considerable experimental support [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c34">34</xref>].</p>
<p>Much of this prior theoretical work has made the additional assumption that grid cell properties are identical, up to a phase shift, within a single module [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c35">35</xref>]. However, as experimentalists have been aware, the distributions of measured orientation and spacing show non-zero variability [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c34">34</xref>]. This could be due to finite recording time, neurophysiological noise, and/or the sensitivity of numerical methods used to fit grid properties. Alternatively, this variability could reflect underlying inhomogeneity, at a fine scale, within modules. Despite the fundamental way in which grid cell function has been informed by their modular organization, to the best of our knowledge, no characterization of the degree and robustness of variability in grid properties within individual modules has been performed.</p>
<p>If robust variability of grid properties does exist, then it is possible that this heterogeneity could be exploited to encode additional information about local space, reducing the requirement of integration across multiple grid modules (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). In particular, when all grid cells in a module have the same grid orientation and spacing, the joint population activity has a translational invariance that persists, even as larger populations of grid cells are considered (<xref rid="fig1" ref-type="fig">Fig. 1</xref>, “Fixed grid properties within module”). Thus, it is not possible to resolve spatial position from population activity. In contrast, if grid cells within a module have variability in their grid orientation and spacing, then – over a finite area – the translational invariance of the population activity is broken. In such a case, distinct patterns of population activity emerge in distinct locations of space (<xref rid="fig1" ref-type="fig">Fig. 1</xref>, “Variable grid properties within module”). As an example, the blue and green grid cells in <xref rid="fig1" ref-type="fig">Fig. 1B</xref> show the most overlap in the upper left half of the arena (denoted by cyan pixels), while the red and blue grid cells show the most overlap in the lower right portion of the arena (denoted by purple pixels). This is despite the fact that the variability in spacing and orientation is only on the order of a few centimeters and degrees, respectively. We expect that these patterns should become more complex and contain more information that could be used to disentangle spatial location as more grid cells are considered. In this paper, we perform detailed analysis of recent state-of-the-art MEC electrophysiological recordings, which were made publicly available [<xref ref-type="bibr" rid="c34">34</xref>]. We characterize the variability of grid orientation and spacing within individual modules and find evidence for small, but robust, variability. This variability is present whether a single value of grid orientation and spacing is assigned to each grid cell, or whether grid distortion [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>] is taken into account by considering three grid orientations and spacings independently. Performing similar analysis on recent normative recurrent neural network (RNN) models of grid cells [<xref ref-type="bibr" rid="c10">10</xref>], we find the presence of comparable variability. This provides another possible explanation for why these RNN models have been found to capture MEC grid cell response profiles [<xref ref-type="bibr" rid="c38">38</xref>]. To assess the functional implications of this heterogeneity, we perform simulation experiments with synthetic, noisy grid cell populations, where we have complete control over the distribution of grid orientation and spacing. We find that the variability in grid cell orientation and spacing, at a similar degree as present in the data we analyze, leads to lower decoding error of local space when using the activity of a <italic>single</italic> module.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Variability in grid cell properties within a module leads to enhanced encoding of local space.</title>
<p>When the activity of three idealized grid cells, all with the same grid spacing and orientation, are considered, the periodicity of the responses limits the amount of information conveyed about local space (Left column – “Fixed grid properties within module”). That is, there are multiple locations in physical space with identical population level activity. However, when three grid cells with variable grid spacing and orientation (in the realm of what is measured within individual grid modules – see Results), their joint activity contains considerably more information (Right column – “Variable grid properties within module”). This benefit of spatial inhomogeneity is expected to increase with larger populations of grid cells. Dashed squares in the joint activity map are enlarged below.</p></caption>
<graphic xlink:href="582373v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Taken together, our results challenge a frequently made assumption in the theoretical literature and support a growing understanding of the spatial information encoded by grid cell populations [<xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c42">42</xref>]. Additionally, our results encourage consideration of the broader benefits that multiple modules may provide, beyond the encoding of local space [<xref ref-type="bibr" rid="c43">43</xref>–<xref ref-type="bibr" rid="c45">45</xref>].</p>
    </sec>
    <sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Robust differences in grid cell properties within individual modules</title>
<p>To determine the extent to which variability of grid properties in individual modules exists, and to what extent this variability is a robust property of the grid code, we analyzed previously published MEC recordings [<xref ref-type="bibr" rid="c34">34</xref>], which include tens to hundreds of grid cells simultaneously recorded. This allows us to characterize the distribution of grid properties within a single grid module, to an extent not possible with other data sets. For each grid cell, we compute the grid spacing (<italic>λ</italic>) and orientation (<italic>θ</italic>) by measuring properties associated with the six hexagonally distributed peaks of the spatial autocorrelogram (SAC), as traditionally performed [<xref ref-type="bibr" rid="c46">46</xref>] (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>; see Materials and Methods). For clarity, we begin by focusing on a single module, recorded from an open field environment (recording identifier: Rat R, Day 1, Module 2 – R12). This module was picked for its long recording time (approximately 130 minutes of recorded activity, as compared to the other open field recordings that have 30 minutes or less of recorded activity) and for its large number of simultaneously recorded putative grid cells (<italic>N</italic> = 168). In this module, we find that grid cells with high grid scores (<italic>&gt;</italic> 0.85; <italic>N</italic> = 74) have a range of grid orientation and spacing (example cells shown in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>; distributions across module shown in <xref rid="fig2" ref-type="fig">Fig. 2E, F</xref>), with <italic>λ</italic> ranging from 65 cm to 90 cm and <italic>θ</italic> ranging from 2° to 9°. Overlaying the SACs of pairs of grid cells with similar grid spacing and different grid orientation (<xref rid="fig2" ref-type="fig">Fig. 2C</xref>) or vice versa (<xref rid="fig2" ref-type="fig">Fig. 2D</xref>) enables visualization of the extent of this variability.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Grid properties are variable within a single grid module (recording ID R12).</title>
<p>(A) Overview of the standard procedure used to calculate the grid spacing and orientation of a given grid cell. First, spike maps are computed by identifying the location of the animal at the time of each spike. Gray line denotes the trajectory of the rat, red dots denote locations of spikes. A rate map is constructed by binning space and normalizing by the amount of time the rat spent in each spatial bin. A spatial autocorrelogram (SAC) is computed and, after the center peak is masked out (white pixels in the center of the spatial autocorrelogram – leading to change in color scale), the grid properties are fit by measuring the length and angle of the three peaks closest to 0°. (B) Example grid cells from the same module (recording ID R12), with estimated grid score, orientation (<italic>θ</italic>), and spacing (<italic>λ</italic>). (C)–(D) SAC overlaid for two pairs of grid cells [from (B)]; one pair with different <italic>θ</italic> and similar <italic>λ</italic> (C) and the other with similar <italic>θ</italic> and different <italic>λ</italic> (D). (E)–(F) Distribution of <italic>θ</italic> (E) and <italic>λ</italic> (F) across all grid cells with grid score <italic>&gt;</italic> 0.85 (N = 74).</p></caption>
<graphic xlink:href="582373v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Because individual grid fields can be cut-off by the boundaries of the environment, it is possible that computing the grid spacing from the SAC (which considers all grid fields) could lead to an under-estimate of <italic>λ</italic> for some grid cells. To verify that the broad distribution of grid spacing, within the same module, that we see is not due to the specifics of the SAC, we recomputed the grid spacing for all grid cells directly from their rate maps, considering only the three grid fields closest to the center of the environment (<xref rid="figS1" ref-type="fig">Fig. S1A</xref>; see Materials and Methods). We find that while the grid spacing estimated from the SAC tends to be larger than the grid spacing estimated from the rate maps, a similarly broad range of <italic>λ</italic> is again present (<xref rid="figS1" ref-type="fig">Fig. S1B, C</xref>).</p>
<p>To assess whether the heterogeneity of grid properties present in a single grid module is a robust feature or attributable to noise (either in the recording or the grid property fitting procedure), we measure the variability in grid orientation and spacing <bold>within</bold> a single grid cell and <bold>between</bold> pairs of grid cells. If the heterogeneity is explainable by noise, then we expect that the within-cell variability will be of the same magnitude as the between-cell variability. In contrast, if the heterogeneity is a robust feature of the grid code, then we expect the within-cell variability will be significantly smaller than the between-cell variability. To measure the within- and between-cell variability, we split the recording into evenly spaced 30 second bins, randomly assigning each temporal bin to one of two equal length halves and computing the grid properties for each half of the recording (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>; see Materials and Methods). We set inclusion criteria to filter out cells that do not have consistent hexagonally structured SACs across splits of the recording (see Materials and Methods). Although these requirements are strict (see <xref rid="figS2" ref-type="fig">Fig. S2</xref> for percent of cells rejected), they set a conservative estimate on the amount of grid property variability, ensuring that we do not artificially inflate the variability due to inclusion of unreliable grid cells. We found that the length of the temporal bin used to split the data does not have a large impact on the percentage of cells accepted by this criteria (<xref rid="figS3" ref-type="fig">Fig. S3</xref>; see Materials and Methods).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Variability of grid properties is a robust feature of individual grid module (recording ID R12).</title>
<p>(A)Schematic overview of approach used to compute the between- and within-cell variability of grid orientation and spacing. (B)–(C) Distribution of within- and between-cell variability of <italic>θ</italic> and <italic>λ</italic>, respectively. Note that the distribution is across all 100 random shuffles of the data into two halves. (D) Average within-cell variability of grid orientation <inline-formula><inline-graphic xlink:href="582373v3_inline38.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, compared to average between-cell variability of grid spacing <inline-formula><inline-graphic xlink:href="582373v3_inline39.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (E) Same as (D), but for <italic>λ</italic>. 1 cell was excluded from (E) for visualization <inline-formula><inline-graphic xlink:href="582373v3_inline40.gif" mime-subtype="gif" mimetype="image"/></inline-formula> but was included in non-parametric statistical analysis. For (D)–(E), <italic>N</italic> = 82.</p></caption>
<graphic xlink:href="582373v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We find that the distribution, across 100 random shuffles of the data into two halves, of within-cell variability of grid orientation and spacing is more concentrated around 0 than the between-cell variability. Comparing the average within- and between-cell variability of grid spacing and orientation reveals that nearly all of the grid cells that passed the criteria for inclusion (<italic>N</italic> = 82) exhibit more between-cell than within-cell variability (<xref rid="fig3" ref-type="fig">Fig. 3D, E</xref>): 95.1% grid cells for orientation <inline-formula><inline-graphic xlink:href="582373v3_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula>; denotes mean across cells) and 100% of grid cells for spacing <inline-formula><inline-graphic xlink:href="582373v3_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. A Wilcoxon-Signed-Rank Test indicates that between-cell variability is significantly greater than within-cell variability, for both orientation and spacing (Δ<italic>θ</italic>: <italic>p &lt;</italic> 0.001, Δ<italic>λ</italic> : <italic>p &lt;</italic> 0.001). We perform a number of control analyses to confirm that our results are robust, finding that: (1) the average grid field width does not significantly affect the between-cell variability (<xref rid="figS4" ref-type="fig">Fig. S4</xref>), (2) the amount of between-cell variability does not change significantly across the recording duration (<xref rid="figS5" ref-type="fig">Fig. S5</xref>), (3) the between-cell variability is not significantly impacted by the boundaries of the arena (<xref rid="figS6" ref-type="fig">Fig. S6</xref>), and (4) the amount of between-cell variability is not driven by the presence of cells that have both head direction and grid cell like tuning (“conjunctive” cells [<xref ref-type="bibr" rid="c46">46</xref>] – <xref rid="figS7" ref-type="fig">Fig. S7</xref>).</p>
<p>In keeping with convention, the reported grid cell properties are the average of those computed for each of the three independent axes in the SAC (Axis 1: aligned to ≈ 0°; Axis 2: aligned to ≈60°; Axis 3: aligned to ≈ ™ 60° [<xref ref-type="bibr" rid="c47">47</xref>]). To ensure that this averaging is not contributing to greater between-cell variability, we repeated the analysis above, restricting ourselves to each axis separately. The results again demonstrate that grid properties are significantly more robust within-cell than between-cell (<xref rid="fig4" ref-type="fig">Fig. 4A, B</xref>). For each axis, the average between-cell variability for every cell was significantly higher than the average within-cell variability (<xref rid="fig4" ref-type="fig">Fig. 4C, D</xref>), as reported by the Wilcoxon-Signed-Rank Test for orientation (<italic>p &lt;</italic> 0.001, for all three axes) and for spacing (<italic>p &lt;</italic> 0.001, for all three axes).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Variability of grid properties, restricted to the same axis, is a robust feature of individual grid module (recording ID R12).</title>
<p>Same analysis as in <xref rid="fig3" ref-type="fig">Fig. 3</xref> (B) – (E), but for variability measured on each axis independently. For visualization, we exclude a small number of cells that were outside the axes limits, including 2, 5, and 10 cells for Axes 1–3, respectively (C); and 3, 4, and 3 cells for Axes 1–3, respectively (D); these cells were included in non-parametric statistical analyses.</p></caption>
<graphic xlink:href="582373v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Having demonstrated that grid cell properties are robustly heterogeneous in a single module, we proceed to analyze the remaining recordings in the data set (<italic>N</italic> = 420 cells across 8 modules; one module in the data set had no cells that passed our criteria) [<xref ref-type="bibr" rid="c34">34</xref>]. Although there are differences across recordings, we find larger between-than within-cell variability for grid orientation and spacing is present across all recordings (<xref rid="fig5" ref-type="fig">Fig. 5A, B</xref>; 80.0% of grid cells have greater between- than within-cell variability for orientation, <inline-formula><inline-graphic xlink:href="582373v3_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> of grid cells have greater between- than within-variability for spacing, <inline-formula><inline-graphic xlink:href="582373v3_inline3a.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula>). A Wilcoxon-Signed-Rank Test finds that these differences are significant (Δ<italic>θ</italic>: <italic>p &lt;</italic> 0.001; Δ<italic>λ</italic> : <italic>p &lt;</italic> 0.001). We do not find evidence suggesting that the within-cell variability is influenced by grid score (<xref rid="fig5" ref-type="fig">Fig. 5C, D</xref>; linear regression for <italic>θ</italic>: <italic>R</italic><sup>2</sup> = 0.03, <italic>p</italic> = 0.55 Wald Test; linear regression for <italic>λ</italic>: <italic>R</italic><sup>2</sup> = 0.07, <italic>p</italic> = 0.14 Wald Test).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Within module grid property variability is a robust feature across modules.</title>
<p>(A) Average within-cell variability of grid orientation <inline-formula><inline-graphic xlink:href="582373v3_inline41.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, compared to average between-cell variability of grid orientation <inline-formula><inline-graphic xlink:href="582373v3_inline42.gif" mime-subtype="gif" mimetype="image"/></inline-formula> for each cell (<italic>N</italic> = 420) across 8 modules (cells colored by their corresponding recording ID). The histogram above the plot shows the distribution of <inline-formula><inline-graphic xlink:href="582373v3_inline43.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and the histogram to the right shows the distribution of <inline-formula><inline-graphic xlink:href="582373v3_inline44.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (B)Same as (A), but for grid spacing. For visualization, 5 cells are excluded <inline-formula><inline-graphic xlink:href="582373v3_inline45.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, but are included in non-parametric statistical analyses. Dashed gray lines show the population mean. (C)–(D) Average within cell variability of <italic>θ</italic> and <italic>λ</italic> (respectively), as a function of grid score. For visualization, 3 and 22 cells are excluded from (C)–(D), respectively, but are included in statistical analyses. Black solid line is linear regression, with <italic>R</italic><sup>2</sup> and <italic>p</italic>-value reported above.</p></caption>
<graphic xlink:href="582373v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To understand whether the observed grid cell variability may be a heretofore unknown property of MEC computational models, we trained recurrent neural networks (RNN) to perform path integration. These RNN models have previously been shown to develop grid responses [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c10">10</xref>] and have been argued to develop continuous attractor network structure [<xref ref-type="bibr" rid="c12">12</xref>], making them normative models for MEC function. We find that the RNNs develop grid responses with a distribution of grid spacing and orientation (<xref rid="figS8" ref-type="fig">Fig. S8A–D</xref>), much like the neural data we analyzed (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Changing the sparsity of the recurrent layer by increasing the strength of weight decay regularization reduces the variability (<xref rid="figS8" ref-type="fig">Fig. S8E–F</xref>).</p>
<p>Taken together, our analysis of large-scale MEC recordings demonstrates that grid cells in the same grid module do not have a single grid spacing and orientation, but instead have a restricted range of values around the module mean. This property is a robust feature that cannot be explained by noise from the recording or fitting procedures and emerges in existing normative models of MEC.</p>
</sec>
<sec id="s2b">
<title>Variability in grid properties within individual modules improves the encoding of local space</title>
<p>The variability of grid cell properties within individual grid modules, while statistically significant, is small in magnitude. Can a computational benefit in the encoding of local space be gained from this level of inhomogeneity? How sensitive might such a computational benefit be to the exact amount of variability present?</p>
<p>To address these questions, we generate populations of synthetic grid cells (see Materials and Methods), where we have complete control over the number of grid cells and their firing properties. For simplicity, we assume that all grid cells in our population have grid orientation and spacing sampled from Gaussian distributions, with means <inline-formula><inline-graphic xlink:href="582373v3_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and standard deviations <italic>σ</italic><sub><italic>λ</italic></sub> and <italic>σ</italic><sub><italic>θ</italic></sub>, respectively. Assigning each grid cell in our population a grid orientation and spacing, we are able to generate “ideal” rate maps [<xref ref-type="bibr" rid="c14">14</xref>]. Sampling from a Poisson process on these ideal rate maps, we generate noisy grid cell rate maps (<xref rid="fig6" ref-type="fig">Fig. 6A</xref>). Using a simple linear decoder (see Materials and Methods), we can examine how decoding error of local space is affected by the number of grid cells in the population and the amount of variability (<italic>σ</italic><sub><italic>λ</italic></sub> and <italic>σ</italic><sub><italic>θ</italic></sub>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Variability in grid properties enables improved decoding of local space from the activity of grid cells within a single module.</title>
<p>(A) Example noisy grid cell rate maps generated from a Poisson process. The size of the square arena is set to 1.5 m × 1.5 m to be consistent with what was used in the experimental set-up analyzed [<xref ref-type="bibr" rid="c34">34</xref>]. (B)–(C) Distribution of sampled grid spacing and orientation from synthetic population, when using <inline-formula><inline-graphic xlink:href="582373v3_inline46.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <inline-formula><inline-graphic xlink:href="582373v3_inline47.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, <italic>σ</italic><sub><italic>λ</italic></sub> = 5 cm, and <italic>σ</italic><sub><italic>θ</italic></sub> = 1°; compare to the distribution measured from real data (<xref rid="fig2" ref-type="fig">Fig. 2E, F</xref>). (D) Decoding error, as a function of grid cell population size, with populations having either no variability in grid properties (black line) or variability similar to what was present in the data analyzed (blue line). The solid line is the mean across 25 independent grid cell populations and the shaded area is ± standard deviation of the 25 independent populations. The dashed black line shows chance level decoding error. (E) Decoding error for synthetic populations and real data for up to <italic>N</italic> = 64 cells (red line). (F) Decoding error, over a grid of <italic>σ</italic><sub><italic>θ</italic></sub> and <italic>σ</italic><sub><italic>λ</italic></sub> values, for populations of <italic>N</italic> = 1024 grid cells. White star denotes values used in (D).</p></caption>
<graphic xlink:href="582373v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We begin by investigating the decoding capabilities of a synthetic module with properties similar to that of the experimentally recorded module that was analyzed in detail (recording identifier R12; <xref rid="fig2" ref-type="fig">Figs. 2</xref>, <xref rid="fig3" ref-type="fig">3</xref>). We therefore set <inline-formula><inline-graphic xlink:href="582373v3_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To determine an appropriate value for <italic>σ</italic><sub><italic>λ</italic></sub> and <italic>σ</italic><sub><italic>θ</italic></sub>, we subtract the mean between-cell variability by the mean within-cell variability <inline-formula><inline-graphic xlink:href="582373v3_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline10.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We interpret these values as the amount of variability in grid spacing and orientation that is not due to noise. Therefore, we set <italic>σ</italic><sub><italic>θ</italic></sub> = 1° and <italic>σ</italic><sub><italic>λ</italic></sub> = 5 cm. This amount of variability leads to similar sampled distributions of <italic>λ</italic> and <italic>θ</italic> as was found in the real data (compare <xref rid="fig6" ref-type="fig">Fig. 6B, C</xref> with <xref rid="fig2" ref-type="fig">Fig. 2E, F</xref>).</p>
<p>Applying the linear decoder to populations of synthetic grid cells, we find that decoding error decreases as the number of grid cells is increased (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>, blue line). When <italic>N</italic> = 1024, the decoding error is ≈ 20 cm, substantially better than random decoding (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>, dashed black line). As expected, this result is not seen in a synthetic population of grid cells with zero variability in grid properties (<italic>σ</italic><sub><italic>θ</italic></sub> = 0° and <italic>σ</italic><sub><italic>λ</italic></sub> = 0 cm) (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>, black line). In particular, while the restricted number of grid firing fields enables a decoding error smaller than chance, the population with fixed grid properties exhibits little change in decoding error with increasing numbers of grid cells. This demonstrates that the improved encoding is specific to populations with inhomogeneity in their grid spacing and orientation.</p>
<p>To validate that the synthetic population is a reasonable surrogate to the experimentally recorded data, we perform the same decoding analysis on grid cells from recording ID R12 (see Materials and Methods). As the real data is limited in the number of cells, we are only able to compare up to populations of <italic>N</italic> = 64 (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>). However, in this restricted range, we find agreement between the synthetic population with grid property variability and the recorded data (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>, compare red and blue lines). This supports our hypothesis that the observed amounts of inhomogeneity in grid spacing and orientation can lead to improved decoding of local space.</p>
<p>To determine the extent to which the decrease in decoding error depends the exact variability of grid spacing and orientation, we perform a grid search over 36 pairs of (<italic>σ</italic><sub><italic>λ</italic></sub>, <italic>σ</italic><sub><italic>θ</italic></sub>) values in [0°, 1°, …, 5°] × [0 cm, 1 cm, …, 5 cm]. As expected, when the variability of both grid properties is large, we find nearly 0 cm decoding error of local space (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>, bottom right). However, there is additionally a range of <italic>σ</italic><sub><italic>λ</italic></sub> and <italic>σ</italic><sub><italic>θ</italic></sub> values that lead to improved decoding, including values smaller than the values matched to the experimental data (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>, above and to the left of the white star). We perform the same grid search on synthetic populations of grid cells with different spacings, <inline-formula><inline-graphic xlink:href="582373v3_inline11.gif" mime-subtype="gif" mimetype="image"/></inline-formula>and <inline-formula><inline-graphic xlink:href="582373v3_inline12.gif" mime-subtype="gif" mimetype="image"/></inline-formula> (<xref rid="figS9" ref-type="fig">Fig. S9A, B</xref>), finding again that the decoding error drops below that of the fixed population with sufficient grid property variability. For the synthetic module with smaller spacing <inline-formula><inline-graphic xlink:href="582373v3_inline13.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, the existence of more grid fields in the 1.5 m × 1.5 m arena leads to greater complexity of interference patterns (<xref rid="fig1" ref-type="fig">Fig. 1</xref>), enabling the same amount of variability in grid spacing and orientation to lead to a sharper decrease in decoding error. Additionally, for the synthetic module with smaller spacing, a given value of <italic>σ</italic><sub><italic>λ</italic></sub> is a larger percentage of <inline-formula><inline-graphic xlink:href="582373v3_inline14.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, as compared to the synthetic module with larger spacing.</p>
<p>Finally, we consider how decoding within a single module compares to decoding across two modules. When the two modules are consecutive (e.g., modules 1 and 2), their mean grid spacing, <inline-formula><inline-graphic xlink:href="582373v3_inline15.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline16.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, has been experimentally found to be related via <inline-formula><inline-graphic xlink:href="582373v3_inline17.gif" mime-subtype="gif" mimetype="image"/></inline-formula> [<xref ref-type="bibr" rid="c22">22</xref>]. In such a case, decoding activity from populations with fixed grid properties leads to nearly 0 cm error (<xref rid="figS10" ref-type="fig">Fig. S10A</xref>, black line). The addition of variability in grid properties does not significantly change the behavior of the decoding (<xref rid="figS10" ref-type="fig">Fig. S10A</xref>, blue line). This suggests that small amounts of inhomogeneity may not disrupt the previously achieved theoretical bounds on decoding from multiple grid modules [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c48">48</xref>]. However, if the two modules being decoded from are non-consecutive (e.g., modules 1 and 3) then the mean grid spacing can be related by an integer multiple, <inline-formula><inline-graphic xlink:href="582373v3_inline18.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. In such a setting, the grid fields of the larger module are a subset of the grid fields of the smaller module, up to a rotation (due to the difference in orientation between the two modules), and we again find that variability in grid properties can improve decoding accuracy (<xref rid="figS10" ref-type="fig">Fig. S10B</xref>, compare black and blue lines). Similar results are found when using experimentally found values of <inline-formula><inline-graphic xlink:href="582373v3_inline19.gif" mime-subtype="gif" mimetype="image"/></inline-formula> [<xref ref-type="bibr" rid="c22">22</xref>] and not assuming an exact<inline-formula><inline-graphic xlink:href="582373v3_inline20.gif" mime-subtype="gif" mimetype="image"/></inline-formula> relationship.</p>
<p>Taken together, these results suggest that <italic>individual</italic> grid modules can exhibit significant encoding of local space via heterogeneity in their grid properties, even when the extent of the variability in <italic>θ</italic> and <italic>λ</italic> is similar to that found in the analysis of the experimental recordings. This benefit can also improve encoding in cases when multiple, non-consecutive modules are considered.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The multiple firing fields of grid cells, organized along a triangular lattice, has been historically interpreted as a limiting feature for the encoding of local space. Particularly influential in shaping this view has been the discovery of the distribution of grid cells into distinct modules, with grid cell spacing (<italic>λ</italic>) and orientation (<italic>θ</italic>) preserved within, but not across, modules [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>], making integration across multiple modules necessary for spatial information to be decoded [<xref ref-type="bibr" rid="c24">24</xref>–<xref ref-type="bibr" rid="c28">28</xref>]. While evidence for discontinuity in the grid cell properties across modules is strong, the corollary assumption, that within-module values of <italic>λ</italic> and <italic>θ</italic> are identical (up to the bounds of noise), has not been systematically studied.</p>
<p>Analyzing recently collected MEC recordings, we found the range of <italic>λ</italic> and <italic>θ</italic> values was large, with examples of grid cell pairs in the same module having over 5° difference in grid orientation and 20 cm difference in grid spacing (<xref rid="fig2" ref-type="fig">Fig. 2</xref>). Statistical analysis shows that the variability is more robust than expected from noise, for the majority of grid cells (<xref rid="fig5" ref-type="fig">Fig. 5</xref>). This was despite the fact that we used a very conservative criteria for assessing whether a grid cell was consistent enough to be included in the analysis. Our comparison of within- and between-cell grid property variability was key to our argument, as it was for previous work using it to study the robustness of differences in peak grid field firing rates [<xref ref-type="bibr" rid="c41">41</xref>]. The absence of its use in the characterization of distribution of <italic>λ</italic> and <italic>θ</italic> [<xref ref-type="bibr" rid="c30">30</xref>] may be why the consistency of this heterogeneity was not identified until now. We find that our conclusion holds when performing a number of control experiments (<xref rid="figS1" ref-type="fig">Figs. S1</xref>, <xref rid="figS4" ref-type="fig">S4</xref>–<xref rid="figS7" ref-type="fig">S7</xref>) and whether we treat each grid field independently or take the average across grid fields (<xref rid="fig3" ref-type="fig">Figs. 3</xref>, <xref rid="fig4" ref-type="fig">4</xref>). This challenges the assumption that the variability observed in the grid orientation and spacing is attributable solely to measurement noise.</p>
<p>We find that normative recurrent neural network models that develop grid cells when optimized to perform path integration [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c10">10</xref>] develop similar amounts of grid property variability (<xref rid="figS8" ref-type="fig">Fig. S8</xref>). This illustrates the ability of these RNN models to capture aspects of grid cell properties that have not been previously studied, and may provide another explanation for why these models have greater similarity to real MEC recordings than other models [<xref ref-type="bibr" rid="c38">38</xref>]. Probing these computational models in greater depth may enable a more detailed understanding of the observed grid property heterogeneity.</p>
<p>We hypothesized that this variability may be used to increase the fidelity at which individual grid modules can encode local space. This idea is consistent with a large body of literature showing that heterogeneity in the responses of populations of neurons increases the robustness of encoding [<xref ref-type="bibr" rid="c49">49</xref>–<xref ref-type="bibr" rid="c52">52</xref>]. We find, in noisy synthetic populations of grid cells, that a level of variability in grid properties similar to what is quantified in the real data can be sufficient to accurately decode information of local space (<xref rid="fig6" ref-type="fig">Fig. 6</xref>). This benefit is increased with larger numbers of grid cells in the population (<xref rid="fig6" ref-type="fig">Fig. 6D</xref>) and is observed over a range of values for the underlying variability (<xref rid="fig6" ref-type="fig">Fig. 6F</xref>). We find that the improvement was most pronounced in modules with small grid spacing (<xref rid="figS9" ref-type="fig">Fig. S9A</xref>), although larger modules can see a decrease in decoding error for amounts of variability consistent with was was found in the analyzed data (<xref rid="figS9" ref-type="fig">Fig. S9B</xref>).</p>
<p>We note that our results are additionally aligned with recent findings of heterogeneity in maximal firing rates across individual grid fields [<xref ref-type="bibr" rid="c39">39</xref>–<xref ref-type="bibr" rid="c41">41</xref>] and grid sheering [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c42">42</xref>]. Our work further demonstrates that, even in the absence of these perturbations, individual grid modules may encode considerably more local spatial information than previously believed.</p>
<p>Finally, models of the formation of orientation maps in visual cortex have demonstrated that slight angular offsets of retinal mosaics, along which retinal receptive fields are organized, can generate complex patterns [<xref ref-type="bibr" rid="c53">53</xref>] similar to those found in visual cortex orientation maps [<xref ref-type="bibr" rid="c54">54</xref>]. Our results indicate that grid cells in MEC may take advantage of a similar computational principle, suggesting that mosaic patterns might be a broadly utilized feature of neural coding [<xref ref-type="bibr" rid="c55">55</xref>].</p>
<sec id="s3a">
<title>Limitations</title>
<p>While the data set we analyzed [<xref ref-type="bibr" rid="c34">34</xref>] represents an advance in the ability to simultaneously record from tens to hundreds of putative grid cells, across grid modules, the MEC remains a challenging brain region to access for large-scale neurophysiological experiments. Indeed, with our conservative inclusion criteria, we were ultimately limited by having only 420 grid cells included in our analysis. Future work can perform more detailed and complete characterizations of grid property heterogeneity, as new neurotechnologies that enable larger yield of grid cells are developed [<xref ref-type="bibr" rid="c56">56</xref>, <xref ref-type="bibr" rid="c57">57</xref>].</p>
<p>Our decoding analysis, while demonstrating the possibility that variability in grid properties can be used by individual grid modules to enhance the encoding of local spatial information, made several simplifying assumptions, including: (1) independent Poisson noise for neural activity, (2) linear decoding, and (3) normal distribution of grid properties. Although comparison to real data showed that these assumptions are reasonable (<xref rid="fig6" ref-type="fig">Fig. 6E</xref>), future work can assess the extent to which these restrictions can be lifted (e.g., to incorporate correlated neural noise), while still enabling individual grid modules to have low decoding error.</p>
</sec>
<sec id="s3b">
<title>Open questions</title>
<p>The heterogeneity in grid properties we characterize motivates the investigation of several new lines of research. Because these are directions that we believe to be fruitful for the field as a whole, we outline them below, with our hypotheses for possible answers.</p>
<sec id="s3b1">
<title>Q1: How does grid property variability affect continuous attractor network structure?</title>
<p>Continuous attractor network models of grid cells [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c16">16</xref>] enjoy considerable experimental support [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c30">30</xref>–<xref ref-type="bibr" rid="c34">34</xref>], making them one of the “canonical” models in neuroscience. However, these models make use of the assumption that all the grid cells in a given module have the same grid orientation and spacing to simplify the network connectivity. In particular, by assuming equal grid orientation and spacing, it becomes possible to arrange grid cells in a two-dimensional space spanned by their phases (i.e., a “neural sheet” [<xref ref-type="bibr" rid="c16">16</xref>]). Neurons close in this space are wired with excitatory connections and neurons far in this space are wired with inhibitory connections. As the data set we analyzed was found by others to provide strong support for the basic predictions of continuous attractor networks (i.e., toroidal topology of the activity manifold) [<xref ref-type="bibr" rid="c34">34</xref>], we do not view our results as directly challenging these models. That the RNN models we investigate (<xref rid="figS8" ref-type="fig">Fig. S8</xref>) are explicitly trained to perform path integration (and achieve highly accurate performance [<xref ref-type="bibr" rid="c12">12</xref>]) supports our hypothesis that variability in grid properties does not necessarily destroy the continuous attractor or path integration capabilities of the MEC. Understanding how this is possible is an exciting future direction, and use of geometric [<xref ref-type="bibr" rid="c58">58</xref>] and dynamical systems based tools [<xref ref-type="bibr" rid="c59">59</xref>–<xref ref-type="bibr" rid="c61">61</xref>] may shed new light on this. We hypothesize that the degree in variability of grid spacing and orientation may strike a balance between being small enough to keep the continuous attractor network structure stable, but large enough to enable encoding of local information of space.</p>
</sec>
<sec id="s3b2">
<title>Q2: What causes grid property variability?</title>
<p>A natural direction to address is identifying the source of the heterogeneity in grid cell properties we observe. One hypothesis is that this could be driven by “defects” in the specific connectivity pattern that is needed for a continuous attractor. This could be due to synaptic connections between grid cells and other functional classes of neurons in the MEC, such as border [<xref ref-type="bibr" rid="c62">62</xref>], band [<xref ref-type="bibr" rid="c63">63</xref>], and non-spatial [<xref ref-type="bibr" rid="c39">39</xref>] cells. Because the path integrating RNN models have been found to develop analogous responses to these classes [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c64">64</xref>–<xref ref-type="bibr" rid="c66">66</xref>], as well as capture general properties of MEC activity [<xref ref-type="bibr" rid="c38">38</xref>], we examined the affect of “connectivity noise” in RNNs. In particular, we swept across different strengths of weight decay regularization, which controls the amount of sparsity in the recurrent layer. We find that smaller weight decay (and thus, denser connectivity between units) led to greater variability in grid properties (<xref rid="figS8" ref-type="fig">Fig. S8E, F</xref>). Increasing the weight decay (and thus, enforcing sparser connectivity) led to a reduction of grid property variability (<xref rid="figS8" ref-type="fig">Fig. S8E, F</xref>). This supports the hypothesis that the heterogeneity in grid properties we observe may be due to “non-perfect” connectivity.</p>
<p>Alternatively, the coupling between hippocampus and MEC, which has been shown to lead to variability in grid field firing rates (as experimentally observed) [<xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c67">67</xref>], may lead to differences in grid orientation and spacing. In particular, a previous computational model that learned grid cells from place cell input, using non-negative principal component analysis, has shown that place field width affects grid cell orientation and spacing [<xref ref-type="bibr" rid="c6">6</xref>]. Heterogeneity in the spatial coding properties of place cells has been found along the transverse axis of CA3 [<xref ref-type="bibr" rid="c68">68</xref>–<xref ref-type="bibr" rid="c70">70</xref>], suggesting there may be a systematic differences in the place field widths of hippocampal inputs to MEC grid cells. Further, it was shown that this place-to-grid cell computational model has a linear relationship between place field width and grid spacing, and a non-monotonic relationship between place field width and grid orientation [<xref ref-type="bibr" rid="c6">6</xref>]. These may explain why we find stronger average variability in grid spacing than grid orientation (<xref rid="fig5" ref-type="fig">Fig. 5A, B</xref>).</p>
</sec>
<sec id="s3b3">
<title>Q3: How does grid property variability shape hippocampal representations?</title>
<p>The projections from MEC to hippocampus suggest that the variability in grid properties may influence hippocampal representations (even if grid cells do not comprise the majority of its inputs [<xref ref-type="bibr" rid="c39">39</xref>]). We consider two possible ways in which this may happen. First, given that grid cells have been reported to maintain their grid spacing and orientation across exposures to new environments, while undergoing a change in their grid phase [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c71">71</xref>], the integration across multiple modules has been necessary to explain place field remapping. However, grid phase plays an important role in generating the specific complex interference patterns that emerge when considering the joint activity of grid cells with variable grid properties (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). Thus the reported changes in phase may be sufficient to generate large (and seemingly random) changes in local spatial information conveyed by grid cells to hippocampal cells. This could drive additional changes in the local spatial information projected to hippocampus, as well as explain the significant differences in correlation structure between CA1 neurons across different environments [<xref ref-type="bibr" rid="c72">72</xref>].</p>
<p>And second, recent work on hippocampal place field drift [<xref ref-type="bibr" rid="c73">73</xref>–<xref ref-type="bibr" rid="c77">77</xref>] has demonstrated that there is a significant change in the place field location across time, especially in CA1. One possible source of this phenomenon is the reported instability of dendritic spines on the apical dendrites of CA1 place cells [70, 78– 80], ostensibly leading to changes in the MEC inputs to these neurons. However, if grid cells across multiple modules are necessary for local spatial information, turnover in synaptic input is unlikely to cause large changes in the spatial preferences of CA1 neurons, as integration over several scales should provide stable encoding properties. In contrast, different subpopulations of grid cells with variable grid properties can lead to differences in the local spatial information encoded by their joint activity, even if they come from a single module, possibly influencing the spatial preferences of CA1 place cells.</p>
</sec>
<sec id="s3b4">
<title>Q4: Why are there multiple modules?</title>
<p>Given that our results demonstrate the ability of single grid modules to encode information about local space – a feat previously believed to be possible only if activity from multiple grid modules was integrated together – why is does MEC have multiple modules? While the variability removes the necessity for encoding local space with multiple modules, higher fidelity representations is achievable by integrating across multiple modules (<xref rid="figS10" ref-type="fig">Fig. S10</xref>). In addition, the use of grid cells beyond spatial navigation [<xref ref-type="bibr" rid="c81">81</xref>–<xref ref-type="bibr" rid="c87">87</xref>], where hierarchical representations are important [<xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>], may be a sufficient implicit bias for the formation of multiple modules [<xref ref-type="bibr" rid="c88">88</xref>]. In particular, encoding information at multiple distinct scales is critical for multi-scale reasoning, a cognitive function grid cells may support.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Electrophysiology recordings</title>
<p>The neural activity analyzed in this paper comes from a publicly available data set, which has previously been described in detail [<xref ref-type="bibr" rid="c34">34</xref>]. We provide brief summary of the methodology and the experimental paradigms used during the recordings.</p>
<p>Three male rats (Long Evans – Rats Q, R, and S) were implanted with Neuropixels silicon probes [<xref ref-type="bibr" rid="c89">89</xref>, <xref ref-type="bibr" rid="c90">90</xref>]. These probes were targeted at the MEC-parasubiculum region and the surgery was performed as described previously [<xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c90">90</xref>]. After three hours of recovery, recordings were performed.</p>
<p>In the recordings analyzed, the rats foraged for randomly dispersed corn puffs in a 1.5 × 1.5 m<sup>2</sup> square open field arena, with walls of height 50 cm. The rats were familiar with the environment and task, having trained 10–20 times prior to the implantation. The rats were food restricted to motivate their foraging, being kept at a minimum of 90% of their original body weight (300–500 grams).</p>
<p>All procedures in the original study were approved by the Norwegian Food and Safety Authority and done in accordance with the Norwegian Animal Welfare Act and the European Convention for the Protection of Vertebrate Animals used for Experimental and Other Scientific Purposes.</p>
</sec>
<sec id="s4b">
<title>Electrophysiology post-processing</title>
<p>The neural activity analyzed in this paper was post-processed, before made publicly available. We describe, in brief, the post-processing performed [<xref ref-type="bibr" rid="c34">34</xref>], as well as the post-processing we performed on the downloaded data.</p>
<p>Spike sorting, via KiloSort 2.5 [<xref ref-type="bibr" rid="c90">90</xref>], was applied to the data recorded from the Neuropixel probes. Individual units were deemed putative cells if their average spike rate was in the range of 0.5–10Hz, and 99% of their interspike intervals were greater than 2 ms.</p>
<p>For each putative cell, rate maps were constructed by averaging the activity at binned spatial positions in the open field arena. This raw rate map was smoothed, using a Gaussian kernel. The autocorrelation of these rate maps were computed, and a grid score calculated, as described previously [<xref ref-type="bibr" rid="c46">46</xref>].</p>
<p>From the downloaded spike trains, we constructed rate maps and autocorrelograms in a similar manner, using code made publicly available [<xref ref-type="bibr" rid="c8">8</xref>] (<xref rid="fig2" ref-type="fig">Fig. 2</xref>).</p>
</sec>
<sec id="s4c">
<title>Grid module classification</title>
<p>The public data set we analyzed [<xref ref-type="bibr" rid="c34">34</xref>] contained the module identity of all putative grid cells. In brief, these module identities were assigned by first projecting the 2D autocorrelogram of every recorded unit onto a 2D space using the non-linear dimensionality reduction algorithm UMAP [<xref ref-type="bibr" rid="c91">91</xref>]. Then, the DBSCAN algorithm [<xref ref-type="bibr" rid="c92">92</xref>] was used to cluster the units, based on their position in the dimensionally reduced space. Cluster membership served as the basis for grid module classification, with the largest cluster being removed as it was found to not contain spatially selective autocorrelograms. This non-supervised module assignment yielded clusters of high grid scores and similar grid spacing and orientation within each cluster-module. More details can be found in Gardner et al. (2022) [<xref ref-type="bibr" rid="c34">34</xref>]. We performed no further analysis regarding module identity.</p>
</sec>
<sec id="s4d">
<title>Computing grid score, orientation and spacing from the spatial autocorrelogram</title>
<p>Grid spacing and grid orientation were computed according to standard methods described in detail previously [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c93">93</xref>]. Briefly, the goal of the procedure is to identify the location of the six nearest fields in the spatial autocorrelogram (SAC) (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). This was achieved by performing the following steps. First, the SAC was smoothed using a 2D Gaussian filter with <italic>σ</italic> = 1. Second, the center of the SAC was excluded by applying a mask. Because the size of the SAC’s central peak changes for every module, the radius of such mask was re-computed for each module. Third, we thresholded the SAC by applying an extended-maxima transform <monospace>ndimage.maximum_filter</monospace>. Fourth, we identified the center of every field by using the function <monospace>scipy.stats.find_peaks</monospace>.</p>
<p>Once the peaks of every field had been found, we computed the location of every peak in polar coordinates. We then selected the 6 peaks that were closest to the center of the SAC, based on the computed radial components. Because every SAC is symmetric, we considered for further analysis the 3 peaks closest to the X axis in angular distance (Axis 1, 2, and 3 [<xref ref-type="bibr" rid="c47">47</xref>]). Grid spacing was computed as the arithmetic mean of the radial component of the 3 peaks (except when each peak was analyzed separately – <xref rid="fig4" ref-type="fig">Fig. 4</xref>). Given that the SAC dimensions are twice of that of the real arena, we multiplied the SAC radial mean by a factor of 2. Grid orientation was computed as the angular mean of orientations (relative to the x-axis) of the three peaks (except when each peak was analyzed separately – <xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p>
<p>In order to ensure that subsequent analysis was performed only on cells whose SAC’s could be well described by hexagonal structure, we imposed the following constraints: (1) the relative angle between two peaks could not be <italic>&lt;</italic> 30°; (2) the relative angle between two peaks could not be <italic>&gt;</italic> 90°; (3) the values for grid spacing between the peaks could not be substantially different (the ratio of spacings between any two peaks must be <italic>&gt;</italic> 0.5 and <italic>&lt;</italic> 2). Cells that did not meet these criteria were determined to have “Poor grid fit” and were rejected from all subsequent analysis. The percentage of all cells that were removed by this inclusion criteria is shown in <xref rid="figS2" ref-type="fig">Fig. S2</xref>. We note that cells from one of the nine modules in the publicly available data set that we analyzed had 98.4% of cells rejected by this criteria (recording identifier – R23). We therefore did not include it in any subsequent analysis.</p>
<p>To compute the grid score of recorded MEC cells, we made use of previously published code [<xref ref-type="bibr" rid="c8">8</xref>], that is based on metrics that have become standards in quantifying grid cell properties[<xref ref-type="bibr" rid="c46">46</xref>]. For analysis of the distribution of grid properties (<xref rid="fig2" ref-type="fig">Fig. 2E, F</xref>), we included only grid cells with grid scores greater than 0.85. This was done to demonstrate that variability was present even in cells that exhibit robust grid cell properties. In the subsequent analyses, an alternative criteria is used, which considers the reliability of the grid responses (see below).</p>
</sec>
<sec id="s4e">
<title>Computing spacing from the rate maps</title>
<p>In order to characterize the spacing with a method that is less susceptible to the effects of having grid fields at the boundaries of the environment (as the SAC method could be), we compute the spacing using the rate maps of individual grid cells (<xref rid="figS1" ref-type="fig">Fig. S1A</xref>). Once the rate map was computed, we smoothed it with a Gaussian filter, setting <italic>σ</italic> = 1.5. Then, using the function <monospace>scikit-image.feature.peak_local_max</monospace> we extract the position of the center of each firing field. Because our goal with this analysis is to show that the fields in the border do not impact our analysis with the SAC, we restrict ourselves to the 3 peaks that are closest to the center of the arena (<xref rid="figS1" ref-type="fig">Fig. S1A</xref>). We compute the distances between those three peaks to each other and report the spacing as the average of the distances measured.</p>
</sec>
<sec id="s4f">
<title>Within and between cells splits</title>
<p>To characterize the within- and between-cell variability of grid spacing and orientation, we employed the following approach. First, we split the data into bins of fixed length (30 seconds). From this, we randomly assigned each interval to one of two blocks (denoted as blocks <italic>A</italic> and <italic>B</italic>), with exactly half the total number of intervals in each block. For each grid cell, we computed the grid spacing [<inline-formula><inline-graphic xlink:href="582373v3_inline21.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline22.gif" mime-subtype="gif" mimetype="image"/></inline-formula>] and orientation [<inline-formula><inline-graphic xlink:href="582373v3_inline23.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline24.gif" mime-subtype="gif" mimetype="image"/></inline-formula>], from the data in each block. The within-cell differences of grid spacing and orientation was determined as <inline-formula><inline-graphic xlink:href="582373v3_inline25.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and<inline-formula><inline-graphic xlink:href="582373v3_inline26.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. Each grid cell’s properties were also compared those of another grid cell, with the match being made using random sampling without replacement. These comparisons were determined as <inline-formula><inline-graphic xlink:href="582373v3_inline27.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline28.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. This process was repeated 100 times (referring to each iteration as a “shuffle”), per recording. Examples of splits of the data, for different shuffles, is schematically illustrated in <xref rid="fig3" ref-type="fig">Fig. 3A</xref>). The resulting distributions of Δ<italic>θ</italic><sub>within</sub>, Δ<italic>λ</italic><sub>within</sub>, Δ<italic>θ</italic><sub>between</sub>, and Δ<italic>λ</italic><sub>between</sub>, across grid cells and splits of the data were then compared. If the within-cell variability in grid spacing and orientation was smaller than between-cell variability, we concluded that the variability of grid cell properties was a robust feature of the data and not due to noise.</p>
<p>When performing this shuffle analysis, we found that some cells, despite having a good grid fit when all the data was considered, did not have SACs that were well described by hexagonal structure when the data was split in half. We viewed this a manifestation of unreliable grid coding and a possible confound in our quantification of variability. As such, we introduced a new inclusion criteria (replacing that of requiring a grid score of <italic>&gt;</italic> 0.85), only considering cells that had poor grid fits on <italic>&lt;</italic> 5% of all shuffles. The percentage of all cells that were removed by this inclusion criteria is shown in <xref rid="figS2" ref-type="fig">Fig. S2</xref>. In general, we found that cells with high grid score were reliable, although there were exceptions. Additionally, we found that size of the bin used for splitting the data did not significantly affect the percent of cells with good grid fits (passed the prior inclusion criteria) that were considered reliable (<xref rid="figS3" ref-type="fig">Fig. S3</xref>).</p>
</sec>
<sec id="s4g">
<title>Synthetic grid cells</title>
<p>To study how variability in grid cell properties might endow the grid code with computational advantages, we generated synthetic grid cell rate maps, so that we could have complete control over the distribution of their properties. These synthetic grid cell rate maps were constructed as follows.</p>
<p>First, the lengths of each dimension the “arena” within which the simulated grid cells exist (<italic>L</italic><sub><italic>x</italic></sub> and <italic>L</italic><sub><italic>y</italic></sub>) were set. Then, for <italic>N</italic> ∈ ℕ grid cells, the grid spacing and orientation were sampled via <italic>λ</italic><sup>(<italic>i</italic>)</sup> ∼ 𝒩 (<italic>µ</italic><sub><italic>λ</italic></sub>, <italic>σ</italic><sub><italic>λ</italic></sub>) and <italic>θ</italic><sup>(<italic>i</italic>)</sup> ∼ 𝒩 (<italic>µ</italic><sub><italic>θ</italic></sub>, <italic>σ</italic><sub><italic>θ</italic></sub>) where 𝒩 (<italic>µ, σ</italic>) is a normal distribution with mean <italic>µ</italic> and variance <italic>σ</italic><sup>2</sup>. Grid phase was sampled as <italic>ϕ</italic><sup>(<italic>i</italic>)</sup> ∼ 𝒰 ([0, <italic>L</italic><sub><italic>x</italic></sub>] [0, <italic>L</italic><sub><italic>y</italic></sub>]), where <italic>ϕ</italic> is a two dimensional vector, with first component uniformly sampled from [0, <italic>L</italic><sub><italic>x</italic></sub>] and second component uniformly sampled from [0, <italic>L</italic><sub><italic>y</italic></sub>]. To construct a population with no variability in grid properties (to use as a control), we set <italic>σ</italic><sub><italic>λ</italic></sub> = 0 m. and <italic>σ</italic><sub><italic>θ</italic></sub> = 0°.</p>
<p>For each grid cell, we generated idealized grid responses by summing three two-dimensional sinusoids [<xref ref-type="bibr" rid="c14">14</xref>], such that the activity at <bold>x</bold> = (<italic>x, y</italic>) ∈ [−<italic>L</italic><sub><italic>x</italic></sub><italic>/</italic>2, <italic>L</italic><sub><italic>x</italic></sub><italic>/</italic>2] ×[−<italic>L</italic><sub><italic>y</italic></sub><italic>/</italic>2, <italic>L</italic><sub><italic>y</italic></sub><italic>/</italic>2] is given by
<disp-formula id="eqn1">
<graphic xlink:href="582373v3_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula><inline-graphic xlink:href="582373v3_inline29.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the maximal firing rate, <italic>R</italic>(<italic>θ</italic><sub><italic>i</italic></sub>) is the two-dimensional rotation matrix, with rotation <italic>θ</italic><sub><italic>i</italic></sub>, and <bold>k</bold><sub> <italic>j</italic></sub> are the wave vectors with 0°, 0 60 <italic>°</italic>, and angular 120<sup>°</sup> differences [i.e., <bold>k</bold><sub>1</sub> = (1, 0)<sup><italic>T</italic></sup>,<inline-formula><inline-graphic xlink:href="582373v3_inline30.gif" mime-subtype="gif" mimetype="image"/></inline-formula>]. To match the recorded neural data, where individual grid cells have distinct maximal firing rates,<inline-formula><inline-graphic xlink:href="582373v3_inline31.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. We enforced <inline-formula><inline-graphic xlink:href="582373v3_inline32.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to be within [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c30">30</xref>], by setting any sampled values outside of this range to the boundary values (i.e., 2 or 30).</p>
<p>To determine the extent to which local spatial information can be decoded from the activity of populations of grid cells with different degrees of variability in their grid properties, we performed the following analysis.</p>
<p>We generated noisy synthetic spike rates of <italic>N</italic> grid cells by assuming a Poisson process and sampling using the idealized rate maps (<xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>). More concretely, the activity of grid cell <italic>i</italic> at position (<italic>x, y</italic>) was assumed to be a random variable with a Poisson distribution, whose mean was <italic>X</italic><sup>(<italic>i</italic>)</sup>(<italic>x, y</italic>). Thus, the probability of observing <inline-formula><inline-graphic xlink:href="582373v3_inline33.gif" mime-subtype="gif" mimetype="image"/></inline-formula> spikes, at position (<italic>x, y</italic>), is given by
<disp-formula id="eqn2">
<graphic xlink:href="582373v3_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
</p>
</sec>
<sec id="s4h">
<title>Linear decoding synthetic data</title>
<p>For a given resolution of the arena, we generated <inline-formula><inline-graphic xlink:href="582373v3_inline34.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, where <italic>j</italic> = 1, …, 10. That is, we constructed 10 noisy rate maps. We performed cross-validated decoding by averaging across 9 of the 10 rate maps, to get an average rate map <inline-formula><inline-graphic xlink:href="582373v3_inline35.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. For sake of simplicity, consider <inline-formula><inline-graphic xlink:href="582373v3_inline36.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. To decode local position from the held out noisy rate map [e.g. <inline-formula><inline-graphic xlink:href="582373v3_inline37.gif" mime-subtype="gif" mimetype="image"/></inline-formula>], we multiplied the activity at each position by all the positions in the average rate map, taking the sum and assigning the decoded position as that with the largest value. The Euclidean distance between the decoded position and the true position is considered the error. This was performed 10 times (holding out each rate map once) and the average error across all positions in the environment was then averaged across all 10 of the validation splits.</p>
</sec>
<sec id="s4i">
<title>Linear decoding experimental data</title>
<p>To decode the electrophysiological data [<xref ref-type="bibr" rid="c34">34</xref>], we sampled subpopulations of grid cells from the <italic>N</italic> = 82 units (recording ID R12) that passed the criteria described previously. For each subpopulation, we split the recorded activity into 10 evenly spaced temporal bins, and constructed average ratemaps for each bin. This is consistent with what was done in decoding the synthetic data. All ratemaps, for each cell, were normalized by the maximal activity. We then randomly chose 5 of the 10 time bins to serve as the “train” data, the remaining 5 served as the “test” data. The rate maps were averaged and then the linear decoder was applied. We sampled 10 different choices of splitting the data and 25 choices of subpopulation.</p>
</sec>
<sec id="s4j">
<title>Recurrent neural network model</title>
<p>Code used for training and evaluating path integrating recurrent neural network (RNNs) [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>] was pulled from <ext-link ext-link-type="uri" xlink:href="https://github.com/ganguli-lab/grid-pattern-formation">https://github.com/ganguli-lab/grid-pattern-formation</ext-link>. We used the same hyperparameters as are present on the repository (see <xref rid="tbl1" ref-type="table">Table 1</xref>), except when examining the effect of weight regularization on grid property variability, in which case we trained RNNs with weight decay magnitude from {10<sup><italic>−</italic>6</sup>, 10<sup><italic>−</italic>5</sup>, 10<sup><italic>−</italic>4</sup>, 2.5 ·10<sup><italic>−</italic>4</sup> }. The greater the value, the greater the enforced sparsity. These were chosen as they had previously been found to lead to good path integration performance and have high grid scores [<xref ref-type="bibr" rid="c12">12</xref>]. We independently verified that this was true for the resulting RNNs. We computed the grid score, spacing, and orientation using similar code implementations as was used for the neural data. For each value of weight decay, we trained three independent RNNs, using different random seeds.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><title>Parameters used to train RNNs on path integration.</title>
<p>See Sorscher et al. (2019) and (2022) [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>] more information on these parameters.</p></caption>
<graphic xlink:href="582373v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
</sec>
</body>
<back>
<sec id="s5">
<title>Supplemental material</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1:</label>
<caption><title>Variability in grid spacing within a single module exists when computing <italic>λ</italic> directly from the rate maps.</title>
<p>(A) (Left) Example grid cell rate maps from the same module (recording ID R12) with overlaid triangles, corresponding to the spacing between each of the three most central peaks. Grid spacing is computed as the average of the three lengths. (Right) To aid comparison, the triangles are enlarged (with their relative size fixed) and overlaid. Note that these cells are the same ones plotted in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>. (B) Distribution of grid spacing computed using the SAC and the rate maps. (C) The grid spacing of all grid cells, from recording R12, computed using the SAC and the rate map. Red dashed line is the linear regression fit with <italic>R</italic><sup>2</sup> and <italic>p</italic>-value reported above.</p></caption>
<graphic xlink:href="582373v3_figS1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2:</label>
<caption><title>Accepted and rejected cells across all grid modules.</title>
<p>(Left) The percent of all cells, across all modules, that were rejected by our inclusion criteria. Cells that were rejected as not having SACs, computed with all data, that were well described by hexagonal structure (“Rejected: Poor grid fits”) are shown in red. Cells that were rejected as not reliably having SACs, computed from splits of the data, that were well described by hexagonal structure (“Rejected: Unreliable”) are shown in yellow. Cells that met these criteria (“Accepted”) are shown in blue. (Middle) Grid score of all cells, with coloring denoting whether they were accepted or rejected. Dashed gray line denotes population mean. (Right) The number of splits of the data (out of 100) that cells had SAC’s with poor grid fits, as a function of each cell’s grid score.</p></caption>
<graphic xlink:href="582373v3_figS2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3:</label>
<caption><title>Bin length does not affect the percent of cells accepted for analysis.</title>
<p>The percent of cells with good grid fits (i.e., those cells that do not get rejected for having “poor grid fits”) that are accepted by not being deemed unreliable, as a function of the size of the bins used in the shuffle analysis. All modules are plotted (each line is colored based on its recording ID).</p></caption>
<graphic xlink:href="582373v3_figS3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4:</label>
<caption><title>Average field width does not play a significant role in explaining the between-cell variability in spacing.</title>
<p>(A) Schematic illustration of how the average field width was approximated. The first minima along 1-dimensional slices through the center of the SAC are used to estimate the field width. (B) An example ratemap with the four strongest grid fields overlaid with circles having the diameter of the estimated field width. (C) Between-cell spacing variability <inline-formula><inline-graphic xlink:href="582373v3_inline48.gif" mime-subtype="gif" mimetype="image"/></inline-formula> as a function of average field width for all cells from recording R12 that passed our inclusion criteria. Red line is the linear regression fit with <italic>R</italic><sup>2</sup> and <italic>p</italic>-value reported. <italic>R</italic><sup>2</sup>, for all other modules: Q1 = 0.06; Q2 = 0.40; R11 = 0.04; R13 = 0.03; R21 = 0.12; R22 = 0.12; S1 = 0.00.</p></caption>
<graphic xlink:href="582373v3_figS4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS5" position="float" fig-type="figure">
<label>Figure S5:</label>
<caption><title>Between-cell variability does not significantly change across recording time.</title>
<p>(A) Schematic illustrating the sliding window analysis. Red line denotes half of data used to compute the grid properties. (B)-(C) Between-cell variability (estimated by computing the standard deviation of the population <italic>θ</italic> and <italic>λ</italic> values) as a function of sliding window number. The slope estimated from a linear regression fit is reported. Slope in <italic>θ</italic> and <italic>λ</italic>, for all other modules: Q1 = 0.015 and 0.028; Q2 = −0.039 and −0.003; R11 = −0.001 and −0.005; R13 = 0.005 and −0.000; R21 = 0.014 and 0.020; R22 = 0.044 and 0.016; S1 = 0.034 and 0.014.</p></caption>
<graphic xlink:href="582373v3_figS5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure S6:</label>
<caption><title>Variability in grid spacing and orientation is not significantly affected by location of arena boundaries.</title>
<p>(A) Example spike map for one cell (from R12) as increasingly more of the arena is removed. Computed <italic>θ</italic> and <italic>λ</italic> are reported above the spike maps. (B) SACs corresponding to the spike maps. (C)-(D) Between-cell variability (estimated by computing the standard deviation of the population <italic>θ</italic> and <italic>λ</italic> values) as a function of how much of the arena was removed from our analysis for all modules.</p></caption>
<graphic xlink:href="582373v3_figS6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS7" position="float" fig-type="figure">
<label>Figure S7:</label>
<caption><title>Conjunctive head direction grid cells do not significantly affect the variability in grid orientation and spacing</title>
<p>(A)–(B) Same as <xref rid="fig2" ref-type="fig">Fig. 2D</xref>–E, but with conjunctive and non-conjunctive cells differentiated. Note that the slight difference in position of the dots between this figure and <xref rid="fig3" ref-type="fig">Fig. 3</xref> is due to re-computing with a different random seed. (C)–(D) Distribution of between-cell variability in orientation and spacing, when computing <inline-formula><inline-graphic xlink:href="582373v3_inline49.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline50.gif" mime-subtype="gif" mimetype="image"/></inline-formula> with and without the conjunctive cells. To allow paired comparisons, the variability is calculated for only the non-conjunctive cells, but either includes (grey) or does not include (red) variability comparisons to the conjunctive cells. A paired <italic>t</italic> -test is used to determine whether the distributions are significantly different (<italic>p</italic>-values reported in C–D). <italic>p</italic>-value in <italic>θ</italic> and <italic>λ</italic>, for all other modules: Q1 = 0.65 and 0.78; Q2 = 0.86 and 0.92; R11 = 0.13 and 0.75; R13 = 0.99 and 0.97; R21 = 0.29 and 0.98; R22 = 0.87 and 0.95; S1 = 0.07 and 0.77.</p></caption>
<graphic xlink:href="582373v3_figS7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS8" position="float" fig-type="figure">
<label>Figure S8:</label>
<caption><title>Path integrating recurrent neural networks (RNNs) that develop grid cells exhibit variability in spacing and orientation that scales with recurrent weight sparsity.</title>
<p>(A)–(B) Spatial autocorrelogram (SAC) overlaid for two pairs of units in the recurrent layer with high grid score; one pair with different <italic>θ</italic> and similar <italic>λ</italic> (A) and the other with similar <italic>θ</italic> and different <italic>λ</italic> (B). (C)–(D) Distribution of <italic>θ</italic> (C) and <italic>λ</italic> (D) across all units in the recurrent layer with grid score <italic>&gt;</italic> 0.85 (N = 326). Two and four units were excluded for visualization from (C) and (D), respectively, since they were outside the plotting axes. (E)–(F) Standard deviation of <italic>θ</italic> (E) and <italic>λ</italic> (F) distributions, <italic>σ</italic><sub><italic>θ</italic></sub> and <italic>σ</italic><sub><italic>λ</italic></sub> respectively, across different amounts of weight decay used in training. The circle markers indicate the mean across three independently trained networks and the lines indicate the minimum and maximum of all three networks.</p></caption>
<graphic xlink:href="582373v3_figS8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS9" position="float" fig-type="figure">
<label>Figure S9:</label>
<caption><title>Variability in grid properties improves decoding of local space for grid modules with different mean grid spacings.</title>
<p>(A)–(B) Same as <xref rid="fig6" ref-type="fig">Fig. 6F</xref>, for <inline-formula><inline-graphic xlink:href="582373v3_inline51.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline52.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, respectively. <inline-formula><inline-graphic xlink:href="582373v3_inline53.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is set to 0°.</p></caption>
<graphic xlink:href="582373v3_figS9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="figS10" position="float" fig-type="figure">
<label>Figure S10:</label>
<caption><title>Variability in grid properties improves decoding of local space for multiple modules, when the modules have integer multiple mean spacing.</title>
<p>(A)–(D) Same as <xref rid="fig6" ref-type="fig">Fig. 6D</xref>, when decoding from multiple modules. (A) <inline-formula><inline-graphic xlink:href="582373v3_inline54.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 70 cm and <inline-formula><inline-graphic xlink:href="582373v3_inline55.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 10°. (B) <inline-formula><inline-graphic xlink:href="582373v3_inline56.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 100 cm and <inline-formula><inline-graphic xlink:href="582373v3_inline57.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 10°. (C) <inline-formula><inline-graphic xlink:href="582373v3_inline58.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 65.0 cm and <inline-formula><inline-graphic xlink:href="582373v3_inline59.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 10° (D) <inline-formula><inline-graphic xlink:href="582373v3_inline60.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 98.4 cm and <inline-formula><inline-graphic xlink:href="582373v3_inline61.gif" mime-subtype="gif" mimetype="image"/></inline-formula>, 10°. The values in (A)–(B) were chosen such that <inline-formula><inline-graphic xlink:href="582373v3_inline62.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="582373v3_inline63.gif" mime-subtype="gif" mimetype="image"/></inline-formula>. The values in (C)–(D) were chosen to match those found in Rat 14257 from Stensola et al. (2012) [<xref ref-type="bibr" rid="c22">22</xref>].</p></caption>
<graphic xlink:href="582373v3_figS10.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<ack>
<title>Acknowledgments</title>
<p>We thank the members of the Goard Lab, Francisco Acosta, Spencer Smith, Caleb Kemere, Will Dorrell, and the DYNS graduate students for useful discussions surrounding this work. We thank Andreas Herz for suggesting the analysis present in <xref rid="figS1" ref-type="fig">Fig. S1</xref> and William Dorrell for suggesting the analysis present in <xref rid="figS9" ref-type="fig">Fig. S9C</xref>-D. We thank the eLife reviewers for their suggestions. This work was supported by grants to M.J.G. from NIH (R01 NS121919), NSF (1934288), and the Whitehall Foundation, and a grant to X.X.W. from NSF (2318065).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Torkel</given-names> <surname>Hafting</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Fyhn</surname></string-name>, <string-name><given-names>Sturla</given-names> <surname>Molden</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source>, <volume>436</volume>(<issue>7052</issue>):<fpage>801</fpage>–<lpage>806</lpage>, <year>2005</year>.</mixed-citation></ref>
    <ref id="c2"><label>[2]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Alexis</given-names> <surname>Guanella</surname></string-name> and <string-name><given-names>Paul FMJ</given-names> <surname>Verschure</surname></string-name></person-group>. <article-title>A model of grid cells based on a path integration mechanism</article-title>. In <conf-name>16th International Conference on Artificial Neural Networks (ICANN 2006), Athens, Greece, September 10-14, 2006</conf-name>, <fpage>740</fpage>–<lpage>749</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mark C</given-names> <surname>Fuhs</surname></string-name> and <string-name><given-names>David S</given-names> <surname>Touretzky</surname></string-name></person-group>. <article-title>A spin glass model of path integration in rat medial entorhinal cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>26</volume>(<issue>16</issue>):<fpage>4266</fpage>–<lpage>4276</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hugh T</given-names> <surname>Blair</surname></string-name>, <string-name><given-names>Adam C</given-names> <surname>Welday</surname></string-name>, and <string-name><given-names>Kechen</given-names> <surname>Zhang</surname></string-name></person-group>. <article-title>Scale-invariant memory representations emerge from moire interference between grid fields that produce theta oscillations: a computational model</article-title>. <source>Journal of Neuroscience</source>, <volume>27</volume>(<issue>12</issue>):<fpage>3211</fpage>–<lpage>3229</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jonathan J</given-names> <surname>Couey</surname></string-name>, <string-name><given-names>Aree</given-names> <surname>Witoelar</surname></string-name>, <string-name><given-names>Sheng-Jia</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Kang</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>Jing</given-names> <surname>Ye</surname></string-name>, <string-name><given-names>Benjamin</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>Rafal</given-names> <surname>Czajkowski</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name>, <string-name><given-names>Yasser</given-names> <surname>Roudi</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Recurrent inhibitory circuitry as a mechanism for grid formation</article-title>. <source>Nature neuroscience</source>, <volume>16</volume>(<issue>3</issue>):<fpage>318</fpage>–<lpage>324</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yedidyah</given-names> <surname>Dordek</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Soudry</surname></string-name>, <string-name><given-names>Ron</given-names> <surname>Meir</surname></string-name>, and <string-name><given-names>Dori</given-names> <surname>Derdikman</surname></string-name></person-group>. <article-title>Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis</article-title>. <source>Elife</source>, <volume>5</volume>:<elocation-id>e10094</elocation-id>, <year>2016</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Christopher J.</given-names> <surname>Cueva</surname></string-name> and <string-name><given-names>Xue-Xin</given-names> <surname>Wei</surname></string-name></person-group>. <article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title>. In <conf-name>International Conference on Learning Representations</conf-name>, <year>2018</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Andrea</given-names> <surname>Banino</surname></string-name>, <string-name><given-names>Caswell</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>Benigno</given-names> <surname>Uria</surname></string-name>, <string-name><given-names>Charles</given-names> <surname>Blundell</surname></string-name>, <string-name><given-names>Timothy</given-names> <surname>Lillicrap</surname></string-name>, <string-name><given-names>Piotr</given-names> <surname>Mirowski</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Pritzel</surname></string-name>, <string-name><given-names>Martin J</given-names> <surname>Chadwick</surname></string-name>, <string-name><given-names>Thomas</given-names> <surname>Degris</surname></string-name>, <string-name><given-names>Joseph</given-names> <surname>Modayil</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Vector-based navigation using grid-like representations in artificial agents</article-title>. <source>Nature</source>, <volume>557</volume>(<issue>7705</issue>):<fpage>429</fpage>–<lpage>433</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Simon Nikolaus</given-names> <surname>Weber</surname></string-name> and <string-name><given-names>Henning</given-names> <surname>Sprekeler</surname></string-name></person-group>. <article-title>Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity</article-title>. <source>Elife</source>, <volume>7</volume>:<elocation-id>e34560</elocation-id>, <year>2018</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben Sorscher</surname>, <given-names>Gabriel Mel</given-names></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, and <string-name><given-names>Samuel</given-names> <surname>Ocko</surname></string-name></person-group>. <article-title>A unified theory for the origin of grid cells through the lens of pattern formation</article-title>. <source>Advances in neural information processing systems</source>, <volume>32</volume>, <year>2019</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Mikail</given-names> <surname>Khona</surname></string-name>, <string-name><given-names>Sarthak</given-names> <surname>Chandra</surname></string-name>, and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>From smooth cortical gradients to discrete modules: spontaneous and topologically robust emergence of modularity in grid cells</article-title>. <source>bioRxiv</source>, <year>2022</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben Sorscher</surname>, <given-names>Gabriel C Mel</given-names></string-name>, <string-name><given-names>Samuel A</given-names> <surname>Ocko</surname></string-name>, <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name>, and <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name></person-group>. <article-title>A unified theory for the computational and mechanistic origins of grid cells</article-title>. <source>Neuron</source>, <year>2022</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Bruce L</given-names> <surname>McNaughton</surname></string-name>, <string-name><given-names>Francesco P</given-names> <surname>Battaglia</surname></string-name>, <string-name><given-names>Ole</given-names> <surname>Jensen</surname></string-name>, <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Path integration and the neural basis of the’cognitive map’</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>7</volume>(<issue>8</issue>):<fpage>663</fpage>–<lpage>678</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Trygve</given-names> <surname>Solstad</surname></string-name>, <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Gaute T</given-names> <surname>Einevoll</surname></string-name></person-group>. <article-title>From grid cells to place cells: a mathematical model</article-title>. <source>Hippocampus</source>, <volume>16</volume>(<issue>12</issue>):<fpage>1026</fpage>–<lpage>1031</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Edmund T</given-names> <surname>Rolls</surname></string-name>, <string-name><given-names>Simon M</given-names> <surname>Stringer</surname></string-name>, and <string-name><given-names>Thomas</given-names> <surname>Elliot</surname></string-name></person-group>. <article-title>Entorhinal cortex grid cells can map to hippocampal place cells by competitive learning</article-title>. <source>Network: Computation in Neural Systems</source>, <volume>17</volume>(<issue>4</issue>):<fpage>447</fpage>–<lpage>465</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name> and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Accurate path integration in continuous attractor network models of grid cells</article-title>. <source>PLoS computational biology</source>, <volume>5</volume>(<issue>2</issue>):<fpage>e1000291</fpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Licurgo</given-names> <surname>de Almeida</surname></string-name>, <string-name><given-names>Marco</given-names> <surname>Idiart</surname></string-name>, and <string-name><given-names>John E</given-names> <surname>Lisman</surname></string-name></person-group>. <article-title>The input–output transformation of the hippocampal granule cells: from grid cells to place fields</article-title>. <source>Journal of Neuroscience</source>, <volume>29</volume>(<issue>23</issue>):<fpage>7504</fpage>–<lpage>7512</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>John L</given-names> <surname>Kubie</surname></string-name> and <string-name><given-names>Steven E</given-names> <surname>Fox</surname></string-name></person-group>. <article-title>Do the spatial frequencies of grid cells mold the firing fields of place cells?</article-title> <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>13</issue>):<fpage>3860</fpage>–<lpage>3861</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Daniel</given-names> <surname>Bush</surname></string-name>, <string-name><given-names>Caswell</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Manson</surname></string-name>, and <string-name><given-names>Neil</given-names> <surname>Burgess</surname></string-name></person-group>. <article-title>Using grid cells for navigation</article-title>. <source>Neuron</source>, <volume>87</volume>(<issue>3</issue>):<fpage>507</fpage>–<lpage>520</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jake</given-names> <surname>Ormond</surname></string-name> and <string-name><given-names>Bruce L</given-names> <surname>McNaughton</surname></string-name></person-group>. <article-title>Place field expansion after focal mec inactivations is consistent with loss of fourier components and path integrator gain reduction</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>(<issue>13</issue>):<fpage>4116</fpage>–<lpage>4121</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Caitlin S</given-names> <surname>Mallory</surname></string-name>, <string-name><given-names>Kiah</given-names> <surname>Hardcastle</surname></string-name>, <string-name><given-names>Jason S</given-names> <surname>Bant</surname></string-name>, and <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name></person-group>. <article-title>Grid scale drives the scale and long-term stability of place maps</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>2</issue>):<fpage>270</fpage>–<lpage>282</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Hanne</given-names> <surname>Stensola</surname></string-name>, <string-name><given-names>Tor</given-names> <surname>Stensola</surname></string-name>, <string-name><given-names>Trygve</given-names> <surname>Solstad</surname></string-name>, <string-name><given-names>Kristian</given-names> <surname>Frøland</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>The entorhinal grid map is discretized</article-title>. <source>Nature</source>, <volume>492</volume>(<issue>7427</issue>):<fpage>72</fpage>–<lpage>78</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yi</given-names> <surname>Gu</surname></string-name>, <string-name><given-names>Sam</given-names> <surname>Lewallen</surname></string-name>, <string-name><given-names>Amina A</given-names> <surname>Kinkhabwala</surname></string-name>, <string-name><given-names>Cristina</given-names> <surname>Domnisoru</surname></string-name>, <string-name><given-names>Kijung</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>Jeffrey L</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name>, and <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name></person-group>. <article-title>A map-like micro-organization of grid cells in the medial entorhinal cortex</article-title>. <source>Cell</source>, <volume>175</volume>(<issue>3</issue>):<fpage>736</fpage>–<lpage>750</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Xue-Xin</given-names> <surname>Wei</surname></string-name>, <string-name><given-names>Jason</given-names> <surname>Prentice</surname></string-name>, and <string-name><given-names>Vijay</given-names> <surname>Balasubramanian</surname></string-name></person-group>. <article-title>A principle of economy predicts the functional architecture of grid cells</article-title>. <source>Elife</source>, <volume>4</volume>:<elocation-id>e08362</elocation-id>, <year>2015</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Martin</given-names> <surname>Stemmler</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Mathis</surname></string-name>, and <string-name><given-names>Andreas VM</given-names> <surname>Herz</surname></string-name></person-group>. <article-title>Connecting multiple spatial scales to decode the population activity of grid cells</article-title>. <source>Science Advances</source>, <volume>1</volume>(<issue>11</issue>):<fpage>e1500816</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name>, <string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name>, and <string-name><given-names>Ted</given-names> <surname>Brookings</surname></string-name></person-group>. <article-title>What grid cells convey about rat location</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>(<issue>27</issue>):<fpage>6858</fpage>–<lpage>6871</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sameet</given-names> <surname>Sreenivasan</surname></string-name> and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Grid cells generate an analog error-correcting code for singularly precise neural computation</article-title>. <source>Nature neuroscience</source>, <volume>14</volume>(<issue>10</issue>):<fpage>1330</fpage>–<lpage>1337</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alexander</given-names> <surname>Mathis</surname></string-name>, <string-name><given-names>Andreas VM</given-names> <surname>Herz</surname></string-name>, and <string-name><given-names>Martin</given-names> <surname>Stemmler</surname></string-name></person-group>. <article-title>Optimal population codes for space: grid cells outperform place cells</article-title>. <source>Neural computation</source>, <volume>24</volume>(<issue>9</issue>):<fpage>2280</fpage>–<lpage>2317</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>NM</given-names> <surname>Van Strien</surname></string-name>, <string-name><given-names>NLM</given-names> <surname>Cappaert</surname></string-name>, and <string-name><given-names>MP</given-names> <surname>Witter</surname></string-name></person-group>. <article-title>The anatomy of memory: an interactive overview of the parahippocampal–hippocampal network</article-title>. <source>Nature reviews neuroscience</source>, <volume>10</volume>(<issue>4</issue>):<fpage>272</fpage>–<lpage>282</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>KiJung</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>Michael A</given-names> <surname>Buice</surname></string-name>, <string-name><given-names>Caswell</given-names> <surname>Barry</surname></string-name>, <string-name><given-names>Robin</given-names> <surname>Hayman</surname></string-name>, <string-name><given-names>Neil</given-names> <surname>Burgess</surname></string-name>, and <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Specific evidence of low-dimensional continuous attractor dynamics in grid cells</article-title>. <source>Nature neuroscience</source>, <volume>16</volume>(<issue>8</issue>):<fpage>1077</fpage>–<lpage>1084</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Benjamin</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>Maria</given-names> <surname>Mørreaunet</surname></string-name>, and <string-name><given-names>Yasser</given-names> <surname>Roudi</surname></string-name></person-group>. <article-title>Correlations and functional connections in a population of grid cells</article-title>. <source>PLoS computational biology</source>, <volume>11</volume>(<issue>2</issue>):<fpage>e1004052</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard J</given-names> <surname>Gardner</surname></string-name>, <string-name><given-names>Li</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Tanja</given-names> <surname>Wernle</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Correlation structure of grid cells is preserved during sleep</article-title>. <source>Nature neuroscience</source>, <volume>22</volume>(<issue>4</issue>):<fpage>598</fpage>–<lpage>608</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sean G</given-names> <surname>Trettel</surname></string-name>, <string-name><given-names>John B</given-names> <surname>Trimper</surname></string-name>, <string-name><given-names>Ernie</given-names> <surname>Hwaun</surname></string-name>, <string-name><given-names>Ila R</given-names> <surname>Fiete</surname></string-name>, and <string-name><given-names>Laura Lee</given-names> <surname>Colgin</surname></string-name></person-group>. <article-title>Grid cell co-activity patterns during sleep reflect spatial overlap of grid fields during active behaviors</article-title>. <source>Nature neuroscience</source>, <volume>22</volume>(<issue>4</issue>):<fpage>609</fpage>–<lpage>617</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Richard J</given-names> <surname>Gardner</surname></string-name>, <string-name><given-names>Erik</given-names> <surname>Hermansen</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name>, <string-name><given-names>Nils A</given-names> <surname>Baas</surname></string-name>, <string-name><given-names>Benjamin A</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Toroidal topology of population activity in grid cells</article-title>. <source>Nature</source>, <volume>602</volume>(<issue>7895</issue>):<fpage>123</fpage>–<lpage>128</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Will</given-names> <surname>Dorrell</surname></string-name>, <string-name><given-names>Peter E.</given-names> <surname>Latham</surname></string-name>, <string-name><given-names>Timothy E. J.</given-names> <surname>Behrens</surname></string-name>, and <string-name><given-names>James C. R.</given-names> <surname>Whittington</surname></string-name></person-group>. <article-title>Actionable neural representations: Grid cells from minimal constraints</article-title>. In <conf-name>The Eleventh International Conference on Learning Representations</conf-name>, <year>2023</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Dori</given-names> <surname>Derdikman</surname></string-name>, <string-name><given-names>Jonathan R</given-names> <surname>Whitlock</surname></string-name>, <string-name><given-names>Albert</given-names> <surname>Tsao</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Fyhn</surname></string-name>, <string-name><given-names>Torkel</given-names> <surname>Hafting</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Fragmentation of grid cell maps in a multicompartment environment</article-title>. <source>Nature neuroscience</source>, <volume>12</volume>(<issue>10</issue>):<fpage>1325</fpage>–<lpage>1332</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Julija</given-names> <surname>Krupic</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>Stephen</given-names> <surname>Burton</surname></string-name>, <string-name><given-names>Caswell</given-names> <surname>Barry</surname></string-name>, and <string-name><given-names>John</given-names> <surname>O’Keefe</surname></string-name></person-group>. <article-title>Grid cell symmetry is shaped by environmental geometry</article-title>. <source>Nature</source>, <volume>518</volume>(<issue>7538</issue>):<fpage>232</fpage>–<lpage>235</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Aran</given-names> <surname>Nayebi</surname></string-name>, <string-name><given-names>Alexander</given-names> <surname>Attinger</surname></string-name>, <string-name><given-names>Malcolm</given-names> <surname>Campbell</surname></string-name>, <string-name><given-names>Kiah</given-names> <surname>Hardcastle</surname></string-name>, <string-name><given-names>Isabel</given-names> <surname>Low</surname></string-name>, <string-name><given-names>Caitlin S</given-names> <surname>Mallory</surname></string-name>, <string-name><given-names>Gabriel</given-names> <surname>Mel</surname></string-name>, <string-name><given-names>Ben</given-names> <surname>Sorscher</surname></string-name>, <string-name><given-names>Alex H</given-names> <surname>Williams</surname></string-name>, <string-name><given-names>Surya</given-names> <surname>Ganguli</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>34</volume>:<fpage>12167</fpage>–<lpage>12179</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Geoffrey W</given-names> <surname>Diehl</surname></string-name>, <string-name><given-names>Olivia J</given-names> <surname>Hon</surname></string-name>, <string-name><given-names>Stefan</given-names> <surname>Leutgeb</surname></string-name>, and <string-name><given-names>Jill K</given-names> <surname>Leutgeb</surname></string-name></person-group>. <article-title>Grid and nongrid cells in medial entorhinal cortex represent spatial location and environmental features with complementary coding schemes</article-title>. <source>Neuron</source>, <volume>94</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>92</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Revekka</given-names> <surname>Ismakov</surname></string-name>, <string-name><given-names>Omri</given-names> <surname>Barak</surname></string-name>, <string-name><given-names>Kate</given-names> <surname>Jeffery</surname></string-name>, and <string-name><given-names>Dori</given-names> <surname>Derdikman</surname></string-name></person-group>. <article-title>Grid cells encode local positional information</article-title>. <source>Current Biology</source>, <volume>27</volume>(<issue>15</issue>):<fpage>2337</fpage>–<lpage>2343</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Benjamin</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Wennberg</surname></string-name>, <string-name><given-names>Ziwei</given-names> <surname>Huang</surname></string-name>, and <string-name><given-names>Yasser</given-names> <surname>Roudi</surname></string-name></person-group>. <article-title>Grid cells show field-to-field variability and this explains the aperiodic response of inhibitory interneurons</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1701.04893</pub-id>, <year>2017</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gily</given-names> <surname>Ginosar</surname></string-name>, <string-name><given-names>Johnatan</given-names> <surname>Aljadeff</surname></string-name>, <string-name><given-names>Liora</given-names> <surname>Las</surname></string-name>, <string-name><given-names>Dori</given-names> <surname>Derdikman</surname></string-name>, and <string-name><given-names>Nachum</given-names> <surname>Ulanovsky</surname></string-name></person-group>. <article-title>Are grid cells used for navigation? on local metrics, subjective spaces, and black holes</article-title>. <source>Neuron</source>, <year>2023</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jeff</given-names> <surname>Hawkins</surname></string-name>, <string-name><given-names>Marcus</given-names> <surname>Lewis</surname></string-name>, <string-name><given-names>Mirko</given-names> <surname>Klukas</surname></string-name>, <string-name><given-names>Scott</given-names> <surname>Purdy</surname></string-name>, and <string-name><given-names>Subutai</given-names> <surname>Ahmad</surname></string-name></person-group>. <article-title>A framework for intelligence and cortical function based on grid cells in the neocortex</article-title>. <source>Frontiers in neural circuits</source>, <volume>12</volume>:<fpage>121</fpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mirko</given-names> <surname>Klukas</surname></string-name>, <string-name><given-names>Marcus</given-names> <surname>Lewis</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Efficient and flexible representation of higher-dimensional cognitive variables with grid cells</article-title>. <source>PLoS computational biology</source>, <volume>16</volume>(<issue>4</issue>):<fpage>e1007796</fpage>, <year>2020</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Jon W</given-names> <surname>Rueckemann</surname></string-name>, <string-name><given-names>Marielena</given-names> <surname>Sosa</surname></string-name>, <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name>, and <string-name><given-names>Elizabeth A</given-names> <surname>Buffalo</surname></string-name></person-group>. <article-title>The grid code for ordered experience</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>22</volume>(<issue>10</issue>):<fpage>637</fpage>–<lpage>649</lpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Francesca</given-names> <surname>Sargolini</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Fyhn</surname></string-name>, <string-name><given-names>Torkel</given-names> <surname>Hafting</surname></string-name>, <string-name><given-names>Bruce L</given-names> <surname>McNaughton</surname></string-name>, <string-name><given-names>Menno P</given-names> <surname>Witter</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title>. <source>Science</source>, <volume>312</volume>(<issue>5774</issue>):<fpage>758</fpage>–<lpage>762</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Tor</given-names> <surname>Stensola</surname></string-name>, <string-name><given-names>Hanne</given-names> <surname>Stensola</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Shearing-induced asymmetry in entorhinal grid cells</article-title>. <source>Nature</source>, <volume>518</volume>(<issue>7538</issue>):<fpage>207</fpage>–<lpage>212</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alexander</given-names> <surname>Mathis</surname></string-name>, <string-name><given-names>Andreas VM</given-names> <surname>Herz</surname></string-name>, and <string-name><given-names>Martin B</given-names> <surname>Stemmler</surname></string-name></person-group>. <article-title>Resolution of nested neuronal representations can be exponential in the number of neurons</article-title>. <source>Physical review letters</source>, <volume>109</volume>(<issue>1</issue>):<fpage>018103</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Maoz</given-names> <surname>Shamir</surname></string-name> and <string-name><given-names>Haim</given-names> <surname>Sompolinsky</surname></string-name></person-group>. <article-title>Implications of neuronal diversity on population coding</article-title>. <source>Neural computation</source>, <volume>18</volume>(<issue>8</issue>):<fpage>1951</fpage>–<lpage>1986</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mircea I</given-names> <surname>Chelaru</surname></string-name> and <string-name><given-names>Valentin</given-names> <surname>Dragoi</surname></string-name></person-group>. <article-title>Efficient coding in heterogeneous neuronal populations</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>105</volume>(<issue>42</issue>):<fpage>16344</fpage>–<lpage>16349</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Julijana</given-names> <surname>Gjorgjieva</surname></string-name>, <string-name><given-names>Guillaume</given-names> <surname>Drion</surname></string-name>, and <string-name><given-names>Eve</given-names> <surname>Marder</surname></string-name></person-group>. <article-title>Computational implications of biophysical diversity and multiple timescales in neurons and synapses for circuit performance</article-title>. <source>Current opinion in neurobiology</source>, <volume>37</volume>:<fpage>44</fpage>–<lpage>52</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicolas</given-names> <surname>Perez-Nieves</surname></string-name>, <string-name><given-names>Vincent CH</given-names> <surname>Leung</surname></string-name>, <string-name><given-names>Pier Luigi</given-names> <surname>Dragotti</surname></string-name>, and <string-name><given-names>Dan FM</given-names> <surname>Goodman</surname></string-name></person-group>. <article-title>Neural heterogeneity promotes robust learning</article-title>. <source>Nature communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>5791</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Se-Bum</given-names> <surname>Paik</surname></string-name> and <string-name><given-names>Dario L</given-names> <surname>Ringach</surname></string-name></person-group>. <article-title>Retinal origin of orientation maps in visual cortex</article-title>. <source>Nature neuroscience</source>, <volume>14</volume>(<issue>7</issue>):<fpage>919</fpage>–<lpage>925</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Gary G</given-names> <surname>Blasdel</surname></string-name> and <string-name><given-names>Guy</given-names> <surname>Salama</surname></string-name></person-group>. <article-title>Voltage-sensitive dyes reveal a modular organization in monkey striate cortex</article-title>. <source>Nature</source>, <volume>321</volume>(<issue>6070</issue>):<fpage>579</fpage>–<lpage>585</lpage>, <year>1986</year>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Spencer L</given-names> <surname>Smith</surname></string-name> and <string-name><given-names>Ikuko T</given-names> <surname>Smith</surname></string-name></person-group>. <article-title>Life imitates op art</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>7</issue>):<fpage>803</fpage>–<lpage>804</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Ryan J</given-names> <surname>Low</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Gu</surname></string-name>, and <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name></person-group>. <article-title>Cellular resolution optical access to brain regions in fissures: imaging medial prefrontal cortex and grid cells in entorhinal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>52</issue>):<fpage>18739</fpage>–<lpage>18744</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Weijian</given-names> <surname>Zong</surname></string-name>, <string-name><given-names>Horst A</given-names> <surname>Obenhaus</surname></string-name>, <string-name><given-names>Emilie R</given-names> <surname>Skytøen</surname></string-name>, <string-name><given-names>Hanna</given-names> <surname>Eneqvist</surname></string-name>, <string-name><given-names>Nienke L</given-names> <surname>de Jong</surname></string-name>, <string-name><given-names>Ruben</given-names> <surname>Vale</surname></string-name>, <string-name><given-names>Marina R</given-names> <surname>Jorge</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Large-scale two-photon calcium imaging in freely moving mice</article-title>. <source>Cell</source>, <volume>185</volume>(<issue>7</issue>):<fpage>1240</fpage>–<lpage>1256</lpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Francisco</given-names> <surname>Acosta</surname></string-name>, <string-name><given-names>Sophia</given-names> <surname>Sanborn</surname></string-name>, <string-name><given-names>Khanh Dao</given-names> <surname>Duc</surname></string-name>, <string-name><given-names>Manu</given-names> <surname>Madhav</surname></string-name>, and <string-name><given-names>Nina</given-names> <surname>Miolane</surname></string-name></person-group>. <article-title>Quantifying extrinsic curvature in neural manifolds</article-title>. In <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, pages <fpage>610</fpage>–<lpage>619</lpage>, <year>2023</year>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>William T</given-names> <surname>Redman</surname></string-name>, <string-name><given-names>Maria</given-names> <surname>Fonoberova</surname></string-name>, <string-name><given-names>Ryan</given-names> <surname>Mohr</surname></string-name>, <string-name><given-names>Ioannis G</given-names> <surname>Kevrekidis</surname></string-name>, and <string-name><given-names>Igor</given-names> <surname>Mezić</surname></string-name></person-group>. <article-title>Algorithmic (semi-) conjugacy via koopman operator theory</article-title>. In <conf-name>2022 IEEE 61st Conference on Decision and Control (CDC)</conf-name>, pages <fpage>6006</fpage>–<lpage>6011</lpage>. <publisher-name>IEEE</publisher-name>, <year>2022</year>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>William T</given-names> <surname>Redman</surname></string-name>, <string-name><given-names>Juan M</given-names> <surname>Bello-Rivas</surname></string-name>, <string-name><given-names>Maria</given-names> <surname>Fonoberova</surname></string-name>, <string-name><given-names>Ryan</given-names> <surname>Mohr</surname></string-name>, <string-name><given-names>Ioannis G</given-names> <surname>Kevrekidis</surname></string-name>, and <string-name><given-names>Igor</given-names> <surname>Mezić</surname></string-name></person-group>. <article-title>On equivalent optimization of machine learning methods</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">2302.09160</pub-id>, <year>2023</year>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Mitchell</given-names> <surname>Ostrow</surname></string-name>, <string-name><given-names>Adam</given-names> <surname>Eisen</surname></string-name>, <string-name><given-names>Leo</given-names> <surname>Kozachkov</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Beyond geometry: Comparing the temporal structure of computation in neural circuits with dynamical similarity analysis</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>, <year>2024</year>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Trygve</given-names> <surname>Solstad</surname></string-name>, <string-name><given-names>Charlotte N</given-names> <surname>Boccara</surname></string-name>, <string-name><given-names>Emilio</given-names> <surname>Kropff</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Representation of geometric borders in the entorhinal cortex</article-title>. <source>Science</source>, <volume>322</volume>(<issue>5909</issue>):<fpage>1865</fpage>–<lpage>1868</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Julija</given-names> <surname>Krupic</surname></string-name>, <string-name><given-names>Neil</given-names> <surname>Burgess</surname></string-name>, and <string-name><given-names>John</given-names> <surname>O’Keefe</surname></string-name></person-group>. <article-title>Neural representations of location composed of spatially periodic bands</article-title>. <source>Science</source>, <volume>337</volume>(<issue>6096</issue>):<fpage>853</fpage>–<lpage>857</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Vemund</given-names> <surname>Schøyen</surname></string-name>, <string-name><given-names>Markus Borud</given-names> <surname>Pettersen</surname></string-name>, <string-name><given-names>Konstantin</given-names> <surname>Holzhausen</surname></string-name>, <string-name><given-names>Marianne</given-names> <surname>Fyhn</surname></string-name>, <string-name><given-names>Anders</given-names> <surname>Malthe-Sørenssen</surname></string-name>, and <string-name><given-names>Mikkel</given-names> <surname>Elle Lepperød</surname></string-name></person-group>. <article-title>Coherently remapping toroidal cells but not grid cells are responsible for path integration in virtual agents</article-title>. <source>Iscience</source>, <volume>26</volume>(<issue>11</issue>), <year>2023</year>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Markus Borud</given-names> <surname>Pettersen</surname></string-name>, <string-name><given-names>Vemund</given-names> <surname>Sigmundson Schøyen</surname></string-name>, <string-name><given-names>Anders</given-names> <surname>Malthe-Sørenssen</surname></string-name>, and <string-name><given-names>Mikkel</given-names> <surname>Elle Lepperød</surname></string-name></person-group>. <article-title>Decoding the cognitive map: Learning place cells and remapping</article-title>. <source>bioRxiv</source>, <year>2024</year>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Markus</given-names> <surname>Pettersen</surname></string-name>, <string-name><given-names>Vemund</given-names> <surname>Sigmundson Schøyen</surname></string-name>, <string-name><surname>Mattis Dalsætra</surname> <given-names>Østby</given-names></string-name>, <string-name><given-names>Anders</given-names> <surname>Malthe-Sørenssen</surname></string-name>, and <string-name><given-names>Mikkel</given-names> <surname>Elle Lepperød</surname></string-name></person-group>. <article-title>Self-supervised grid cells without path integration</article-title>. <source>bioRxiv</source>, <year>2024</year>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Haggai</given-names> <surname>Agmon</surname></string-name> and <string-name><given-names>Yoram</given-names> <surname>Burak</surname></string-name></person-group>. <article-title>A theory of joint attractor dynamics in the hippocampus and the entorhinal cortex accounts for artificial remapping and grid cell field-to-field variability</article-title>. <source>Elife</source>, <volume>9</volume>:<elocation-id>e56894</elocation-id>, <year>2020</year>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Heekyung</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Cheng</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Sachin S</given-names> <surname>Deshmukh</surname></string-name>, and <string-name><given-names>James J</given-names> <surname>Knierim</surname></string-name></person-group>. <article-title>Neural population evidence of functional heterogeneity along the ca3 transverse axis: pattern completion versus pattern separation</article-title>. <source>Neuron</source>, <volume>87</volume>(<issue>5</issue>):<fpage>1093</fpage>–<lpage>1105</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Li</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Kei M</given-names> <surname>Igarashi</surname></string-name>, <string-name><given-names>Menno P</given-names> <surname>Witter</surname></string-name>, <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Topography of place maps along the ca3-to-ca2 axis of the hippocampus</article-title>. <source>Neuron</source>, <volume>87</volume>(<issue>5</issue>):<fpage>1078</fpage>–<lpage>1092</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>William T</given-names> <surname>Redman</surname></string-name>, <string-name><given-names>Nora S</given-names> <surname>Wolcott</surname></string-name>, <string-name><given-names>Luca</given-names> <surname>Montelisciani</surname></string-name>, <string-name><given-names>Gabriel</given-names> <surname>Luna</surname></string-name>, <string-name><given-names>Tyler D</given-names> <surname>Marks</surname></string-name>, <string-name><given-names>Kevin K</given-names> <surname>Sit</surname></string-name>, <string-name><given-names>Che-Hang</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>Spencer</given-names> <surname>Smith</surname></string-name>, and <string-name><given-names>Michael J</given-names> <surname>Goard</surname></string-name></person-group>. <article-title>Long-term transverse imaging of the hippocampus with glass microperiscopes</article-title>. <source>Elife</source>, <volume>11</volume>:<elocation-id>e75391</elocation-id>, <year>2022</year>.</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Marianne</given-names> <surname>Fyhn</surname></string-name>, <string-name><given-names>Torkel</given-names> <surname>Hafting</surname></string-name>, <string-name><given-names>Alessandro</given-names> <surname>Treves</surname></string-name>, <string-name><given-names>May-Britt</given-names> <surname>Moser</surname></string-name>, and <string-name><given-names>Edvard I</given-names> <surname>Moser</surname></string-name></person-group>. <article-title>Hippocampal remapping and grid realignment in entorhinal cortex</article-title>. <source>Nature</source>, <volume>446</volume>(<issue>7132</issue>):<fpage>190</fpage>–<lpage>194</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Eliott Robert Joseph</given-names> <surname>Levy</surname></string-name>, <string-name><given-names>Simón</given-names> <surname>Carrillo-Segura</surname></string-name>, <string-name><given-names>Eun Hye</given-names> <surname>Park</surname></string-name>, <string-name><given-names>William Thomas</given-names> <surname>Redman</surname></string-name>, <string-name><given-names>José Rafael</given-names> <surname>Hurtado</surname></string-name>, <string-name><given-names>SueYeon</given-names> <surname>Chung</surname></string-name>, and <string-name><given-names>André Antonio</given-names> <surname>Fenton</surname></string-name></person-group>. <article-title>A manifold neural population code for space in hippocampal coactivity dynamics independent of place fields</article-title>. <source>Cell Reports</source>, <volume>42</volume>(<issue>10</issue>), <year>2023</year>.</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Emily A</given-names> <surname>Mankin</surname></string-name>, <string-name><given-names>Fraser T</given-names> <surname>Sparks</surname></string-name>, <string-name><given-names>Begum</given-names> <surname>Slayyeh</surname></string-name>, <string-name><given-names>Robert J</given-names> <surname>Sutherland</surname></string-name>, <string-name><given-names>Stefan</given-names> <surname>Leutgeb</surname></string-name>, and <string-name><given-names>Jill K</given-names> <surname>Leutgeb</surname></string-name></person-group>. <article-title>Neuronal code for extended time in the hippocampus</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>47</issue>):<fpage>19462</fpage>–<lpage>19467</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Yaniv</given-names> <surname>Ziv</surname></string-name>, <string-name><given-names>Laurie D</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>Eric D</given-names> <surname>Cocker</surname></string-name>, <string-name><given-names>Elizabeth O</given-names> <surname>Hamel</surname></string-name>, <string-name><given-names>Kunal K</given-names> <surname>Ghosh</surname></string-name>, <string-name><given-names>Lacey J</given-names> <surname>Kitch</surname></string-name>, <string-name><given-names>Abbas</given-names> <surname>El Gamal</surname></string-name>, and <string-name><given-names>Mark J</given-names> <surname>Schnitzer</surname></string-name></person-group>. <article-title>Long-term dynamics of ca1 hippocampal place codes</article-title>. <source>Nature neuroscience</source>, <volume>16</volume>(<issue>3</issue>):<fpage>264</fpage>–<lpage>266</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Thomas</given-names> <surname>Hainmueller</surname></string-name> and <string-name><given-names>Marlene</given-names> <surname>Bartos</surname></string-name></person-group>. <article-title>Parallel emergence of stable and dynamic memory engrams in the hippocampus</article-title>. <source>Nature</source>, <volume>558</volume>(<issue>7709</issue>):<fpage>292</fpage>–<lpage>296</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Walter G</given-names> <surname>Gonzalez</surname></string-name>, <string-name><given-names>Hanwen</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Anna</given-names> <surname>Harutyunyan</surname></string-name>, and <string-name><given-names>Carlos</given-names> <surname>Lois</surname></string-name></person-group>. <article-title>Persistence of neuronal representations through time and damage in the hippocampus</article-title>. <source>Science</source>, <volume>365</volume>(<issue>6455</issue>):<fpage>821</fpage>–<lpage>825</lpage>, <year>2019</year>.</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Can</given-names> <surname>Dong</surname></string-name>, <string-name><given-names>Antoine D</given-names> <surname>Madar</surname></string-name>, and <string-name><given-names>Mark EJ</given-names> <surname>Sheffield</surname></string-name></person-group>. <article-title>Distinct place cell dynamics in ca1 and ca3 encode experience in new environments</article-title>. <source>Nature communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>2977</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Adi</given-names> <surname>Mizrahi</surname></string-name>, <string-name><given-names>Justin C</given-names> <surname>Crowley</surname></string-name>, <string-name><given-names>Eran</given-names> <surname>Shtoyerman</surname></string-name>, and <string-name><given-names>Lawrence C</given-names> <surname>Katz</surname></string-name></person-group>. <article-title>High-resolution in vivo imaging of hippocampal dendrites and spines</article-title>. <source>Journal of Neuroscience</source>, <volume>24</volume>(<issue>13</issue>):<fpage>3147</fpage>–<lpage>3151</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c79"><label>[79]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alessio</given-names> <surname>Attardo</surname></string-name>, <string-name><given-names>James E</given-names> <surname>Fitzgerald</surname></string-name>, and <string-name><given-names>Mark J</given-names> <surname>Schnitzer</surname></string-name></person-group>. <article-title>Impermanence of dendritic spines in live adult ca1 hippocampus</article-title>. <source>Nature</source>, <volume>523</volume>(<issue>7562</issue>):<fpage>592</fpage>–<lpage>596</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c80"><label>[80]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Thomas</given-names> <surname>Pfeiffer</surname></string-name>, <string-name><given-names>Stefanie</given-names> <surname>Poll</surname></string-name>, <string-name><given-names>Stephane</given-names> <surname>Bancelin</surname></string-name>, <string-name><given-names>Julie</given-names> <surname>Angibaud</surname></string-name>, <string-name><given-names>VVG Krishna</given-names> <surname>Inavalli</surname></string-name>, <string-name><given-names>Kevin</given-names> <surname>Keppler</surname></string-name>, <string-name><given-names>Manuel</given-names> <surname>Mittag</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Fuhrmann</surname></string-name>, and <string-name><given-names>U Valentin</given-names> <surname>Nägerl</surname></string-name></person-group>. <article-title>Chronic 2p-sted imaging reveals high turnover of dendritic spines in the hippocampus in vivo</article-title>. <source>Elife</source>, <volume>7</volume>:<elocation-id>e34700</elocation-id>, <year>2018</year>.</mixed-citation></ref>
<ref id="c81"><label>[81]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nathaniel J</given-names> <surname>Killian</surname></string-name>, <string-name><given-names>Michael J</given-names> <surname>Jutras</surname></string-name>, and <string-name><given-names>Elizabeth A</given-names> <surname>Buffalo</surname></string-name></person-group>. <article-title>A map of visual space in the primate entorhinal cortex</article-title>. <source>Nature</source>, <volume>491</volume>(<issue>7426</issue>):<fpage>761</fpage>–<lpage>764</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c82"><label>[82]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Alexandra O</given-names> <surname>Constantinescu</surname></string-name>, <string-name><given-names>Jill X</given-names> <surname>O’Reilly</surname></string-name>, and <string-name><given-names>Timothy EJ</given-names> <surname>Behrens</surname></string-name></person-group>. <article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title>. <source>Science</source>, <volume>352</volume>(<issue>6292</issue>):<fpage>1464</fpage>–<lpage>1468</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c83"><label>[83]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Dmitriy</given-names> <surname>Aronov</surname></string-name>, <string-name><given-names>Rhino</given-names> <surname>Nevers</surname></string-name>, and <string-name><given-names>David W</given-names> <surname>Tank</surname></string-name></person-group>. <article-title>Mapping of a non-spatial dimension by the hippocampal– entorhinal circuit</article-title>. <source>Nature</source>, <volume>543</volume>(<issue>7647</issue>):<fpage>719</fpage>–<lpage>722</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c84"><label>[84]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Joshua B</given-names> <surname>Julian</surname></string-name>, <string-name><given-names>Alexandra T</given-names> <surname>Keinath</surname></string-name>, <string-name><given-names>Giulia</given-names> <surname>Frazzetta</surname></string-name>, and <string-name><given-names>Russell A</given-names> <surname>Epstein</surname></string-name></person-group>. <article-title>Human entorhinal cortex represents visual space using a boundary-anchored grid</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>2</issue>):<fpage>191</fpage>–<lpage>194</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c85"><label>[85]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Matthias</given-names> <surname>Nau</surname></string-name>, <string-name><given-names>Tobias</given-names> <surname>Navarro</surname></string-name> Schröder, <string-name><given-names>Jacob LS</given-names> <surname>Bellmund</surname></string-name>, and <string-name><given-names>Christian F</given-names> <surname>Doeller</surname></string-name></person-group>. <article-title>Hexadirectional coding of visual space in human entorhinal cortex</article-title>. <source>Nature neuroscience</source>, <volume>21</volume>(<issue>2</issue>):<fpage>188</fpage>–<lpage>190</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c86"><label>[86]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Niklas</given-names> <surname>Wilming</surname></string-name>, <string-name><given-names>Peter</given-names> <surname>König</surname></string-name>, <string-name><given-names>Seth</given-names> <surname>König</surname></string-name>, and <string-name><given-names>Elizabeth A</given-names> <surname>Buffalo</surname></string-name></person-group>. <article-title>Entorhinal cortex receptive fields are modulated by spatial attention, even without movement</article-title>. <source>Elife</source>, <volume>7</volume>:<elocation-id>e31745</elocation-id>, <year>2018</year>.</mixed-citation></ref>
<ref id="c87"><label>[87]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Sujaya</given-names> <surname>Neupane</surname></string-name>, <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name>, and <string-name><given-names>Mehrdad</given-names> <surname>Jazayeri</surname></string-name></person-group>. <article-title>Mental navigation in the primate entorhinal cortex</article-title>. <source>Nature</source>, pages <fpage>1</fpage>–<lpage>8</lpage>, <year>2024</year>.</mixed-citation></ref>
<ref id="c88"><label>[88]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Rylan</given-names> <surname>Schaeffer</surname></string-name>, <string-name><given-names>Mikail</given-names> <surname>Khona</surname></string-name>, <string-name><given-names>Tzuhsuan</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Cristobal</given-names> <surname>Eyzaguirre</surname></string-name>, <string-name><given-names>Sanmi</given-names> <surname>Koyejo</surname></string-name>, and <string-name><given-names>Ila</given-names> <surname>Fiete</surname></string-name></person-group>. <article-title>Self-supervised learning of representations for space generates multi-modular grid cells</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>36</volume>, <year>2024</year>.</mixed-citation></ref>
<ref id="c89"><label>[89]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>James J</given-names> <surname>Jun</surname></string-name>, <string-name><given-names>Nicholas A</given-names> <surname>Steinmetz</surname></string-name>, <string-name><given-names>Joshua H</given-names> <surname>Siegle</surname></string-name>, <string-name><given-names>Daniel J</given-names> <surname>Denman</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>Brian</given-names> <surname>Barbarits</surname></string-name>, <string-name><given-names>Albert K</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Costas A</given-names> <surname>Anastassiou</surname></string-name>, <string-name><given-names>Alexandru</given-names> <surname>Andrei</surname></string-name>, <string-name><given-names>Çagatay</given-names> <surname>Aydin</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title>. <source>Nature</source>, <volume>551</volume>(<issue>7679</issue>):<fpage>232</fpage>–<lpage>236</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c90"><label>[90]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Nicholas A</given-names> <surname>Steinmetz</surname></string-name>, <string-name><given-names>Cagatay</given-names> <surname>Aydin</surname></string-name>, <string-name><given-names>Anna</given-names> <surname>Lebedeva</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Okun</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>Marius</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>Maxime</given-names> <surname>Beau</surname></string-name>, <string-name><given-names>Jai</given-names> <surname>Bhagat</surname></string-name>, Claudia Böhm, <string-name><given-names>Martijn</given-names> <surname>Broux</surname></string-name>, <etal>et al.</etal></person-group> <article-title>Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings</article-title>. <source>Science</source>, <volume>372</volume>(<issue>6539</issue>):<fpage>eabf4588</fpage>, <year>2021</year>.</mixed-citation></ref>
<ref id="c91"><label>[91]</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>Leland</given-names> <surname>McInnes</surname></string-name>, <string-name><given-names>John</given-names> <surname>Healy</surname></string-name>, and <string-name><given-names>James</given-names> <surname>Melville</surname></string-name></person-group>. <article-title>Umap: Uniform manifold approximation and projection for dimension reduction</article-title>. <source>arXiv</source> <pub-id pub-id-type="arxiv">1802.03426</pub-id>, <year>2018</year>.</mixed-citation></ref>
<ref id="c92"><label>[92]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Martin</given-names> <surname>Ester</surname></string-name>, <string-name><given-names>Hans-Peter</given-names> <surname>Kriegel</surname></string-name>, <string-name><given-names>Jörg</given-names> <surname>Sander</surname></string-name>, <string-name><given-names>Xiaowei</given-names> <surname>Xu</surname></string-name>, <etal>et al.</etal></person-group> <article-title>A density-based algorithm for discovering clusters in large spatial databases with noise</article-title>. In <source>kdd</source>, volume <volume>96</volume>, pages <fpage>226</fpage>–<lpage>231</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c93"><label>[93]</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>William N</given-names> <surname>Butler</surname></string-name>, <string-name><given-names>Kiah</given-names> <surname>Hardcastle</surname></string-name>, and <string-name><given-names>Lisa M</given-names> <surname>Giocomo</surname></string-name></person-group>. <article-title>Remembered reward locations restructure entorhinal spatial maps</article-title>. <source>Science</source>, <volume>363</volume>(<issue>6434</issue>):<fpage>1447</fpage>–<lpage>1452</lpage>, <year>2019</year>.</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100652.2.sa4</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Peyrache</surname>
<given-names>Adrien</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>McGill University</institution>
</institution-wrap>
<city>Montreal</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Convincing</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This <bold>valuable</bold> study examines the variability in spacing and direction of entorhinal grid cells, providing <bold>convincing</bold> evidence that such variability helps disambiguate locations within an environment. This study will be of interest to neuroscientists working on spatial navigation and, more broadly, on neural coding.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100652.2.sa3</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The present paper by Redman et al. investigated the variability of grid cell properties in the MEC by analyzing publicly available large-scale neural recording data. Although previous studies have proposed that grid spacing and orientation are homogeneous within the same grid module, the authors found a small but robust variability in grid spacing and orientation across grid cells in the same module. The authors also showed, through model simulations, that such variability is useful for decoding spatial position.</p>
<p>Strengths:</p>
<p>The results of this study provide novel and intriguing insights into how grid cells compose the cognitive map in the axis of the entorhinal cortex and hippocampus. This study analyzes large data sets in an appropriate manner and the results are convincing.</p>
<p>Comments on revisions:</p>
<p>In the revised version of the manuscript, the authors have addressed all the concerns I raised.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100652.2.sa2</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents an interesting and useful analysis of grid cell heterogeneity, showing that the experimentally observed heterogeneity of spacing and orientation within a grid cell module can allow more accurate decoding of location from a single module.</p>
<p>Strengths:</p>
<p>(1) I found the statistical analysis of the grid cell variability to be very systematic and convincing. I also found the evidence for enhanced decoding of location based on between cell variability within a module to be convincing and important, supporting their conclusions.</p>
<p>(2) Theoreticians have developed models that focus on the use of grid cells that are highly regular in their parameters, and usually vary only in the spatial phase of cells within modules and the spacing and orientation between modules. This focus on consistency is partly to obtain the generalization of the grid cell code to a broad range of previously unvisited locations. In contrast, most experimentalists working with grid cells know that many if not most grid cells show high variability of firing fields, as demonstrated in the figures in experimental papers. The authors of this current paper have highlighted this discrepancy, and shown that the variability shown in the data could actually enhance decoding of location.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100652.2.sa1</article-id>
<title-group>
<article-title>Reviewer #3 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>Redman and colleagues analyze grid cell data obtained from public databases. They show that there is significant variability in spacing and orientation within a module. They show that the difference in spacing and orientation for a pair of cells is larger than the one obtained for two independent maps of the same cell. They speculate that this variability could be useful to disambiguate the rat position if only information from a single module is used by a decoder.</p>
<p>Strengths:</p>
<p>The strengths of this work lie in its conciseness, clarity, and the potential significance of its findings for the grid cell community, which has largely overlooked this issue for the past two decades. Their hypothesis is well stated and the analyses are solid.</p>
<p>Weaknesses:</p>
<p>Major weaknesses identified in the original version have been addressed.</p>
<p>The authors have addressed all of our concerns, providing control analyses that strengthen their claim.</p>
</body>
</sub-article>
<sub-article id="sa4" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.100652.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Redman</surname>
<given-names>William T</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4147-2026</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Acosta-Mendoza</surname>
<given-names>Santiago</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0009-0003-6698-476X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Wei</surname>
<given-names>Xue-Xin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2761-477X</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Goard</surname>
<given-names>Michael J</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5366-8501</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<p>We thank the reviewers for their time and thoughtful comments. We believe that the further analyses suggested have made the results clearer and more robust. Below, we briefly highlight the key points addressed in the revision and the new evidence supporting them. Then, we address each reviewer’s critiques point-by-point.</p>
<p>- Changes in variability with respect to time/experience</p>
<p>Both reviewers #1 and #3 asked whether the variability in grid properties observed was dependent on time or experience. This is an important point, given that such a dependence on time could lead to interesting hypotheses about the underlying dynamics of the grid code. However, in the new analyses we performed, we do not observe changes in grid variability within a session (Fig S5 of the revised manuscript), suggesting that the grid variability seen is constant within the timescale of the data set.</p>
<p>- The assumption of constant grid parameters in the literature</p>
<p>Reviewer #2 pointed out that it had been appreciated by experimentalists that grid properties are variable within a module. We agree that we may have overstated the universality of this assumption in the original manuscript, and we have toned down the language in the revision. However, we note that many previous theoretical studies assumed these properties to be constant, within a given module. We provide some examples below, and have added evidence of this assertion, with citations to the theoretical literature, to the revised manuscript .</p>
<p>- Additional sources of variability</p>
<p>Reviewer #3 pointed out additional sources that might explain the variability observed in the paper (beyond time and experience). These sources include: field width, border location, and the impact of conjunctive cells. We have run additional analyses and have found no significant impact on the observed variability from any of these factors. We believe that these are important controls, and have added them to the manuscript (Fig S4-S7 of the revised manuscript)</p>
<p>- Analysis of computational models</p>
<p>Reviewer #3 noted that our results could be strengthened by performing similar analyses on the output of computational models of grid cells. This is a good idea. We have now measured the variability of grid properties in a recent normative recurrent neural network (RNN) model that develops grid cells when trained to perform path integration (Sorscher et al., 2019). This model has been shown to develop signatures of a 2D toroidal attractor (Sorscher et al., 2023) and achieves a high accuracy on a simple path integration task. Interestingly, the units with the greatest grid scores also exhibit a range of grid spacings and grid orientations (Fig S8 of the revised manuscript). Furthermore, by decreasing the amount of sparsity (through decreasing the weight decay regularization), we found an increase in the variability of the grid properties. This analysis demonstrates a heretofore unknown similarity between the RNN models trained to perform path integration and recorded grid cells from MEC. It additionally provides a framework for computational analysis of the emergence of grid property variability.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #1:</bold></p>
<p>(1) Is the variability in grid spacing and orientation that the authors found intrinsically organized or is it shaped by experience? Previous research has shown that grid representations can be modified through experience (e.g., Boccara et al., Science 2019). To understand the dynamics of the network, it would be important to investigate whether robust variability exists from the beginning of the task period (recording period) or whether variability emerges in an experience-dependent manner within a session.</p>
</disp-quote>
<p>This is an interesting question that was not addressed in the paper. To test this, we performed additional analysis to resolve whether the variability changes across a session.</p>
<p>Using a sliding window, we have measured changes in variability with respect to recording time (Fig S5A). To this end, we compute grid orientation and spacing over a time-window whose length is half the total length of the recording. From the population distribution of orientation and spacing values, we compute the standard deviation as a measure of variability. We repeat the same procedure, sliding the window forward until the variability for the second half of the recording is computed.</p>
<p>We applied this approach to recording ID R12 (the same as in Figs 2-4) given that this recording session was significantly longer than the rest (nearly two hours). Results are shown in Fig S5B-C. For both orientation and spacing, no changes of variability with respect to time can be observed. Similar results were found for other modules (see caption of Fig S5 for statistics).</p>
<p>We also note that the rats were already familiarized with the environment for 10-20 sessions prior to the recordings, so there may not be further learning during the period of the grid cell recordings. No changes in variability can be seen in Rat R across days (e.g., in Fig 5B R12 and R22 have similar distributions of variability). However, we note that it may be possible that there are changes in grid properties at time-scales greater than the recordings.</p>
<disp-quote content-type="editor-comment">
<p>(2) It is important to consider the optimal variability size. The larger the variability, the better it is for decoding. On the other hand, as the authors state in the</p>
</disp-quote>
<p>Discussion, it is assumed that variability does not exist in the continuous attractor model. Although this study describes that it does not address how such variability fits the attractor theory, it would be better if more detailed ideas and suggestions were provided as to what direction the study could take to clarify the optimal size of variability.</p>
<p>We appreciate this suggestion and agree that more discussion is warranted on how our results can be reconciled with previously observed attractor dynamics. To explore this, we studied the recurrent neural network (RNN) model from Sorscher et al. (2019), which develops grid responses when trained on path integration. This network has previously been found to develop signatures of toroidal topology (Sorscher et al., 2023), yet we find its grid responses also contain heterogeneity in grid properties (Fig S8). By decreasing the strength of the weight decay regularization (which leads to denser connectivity in the recurrent layer), we find an increase in the grid property variability. Interestingly, decreasing the weight decay regularization has been previously found to lead to weaker grid responses and worse ability of the RNN to perform path integration on environments larger than it was trained on. This approach not only provides preliminary evidence to our claim that too much variability can lead to weaker continuous attractor structure, but also provides a modeling framework with which future work can explore this question in more detail. We have added discussion of this issue to the manuscript text (Discussion).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #2:</bold></p>
<p>(1) Even though theoreticians might have gotten the mistaken impression that grid cells are highly regular, this might be due to an overemphasis on regularity in a subset of papers. Most experimentalists working with grid cells know that many if not most grid cells show high variability of firing fields within a single neuron, though this analysis focuses on between neurons. In response to this comment, the reviewers should tone down and modify their statements about what are the current assumptions of the field (and if possible provide a short supplemental section with direct quotes from various papers that have made these assumptions).</p>
</disp-quote>
<p>We agree that some experimentalists are aware of variability in the recorded grid response patterns and that this work may not come as a complete surprise to them. We have toned down our language in the Introduction, changing “our results challenge a long-held assumption” to “our results challenge a frequently made assumption in the theoretical literature”. Additionally, we have added a caveat that “experimentalists have been aware” of the observed variability in grid properties.</p>
<p>We would like to emphasize that the lack of work carefully examining the robustness of this variability has prevented a firm understanding of whether this is an inherent property of grid cells or due to measurement noise. The impact of this can be seen in theoretical neuroscience work where a considerable number of articles (including recent publications) start with the assumption that all grid cells within a module have identical properties, with the exception of phase shift and noise. We have now cited a number of these papers in the Introduction, to provide specific references. To further illustrate the pervasiveness of this assumption being explicitly made in theoretical neuroscience, below we provide quotes from a few important papers:</p>
<p>“Cells with a common spatial period also share a common grid orientation; their responses differ only by spatial translations, or different preferred firing phases, with respect to their common response period” (Sreenivasan and Fiete, 2011)”</p>
<p>“Grid cells are organized into discrete modules; within each module, the spatial scale and orientation of the grid lattice are the same, but the lattice for different cells is shifted in space.” (Stemmler et al., 2015)”</p>
<p>“Recently, it was shown that grid cells are organized in discrete modules within which cells share the same orientation and periodicity but vary randomly in phase” (Wei et al., 2015)”</p>
<p>“...cells within one module have receptive fields that are translated versions of one another, and different modules have firing lattices of different scales and orientations” (Dorrell et al., 2023)”</p>
<p>In these works, this assumption is used to derive properties relating to the computational properties of grid cells (e.g., error correction, optimal scaling between grid spacings in different modules).</p>
<p>In addition, since grid cells are assumed to be identical in the computational neuroscience community, there has been little work on quantifying how much variability a given model produces. This makes it challenging to understand how consistent different models are with our observations. This is illustrated in our analysis of a recent recurrent neural network (RNN) model of grid cells (Fig S8), which does exhibit variability.</p>
<disp-quote content-type="editor-comment">
<p>(2) The authors state that &quot;no characterization of the degree and robustness of variability in grid properties within individual modules has been performed.&quot; It is always dangerous to speak in absolute terms about what has been done in scientific studies. It is true that few studies have had the number of grid cells necessary to make comparisons within and between modules, but many studies have clearly shown the distribution of spacing in neuronal data (e.g. Hafting et al., 2005; Barry et al., 2007; Stensola et al., 2012; Hardcastle et al., 2015) so the variability has been visible in the data presentations. Also, most researchers in the field are well aware that highly consistent grid cells are much rarer than messy grid cells that have unevenly spaced firing fields. This doesn't hurt the importance of the paper, but they need to tone down their statements about the lack of previous awareness of variability (specific locations are noted in the specific comments).</p>
</disp-quote>
<p>We have toned down our language in the Introduction. However, we note that our point that no detailed analysis had been done on measuring the robustness of this variability stands. Thus, for the general community, it has not been clear whether this previously observed variability is noise or a real feature of the grid code.</p>
<disp-quote content-type="editor-comment">
<p>(3) The methods section needs to have a separate subheading entitled: How grid cells were assigned to modules&quot; that clearly describes how the grid cells were assigned to a module (i.e. was this done by Gardner et al., or done as part of this paper's post-processing?</p>
</disp-quote>
<p>We thank the reviewer for pointing out this missing information. We have added a new subsection in the Materials and Methods section, entitled “Grid module classification” to clarify how the grid cells are assigned to modules. In short, this was done by Gardner et al. (2022) using an unsupervised clustering approach that was viewed as enabling a less biased identification of modules. We did not perform any additional processing steps on module identity.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer #3:</bold></p>
<p>(1) One possible explanation of the dispersion in lambda (not in theta) could be variability in the typical width of the field. For a fixed spacing, wider fields might push the six fields around the center of the autocorrelogram toward the outside, depending on the details of how exactly the position of these fields is calculated. We recommend authors show that lambda does not correlate with field width, or at least that the variability explained by field width is smaller than the overall lambda variability.</p>
</disp-quote>
<p>We agree that this option had not been carefully ruled out by our previous analyses. To tackle this question, we compute the field width of a given cell using the value at the minima of its spatial autocorrelogram (Fig S4A-B). For all cells in recording ID R12, there is a non-significant negative linear correlation between grid field width and between-cell variability (Fig S4C) . The variability explained by the width of the field is 4% of the variability, as indicated by the R<sup>2</sup> value of the linear fit. Similar results were found for all other modules (see caption of Fig S4C for statistics). Therefore, we do not think that grid field width explains spacing variability.</p>
<disp-quote content-type="editor-comment">
<p>(2) An alternative explanation could be related to what happens at the borders. The authors tackle this issue in Figure S2 but introduce a different way of measuring lambda based on three fields, which in our view is not optimal. We recommend showing that the dispersions in lambda and theta remain invariant as one removes the border-most part of the maps but estimating lambda through the autocorrelogram of the remaining part of the map. Of course, there is a limit to how much can be removed before measures of lambda and theta become very noisy.</p>
</disp-quote>
<p>We have performed additional analysis to explore the role of borders in grid property variability. To do so, we have followed the suggestion by the reviewer and have re-analyzed grid properties from the autocorrelogram when the border-most part of the maps are removed (Fig S6A-B). For all modules, we do not see any changes in variability (computed as the standard deviation of the population distribution) for either orientation or spacing. As predicted by the reviewer, after removing about 25% of the border-most part of the environment we start seeing changes in variability, as measures of theta and lambda become noisy and computed over a smaller spatial range. This result holds for all other modules (Fig S6C-D).</p>
<disp-quote content-type="editor-comment">
<p>(3) A third possibility is slightly more tricky. Some works (for example Kropff et al, 2015) have shown that fields anticipate the rat position, so every time the rat traverses them they appear slightly displaced opposite to the direction of movement. The amount of displacement depends on the velocity. Maps that we construct out of a whole session should be deformed in a perfectly symmetric way if rats traverse fields in all directions and speeds. However, if the cell is conjunctive, we would expect a deformation mainly along the cell's preferred head direction. Since conjunctive cells have all possible preferred directions, and many grid cells are not conjunctive at all, this phenomenon could create variability in theta and lambda that is not a legitimate one but rather associated with the way we pool data to construct maps. To rule away this possibility, we recommend the authors study the variability in theta and lambda of conjunctive vs non-conjunctive grid cells. If the authors suspect that this phenomenon could explain part of their results, they should also take into account the findings of Gerlei and colleagues (2020) from the Nolan lab, that add complexity to this issue.</p>
</disp-quote>
<p>We appreciate the reviewer pointing out the possible role conjunctive cells may play. To investigate how conjunctive cells may affect the observed grid property variability, we have performed additional analyses taking into account if the grid cells included in the study are conjunctive. Comparing within- and between-cell variability of conjunctive vs. non-conjunctive cells in recording R12, we do not see any qualitative differences for either orientation or spacing (Fig S7A-B). When excluding conjunctive cells from the between-variability comparison, we do not see any significant difference compared to when these cells are included (Fig S7C-D). As such, it does not appear that conjunctive cells are the source of variability in the population.</p>
<p>We further note that the number of putative conjunctive cells varied across modules and recordings. For instance, in recording Q1 and Q2, Gardner et al. (2022) reported 3 (out of 97) and 1 (out of 66) conjunctive cells, respectively. Given that we see variability robustly across recordings (Fig 5), we do not believe that conjunctive cells can explain the presence of variability we observe.</p>
<disp-quote content-type="editor-comment">
<p>(4) The results in Figure 6 are correct, but we are not convinced by the argument. The fact that grid cells fire in the same way in different parts of the environment and in different environments is what gives them their appeal as a platform for path integration since displacement can be calculated independently of the location of the animal. Losing this universal platform is, in our view, too much of a price to pay when the only gain is the possibility of decoding position from a single module (or non-adjacent modules) which, as the authors discuss, is probably never the case. Besides, similar disambiguation of positions within the environment would come for free by adding to the decoding algorithm spatial cells (non-hexagonal but spatially stable), which are ubiquitous across the entorhinal cortex. Thus, it seems to us that - at least along this line of argumentation - with variability the network is losing a lot but not gaining much.</p>
</disp-quote>
<p>We agree that losing the continuous attractor network (CAN) structure and the ability to path integrate would be a very large loss. However, we do not believe that the variability we observe necessarily destroys either the CAN or path integration. We argue this for two reasons. First, the data we analyzed [from Gardner et al. (2022)] is exactly the data set that was found to have toroidal topology and therefore viewed to be consistent with a major prediction of CANs. Thus, the amount of variability in grid properties does not rule out the underlying presence of a continuous attractor. Second, path integration may still be possible with grid cells that have variable properties. To illustrate this, we analyzed data from Sorscher et al. (2019) recurrent neural network model (RNN) that was trained explicitly on path integration, and found that the grid representations that emerged had variability in spacing and orientation (see point #6 below).</p>
<disp-quote content-type="editor-comment">
<p>(5) In Figure 4 one axis has markedly lower variability. Is this always the same axis? Can the authors comment more on this finding?</p>
</disp-quote>
<p>We agree that in Fig 4 the first axis has lower variability. We believe that this is specific to the module R12 and does not reflect any differences in axis or bias in the methods used to compute the axis metrics. To test this, we have performed the same analyses for other modules, finding that other recordings do not exhibit the same bias. Results for the modules with the most cells are shown below (Author response image 1).</p>
<fig id="sa4fig1">
<label>Author response image 1.</label>
<caption>
<title>Grid propertied along Axis 1 are not less variable for many recorded grid modules.</title>
<p>Same as Fig.4C-D, but for four other recorded modules. Note that the variability along each axis is similar.</p>
</caption>
<graphic mime-subtype="jpg" xlink:href="elife-100652-sa4-fig1.jpg" mimetype="image"/>
</fig>
<disp-quote content-type="editor-comment">
<p>(6) The paper would gain in depth if maps coming out of different computational models could be analyzed in the same way.</p>
</disp-quote>
<p>We agree with the reviewer that examining computational models using the same approach would strengthen our results and we appreciate the suggestion. To address this, we have analyzed the results from a previous normative model for grid cells [Sorscher et al., (2019)] that trained a recurrent neural network (RNN) model to perform path integration and found that units developed grid cell like responses. These models have been found to exhibit signatures of toroidal attractor dynamics [Sorscher et al. (2023)] and exhibit a diversity of responses beyond pure grid cells, making them a good starting point for understanding whether models of MEC may contain uncharacterized variability in grid properties.</p>
<p>We find that RNN units in these normative models exhibit similar amounts of variability in grid spacing and orientation as observed in the real grid cell recordings (Fig S8A-D). This provides additional evidence that this variability may be expected from a normative framework, and that the variability does not destroy the ability to path integrate (which the RNN is explicitly trained to perform).</p>
<p>The RNN model offers possibilities to assess what might cause this variability. While we leave a detailed investigation of this to future work, we varied the weight decay regularization hyper-parameter. This value controls how sparse the weights in the hidden recurrent layer are. Large weight decay regularization strength encourages sparser connectivity, while small weight decay regularization strength allows for denser connectivity. We find that increasing this penalty (and enforcing sparser connectivity) decreases the variability of grid properties (Fig S8E-F). This suggests that the observed variability in the Gardner et al. (2022) data set could be due to the fact that grid cells are synaptically connected to other, non-grid cells in MEC.</p>
<disp-quote content-type="editor-comment">
<p>(7) Similarly, it would be very interesting to expand the study with some other data to understand if between-cell delta_theta and delta_lambda are invariant across environments. In a related matter, is there a correlation between delta_theta (delta_lambda) for the first vs for the second half of the session? We expect there should be a significant correlation, it would be nice to show it.</p>
</disp-quote>
<p>We agree this would be interesting to examine. For this analysis, it is essential to have a large number of grid cells, and we are not aware of other published data sets with comparable cell numbers using different environments.</p>
<p>Using a sliding window analysis, we have characterized changes in variability with respect to the recording time (Figure S5A). To do so, we compute grid orientation and spacing over a time-window whose length is half of the total length of the recording. From the population distribution of orientation and spacing values, we compute the standard deviation as a measure of between-cell variability. We repeat the same procedure, sliding the window forward until the variability for the second half of the recording is computed.</p>
<p>We applied this approach to recording ID R12 (the same as in Figs 2-4) given that this recording session was significantly longer than the rest (almost two hours). Results are shown in Fig S5 B-C. For both orientation and spacing, no systematic changes of variability with respect to time were observed. Similar results were found for other modules (see caption of Fig S5 for statistics).</p>
<p>We also note that the rats were already familiarized with the environment for 10-20 sessions prior to the recordings, so there may not be further learning during the period of the grid cell recordings. No changes in variability can be seen in Rat R across days (e.g., in Fig 5B R12 and R22 have similar distributions of variability). However, we note that it may be possible that there are changes in grid properties at time-scales greater than the recordings.</p>
</body>
</sub-article>
</article>