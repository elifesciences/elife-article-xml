<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">98485</article-id>
<article-id pub-id-type="doi">10.7554/eLife.98485</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98485.2</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.3</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A deep learning framework for automated and generalized synaptic event analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>O’Neill</surname>
<given-names>Philipp S</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4143-4166</contrib-id>
<name>
<surname>Baccino-Calace</surname>
<given-names>Martín</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8235-8257</contrib-id>
<name>
<surname>Rupprecht</surname>
<given-names>Peter</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lee</surname>
<given-names>Sungmoo</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hao</surname>
<given-names>Yukun A</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0492-1961</contrib-id>
<name>
<surname>Lin</surname>
<given-names>Michael Z</given-names>
</name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9107-0482</contrib-id>
<name>
<surname>Friedrich</surname>
<given-names>Rainer W</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1624-6761</contrib-id>
<name>
<surname>Müller</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6151-2363</contrib-id>
<name>
<surname>Delvendahl</surname>
<given-names>Igor</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>igor.delvendahl@physiologie.uni-freiburg.de</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>Department of Molecular Life Sciences, University of Zurich (UZH)</institution></institution-wrap>, <city>Zurich</city>, <country country="CH">Switzerland</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>Neuroscience Center Zurich</institution></institution-wrap>, <city>Zurich</city>, <country country="CH">Switzerland</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0245cg223</institution-id><institution>Institute of Physiology, Faculty of Medicine, University of Freiburg</institution></institution-wrap>, <city>Freiburg</city>, <country country="DE">Germany</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>Brain Research Institute, University of Zurich</institution></institution-wrap>, <city>Zurich</city>, <country country="CH">Switzerland</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Neurobiology, Stanford University</institution></institution-wrap>, <city>Stanford</city>, <country country="US">United States</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Bioengineering, Stanford University</institution></institution-wrap>, <city>Stanford</city>, <country country="US">United States</country></aff>
<aff id="a7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01bmjkv45</institution-id><institution>Friedrich Miescher Institute for Biomedical Research</institution></institution-wrap>, <city>Basel</city>, <country country="CH">Switzerland</country></aff>
<aff id="a8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s6k3f65</institution-id><institution>Faculty of Natural Sciences, University of Basel</institution></institution-wrap>, <city>Basel</city>, <country country="CH">Switzerland</country></aff>
<aff id="a9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University Research Priority Program (URPP), Adaptive Brain Circuits in Development and Learning (AdaBD), University of Zurich</institution></institution-wrap>, <city>Zurich</city>, <country country="CH">Switzerland</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Dickman</surname>
<given-names>Dion K</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Southern California</institution>
</institution-wrap>
<city>Los Angeles</city>
<country>United States of America</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Chen</surname>
<given-names>Lu</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution>Stanford University</institution>
</institution-wrap>
<city>Stanford</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2024-06-28">
<day>28</day>
<month>06</month>
<year>2024</year>
</pub-date>
<pub-date date-type="update" iso-8601-date="2025-02-17">
<day>17</day>
<month>02</month>
<year>2025</year>
</pub-date>
<volume>13</volume>
<elocation-id>RP98485</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2024-04-11">
<day>11</day>
<month>04</month>
<year>2024</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2024-04-03">
<day>03</day>
<month>04</month>
<year>2024</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.02.565316"/>
</event>
<event>
<event-desc>Reviewed preprint v1</event-desc>
<date date-type="reviewed-preprint" iso-8601-date="2024-06-28">
<day>28</day>
<month>06</month>
<year>2024</year>
</date>
<self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98485.1"/>
<self-uri content-type="editor-report" xlink:href="https://doi.org/10.7554/eLife.98485.1.sa3">eLife assessment</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98485.1.sa2">Reviewer #1 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98485.1.sa1">Reviewer #2 (Public Review):</self-uri>
<self-uri content-type="referee-report" xlink:href="https://doi.org/10.7554/eLife.98485.1.sa0">Reviewer #3 (Public Review):</self-uri>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2024, O’Neill et al</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>O’Neill et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-98485-v2.pdf"/>
<abstract>
<title>Abstract</title><p>Quantitative information about synaptic transmission is key to our understanding of neural function. Spontaneously occurring synaptic events carry fundamental information about synaptic function and plasticity. However, their stochastic nature and low signal-to-noise ratio present major challenges for the reliable and consistent analysis. Here, we introduce miniML, a supervised deep learning- based method for accurate classification and automated detection of spontaneous synaptic events. Comparative analysis using simulated ground-truth data shows that miniML outperforms existing event analysis methods in terms of both precision and recall. miniML enables precise detection and quantification of synaptic events in electrophysiological recordings. We demonstrate that the deep learning approach generalizes easily to diverse synaptic preparations, different electrophysiological and optical recording techniques, and across animal species. miniML provides not only a comprehensive and robust framework for automated, reliable, and standardized analysis of synaptic events, but also opens new avenues for high-throughput investigations of neural function and dysfunction.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Synaptic transmission</kwd>
<kwd>Machine learning</kwd>
<kwd>Data analysis</kwd>
<kwd>Neurons</kwd>
<kwd>Electrophysiology</kwd>
<kwd>Imaging</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Our main revisions are:
(i)We have generated a graphical user interface for miniML to facilitate use of the software.
(ii)We have expanded the benchmarking to compare miniML with SimplyFire, both on synthetic and real data.
(iii)We have added additional examples of the use of miniML for detecting mEPSPs in electrophysiology and voltage imaging data.
</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Synaptic communication serves as the fundamental basis for a wide spectrum of brain functions, from computa- tion and sensory integration to learning and memory. Synaptic transmission either arises from spontaneous or action potential-evoked fusion of neurotransmitter-filled synaptic vesicles (<xref ref-type="bibr" rid="c33">Kaeser and Regehr, 2014</xref>) resulting in an electrical response in the postsynaptic cell. Such synaptic events are a salient feature of all neural circuits and can be recorded using electrophysiological or imaging techniques.</p>
<p>Random fluctuations in the release machinery or intracellular Ca2+ concentration cause spontaneous fusions of single vesicles (’miniature events’) (<xref ref-type="bibr" rid="c34">Kavalali, 2015</xref>), which play an important role in synaptic development and stability (<xref ref-type="bibr" rid="c11">Banerjee et al., 2021</xref>; <xref ref-type="bibr" rid="c33">Kaeser and Regehr, 2014</xref>; <xref ref-type="bibr" rid="c34">Kavalali, 2015</xref>; McKinney et al., 1999). Measurements of amplitude, kinetics, and timing of these events provide essential information about the function of individual synapses and neural circuits. Miniature events are therefore key to our understanding of fundamental processes, such as synaptic plasticity or synaptic computation that support neural function (<xref ref-type="bibr" rid="c1">Abbott and Regehr, 2004</xref>; <xref ref-type="bibr" rid="c25">Holler et al., 2021</xref>). For example, amplitude changes of miniature events are a proxy of neurotransmitter receptor modulation, which is thought to be the predominant mechanism driving activity-dependent long-term alterations in synaptic strength (<xref ref-type="bibr" rid="c26">Huganir and Nicoll, 2013</xref>; <xref ref-type="bibr" rid="c44">Malinow and Malenka, 2002</xref>) and homeostatic synaptic plasticity (<xref ref-type="bibr" rid="c50">O’Brien et al., 1998</xref>; <xref ref-type="bibr" rid="c68">Turrigiano et al., 1998</xref>). In addition to spontaneous vesicle fusions, synaptic events can also result from presynaptic action potentials in neural networks. Alterations in spontaneous neurotransmission have been observed in models of different neurodevelopmental and neurodegenerative disorders (<xref ref-type="bibr" rid="c4">Alten et al., 2021</xref>; Ardiles et al., 2012; <xref ref-type="bibr" rid="c48">Miller et al., 2014</xref>). Thus, a comprehensive analysis of synaptic events is paramount for studying synaptic function and understanding neural diseases. However, the detection and quantification of synaptic events in electrophysiological or fluorescence recordings remains a major challenge. Synaptic events are often small in size, resulting in a low signal-to-noise ratio (SNR), and their stochastic occurrence further complicates reliable detection and evaluation.</p>
<p>Several techniques have been developed for synaptic event detection: Finite-difference approaches use crossings of a predefined threshold, typically in either raw data (<xref ref-type="bibr" rid="c35">Kim et al., 2021</xref>), baseline-normalized data (<xref ref-type="bibr" rid="c38">Kudoh and Taguchi, 2002</xref>), or their derivatives (<xref ref-type="bibr" rid="c5">Ankri et al., 1994</xref>). Template-based methods use a predefined template and generate a matched filter via a scaling factor (<xref ref-type="bibr" rid="c15">Clements and Bekkers, 1997</xref>; <xref ref-type="bibr" rid="c32">Jonas et al., 1993</xref>), deconvolution (Perńıa-Andrade et al., 2012) or an optimal filtering approach (<xref ref-type="bibr" rid="c60">Shi et al., 2010</xref>; Zhang, Schlö gl, et al., 2021). In addition, Bayesian inference (<xref ref-type="bibr" rid="c47">Merel et al., 2016</xref>), machine learning (Wang, Marino, et al., 2024), and peak finding routines (<xref ref-type="bibr" rid="c49">Mori et al., 2024</xref>) can be used to detect synaptic events. These techniques have facilitated the analysis of synaptic events. However, they also have several relevant limitations, such as a strong dependence of detection performance on a threshold and other decisive hyperparameters, or the need for visual inspection of results by an experienced investigator to avoid false positives. The widespread use of synaptic event recordings and the difficulty in obtaining results that are reliable across investigators and laboratories highlight the need for an automated, accurate, efficient, and reproducible synaptic event analysis method.</p>
<p>Artificial intelligence (AI) technologies such as deep learning (<xref ref-type="bibr" rid="c39">LeCun et al., 2015</xref>) can significantly enhance biological data analysis (<xref ref-type="bibr" rid="c57">Richards et al., 2022</xref>) and thus contribute to a better understanding of neural function. Convolutional neural networks (CNNs) are especially effective for image classification, but can also be used for one-dimensional data (<xref ref-type="bibr" rid="c21">Fawaz et al., 2019</xref>; Wang, Yan, et al., 2016). CNNs have been successfully applied in neuroscience to segment brain regions (<xref ref-type="bibr" rid="c29">Iqbal et al., 2019</xref>), detect synaptic vesicles in electron microscopy images (<xref ref-type="bibr" rid="c27">Imbrosci et al., 2022</xref>), identify spikes in Ca2+ imaging data (Rupprecht, Carta, et al., 2021), and localize neurons in brain slices (<xref ref-type="bibr" rid="c74">Yip et al., 2021</xref>), or neurons with fluorescence signals in time-series data (<xref ref-type="bibr" rid="c17">Denis et al., 2020</xref>; Sitá et al., 2022).</p>
<p>Here, we present <italic>miniML</italic>, a novel approach for detecting and analyzing spontaneous synaptic events using supervised deep learning. The miniML model is an end-to-end classifier trained on an extensive dataset of annotated synaptic events. When applied to time-series data, miniML provides high-performance event detection with negligible false-positive rates, outperforming existing methods. The method is fast, robust to threshold choice, and generalizable across diverse data types, allowing for efficient and reproducible analysis, even for large datasets. We anticipate that AI-based synaptic event analysis will greatly enhance the investigation of synaptic function, plasticity, and computation from the individual synapse to the network level.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>miniML enables highly accurate classification of synaptic events</title>
<p>To investigate whether an AI model can detect stochastic synaptic events in noisy single-trial time-series data, we designed a deep neural network consisting of CNN, long short-term memory (LSTM), and fully connected dense layers (’miniML’, <xref rid="fig1" ref-type="fig">Figure 1A</xref>). The CNN-LSTM model takes a section of a univariate time-series recording as input and outputs a label for that section of data (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). The miniML model is trained to classify short segments of electrophysiological data as either positive or negative for a synaptic event using supervised learning. The trained classifier can then be applied to unseen time-series data to localize events.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>High performance classification of synaptic events using a deep neural network.</title>
<p>(<bold>A</bold>) Overview of the analysis workflow. Data segments from electrophysiological recordings are extracted and labeled to train an artificial neural network. The deep learning-based model is then applied to detect events in novel time-series data. (<bold>B</bold>) Schematic of the model design. Data is input to a convolutional network consisting of blocks of 1D convolutional, ReLU, and average pooling layers. The output of the convolutional layers is processed by a bidirectional LSTM block, followed by two fully connected layers. The final output is a label in the interval [0, 1]. (<bold>C</bold>) Loss (binary crossentropy) and accuracy of the model over training epochs for training and validation data. (<bold>D</bold>) Receiver operating characteristic of the best performing model. Area under the curve (AUC) is indicated; dashed line indicates performance of a random classifier. (<bold>E</bold>) UMAP representation of the training data as input to the final layer of the model, indicating linear separability of the two event classes after model training.</p>
</caption>
<graphic xlink:href="565316v3_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig1s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1-figure supplement 1</label>
<caption><title>Visualization of model training.</title>
<p>(<bold>A</bold>) Saliency maps (<xref ref-type="bibr" rid="c61">Simonyan et al., 2013</xref>) for four example events of the training data. Darker regions indicate discriminative data segments. Data and saliency values are min-max scaled. (<bold>B</bold>) The miniML model transforms input to enhance separability. Shown is a Uniform Manifold Approximation and Projection (UMAP) dimensionality reduction of the original training dataset. (<bold>C</bold>) UMAP of the input to the final ML model layer. Examples of labeled training samples are illustrated. Model training greatly improves linear separability of the two labeled event classes.</p></caption>
<graphic xlink:href="565316v3_fig1s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig1s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1-figure supplement 2</label>
<caption><title>Impact of dataset size, class balance, and model architecture on training performance.</title>
<p>(<bold>A</bold>) To test how size of the training dataset impacts model training, we took random subsamples from the MF–GC dataset and trained miniML models using fivefold cross validation. (<bold>B–D</bold>) Comparison of loss (<bold>B</bold>), accuracy (<bold>C</bold>) and area under the ROC curve (AUC; <bold>D</bold>) across increasing dataset sizes. Data are means of model training sessions with k-fold cross-validation. Shaded areas represent SD. Note the log-scale of the abscissa. (<bold>E</bold>) Comparison of model training with unbalanced training data. (<bold>F</bold>) Area under the ROC curve for models trained with different levels of unbalanced training data. Unbalanced datasets impair classification performance. (<bold>G</bold>) Accuracy and F1 score for different model architectures plotted against number of free parameters. The CNN-LSTM architecture provided the best model performance with the lowest number of free parameters. <italic>EarlyStopping</italic> was used for all models to prevent overfitting (difference between training and validation accuracy <italic>&lt;</italic>0.3%). ResNet, Residual Neural Network; MLP, multi-layer perceptron.</p></caption>
<graphic xlink:href="565316v3_fig1s2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To train the miniML model, we first extracted a large number of synaptic events and corresponding event-free sections from previous voltage-clamp recordings of cerebellar mossy fiber to granule cell (MF–GC) miniature excitatory postsynaptic currents (mEPSCs) (Delvendahl et al., 2019). All samples were then visually inspected and labeled to generate the training dataset. We applied data augmentation techniques to include examples of typical false positives in the training data (Materials and Methods). In total, the training data comprised ∼30,000 samples that were split into training and validation sets (0.75/0.25). Across training epochs, loss decreased and accuracy increased, stabilizing after ∼30 epochs (<xref rid="fig1" ref-type="fig">Figure 1C</xref>). The model with the highest validation accuracy was selected for further use, achieving 98.4% (SD 0.1, fivefold cross-validation). Saliency map analysis (<xref ref-type="bibr" rid="c61">Simonyan et al., 2013</xref>) indicated that the AI model mainly relied on the data sections around the peak of synaptic events to discriminate with respect to the labels (<xref rid="fig1s1" ref-type="fig">Figure 1—figure supplement 1</xref>). The trained miniML model achieved an area under the receiver operating characteristic (ROC) curve close to 1 (<xref rid="fig1" ref-type="fig">Figure 1D</xref>), indicating almost perfect separability of the classes (<xref rid="fig1" ref-type="fig">Figure 1E</xref>, <xref rid="fig1s1" ref-type="fig">Figure 1—figure supplement 1</xref>). Deep learning typically requires large datasets for training (<xref ref-type="bibr" rid="c69">van der Ploeg et al., 2014</xref>). To investigate how miniML’s classification performance depended on the dataset size, we systematically varied the number of training samples. As expected, the accuracy increased with larger datasets. However, the performance gain was marginal when exceeding 5,000 samples (<italic>&lt;</italic>0.2%, <xref rid="fig1s2" ref-type="fig">Figure 1—figure supplement 2</xref>), indicating that relatively small datasets suffice for effective model training (<xref ref-type="bibr" rid="c10">Bailly et al., 2022</xref>). These results demonstrate the efficacy of supervised deep learning in accurately classifying segments of neurophysiological data containing synaptic events.</p>
</sec>
<sec id="s2b">
<title>The miniML model robustly detects synaptic events in electrophysiological recordings</title>
<p>Synaptic events are typically recorded as continuous time-series data of either membrane voltage or current. To apply the trained classifier to detect events in arbitrarily long data, we used a sliding window approach (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). Time-series data are divided into overlapping sections that correspond to the input shape of the CNN-LSTM classifier. We used a stride for the sliding window, which reduces the number of inferences needed and speeds up the computation time while maintaining high detection performance (<xref rid="fig2s1" ref-type="fig">Figure 2—figure supplement 1</xref>). By reshaping the data into short sections, model inference can be run in batches and employ parallel processing techniques, including graphics processing unit (GPU) computing, resulting in analysis times of a few seconds for minute-long recordings (<xref rid="fig2s1" ref-type="fig">Figure 2—figure supplement 1</xref>). The miniML detection method is thus time-efficient and can be easily integrated into (high-throughput) data analysis pipelines. To facilitate the use of the method, miniML also includes a graphical user interface (<xref rid="fig2s2" ref-type="fig">Figure 2—figure supplement 2</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Applying AI-based classification to robustly detect synaptic events in electrophysiological time-series data.</title>
<p>(<bold>A</bold>) Event detection workflow using a deep learning classifier. Time-series data are reshaped with a window size corresponding to length of the training data and a given stride. Peak detection of the model output allows event localization (orange dashed line indicates peak threshold) and subsequent quantification. (<bold>B</bold>) Example of event detection in a voltage-clamp recording from a cerebellar granule cell. Top: Prediction trace (top) and corresponding raw data with synaptic events (bottom). Dashed line indicates minimum peak height of 0.5, orange circles indicate event localizations. (<bold>C</bold>) Zoom in for different data segments. Detected events are highlighted by gray boxes. (<bold>D</bold>) Event analysis for the cell shown in (<bold>B</bold>) (total recording time, 120 s). Top: Detected events with average (light blue). Bottom: Event amplitude histogram.</p>
<p><xref rid="fig2s1" ref-type="fig">Figure 2—figure supplement 1</xref>. Fast computation time for event detection using miniML.</p>
</caption>
<graphic xlink:href="565316v3_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2-figure supplement 1</label>
<caption><title>Fast computation time for event detection using miniML.</title>
<p>(<bold>A</bold>) Detected events and analysis runtime plotted versus stride. Note that runtime can be minimized by using stride sizes up to 5% of the event window size without impacting detection performance. (<bold>B</bold>) Analysis runtime with different computer hardware for a 120-s long recording at 50 kHz sampling (total of 6,000,000 samples). Runtime is given as wall time including event analysis. GPU computing enables analysis runtimes shorter than 20 s.</p></caption>
<graphic xlink:href="565316v3_fig2s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2-figure supplement 2</label>
<caption><title>A graphical user interface for miniML.</title>
<p>(<bold>A</bold>) Workflow for synaptic event analysis using miniML. Optional steps include data pre-processing, model selection, and event rejection. (<bold>B</bold>) Screenshot of the graphical user interface (GUI). Users can use the GUI to load, inspect, and analyze data. After running the event detection, all detected events are marked by red dots. Individual event parameters are displayed in tabular form, where events can be enlarged and rejected. The final results can be saved in different formats via the GUI.</p></caption>
<graphic xlink:href="565316v3_fig2s2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig2s3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2-figure supplement 3</label>
<caption><title>miniML performance on event-free data.</title>
<p>(<bold>A</bold>) Confidence (top) and raw data with detected events from a MF–GC recording. Dashed line indicates miniML minimum peak height. (<bold>B</bold>) Recording from the same cell after addition of blockers of synaptic transmission (NBQX, APV, Bicuculline, Strychnine). miniML does not detect any events under these conditions. Note that addition of Bicuculline blocks tonic inhibition in cerebellar GCs, causing a reduction in holding current and reduced noise (<xref ref-type="bibr" rid="c37">Kita et al., 2021</xref>).</p></caption>
<graphic xlink:href="565316v3_fig2s3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>The output of the miniML model predicts the label—no event or event—for each time step (i.e., stride) with a numerical value ranging from zero to one (<xref rid="fig1" ref-type="fig">Figure 1B</xref>). These output values can be interpreted as the confidence that the corresponding data segment contains a synaptic event. Model inference thus outputs a prediction trace that has a slightly lower temporal resolution compared to the original input according to the stride of the sliding window. Using peak finding on the prediction trace allows extracting data segments with individual events from a recording. While 0.5 represents a reasonable minimum peak value, the exact choice is not critical to detection performance (see below). To quantify events, extracted sections are checked for overlapping events, and detected events are aligned by the steepest slope, allowing calculation of individual event statistics. <xref rid="fig2" ref-type="fig">Figure 2B</xref> illustrates the sliding window approach to detect spontaneous events in time-series data, such as a continuous voltage-clamp recording of spontaneous mEPSCs in a cerebellar GC. miniML provided a clear peak for all synaptic events present, without false positives (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, <xref rid="fig2s3" ref-type="fig">Figure 2—figure supplement 3</xref>), allowing fast and reliable event quantification (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). These data demonstrate that a deep learning model can be applied to detect synaptic events in electrophysiological time-series data.</p>
</sec>
<sec id="s2c">
<title>AI-based event detection is superior to previous methods</title>
<p>To benchmark the AI model’s event detection performance, we compared it with commonly used template-based approaches (template-matching (<xref ref-type="bibr" rid="c15">Clements and Bekkers, 1997</xref>) and deconvolution (Perńıa-Andrade et al., 2012)), and a finite-threshold-based method (<xref ref-type="bibr" rid="c38">Kudoh and Taguchi, 2002</xref>). Some of these—or similar—algorithms are implemented in proprietary software solutions to record and analyze electrophysiological data. We also included a Bayesian event detection approach (<xref ref-type="bibr" rid="c47">Merel et al., 2016</xref>), SimplyFire (<xref ref-type="bibr" rid="c49">Mori et al., 2024</xref>), and the automated event detection routine of MiniAnalysis software (Synaptosoft Inc.). To compare synaptic event detection methods, we developed a standardized benchmarking pipeline (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). We first performed event-free voltage-clamp recordings from mouse cerebellar GCs (in the presence of blockers of inhibitory and excitatory transmission; see Materials and Methods and <xref rid="fig2s3" ref-type="fig">Figure 2—figure supplement 3</xref>). We then generated synthetic events with a two-exponential time course and superimposed these on the raw recording to produce ground-truth data (Figure 3B–C). Event amplitudes were drawn from a log-normal distribution (<xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>) with varying means to cover the range of SNRs typically observed in recordings of miniature events (2–15 dB, data from n = 170 GC recordings, Figure 3D). We generated events with kinetics that closely matched the template used for the matched-filtering approaches, thus ensuring a conservative comparison of miniML with other methods applied under optimal conditions (i.e., using the exact average event shape as template). To measure detection performance, we calculated recall (i.e., sensitivity), precision (fraction of correct identifications) and the F1 score. Recall depended on SNR for all methods, with miniML and deconvolution showing the highest values (<xref rid="fig3" ref-type="fig">Figure 3E</xref>). The precision was highest for miniML, which detected no false positives at any SNR, in contrast to all other methods (<xref rid="fig3" ref-type="fig">Figure 3F</xref>). When assessing overall performance, miniML provided the highest F1 scores across SNRs (<xref rid="fig3" ref-type="fig">Figure 3G</xref>). In addition, miniML showed superior results when changing event kinetics, indicating higher robustness to variable event shapes (<xref rid="fig3" ref-type="fig">Figure 3H</xref> and <xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>), which may be particularly important in neurons with diverse synaptic inputs due to mixed receptor populations (<xref ref-type="bibr" rid="c41">Lesperance et al., 2020</xref>), or during pharmacological experiments (<xref ref-type="bibr" rid="c30">Ishii et al., 2020</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Systematic benchmarking demonstrates that AI-based event detection is superior to previous methods.</title>
<p>(<bold>A</bold>) Scheme of event detection benchmarking. Six methods are compared using precision and recall metrics. (<bold>B</bold>) Event-free recordings were superimposed with generated events to create ground-truth data. Depicted example data have a signal-to-noise ratio (SNR) of 9 dB. (<bold>C</bold>) Left: Output of the detection methods for data in (<bold>B</bold>). Right: Improvement in SNR relative to the data. Note that MiniAnalysis is omitted from this analysis because the software does not provide output trace data. (<bold>D</bold>) SNR from mEPSC recordings at MF–GC synapses (n = 170, whiskers cover full range of data). (<bold>E–G</bold>) Recall, precision, and F1 score versus SNR for the six different methods. Data are averages of three independent runs for each SNR. (<bold>H</bold>) Average F1 score versus event kinetics. Detection methods relying on an event template (template-matching, deconvolution and Bayesian) are not very robust to changes in event kinetics. (<bold>I</bold>) Evaluating the threshold dependence of detection methods. Asterisk marks event close to detection threshold. (<bold>J</bold>) Number of detected events vs. threshold (in % of default threshold value, range 5–195) for different methods. Dashed line indicates true event number.</p>
</caption>
<graphic xlink:href="565316v3_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig3s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3-figure supplement 1</label>
<caption><title>Extended benchmarking and threshold dependence of event detection.</title>
<p>(<bold>A</bold>) Example event-free recording (Top) and power density spectrum of the data (Bottom). (<bold>B</bold>) Top: Data trace from (<bold>A</bold>) superimposed with simulated synaptic events. Bottom: Amplitude histogram of simulated events for a signal-to-noise ratio (SNR) of 10.8 dB. Solid line represents the log-normal distribution from which event amplitudes were drawn; Inset shows individual simulated events. (<bold>C</bold>) Recall, precision and F1 score for five detection methods with simulated events that have a 2× faster decay than in <xref rid="fig3" ref-type="fig">Figure 3</xref>. (<bold>D</bold>) Same as in (<bold>C</bold>), but for events with a 4.5× slower decay time constant. Note that we did not adjust detection hyperparameters for the comparisons in (<bold>C</bold>) and (<bold>D</bold>). (<bold>E</bold>) Runtime of five different synaptic event detection methods for a 120-s section of data recorded with 50 kHz sampling rate. Average wall-time of five runs. * miniML was run using a GPU, and data were downsampled to 20 kHz for the Bayesian analysis. (<bold>F</bold>) Normalized amplitude of the peaks in the different methods’ detection traces (”detection peak amplitude”) versus normalized event amplitude. A linear dependence indicates that detection traces contain implicit amplitude information, which will cause a strong threshold dependence of the detection result. Note that miniML’s output does not contain amplitude information. (<bold>G</bold>) F1 score versus threshold (in % of default threshold value, range 5–195) for different methods.</p></caption>
<graphic xlink:href="565316v3_fig3s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Conventional event detection methods typically produce a detection trace with a shape identical to the input data and values in an arbitrary range (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). In contrast, miniML generates output in the interval [0, 1], which can be interpreted as the confidence of event occurrence. The output of event detection methods—the detection trace—significantly increases the SNR with respect to the original data. In our benchmark scenario, miniML and the Bayesian method provided the greatest discrimination from background noise (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). To locate the actual event positions, a threshold must be applied to the detection trace, which can drastically affect the results, as peaks in the detection trace may depend on the event amplitude. Intriguingly, miniML’s detection trace peaks did not depend on event amplitudes, setting it apart from other methods (<xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>). For template-based methods, recommendations on threshold selection are provided (<xref ref-type="bibr" rid="c15">Clements and Bekkers, 1997</xref>; Perńıa-Andrade et al., 2012), but users usually need to adjust this parameter according to their specific data. The choice of the threshold strongly influences the detection performance, as even small changes can lead to marked increases in false positives or false negatives. To investigate the threshold dependence of different methods, we systematically varied the threshold and analyzed the number of detected events and the F1 score. Notably, miniML’s detection performance remained consistent over a wide range (5–195%, Figure 3I–J and <xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>), with false positives occurring only at the lower threshold limit (5%, corresponding to a cutoff value of 0.025 in the detection trace). Conversely, the other detection methods were very sensitive to threshold changes (<xref rid="fig3" ref-type="fig">Figure 3J</xref> and <xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>) due to the comparatively low SNR ratio of the output traces (but note that the Bayesian method is only slightly threshold dependent). In addition, the miniML threshold (i.e., minimum peak) value is bounded with an intuitive meaning. These comparisons underscore that miniML requires no prior knowledge of the exact event shape and is virtually threshold independent, thus enabling reliable event detection.</p>
</sec>
<sec id="s2d">
<title>miniML reliably detects spontaneous synaptic events in out-of-sample data</title>
<p>We trained the miniML model using data from cerebellar MF–GC synapses, which enabled reliable analysis of mEPSC recordings from this preparation (<xref rid="fig2" ref-type="fig">Figure 2</xref>, <xref rid="fig4" ref-type="fig">Figure 4A–C</xref>). Comparison with several previous detection methods revealed that miniML detected more events with a waveform that is consistent with mEPSCs (<xref rid="fig4" ref-type="fig">Figure 4D</xref>, <xref rid="fig4s1" ref-type="fig">Figure 4—figure supplement 1</xref>). Synaptic event properties, such as kinetics, SNR, or frequency, are variable between preparations. This heterogeneity is caused by, for example, differences in the number and type of postsynaptic neurotransmitter receptors, the number of synaptic inputs, postsynaptic membrane properties, or presynaptic release properties. To test miniML’s generalizability, we evaluated the detection performance on out-of-sample data. We recorded and analyzed data from the mouse calyx of Held synapse (<xref rid="fig4" ref-type="fig">Figure 4E</xref>), a large axosomatic synapse in the auditory brainstem that relays rate-coded information over a large bandwidth and with high temporal precision. Due to the large number of active zones, synaptic events are quite frequent. Despite being trained on cerebellar MF–GC data, miniML accurately detected mEPSCs in these recordings (<xref rid="fig4" ref-type="fig">Figure 4F–H</xref>). This result may be facilitated by the fast event kinetics at the calyx of Held, which approach those of MF–GC mEPSCs. To test whether miniML could also detect events with slower kinetics, we applied it to recordings from cerebellar Golgi cells (<xref rid="fig4" ref-type="fig">Figure 4I</xref>) (<xref ref-type="bibr" rid="c37">Kita et al., 2021</xref>). These interneurons in the cerebellar cortex provide surround inhibition to GCs and receive input from parallel fiber and MF synapses. miniML reliably detected synaptic events in these recordings (<xref rid="fig4" ref-type="fig">Figure 4J–L</xref>), although event decay kinetics were slower compared to the training data (Golgi cell, 1.83 ms (SD 0.44 ms, n = 10 neurons), GC training data, 0.9 ms). Synaptic events are also commonly recorded from neuronal culture preparations. We determined whether miniML could be applied to recordings from cultured human induced pluripotent stem cell (iPSC)-derived neurons. We patch-clamped neurons in eight-week-old cultures of predominantly cortical glutamatergic identity (<xref ref-type="bibr" rid="c7">Asadollahi et al., 2023</xref>) and recorded spontaneous synaptic events in voltage-clamp (<xref rid="fig4" ref-type="fig">Figure 4M</xref>). Using miniML on these human iPSC-derived neuron data showed robust detection of synaptic events (<xref rid="fig4" ref-type="fig">Figure 4N–P</xref>), which had an average frequency of 0.15 Hz (SD 0.24 Hz, n = 56 neurons). Taken together, the consistent performance across different synaptic preparations indicates that the miniML model can be directly applied to out-of-sample data with similar kinetics. miniML also provided higher event detection accuracy than template-based or finite-threshold-based detection methods, which are prone to false positives (<xref rid="fig4" ref-type="fig">Figure 4D,H,L,P</xref>, <xref rid="fig4s1" ref-type="fig">Figure 4—figure supplement 1</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Application of miniML to electrophysiological recordings from diverse synaptic preparations.</title>
<p>(<bold>A</bold>) Schematic and example voltage-clamp recordings from mouse cerebellar GCs. Orange circles mark detected events. (<bold>B</bold>) Representative individual mEPSC events detected by miniML. (<bold>C</bold>) All detected events from (<bold>A</bold>), aligned and overlaid with average (light blue). (<bold>C</bold>) Detected event numbers for miniML, template matching, and deconvolution. (<bold>E–H</bold>) Same as in (<bold>A–D</bold>), but for recordings from mouse calyx of Held synapses. (<bold>I–L</bold>) Same as in (<bold>A–D</bold>), but for recordings from mouse cerebellar Golgi cells. (<bold>M–P</bold>) Same as in (<bold>A–D</bold>), but for recordings from cultured human induced pluripotent stem cell (iPSC)-derived neurons. miniML consistently detects spontaneous events in all four preparations and retrieves more events than matched-filtering approaches.</p>
</caption>
<graphic xlink:href="565316v3_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig4s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4-figure supplement 1</label>
<caption><title>Event detection in different synaptic preparations.</title>
<p>(<bold>A</bold>) Amplitude histogram with kernel-density estimate (light blue line) of miniML-detected events for the mouse granule cell recording shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>. (<bold>B</bold>) Detected events for miniML and matched-filtering approaches. Color-coded representative examples of events unique to any of the three detection methods are shown. (<bold>C</bold>) Detected events for miniML and two finite-threshold approaches with representative unique detected events. (<bold>D–F</bold>) Same as in (<bold>A–C</bold>), but for the calyx of Held example. (<bold>G–H</bold>) Same as in (<bold>A–C</bold>), but for the Golgi cell example. Due to the slower event kinetics, miniML was run with a 1.5× larger window size. (<bold>J–L</bold>) Same as in (<bold>A–C</bold>), but for the hiPSC-derived neuron example. Example event traces were filtered for display purposes with a 18-samples Hann window.</p></caption>
<graphic xlink:href="565316v3_fig4s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2e">
<title>Generalization of miniML to diverse event and data types via transfer learning</title>
<p>While applicable to out of sample data (<xref rid="fig4" ref-type="fig">Figure 4</xref>), simulations indicated that larger differences in event kinetics and waveform may ultimately hinder detection when using the MF–GC mEPSC model (<xref rid="fig5s1" ref-type="fig">Figure 5—figure supplement 1</xref>). Not only distinct event waveforms, but also different hardware and recording conditions may affect the characteristics of the recorded data. Scalability and generalizability are important for robust event detection. To facilitate the application of miniML to different preparations, recording conditions, and data types, we employed a transfer learning (TL) strategy. TL is a powerful technique in machine learning that allows for the transfer of knowledge learned from one task or domain to another (<xref ref-type="bibr" rid="c14">Caruana, 1994</xref>; <xref ref-type="bibr" rid="c75">Yosinski et al., 2014</xref>). TL is widely used with CNNs to take advantage of large pre-trained models and repurpose them to solve new, unseen tasks (<xref ref-type="bibr" rid="c66">Theodoris et al., 2023</xref>). Importantly, only a part of the network needs to be trained for the novel task, which significantly reduces the number of training samples needed and speeds up training while avoiding overfitting (<xref ref-type="bibr" rid="c75">Yosinski et al., 2014</xref>). We therefore reasoned that TL based on freezing the convolutional layers during training of our pre-trained network could be used to train a new model to detect events with different shapes and/or kinetics, using a lower number of training samples (<xref rid="fig5" ref-type="fig">Figure 5A</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Transfer learning allows analyzing different types of events with small amounts of training data.</title>
<p>(<bold>A</bold>) Illustration of the transfer learning (TL) approach. The convolutional layers of a trained miniML model are frozen before retraining with new, smaller datasets. (<bold>B</bold>) Comparison of TL and full training for MF–GC mEPSP data. Shown are loss and accuracy versus sample size for full training and TL. Indicated subsets of the dataset were split into training and validation sets; lines are averages of fivefold crossvalidation, shaded areas represent 95% CI. Note the log-scale of the abscissa. (<bold>C</bold>) Average AUC, accuracy, and training time of models trained with the full dataset or with TL on a reduced dataset. TL models yield comparable classification performance with reduced training time using only 12.5% of the samples. (<bold>D</bold>) Example recordings of synaptic events in voltage-clamp and current-clamp mode consecutively from the same neuron. (<bold>E</bold>) Average peak normalized mEPSC and mEPSP waveform from the example in (<bold>D</bold>). Note the different event kinetics depending on recording mode. (<bold>F</bold>) Event frequency for mEPSPs plotted against mEPSCs. Dashed line represents identity, solid line with shaded area represents linear regression.</p>
</caption>
<graphic xlink:href="565316v3_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5-figure supplement 1</label>
<caption><title>Recall depends on event kinetics.</title>
<p>(<bold>A</bold>) Recall versus event kinetics for the MF–GC model. Kinetics (i.e., rise and decay time constants) of simulated events were changed as indicated. miniML robustly detects events with up to ∼4-fold slower kinetics (dark blue, dashed line indicates 80% recall). Resampling of the data improves the recall in synthetic data with altered event kinetics (light blue). (<bold>B</bold>) Event kinetics for different preparations and/or recording modes. Data are normalized to MF–GC mEPSCs (dashed line).</p></caption>
<graphic xlink:href="565316v3_fig5s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig5s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5-figure supplement 2</label>
<caption><title>Transfer learning facilitates model training across different datasets.</title>
<p>(<bold>A</bold>) Loss and accuracy versus number of training dataset samples for three different datasets (mouse MF–GC mEPSPs, <italic>Drosophila</italic> NMJ mEPSCs, zebrafish (ZF) spontaneous EPSCs). Dashed lines indicate transfer learning (TL), whereas solid lines depict results from full training. Points are averages of fivefold cross-validation and shaded areas represent 95% CI. (<bold>B</bold>) Average AUC, accuracy, and training time for TL using 500 samples, and full training using 4,000 samples. Error bars denote 95% CI.</p></caption>
<graphic xlink:href="565316v3_fig5s2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>We tested the use of TL for miniML with recordings of miniature excitatory postsynaptic potentials (mEPSPs) in mouse cerebellar GCs. These events have opposite polarity and slower kinetics compared to the original mEPSC training data. We compared TL-based model training to full training (with all layers trainable and reinitialized weights), varying the training sample size. Whereas accuracy increased and loss decreased with the number of samples (<xref rid="fig5" ref-type="fig">Figure 5B</xref>), TL models performed well with as few as 400 samples. Under these conditions, accuracy was only slightly lower than for full training with almost ten times the sample size (median accuracy, 95.4 versus 96.1%; <xref rid="fig5" ref-type="fig">Figure 5C</xref>). This suggests that TL can significantly reduce the sample size needed for training, minimizing the time-consuming process of event extraction and labeling. Across different datasets, TL-trained models performed comparably to those trained from scratch (<xref rid="fig5s2" ref-type="fig">Figure 5—figure supplement 2</xref>). Taken together, these data demonstrate that TL allows model training with an order of magnitude smaller amount of training data, making miniML easily transferable to new datasets with diverse characteristics.</p>
<p>To investigate how a TL-trained model performs in event detection, we analyzed data in different recording modes (voltage-clamp vs. current-clamp). The different noise conditions typically encountered in current-clamp and the difference between EPSP and EPSC waveforms necessitate distinct detection schemes for common detection methods. Recording miniature EPSPs and EPSCs in the same cerebellar GCs (Figure 5D–E) enabled us to compare detection performance via event frequency. mEPSPs could be approximated by a two-exponential time course, similar to postsynaptic currents. However, their kinetics were considerably slower due to the charging of the plasma membrane (<xref rid="fig5" ref-type="fig">Figure 5E</xref>). We used a TL-trained model for mEPSP detection and the standard miniML model for mEPSCs. Remarkably, the average event frequencies were very similar in the two different recording modes (voltage-clamp: 0.49 Hz, SD 0.53 Hz, current-clamp: 0.54 Hz, SD 0.6 Hz, n = 15 for both) and highly correlated across neurons (<xref rid="fig5" ref-type="fig">Figure 5F</xref>). This highlights the reliability of TL with small training sets for the consistent detection of synaptic events in datasets with varying characteristics.</p>
</sec>
<sec id="s2f">
<title>miniML reveals diversity of synaptic event kinetics in an <italic>ex vivo</italic> whole brain preparation</title>
<p>The presence of overlapping and highly variable event shapes pose additional challenges for event detection methods. To evaluate miniML’s performance in such complex scenarios, we analyzed a dataset recorded from principal neurons of the adult zebrafish telencephalon (<xref ref-type="bibr" rid="c59">Rupprecht and Friedrich, 2018</xref>). We focused on spontaneous excitatory inputs to these neurons, characterized by diverse event shapes and frequencies (<xref ref-type="bibr" rid="c59">Rupprecht and Friedrich, 2018</xref>). Training via TL (see Materials and Methods) yielded a model that enabled the reliable detection of spontaneous excitatory currents (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). Analysis of event properties across cells revealed broad distributions of event statistics (<xref rid="fig6" ref-type="fig">Figure 6B</xref>), including a large diversity of event rise and decay kinetics (<xref rid="fig6" ref-type="fig">Figure 6B–C</xref>). Notably, miniML consistently identified synaptic events with diverse kinetics and shapes. We next used the extracted event kinetics features of individual neurons (<xref rid="fig6" ref-type="fig">Figure 6D–H</xref>) to demonstrate miniML’s utility in better understanding the diversity of an existing dataset. First, we explored whether the anatomical location of each neuron could predict event decay times. We mapped the recorded neurons to an anatomical reference and plotted decay times as a function of their position but did not find a strong relationship (<xref rid="fig6" ref-type="fig">Figure 6I</xref>; correlation with position p<italic>&gt;</italic>0.05 in all 3 dimensions). Second, we tested the hypothesis that slower event kinetics are associated with larger cells. In large cells, EPSCs may undergo stronger filtering as they propagate from synaptic sites to the soma. Consistent with this idea, we observed correlations between decay and rise times across neurons (<xref rid="fig6" ref-type="fig">Figure 6J</xref>). Furthermore, the distribution of decay times (examples shown in <xref rid="fig6" ref-type="fig">Figure 6H</xref>) was broader for neurons with longer decay times (<xref rid="fig6" ref-type="fig">Figure 6K</xref>), suggesting a broader distribution of distances from synapses to the cell body. Finally, for a subset of neurons, we recorded input resistance, which approximates the cell membrane resistance and is therefore a proxy for cell size. Input resistance was negatively correlated with decay times across neurons (<xref rid="fig6" ref-type="fig">Figure 6L</xref>), consistent with the hypothesis that diverse event kinetics across neurons are determined by the conditions of synaptic event propagation to the soma and, more specifically, cell size. Taken together, this analysis underscores the versatility of miniML, as it can be successfully applied to new datasets with varying recording conditions. miniML consistently extracted synaptic events across a spectrum of event kinetics, enabling the identification and investigation of key factors determining event kinetics and other event-related properties across neurons.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Synaptic event detection for neurons in a full-brain explant preparation of adult zebrafish.</title>
<p>(<bold>A</bold>) Application of TL to facilitate event detection for EPSC recordings. (<bold>B</bold>) Extraction of amplitudes, event frequencies, decay times, and rise time for all neurons in the dataset (n = 34). (<bold>C</bold>) Typical (mean) event kinetics for the analyzed neurons, ordered by decay times. Event traces are peak-normalized. (<bold>D–E</bold>) Example recordings with slow (dark blue) and fast (light blue) event kinetics. (<bold>F</bold>) Examples of events taken from (<bold>D–E</bold>), illustrating the diversity of kinetics within and across neurons. (<bold>G</bold>) Distribution of event decay kinetics across single events for two example neurons (traces shown in (<bold>D–E</bold>)). (<bold>H</bold>) Distribution of event rise kinetics across single events for same two example neurons. (<bold>I</bold>) Mapping of decay times (color-coded as in (<bold>C</bold>)) onto the anatomical map of the recording subregion of the telencephalon. (<bold>J</bold>) Mean decay and rise times are correlated across neurons. (<bold>K</bold>) Decay time distributions are broader (SD of decay time distributions) when mean decay times are larger. (<bold>L</bold>) Input resistance as a proxy for cell size is negatively correlated with the decay time.</p></caption>
<graphic xlink:href="565316v3_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2g">
<title>miniML robustly detects mEPSC differences upon genetic receptor perturbation</title>
<p>We next applied miniML to analyze data obtained from the <italic>Drosophila melanogaster</italic> larval neuromuscular junction (NMJ) (<xref ref-type="bibr" rid="c9">Baccino-Calace et al., 2022</xref>). This synapse is characterized by a higher frequency of spontaneous synaptic events with a slower time course compared with typical brain slice recordings. In addition, mEPSC recordings are performed using two-electrode voltage-clamp, which can be associated with large leak currents and rather low SNR. Because of the different event shapes in these data, we used a small dataset of manually extracted NMJ events to train a TL model. The TL model was able to reliably detect synaptic events in wild-type (WT) NMJ recordings (<xref rid="fig7" ref-type="fig">Figure 7A–C</xref>). We next assessed event detection upon deletion of the non-essential glutamate receptor subunit <italic>GluRIIA</italic>, which causes a strong reduction in mEPSC amplitude and faster kinetics (<xref ref-type="bibr" rid="c18">DiAntonio et al., 1999</xref>). A separate TL model allowed reliable synaptic event detection in recordings from <italic>GluRIIA</italic> mutant larvae (<xref rid="fig7" ref-type="fig">Figure 7D–F</xref>). We observed a 54% reduction in mEPSC amplitude compared to WT (<xref rid="fig7" ref-type="fig">Figure 7G</xref>), consistent with previous reports (<xref ref-type="bibr" rid="c18">DiAntonio et al., 1999</xref>; <xref ref-type="bibr" rid="c53">Petersen et al., 1997</xref>). In addition, the event frequency was reduced by 64% (<xref rid="fig7" ref-type="fig">Figure 7H</xref>). Although event amplitude distributions had a similar shape in both genotypes (<xref rid="fig7" ref-type="fig">Figure 7C and F</xref>), small events below the detection limit in <italic>GluRIIA</italic> synapses may contribute to the observed frequency difference. Half decay and rise times were also shorter at <italic>GluRIIA</italic> than at WT NMJs (−58% and −18%, respectively) (<xref rid="fig7" ref-type="fig">Figure 7I–J</xref>), which can be explained by the faster desensitization of the remaining <italic>GluRIIB</italic> receptors (<xref ref-type="bibr" rid="c18">DiAntonio et al., 1999</xref>). Thus, miniML can be applied to two-electrode voltage clamp recordings at the <italic>Drosophila</italic> NMJ and robustly resolves group differences upon genetic receptor perturbation.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Event detection at <italic>Drosophila</italic> neuromuscular synapses upon altered glutamate receptor composition.</title>
<p>(<bold>A</bold>) Two-electrode voltage-clamp recordings from wild-type (WT) <italic>Drosophila</italic> NMJs were analyzed using miniML with transfer learning. (<bold>B</bold>) Left: Example voltage-clamp recording with detected events highlighted. Right: Three individual mEPSCs on expanded time scale. (<bold>C</bold>) Left: All detected events from the example in (<bold>B</bold>) overlaid with the average (blue line). Right: Event amplitude histogram. (<bold>D–F</bold>) Same as in (<bold>A–C</bold>), but for <italic>GluRIIA</italic> mutant flies. (<bold>G</bold>) Comparison of median event amplitude for WT and GluRIIA NMJs. Both groups are plotted on the left axes; Cohen’s d is plotted on a floating axis on the right as a bootstrap sampling distribution (red). The mean difference is depicted as a dot (black); the 95% confidence interval is indicated by the ends of the vertical error bar. (<bold>H</bold>) Event frequency is lower in <italic>GluRIIA</italic> mutant NMJs than in WT. (<bold>I</bold>) Knockout of <italic>GluRIIA</italic> speeds event decay time. (<bold>J</bold>) Faster event rise times in <italic>GluRIIA</italic> mutant NMJs.</p></caption>
<graphic xlink:href="565316v3_fig7.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
</sec>
<sec id="s2h">
<title>miniML enables reliable analysis of optical miniature events from live imaging experiments</title>
<p>To assess miniML’s generalizability to non-electrophysiological data, we applied it to analyze events in time-series data from live fluorescence imaging experiments. Recent developments of highly sensitive fluorescent probes have enabled the recording of synaptic events using various imaging techniques (<xref ref-type="bibr" rid="c2">Abdelfattah et al., 2023</xref>; <xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>; <xref ref-type="bibr" rid="c23">Hao et al., 2024</xref>; <xref ref-type="bibr" rid="c55">Ralowicz et al., 2024</xref>). These technological advances offer exciting new possibilities in the study of synaptic function, but also generate novel challenges for the detection and analysis of synaptic events. Live imaging datasets typically feature a lower sampling rate, lower SNR, and distinct noise profile compared with electrophysiological recordings. Nevertheless, the waveforms of imaged synaptic release events, voltage changes, or Ca2+ transients closely resemble those used to train miniML. Thus, we hypothesized that miniML could also be employed for event detection in fluorescence imaging data.</p>
<p>We first used miniML to analyze a previously published dataset (<xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>) from rat neuronal cultures expressing the glutamate sensor iGluSnFR3 (<xref rid="fig8" ref-type="fig">Figure 8A–B</xref>). These data, recorded in the presence of TTX, contain spontaneous transient increases in fluorescence intensity representing individual release events (’optical minis’). Initially, we selected a small subset of events from the data to train a TL model. Given the low sampling rate of the imaging (100 Hz), we upsampled the data by a factor of 10 to match the model’s input shape. The TL model was subsequently applied to all detected sites (n = 1524) within the widefield recording (<xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>). A qualitative assessment of the imaging traces showed excellent event detection, with miniML consistently localizing the iGluSnFR3 fluorescence transients at varying SNRs (<xref rid="fig8" ref-type="fig">Figure 8C</xref>). The detected optical minis had similar kinetics to those reported in (<xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>) (10–90% rise time, median 21.8 ms; half decay time, median 48.7 ms, <xref rid="fig8" ref-type="fig">Figure 8D</xref>). In addition, analysis of event frequencies across sites revealed a power-law distribution (<xref rid="fig8" ref-type="fig">Figure 8E</xref>), consistent with fractal behavior of glutamate release (<xref ref-type="bibr" rid="c43">Lowen et al., 1997</xref>). Thus, miniML can reliably detect synaptic release events in time-series data from iGluSnFR3 recordings.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Optical detection of spontaneous glutamate release events in cultured neurons using iGluSnFR3 and miniML.</title>
<p>(<bold>A</bold>) miniML was applied to recordings from rat primary culture neurons expressing iGluSnFR3. Data from (<xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>). (<bold>B</bold>) Example epifluorescence image of iGluSnFR3-expressing cultured neurons. Image shows a single frame with three example regions of interest (ROIs) indicated. (<bold>C</bold>) Δ<italic>F/F</italic><sub>0</sub> traces for the regions shown in (<bold>B</bold>). Orange circles indicate detected optical minis. Note the different signal-to-noise ratios of the examples. (<bold>D</bold>) Top: Heatmap showing average optical minis for all sites with detected events, sorted by amplitude. Bottom: Grand average optical mini. (<bold>E</bold>) Top: Histogram with kernel density estimate (solid line) of event amplitudes for n = 1524 ROIs of the example in (<bold>B</bold>). Note the log-scale of the abscissa. Bottom: Event frequency histogram.</p></caption>
<graphic xlink:href="565316v3_fig8.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>To further investigate miniML’s performance in imaging data, we performed simultaneous electrophysiological and fluorescence recordings of mEPSPs in cultured rat hippocampal neurons expressing ASAP5-Kv (<xref ref-type="bibr" rid="c23">Hao et al., 2024</xref>) at physiological temperature (<xref rid="fig9" ref-type="fig">Figure 9A–B</xref>). ASAP5 allows resolving small voltage changes in the mV-range, as illustrated by the close correlation of optical and current-clamp signals (<xref rid="fig9" ref-type="fig">Figure 9C</xref>). However, event detection is more challenging in voltage imaging data due to the lower SNR compared to electrophysiological recordings (ASAP5: 2.26, SD 0.46; electrophysiology: 4.79, SD 0.74; n = 5 neurons; <xref rid="fig9s1" ref-type="fig">Figure 9—figure supplement 1</xref>).</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9:</label>
<caption><title>Optical detection of mEPSPs in cultured rat hippocampal neurons using ASAP5 and miniML.</title>
<p>(<bold>A</bold>) Event detection in simultaneous recordings of membrane voltage using electrophysiology and ASAP5 voltage imaging. (<bold>B</bold>) Example epifluorescence image of an ASAP5-Kv-expressing cultured rat hippocampal neuron. (<bold>C</bold>) Δ<italic>F/F</italic><sub>0</sub> trace (Top) and simultaneous current-clamp recording (Bottom) from the neuron shown in (<bold>B</bold>). Events detected by miniML are indicated. (<bold>D</bold>) Detected events from the example recording in (<bold>B–C</bold>) overlaid with average (colored line). Top: ASAP5-Kv recording. Bottom: Current-clamp recording. (<bold>E</bold>) Top: Peak-scaled average mEPSP (dark blue) and optical events (light blue). Bottom: Amplitudes measured in optical and electrophysiological data are highly correlated (Pearson correlation coefficient r = 0.9). (<bold>F</bold>) Comparative analysis of detection performance in ASAP5 data. (<bold>G</bold>) Example ASAP5 recording with detected event positions indicated for different analysis methods (miniML, template matching, deconvolution). (<bold>H</bold>) Precision and Recall of all three detection methods for n = 5 neurons. miniML showed highest recall (miniML, 0.38±0.05; template matching, 0.25±0.05 (Cohen’s d, 1.25); deconvolution, 0.09±0.01 (Cohen’s d, 3.87); mean ± SEM) with similar precision (0.93±0.01; 0.95±0.02; 0.8±0.1). Bars are median values.</p>
</caption>
<graphic xlink:href="565316v3_fig9.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig9s1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9-figure supplement 1</label>
<caption><title>mEPSP detection in ASAP5 recordings.</title>
<p>(<bold>A</bold>) Signal-to-noise ratio (SNR) of mEPSPs in electrophysiology and ASAP5 for five neurons. (<bold>B</bold>) Overlay of simultaneous current-clamp and ASAP5 recording. Events detected by miniML in both types of recordings are indicated. (<bold>C</bold>) Examples of mEPSPs detected in the electrophysiology data that were either detected by miniML in ASAP5 data (orange dots) or missed (blacked dots). (<bold>D</bold>) mEPSP amplitude versus optical amplitude for detected events in five neurons (color-coded). Line is a linear fit to the data (slope = 0.95 mV/−%Δ<italic>F/F</italic><sub>0</sub>, Pearson correlation coefficient = 0.83). (<bold>E</bold>) Event half decay time (Left) and rise time (Right) for five neurons from electrophysiology (’Ephys’) and ASAP5 data. Data were calculated from the event averages of each cell. The lower sampling rate of imaging acquisition (400 Hz) vs. electrophysiology (10,000 Hz) likely contributes to the slower event kinetics observed in ASAP5 data.</p></caption>
<graphic xlink:href="565316v3_fig9s1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<fig id="fig9s2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9-figure supplement 2</label>
<caption><title>Methods comparison for event detection in ASAP5 recordings.</title>
<p>(<bold>A</bold>) Average waveforms of detected events in ASAP5 data for miniML (n = 101 events), template-matching (n = 82 events), and deconvolution (n = 15 events). Data are from the example shown in <xref rid="fig9" ref-type="fig">Figure 9C</xref>, shaded areas represent SEM. (<bold>B</bold>) Recall of events in ASAP5 data versus mEPSP amplitude for miniML, template-matching, and deconvolution. Lines are averages of five recordings with SEM as shaded area. (<bold>C</bold>) F1 score for event detection in ASAP5 data was higher using miniML (0.53±0.04, mean ± SEM) than for template-matching (0.39±0.05; Cohen’s d, 1.35) and deconvolution (0.17±0.01; Cohen’s d, 5.1). Bars are median values, n = 5 neurons.</p></caption>
<graphic xlink:href="565316v3_fig9s2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Photon shot noise and other sources of noise in the imaging setup not only limit the SNR, but can also lead to spurious event detection (Sjulson and Miesenbö ck, 2007; <xref ref-type="bibr" rid="c72">Wilt et al., 2013</xref>).</p>
<p>We trained separate miniML TL models to detect mEPSPs in optical and current-clamp data (<xref rid="fig9" ref-type="fig">Figure 9C–D</xref>). Optical detection yielded events that closely resembled mEPSPs, with slightly slower decay and rise kinetics (<xref rid="fig9" ref-type="fig">Figure 9E</xref>, <xref rid="fig9s1" ref-type="fig">Figure 9—figure supplement 1</xref>). Analysis of the corresponding amplitudes of mEPSPs and optically detected events confirmed that ASAP5 linearly reports subthreshold voltage changes in neurons (<xref rid="fig9" ref-type="fig">Figure 9E</xref>), in line with previous reports (<xref ref-type="bibr" rid="c23">Hao et al., 2024</xref>). These results demonstrate that miniML can detect optical mEPSPs recorded using ASAP5 under challenging low SNR conditions. To provide a more quantitative assessment of detection performance, we compared miniML to established template-based analysis methods (<xref rid="fig9" ref-type="fig">Figure 9F</xref>). The simultaneous recording of membrane voltage through both optical and electrophysiological means allowed us to establish ground truth data for evaluating detection performance. We quantified the detection of events by different methods in imaging data relative to electrophysiology, with threshold settings of the matched-filtering approaches adjusted to achieve ∼90% precision. High precision of event detection is crucial for robust data analysis. At comparable levels of precision, miniML demonstrated superior recall of mEPSPs compared with template matching or deconvolution (<xref rid="fig9" ref-type="fig">Figure 9G–H</xref>). Notably, miniML detected ∼40% of mEPSPs in optical data, a substantial improvement over template-based methods (<xref rid="fig9" ref-type="fig">Figure 9H</xref>, <xref rid="fig9s2" ref-type="fig">Figure 9—figure supplement 2</xref>). These voltage imaging experiments suggest that miniML facilitates the optical detection of mEPSPs. More extensive training datasets may further increase the recall performance of miniML in imaging datasets with low SNR.</p>
<p>Together, the analysis of different optical recordings containing synaptic events demonstrates that miniML generalizes beyond electrophysiological data and can support the evaluation of fluorescence imaging experiments. miniML may thus pave the way for high-throughput analyses of spontaneous events in diverse imaging datasets.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Here, we developed and evaluated miniML, a novel supervised deep learning approach for detecting spontaneous synaptic events. miniML provides a comprehensive and versatile framework for analyzing synaptic events in diverse time-series data with several important advantages over current methods.</p>
<p>miniML outperforms existing methods for synaptic event detection, particularly with respect to false positives, which is crucial for the accurate quantification of synaptic transmission. In addition to its high precision, miniML’s detection performance is robust to threshold choice. This effectively eliminates the trade-off between false positive and false negative rates typically present in event detection methods (<xref ref-type="bibr" rid="c15">Clements and Bekkers, 1997</xref>; <xref ref-type="bibr" rid="c47">Merel et al., 2016</xref>; <xref ref-type="bibr" rid="c49">Mori et al., 2024</xref>; Perńıa-Andrade et al., 2012). Thus, miniML is highly reproducible and overcomes the need for laborious manual event inspection, enabling automated synaptic event analysis. Automated, reproducible data analysis is key to open science and the use of publicly available datasets (<xref ref-type="bibr" rid="c8">Ascoli et al., 2017</xref>; <xref ref-type="bibr" rid="c22">Ferguson et al., 2014</xref>). The Python-based software implementation of miniML enables a high degree of automatization, making it well suited for analyzing large-scale neurophysiological datasets. In addition, we provide a graphical user interface akin to MiniAnalysis or SimplyFire software.</p>
<p>Our results from a wide variety of preparations, covering different species and neuronal preparations, demonstrate the robustness of miniML and its general applicability to the study of synaptic physiology. miniML generalizes well to different experimental preparations, conditions, and data types. Data with event waveforms that are similar to the original miniML training data (i.e., mouse cerebellar GC recordings) can be analyzed immediately. For more distinct event waveforms or data properties, data resampling and/or retraining of a classifier via TL enables event detection optimized for the respective recording conditions. Importantly, TL requires only a few hundred labeled events for training, facilitating its application to novel datasets. For example, an existing miniML model can be easily retrained to eliminate potential false positives, e.g., due to sudden changes in noise characteristics. This scalability is a key advantage of the miniML approach over traditional methods.</p>
<p>Event detection with miniML is not restricted to electrophysiological data, but can also be applied to optical time-series data derived from live imaging experiments using, e.g., reporters of glutamate (<xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>), Ca2+ (<xref ref-type="bibr" rid="c67">Tran and Stricker, 2021</xref>), or membrane voltage (<xref ref-type="bibr" rid="c42">Li et al., 2020</xref>). Optical recordings of subthreshold synaptic events pose a particular challenge due to the low sampling rate and low SNR. The development of novel optical sensors with improved sensitivity and temporal resolution (<xref ref-type="bibr" rid="c20">Evans et al., 2023</xref>; <xref ref-type="bibr" rid="c23">Hao et al., 2024</xref>; Zhang, Rózsa, et al., 2023) enables optical recordings of subthreshold synaptic events, but also necessitates efficient and robust event detection methods. Our results demonstrate that miniML performs better than previous methods in identifying small synaptic events in fluorescence imaging data. This advance may transform our ability to extract meaningful information from challenging optical recordings and may open up new possibilities for studying synaptic activity across large populations of neurons simultaneously. Similarly, miniML can be used for the analysis of evoked postsynaptic responses, such as failure analysis or quantification of unitary events, and for functional connectivity studies (<xref ref-type="bibr" rid="c13">Campagnola et al., 2022</xref>). In addition, the method could be extended to other areas of biology and medicine where signal detection is critical, such as clinical neurophysiology or imaging.</p>
<p>Comprehensive performance comparison is essential for evaluation and selection of analysis methods. Benchmarking requires standardized ground truth data, but unlike, e.g., spike inference from Ca2+ imaging (<xref ref-type="bibr" rid="c65">Theis et al., 2016</xref>), these are usually not available for spontaneous synaptic event recordings. Furthermore, the results can vary between simulated and real data (<xref ref-type="bibr" rid="c65">Theis et al., 2016</xref>). We therefore established a benchmarking pipeline using event-free recordings with synthetic events, circumventing differences in the noise power spectrum of simulations vs. recordings (<xref ref-type="bibr" rid="c47">Merel et al., 2016</xref>; Perńıa-Andrade et al., 2012). This approach may provide a general toolbox for evaluating the effectiveness of different synaptic event detection methods. Our comparison of detection performance in real-world data further underscores that miniML’s detection performance surpasses existing methods.</p>
<p>The supervised learning approach of miniML requires labeled training data. Although TL requires only a few hundred training samples, these data need to be collected and annotated by the user. Future directions may explore unsupervised learning techniques such as autoencoders to reduce the dependence on annotated training data. In addition, at very high event frequencies, individual synaptic events may overlap, making their separation difficult. Although miniML can generally detect overlapping events, very close events may not always be detected and there may be a lower bound. Implementing additional techniques such as wavelet transformation, or spectral domain or shapelet analysis (<xref ref-type="bibr" rid="c12">Batal and Hauskrecht, 2009</xref>; <xref ref-type="bibr" rid="c73">Ye and Keogh, 2009</xref>) may improve the accuracy of event detection, especially for overlapping events.</p>
<p>miniML presents an innovative data analysis method that will advance the field of synaptic physiology. Its open-source Python software design ensures seamless integration into existing data analysis pipelines and enables widespread use of the method, fostering the development of new applications and further innovation. Remarkably, despite its deep learning approach, miniML runs at relatively rapid speed on commodity hardware. Due to its robust, generalizable, and unbiased detection performance, miniML allows researchers to perform more accurate and efficient synaptic event analysis. A standardized, more efficient, and reproducible analysis of synaptic events will promote important new insights into synaptic physiology and dysfunction (<xref ref-type="bibr" rid="c40">Lepeta et al., 2016</xref>; <xref ref-type="bibr" rid="c78">Zoghbi and Bear, 2012</xref>) and help improve our understanding of neural function.</p>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Electrophysiological recordings</title>
<p>Animals were treated according to national and institutional guidelines. All experiments were approved by the Cantonal Veterinary Office of Zurich (approval number no. ZH206/2016 and ZH009/2020). Experiments were performed in male and female C57BL/6J mice (Janvier Labs, France). Mice were 1–5-months-old, but for recordings from the Calyx of Held, which were performed in P9 animals. Animals were housed in groups of 3–5 in standard cages under a 12h-light/12h-dark cycle with food and water ad libitum. Mice were sacrificed by rapid decapitation after isoflurane anesthesia. The cerebellar vermis was quickly removed and mounted in a chamber filled with chilled extracellular solution. Parasagittal 300-µm thick slices were cut with a Leica VT1200S vibratome (Leica Microsystems, Germany), transferred to an incubation chamber at 35 °C for 30 minutes, and then stored at room temperature until experiments. The extracellular solution (artificial cerebrospinal fluid, ACSF) for slicing and storage contained (in mM): 125 NaCl, 25 NaHCO<sub>3</sub>, 20 D-glucose, 2.5 KCl, 2 CaCl<sub>2</sub>, 1.25 NaH<sub>2</sub>PO<sub>4</sub>, 1 MgCl<sub>2</sub>, aerated with 95% O<sub>2</sub> and 5% CO<sub>2</sub>.</p>
<p>Slices were visualized using an upright microscope with a 60×, 1 NA water immersion objective, infrared optics, and differential interference contrast (Scientifica, UK). The recording chamber was continuously perfused with ACSF. For event-free recordings, we blocked excitatory and inhibitory transmission using ACSF supplemented with 50 µM D-APV, 10 µM NBQX, 10 µM bicuculline, and 1 µM strychnine. Patch pipettes (open-tip resistances of 3–8 MΩ) were filled with solution containing (in mM): 150 K-D-gluconate, 10 NaCl, 10 HEPES, 3 MgATP, 0.3 NaGTP, 0.05 ethyleneglycol-bis(2-aminoethylether)-N,N,N’,N’-tetraacetic acid (EGTA), pH adjusted to 7.3 with KOH. Voltage-clamp and current-clamp recordings were made using a HEKA EPC10 amplifier controlled by Patchmaster software (HEKA Elektronik GmbH, Germany). Voltages were corrected for a liquid junction potential of +13 mV. Experiments were performed at room temperature (21–25 °C). Miniature EPSCs (mEPSCs) were recorded at a holding potential of −100 mV or −80 mV, and miniature EPSPs (mEPSPs) at the resting membrane potential. Data were filtered at 2.9 kHz and digitized at 50 kHz. Synaptic event recording periods typically lasted 120 s.</p>
<p>Voltage-clamp recordings were performed on human iPSC-derived neurons as described in (<xref ref-type="bibr" rid="c7">Asadollahi et al., 2023</xref>). We recorded spontaneous EPSCs in 8-week-old neuronal cultures with cortical glutamatergic identity at a holding potential of −80 mV and with ACSF consisting of (in mM): 135 NaCl, 10 HEPES, 10 D-glucose, 5 KCl, 2 CaCl<sub>2</sub>, 1 MgCl<sub>2</sub>. Synaptic events were observed in ∼ 51% of neurons. The experimental procedures for recordings of spontaneous or miniature EPSCs in zebrafish and <italic>Drosophila</italic> are described in (<xref ref-type="bibr" rid="c59">Rupprecht and Friedrich, 2018</xref>) and (<xref ref-type="bibr" rid="c9">Baccino-Calace et al., 2022</xref>), respectively.</p>
</sec>
<sec id="s4b">
<title>Combined electrophysiological and optical mEPSP recordings</title>
<p>Cultured rat hippocampal neurons at 17–20 days in vitro were patched and imaged simultaneously at physiologi- cal temperature (30–34 °C) following the procedure described in (<xref ref-type="bibr" rid="c23">Hao et al., 2024</xref>). Briefly, for electrophysiological recordings, extracellular solution containing 145 mM NaCl, 3 mM KCl, 2 mM CaCl<sub>2</sub>, 2 mM MgCl<sub>2</sub>, 10 mM HEPES, and 10 mM glucose (pH adjusted to 7.4, Osmolarity 310 mOsm/kg) supplemented with TTX (1 µM) and PTX (50 µM), and a pipette solution containing 123 mM K-gluconate, 10 mM KCl, 8 mM NaCl, 1 mM MgCl<sub>2</sub>, 10 mM HEPES, 1 mM EGTA, 0.1 mM CaCl<sub>2</sub>, 1.5 mM MgATP, 0.2 mM Na<sub>4</sub>GTP, and 4 mM glucose (pH adjusted to 7.2, Osmolarity 295–300 mOsm/kg) were used. Whole-cell recordings were performed with a Multiclamp 700B amplifier, Digidata 1440A digitizer, and pClamp software (all from Molecular Devices). Patch pipettes (open-tip resistances of 3–5 MΩ) were produced from glass capillaries with filament (outer diameter, 1.5 mm; inner diameter, 1.0 mm; King Precision Glass). Electrophysiological recordings were digitized at 10 kHz for acquisition and then lowpass filtered with 1 kHz cut-off frequency in post-processing in Clampfit.</p>
<p>ASAP5-Kv was excited using a SOLIS-470C LED light source (Thorlabs) with a 482/18-nm bandpass filter (Semrock); excitation light was set to provide an irradiance of 46 mW/mm2. Imaging data were acquired at 400 fps using an iXon 860 EMCCD camera (Andor – Oxford Instruments) and a 525/50-nm bandpass filter (Semrock). Neurons requiring more than ±100 pA holding current to maintain the resting membrane potential near −70 mV were excluded from the analysis.</p>
</sec>
<sec id="s4c">
<title>Training data and annotation</title>
<p>We used synaptic event recordings from a previous publication (Delvendahl et al., 2019) to generate the training dataset. mEPSCs were extracted based on low-threshold template-matching. Corresponding sections of data without events were randomly selected from the recordings. The extracted windows had a length of 600 data points. Given our sampling rate of 50 kHz, this corresponds to 12 ms. We subsequently manually scored data sections as event-containing (label=1) or not event-containing (label=0). The ratio of events to non-events was kept close to one to ensure efficient training. Based on empirical observations of model performance, we included relatively small amplitude events that are often missed by other methods. Similarly, including negative examples that are commonly picked up as false positives, resulted in more accurate prediction traces. We used data augmentation techniques to further improve model discrimination. We simulated waveforms of non-synaptic origin, which are occasionally encountered during recordings, and superimposed them onto noise recordings. Examples included rapid current transients that can be caused by peristaltic perfusion pump systems often used in brain slices recordings, or slow currents with a symmetric rise and decay time course. A total of 4500 segments were created and labeled as 0. To maintain the ratio of negative to positive examples, we added an equivalent number of synthetic synaptic events. The biexponential waveform described in the section <italic>Benchmarking detection methods</italic> was used for event simulation. The final training dataset contained 30,140 samples (21,140 from recorded traces and 9,000 simulated samples).</p>
</sec>
<sec id="s4d">
<title>Deep learning model architecture</title>
<p>We built a discriminative end-to-end deep learning model for one-dimensional time-series classification (<xref ref-type="bibr" rid="c21">Fawaz et al., 2019</xref>). The neural network architecture comprised multiple convolutional layers, an LSTM layer, and a fully connected layer. The approach is related to networks designed for event detection in audio (<xref ref-type="bibr" rid="c51">Passricha and Aggarwal, 2019</xref>) and image data (<xref ref-type="bibr" rid="c19">Donahue et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Islam et al., 2020</xref>), or classification of genomic data (Tasdelen and Sen, 2021). The combination of convolutional and recurrent neural network layers helps to improve the classification performance for time-series data. In particular, LSTM layers allow learning temporal features. An initial comparison of different types of network architectures showed superior performance of a CNN-LSTM model over CNN-Dense, Residual Neural Network (ResNet), and multi-layer perceptron (MLP) architectures (<xref rid="fig1s2" ref-type="fig">Figure 1—figure supplement 2</xref>).</p>
<p>The miniML model was built using TensorFlow, an open-source machine learning library for Python (<xref ref-type="bibr" rid="c45">Martín Abadi et al., 2015</xref>). To optimize the model architecture, we performed a Bayesian optimization of hyperparameters. Hyperparameter ranges were chosen for the free parameters of all layers. Optimization was then performed with a maximum number of trials of 50. Models were evaluated using the validation dataset. Because a higher number of free parameters tended to increase inference times, we then empirically tuned the chosen hyperparameter combination to achieve a trade-off between number of free parameters and accuracy.</p>
<p>The deep learning network takes batches of one-dimensional (1D) univariate time-series data as input, which are converted into a tensor of shape (batch size, data length, 1). The data is passed to three convolutional blocks. Each block consists of a 1D convolutional layer with a leaky Rectified Linear Unit (leaky ReLU) activation function followed by an average pooling layer. To avoid overfitting, each convolutional block uses batch normalization (<xref ref-type="bibr" rid="c28">Ioffe and Szegedy, 2015</xref>). Batch normalization reduced training time by about two times and improved the accuracy of the resulting model. We added a fourth convolutional block that includes a convolutional layer, batch normalization and a leaky ReLU activation function, but no average pooling layer. The output of the final convolutional layer passes through a bidirectional recurrent layer of LSTM units. The final layers consist of a fully connected dense unit layer and a dropout layer (Srivastava et al., 2014), followed by a single unit with sigmoid activation. The output of the neural network is a scalar between [0, 1]. The layers and parameters used, including output shape and number of trainable parameters, are summarized in Supplementary Table 1. The total number of trainable parameters was 190,913.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Overview of model architecture</title></caption>
<graphic xlink:href="565316v3_tbl1.tif" mime-subtype="tiff" mimetype="image"/>
</table-wrap>
</sec>
<sec id="s4e">
<title>Training and evaluation</title>
<p>The network was trained with Tensorflow 2.12 and Python 3.10 with CUDA 11.4. Datasets were scaled between zero and one, and split into training and validation data (0.75/0.25). The model was compiled using the Adam optimizer (<xref ref-type="bibr" rid="c36">Kingma and Ba, 2014</xref>) with AMSGrad (<xref ref-type="bibr" rid="c56">Reddi et al., 2019</xref>). We trained the classifier using a learning rate <italic>η</italic> = 2E–5 and batch size of 128 on the training data. Training was run for maximum 100 epochs with early stopping to avoid overfitting. Validation data was used to evaluate training performance. Early stopping was applied when the validation loss did not improve for eight consecutive epochs. Typically, training lasted for 20–40 epochs. We used binary cross-entropy loss and binary accuracy as measures of performance during training. The best performing model was selected from a training run, and a receiver-operating characteristic (ROC) was calculated. We used accuracy and area under the ROC curve to evaluate training performance. To accelerate training, we used a GPU; training time for the neural network was ∼8 minutes on a workstation with NVIDIA Tesla P100 16 GB GPU (Intel Xeon 2.2 GHz CPU, 16 GB RAM).</p>
</sec>
<sec id="s4f">
<title>Model and training visualization</title>
<p>To analyze the discriminative data segments, we calculated per-sample saliency maps with SmoothGrad using the Python package <italic>tf-keras-viz</italic> (<xref ref-type="bibr" rid="c61">Simonyan et al., 2013</xref>). Saliency maps were smoothed with a 10-point running average filter for display purposes. To illustrate the transformation by the model, we performed dimensionality reduction of the training data by Uniform Manifold Approximation and Projection (UMAP), using either the raw dataset or the input to the final layer of the deep learning model.</p>
</sec>
<sec id="s4g">
<title>Applying the classifier for event detection</title>
<p>The trained miniML classifier takes sections of data with predefined length as input. To detect events in arbitrarily long recordings, a sliding window procedure is used. Time-series data from voltage-clamp or current-clamp recordings is segmented using a sliding window with stride. The resulting 2D-tensor is min-max scaled and then used as model input for inference. To overcome the potential limitation of long computation times, we used a sliding window with stride procedure. Using a stride <italic>&gt;</italic> 1 significantly reduces the inference time of the model (<xref rid="fig2s1" ref-type="fig">Figure 2—figure supplement 1</xref>), especially for data with high sampling rates or long recording times.</p>
<p>This approach results in a prediction trace with data being spaced at <italic>samplinginterval</italic> ∗ <italic>stride</italic>. With <italic>stride</italic> = 20, a 120-s recording at 50 kHz sampling rate can be analyzed in ∼15 s on a laptop computer (Apple M1Pro, 16 GB</p>
<p>RAM, <xref rid="fig2s1" ref-type="fig">Figure 2—figure supplement 1</xref>). To maintain temporal precision, the prediction trace is resampled to the sampling frequency of the raw data.</p>
<p>Events in the input data trace result in distinct peaks in the prediction trace. We applied a maximum filter to enhance post-processing of the data. Thus, by applying a threshold to the prediction trace, synaptic event positions can be detected. Our analyses indicate that the absolute threshold value is not important in the [0.05, 0.95] range (<xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>). Data following a threshold crossing in the prediction trace are cut from the raw data and aligned by steepest rise. To find the point of steepest rise, a peak-search is performed in the first derivative of the short data segment. If multiple peaks are detected, any peak that has a prominence ⩾ 0.25 relative to the largest detected prominence is treated as an additional event that is in close proximity or overlapping. The resulting 2D-array with aligned events can be further analyzed to obtain descriptive statistics on, e.g., amplitude, charge, or rise and decay kinetics of individual events.</p>
</sec>
<sec id="s4h">
<title>Benchmarking detection methods</title>
<p>We compared the deep learning-based miniML with the following previously described detection methods: template-matching (<xref ref-type="bibr" rid="c15">Clements and Bekkers, 1997</xref>), deconvolution (Perńıa-Andrade et al., 2012), a finite threshold- based approach (<xref ref-type="bibr" rid="c38">Kudoh and Taguchi, 2002</xref>), the commercial MiniAnalysis software (version 6.0.7, Synaptosoft), SimplyFire (<xref ref-type="bibr" rid="c49">Mori et al., 2024</xref>), and a Bayesian inference procedure (<xref ref-type="bibr" rid="c47">Merel et al., 2016</xref>). Detection methods were implemented in Python 3.9, except for the Bayesian method, which was run using Matlab R2022a, and MiniAnalysis running as stand-alone software on Windows.</p>
<p>We used generated standardized data to benchmark the detection performance of different methods. Event- free recordings (see section <italic>Electrophysiological recordings</italic> for details) were superimposed with simulated events having a biexponential waveform:
<disp-formula>
<graphic xlink:href="565316v3_ueqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
Where <italic>I</italic>(<italic>t</italic>) is the current as a function of time, and <italic>τ<sub>rise</sub></italic>and <italic>τ<sub>decay</sub></italic>are the rise and decay time constants, respectively. Simulated event amplitudes were drawn from a log-normal distribution with variance 0.4. Mean amplitude was varied to generate diverse signal-to-noise ratios (SNR; mean event amplitude / standard deviation of the noise). Decay time constants were drawn from a normal distribution with mean 1.0 ms and variance 0.25 ms (Delvendahl et al., 2019). Generated events were randomly placed in the event-free recording with an average frequency of 0.7 Hz and a minimum spacing of 3 ms. Generated traces provided ground-truth data for the evaluation of the different methods.</p>
<p>To quantify detection performance of different methods over a range of signal-to-noise ratios, we calculated the number of true positives (TP), false positives (FP) and false negatives (FN). From these, the following metrics were calculated:
<disp-formula>
<graphic xlink:href="565316v3_ueqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula>
<graphic xlink:href="565316v3_ueqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula>
<graphic xlink:href="565316v3_ueqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
For all metrics, higher values indicate a better model performance. We also evaluated detection performance under non-optimal conditions (i.e., events that did not precisely match the event template or the training data). To do this, we varied the kinetics of simulated events by either increasing (mean = 4.5 ms) or decreasing (mean = 0.5 ms) the average decay time constant (<xref rid="fig3s1" ref-type="fig">Figure 3—figure supplement 1</xref>).</p>
</sec>
<sec id="s4i">
<title>Hyperparameter settings of detection methods</title>
<p>For all benchmarking conditions, threshold settings were: −4 for template-matching, 5 ∗ <italic>SD</italic> of the detection trace for deconvolution, and −4 pA for the finite-threshold method. Both template-based methods used a template waveform corresponding to the average of the simulated events. For the Bayesian detection approach, we used the code provided by the authors (<xref ref-type="bibr" rid="c47">Merel et al., 2016</xref>) with the following adjustments to the default hyperparameters: minimum amplitude = 3.5, noise <italic>φ</italic> = [0.90; −0.52], rate = 0.5. We chose a cutoff of 6 ∗ <italic>SD</italic> of the event time posteriors as threshold. For SimplyFire, we used the following hyperparameters: direction = −1; kernel = 300; stride = 100; minimum amplitude = 4 pA. MiniAnalysis (Synaptosoft) was run in the automatic detection mode with default settings for EPSCs and amplitude and area thresholds of −4 pA and 4, respectively. We used a minimum peak height of 0.5 and minimum peak width of 10 strides for miniML. To compare different event detection methods in real-world data (Figure 4, we used the following detection hyperparameters: We applied miniML with a window size of 600 and minimum peak width of 5 strides. For the Golgi cell data, window size was set to 900 to account for the slower event kinetics. Detection thresholds for template matching and deconvolution thresholds were set at −3.5 and 6, respectively. We adjusted the template for each preparation based on the average waveform extracted with a generic template. MiniAnalysis was run with area and amplitude thresholds of 5; amplitude threshold was increased to −20 pA and −8 pA for Calyx of Held and Golgi cell data, respectively.</p>
</sec>
<sec id="s4j">
<title>Transfer learning</title>
<p>To make the miniML method applicable to data with different characteristics, such as different event kinetics, noise characteristics, or recording mode, we used a transfer learning (TL) approach (<xref ref-type="bibr" rid="c54">Pratt et al., 1991</xref>). We froze the convolutional layers of the fully trained MF–GC miniML model, resulting in a new model with only the LSTM and dense layers being trainable. Thus, the convolutional layers act as pre-trained feature detectors and much fewer training samples are required. Hyperparameters and training conditions were the same as for full-training (see <italic>Training and evaluation</italic>) with the following exceptions: learning rate <italic>η</italic> = 2E–8, patience = 15, batch size = 32, dropout rate = 0.5. The training data were resampled to 600 data points to match the input shape of the original model.</p>
<p>To compare TL with full training, random subsets of the training data (mEPSPs in cerebellar GCs, spontaneous EPSCs in zebrafish, or mEPSCs at the <italic>Drosophila</italic> neuromuscular junction) with increasing size were generated and used to train models using fivefold cross-validation. We always used the same size of the validation dataset. After comparing TL with full training (<xref rid="fig5" ref-type="fig">Figure 5</xref>, <xref rid="fig5s2" ref-type="fig">Figure 5—figure supplement 2</xref>), we trained separate models to analyze the different datasets using subsets of the available training data.</p>
</sec>
<sec id="s4k">
<title>Quantifications</title>
<p>Computing times were quantified using the performance counter function of the built-in Python package <italic>time</italic> and are given as wall times. Statistical comparisons were made using permutation t-tests with 5000 reshuffles (<xref ref-type="bibr" rid="c24">Ho et al., 2019</xref>). Effect sizes are reported as Cohen’s d or mean difference, with 95% confidence intervals obtained by bootstrapping (5000 samples; the confidence interval is bias-corrected and accelerated) (<xref ref-type="bibr" rid="c24">Ho et al., 2019</xref>).</p>
<p>Raw data were filtered with a Hann window for event analysis after the detection. The number of points of the filter window was adjusted based on the sampling rate (default, 20 samples). Event amplitudes were quantified as difference between detected event peaks and a short baseline window before event onset. Decay times refer to half decay times (time until signal reaches 50% of the amplitude), and rise times were quantified as 10–90% rise times (time between 10% and 90% of the amplitude). These event statistics were calculated for each individual event and averaged per cell.</p>
</sec>
<sec id="s4l" sec-type="data-availability">
<title>Data and code availability</title>
<p>Datasets used for model training are available from Zenodo [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14507343">https://doi.org/10.5281/zenodo.14507343</ext-link>]. miniML source code and pre-trained models are available online [<ext-link ext-link-type="uri" xlink:href="https://github.com/delvendahl/miniML">https://github.com/delvendahl/miniML</ext-link>], including analysis code. All code was implemented in Python with the following libraries: TensorFlow, SciPy, NumPy, Matplotlib, Pandas, h5py, scikit-learn, pyABF, dabest.</p>
<p>We used datasets from previous publications to generate training sets and to assess the application of miniML in zebrafish and <italic>Drosophila</italic> (<xref ref-type="bibr" rid="c9">Baccino-Calace et al., 2022</xref>; Delvendahl et al., 2019; <xref ref-type="bibr" rid="c59">Rupprecht and Friedrich, 2018</xref>). In addition, a published dataset (<xref ref-type="bibr" rid="c3">Aggarwal et al., 2023</xref>) was used to probe the application of miniML to imaging data [<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25378/janelia.21985406">https://doi.org/10.25378/janelia.21985406</ext-link>].</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Mark D. Robinson and Anu G. Nair for helpful discussions.</p>
<p>This work received funding by the Swiss National Science Foundation (grant PZ00P3 174018 to I.D., grant PZ00P3 209114 to P.R., grant 310030B 152833/1 to R.F.), the German Research Foundation (grants DE 3925/1-1 and 3925/2-1 to I.D.), the NIH (grant 1R01NS116589 to M.L.), the Novartis Research Foundation (to R.F.), the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement No 742576 to R.F.), a fellowship from the Boehringer Ingelheim Fonds (to P.R.), and the UZH Alumni Research Talent Development fund (to I.D.). The funding bodies had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abbott</surname>, <given-names>L. F.</given-names></string-name>, &amp; <string-name><surname>Regehr</surname>, <given-names>W. G</given-names></string-name></person-group>. (<year>2004</year>). <article-title>Synaptic computation</article-title>. <source>Nature</source>, <volume>431</volume>(<issue>7010</issue>), <fpage>796</fpage>–<lpage>803</lpage>. <pub-id pub-id-type="doi">10.1038/nature03010</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abdelfattah</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Y.-C.</given-names></string-name>, <string-name><surname>Reep</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tsegaye</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Tsang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Arthur</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Rehorova</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Olson</surname>, <given-names>C. V.</given-names></string-name>, <string-name><surname>Shuai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Fu</surname>, <given-names>T.-M.</given-names></string-name>, <string-name><surname>Milkie</surname>, <given-names>D. E.</given-names></string-name>, <string-name><surname>Moya</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Weber</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Lemire</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Falco</surname>, <given-names>N.</given-names></string-name>, <etal>…</etal> <string-name><surname>Kolb</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Sensitivity optimization of a rhodopsin-based fluorescent voltage indicator</article-title>. <source>Neuron</source>, <volume>111</volume>(<issue>10</issue>), <fpage>1547</fpage>–<lpage>1563.e9.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.009</pub-id></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aggarwal</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Ralowicz</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Bergerson</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Tomaska</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mohar</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Hanson</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Hasseman</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Reep</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Tsegaye</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Yao</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ji</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Kloos</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Walpita</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mohr</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Tillberg</surname>, <given-names>P. W.</given-names></string-name>, <string-name><surname>Looger</surname>, <given-names>L. L.</given-names></string-name>, <etal>…</etal> <string-name><surname>Podgorski</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Glutamate indicators with improved activation kinetics and localization for imaging synaptic transmission</article-title>. <source>Nature Methods</source>, <volume>20</volume>(<issue>6</issue>), <fpage>925</fpage>–<lpage>934</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-023-01863-6</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alten</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Shin</surname>, <given-names>O.-H.</given-names></string-name>, <string-name><surname>Esquivies</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>P.-Y.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>K. I.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chung</surname>, <given-names>W. K.</given-names></string-name>, <string-name><surname>Monteggia</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Brunger</surname>, <given-names>A. T.</given-names></string-name>, &amp; <string-name><surname>Kavalali</surname>, <given-names>E. T</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Role of aberrant spontaneous neurotransmission in snap25- associated encephalopathies</article-title>. <source>Neuron</source>, <volume>109</volume>(<issue>1</issue>), <fpage>59</fpage>–<lpage>72.e5.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2020.10.012</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ankri</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Legendre</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Faber</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Korn</surname>, <given-names>H</given-names></string-name></person-group>. (<year>1994</year>). <article-title>Automatic detection of spontaneous synaptic responses in central neurons</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>52</volume>(<issue>1</issue>), <fpage>87</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/0165-0270(94)90060-4</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Ardiles, Á . O., Tapia-Rojas, C. C., Mandal, M., Alexandre, F., Kirkwood, A., Inestrosa, N. C., &amp; Palacios, A. G</collab></person-group>. (<year>2012</year>). <article-title>Postsynaptic dysfunction is associated with spatial and object recognition memory loss in a natural model of alzheimer’s disease</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>34</issue>), <fpage>13835</fpage>–<lpage>13840</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1201209109</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Asadollahi</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Delvendahl</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Muff</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tan</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Rodŕıguez</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Turan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Russo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Oneda</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Joset</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Boonsawat</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Masood</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mocera</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ivanovski</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Baumer</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bachmann-Gagescu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Schlapbach</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rehrauer</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Steindl</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Begemann</surname>, <given-names>A.</given-names></string-name>, <string-name><given-names>. . .</given-names> <surname>Rauch</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Pathogenic <italic>SCN2A</italic> variants cause early-stage dysfunction in patient-derived neurons</article-title>. <source>Human Molecular Genetics</source>, <volume>32</volume>(<issue>13</issue>), <fpage>2192</fpage>–<lpage>2204</lpage>. <pub-id pub-id-type="doi">10.1093/hmg/ddad048</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ascoli</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Maraver</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Nanda</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Polavaram</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Armañanzas</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Win–win data sharing in neuroscience</article-title>. <source>Nature Methods</source>, <volume>14</volume>(<issue>2</issue>), <fpage>112</fpage>–<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4152</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baccino-Calace</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Mü ller</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2022</year>). <article-title>The e3 ligase thin controls homeostatic plasticity through neurotransmitter release repression</article-title>. <source>eLife</source>, <volume>11</volume>. <pub-id pub-id-type="doi">10.7554/elife.71437</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bailly</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Blanc</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Francis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Guillotin</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Jamal</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Wakim</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Roy</surname>, <given-names>P</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Effects of dataset size and interactions on the prediction performance of logistic regression and deep learning models</article-title>. <source>Computer Methods and Programs in Biomedicine</source>, <volume>213</volume>, <fpage>106504</fpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2021.106504</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Banerjee</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vernon</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jiao</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Ruchti</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Asadzadeh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Burri</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Stowers</surname>, <given-names>R. S.</given-names></string-name>, &amp; <string-name><surname>McCabe</surname>, <given-names>B. D</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Miniature neurotransmission is required to maintain drosophila synaptic structures during ageing</article-title>. <source>Nature Communications</source>, <volume>12</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1038/s41467-021-24490-1</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Batal</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Hauskrecht</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2009</year>). <article-title>A supervised time series feature extraction technique using DCT and DWT</article-title>. <conf-name>2009 International Conference on Machine Learning and Applications</conf-name>. <pub-id pub-id-type="doi">10.1109/icmla.2009.13</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Campagnola</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Seeman</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Chartrand</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hoggarth</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gamlin</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ito</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Trinh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Davoudian</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Radaelli</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>M.-H.</given-names></string-name>, <string-name><surname>Hage</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Braun</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Alfiler</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Andrade</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bohn</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dalley</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Henry</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kebede</surname>, <given-names>S.</given-names></string-name>, <etal>…</etal> <string-name><surname>Jarsky</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Local connectivity and synaptic dynamics in mouse and human neocortex</article-title>. <source>Science</source>, <volume>375</volume>(<fpage>6585</fpage>). <pub-id pub-id-type="doi">10.1126/science.abj5861</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Caruana</surname>, <given-names>R.</given-names></string-name> (). . In <string-name><given-names>G.</given-names> <surname>Tesauro</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Touretzky</surname></string-name>, &amp; <string-name><given-names>T.</given-names> <surname>Leen</surname></string-name></person-group><year>1994</year>). <article-title>Learning many related tasks at the same time with backpropagation</article-title>. <conf-name>Advances in neural information processing systems</conf-name>. <publisher-name>MIT Press</publisher-name>. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf</ext-link></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clements</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Bekkers</surname>, <given-names>J</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Detection of spontaneous synaptic events with an optimally scaled template</article-title>. <source>Biophysical Journal</source>, <volume>73</volume>(<issue>1</issue>), <fpage>220</fpage>–<lpage>229</lpage>. <pub-id pub-id-type="doi">10.1016/s0006-3495(97)78062-7</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Delvendahl</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Kita</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Müller</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Rapid and sustained homeostatic control of presynaptic exocytosis at a central synapse</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>116</volume>(<issue>47</issue>), <fpage>23783</fpage>–<lpage>23789</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1909675116</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denis</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dard</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Quiroli</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Cossart</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Picardo</surname>, <given-names>M. A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>DeepCINAC: A deep-learning-based python toolbox for inferring calcium imaging neuronal activity based on movie visualization</article-title>. <source>eneuro</source>, <volume>7</volume> (<issue>4</issue>), <elocation-id>ENEURO.0038–20.2020</elocation-id>. <pub-id pub-id-type="doi">10.1523/eneuro.0038-20.2020</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiAntonio</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Heckmann</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Goodman</surname>, <given-names>C. S</given-names></string-name></person-group>. (<year>1999</year>). <article-title>Glutamate receptor expression regulates quantal size and quantal content at the <italic>Drosophila</italic> neuromuscular junction</article-title>. <source>The Journal of Neuroscience</source>, <volume>19</volume>(<issue>8</issue>), <fpage>3023</fpage>–<lpage>3032</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.19-08-03023.1999</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Donahue</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hendricks</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Rohrbach</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Venugopalan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Guadarrama</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Saenko</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Darrell</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2017</year>). <article-title>Long-term recurrent convolutional networks for visual recognition and description</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>39</volume>(<issue>4</issue>), <fpage>677</fpage>–<lpage>691</lpage>. <pub-id pub-id-type="doi">10.1109/tpami.2016.2599174</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Evans</surname>, <given-names>S. W.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>D.-Q.</given-names></string-name>, <string-name><surname>Chavarha</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Plitt</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Taxidis</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Madruga</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>F.-J.</given-names></string-name>, <string-name><surname>van Keulen</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Suomivuori</surname>, <given-names>C.-M.</given-names></string-name>, <string-name><surname>Pang</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hao</surname>, <given-names>Y. A.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Pradhan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <etal>. . .</etal> <string-name><surname>Lin</surname>, <given-names>M. Z.</given-names></string-name></person-group> (<year>2023</year>). <article-title>A positively tuned voltage indicator for extended electrical recordings in the brain</article-title>. <source>Nature Methods</source>, <volume>20</volume>(<issue>7</issue>), <fpage>1104</fpage>–<lpage>1113</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-023-01913-z</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fawaz</surname>, <given-names>H. I.</given-names></string-name>, <string-name><surname>Forestier</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Weber</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Idoumghar</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Muller</surname>, <given-names>P.-A</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Deep learning for time series classification: A review</article-title>. <source>Data Mining and Knowledge Discovery</source>, <volume>33</volume>(<issue>4</issue>), <fpage>917</fpage>–<lpage>963</lpage>. <pub-id pub-id-type="doi">10.1007/s10618-019-00619-1</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferguson</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Nielson</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Cragin</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Bandrowski</surname>, <given-names>A. E.</given-names></string-name>, &amp; <string-name><surname>Martone</surname>, <given-names>M. E</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Big data from small data: Data-sharing in the ’long tail’ of neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume> (<issue>11</issue>), <fpage>1442</fpage>–<lpage>1447</lpage>. <pub-id pub-id-type="doi">10.1038/nn.3838</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hao</surname>, <given-names>Y. A.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Natale</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gomez</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Taxidis</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>O’Neill</surname>, <given-names>P. S.</given-names></string-name>, <string-name><surname>Villette</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bradley</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sheng</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Boyden</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Delvendahl</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Golshani</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Wernig</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Feldman</surname>, <given-names>D. E.</given-names></string-name>, <etal>…</etal> <string-name><surname>Lin</surname>, <given-names>M. Z.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A fast and responsive voltage indicator with enhanced sensitivity for unitary synaptic events</article-title>. <source>Neuron</source>. <pub-id pub-id-type="doi">10.1016/j.neuron.2024.08.019</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ho</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tumkaya</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Aryal</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Claridge-Chang</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Moving beyond p values: Data analysis with estimation graphics</article-title>. <source>Nature Methods</source>, <volume>16</volume>(<issue>7</issue>), <fpage>565</fpage>–<lpage>566</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0470-3</pub-id></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kö stinger</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>K. A. C.</given-names></string-name>, <string-name><surname>Schuhknecht</surname>, <given-names>G. F. P.</given-names></string-name>, &amp; <string-name><surname>Stratford</surname>, <given-names>K. J.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Structure and function of a neocortical synapse</article-title>. <source>Nature</source>, <volume>591</volume>(<issue>7848</issue>), <fpage>111</fpage>–<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-03134-2</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huganir</surname>, <given-names>R. L.</given-names></string-name>, &amp; <string-name><surname>Nicoll</surname>, <given-names>R. A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>AMPARs and synaptic plasticity: The last 25 years</article-title>. <source>Neuron</source>, <volume>80</volume>(<issue>3</issue>), <fpage>704</fpage>–<lpage>717</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.025</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Imbrosci</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schmitz</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Orlando</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2022</year>). <article-title>Automated detection and localization of synaptic vesicles in electron microscopy images</article-title>. <source>eneuro</source>, <volume>9</volume>(<issue>1</issue>), <elocation-id>ENEURO.0400–20.2021</elocation-id>. <pub-id pub-id-type="doi">10.1523/eneuro.0400-20.2021</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Ioffe</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Szegedy</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title>. <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/ARXIV.1502.03167</pub-id>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iqbal</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Khan</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Karayannis</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2019</year>). <article-title>Developing a brain atlas through deep learning</article-title>. <source>Nature Machine Intelligence</source>, <volume>1</volume>(<issue>6</issue>), <fpage>277</fpage>–<lpage>287</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-019-0058-8</pub-id></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ishii</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Stolz</surname>, <given-names>J. R.</given-names></string-name>, &amp; <string-name><surname>Swanson</surname>, <given-names>G. T</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Auxiliary proteins are the predominant determinants of differ- ential efficacy of clinical candidates acting as AMPA receptor positive allosteric modulators</article-title>. <source>Molecular Pharmacology</source>, <volume>97</volume> (<issue>5</issue>), <fpage>336</fpage>–<lpage>350</lpage>. <pub-id pub-id-type="doi">10.1124/mol.119.118554</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Islam</surname>, <given-names>M. Z.</given-names></string-name>, <string-name><surname>Islam</surname>, <given-names>M. M.</given-names></string-name>, &amp; <string-name><surname>Asraf</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2020</year>). <article-title>A combined deep CNN-LSTM network for the detection of novel coronavirus (COVID-19) using x-ray images</article-title>. <source>Informatics in Medicine Unlocked</source>, <volume>20</volume>, <fpage>100412</fpage>. <pub-id pub-id-type="doi">10.1016/j.imu.2020.100412</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jonas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Major</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Sakmann</surname>, <given-names>B</given-names></string-name></person-group>. (<year>1993</year>). <article-title>Quantal components of unitary EPSCs at the mossy fibre synapse on CA3 pyramidal cells of rat hippocampus</article-title>. <source>The Journal of Physiology</source>, <volume>472</volume>(<issue>1</issue>), <fpage>615</fpage>–<lpage>663</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1993.sp019965</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kaeser</surname>, <given-names>P. S.</given-names></string-name>, &amp; <string-name><surname>Regehr</surname>, <given-names>W. G</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Molecular mechanisms for synchronous, asynchronous, and spontaneous neurotransmitter release</article-title>. <source>Annual Review of Physiology</source>, <volume>76</volume>(<issue>1</issue>), <fpage>333</fpage>–<lpage>363</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-physiol-021113-170338</pub-id></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kavalali</surname>, <given-names>E. T</given-names></string-name></person-group>. (<year>2015</year>). <article-title>The mechanisms and functions of spontaneous neurotransmitter release</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>16</volume>(<issue>1</issue>), <fpage>5</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3875</pub-id></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>Y. G.</given-names></string-name>, <string-name><surname>Shin</surname>, <given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Kim</surname>, <given-names>S. J</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Minhee analysis package: An integrated software package for detection and management of spontaneous synaptic events</article-title>. <source>Molecular Brain</source>, <volume>14</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1186/s13041-021-00847-x</pub-id></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name>, &amp; <string-name><surname>Ba</surname>, <given-names>J</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/ARXIV.1412.6980</pub-id>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kita</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Albergaria</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Machado</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Carey</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Müller</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Delvendahl</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2021</year>). <article-title>GluA4 facilitates cerebellar expansion coding and enables associative memory formation</article-title>. <source>eLife</source>, <volume>10</volume>. <pub-id pub-id-type="doi">10.7554/elife.65152</pub-id></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kudoh</surname>, <given-names>S. N.</given-names></string-name>, &amp; <string-name><surname>Taguchi</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2002</year>). <article-title>A simple exploratory algorithm for the accurate and fast detection of spontaneous synaptic events</article-title>. <source>Biosensors and Bioelectronics</source>, <volume>17</volume> (<issue>9</issue>), <fpage>773</fpage>–<lpage>782</lpage>. <pub-id pub-id-type="doi">10.1016/s0956-5663(02)00053-2</pub-id></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G</given-names></string-name></person-group>. (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>(<issue>7553</issue>), <fpage>436</fpage>–<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lepeta</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lourenco</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Schweitzer</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Adami</surname>, <given-names>P. V. M.</given-names></string-name>, <string-name><surname>Banerjee</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Catuara-Solarz</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>de La Fuente Revenga</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Guillem</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Haidar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ijomone</surname>, <given-names>O. M.</given-names></string-name>, <string-name><surname>Nadorp</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Qi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Perera</surname>, <given-names>N. D.</given-names></string-name>, <string-name><surname>Refsgaard</surname>, <given-names>L. K.</given-names></string-name>, <string-name><surname>Reid</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Sabbar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sahoo</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schaefer</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Sheean</surname>, <given-names>R. K.</given-names></string-name>, <string-name><given-names>. . .</given-names> <surname>Seidenbecher</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Synaptopathies: Synaptic dysfunction in neurological disorders - a review from students to students</article-title>. <source>Journal of Neurochemistry</source>, <volume>138</volume>(<issue>6</issue>), <fpage>785</fpage>–<lpage>805</lpage>. <pub-id pub-id-type="doi">10.1111/jnc.13713</pub-id></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lesperance</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>Y.-M.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>L.-Y</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Delayed expression of activity-dependent gating switch in synaptic AMPARs at a central synapse</article-title>. <source>Molecular Brain</source>, <volume>13</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1186/s13041-019-0536-2</pub-id></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Chavarha</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kobayashi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yoshinaga</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nakajima</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>M. Z.</given-names></string-name>, &amp; <string-name><surname>Inoue</surname>, <given-names>T</given-names></string-name></person-group>. (<year>2020</year>). <article-title>Two-photon voltage imaging of spontaneous activity from multiple neurons reveals network activity in brain tissue</article-title>. <source>iScience</source>, <volume>23</volume>(<issue>8</issue>), <fpage>101363</fpage>. <pub-id pub-id-type="doi">10.1016/j.isci.2020.101363</pub-id></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lowen</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Cash</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Poo</surname>, <given-names>M.-m.</given-names></string-name>, &amp; <string-name><surname>Teich</surname>, <given-names>M. C.</given-names></string-name></person-group> (<year>1997</year>). <article-title>Quantal neurotransmitter secretion rate exhibits fractal behavior</article-title>. <source>The Journal of Neuroscience</source>, <volume>17</volume> (<issue>15</issue>), <fpage>5666</fpage>–<lpage>5677</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.17-15-05666.1997</pub-id></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malinow</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Malenka</surname>, <given-names>R. C</given-names></string-name></person-group>. (<year>2002</year>). <article-title>AMPA receptor trafficking and synaptic plasticity</article-title>. <source>Annual Review of Neuroscience</source>, <volume>25</volume>(<issue>1</issue>), <fpage>103</fpage>–<lpage>126</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.25.112701.142758</pub-id></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="web"><person-group person-group-type="author"><string-name><surname>Martín</surname> <given-names>Abadi</given-names></string-name>, <string-name><surname>Ashish</surname> <given-names>Agarwal</given-names></string-name>, <string-name><surname>Paul</surname> <given-names>Barham</given-names></string-name>, <string-name><surname>Eugene</surname> <given-names>Brevdo</given-names></string-name>, <string-name><surname>Zhifeng</surname> <given-names>Chen</given-names></string-name>, <string-name><surname>Craig</surname> <given-names>Citro</given-names></string-name>, <string-name><given-names>Greg S.</given-names> <surname>Corrado</surname></string-name>, <string-name><surname>Andy</surname> <given-names>Davis</given-names></string-name>, <string-name><surname>Jeffrey</surname> <given-names>Dean</given-names></string-name>, <string-name><surname>Matthieu</surname> <given-names>Devin</given-names></string-name>, <string-name><surname>Sanjay</surname> <given-names>Ghemawat</given-names></string-name>, <string-name><surname>Ian</surname> <given-names>Goodfellow</given-names></string-name>, <string-name><surname>Andrew</surname> <given-names>Harp</given-names></string-name>, <string-name><surname>Geoffrey</surname> <given-names>Irving</given-names></string-name>, <string-name><surname>Michael</surname> <given-names>Isard</given-names></string-name>, <string-name><surname>Jia</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Rafal</surname> <given-names>Jozefowicz</given-names></string-name>, <string-name><surname>Lukasz</surname> <given-names>Kaiser</given-names></string-name>, <string-name><surname>Manjunath</surname> <given-names>Kudlur</given-names></string-name>, <etal>…</etal> <string-name><surname>Xiaoqiang</surname> <given-names>Zheng</given-names></string-name></person-group>. (<year>2015</year>). <source>TensorFlow: Large-scale machine learning on heterogeneous systems</source> [Software available from <ext-link ext-link-type="uri" xlink:href="http://tensorflow.org">tensorflow.org</ext-link>]. <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKinney</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Capogna</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dürr</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Gähwiler</surname>, <given-names>B.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Miniature synaptic events maintain dendritic spines via AMPA receptor activation</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume>(<issue>1</issue>), <fpage>44</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1038/4548</pub-id></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shababo</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Naka</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Adesnik</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Paninski</surname>, <given-names>L</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Bayesian methods for event analysis of intracellular currents</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>269</volume>, <fpage>21</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.05.015</pub-id></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miller</surname>, <given-names>E. C.</given-names></string-name>, <string-name><surname>Teravskis</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Dummer</surname>, <given-names>B. W.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Huganir</surname>, <given-names>R. L.</given-names></string-name>, &amp; <string-name><surname>Liao</surname>, <given-names>D</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Tau phosphorylation and tau mislocalization mediate soluble aβ oligomer-induced ampa glutamate receptor signaling deficits</article-title>. <source>European Journal of Neuroscience</source>, <volume>39</volume>(<issue>7</issue>), <fpage>1214</fpage>–<lpage>1224</lpage>. <pub-id pub-id-type="doi">10.1111/ejn.12507</pub-id></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mori</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rosko</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Farnsworth</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Carrasco</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Broomandkhoshbacht</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pareja-Navarro</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Pejmun Haghighi</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Simplyfire: An open-source, customizable software application for the analysis of synaptic events</article-title>. <source>eneuro</source>, <volume>11</volume>(<issue>1</issue>), <elocation-id>ENEURO.0326–23.2023</elocation-id>. <pub-id pub-id-type="doi">10.1523/eneuro.0326-23.2023</pub-id></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O’Brien</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Kamboj</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ehlers</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Rosen</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Fischbach</surname>, <given-names>G. D.</given-names></string-name>, &amp; <string-name><surname>Huganir</surname>, <given-names>R. L</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Activity- dependent modulation of synaptic AMPA receptor accumulation</article-title>. <source>Neuron</source>, <volume>21</volume>(<issue>5</issue>), <fpage>1067</fpage>–<lpage>1078</lpage>. <pub-id pub-id-type="doi">10.1016/s0896-6273(00)80624-8</pub-id></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Passricha</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Aggarwal</surname>, <given-names>R. K</given-names></string-name></person-group>. (<year>2019</year>). <article-title>A hybrid of deep CNN and bidirectional LSTM for automatic speech recognition</article-title>. <source>Journal of Intelligent Systems</source>, <volume>29</volume>(<issue>1</issue>), <fpage>1261</fpage>–<lpage>1274</lpage>. <pub-id pub-id-type="doi">10.1515/jisys-2018-0372</pub-id></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Perńıa-Andrade, A. J., Goswami, S. P., Stickler, Y., Frö be, U., Schlö gl, A., &amp; Jonas, P</collab></person-group>. (<year>2012</year>). <article-title>A deconvolution- based method with high sensitivity and temporal resolution for detection of spontaneous synaptic currents in vitro and in vivo</article-title>. <source>Biophysical Journal</source>, <volume>103</volume>(<issue>7</issue>), <fpage>1429</fpage>–<lpage>1439</lpage>. <pub-id pub-id-type="doi">10.1016/j.bpj.2012.08.039</pub-id></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petersen</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Fetter</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Noordermeer</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Goodman</surname>, <given-names>C. S.</given-names></string-name>, &amp; <string-name><surname>DiAntonio</surname>, <given-names>A</given-names></string-name></person-group>. (<year>1997</year>). <article-title>Genetic analysis of glutamate receptors in drosophila reveals a retrograde signal regulating presynaptic transmitter release</article-title>. <source>Neuron</source>, <volume>19</volume>(<issue>6</issue>), <fpage>1237</fpage>–<lpage>1248</lpage>. <pub-id pub-id-type="doi">10.1016/s0896-6273(00)80415-8</pub-id></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Pratt</surname>, <given-names>L. Y.</given-names></string-name>, <string-name><surname>Mostow</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Kamm</surname>, <given-names>C. A.</given-names></string-name></person-group> (<year>1991</year>). <article-title>Direct transfer of learned information among neural networks</article-title>. <conf-name>AAAI Conference on Artificial Intelligence</conf-name>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ralowicz</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Hokeness</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Hoppa</surname>, <given-names>M. B</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Frequency of spontaneous neurotransmission at individual boutons corresponds to the size of the readily releasable pool of vesicles. <italic>The Journal of Neuroscience</italic></article-title>, <source>e</source><volume>1253232024</volume>. <pub-id pub-id-type="doi">10.1523/jneurosci.1253-23.2024</pub-id></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Reddi</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Kale</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Kumar</surname>, <given-names>S</given-names></string-name></person-group>. (<year>2019</year>). <article-title>On the convergence of adam and beyond</article-title>. <source>arXiv</source>, <pub-id pub-id-type="doi">10.48550/ARXIV.1904.09237</pub-id>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tsao</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Zador</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2022</year>). <article-title>The application of artificial intelligence to biology and neuroscience</article-title>. <source>Cell</source>, <volume>185</volume>(<issue>15</issue>), <fpage>2640</fpage>–<lpage>2643</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2022.06.047</pub-id></mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rupprecht</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Carta</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hoffmann</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Echizen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Blot</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kwan</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hofer</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Kitamura</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Helmchen</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Friedrich</surname>, <given-names>R. W</given-names></string-name></person-group>. (<year>2021</year>). <article-title>A database and deep learning toolbox for noise-optimized, generalized spike inference from calcium imaging</article-title>. <source>Nature Neuroscience</source>, <volume>24</volume>(<issue>9</issue>), <fpage>1324</fpage>–<lpage>1337</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-021-00895-5</pub-id></mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rupprecht</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Friedrich</surname>, <given-names>R. W</given-names></string-name></person-group>. (<year>2018</year>). <article-title>Precise synaptic balance in the zebrafish homolog of olfactory cortex</article-title>. <source>Neuron</source>, <volume>100</volume>(<issue>3</issue>), <fpage>669</fpage>–<lpage>683.e5.</lpage> <pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.013</pub-id></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Nenadic</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Xu</surname>, <given-names>X</given-names></string-name></person-group>. (<year>2010</year>). <article-title>Novel use of matched filtering for synaptic event detection and extraction</article-title>. <source>PLoS ONE</source>, <volume>5</volume>(<issue>11</issue>), <fpage>e15517</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0015517</pub-id></mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Simonyan</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Vedaldi</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Zisserman</surname>, <given-names>A</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Deep inside convolutional networks: Visualising image classification models and saliency maps</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/ARXIV.1312.6034</pub-id></mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Sitá, L., Brondi, M., de Leon Roig, P. L., Curreli, S., Panniello, M., Vecchia, D., &amp; Fellin, T.</collab></person-group> (<year>2022</year>). <article-title>A deep-learning approach for online cell identification and trace extraction in functional two-photon calcium imaging</article-title>. <source>Nature Communications</source>, <volume>13</volume>(<issue>1</issue>), <fpage>1529</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-022-29180-0</pub-id></mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sjulson</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Miesenböck</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Optical recording of action potentials and other discrete physiological events: A perspective from signal detection theory</article-title>. <source>Physiology</source>, <volume>22</volume>(<issue>1</issue>), <fpage>47</fpage>–<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1152/physiol.00036.2006</pub-id></mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Srivastava</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Salakhutdinov</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2014</year>). <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>. <source>The journal of machine learning research</source>, <volume>15</volume>(<issue>1</issue>), <fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation></ref>
<ref id="c64a"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tasdelen</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Sen</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2021</year>). <article-title>A hybrid CNN-LSTM model for pre-miRNA classification</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41598-021-93656-0</pub-id></mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Theis</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Froudarakis</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Reimer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Román Rosón</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Baden</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Euler</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tolias</surname>, <given-names>A. S.</given-names></string-name>, &amp; <string-name><surname>Bethge</surname>, <given-names>M</given-names></string-name></person-group>. (<year>2016</year>). <article-title>Benchmarking spike rate inference in population calcium imaging</article-title>. <source>Neuron</source>, <volume>90</volume>(<issue>3</issue>), <fpage>471</fpage>–<lpage>482</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.014</pub-id></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Theodoris</surname>, <given-names>C. V.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Chopra</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chaffin</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Sayed</surname>, <given-names>Z. R. A.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Mantineo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Brydon</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Zeng</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>X. S.</given-names></string-name>, &amp; <string-name><surname>Ellinor</surname>, <given-names>P. T</given-names></string-name></person-group>. (<year>2023</year>). <article-title>Transfer learning enables predictions in network biology</article-title>. <source>Nature</source>, <volume>618</volume>(<issue>7965</issue>), <fpage>616</fpage>–<lpage>624</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-023-06139-9</pub-id></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tran</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Stricker</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Spontaneous and action potential-evoked ca2+ release from endoplasmic reticulum in neocortical synaptic boutons</article-title>. <source>Cell Calcium</source>, <volume>97</volume>, <fpage>102433</fpage>. <pub-id pub-id-type="doi">10.1016/j.ceca.2021.102433</pub-id></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turrigiano</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Leslie</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Desai</surname>, <given-names>N. S.</given-names></string-name>, <string-name><surname>Rutherford</surname>, <given-names>L. C.</given-names></string-name>, &amp; <string-name><surname>Nelson</surname>, <given-names>S. B</given-names></string-name></person-group>. (<year>1998</year>). <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title>. <source>Nature</source>, <volume>391</volume>(<issue>6670</issue>), <fpage>892</fpage>–<lpage>896</lpage>. <pub-id pub-id-type="doi">10.1038/36103</pub-id></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Ploeg</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Austin</surname>, <given-names>P. C.</given-names></string-name>, &amp; <string-name><surname>Steyerberg</surname>, <given-names>E. W.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Modern modelling techniques are data hungry: A simulation study for predicting dichotomous endpoints</article-title>. <source>Bller Medical Research Methodology</source>, <volume>14</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1186/1471-2288-14-137</pub-id></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>N.-S.</given-names></string-name>, <string-name><surname>Marino</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Malinow</surname>, <given-names>R</given-names></string-name></person-group>. (<year>2024</year>). <article-title>Detecting unitary synaptic events with machine learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>121</volume>(<fpage>6</fpage>). <pub-id pub-id-type="doi">10.1073/pnas.2315804121</pub-id></mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Oates</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Time series classification from scratch with deep neural networks: A strong baseline</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/ARXIV.1611.06455</pub-id></mixed-citation></ref>
<ref id="c72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilt</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Fitzgerald</surname>, <given-names>J. E.</given-names></string-name>, &amp; <string-name><surname>Schnitzer</surname>, <given-names>M. J</given-names></string-name></person-group>. (<year>2013</year>). <article-title>Photon shot noise limits on optical detection of neuronal spikes and estimation of spike timing</article-title>. <source>Biophysical Journal</source>, <volume>104</volume>(<issue>1</issue>), <fpage>51</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1016/j.bpj.2012.07.058</pub-id></mixed-citation></ref>
<ref id="c73"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Ye</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Keogh</surname>, <given-names>E</given-names></string-name></person-group>. (<year>2009</year>). <article-title>Time series shapelets</article-title>. <conf-name>Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</conf-name>. <pub-id pub-id-type="doi">10.1145/1557019.1557122</pub-id></mixed-citation></ref>
<ref id="c74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yip</surname>, <given-names>M. C.</given-names></string-name>, <string-name><surname>Gonzalez</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Valenta</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Rowan</surname>, <given-names>M. J. M.</given-names></string-name>, &amp; <string-name><surname>Forest</surname>, <given-names>C. R</given-names></string-name></person-group>. (<year>2021</year>). <article-title>Deep learning-based real-time detection of neurons in brain slices for in vitro physiology</article-title>. <source>Scientific Reports</source>, <volume>11</volume>(<fpage>1</fpage>). <pub-id pub-id-type="doi">10.1038/s41598-021-85695-4</pub-id></mixed-citation></ref>
<ref id="c75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yosinski</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Clune</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Lipson</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2014</year>). <article-title>How transferable are features in deep neural networks?</article-title>. <source>NIPS</source>.</mixed-citation></ref>
<ref id="c76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Schlö gl</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Vandael</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Jonas</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>MOD: A novel machine-learning optimal-filtering method for accurate and efficient detection of subthreshold synaptic events in vivo</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>357</volume>, <fpage>109125</fpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2021.109125</pub-id></mixed-citation></ref>
<ref id="c77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Rózsa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Liang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bushey</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wei</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Reep</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Broussard</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Tsang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tsegaye</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Narayan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Obara</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Lim</surname>, <given-names>J.-X.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Ahrens</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>G. C.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S. S.-H.</given-names></string-name>, <string-name><surname>Korff</surname>, <given-names>W. L.</given-names></string-name>, <etal>…</etal> <string-name><surname>Looger</surname>, <given-names>L. L.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Fast and sensitive gcamp calcium indicators for imaging neural populations</article-title>. <source>Nature</source>, <volume>615</volume>(<issue>7954</issue>), <fpage>884</fpage>–<lpage>891</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-023-05828-9</pub-id></mixed-citation></ref>
<ref id="c78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zoghbi</surname>, <given-names>H. Y.</given-names></string-name>, &amp; <string-name><surname>Bear</surname>, <given-names>M. F</given-names></string-name></person-group>. (<year>2012</year>). <article-title>Synaptic dysfunction in neurodevelopmental disorders associated with autism and intellectual disabilities</article-title>. <source>Cold Spring Harbor Perspectives in Biology</source>, <volume>4</volume>(<issue>3</issue>), <fpage>a009886</fpage>–<lpage>a009886</lpage>. <pub-id pub-id-type="doi">10.1101/cshperspect.a009886</pub-id></mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98485.2.sa3</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dickman</surname>
<given-names>Dion K</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution>University of Southern California</institution>
</institution-wrap>
<city>Los Angeles</city>
<country>United States of America</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Compelling</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This paper presents miniML, an AI-based framework for the detection of synaptic events. Benchmark results presented in the paper are <bold>compelling</bold>, demonstrating the superiority of miniML over current state-of-the-art alternatives. The performance of miniML is demonstrated across various experimental paradigms, showing that miniML has the potential to become a <bold>valuable</bold> tool for the analysis of synaptic signals.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98485.2.sa2</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>O'Neill et al. have developed a software analysis application, miniML, that enables the quantification of electrophysiological events. They utilize a supervised deep learned-based method to optimize the software. miniML is able to quantify and standardize the analyses of miniature events, using both voltage and current clamp electrophysiology, as well as optically driven events using iGluSnFR3, in a variety of preparations, including in the cerebellum, calyx of held, golgi cell, human iPSC cultures, zebrafish, and Drosophila. The software appears to be flexible, in that users are able to hone and adapt the software to new preparations and events. Importantly, miniML is an open source software free for researchers to use and enables users to adapt new features using Python.</p>
<p>Overall this new software has the potential to become widely used in the field and an asset to researchers. Importantly, a new graphical user interface has been generated that enables more user control and a more user-friendly experience. Further, the authors demonstrate how miniML performs relative to other platforms that have been developed, and highlight areas where miniML works optimally. With these revisions, miniML should now be of considerable benefit and utility to a variety of researchers.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98485.2.sa1</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>This paper presents miniML as a supervised method for detection of spontaneous synaptic events. Recordings of such events are typically of low SNR, where state-of-the-art methods are prone to high false favourable rates. Unlike current methods, training miniML requires neither prior knowledge of the kinetics of events nor the tuning of parameters/thresholds.</p>
<p>The proposed method comprises four convolutional networks, followed by a bi-directional LSTM and a final fully connected layer, which outputs a decision event/no event per time window. A sliding window is used when applying miniML to a temporal signal, followed by an additional estimation of events' time stamps. miniML outperforms current methods for simulated events superimposed on real data (with no events) and presents compelling results for real data across experimental paradigms and species.</p>
<p>Strengths:</p>
<p>The authors present a pipeline for benchmarking based on simulated events superimposed on real data (with no events). Compared to five other state-of-the-art methods, miniML leads to the highest detection rates and is most robust to specific choices of threshold values for fast or slow kinetics. A major strength of miniML is the ability to use it for different datasets. For this purpose, the CNN part of the model is held fixed and the subsequent networks are trained to adapt to the new data. This Transfer Learning (TL) strategy reduces computation time significantly and more importantly, it allows for using a substantially smaller data set (compared to training a full model) which is crucial as training is supervised (i.e. uses labeled examples).</p>
<p>Weaknesses:</p>
<p>
The authors do not indicate how the specific configuration of miniML was set, i.e. number of CNNs, units, LSTM, etc. Please provide further information regarding these design choices, whether they were based on similar models or if chosen based on performance.</p>
<p>The data for the benchmark system was augmented with equal amounts of segments with/without events. Data augmentation was undoubtedly crucial for successful training.</p>
<p>
(1) Does a balanced dataset reflect the natural occurrence of events in real data? Could the authors provide more information regarding this matter?</p>
<p>
(2) Please provide a more detailed description of this process as it would serve users aiming to use this method for other sub-fields.</p>
<p>The benchmarking pipeline is indeed valuable and the results are compelling. However, the authors do not provide comparative results for miniML for real data (figures 4-8). TL does not apply to the other methods. In my opinion, presenting the performance of other methods, trained using the smaller dataset would be convincing of the modularity and applicability of the proposed approach.</p>
<p>Impact:</p>
<p>Accurate detection of synaptic events is crucial for the study of neural function. miniML has a great potential to become a valuable tool for this purpose as it yields highly accurate detection rates, it is robust, and is relatively easily adaptable to different experimental setups.</p>
<p>Comments on revisions:</p>
<p>The revised manuscript presents a compelling framework. The performance of mini ML is thouroughly explored and compared to several benchmarks. The training process along with other technical issues are now described in a satisfactory level of detail.</p>
<p>
I think the authors did a great job. They answered all claims and concerns raised by me and the other reviewers.</p>
</body>
</sub-article>
<sub-article id="sa3" article-type="author-comment">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.98485.2.sa0</article-id>
<title-group>
<article-title>Author response:</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>O’Neill</surname>
<given-names>Philipp S</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Baccino-Calace</surname>
<given-names>Martín</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4143-4166</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Rupprecht</surname>
<given-names>Peter</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8235-8257</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Lee</surname>
<given-names>Sungmoo</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hao</surname>
<given-names>Yukun A</given-names>
</name>
<role specific-use="author">Author</role>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lin</surname>
<given-names>Michael Z</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0492-1961</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Friedrich</surname>
<given-names>Rainer W</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9107-0482</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Müller</surname>
<given-names>Martin</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1624-6761</contrib-id></contrib>
<contrib contrib-type="author">
<name>
<surname>Delvendahl</surname>
<given-names>Igor</given-names>
</name>
<role specific-use="author">Author</role>
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6151-2363</contrib-id></contrib>
</contrib-group>
</front-stub>
<body>
<p>The following is the authors’ response to the original reviews.</p>
<disp-quote content-type="editor-comment">
<p><bold>Public Reviews:</bold></p>
<p><bold>Reviewer 1 (Public Review):</bold></p>
<p>O’Neill et al. have developed a software analysis application, miniML, that enables the quantification of electrophysiological events. They utilize a supervised deep learned-based method to optimize the software. miniML is able to quantify and standardize the analyses of miniature events, using both voltage and current clamp electrophysiology, as well as optically driven events using iGluSnFR3, in a variety of preparations, including in the cerebellum, calyx of held, Golgi cell, human iPSC cultures, zebrafish, and Drosophila. The software appears to be flexible, in that users are able to hone and adapt the software to new preparations and events. Importantly, miniML is an open-source software free for researchers to use and enables users to adapt new features using Python.</p>
<p>Overall this new software has the potential to become widely used in the field and an asset to researchers. However, the authors fail to discuss or even cite a similar analysis tool recently developed (SimplyFire), and determine how miniML performs relative to this platform. There are a handful of additional suggestions to make miniML more user-friendly, and of broad utility to a variety of researchers, as well as some suggestions to further validate and strengthen areas of the manuscript:</p>
<p>(1) miniML relative to existing analysis methods: There is a major omission in this study, in that a similar open source, Python-based software package for event detection of synaptic events appears to be completely ignored. Earlier this year, another group published SimplyFire in eNeuro (Mori et al., 2024; doi: 10.1523/eneuro.0326-23.2023). Obviously, this previous study needs to be discussed and ideally compared to miniML to determine if SimplyFire is superior or similar in utility, and to underscore differences in approach and accuracy.</p>
</disp-quote>
<p>We thank the reviewer for bringing this interesting publication to our attention. We have included SimplyFire in our benchmarking for comprehensive comparison with miniML. The approach taken by SimplyFire differs from miniML in a number of ways. Our results show that miniML provides higher recall and precision than SimplyFire (revised Figure 3). We appreciate that SimplyFire provides a user-interface similar to the commonly used MiniAnalysis software. In addition, the peak-finding-based approach of SimplyFire makes it relatively robust to event shape, which facilitates analysis of diverse data. However, we noted a strong threshold-dependence and long run time of SimplyFire (revised Figure 3 and Figure 3—figure supplement 1). In addition, SimplyFire is not robust against various types of noise typically encountered in electrophysiological recordings. Our extended benchmark analysis thus indicates that AI-based event detection is superior to existing algorithmic approaches, including SimplyFire.</p>
<disp-quote content-type="editor-comment">
<p>(2) The manuscript should comment on whether miniML works equally well to quantify current clamp events (voltage; e.g. EPSP/mEPSPs) compared to voltage clamp (currents, EPSC/mEPSCs), which the manuscript highlights. Are rise and decay time constants calculated for each event similarly?</p>
</disp-quote>
<p>miniML works equally well for current- and voltage events (Figure 5, Figure 9). In general, events of opposite polarity can be analyzed by simply inverting the data. Transfer learning models may further improve the detection.</p>
<p>For each detected event, independent of data/recording type, rise times are calculated as 10–90% times (baseline–peak), and decay times are calculated as time to 50% of the peak. In addition, event decay time constants are calculated from a fit to the event average. With miniML being open-source, researchers can adapt the calculations of event statistics to their needs, if desired. In the revised manuscript, we have expanded the Methods section that describes the quantification of event statistics (Methods, Quantification).</p>
<disp-quote content-type="editor-comment">
<p>(3) The interface and capabilities of miniML appear quite similar to Mini Analysis, the free software that many in the field currently use. While the ability and flexibility for users to adapt and adjust miniML for their own uses/needs using Python programming is a clear potential advantage, can the authors comment, or better yet, demonstrate, whether there is any advantage for researchers to use miniML over Mini Analysis or SimplyFire if they just need the standard analyses?</p>
</disp-quote>
<p>Following the reviewer’s suggestion, we developed a graphical user interface (GUI) for miniML to enhance its usability (Figure 2—figure supplement 2), which is provided on the GitHub repository. Our comprehensive benchmark analysis demonstrated that miniML outperforms existing tools such as MiniAnalysis and SimplyFire. The main advantages are (i) increased reliability of results, which eliminates the need for visual inspection; (ii) fast runtime and easy automation; (iii) superior detection performance as demonstrated by higher recall in both synthetic and real data; (iv) open-source Python-based design. We believe that these advantages make miniML a valuable tool for researchers recording various types of synaptic events, offering a more efficient and reliable solution compared to existing methods.</p>
<disp-quote content-type="editor-comment">
<p>(4) Additional utilities for miniML: The authors show miniML can quantify miniature electrophysiological events both current and voltage clamp, as well as optical glutamate transients using iGluSnFR. As the authors mention in the discussion, the same approach could, in principle, be used to quantify evoked (EPSC/EPSP) events using electrophysiology, Ca2+ events (using GCaMP), and AP waveforms using voltage indicators like ASAP4. While I don’t think it is reasonable to ask the authors to generate any new experimental data, it would be great to see how miniML performs when analysing data from these approaches, particularly to quantify evoked synaptic events and/or Ca2+ (ideally postsynaptic Ca2+ signals from miniature events, as the Drosophila NMJ have developed nice approaches).</p>
</disp-quote>
<p>In the revised manuscript, we have extended the application examples of miniML. We applied miniML to detect mEPSPs recorded with the novel voltage-sensitive indicator ASAP5 (Figure 9 and Figure 9—figure supplement 1). We performed simultaneous recordings of membrane voltage through electrophysiology and ASAP5 voltage imaging in rat cultured neurons at physiological temperature. Data were analyzed using miniML, with electrophysiology data being used as ground-truth for assessing detection performance in imaging data. Our results demonstrate that miniML robustly detects mEPSPs in current-clamp, and can localize corresponding transients in imaging data. Furthermore, we observed that miniML performs better than template matching and deconvolution on ASAP5 imaging data (Figure 9 and Figure 9—figure supplement 2).</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 2 (Public Review):</bold></p>
<p>This paper presents miniML as a supervised method for the detection of spontaneous synaptic events. Recordings of such events are typically of low SNR, where state-of-the-art methods are prone to high false positive rates. Unlike current methods, training miniML requires neither prior knowledge of the kinetics of events nor the tuning of parameters/thresholds.</p>
<p>The proposed method comprises four convolutional networks, followed by a bi-directional LSTM and a final fully connected layer which outputs a decision event/no event per time window. A sliding window is used when applying miniML to a temporal signal, followed by an additional estimation of events’ time stamps. miniML outperforms current methods for simulated events superimposed on real data (with no events) and presents compelling results for real data across experimental paradigms and species. Strengths:</p>
<p>The authors present a pipeline for benchmarking based on simulated events superimposed on real data (with no events). Compared to five other state-of-the-art methods, miniML leads to the highest detection rates and is most robust to specific choices of threshold values for fast or slow kinetics. A major strength of miniML is the ability to use it for different datasets. For this purpose, the CNN part of the model is held fixed and the subsequent networks are trained to adapt to the new data. This Transfer Learning (TL) strategy reduces computation time significantly and more importantly, it allows for using a substantially smaller data set (compared to training a full model) which is crucial as training is supervised (i.e. uses labeled examples).</p>
<p>Weaknesses:</p>
<p>The authors do not indicate how the specific configuration of miniML was set, i.e. number of CNNs, units, LSTM, etc. Please provide further information regarding these design choices, whether they were based on similar models or if chosen based on performance.</p>
<p>The data for the benchmark system was augmented with equal amounts of segments with/without events. Data augmentation was undoubtedly crucial for successful training.</p>
<p>(1) Does a balanced dataset reflect the natural occurrence of events in real data? Could the authors provide more information regarding this matter?</p>
</disp-quote>
<p>In a given recording, the event frequency determines the ratio of event-containing vs. nonevent-containing data segments. Whereas many synapses have a skew towards non-events, high event frequencies as observed, e.g., in pyramidal cells or Purkinje neurons, can shift the ratio towards event-containing data.</p>
<p>For model training, we extracted data segments from mEPSC recordings in cerebellar granule cells, which have a low mEPSC frequency (about 0.2 Hz, Delvendahl et al. 2019). Unbalanced training data may complicate model training (Drummond and Holte 2003; Prati et al. 2009; Tyagi and Mittal 2020). We therefore decided to balance the training dataset for miniML by down-sampling the majority class (i.e., non-event segments), so that the final datasets for model training contained roughly equal amounts of events and non-events.</p>
<disp-quote content-type="editor-comment">
<p>(2) Please provide a more detailed description of this process as it would serve users aiming to use this method for other sub-fields.</p>
</disp-quote>
<p>We thank the reviewer for raising this point. In the revised manuscript, we present a systematic analysis of the impact of imbalanced training data on model training (Figure 1—figure supplement 2). In addition, we have revised the description of model training and data augmentation in the Methods section (Methods, Training data and annotation).</p>
<disp-quote content-type="editor-comment">
<p>The benchmarking pipeline is indeed valuable and the results are compelling. However, the authors do not provide comparative results for miniML for real data (Figures 4-8). TL does not apply to the other methods. In my opinion, presenting the performance of other methods, trained using the smaller dataset would be convincing of the modularity and applicability of the proposed approach.</p>
</disp-quote>
<p>Quantitative comparison of synaptic detection methods on real-world data is challenging because the lack of ground-truth data prevents robust, quantitative analyses. Nevertheless, we compared miniML to common template-based and finite-threshold based methods on four different types of synapses. We noted that miniML generally detects more events, whereas other methods are susceptible to false-positives (Figure 4—figure supplement 1). In addition, we analyzed the performance of miniML on voltage imaging data (Figure 9). Simultaneous recordings of electrophysiological and imaging data allowed a quantitative comparison of detection methods in this dataset. Our results demonstrate that miniML provides higher recall for optical minis recorded using ASAP5 (Figure 9 and Figure 9—figure supplement 2; F1 score, Cohen’s d 1.35 vs. template matching and 5.1 vs. deconvolution).</p>
<disp-quote content-type="editor-comment">
<p>Impact:</p>
<p>Accurate detection of synaptic events is crucial for the study of neural function. miniML has a great potential to become a valuable tool for this purpose as it yields highly accurate detection rates, it is robust, and is relatively easily adaptable to different experimental setups.</p>
<p>Additional comments:</p>
<p>Line 73: the authors describe miniML as &quot;parameter-free&quot;. Indeed, miniML does not require the selection of pulse shape, rise/fall time, or tuning of a threshold value. Still, I would not call it &quot;parameter-free&quot; as there are many parameters to tune, starting with the number of CNNs, and number of units through the parameters of the NNs. A more accurate description would be that as an AI-based method, the parameters of miniML are learned via training rather than tuned by the user.</p>
</disp-quote>
<p>We agree that a deep learning model is not parameter-free, and this term may be misleading. We have therefore changed this sentence in the introduction as follows: &quot;The method is fast, robust to threshold choice, and generalizable across diverse data types [...]&quot;</p>
<disp-quote content-type="editor-comment">
<p>Line 302: the authors describe miniML as &quot;threshold-independent&quot;. The output trace of the model has an extremely high SNR so a threshold of 0.5 typically works. Since a threshold is needed to determine the time stamps of events, I think a better description would be &quot;robust to threshold choice&quot;.</p>
</disp-quote>
<p>To detect event localizations, a peak search is performed on the model output, which uses a minimum peak height parameter (or threshold). Extreme values for this parameter do indeed have a small impact on detection performance (Figure 3J). We have changed the description in the introduction and discussion according to the reviewer’s suggestion.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3 (Public Review):</bold></p>
<p>miniML as a novel supervised deep learning-based method for detecting and analyzing spontaneous synaptic events. The authors demonstrate the advantages of using their methods in comparison with previous approaches. The possibility to train the architecture on different tasks using transfer learning approaches is also an added value of the work. There are some technical aspects that would be worth clarifying in the manuscript:</p>
<p>(1) LSTM Layer Justification: Please provide a detailed explanation for the inclusion of the LSTM layer in the miniML architecture. What specific benefits does the LSTM layer offer in the context of synaptic event detection?</p>
</disp-quote>
<p>Our model design choice was inspired by similar approaches in the literature (Donahue et al. 2017; Islam et al. 2020; Passricha and Aggarwal 2019; Tasdelen and Sen 2021; Wang et al. 2020). Convolutional and recurrent neural networks are often combined for time-series classification problems as they allow learning spatial and temporal features, respectively. Combining the strengths of both network architectures can thus help improve the classification performance. Indeed, a CNN-LSTM architecture proved to be superior in both training accuracy and detection performance (Figure 1—figure supplement 2). Further, this architecture requires fewer free parameters than comparable model designs using fully connected layers instead. The revised manuscript shows a comparison of different model architectures (Figure 1—figure supplement 2), and we added the following description to the text (Methods, <italic>Deep learning model architecture</italic>):</p>
<p>&quot;The combination of convolutional and recurrent neural network layers helps to improve the classification performance for time-series data. In particular, LSTM layers allow learning temporal features.&quot;</p>
<disp-quote content-type="editor-comment">
<p>(2) Temporal Resolution: Can you elaborate on the reasons behind the lower temporal resolution of the output? Understanding whether this is due to specific design choices in the model, data preprocessing, or post-processing will clarify the nature of this limitation and its impact on the analysis.</p>
</disp-quote>
<p>When running inference on a continuous recording, we choose to use a sliding window approach with stride. Therefore, the model output has a lower temporal resolution than the raw data, which is determined by the stride length (i.e., how many samples to advance the sliding window). While using a stride is not required, it significantly reduces inference time (cf. Figure 2—figure supplement 1). We recommend a stride of 20 samples, which does not impact the detection of events. Any subsequent quantification of events (amplitude, area, risetimes, etc.) is performed on raw data. Based on the reviewer’s comment, we have adapted the code to resample the prediction trace to the sampling rate of the original data. This maintains temporal precision and avoids confusion.</p>
<p>The Methods now include the following statement:</p>
<p>&quot;To maintain temporal precision, the prediction trace is resampled to the sampling frequency of the raw data.&quot;</p>
<disp-quote content-type="editor-comment">
<p>(3) Architecture optimization: how was the architecture CNN+LSTM optimized in terms of a number of CNN layers and size?</p>
</disp-quote>
<p>We performed a Bayesian optimization over a defined range of hyperparameters in combination with empirical hyperparameter tuning. We now describe this in the Methods section as follows:</p>
<p>&quot;To optimise the model architecture, we performed a Bayesian optimisation of hyperparameters. Hyperparameter ranges were chosen for the free parameters of all layers. Optimisation was then performed with a maximum number of trials of 50. Models were evaluated using the validation dataset. Because higher number of free parameters tended to increase inference times, we then empirically tuned the chosen hyperparameter combination to achieve a trade-off between number of free parameters and accuracy.&quot;</p>
<disp-quote content-type="editor-comment">
<p><bold>Recommendations For The Authors</bold></p>
<p><bold>Reviewing Editor (Recommendations For The Authors):</bold></p>
<p>Overall suggestions to the authors:</p>
<p>(1) Directly compare miniML with SimplyFire (which was not cited or discussed in the original manuscript), with both idealized and actual data. Discuss the pros/cons of each software.</p>
</disp-quote>
<p>We have conducted an extensive comparison between miniML and SimplyFire using both simulated and actual experimental data. This analysis is now presented in the revised Figure 3, Figure 3—figure supplement 1, and Figure 4—figure supplement 1. In addition, we have included relevant citations for SimplyFire in our manuscript. These additions provide a more comprehensive and balanced view of the available tools in the field, positioning our work within the broader context of existing solutions.</p>
<disp-quote content-type="editor-comment">
<p>(2) Generate a better user interface akin to MiniAnalysis or SimplyFire.</p>
</disp-quote>
<p>We thank the editor and reviewers for the suggestion to improve the user interface. We have created a user-friendly graphical user interface (GUI) for miniML that is available on our GitHub repository. This GUI is now showcased in Figure 2—figure supplement 2 of the manuscript. The new interface allows users to load and analyze data through an intuitive point-and-click system, visualize results in real-time, and adjust parameters easily without coding knowledge. We have incorporated user feedback to refine the interface and improve user experience. These improvements significantly enhance the accessibility of miniML, making it more user-friendly for researchers with varying levels of programming expertise.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 1 (Recommendations For The Authors):</bold></p>
<p>Related to point (1) of the Public Review, we have taken the liberty to compare electrophysiological data using miniAnalysis, SimiplyFire, and miniML. In our comparison, we note the following in our experience:</p>
<p>(1.1) In contrast to both SimplyFire and miniAnalysis, miniML does not currently have a user-friendly interface where the user can directly control or change the parameters of interest, nor does miniML have a user control center, so the user cannot simply type or select the mini manually. Rather, if any parameter needs to be changed, the user needs to read, understand, and change the original source code to generate the preferred change. This level of &quot;activation energy&quot; and required user coding expertise in computer science, which many researchers do not have, renders miniML much less accessible when directly compared to SimplyFire and miniAnalysis. Hence, unless miniML’s interface can be made more user-friendly, this is a major disadvantage, especially when compared to SimplyFire, which has many of the same features as miniML but with a much easier interface and user controls.</p>
</disp-quote>
<p>As suggested by the reviewer, we have created a graphical user interface (GUI) for miniML. The GUI allows easy data loading, filtering, analysis, event inspection, and saving of results without the need for writing Python code. Figure 2—figure supplement 2 illustrates the typical workflow for event analysis with miniML using the GUI and a screenshot of the user interface. Code to use miniML via the GUI is now included in the project’s GitHub repository. The GUI provides a simple and intuitive way to analyze synaptic events, whereas running miniML as Python script allows for more customization and a high degree of automatization.</p>
<disp-quote content-type="editor-comment">
<p>(1.2) We compared electrophysiological miniature events between miniML, SimplyFire, and miniAnalysis. All three achieved similar mean amplitudes in &quot;wild type&quot; conditions, and conditions in which mini events were enhanced and diminished, so the overall means and utilities are similar, with miniML and SimplyFire being preferred given the flexibility and much faster analysis. We did note a few differences, however. SimplyFire tends to capture a high number of mini-events over miniML, especially in conditions of diminished mini amplitude (e.g., miniML found 76 events, while SimplyFire 587). The mean amplitudes, however, were similar. It seems that in data with low SNR, SimplyFire captures many more events as real minis that are probably noise, while miniML is more selective, which might be an advantage in miniML. That being said, we found SimplyFire to be superior in many respects, not least of which the user interface and experience.</p>
</disp-quote>
<p>We appreciate the reviewer’s thorough comparison of miniML, SimplyFire, and MiniAnalysis. While we acknowledge SimplyFire’s user-friendly interface, our study highlights several advantages of AI-based event analysis over conventional algorithmic approaches. Our updated benchmark analysis revealed better detection performance of miniML compared with SimplyFire (revised Figure 3), which had similar performance to deconvolution. As already noted by the reviewer, high false positive rates are a major issue of the SimplyFire approach. Although a minimum amplitude cutoff can partially resolve this problem, detection performance is highly sensitive to threshold setting (revised Figure 3). Another apparent disadvantage of SimplyFire is its relatively slow runtime (Figure 3—figure supplement 1). Finally, we have enhanced miniML’s accessibility by providing a graphical user interface that is easy to use and provides additional functionality.</p>
<disp-quote content-type="editor-comment">
<p>Some technical comments:</p>
<p>(1) Improvements to the dependence version of miniML: There is a need to clarify the dependence version of the python and tensor flow used in this study and in the GitHub. We used Python version 3.8.19 to load the miniML model. However, if Python versions &gt;=3.9, as described on the GitHub provided, it is difficult to have a matched h5py version installed. It is also inaccurate to say using Python &gt;=3.9, because tensor flow version for this framework needs to be around 2.13. However, if using Python &gt;=3.10, it will only allow 2.16 version tensor flow to be the download choice. Therefore, as a Python framework, the dependency version needs to be specified on GitHub to allow researchers to access the model using the entire work.</p>
</disp-quote>
<p>Thank you for highlighting this issue. We have now included specific version numbers in the requirements to avoid version conflicts and to ensure proper functioning of the code.</p>
<disp-quote content-type="editor-comment">
<p>(2) Due to the intrinsic characteristics of the trained model, every model is only suitable for analyzing data with similar attributes. It is hard for researchers without a strong computer science background to train a new model themselves for their specific data. Therefore, it would be preferred if there were more available transfer learning models on GitHub accessible for researchers to adapt to their data.</p>
</disp-quote>
<p>We would like to thank the reviewer for this feedback. Trained models (such as the default model) can often be used on different data (see, e.g., Figure 4, where data from four distinct synaptic preparations were analyzed with the base model, and Figure 5—figure supplement 1). However, changes in event waveform and/or noise characteristics may necessitate transfer learning to obtain optimal results with miniML. We have revised the description and tutorial for model training on the project’s GitHub repository to provide more guidance in this process. In addition, we now provide a tutorial on how to use existing models on out-of-sample data with distinct kinetics, using resampling. We hope these updates to the miniML GitHub repository will facilitate the use of the method.</p>
<p>Following the suggestion by the reviewer, we have provided the transfer learning models used for the manuscript on the project’s GitHub repository to increase the number of available machine learning models for event detection. In addition, users of miniML are encouraged to supply their custom models. We hope that this will facilitate model exchange between laboratories in the future.</p>
<disp-quote content-type="editor-comment">
<p><bold>Reviewer 3:</bold></p>
<p>I congratulate all authors for the convincing demonstration of their methodology, I do not have additional recommendations.</p>
</disp-quote>
<p>We would like to thank the reviewer for the positive assessment of our manuscript.</p>
<p>References</p>
<p>Delvendahl, I., Kita, K., &amp; Müller, M. (2019). Rapid and sustained homeostatic control of presynaptic exocytosis at a central synapse. <italic>Proceedings of the National Academy of Sciences</italic>, <italic>116</italic>(47), 23783–23789. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1909675116">https://doi.org/10.1073/pnas.1909675116</ext-link></p>
<p>Donahue, J., Hendricks, L. A., Rohrbach, M., Venugopalan, S., Guadarrama, S., Saenko, K., &amp; Darrell, T. (2017). Long-term recurrent convolutional networks for visual recognition and description. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic>, <italic>39</italic>(4), 677–691. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/tpami.2016.2599174">https://doi.org/10.1109/tpami.2016.2599174</ext-link></p>
<p>Drummond, C., &amp; Holte, R. C. (2003). C4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling. https: //api.semanticscholar.org/CorpusID:204083391</p>
<p>Islam, M. Z., Islam, M. M., &amp; Asraf, A. (2020). A combined deep CNN-LSTM network for the detection of novel coronavirus (COVID-19) using x-ray images. <italic>Informatics in Medicine Unlocked</italic>, <italic>20</italic>, 100412. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.imu.2020.100412">https://doi.org/10.1016/j.imu.2020.100412</ext-link></p>
<p>Passricha, V., &amp; Aggarwal, R. K. (2019). A hybrid of deep CNN and bidirectional LSTM for automatic speech recognition. <italic>Journal of Intelligent Systems</italic>, <italic>29</italic>(1), 1261–1274. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1515/jisys-2018-0372">https://doi.org/10.1515/jisys-2018-0372</ext-link></p>
<p>Prati, R. C., Batista, G. E. A. P. A., &amp; Monard, M. C. (2009). Data mining with imbalanced class distributions: Concepts and methods. <italic>Indian International Conference on Artificial Intelligence</italic>. <ext-link ext-link-type="uri" xlink:href="https://api.semanticscholar.org/CorpusID:16651273">https://api.semanticscholar.org/CorpusID:16651273</ext-link></p>
<p>Tasdelen, A., &amp; Sen, B. (2021). A hybrid CNN-LSTM model for pre-miRNA classification. <italic>Scientific Reports</italic>, <italic>11</italic>(1). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10">https://doi.org/10</ext-link>. 1038/s41598-021-93656-0</p>
<p>Tyagi, S., &amp; Mittal, S. (2020). Sampling approaches for imbalanced data classification problem in machine learning. In P. K. Singh, A. K. Kar, Y. Singh, M. H. Kolekar, &amp; S. Tanwar (Eds.), <italic>Proceedings of icric 2019</italic> (pp. 209–221). Springer International Publishing.</p>
<p>Wang, H., Zhao, J., Li, J., Tian, L., Tu, P., Cao, T., An, Y., Wang, K., &amp; Li, S. (2020). Wearable sensor-based human activity recognition using hybrid deep learning techniques. <italic>Security and Communication Networks</italic>, <italic>2020</italic>, 1–12. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2020/">https://doi.org/10.1155/2020/</ext-link> 2132138</p>
</body>
</sub-article>
</article>