<?xml version="1.0" ?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.3 20210610//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">elife</journal-id>
<journal-id journal-id-type="publisher-id">eLife</journal-id>
<journal-title-group>
<journal-title>eLife</journal-title>
</journal-title-group>
<issn publication-format="electronic" pub-type="epub">2050-084X</issn>
<publisher>
<publisher-name>eLife Sciences Publications, Ltd</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">109028</article-id>
<article-id pub-id-type="doi">10.7554/eLife.109028</article-id>
<article-id pub-id-type="doi" specific-use="version">10.7554/eLife.109028.1</article-id>
<article-version-alternatives>
<article-version article-version-type="publication-state">reviewed preprint</article-version>
<article-version article-version-type="preprint-version">1.6</article-version>
</article-version-alternatives>
<article-categories><subj-group subj-group-type="heading">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Domain-adaptive matching bridges synthetic and <italic>in vivo</italic> neural dynamics for neural circuit connectivity inference</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sheng</surname>
<given-names>Kaiwen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">*</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Shanghang</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Shenjian</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Yutao</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Beau</surname>
<given-names>Maxime</given-names>
</name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Qu</surname>
<given-names>Peng</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>Xiaofei</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Youhui</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6024-3854</contrib-id>
<name>
<surname>Ma</surname>
<given-names>Lei</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<email>lei.ma@pku.edu.cn</email>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Du</surname>
<given-names>Kai</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
<email>kai_du@tsinghua.edu.cn</email>
</contrib>
<aff id="a1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Institute for Artificial Intelligence, Peking University</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>School of Computer Science, Peking University</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016a74861</institution-id><institution>Beijing Academy of Artificial Intelligence</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cve4549</institution-id><institution>Department of Computer Science and Technology, Tsinghua University</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cve4549</institution-id><institution>Department of Psychological and Cognitive Sciences, Tsinghua University</institution></institution-wrap>, <city>Beijing</city>, <country country="CN">China</country></aff>
<aff id="a6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Wolfson Institute for Biomedical Research, University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">United Kingdom</country></aff>
</contrib-group>
<contrib-group content-type="section">
<contrib contrib-type="editor">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role>Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country country="CA">Canada</country>
</aff>
</contrib>
<contrib contrib-type="senior_editor">
<name>
<surname>Poirazi</surname>
<given-names>Panayiota</given-names>
</name>
<role>Senior Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution>
</institution-wrap>
<city>Heraklion</city>
<country country="GR">Greece</country>
</aff>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="present-address"><label>*</label><p>Present address: Department of Bioengineering, Stanford University, United States.</p></fn>
<fn fn-type="coi-statement"><p>Competing interests: No competing interests declared</p></fn>
</author-notes>
<pub-date date-type="original-publication" iso-8601-date="2026-01-22">
<day>22</day>
<month>01</month>
<year>2026</year>
</pub-date>
<volume>15</volume>
<elocation-id>RP109028</elocation-id>
<history>
<date date-type="sent-for-review" iso-8601-date="2025-09-24">
<day>24</day>
<month>09</month>
<year>2025</year>
</date>
</history>
<pub-history>
<event>
<event-desc>Preprint posted</event-desc>
<date date-type="preprint" iso-8601-date="2025-08-26">
<day>26</day>
<month>08</month>
<year>2025</year>
</date>
<self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.03.510694"/>
</event>
</pub-history>
<permissions>
<copyright-statement>© 2026, Sheng et al</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>Sheng et al</copyright-holder>
<ali:free_to_read/>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="elife-preprint-109028-v1.pdf"/>
<abstract><p>Accurately inferring neural circuit connectivity from <italic>in vivo</italic> recordings is essential for understanding the computations that support behavior and cognition. However, current deep learning approaches are limited by incomplete observability and the lack of ground-truth labels in real experiments. Consequently, models are often trained on synthetic data, which leads to the well-known “model mismatch” problem when simulated dynamics diverge from true neural activity. To overcome these challenges, we present Deep Domain-Adaptive Matching (DeepDAM), a training framework that adaptively <italic>matches</italic> synthetic and <italic>in vivo</italic> data domains for neural connectivity inference. Specifically, DeepDAM fine-tunes deep neural networks on a combined dataset of synthetic simulations and unlabeled <italic>in vivo</italic> recordings, aligning the model’s feature representations with real neural dynamics to mitigate model mismatch. We demonstrate this approach in rodent hippocampal CA1 circuits as a proof-of-concept, achieving near-perfect connectivity inference performance (Matthews correlation coefficient ∼0.97–1.0) and substantially surpassing classical methods (∼0.6–0.7). We further demonstrate robustness across multiple recording conditions within this hippocampal dataset. Additionally, to illustrate its broader applicability, we extend the framework to two distinct systems without altering the core methodology: a stomatogastric microcircuit in <italic>Cancer borealis</italic> (<italic>ex vivo</italic>) and single-neuron intracellular recordings in mouse, where DeepDAM significantly improves efficiency and accuracy over standard approaches. By effectively leveraging synthetic data for <italic>in vivo</italic> and <italic>ex vivo</italic> analysis, DeepDAM offers a generalizable strategy for overcoming model mismatch and represents a critical step towards data-driven reconstruction of functional neural circuits.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Key words</title>
<kwd><italic>in-vivo</italic> recording</kwd>
<kwd>extracellular electrophysiology</kwd>
<kwd>neural circuit</kwd>
<kwd>synaptic connectivity</kwd>
<kwd>deep learning</kwd>
<kwd>model mismatch</kwd>
<kwd>out-of-distribution (OOD)</kwd>
</kwd-group>
<custom-meta-group>
<custom-meta specific-use="meta-only">
<meta-name>publishing-route</meta-name>
<meta-value>prc</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
<notes>
<fn-group content-type="summary-of-updates">
<title>Summary of Updates:</title>
<fn fn-type="update"><p>Change title of the original manuscript</p></fn>
</fn-group>
</notes>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Neural circuit connectivity underlies information processing and transmission within the brain, fundamentally shaping network dynamics, behavior, and cognition (<xref ref-type="bibr" rid="c1">1</xref>–<xref ref-type="bibr" rid="c4">4</xref>). Therefore, accurately reconstructing this connectivity from <italic>in vivo</italic> recordings is essential for deciphering the computational mechanisms that enable brain function. However, the intrinsic complexity and heterogeneity of neural networks—characterized by diverse neuron types and intricate synaptic interactions—pose serious challenges for connectivity inference. A primary issue is that even state-of-the-art <italic>in vivo</italic> recording techniques (<xref ref-type="bibr" rid="c5">5</xref>–<xref ref-type="bibr" rid="c7">7</xref>) capture only a limited subset of neurons within a vast network, leading to incomplete or ambiguous observations. Moreover, researchers often lack detailed prior knowledge of the specific circuitry under investigation, further complicating inference.</p>
<p>One strategy to address these incomplete observations is <italic>model-based inference</italic>, which relies on a hypothesized model to simulate (or synthesize) neural activity comparable to experimental data and then infers the connections that best explain the observations (<xref ref-type="bibr" rid="c8">8</xref>). Traditionally, unsupervised model-based methods such as the inverse Ising model (<xref ref-type="bibr" rid="c9">9</xref>) and Generalized Linear Models (GLMs) (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>) have been widely used for this purpose. These approaches can be effective in simpler neural circuits (for example, in early sensory areas); however, they struggle in brain regions with dense recurrent connectivity. In fact, Das and Fiete (<xref ref-type="bibr" rid="c8">8</xref>) showed using synthetic benchmarks that such methods face an inevitable limitation in complex networks with strong recurrent loops. This shortcoming—referred to as the <italic>“model mismatch”</italic> problem—arises when the simplistic or assumed dynamics of the computational model diverge from the true neural dynamics, causing inference errors.</p>
<p>In recent years, deep learning has emerged as a powerful alternative for neural connectivity inference, owing to its capacity to capture complex patterns in large datasets. Deep learning methods typically train a deep neural network (DNN) entirely on synthetic data generated from a chosen biophysical model (<xref ref-type="bibr" rid="c12">12</xref>), thereby sidestepping the need for exhaustive ground-truth labels (which are practically unattainable for <italic>in vivo</italic> recordings). Indeed, our preliminary investigations indicated that a DNN with a ResNet-LSTM architecture outperforms traditional model-based approaches on purely synthetic datasets (<xref ref-type="bibr" rid="c13">13</xref>). However, when we applied such a model—trained on synthetic data—to real <italic>in vivo</italic> recordings, its performance degraded markedly. This gap in performance prompted us to scrutinize the root cause: the disparity between synthetic training data and real neural data.</p>
<p>In this work, we identified a substantial difference between the feature distributions of synthetic spike data and those of real <italic>in vivo</italic> recordings, underscoring the severity of the model mismatch problem. This insight led us to develop Deep Domain-Adaptive Matching (DeepDAM) — a model-based deep learning framework tailored for neural connectivity inference that leverages both unlabeled <italic>in vivo</italic> spike trains and synthetic neural data. The key innovation of DeepDAM is to reframe the classical model mismatch issue as a data distribution mismatch, essentially treating it as an out-of-distribution (OOD) challenge. While domain adaptation for OOD data is well-established in machine learning (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>), to our knowledge it has not been applied in this particular context within computational neuroscience.</p>
<p>The DeepDAM framework operates in two stages: first by employing domain adaptation to align the DNN’s feature space with the dynamics of real neural data (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>), and then by using self-training to iteratively refine the network on unlabeled <italic>in vivo</italic> data (<xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c20">20</xref>). This two-phase strategy mitigates discrepancies between the synthetic and real data domains, thereby improving inference accuracy when the model is applied to real experiments. Since <italic>in vivo</italic> datasets with definitive connectivity labels are exceedingly scarce, validating our approach on a rigorously labeled <italic>in vivo</italic> dataset is a notable aspect of this study. To our best knowledge, we are the first to leverage this particular labeled dataset for model validation, and among the first to explicitly address model mismatch from an OOD perspective in neural circuit inference.</p>
<p>Using this unique dataset, we demonstrate that DeepDAM can accurately infer monosynaptic connections from multi-electrode extracellular recordings in the mouse hippocampal CA1 region. Notably, CA1 is a challenging testbed due to limited prior circuit knowledge and strongly recurrent connections that introduce additional confounds (<xref ref-type="bibr" rid="c8">8</xref>) (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>). By coupling a ResNet-LSTM architecture with our domain-adaptive training strategy, the framework achieved near-perfect connectivity inference performance, with Matthews correlation coefficient (MCC) values around 0.97–1.0. This is a dramatic improvement over popular model-based inference methods (such as GLMs and other statistical models) (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>), which typically reach only ∼0.6–0.7 on the same task. Moreover, DeepDAM maintained high accuracy across multiple recording sessions and experimental conditions, each presenting varying levels of data ambiguity. This robustness under different noise levels and experimental perturbations highlights the framework’s reliability and adaptive capacity in realistic, less-controlled scenarios.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 1.</label>
<caption><title>DeepDAM inference framework for monosynaptic connectivity inference in CA1 networks.</title>
<p>(<bold>a</bold>) Challenges associated with inferring connectivity, including incomplete observations and strong recurrent connections. (<bold>b</bold>) DeepDAM framework. The gray boxes outlined the five major steps of the framework. Note that both the feature extractor and the classifier are shared between synthetic and experimental data. (<bold>c</bold>) Distributional discrepancy of experimental and synthetic data caused by model mismatch in the model construction step. (<bold>d</bold>) Synthetic and experimental feature distributions after training the traitional model-based inference framework. (<bold>e</bold>) Synthetic and experimental feature distributions after training our DeepDAM framework.</p></caption>
<graphic xlink:href="510694v6_fig1.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Because the hippocampal dataset is, to our knowledge, the only <italic>in vivo</italic> resource with both spike recordings and ground-truth connectivity, we also explored DeepDAM’s generalization to other neural inference problems across different species and scales. In these proof-of-concept explorations, we applied the framework to classic system identification tasks ranging from single neurons to small circuits. For example, DeepDAM successfully recovered biophysical parameters from large-scale intracellular recordings of individual mouse neurons (<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>), and it identified synaptic connectivity in a well-characterized microcircuit (the stomatogastric ganglion of <italic>Cancer borealis</italic>). In each case, our approach outperformed standard model-fitting techniques (<xref ref-type="bibr" rid="c23">23</xref>) in both accuracy and efficiency, suggesting that the underlying domain-adaptive strategy has broad potential applicability beyond the initial hippocampal circuit demonstration.</p>
<p>In summary, DeepDAM offers a new paradigm for neural circuit connectivity inference by seamlessly integrating synthetic model data with real <italic>in vivo</italic> data through domain adaptation. Rather than a one-off use of domain adaptation or self-training, it provides a general framework that mitigates model mismatch and extends the power of computational models to more faithfully capture real neural dynamics. We emphasize that our study is presented as a proof-of-concept in rodent hippocampal circuits. Nonetheless, this demonstration illustrates the potential of DeepDAM to be extended to other brain regions and species in future work. By addressing the long-standing model mismatch problem, our approach paves the way for more accurate data-driven reconstructions of neural circuits. Ultimately, we envision that such domain-adaptive techniques will contribute to the ambitious goal of reconstructing mammalian brain connectivity from large-scale <italic>in vivo</italic> data.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Addressing model mismatch in neural connectivity inference as an out-of-distribution problem</title>
<p>There are two primary challenges arising in the efforts of inferring monosynaptic connectivity from <italic>in vivo</italic> spike trains: (<xref ref-type="bibr" rid="c1">1</xref>) Incomplete observations, where the interaction between the recorded and unrecorded neurons can falsely enhance spike-timing correlations, leading to potential misinterpretations (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>); and (<xref ref-type="bibr" rid="c2">2</xref>) Unknown network complexity, where computational models cannot fully cover the intricate nature of brain networks, such as cell-type-specific biophysics and recurrent connections. These challenges manifest themselves as significant ‘model mismatch’, i.e., discrepancies between the response of the computational model and the actual dynamics of the neural network (as shown in <xref rid="fig1" ref-type="fig">Fig. 1c</xref>). We further highlight this mismatch through t-distributed stochastic neighbor embedding (t-SNE) visualizations (<xref ref-type="bibr" rid="c24">24</xref>), revealing a clear misalignment between model-generated cross-correlograms (CCGs) from synthetic data and those directly obtained from the extracellular multi-electrode recordings in CA1 (<xref ref-type="bibr" rid="c25">25</xref>) when displayed in a two-dimensional space (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>).</p>
<p>The basic idea of solving the model mismatch problem is to consider it as an OOD problem in machine learning, i.e., the performance is significantly degraded due to the mismatch in distribution between the model-generated dynamics and the actual neural dynamics. Domain adaptation techniques are commonly used to deal with the OOD problem by effectively bridging the gap between different data distributions (<xref ref-type="bibr" rid="c26">26</xref>). Based on recent advances in unsupervised domain adaptation techniques, instead of narrowing such gap in the original data space, we focus on the shared and informative features of synthetic and experimental data (i.e., feature space) (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>). Accordingly, we developed a comprehensive, DNN-based framework for inferring monosynaptic connectivity from unlabeled <italic>in vivo</italic> extracellular recordings:
<list list-type="order">
<list-item><p><bold>Model formulation</bold>: A multiple-timescale adaptive threshold (MAT) network (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>), is formulated to mimic the dynamics of the underlying neural network. This MAT network is a recurrent neural network with excitatory and inhibitory synaptic weights that follow lognormal distributions (<xref ref-type="supplementary-material" rid="supp1">Fig. S1b</xref>) to generate firing rate distributions similar to hippocampal neural recordings (<xref ref-type="supplementary-material" rid="supp1">Fig. S1a</xref>) (<xref ref-type="bibr" rid="c12">12</xref>).</p></list-item>
<list-item><p><bold>Training dataset synthetization</bold>: Exploiting the transparency of this computational model, a synthetic dataset is produced, providing ground-truth labels for cross-correlograms (CCGs) of neuron pairs (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). These CCGs, a series of time-lagged correlations between two spike trains, serve as crucial tools for understanding neural connectivity patterns (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>).</p></list-item>
<list-item><p><bold>Feature alignment</bold>: Features that are shared between the synthetic CCGs and experimental CCGs and informative of the underlying connectivity are extracted from DNN-based feature extractors using domain adaptation (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>).</p></list-item>
<list-item><p><bold>Classifier construction</bold>: The DNN’s classifier is first trained on labeled synthetic data via supervised learning. This step complements feature alignment by making sure that the features are informative about neural connectivity (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>).</p></list-item>
<list-item><p><bold>Classifier refinement</bold>: To address potential overfitting of the DNN’s classifier to synthetic data, we refine the classifier through self-training using unlabeled experimental data. This is achieved by assigning pseudo labels to the experimental data (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>).</p></list-item>
</list>
Our primary contribution to this field is the transformation of the conventional three-step model-based inference method (comprising steps 1, 2, and 4) into a five-step, DNN-based framework (‘DeepDAM’). Traditional inference methods, lacking feature alignment and classifier refinement steps, are prone to training biases, as both the feature extractor and classifier are trained exclusively on labeled synthetic data, ignoring the experimental dataset. This results in the classifier overfitting to synthetic features and skewing inferences when applied to experimental data (<xref rid="fig1" ref-type="fig">Fig. 1d</xref>, <xref ref-type="supplementary-material" rid="supp1">Fig. S2a &amp; b</xref>). In contrast, our construction of DeepDAM, which includes Feature alignment and Classifier refinement, can effectively address these shortcomings, thereby minimizing the gap in feature space, preventing the classifiers from overfitting the synthetic data, and ensuring more accurate inferences on experimental data (<xref rid="fig1" ref-type="fig">Fig. 1e</xref> &amp; <xref ref-type="supplementary-material" rid="supp1">Fig. S2c</xref>).</p>
<p>It is noteworthy that, unlike traditional model-fitting methods, which typically focus on adjusting model parameters to fit data, our approach emphasizes ‘model-generation’, where the MAT model is used as a generative tool to create synthetic training datasets without fine-tuning the MAT model parameters. This shifts from direct model optimization to ‘model-generation’ is crucial, as fine-tuning the biophysical network model is often impractical due to the complexities of the underlying biological mechanisms and the extensive number of model parameters (see Discussions).</p>
<p>In the following sections, we will first elaborate on key design elements of the DeepDAM framework’s implementation, highlighting its inherent flexibility for diverse applications. Next, we will further validate the performance and robustness of the DeepDAM in comparison to traditional inference methods, using real <italic>in vivo</italic> spike data for inferring monosynaptic connectivity. Finally, we will illustrate the versatility of DeepDAM framework by extending its application to traditional model-fitting tasks, such as inferring biophysical properties of large-scale single neurons from <italic>in vitro</italic> and a microcircuit from <italic>ex vivo</italic> data.</p>
</sec>
<sec id="s2b">
<title>Key implementations of DeepDAM</title>
<p>In our DeepDAM framework, we strategically incorporate and customize two core machine learning strategies—domain adaptation and self-training—to address the OOD challenges inherent in inferring monosynaptic connectivity from <italic>in vivo</italic> recordings. This fusion involves several critical designs, which we will elaborate on below.</p>
<p>The first key design element of DeepDAM is the formulation of composite loss functions for the DNN. In addition to the standard loss function for assessing regular connectivity accuracy, we introduce a composite loss function <italic>L</italic> comprising the domain adaptation loss (<italic>L<sub>DA</sub></italic>) and the self-training loss (<italic>L<sub>ST</sub></italic>) given by <xref rid="eqn1" ref-type="disp-formula">Equation 1</xref>. Here, <italic>L<sub>DA</sub></italic> facilitates Feature Alignment, while <italic>L<sub>ST</sub></italic> is crucial for Classifier Refinement.
<disp-formula id="eqn1">
<graphic xlink:href="510694v6_eqn1.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
The domain adaptation loss in our DeepDAM framework is derived from unsupervised domain adaptation techniques (<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>) to reduce the distributional discrepancy between synthetic data (<italic>x<sub>syn</sub></italic>) and unlabeled experimental data (<italic>x<sub>exp</sub></italic>) in the feature space, as indicated in <xref rid="eqn2" ref-type="disp-formula">Equation 2</xref> and <xref rid="eqn3" ref-type="disp-formula">3</xref>:
<disp-formula id="eqn2">
<graphic xlink:href="510694v6_eqn2.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn3">
<graphic xlink:href="510694v6_eqn3.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula id="inline-eqn-1"><inline-graphic xlink:href="510694v6_inline1.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the cross-entropy loss function <inline-formula id="inline-eqn-2"><inline-graphic xlink:href="510694v6_inline2.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the label for <inline-formula id="inline-eqn-3"><inline-graphic xlink:href="510694v6_inline3.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the classifier and <italic>λ<sub>dis</sub></italic> is the scaling factor of the discrepancy loss. Next, the synthetic and experimental features are categorized into clusters through the spherical K-means clustering algorithm, based on synthetic labels and pairwise feature similarity (Methods) (<xref ref-type="bibr" rid="c14">14</xref>). Our aim is to strategically minimize the distance <italic>L<sub>dis</sub></italic> between features within the same cluster and maximize it between different clusters (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>; Methods). At last, the feature extractor <italic>f</italic> and the classifier <italic>g</italic> are co-trained via the DNN to ensure high performance on labeled synthetic data. Overall, the domain adaptation loss (<italic>L<sub>DA</sub></italic>) facilitates the transfer of this high performance from synthetic to experimental data.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 2.</label>
<caption><title>DeepDAM framework architecture.</title>
<p>(<bold>a</bold>) Workflow of the DeepDAM framework. The temporal feature extractor and regressor/classifier are shared between synthetic data and experimental data. Firstly, temporal features of synthetic data and experimental data are extracted by the shared temporal feature extractor. The output space from the temporal feature extractor is the feature space. Secondly, the shared regressor or classifier processes the temporal features and outputs the inferred neural properties. Sup. learning stands for supervised learning. (<bold>b</bold>) The training workflow of our inference framework. Each dot or cross represented a sample feature. <bold>(c)</bold> Pseudo code. The equation IDs correspond to the IDs in the main text and <bold>d</bold>. <bold>(d)</bold> The function of each unified backbone equation.</p></caption>
<graphic xlink:href="510694v6_fig2.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>However, even with domain adaptation, the classifier is not optimized on experimental features and the feature alignment process cannot perfectly align these two feature distributions, potentially making the classifier overfit on synthetic features. Therefore, we integrated self-training to explicitly refine the classifier to the experimental features (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>) (<xref ref-type="bibr" rid="c16">16</xref>–<xref ref-type="bibr" rid="c19">19</xref>). Self-training applies “supervised learning” on unlabeled experimental data by tagging pseudo-labels to them, which are given by the synthetic data labels within the same cluster (<xref rid="fig2" ref-type="fig">Fig. 2b</xref> &amp; <xref rid="fig2" ref-type="fig">d</xref>; <xref rid="eqn4" ref-type="disp-formula">Equation 4</xref>).
<disp-formula id="eqn4">
<graphic xlink:href="510694v6_eqn4.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula id="inline-eqn-4"><inline-graphic xlink:href="510694v6_inline4.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the pseudo label of experimental datum <inline-formula id="inline-eqn-5"><inline-graphic xlink:href="510694v6_inline5.gif" mime-subtype="gif" mimetype="image"/></inline-formula> and <italic>G</italic> (·) is a binary function for gating <inline-formula id="inline-eqn-6"><inline-graphic xlink:href="510694v6_inline6.gif" mime-subtype="gif" mimetype="image"/></inline-formula> to the self-training process.</p>
<p>A second key design element in our DeepDAM framework is the introduction of a gating mechanism. At the beginning of the training process, the features extracted and the pseudo-labels assigned may be noisy and inaccurate. To address this, we introduce a gating mechanism that filters out experimental data if their distances to the designated cluster center exceed a predefined threshold. This filtration occurs during the domain adaptation and self-training phases of the current training iteration (as illustrated in <xref rid="fig2" ref-type="fig">Fig. 2a</xref>; <italic>G<sub>ST</sub></italic> (·) in <xref rid="eqn4" ref-type="disp-formula">Equation 4</xref> and <italic>G<sub>DA</sub></italic> (·) in <xref rid="eqn7" ref-type="disp-formula">Equation 7</xref>), which ensures that only the most relevant and reliable data contribute to the model training at each step. The above steps, including the gating mechanism, are repeated iteratively until the model converges. In each training step, backpropagation (<xref ref-type="bibr" rid="c31">31</xref>) is employed to refine the parameters of both the feature extractor and the classifier, guided by the composite loss function described earlier (Methods and SI Appendix).</p>
<p>For clarity and reference, the pseudo code encapsulating the entire training process of our framework is concisely summarized in <xref rid="fig2" ref-type="fig">Fig. 2c</xref>.</p>
<p>At last, it should be noted that we utilized all available experimental data for domain adaptation and self-training rather than dividing it into validation and testing sets, but we should also emphasize that none of the ground-truth were used throughout the training process. Our primary concern for such design is to reflect the real-world conditions of neuroscience research, where the goal is to infer connectivity from all available data without relying on ground-truth labels. Nevertheless, we have also conducted cross-animal validation experiments to evaluate our model’s generalization capabilities (see Discussions for details).</p>
</sec>
<sec id="s2c">
<title>Visualization of feature space evolution during training</title>
<p>Next, to gain insights into the underlying mechanisms of our DeepDAM framework, we visualized the evolution of the feature space during the training process, comparing to the conventional deep learning approach lacking domain adaptation and self-training (i.e. <italic>vanilla</italic> deep learning method). The synthetic dataset was generated by the MAT neural network and the experimental data were a subset of extracellular recordings from the CA1 region of freely-running mice with ground-truth labels (<xref ref-type="bibr" rid="c25">25</xref>) (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>; SI Appendix). After training, both methods were capable of accurately distinguishing between synthetic connection and non-connection data (<xref rid="fig3" ref-type="fig">Fig. 3b</xref> bottom right) but only DeepDAM can successfully classify experimental data.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 3.</label>
<caption><title>Training workflow of our inference framework.</title>
<p>(<bold>a</bold>) The experimental data and computational model for inferring the existence of monosynaptic connectivity between pairs of neurons. (<bold>b</bold>) Evolution of feature representations across the training process for both our inference method and the vanilla deep learning method without self-training and domain adaptation. The same fifty randomly selected synthetic connection and non-connection samples are shown in each subplot. (<bold>c</bold>) The evolution of distance matrix of synthetic and experimental features across the training process for both our inference method and the vanilla deep learning method without self-training and domain adaptation. Distance between two groups was calculated as the average Euclidean distance between each pair of features.</p></caption>
<graphic xlink:href="510694v6_fig3.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>For the vanilla deep learning method, the experimental data were inappropriately positioned between the synthetic data labeled as ‘connection’ and ‘non-connection’ in the feature space (<xref rid="fig3" ref-type="fig">Fig. 3b</xref> bottom right). This misplacement was due to the persistent distributional discrepancy between synthetic and experimental data in the input space, a consequence of model mismatch. As a result, the classifier tended to overfit to the synthetic data, leading to inaccurate classification of the experimental features.</p>
<p>In stark contrast, our method effectively utilized domain adaptation to cluster synthetic and experimental data of the same class together in the feature space (<xref rid="fig3" ref-type="fig">Fig. 3b</xref> top right). This strategic grouping allowed the classifier, which was trained on synthetic data via supervised learning and on experimental data via self-training, to accurately classify both datasets. As the training proceeds, the pseudo labels for self-training and overall prediction performance gradually stabilized (<xref ref-type="supplementary-material" rid="supp1">Fig. S3</xref>). To quantitatively evaluate the discrepancy between synthetic and experimental feature distributions, we computed the distance between these distributions (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>). The results revealed that our approach successfully differentiated connection and non-connection data in the feature space, enhancing the classifier’s ability to distinguish them accurately (<xref rid="fig3" ref-type="fig">Fig. 3c</xref> top). On the contrary, the vanilla deep learning method struggled to differentiate between synthetic connection data and experimental non-connection data, and between experimental non-connection and connection data, due to model mismatch and the classifier’s overfitting to synthetic data (<xref rid="fig3" ref-type="fig">Fig. 3c</xref> bottom).</p>
<p>To conclude, our DeepDAM framework offers an effective solution to the model mismatch challenge, ensuring the transfer of precise neural connectivity inference from synthetic data to experimental data.</p>
</sec>
<sec id="s2d">
<title>Evaluation of DeepDAM framework’s performance on <italic>in vivo</italic> dataset</title>
<p>In this section, we systematically evaluated the performance of our framework on real <italic>in vivo</italic> spike data with ground-truth labels, comparing with existing popular methods. Specifically, we benchmarked against three methods: the vanilla deep learning approach, CoNNECT (<xref ref-type="bibr" rid="c12">12</xref>), and GLMCC (<xref ref-type="bibr" rid="c10">10</xref>).</p>
<p>Briefly, CoNNECT is essentially a deep learning method without domain adaptation and self-training, similar to the vanilla deep learning method but with a normal convolution neural network architecture. The authors of CoNNECT provided a DNN model trained solely on synthetic data generated from the same MAT network; we directly utilized their published model for performance comparison (<xref ref-type="bibr" rid="c12">12</xref>). GLMCC, in contrast, employed a Generalized Linear Model to fit CCGs based on synthetic data, which was then applied to rat hippocampal data. In our evaluation, we adopted their model with the original hyperparameters (<xref ref-type="bibr" rid="c10">10</xref>), as these were specifically tuned for hippocampal data, ensuring a fair comparison (SI Appendix).</p>
<p>It is essential to highlight the fundamental challenge in neuroscience research: the scarcity of <italic>in vivo</italic> datasets with reliable ground-truth connectivity labels. Direct measurement of synaptic connections in living brains is extraordinarily difficult with current technologies, making ground-truth connectivity largely unknown in most cases. This unique <italic>in vivo</italic> dataset is, to our knowledge, one of the few available with “inferred labels” provided by the dataset creators (<xref ref-type="bibr" rid="c25">25</xref>).</p>
<p>Specifically, the dataset’s labels were established using two distinct labeling methods, resulting in two subsets: (<xref ref-type="bibr" rid="c1">1</xref>) a smaller subset with reliable labels, validated by the consistency between the two labeling methods. This subset allows us to rigorously test the efficacy of our method on real <italic>in vivo</italic> data, providing a dependable benchmark for our techniques; (<xref ref-type="bibr" rid="c2">2</xref>) a larger subset with less reliable labels, characterized by inconsistencies between the labeling methods. This subset represents a more common scenario in real-world research, where data ambiguity is prevalent.</p>
<p>Our performance evaluation was based on the smaller subset with reliable labels (<xref ref-type="bibr" rid="c25">25</xref>). It is important to emphasize that we did not use any ground-truth labels for training our model; these labels were solely employed to assess the model’s prediction accuracy on the experimental data.</p>
<p>We chose MCC as our performance metric, as it is consistent with previous literature (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>) and also suitable for imbalanced classification problems (<xref ref-type="bibr" rid="c32">32</xref>). The results showed that these three existing methods exhibited noticeable limitations. Predominantly, the vanilla deep learning method tended towards false positives, while both GLMCC and CoNNECT generated a mix of false positives and negatives (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). In contrast, our method overcame these limitations, achieving an MCC of 0.91 (SD=0.08), in stark contrast to vanilla deep learning (0.61, SD=0.15), GLMCC (0.62), and CoNNECT (0.58).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 4.</label>
<caption><title>Performance comparison on experimental dataset with ground-truth labels.</title>
<p>(<bold>a</bold>) Example experimental CCGs and predictions by different methods. Ours: our aggregated version. V-DNN: the vanilla deep learning method. V-DNN-Agg: the aggregated vanilla deep learning method. CCGs were plotted from [-20, 20] ms based on the neuron spike timing with the time bin of 0.2 ms. (<bold>b</bold>) Performance comparison between 6 different methods. Error bar: standard deviation across 10 random seeds.</p></caption>
<graphic xlink:href="510694v6_fig4.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Interestingly, we observed some performance fluctuations across different random seeds during the training process (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>). To address this variability and ensure the robustness of our method, we introduced an aggregated version of our method, where we trained the same framework across various random initializations, generating a unique inference prediction for each experimental data point with each random seed. Subsequently, we employed a majority vote algorithm to aggregate the results from all random seeds. Remarkably, our aggregated method successfully eliminated noisy inference results and achieved an outstanding MCC of 0.97 (SD=0.04; <xref rid="fig4" ref-type="fig">Fig. 4b</xref>; Methods). Our aggregated method is fairly robust which achieved an MCC of 0.91-0.98 across a wide range of hyperparameters and such high performance can only be achieved with both self-training and domain adaptation (<xref ref-type="supplementary-material" rid="supp1">Fig. S6</xref>). Similarly, the hyperparameter combination we selected was also based on the consistency across random seeds (SI Appendix).</p>
</sec>
<sec id="s2e">
<title>Evaluation of DeepDAM framework’s robustness on <italic>in vivo</italic> dataset</title>
<p>A hallmark of a robust inference algorithm is its consistent performance across varied experimental scenarios. For example, the length of experimental recordings has been shown to significantly influence inference accuracy (<xref ref-type="bibr" rid="c10">10</xref>). Therefore, we assessed our method’s resilience by subsampling experimental recordings to varying lengths. As recording length increased, our method’s performance consistently improved (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). This is likely due to longer recordings capturing more nuanced neuron interactions, whereas shorter lengths can obscure critical temporal patterns (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>). Impressively, even when restricted to 20-minute recordings, our method surpassed other frameworks tested on full-length recordings (0.15-0.27 higher MCC score) (<xref rid="fig4" ref-type="fig">Fig. 4b</xref> &amp; <xref rid="fig5" ref-type="fig">Fig. 5b</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 5.</label>
<caption><title>Performance stability across different experimental conditions and computational model structures.</title>
<p>(<bold>a</bold>) Experimental CCG of the same pair of neurons at different recording lengths. (<bold>b</bold>) MCC, false positives, false negatives of our aggregated method at different recording lengths. Error bar: standard deviation across 10 random seeds. (<bold>c</bold>) Synthetic CCGs generated from MAT neural networks of different sizes. (<bold>d</bold>) MCC, false positives, false negatives of our aggregated method at different sizes of MAT neural networks. Error bar: standard deviation across 10 random seeds.</p></caption>
<graphic xlink:href="510694v6_fig5.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Furthermore, considering the variability in computational models employed in different studies, we tested our method’s robustness by training it using three MAT neural networks of different sizes (<italic>N<sub>neuron</sub></italic> = 250, 500, 1000, respectively). The consistent outcome was a robust MCC of 0.99 for <italic>N<sub>neuron</sub></italic> = 250 (SD=0.02), 0.97 for <italic>N<sub>neuron</sub></italic> = 500 (SD=0.03), 0.97 for <italic>N<sub>neuron</sub></italic> = 1000 (SD=0.03), across all models (<xref rid="fig5" ref-type="fig">Fig. 5d</xref>), highlighting our framework’s adaptability across diverse research scenarios. Interestingly, the highest performance, though marginally, was achieved at <italic>N<sub>neuron</sub></italic> = 250, suggesting that this size of network model might best mimic the dynamics of the recorded neural ensemble.</p>
<p>We conducted a detailed analysis of the failure cases of all tested methods. Notably, our techniques only resulted in false positives, which were potentially caused by three different reasons. The first reason is the covariation between pairs of neurons (blue examples in <xref ref-type="supplementary-material" rid="supp1">Fig. S4</xref>), where the center peak starts before 0s and the amplitude of the peak is not significantly larger than the baseline fluctuation. Secondly, insufficient number of spikes can lead to false negatives (purple examples in <xref ref-type="supplementary-material" rid="supp1">Fig. S4</xref>). The third reason is that the CCGs for these cases exhibited small peaks just after zero lag (orange examples in <xref ref-type="supplementary-material" rid="supp1">Fig. S4</xref>), which likely led DeepDAM to misclassify them as connections. These concerned CCGs displayed slight peaks after zero, likely causing DeepDAM to incorrectly classify them (<xref ref-type="supplementary-material" rid="supp1">Fig. S4</xref>). Furthermore, these CCGs generally had fewer spikes compared to the clearer CCGs in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, supporting our observation that longer recording durations tend to enhance performance. We highlighted that the aggregated DeepDAM only failed four CCGs in all 100 aggregations, demonstrating the robustness of our approach. Conversely, other methods displayed a mix of false positives and false negatives. For the false positives of all other methods, some of them are also due to the lack of spikes (purple examples in <xref ref-type="supplementary-material" rid="supp1">Fig. S4</xref>). While for other mistakes, interestingly, they are largely affected by covariations between pairs of neurons, which widely exist in the neural system, such neural oscillations and common inputs. Strikingly, all other methods made false negatives even if there is a very clear peak after 0s in the CCGs (right two columns from 3<sup>rd</sup> to 6<sup>th</sup> rows of <xref ref-type="supplementary-material" rid="supp1">Fig. S4</xref>). This suggests that methods developed solely on synthetic data have limited generalization capabilities on real experimental data.</p>
<p>To conclude, our DeepDAM framework, which synergizes domain adaptation and self-training, effectively mitigates the model mismatch problem and delivers outstanding performance in inferring monosynaptic connectivity from <italic>in vivo</italic> spike trains. Its demonstrated robustness, evident in handling varying recording lengths and accommodating different model structures, establishes the framework as a versatile and powerful tool, offering significant potential for broader applications in the field.</p>
</sec>
<sec id="s2f">
<title>Application to a broader <italic>in vivo</italic> spike dataset with low signal-to-noise ratios</title>
<p>As we transition our method to real-world scenarios, its capability to effectively handle uncertain or noisy data becomes increasingly important. In the previous section, our framework was tested on the subset from English et al. (<xref ref-type="bibr" rid="c25">25</xref>), where ground-truth labels were consistently established by two labeling methods, suggesting that the CCGs from this subset typically exhibit high signal-to-noise ratios. To further evaluate the robustness of our framework, we expanded our analysis to include a broader set of <italic>in vivo</italic> spike data characterized by lower signal-to-noise ratios. This larger dataset, also sourced from English et al.’s study (<xref ref-type="bibr" rid="c25">25</xref>), contained CCGs with inconsistent or uncertain ground-truth labels according to the two tagging methods, which is a scenario likely more common in <italic>in vivo</italic> experiments (<xref rid="fig6" ref-type="fig">Fig. 6a</xref>; SI Appendix).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Fig. 6.</label>
<caption><title>Consistency between the prediction of our aggregated method and two label methods.</title>
<p>(<bold>a</bold>) All CCGs with evoked spikes in the large dataset. For labeling, the CCGs were calculated based on ‘presynaptic’ spikes that were evoked via juxtacellular stimulation. Spontaneous spikes of the ‘presynaptic’ neuron were discarded. The label is given by label method 1. Color bar denotes the spike count, normalized by the maximal value of all CCGs. (<bold>b</bold>) Illustration of two label methods. The green line represents the slowly co-modulated baseline (SI Appendix). Briefly, label method 1 tests the lowest p-value of time bins in both directions (<italic>p</italic><sub>causal</sub> and <italic>p</italic><sub>fast</sub>) while label method 2 tests the p-value of three consecutive time bins in one direction (SI Appendix). (<bold>c</bold>) Experimental CCGs and predictions of our aggregated method and two label methods. For inference, CCGs were calculated using spontaneous spikes of the ‘presynaptic’ neuron, while evoked spikes and the corresponding stimulus window was discarded from the CCGs. The numbers in the parentheses indicate the count of random seeds out of 10 that resulted in the same inference outcome. (<bold>d</bold>) MCC between our aggregated method and two label methods on the amibigous dataset. (<bold>e</bold>) Consistency matrix between our aggregated method and two ground-truth tagging methods on the amibigous dataset. For example, the upper-left grid in the left panel represented the percentage of experimental CCGs in the dataset that both our aggregated method and label method 1 inferred as non-connections.</p></caption>
<graphic xlink:href="510694v6_fig6.tif" mime-subtype="tiff" mimetype="image"/>
</fig>
<p>Interestingly, in the enlarged experimental dataset, our aggregated method demonstrated greater consistency with the second ground-truth tagging method, which tended to identify connections more frequently than the first method (<xref rid="fig6" ref-type="fig">Fig. 6b</xref> &amp; <xref rid="fig6" ref-type="fig">c</xref>; Discussions). Yet, discrepancies still existed between our method and both tagging methods, requiring further investigation to determine the most effective approach. Our previous evaluations suggest that increasing the recording time, reducing CCG noise and enhancing prediction reliability could be a viable solution (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>). Additionally, we encountered cases where neither tagging method could definitively classify certain CCGs as connections or non-connections due to insignificant statistical tests. Our inference method also exhibited uncertainty in these cases (<xref rid="fig6" ref-type="fig">Fig. 6a</xref> bottom row), primarily due to these CCGs having an insufficient number of spikes for the corresponding neuron pairs (<xref rid="fig6" ref-type="fig">Fig. 6a</xref> bottom row), underscoring the need for longer recording times for neurons with lower firing rates to yield convincing inferences (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>) (<xref ref-type="bibr" rid="c10">10</xref>).</p>
<p>In conclusion, our findings emphasize the need for comprehensive evaluations using large-scale datasets with accurate ground truths to refine and enhance connectivity inference methods. Improving the ground-truth labeling process and expanding the dataset size are essential steps to enhance the robustness of the inference methods.</p>
</sec>
<sec id="s2g">
<title>Versatility in inferring biophysical properties across neural scales</title>
<p><italic>In vivo</italic> datasets that pair spike trains with ground-truth synaptic connectivity are extremely scarce, limiting our ability to directly validate connectivity inference methods. In fact, to our best knowledge, only a single <italic>in vivo</italic> spike recording dataset with known synaptic connections (from mouse hippocampal CA1) is available, which makes comprehensive performance evaluation challenging. To address this and demonstrate DeepDAM’s effectiveness on broad real experimental data, we turn to <italic>ex vivo</italic> and <italic>in vitro</italic> preparations as alternative validation platforms. In this final section, we showcase DeepDAM’s adaptability under broader model-mismatch conditions across different recording modalities, species, and observation scales, by applying it to traditional model-fitting tasks. This indirect evaluation strategy, using controlled laboratory datasets, supports the generalizability of our approach despite the paucity of fully-labeled <italic>in vivo</italic> benchmarks.</p>
<p>Based on the foundational <xref rid="eqn1" ref-type="disp-formula">Equations 1</xref>–<xref rid="eqn4" ref-type="disp-formula">4</xref> of our framework, we can extend DeepDAM beyond connectivity mapping to infer detailed biophysical properties of single neurons and microcircuits. The key modification required is a subtle adjustment of the discrepancy loss function <italic>L<sub>dis</sub></italic>, (as described SI Appendix) to target specific observable features. Unlike the model-generation paradigm used earlier for connectivity inference, where synthetic data are generated from a fixed model, here we employ a model-fitting paradigm. In these tasks, model parameters are iteratively optimized so that the model’s responses closely align with experimental observations in terms of the chosen biophysical features (SI Appendix). This shift from data generation to model fitting illustrates DeepDAM’s versatility: with minimal changes to the loss function, the same framework can capture neuron-level properties (e.g. membrane dynamics) or circuit-level behaviors from experimental recordings, further demonstrating its robustness across neural scales.</p>
<p>As for the single-neuron inference, our framework inferred biophysical properties of single neurons of mice using intracellular recordings <italic>in vitro</italic>, including passive membrane dynamics, background noise magnitude, and attributes of ion channels. Such properties align with the parameters set by the single-compartment Hodgkin-Huxley model, known for capturing the vast dynamics of cortical neurons (<xref ref-type="supplementary-material" rid="supp1">Fig. S8a</xref>; SI Appendix) (<xref ref-type="bibr" rid="c33">33</xref>). We assessed the z-scored mean absolute error (MAE) between biophysical patterns of synthetic model responses and experimental data, including spiking statistics and membrane potential behaviors (<xref ref-type="supplementary-material" rid="supp1">Fig. S8d</xref>). Impressively, our method efficiently inferred biophysical patterns of 574 neurons across 14 brain regions in the Allen Cell Types Dataset (<xref ref-type="bibr" rid="c22">22</xref>) (<xref ref-type="supplementary-material" rid="supp1">Fig. S9a</xref> &amp; <bold>b</bold>), with consistent inference accuracy across different regions. The results revealed that both spike and subthreshold patterns, which were major distinguishable patterns across different neuron types, were well predicted, suggesting that our inference results could potentially facilitate studying subcellular differences among different neuron types. Among all biophysical patterns, the half-spike width is the most poorly tuned biophysical pattern (<xref ref-type="supplementary-material" rid="supp1">Fig. S9c</xref> &amp; <bold>d</bold>). One potential reason is that the single-compartment HH model is relatively simple to fit this biophysical pattern, while increasing the number of (sodium) channels could give a better fitting accuracy (<xref ref-type="bibr" rid="c21">21</xref>). Moreover, when compared with evolutionary search (ES) algorithm, perhaps the most popular method for single-neuron biophysics inference (<xref ref-type="bibr" rid="c23">23</xref>), our approach demonstrated superior performance, achieving an inference score 1.89 times higher (i.e., number of good inferences given the same sample size; our framework: 574, ES: 303, total: 1052) (<xref ref-type="supplementary-material" rid="supp1">Fig. S9e</xref>; SI Appendix).</p>
<p>Furthermore, our framework inferred 31 distinct properties of the intricate stomatogastric ganglion microcircuit of the <italic>Cancer Borealis</italic> (<xref ref-type="supplementary-material" rid="supp1">Fig. S10a</xref>), including nuances of enriched ion channels and diverse synapses, which aims to replicate the circuit’s characteristic pyloric patterns (<xref ref-type="supplementary-material" rid="supp1">Fig. S10b</xref>, <bold>c</bold> &amp; <bold>d</bold>; SI Appendix). To compare the inference performance, we evaluated it against the state-of-the-art sequential neural posterior estimation (SNPE) method, specifically SNPE-C (<xref ref-type="bibr" rid="c34">34</xref>) (performance given by the pretrained model from Gonçalves et al. (<xref ref-type="bibr" rid="c23">23</xref>); SI Appendix). Our framework demonstrated a 2.54 times smaller inference error for all biophysical patterns than SNPE-C (MAE of our framework: 0.0136, SD=0.0024; MAE of SNPE-C: 0.0346, SD=0.0048) (<xref ref-type="supplementary-material" rid="supp1">Fig. S10e</xref>). In summary, the above results underscore our framework’s robustness and adaptability in neural analysis across multiple scales.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>The present study introduced a novel model-based deep learning inference framework to tackle the challenging problem of neural circuit connectivity reconstruction from <italic>in vivo</italic> recordings, which could potentially bridge the gap between neural circuitry functions and its architecture (<xref ref-type="bibr" rid="c1">1</xref>-<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>). It has been demonstrated that our framework overcame the limitations of existing model-based methods (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>) and accurately inferred monosynaptic connectivity in the hippocampal CA1 region of freely running mice. The key innovation of our approach lies in its ability to address the challenges posed by the model mismatch problem, which often leads to biased inference outcomes (<xref ref-type="bibr" rid="c8">8</xref>). By integrating domain adaptation and self-training with computational models, our framework adapts to discrepancies between synthetic and experimental data and establishes a mapping from paired spike trains to their corresponding connectivity, significantly improving the accuracy of connectivity inference. We have also demonstrated the broad applicability of our framework in inferring biophysical properties of single neurons and microcircuits. By extending the framework to classic model fitting tasks, we successfully inferred a range of biophysical properties of single neurons in the mouse primary visual cortex and the stomatogastric ganglion microcircuit of the <italic>Cancer Borealis</italic>. In both cases, our approach outperformed existing methods in accuracy and efficiency (<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c23">23</xref>), highlighting its potential for advancing the study of neural computation and function across various observational scales.</p>
<p>One of the major challenges in inferring neural connectivity from <italic>in vivo</italic> recordings is the partial observation of neurons, leading to incomplete knowledge of the network. As a result, computational models for the biological network often fail to capture the repertoire of the real biological dynamics and could produce substantial model artefacts. Eliminating such mismatch based on prior knowledge is difficult due to the intricate complexity and insufficient understanding of the underlying biological mechanisms. Consequently, this unrecognized mismatch can introduce bias in connectivity inference when solely relying on the model as the reference (<xref ref-type="bibr" rid="c8">8</xref>). Our framework effectively addressed these obstacles by learning domain-invariant and informative features through domain adaptation and fine-tuning the connectivity classifier using experimental data through self-training. As a result, our method achieved 0.97-1.0 MCC to infer monosynaptic connectivity of the mice CA1 region on a dataset with high signal-to-noise ratio, significantly exceeding current methods. Notably, these existing methods all demonstrated near-perfect inference results on synthetic data (<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c12">12</xref>) but failed to transfer such excellent performance to real experimental data. Similar trend was seen in model-based inference framework in other domains, such as inferring biophysical properties of single neurons and microcircuits (<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c37">37</xref>). Our study not only formulated this issue as the OOD problem in machine learning and proposed a unified solution, but also emphasized the importance of evaluating model-based inference techniques using real experimental data.</p>
<p>Another straightforward and widely used method for tackling the mismatch problem in robotics and autonomous driving is domain randomization (<xref ref-type="bibr" rid="c38">38</xref>–<xref ref-type="bibr" rid="c40">40</xref>). This technique involves varying model parameters to enhance the diversity of synthetic data from different domains. This could potentially enable the machine learning model to learn domain-invariant features that can generalize to real-world applications. Therefore, to test whether increasing the diversity and size of the synthetic dataset can lead to better inference, we simulated 27 different MAT networks by altering three keys: background noise, connectivity ratio, and network size to generate a synthetic dataset that was 16.5 times larger than the previous one (SI Appendix). Our tests revealed that domain randomization increased the MCC by 0.11 for the vanilla deep learning method and by 0.03 for its aggregated version. Despite these improvements, the performance still lagged behind that of DeepDAM and its aggregated version by 0.17 and 0.23 MCC respectively (<xref ref-type="supplementary-material" rid="supp1">Fig. S5</xref>). These results suggest that while domain randomization can enhance the performance of standard deep learning approaches, the benefits are relatively modest, likely due to the high-dimensional nature of the model and the complexity of the neural structure involved. Interestingly, domain randomization can also improve the performance of DeepDAM to an MCC score of 1.0, suggesting that combining DeepDAM with other advanced machine learning techniques is a promising future direction.</p>
<p>The development of large-scale datasets with ground-truth connectivity is urgently needed to further assess and compare these methods, especially because different methods yield varying inference predictions on ambiguous data (<xref rid="fig6" ref-type="fig">Fig. 6</xref>) and the performance of our method has already saturated on this available dataset. For these ambiguous data, our aggregated method was generally more consistent with the second modified label tagging method (<xref rid="fig6" ref-type="fig">Fig. 6d</xref>). We introduced this modified label tagging method because we observed that the experimentally validated ground-truth tagging method was excessively strict. It demanded the peak of the CCG to significantly surpass the slowly co-modulated baseline by a considerable margin (<italic>p</italic><sub>fast</sub> threshold: 0.001) and to be significantly higher than the peak in the anti-causal direction (<xref rid="fig6" ref-type="fig">Fig. 6b</xref>) (<xref ref-type="bibr" rid="c25">25</xref>). We relaxed the <italic>p</italic><sub>fast</sub> threshold to 0.01 for three consecutive time bins and eliminated the requirement in the anti-causal direction (i.e., no <italic>p</italic><sub>causal</sub> prerequisites; <xref rid="fig6" ref-type="fig">Fig. 6b</xref>; SI Appendix), so the adjusted tagging method leaned toward identifying more connections. The fact that our aggregated inference method aligned closer with the modified tagging method (<xref rid="fig6" ref-type="fig">Fig. 6c</xref> top row) suggests that the original labeling criterion often led to the classification of numerous CCGs with relevant connectivity information as indeterminate, such as bidirectional connections or those coexisting with network modulations (<xref rid="fig6" ref-type="fig">Fig. 6c</xref> top row). Mitigating the impact of these ambiguous CCGs can be accomplished in several ways. Firstly, extending the recording duration (<xref rid="fig5" ref-type="fig">Fig. 5b</xref>) can be a straightforward approach, as many experimental methods support chronic recordings of the same group of neurons (<xref ref-type="bibr" rid="c7">7</xref>). Alternatively, incorporating probabilistic methods (<xref ref-type="bibr" rid="c41">41</xref>), ensemble methods (<xref ref-type="bibr" rid="c42">42</xref>) or noisy pseudo labels (<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>) into our existing framework can explicitly model the uncertainty in our inference results of such ambiguous cases.</p>
<p>In this study, we tailored the DeepDAM training process to better address real-world experimental requirements. We included all experimental data for domain adaptation and self-training without using any ground-truth labels during training. We chose not to segregate data from different animals for evaluation primarily for two reasons: 1) Ideally, in real-world settings, we aim to analyze all available data rather than just a subset; 2) Variations in data across subjects can reduce inference accuracy, similar to the mismatch observed between synthetic and experimental data. Addressing the issue of cross-subject generalization, which remains a challenge in the field, we assessed the generalization ability of our DeepDAM framework using a “leave-one-animal-out” approach across 8 mice. The generalization performance of both DeepDAM (MCC: 0.88, SD=0.15) and its aggregated version (MCC: 0.95, SD=0.07) was remarkably close to their initial performance (DeepDAM MCC: 0.91, SD=0.08; DeepDAM aggregated MCC: 0.97, SD=0.03) and substantially better than other methods, including those employing domain randomization (<xref ref-type="supplementary-material" rid="supp1">Fig. S7</xref>). These results underscore the adaptability and effectiveness of DeepDAM in handling the inherent variability found in real-world experimental settings.</p>
<p>We evaluated DeepDAM’s performance in recovering monosynaptic connections within hippocampal CA1, but its applicability to other areas remains to be determined. Because neural circuits differ fundamentally across regions—giving rise to unique physiological processes and activity signatures (<xref ref-type="bibr" rid="c43">43</xref>)—DeepDAM relies on a computational model that captures those region-specific firing dynamics, and our use of the MAT network is tailored to CA1 properties. Going forward, extending DeepDAM will require generating ground-truth spiking datasets from diverse brain regions and developing biophysical models that faithfully reproduce each region’s electrophysiological characteristics.</p>
<p>Our results also demonstrated the robustness and general applicability of the DeepDAM framework, which is designed to be agnostic to other diversity of the neural system. The framework depends on a mechanistic model capable of approximating the dynamics of the underlying systems, such as the firing rate distribution. Therefore, once such simple mechanistic model has been obtained in another species, such as the visual cortex in primates (<xref ref-type="bibr" rid="c44">44</xref>), the DeepDAM framework can be easily applied. While it stands to gain from advancements in more precise mechanistic models, its potential significance lies in advancing the mechanistic modeling of large-scale neural circuits. This is crucial because commonly used techniques like artificial recurrent neural networks (<xref ref-type="bibr" rid="c45">45</xref>) and multicompartmental neuronal networks (<xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c47">47</xref>) often encounter significant model mismatch issues, which can also be conceptualized as an OOD problems (<xref ref-type="bibr" rid="c48">48</xref>). Therefore, we anticipate that DeepDAM, in conjunction with these modeling techniques, will promote a closed-loop, iterative process for designing and refining mechanistic models to directly deduce the system’s architecture and biophysics from experimental data.</p>
<p>Furthermore, DeepDAM can be easily extended to the classic model-fitting task for biophysical inference of single neurons and microcircuits through changing the discrepancy loss and achieved high accuracy and efficiency in both cases. One limitation of DeepDAM on model fitting is that it can only output one set of feasible neural properties to match the experimental data, while many studies have demonstrated there can be multiple sets of feasible properties (<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c49">49</xref>-<xref ref-type="bibr" rid="c54">54</xref>). This could be potentially achieved by replacing the <italic>L<sub>syn</sub></italic> with the loss function used in SNPE methods (<xref ref-type="bibr" rid="c23">23</xref>).</p>
<p>In summary, our framework substantially alleviates the OOD problem in inferring monosynaptic connectivity and biophysical properties from experimental data. This indicates that our framework can potentially facilitate the interaction between experimental and modeling studies to uncover the relationship between low-level neural properties and high-level functions. With continued advancements in experimental techniques and machine learning methodologies, our framework paves the way for a more profound understanding of the brain’s intricate mechanisms, leading to transformative insights into neural computation and cognition. For example, our framework explores the mechanistic formation of neural representations underlying behavior and cognition from a bottom-up perspective, providing a valuable complement to current statistical and geometrical analysis on large-scale neural representations from a top-down perspective (<xref ref-type="bibr" rid="c55">55</xref>–<xref ref-type="bibr" rid="c57">57</xref>). Furthermore, our framework is in line with the rising next-generation ML techniques that utilize simulators and synthetic data to learn richer representations of real world (<xref ref-type="bibr" rid="c39">39</xref>). By tightly fusing computational modeling and ML techniques, our framework may inspire ground-breaking methods that use enriched computational models and synthetic data to address cutting-edge challenges in multiple fields (<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c58">58</xref>), such as behavioral analysis (<xref ref-type="bibr" rid="c59">59</xref>), protein function prediction (<xref ref-type="bibr" rid="c60">60</xref>), and medicine and healthcare (<xref ref-type="bibr" rid="c61">61</xref>).</p>
</sec>
<sec id="s4">
<title>Methods</title>
<p>Here we describe the training details of the DeepDAM framework and leaves other methodological details in SI Appendix.</p>
<sec id="s4a">
<title>Training configurations to infer monosynaptic connectivity</title>
<p>As our method is purely unsupervised and does not use any ground-truth labels, the entire experimental dataset was used for domain adaptation and self-training during the training process. The batch size is 800. The initial learning rate is 0.0001 and adapted by the Adam optimizer (<xref ref-type="bibr" rid="c62">62</xref>) with <italic>β</italic><sub>1</sub> = 0.9 and <italic>β</italic><sub>2</sub> = 0.999. The discrepancy loss is defined as the estimated contrastive domain discrepancy at the feature space (<xref ref-type="bibr" rid="c14">14</xref>), given by <xref rid="eqn5" ref-type="disp-formula">Equation 5</xref>-<xref rid="eqn7" ref-type="disp-formula">7</xref>:
<disp-formula id="eqn5">
<graphic xlink:href="510694v6_eqn5.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn6">
<graphic xlink:href="510694v6_eqn6.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn7">
<graphic xlink:href="510694v6_eqn7.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>G<sub>DA</sub></italic> (·) is the binary gating function for domain adaptation and <inline-formula id="inline-eqn-7"><inline-graphic xlink:href="510694v6_inline7.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the empirical kernel mean embedding which describes the distributional distance at the feature space between synthetic data labeled as class <italic>c</italic> and experimental data pseudo-labeled as class <italic>c’</italic> The empirical kernel mean embeddings of the contrastive domain discrepancy are given by <xref rid="eqn8" ref-type="disp-formula">Equation 8</xref>-<xref rid="eqn12" ref-type="disp-formula">12</xref>:
<disp-formula id="eqn8">
<graphic xlink:href="510694v6_eqn8.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn9">
<graphic xlink:href="510694v6_eqn9.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn10">
<graphic xlink:href="510694v6_eqn10.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn11">
<graphic xlink:href="510694v6_eqn11.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn12">
<graphic xlink:href="510694v6_eqn12.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <italic>k<sup>l</sup></italic> is a Gaussian kernel with a variance of 2<italic><sup>l</sup></italic>normalized by the number of samples and total distance (<xref ref-type="bibr" rid="c14">14</xref>). The discrepancy scaling factor is set as 0.001. The gating threshold for eliminating experimental data from domain adaptation is 0.01 and the gating threshold for self-training is 0.01.</p>
<p>Pseudo labels of experimental data are obtained by applying the spherical K-means clustering algorithm at the feature space among synthetic data and experimental data. The initial cluster center of class is the synthetic cluster center, given by <xref rid="eqn13" ref-type="disp-formula">Equation 13</xref>. Then the algorithm repeats the following two steps until convergence or reaching the maximum iterations: 1) assigning each experimental datum a pseudo label based on the nearest cluster center; 2) updating the cluster center based on experimental data, given by <xref rid="eqn14" ref-type="disp-formula">Equation 14</xref>.
<disp-formula id="eqn13">
<graphic xlink:href="510694v6_eqn13.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
<disp-formula id="eqn14">
<graphic xlink:href="510694v6_eqn14.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula>
where <inline-formula id="inline-eqn-8"><inline-graphic xlink:href="510694v6_inline8.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the initial cluster center for class <inline-formula id="inline-eqn-9"><inline-graphic xlink:href="510694v6_inline9.gif" mime-subtype="gif" mimetype="image"/></inline-formula> is the cluster center for class <italic>c</italic> at training step <italic>t</italic> and <italic>I</italic> is the indicator function. Crucially, synthetic data labels ensure that synthetic data with different labels are not grouped in the same cluster.</p>
<p>As the nature of the pseudo labels is noisy, we incorporated the generalized cross-entropy loss (<xref rid="eqn15" ref-type="disp-formula">Equation 15</xref>) which has been demonstrated to be noise-robust (<xref ref-type="bibr" rid="c20">20</xref>).
<disp-formula id="eqn15">
<graphic xlink:href="510694v6_eqn15.gif" mime-subtype="gif" mimetype="image"/>
</disp-formula></p>
<p>Where <italic>p<sub>c</sub></italic> = <italic>g</italic>(<italic>f</italic>(<italic>x<sub>exp</sub></italic>)) is the output probability for class<italic>c</italic> of DeepDAM. In this study, we chose <italic>q</italic> = 1</p>
<p>For the aggregated method, we trained the DeepDAM framework for 10 different random seeds. The we ensembled the predictions from 5 randomly selected seeds and used the mean probability for each class as the aggregated probability to compute our performance metrics. This procedure was repeated 100 times.</p>
</sec>
</sec>

</body>
<back>
<sec id="s8" sec-type="data-availability">
<title>Data and materials availability</title>
<p>The source code of our framework in this work will be available upon acceptance. All data are available in the main text or the supplementary materials.</p>
</sec>
<sec id="additional-info" sec-type="additional-information">
<title>Additional information</title>
<sec id="s5">
<title>Funding</title>
<p>This work is supported by National Key R&amp;D Program of China (N0. 2020AAA0130400).</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>Conceptualization and methodology: KS, KD</p>
<p>Investigation and analysis: KS, SZ, MB, PQ, LY</p>
<p>Discussion: KS, SZ, MB, PQ, LY, XL, LH, YZ, LM, KD</p>
<p>Supervision: KD, LM</p>
<p>Writing—original draft: KS, SZ, MB, LM, KD</p>
</sec>
</sec>
<sec id="additional-files" sec-type="supplementary-material">
<title>Additional files</title>
<supplementary-material id="supp1">
<label>SI Appendix</label>
<media xlink:href="supplements/510694_file02.docx"/>
</supplementary-material>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name>, <string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Axel</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name>, <string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name></person-group>, <article-title>Optimal degrees of synaptic connectivity</article-title>. <source>Neuron</source> <volume>93</volume>, <fpage>1153</fpage>–<lpage>1164.e1157</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name>, <string-name><given-names>S. C.</given-names> <surname>Turaga</surname></string-name></person-group>, <article-title>Constraining computational models using electron microscopy wiring diagrams</article-title>. <source>Curr Opin Neurobiol</source> <volume>58</volume>, <fpage>94</fpage>–<lpage>100</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name></person-group>, <article-title>Is cortical connectivity optimized for storing information?</article-title> <source>Nat Neurosci</source> <volume>19</volume>, <fpage>749</fpage>–<lpage>755</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Mrsic-Flogel</surname></string-name></person-group>, <article-title>Cortical connectivity and sensory coding</article-title>. <source>Nature</source> <volume>503</volume>, <fpage>51</fpage>–<lpage>58</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Buzsaki</surname></string-name></person-group>, <article-title>Large-scale recording of neuronal ensembles</article-title>. <source>Nat Neurosci</source> <volume>7</volume>, <fpage>446</fpage>–<lpage>451</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. J.</given-names> <surname>Jun</surname></string-name>, <string-name><given-names>N. A.</given-names> <surname>Steinmetz</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Siegle</surname></string-name>, <string-name><given-names>D. J.</given-names> <surname>Denman</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Barbarits</surname></string-name>, <string-name><given-names>A. K.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>C. A.</given-names> <surname>Anastassiou</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Andrei</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Aydin</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Barbic</surname></string-name>, <string-name><given-names>T. J.</given-names> <surname>Blanche</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Bonin</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Couto</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Dutta</surname></string-name>, <string-name><given-names>S. L.</given-names> <surname>Gratiy</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Gutnisky</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hausser</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Karsh</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Ledochowitsch</surname></string-name>, <string-name><given-names>C. M.</given-names> <surname>Lopez</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Mitelut</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Musa</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Okun</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Putzeys</surname></string-name>, <string-name><given-names>P. D.</given-names> <surname>Rich</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Rossant</surname></string-name>, <string-name><given-names>W. L.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Svoboda</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Carandini</surname></string-name>, <string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Koch</surname></string-name>, <string-name><given-names>J.</given-names> <surname>O’Keefe</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Harris</surname></string-name></person-group>, <article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title>. <source>Nature</source> <volume>551</volume>, <fpage>232</fpage>–<lpage>236</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. A.</given-names> <surname>Steinmetz</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Aydin</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Lebedeva</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Okun</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bauza</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Beau</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Bhagat</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Bohm</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Broux</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Colonell</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>Gardner</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Karsh</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Kloosterman</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Kostadinov</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Mora-Lopez</surname></string-name>, <string-name><given-names>J.</given-names> <surname>O’Callaghan</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Putzeys</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Sauerbrei</surname></string-name>, <string-name><given-names>R. J. J.</given-names> <surname>van Daal</surname></string-name>, <string-name><given-names>A. Z.</given-names> <surname>Vollan</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Welkenhuysen</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Ye</surname></string-name>, <string-name><given-names>J. T.</given-names> <surname>Dudman</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Dutta</surname></string-name>, <string-name><given-names>A. W.</given-names> <surname>Hantman</surname></string-name>, <string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>A. K.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>E. I.</given-names> <surname>Moser</surname></string-name>, <string-name><given-names>J.</given-names> <surname>O’Keefe</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Renart</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Svoboda</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hausser</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Haesler</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Carandini</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Harris</surname></string-name></person-group>, <article-title>Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings</article-title>. <source>Science</source> <volume>372</volume>, (<year>2021</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Das</surname></string-name>, <string-name><given-names>I. R.</given-names> <surname>Fiete</surname></string-name></person-group>, <article-title>Systematic errors in connectivity inferred from activity in strongly recurrent networks</article-title>. <source>Nat Neurosci</source> <volume>23</volume>, <fpage>1286</fpage>–<lpage>1296</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Berry</surname>, <suffix>2nd</suffix></string-name>, <string-name><given-names>R.</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Bialek</surname></string-name></person-group>, <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source> <volume>440</volume>, <fpage>1007</fpage>–<lpage>1012</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Kobayashi</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kurita</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kurth</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kitano</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Mizuseki</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Diesmann</surname></string-name>, <string-name><given-names>B. J.</given-names> <surname>Richmond</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Shinomoto</surname></string-name></person-group>, <article-title>Reconstructing neuronal circuitry from parallel spike trains</article-title>. <source>Nat Commun</source> <volume>10</volume>, <fpage>4468</fpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Truccolo</surname></string-name>, <string-name><given-names>U. T.</given-names> <surname>Eden</surname></string-name>, <string-name><given-names>M. R.</given-names> <surname>Fellows</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Donoghue</surname></string-name>, <string-name><given-names>E. N.</given-names> <surname>Brown</surname></string-name></person-group>, <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title>. <source>J Neurophysiol</source> <volume>93</volume>, <fpage>1074</fpage>–<lpage>1089</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Endo</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kobayashi</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Bartolo</surname></string-name>, <string-name><given-names>B. B.</given-names> <surname>Averbeck</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Sugase-Miyamoto</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Hayashi</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kawano</surname></string-name>, <string-name><given-names>B. J.</given-names> <surname>Richmond</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Shinomoto</surname></string-name></person-group>, <article-title>A convolutional neural network for estimating synaptic connectivity from spike trains</article-title>. <source>Sci Rep</source> <volume>11</volume>, <issue>12087</issue> (<year>2021</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>K.</given-names> <surname>Sheng</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Qu</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>L.</given-names> <surname>He</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Du</surname></string-name></person-group>, <article-title>A general LSTM-based deep learning method for estimating neuronal models and inferring neural circuitry</article-title>. <source>bioRxiv</source>, 2021.2003.2014.434027 (<year>2021</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Kang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>A. G.</given-names> <surname>Hauptmann</surname></string-name></person-group>, <article-title>Contrastive adaptation network for unsupervised domain adaptation</article-title>. <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>, <fpage>4888</fpage>-<lpage>4897</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Ganin</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Ustinova</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Ajakan</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Germain</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Laviolette</surname></string-name>, <string-name><given-names>M.</given-names> <surname>March</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Lempitsky</surname></string-name></person-group>, <article-title>Domain-adversarial training of neural networks</article-title>. <source>Journal of machine learning research</source> <volume>17</volume>, <fpage>1</fpage>–<lpage>35</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Iscen</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Tolias</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Avrithis</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Chum</surname></string-name></person-group>, in <conf-name>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</conf-name>. (<year>2019</year>), pp. <fpage>5070</fpage>–<lpage>5079</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>D.-H.</given-names> <surname>Lee</surname></string-name></person-group>, in <conf-name>Workshop on challenges in representation learning, ICML</conf-name>. (<publisher-loc>Atlanta</publisher-loc>, <year>2013</year>), vol. <volume>3</volume>, pp. <fpage>896</fpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Arazo</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Ortego</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Albert</surname></string-name>, <string-name><given-names>N. E.</given-names> <surname>O’Connor</surname></string-name>, <string-name><given-names>K.</given-names> <surname>McGuinness</surname></string-name></person-group>, in <conf-name>2020 International Joint Conference on Neural Networks (IJCNN)</conf-name>. (<publisher-name>IEEE</publisher-name>, <year>2020</year>), pp. <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Gong</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Ding</surname></string-name>, <string-name><given-names>Z. M.</given-names> <surname>Tao</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Zheng</surname></string-name></person-group>, in <conf-name>Proceedings of the European Conference on Computer Vision (ECCV)</conf-name>. (<year>2018</year>), pp. <fpage>299</fpage>–<lpage>315</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sabuncu</surname></string-name></person-group>, <article-title>Generalized cross entropy loss for training deep neural networks with noisy labels</article-title>. <source>Advances in neural information processing systems</source> <volume>31</volume>, (<year>2018</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. W.</given-names> <surname>Gouwens</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Berg</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Feng</surname></string-name>, <string-name><given-names>S. A.</given-names> <surname>Sorensen</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Hawrylycz</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Koch</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Arkhipov</surname></string-name></person-group>, <article-title>Systematic generation of biophysically detailed models for diverse cortical neuron types</article-title>. <source>Nat Commun</source> <volume>9</volume>, <fpage>710</fpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="other"><article-title>Allen Cell Types Database</article-title>. (<year>2015</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P. J.</given-names> <surname>Gonçalves</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Lueckmann</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Deistler</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nonnenmacher</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Ocal</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Bassetto</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Chintaluri</surname></string-name>, <string-name><given-names>W. F.</given-names> <surname>Podlaski</surname></string-name>, <string-name><given-names>S. A.</given-names> <surname>Haddad</surname></string-name>, <string-name><given-names>T. P.</given-names> <surname>Vogels</surname></string-name>, <string-name><given-names>D. S.</given-names> <surname>Greenberg</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Macke</surname></string-name></person-group>, <article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title>. <source>eLife</source> <volume>9</volume>, (<year>2020</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Van der Maaten</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name></person-group>, <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of machine learning research</source> <volume>9</volume>, (<year>2008</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. F.</given-names> <surname>English</surname></string-name>, <string-name><given-names>S.</given-names> <surname>McKenzie</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Evans</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Yoon</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Buzsaki</surname></string-name></person-group>, <article-title>Pyramidal cell-interneuron circuit architecture and dynamics in hippocampal networks</article-title>. <source>Neuron</source> <volume>96</volume>, <fpage>505</fpage>–<lpage>520.e507</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Deng</surname></string-name></person-group>, <article-title>Deep visual domain adaptation: A survey</article-title>. <source>Neurocomputing</source> <volume>312</volume>, <fpage>135</fpage>–<lpage>153</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Ben-David</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Blitzer</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Crammer</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kulesza</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Pereira</surname></string-name>, <string-name><given-names>J. W.</given-names> <surname>Vaughan</surname></string-name></person-group>, <article-title>A theory of learning from different domains</article-title>. <source>Mach Learn</source> <volume>79</volume>, <fpage>151</fpage>–<lpage>175</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Ben-David</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Blitzer</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Crammer</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Pereira</surname></string-name></person-group>, <article-title>Analysis of representations for domain adaptation</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>19</volume>, <fpage>137</fpage>–<lpage>144</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names> <surname>Aertsen</surname></string-name>, <string-name><given-names>G. L.</given-names> <surname>Gerstein</surname></string-name></person-group>, <article-title>Evaluation of neuronal connectivity: sensitivity of cross-correlation</article-title>. <source>Brain Res</source> <volume>340</volume>, <fpage>341</fpage>–<lpage>354</lpage> (<year>1985</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Ostojic</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Hakim</surname></string-name></person-group>, <article-title>How connectivity, background activity, and synaptic properties shape the cross-correlation between spike trains</article-title>. <source>J Neurosci</source> <volume>29</volume>, <fpage>10234</fpage>–<lpage>10253</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. E.</given-names> <surname>Rumelhart</surname></string-name>, <string-name><given-names>G. E.</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>Williams</surname></string-name></person-group>, <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source> <volume>323</volume>, <fpage>533</fpage>–<lpage>536</lpage> (<year>1986</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Chicco</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Jurman</surname></string-name></person-group>, <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title>. <source>BMC genomics</source> <volume>21</volume>, <fpage>1</fpage>–<lpage>13</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Pospischil</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Toledo-Rodriguez</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Monier</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Piwkowska</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Bal</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Fregnac</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Markram</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Destexhe</surname></string-name></person-group>, <article-title>Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons</article-title>. <source>Biol Cybern</source> <volume>99</volume>, <fpage>427</fpage>–<lpage>441</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Greenberg</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nonnenmacher</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Macke</surname></string-name></person-group>, <article-title>Automatic Posterior Transformation for Likelihood-Free Inference</article-title>. <conf-name>Proceedings of the 36th International Conference on Machine Learning</conf-name> <volume>97</volume>, <fpage>2404</fpage>--<lpage>2414</lpage> (<year>2019</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. A.</given-names> <surname>Jarrell</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>A. E.</given-names> <surname>Bloniarz</surname></string-name>, <string-name><given-names>C. A.</given-names> <surname>Brittin</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>J. N.</given-names> <surname>Thomson</surname></string-name>, <string-name><given-names>D. G.</given-names> <surname>Albertson</surname></string-name>, <string-name><given-names>D. H.</given-names> <surname>Hall</surname></string-name>, <string-name><given-names>S. W.</given-names> <surname>Emmons</surname></string-name></person-group>, <article-title>The connectome of a decision-making neural network</article-title>. <source>Science</source> <volume>337</volume>, <fpage>437</fpage>–<lpage>444</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. L.</given-names> <surname>Briggman</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Helmstaedter</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Denk</surname></string-name></person-group>, <article-title>Wiring specificity in the direction-selectivity circuit of the retina</article-title>. <source>Nature</source> <volume>471</volume>, <fpage>183</fpage>–<lpage>188</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Ben-Shalom</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Balewski</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Siththaranjan</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Baratham</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kyoung</surname></string-name>, <string-name><given-names>K. G.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>K. J.</given-names> <surname>Bender</surname></string-name>, <string-name><given-names>K. E.</given-names> <surname>Bouchard</surname></string-name></person-group>, <article-title>Inferring neuronal ionic conductances from membrane potentials using CNNs</article-title>. <source>bioRxiv</source>, 727974 (<year>2019</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Tobin</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Fong</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ray</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schneider</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Zaremba</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Abbeel</surname></string-name></person-group>, in <conf-name>2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</conf-name>. (<publisher-name>IEEE</publisher-name>, <year>2017</year>), pp. <fpage>23</fpage>–<lpage>30</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. M.</given-names> <surname>de Melo</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Torralba</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Guibas</surname></string-name>, <string-name><given-names>J.</given-names> <surname>DiCarlo</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Chellappa</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hodgins</surname></string-name></person-group>, <article-title>Next-generation deep learning based on simulators and synthetic data</article-title>. <source>Trends Cogn Sci</source> <volume>26</volume>, <fpage>174</fpage>–<lpage>187</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>X. B.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Andrychowicz</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Zaremba</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Abbeel</surname></string-name></person-group>, in <conf-name>2018 IEEE international conference on robotics and automation (ICRA)</conf-name>. (<publisher-name>IEEE</publisher-name>, <year>2018</year>), pp. <fpage>3803</fpage>–<lpage>3810</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><given-names>Y.</given-names> <surname>Gal</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Ghahramani</surname></string-name></person-group>, in <conf-name>international conference on machine learning</conf-name>. (<publisher-name>PMLR</publisher-name>, <year>2016</year>), pp. <fpage>1050</fpage>–<lpage>1059</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Lakshminarayanan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Pritzel</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Blundell</surname></string-name></person-group>, <article-title>Simple and scalable predictive uncertainty estimation using deep ensembles</article-title>. <source>Advances in neural information processing systems</source> <volume>30</volume>, (<year>2017</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. A.</given-names> <surname>Machado</surname></string-name>, <string-name><given-names>I. V.</given-names> <surname>Kauvar</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Deisseroth</surname></string-name></person-group>, <article-title>Multiregion neuronal activity: the forest and the trees</article-title>. <source>Nature Reviews Neuroscience</source> <volume>23</volume>, <fpage>683</fpage>–<lpage>704</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Schrimpf</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kubilius</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>N. A.</given-names> <surname>Ratan Murty</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Ajemian</surname></string-name>, <string-name><given-names>J. J.</given-names> <surname>DiCarlo</surname></string-name></person-group>, <article-title>Integrative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence</article-title>. <source>Neuron</source> <volume>108</volume>, <fpage>413</fpage>–<lpage>423</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names> <surname>Durstewitz</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Koppe</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Thurm</surname></string-name></person-group>, <article-title>Reconstructing computational system dynamics from neural data with recurrent neural networks</article-title>. <source>Nat Rev Neurosci</source> <volume>24</volume>, <fpage>693</fpage>–<lpage>710</lpage> (<year>2023</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y. N.</given-names> <surname>Billeh</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Cai</surname></string-name>, <string-name><given-names>S. L.</given-names> <surname>Gratiy</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Dai</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Iyer</surname></string-name>, <string-name><given-names>N. W.</given-names> <surname>Gouwens</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Abbasi-Asl</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Siegle</surname></string-name>, <string-name><given-names>S. R.</given-names> <surname>Olsen</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Koch</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Mihalas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Arkhipov</surname></string-name></person-group>, <article-title>Systematic integration of structural and functional data into multi-scale models of mouse primary visual cortex</article-title>. <source>Neuron</source> <volume>106</volume>, <fpage>388</fpage>–<lpage>403.e318</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Markram</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Muller</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ramaswamy</surname></string-name>, <string-name><given-names>M. W.</given-names> <surname>Reimann</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Abdellah</surname></string-name>, <string-name><given-names>C. A.</given-names> <surname>Sanchez</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ailamaki</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Alonso-Nanclares</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Antille</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Arsever</surname></string-name>, <string-name><given-names>G. A.</given-names> <surname>Kahou</surname></string-name>, <string-name><given-names>T. K.</given-names> <surname>Berger</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Bilgili</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Buncic</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Chalimourda</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Chindemi</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Courcol</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Delalondre</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Delattre</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Druckmann</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Dumusc</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Dynes</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Eilemann</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Gal</surname></string-name>, <string-name><given-names>M. E.</given-names> <surname>Gevaert</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Ghobril</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gidon</surname></string-name>, <string-name><given-names>J. W.</given-names> <surname>Graham</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Haenel</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Hay</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Heinis</surname></string-name>, <string-name><given-names>J. B.</given-names> <surname>Hernando</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hines</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Kanari</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Keller</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kenyon</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Khazen</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>J. G.</given-names> <surname>King</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Kisvarday</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Kumbhar</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Lasserre</surname></string-name>, <string-name><given-names>J. V.</given-names> <surname>Le Be</surname></string-name>, <string-name><given-names>B. R.</given-names> <surname>Magalhaes</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Merchan-Perez</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Meystre</surname></string-name>, <string-name><given-names>B. R.</given-names> <surname>Morrice</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Muller</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Munoz-Cespedes</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Muralidhar</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Muthurasa</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Nachbaur</surname></string-name>, <string-name><given-names>T. H.</given-names> <surname>Newton</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nolte</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ovcharenko</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Palacios</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Pastor</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Perin</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Ranjan</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Riachi</surname></string-name>, <string-name><given-names>J. R.</given-names> <surname>Rodriguez</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Riquelme</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Rossert</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Sfyrakis</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>J. C.</given-names> <surname>Shillcock</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Silberberg</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Silva</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Tauheed</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Telefont</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Toledo-Rodriguez</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Trankler</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Van Geit</surname></string-name>, <string-name><given-names>J. V.</given-names> <surname>Diaz</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Walker</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Zaninetta</surname></string-name>, <string-name><given-names>J.</given-names> <surname>DeFelipe</surname></string-name>, <string-name><given-names>S. L.</given-names> <surname>Hill</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Schurmann</surname></string-name></person-group>, <article-title>Reconstruction and simulation of neocortical microcircuitry</article-title>. <source>Cell</source> <volume>163</volume>, <fpage>456</fpage>–<lpage>492</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Qian</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Zavatone-Veth</surname></string-name>, <string-name><given-names>B. S.</given-names> <surname>Ruben</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Pehlevan</surname></string-name></person-group>, <article-title>Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics</article-title>. <source>bioRxiv</source>, 2024.2005. 2024.595741 (<year>2024</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. A.</given-names> <surname>Prinz</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Bucher</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Marder</surname></string-name></person-group>, <article-title>Similar network activity from disparate circuit parameters</article-title>. <source>Nat Neurosci</source> <volume>7</volume>, <fpage>1345</fpage>–<lpage>1352</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. G.</given-names> <surname>Metzen</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Chacron</surname></string-name></person-group>, <article-title>Stimulus background influences phase invariant coding by correlated neural activity</article-title>. <source>eLife</source> <volume>6</volume>, (<year>2017</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Marder</surname></string-name>, <string-name><given-names>M. L.</given-names> <surname>Goeritz</surname></string-name>, <string-name><given-names>A. G.</given-names> <surname>Otopalik</surname></string-name></person-group>, <article-title>Robust circuit rhythms in small circuits arise from variable circuit components and mechanisms</article-title>. <source>Curr Opin Neurobiol</source> <volume>31</volume>, <fpage>156</fpage>–<lpage>163</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. J.</given-names> <surname>Gutierrez</surname></string-name>, <string-name><given-names>T.</given-names> <surname>O’Leary</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Marder</surname></string-name></person-group>, <article-title>Multiple mechanisms switch an electrically coupled, synaptically inhibited neuron between competing rhythmic oscillators</article-title>. <source>Neuron</source> <volume>77</volume>, <fpage>845</fpage>–<lpage>858</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. M.</given-names> <surname>Alonso</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Marder</surname></string-name></person-group>, <article-title>Visualization of currents in neural models with similar behavior and different conductance densities</article-title>. <source>eLife</source> <volume>8</volume>, (<year>2019</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names> <surname>Marder</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Goaillard</surname></string-name></person-group>, <article-title>Variability, compensation and homeostasis in neuron and network function</article-title>. <source>Nat Rev Neurosci</source> <volume>7</volume>, <fpage>563</fpage>–<lpage>574</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. E.</given-names> <surname>Urai</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Doiron</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Leifer</surname></string-name>, <string-name><given-names>A. K.</given-names> <surname>Churchland</surname></string-name></person-group>, <article-title>Large-scale neural recordings call for new insights to link brain and behavior</article-title>. <source>Nat Neurosci</source> <volume>25</volume>, <fpage>11</fpage>–<lpage>19</lpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Jazayeri</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ostojic</surname></string-name></person-group>, <article-title>Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity</article-title>. <source>Curr Opin Neurobiol</source> <volume>70</volume>, <fpage>113</fpage>–<lpage>120</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Chung</surname></string-name>, <string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name></person-group>, <article-title>Neural population geometry: An approach for understanding biological and artificial neural networks</article-title>. <source>Curr Opin Neurobiol</source> <volume>70</volume>, <fpage>137</fpage>–<lpage>144</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Sapoval</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Aghazadeh</surname></string-name>, <string-name><given-names>M. G.</given-names> <surname>Nute</surname></string-name>, <string-name><given-names>D. A.</given-names> <surname>Antunes</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Balaji</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Baraniuk</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Barberan</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Dannenfelser</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Dun</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Edrisi</surname></string-name>, <string-name><given-names>R. A. L.</given-names> <surname>Elworth</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Kille</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kyrillidis</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Nakhleh</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Wolfe</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Yan</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Yao</surname></string-name>, <string-name><given-names>T. J.</given-names> <surname>Treangen</surname></string-name></person-group>, <article-title>Current progress and open challenges for applying deep learning across the biosciences</article-title>. <source>Nat Commun</source> <volume>13</volume>, <fpage>1728</fpage> (<year>2022</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. A.</given-names> <surname>Bolanos</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Xiao</surname></string-name>, <string-name><given-names>N. L.</given-names> <surname>Ford</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>LeDue</surname></string-name>, <string-name><given-names>P. K.</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Doebeli</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Rhodin</surname></string-name>, <string-name><given-names>T. H.</given-names> <surname>Murphy</surname></string-name></person-group>, <article-title>A three-dimensional virtual mouse generates synthetic training data for behavioral analysis</article-title>. <source>Nat Methods</source> <volume>18</volume>, <fpage>378</fpage>–<lpage>381</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Wan</surname></string-name>, <string-name><given-names>D. T.</given-names> <surname>Jones</surname></string-name></person-group>, <article-title>Protein function prediction is improved by creating synthetic feature samples with generative adversarial networks</article-title>. <source>Nat Mach Intell</source> <volume>2</volume>, <fpage>540</fpage>–<lpage>550</lpage> (<year>2020</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. J.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>M. Y.</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>T. Y.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>D. F. K.</given-names> <surname>Williamson</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Mahmood</surname></string-name></person-group>, <article-title>Synthetic data in machine learning for medicine and healthcare</article-title>. <source>Nat Biomed Eng</source> <volume>5</volume>, <fpage>493</fpage>–<lpage>497</lpage> (<year>2021</year>).</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="preprint"><person-group person-group-type="author"><string-name><given-names>D. P.</given-names> <surname>Kingma</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Ba</surname></string-name></person-group>, <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv</source>:<pub-id pub-id-type="arxiv">1412.6980</pub-id>, (<year>2014</year>).</mixed-citation></ref>
</ref-list>
</back>
<sub-article id="sa0" article-type="editor-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109028.1.sa2</article-id>
<title-group>
<article-title>eLife Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Naud</surname>
<given-names>Richard</given-names>
</name>
<role specific-use="editor">Reviewing Editor</role>
<aff>
<institution-wrap>
<institution-id institution-id-type="ror">https://ror.org/03c4mmv16</institution-id><institution>University of Ottawa</institution>
</institution-wrap>
<city>Ottawa</city>
<country>Canada</country>
</aff>
</contrib>
</contrib-group>
<kwd-group kwd-group-type="evidence-strength">
<kwd>Incomplete</kwd>
</kwd-group>
<kwd-group kwd-group-type="claim-importance">
<kwd>Valuable</kwd>
</kwd-group>
</front-stub>
<body>
<p>This article reports an algorithm for inferring the presence of synaptic connection between neurons based on naturally occurring spiking activity of a neuronal network. One key improvement is to combine self-supervised and synthetic approaches to learn to focus on features that generalize to the conditions of the observed network. This <bold>valuable</bold> contribution is currently supported by <bold>incomplete</bold> evidence.</p>
</body>
</sub-article>
<sub-article id="sa1" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109028.1.sa1</article-id>
<title-group>
<article-title>Reviewer #1 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>Summary:</p>
<p>The authors proposed a new method to infer connectivity from spike trains whose main novelty relies on their approach to mitigate the problem of model mismatch. The latter arises when the inference algorithm is trained or based on a model that does not accurately describe the data. They propose combining domain adaptation with a deep neural architecture and in an architecture called DeepDAM. They apply DeepDAM to an in vivo ground-truth dataset previously recorded in mouse CA1, show that it performs better than methods without domain adaptation, and evaluate its robustness. Finally, they show that their approach can also be applied to a different problem i.e., inferring biophysical properties of individual neurons.</p>
<p>Strengths:</p>
<p>(1) The problem of inferring connectivity from extracellular recording is a very timely one: as the yield of silicon probes steadily increases, the number of simultaneously recorded pairs does so quadratically, drastically increasing the possibility of detecting connected pairs.</p>
<p>(2) Using domain adaptation to address model mismatch is a clever idea, and the way the authors introduced it into the larger architecture seems sensible.</p>
<p>(3) The authors clearly put a great effort into trying to communicate the intuitions to the reader.</p>
<p>Weaknesses:</p>
<p>(1) The validation of the approach is incomplete: due to its very limited size, the single ground-truth dataset considered does not provide a sufficient basis to draw a strong conclusion. While the authors correctly note that this is the only dataset of its kind, the value of this validation is limited compared to what could be done by carefully designing in silico experiments.</p>
<p>(2) Surprisingly, the authors fail to compare their method to the approach originally proposed for the data they validate on (English et al., 2017).</p>
<p>(3) The authors make a commendable effort to study the method's robustness by pushing the limits of the dataset. However, the logic of the robustness analysis is often unclear, and once again, the limited size of the dataset poses major limitations to the authors.</p>
<p>(4) The lack of details concerning both the approach and the validation makes it challenging for the reader to establish the technical soundness of the study.</p>
<p>Although in the current form this study does not provide enough basis to judge the impact of DeepDAM in the broader neuroscience community, it nevertheless puts forward a valuable and novel idea: using domain adaptation to mitigate the problem of model mismatch. This approach might be leveraged in future studies and methods to infer connectivity.</p>
</body>
</sub-article>
<sub-article id="sa2" article-type="referee-report">
<front-stub>
<article-id pub-id-type="doi">10.7554/eLife.109028.1.sa0</article-id>
<title-group>
<article-title>Reviewer #2 (Public review):</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<anonymous/>
<role specific-use="referee">Reviewer</role>
</contrib>
</contrib-group>
</front-stub>
<body>
<p>The article is very well written, and the new methodology is presented with care. I particularly appreciated the step-by-step rationale for establishing the approach, such as the relationship between K-means centers and the various parameters. This text is conveniently supported by the flow charts and t-SNE plots. Importantly, I thought the choice of state-of-the-art method was appropriate and the choice of dataset adequate, which together convinced me in believing the large improvement reported. I thought that the crossmodal feature-engineering solution proposed was elegant and seems exportable to other fields. Here are a few notes.</p>
<p>
While the validation data set was well chosen and of high quality, it remains a single dataset and also remains a non-recurrent network. The authors acknowledge this in the discussion, but I wanted to chime in to say that for the method to be more than convincing, it would need to have been tested on more datasets. It should be acknowledged that the problem becomes more complicated in a recurrent excitatory network, and thus the method may not work as well in the cortex or in CA3.</p>
<p>While the data is shown to work in this particular dataset (plus the two others at the end), I was left wondering when the method breaks. And it should break if the models are sufficiently mismatched. Such a question can be addressed using synthetic-synthetic models. This was an important intuition that I was missing, and an important check on the general nature of the method that I was missing.</p>
<p>While the choice of state-of-the-art is good in my opinion, I was looking for comments on the methods prior to that. For instance, methods such based on GLMs have been used by the Pillow, Paninski, and Truccolo groups. I could not find a decent discussion of these methods in the main text and thought that both their acknowledgement and rationale for dismissing were missing.</p>
<p>While most of the text was very clear, I thought that page 11 was odd and missing much in terms of introductions. Foremost is the introduction of the dataset, which is never really done. Page 11 refers to 'this dataset', while the previous sentence was saying that having such a dataset would be important and is challenging. The dataset needs to be properly described: what's the method for labeling, what's the brain area, what were the spike recording methodologies, what is meant by two labeling methodologies, what do we know about the idiosyncrasies of the particular network the recording came from (like CA1 is non-recurrent, so which connections)? I was surprised to see 'English et al.' cited in text only on page 13 since their data has been hailed from the beginning.</p>
<p>Further elements that needed definition are the Nsyn and i, which were not defined in the cortex of Equation 2-3: I was not sure if it referred to different samples or different variants of the synthetic model. I also would have preferred having the function f defined earlier, as it is defined for Equation 3, but appears in Equation 2.</p>
<p>When the loss functions are described, it would be important to define 'data' and 'labels' here. This machine learning jargon has a concrete interpretation in this context, and making this concrete would be very important for the readership.</p>
<p>While I appreciated that there was a section on robustness, I did not find that the features studied were the most important. In this context, I was surprised that the other datasets were relegated to supplementary, as these appeared more relevant.</p>
<p>Some of the figures have text that is too small. In particular, Figure 2 has text that is way too small. It seemed to me that the pseudo code could stand alone, and the screenshot of the equations did not need to be repeated in a figure, especially if their size becomes so small that we can't even read them.</p>
</body>
</sub-article>
</article>