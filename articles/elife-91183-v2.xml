<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91183</article-id><article-id pub-id-type="doi">10.7554/eLife.91183</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Serial attentional resource allocation during parallel feature value tracking</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-48782"><name><surname>Merkel</surname><given-names>Christian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8730-5653</contrib-id><email>christian.merkel@med.ovgu.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-335444"><name><surname>Burgmann</surname><given-names>Luise</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-198091"><name><surname>Bartsch</surname><given-names>Mandy Viktoria</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9276-5160</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-335446"><name><surname>Schoenfeld</surname><given-names>Mircea Ariel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-80657"><name><surname>Hopf</surname><given-names>Jens-Max</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Department of Neurology, Otto-von-Guericke University</institution></institution-wrap><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zwmgk08</institution-id><institution>Department of Behavioral Neurology, Leibnitz Institute for Neurobiology</institution></institution-wrap><addr-line><named-content content-type="city">Magdeburg</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04bkje958</institution-id><institution>Kliniken Schmieder</institution></institution-wrap><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>15</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e91183</elocation-id><history><date date-type="received" iso-8601-date="2023-07-20"><day>20</day><month>07</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2023-12-11"><day>11</day><month>12</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2023-09-13"><day>13</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.09.12.557201"/></event></pub-history><permissions><copyright-statement>Â© 2023, Merkel et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Merkel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91183-v2.pdf"/><abstract><p>The visual system has evolved the ability to track features like color and orientation in parallel. This property aligns with the specialization of processing these feature dimensions in the visual cortex. But what if we ask to track changing feature-values within the same feature dimension? Parallel tracking would then have to share the same cortical representation, which would set strong limitations on tracking performance. We address this question by measuring the precision of color representations when human observers track the color of two superimposed dot clouds that simultaneously change color along independent trajectories in color-space. We find that tracking precision is highly imbalanced between streams and that tracking precision changes over time by alternating between streams at a rate of ~1 Hz. These observations suggest that, while parallel color tracking is possible, it is highly limited, essentially allowing for only one color-stream to be tracked with precision at a given time.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>feature-based attention</kwd><kwd>attentional tracking</kwd><kwd>color vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB1436/B05</award-id><principal-award-recipient><name><surname>Hopf</surname><given-names>Jens-Max</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The strong limitation of human subjects in tracking two color streams simultaneously as they independently traverse color space can be attributed to attention alternating slowly and sequentially between streams.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The visual system is very proficient in maintaining information about multiple objects at different locations over time. One common assumption to explain this is a central processing resource that is flexibly distributed amongst multiple relevant locations. Such spatial selection mechanism also enables the formation of parallel mental representations of several visual features at distinct locations through visual working memory (<xref ref-type="bibr" rid="bib29">Luck and Vogel, 1997</xref>; <xref ref-type="bibr" rid="bib36">Scolari et al., 2008</xref>; <xref ref-type="bibr" rid="bib19">Fukuda et al., 2010</xref>; <xref ref-type="bibr" rid="bib31">Merkel et al., 2021</xref>). Furthermore, in a dynamic setting, multiple relevant items can be tracked through retinal space over extended periods of time by enhancing the representations of individual spatial locations simultaneously (<xref ref-type="bibr" rid="bib1">Alvarez and Franconeri, 2007</xref>; <xref ref-type="bibr" rid="bib8">Cavanagh and Alvarez, 2005</xref>; <xref ref-type="bibr" rid="bib30">Merkel et al., 2014</xref>). The actual distribution of the central processing resource amongst the relevant locations is hereby internally adjusted by task properties that enhance or reduce interference within the relevant, location-based, domain (<xref ref-type="bibr" rid="bib37">Shim et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Franconeri et al., 2010</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2013</xref>).</p><p>Interestingly, it has been suggested that the above-mentioned attentional tracking processes may not solely be tied to visual information traveling through space. <xref ref-type="bibr" rid="bib6">Blaser et al., 2000</xref> showed that a visual object can be tracked through feature-space, in that separate visual features can follow independent trajectories despite them being spatially inseparable. In other words, subjects are able to track an object through a simultaneous change in three separate feature dimensions. This finding supports the notion of object-based attention, according to which attention modulates the processing within all relevant features dimensions the attended object is comprised of <xref ref-type="bibr" rid="bib14">Duncan et al., 1997</xref>; <xref ref-type="bibr" rid="bib35">Schoenfeld et al., 2014</xref>.</p><p>Recent data suggests, that the visual system may also be able to maintain multiple changing feature values within a single feature dimension (<xref ref-type="bibr" rid="bib34">Re et al., 2019</xref>). For example, <xref ref-type="bibr" rid="bib34">Re et al., 2019</xref> asked subjects to continuously attend two superimposed color dot-motion clouds and detect faint saturation changes in either of them. The time-course of response accuracies to detect changes of the target colors was taken to suggest an oscillatory mechanism that allocates feature-based resources sequentially to all relevant items within the feature dimension (<xref ref-type="bibr" rid="bib34">Re et al., 2019</xref>). Such rhythmic sampling of visual information to overcome resource limitations has been discussed before extensively as a mechanistic framework for spatial attention (<xref ref-type="bibr" rid="bib27">Landau and Fries, 2012</xref>; <xref ref-type="bibr" rid="bib15">Fiebelkorn et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">VanRullen, 2016</xref>), and it has been linked to oscillatory processes measured with electrophysiological recordings (<xref ref-type="bibr" rid="bib7">Busch et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Song et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Landau et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Kienitz et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Fiebelkorn et al., 2018</xref>).</p><p>However, all previous research investigating multiple feature tracking addressed the issue by asking observers to track constant feature-values (e.g. two superimposed colors) to detect target changes in another feature dimension (e.g. a change in luminance or saturation). Importantly, this form of feature tracking does not require to continuously update the representation of changing feature-values within the attended streams. Hence, the question whether feature-based processes are able to maintain representations of multiple changing feature-values within the same dimension, in analogy to multiple-object tracking in space (<xref ref-type="bibr" rid="bib1">Alvarez and Franconeri, 2007</xref>; <xref ref-type="bibr" rid="bib35">Schoenfeld et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Pylyshyn, 1989</xref>), remains entirely unresolved.</p><p>Here, we report experiments designed to directly assess the process of feature-based resource allocation during a highly controlled multiple color tracking task, where subjects are asked to simultaneously track changing but spatially inseparable color streams. Specifically, observers track the color of two superimposed dot clouds (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) that simultaneously move along independent trajectories through color-space. Color-stream specific resource allocation is quantified as the precision of reporting one or both of the tracked colors. Cognitive resource distribution indexed by the proportion of precision estimates is measured across pairs of simultaneously tracked color-streams (experiments 1 and 2) and across variable tracking intervals (experiments 3 a-c).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental paradigm.</title><p>(<bold>a</bold>) General stimulus design: Two superimposed dot clouds are present throughout each trial. Subjects are asked to attend to the colors of both objects as they transit through hue-space. In order to remove any spatial depth cue specific to one of the two objects, individual dots are superimposed randomly at time of initial generation. Furthermore, 10 random dots of each objects interchange identity (current color) at each frame throughout the trial, thus discouraging the generation of any spatial configuration in aiding feature tracking. At the end of the tracking phase one (Exp2/Exp3) or two (Exp1) of the streams change to grey and subjects have to report the last perceived color for that object using a dial. (<bold>b</bold>) Experimental design and time-course of trials for all three experiments. In experiment 1 and 2, subjects are tracking both color streams for 6â8 sec after which they are asked to report back on one (Exp1) or two (Exp2) of the streams. Probed color streams turn grey to indicate that they have to be reported on. After dialing in the response with a cursor on a centrally presented color wheel, subjects receive feedback about the correct response via a second cursor on the same color wheel. Experiment 3 additionally introduces a luminance cue for one of the two streams at the beginning of the tracking phase in 50% of the trials. After a set of fixed tracking intervals, subjects have to report on the color of the previously cued or uncued stream. This experiment is performed with three different sets (SOA200/SOA100/SOA40) of six tracking intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91183-fig1-v2.tif"/></fig></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment 1</title><p>Subjects were asked to simultaneously track the color change of two donut-shaped superimposed dot-clouds as illustrated in <xref ref-type="fig" rid="fig1">Figure 1a</xref>. The calculated trajectories of the two dot-clouds through color space were kept random and unpredictable, while never falling below a minimum critical distance, on each experimental trial, and the stimuli were designed in a manner that tracking could not be based on spatial information (c.f. Materials and methods for details). Color tracking lasted for 6â8 sec, after which both dot-clouds turned into grey. This prompted subjects to report the last perceived color of each stream (Target a, Target b; before turning grey), by dialing in the color as precisely as possible on a color wheel using a rotary knob (<xref ref-type="fig" rid="fig1">Figure 1b</xref> â Exp1). The color wheel was presented twice requiring the subjects to dial in the colors of both streams in sequence (Response 1, Response 2). The precision of the representation for the two color-streams (Precision High, Precision Low) was derived from the two standard deviations of a mixture model of two von Mises distributions containing pairs of target-response distances for all trials. On a given trial, the target (a or b) with the smaller of the two possible target-response distances (min(response 1 â target a, response 1 â target b)) was assigned to response 1 assuming a generally higher confidence for the first given response. The remaining target-response pair was assigned to response 2 (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Despite the fact that the resulting absolute precision estimates varied considerably across subjects (range(p1)=9.489, range(p2)=30.995) (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), corresponding precision estimates of both streams showed a similar relation across subjects with a correlation of <italic>r</italic>=0.602 (p=0.008). A regression model with zero intercept (p1=a*p2) yielded a significant slope of a=1.748 (F(1,17) = 10.588, p=0.0046), suggesting an unbalanced resource distribution between the two streams of a similar ratio in each subject, independent of the overall precision of the subject. Importantly, none of the pairs of precision estimates were different from a Monte-Carlo simulated set of responses in any of the subjects (p&gt;0.811), which indicates that the variation in precision between streams arises from independent distributions. However, subjects were to report the color of the two streams in sequence. This may have caused recency effects, that is, the representation of the later reported color may decay while giving the first response. Accordingly, the difference of precision estimates may not reflect an imbalance of resource allocation during color tracking but a recency-based imbalance generated in the response phase of the experimental trial. Experiment 2 addresses this possibility, by having sixteen subjects of experiment 1 track two color streams exactly as in experiment 1, but report only one color-stream on a given trial (<xref ref-type="fig" rid="fig1">Figure 1b</xref> â Exp2). The logic behind this manipulation is that the precision measures of just one color will still sample from both color-streams randomly, but remove the ambiguity of two possible target-response pairings for each trial. The precision estimates will therefore represent a mixture of distributions. A difference between precision distributions as seen in experiment 1, would verify that the imbalance of resource allocation is truly arising in the color tracking phase.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Aquisition of precision responses per trial and mixture modeling.</title><p>(<bold>a</bold>) In experiment 1, subjects have to report on two target colors in sequence. The two reports (R<sub>1</sub> and R<sub>2</sub>) and the two target colors (T<sub>a</sub> and T<sub>b</sub>) for one trial create two possible target-response associations (and therefore two pairs of precision responses): (T<sub>a</sub>-R<sub>1</sub> &amp; T<sub>b</sub>-R<sub>2</sub>) or (T<sub>b</sub>-R<sub>1</sub> &amp; T<sub>a</sub>-R<sub>2</sub>). This ambiguity was resolved by pairing targets and responses based on the minimum angular difference for the first response to one of the targets (Min(T<sub>x</sub>-R<sub>1</sub>)), and the second response to the remaining target (T<sub>~x</sub>-R<sub>2</sub>). This pairing is based on the assumption, that the first response would always be performed with higher confidence by the subject. It would be highly unlikely that for one trial subjectsâ first response would be a guess and the second response an accurate report based on correct tracking if they were able to only track one stream. Additionally, this leaves the possibility of the second response still being more accurate than the first response (by chance or actual cognitive performance). (<bold>b</bold>) Von Mises mixture model utilized in the experiments. The distribution of all precision responses (angular differences between response and target) would be the sum of responding each of the two tracked streams with each being allocated a certain cognitive resource (high/low precision). (<bold>c</bold>) The results of the Von Mises mixture models (as the pair of precision responses) is tested by Monte-Carlo-simulations. We tested, whether the parameters of the Von Mises mixture models can be estimated in artificially simulated sets of high and low precision target-response distributions based on those same parameters. The simulations confirm, that the estimated parameters reflect the ground truth of a sum of two precision distributions and not just a general inherent property of the model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91183-fig2-v2.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Results of uneven attentional resource distribution amongst two parallel color streams in experiment 1 and 2.</title><p>(<bold>a</bold>) High and low precision estimates for each of the tested subjects (n = 18) (blue dots) in experiment 1. Additionally, the results of the Monte-Carlo simulated precision estimates for each subject are displayed as two-dimensional gaussians. In general, subjects are almost twice as precise in responding to one of the streams compared to the other stream. (<bold>b</bold>) Results for experiment 2. When subjects (n = 16) have to report only one stream per trial (while still attending to both color streams), the general ratio of high and low precision responses remain. (<bold>c</bold>) The same subjects show very similar pairs of precision estimates in both experiments (irrespective of whether they have to respond to both streams sequentially or only one stream). (<bold>d</bold>) Estimated precision estimates for mixture models containing two distributions with equal deviation based on the means of each subjects estimated precisions. The estimated precision ratio of Mixture models containing two equal distributions reaches a median ratio of 1.24 and values higher than 1.928 in less than 5% of conducted simulations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91183-fig3-v2.tif"/></fig></sec><sec id="s2-2"><title>Experiment 2</title><p>The precision measures were subjected to a von Mises mixture model testing two distributions. Monte-Carlo estimates confirm that they do not differ from simulated datasets in any of the subjects (p&gt;0.972). Furthermore, the pairs of high and low precision values are found to be correlated across subjects (<italic>r</italic>=0.579, p=0.019). As in experiment 1, the relation between high and low precision estimates was analyzed via linear regression with zero intercept, which yielded a significant slope of a=1.928 (F(1,15) = 21.84, p=0.0003; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). The slope of slightly below 2 is similar to the one obtained when testing both color-streams in experiment 1, suggesting a robust subject-specific feature-based resource allocation that varies little across tasks.</p><p>In order to test computational biases of the mixture model itself potentially producing high ratios between the two precision estimates, a number of target-response distributions were created using von-Mises mixture distributions based on precision responses with equal deviation. Those distributions were based on the estimated deviations for each subject by taking the mean of the high and low precision estimates. <xref ref-type="fig" rid="fig3">Figure 3d</xref> illustrates a histogram of estimated ratios for 1000 permutations of our model if the true ratio between precisions, that is the slope, would be equal to 1. With a median of 1.24 less than 5% of the estimated ratios reach a value larger than 1.928.</p><p>Because all but two of the subjects of experiment 1 took also part in experiment 2, the difference in precision estimates between streams could be tested for consistency across experiments. The consistency was quantified by the Euclidean distances between precision estimates in experiment 1 and 2 (s=167.79) spanning a two-dimensional precision space containing high- and low precision measures. Specifically, for each subject, we determined the distance between corresponding points of high and low precision values in experiment 1 and experiment 2 (precision pairs). The distances were then summed and tested against the sums of 10,000 random permutations of the estimated precision pairs among subjects. The measured distance between experiments was significantly smaller than the distance obtained from random permutations (p=0.0363; <xref ref-type="fig" rid="fig3">Figure 3c</xref>), indicating that the ratio of precision estimates is very consistent within the same subjects across experiments.</p></sec><sec id="s2-3"><title>Experiments 3a-c</title><p>The ~2:1 ratio of precision estimates for the two color-streams suggests a strongly limited resource that entails a trade-off assignment during tracking. This could arise because subjects consistently devote more resources to one stream than the other. Another possibility is that over time the trade-off allocation changes in a dynamic fashion randomly or periodically between the attended feature values. Experiments 1 and 2 do not allow to distinguish among these possibilities, because which stream is actually attended at a given moment remains undefined, as we just probe at a random time point after the onset of the color-streams.</p><p>Experiments 3 a-c were designed to allow us to systematically assess the temporal dynamics of the attentional allocation process among the attended color streams. To this end, two critical modifications of the experimental design were introduced (<xref ref-type="fig" rid="fig1">Figure 1b</xref> â Exp3): (1) At the beginning of half of the trials the brightness of one dot group was transiently enhanced (cue-present trials), to bias subjects to attend this stream first and therefore temporally reset any time-varying process involved in resource allocation towards that stream. This ultimately enabled us to identify the cued and uncued stream at the point of report. (2) The precision estimates were systematically sampled at six fixed SOA increments starting at 2.8 sec after the brightness cue. The actual time-course of concurrent resource allocation during double color tracking is unknown yet. In order to gauge change rate optimally, we used three SOA versions (a-c) of the experiment, each run in a different group of participants (n=15). Specifically, SOA increments of 200ms, 100ms, and 40ms were run in experiment 3a, 3b, and 3c, respectively.</p><p><xref ref-type="fig" rid="fig4">Figure 4a</xref> displays the pairs of low and high precision estimates for cue-present (red) and cue-absent trials (blue) of each subject separately for experiment 3a-c. Correlations and regressions between low and high precision estimates were calculated excluding three, three and two visible outliers in each of the experiments, respectively. For experiment 3a (SOA200), low and high precision estimates were correlated for the cue-present trials (<italic>r</italic>=0.5802; p=0.048) as well as the cue-absent trials (<italic>r</italic>=0.6617; p=0.0191). The slope of these relations was similar for both factor levels (cue-present: a=1.8205 (F(1,11) = 13.124; p=0.004) | cue-absent: a=1.9145 (F(1,11) = 15.328; p=0.0024)). Hereby, using cued and uncued trials to derive high and low precision estimates yields highly similar results (d=60.6866, p&lt;0.001). In experiment 3b (SOA100), the pattern of results was comparable. High and low precision estimates were correlated for cued (<italic>r</italic>=0.6526; p=0.021) and uncued trials (<italic>r</italic>=0.5362; p=0.0589), whereby both relations exhibited a significant slope cued: a=1.8658 (F(1,11) = 15.867; p=0.0021) | uncued: a=1.9334 (F(1,11) = 10.737; p=0.0073). For this experiment, precision estimates did also not differ between cued and uncued trials (d=62.1927, p&lt;0.001). The cued trials in experiment 3c (SOA40) showed a significant relation between high and low precision estimates as well (<italic>r</italic>=0.6353; p=0.0196) with a significant ratio of a=1.6376 (F(1,12) = 19.99; p&lt;0.001). Likewise, the estimates for the uncued trials were correlated (<italic>r</italic>=0.6069; p=0.0278) with a slope of a=1.7290 (F(1,12) = 18.424 p=0.001). Both patterns of results were highly similar (d=64.1139, p=0.0012).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Results of Experiment 3.</title><p>(<bold>a</bold>) The ratio of slightly lower than 2 remains present between high and low precision estimates for experiment 3. Importantly, individual precision estimates do not vary between trials in which a luminance cue is presented and in trials in which the luminance change is absent. The cue itself does not influence the general precision estimate collapsed over tracking intervals. This holds true for all three sets of tracking intervals (SOA200 /SOA100/SOA40). (<bold>b</bold>) Precision estimate time courses with standard errors (n = 15). For cue-absent trials no systematic variation in precision estimates could be observed over time in Exp3a (SOA200) and Exp3c (SOA100). However, precision estimates changed for different tracking intervals in cue-absent trials in Exp3b (SOA100). In Exp3a (SOA200), precision estimates for the cued and uncued stream differed over time. Specifically, precision for both streams seemed to exhibit an antiphasic relation at around 1Hz. Phase differences of precision estimates between cued and uncued streams cluster around 180Â° in this case. Exp3b (SOA100) shows a very similar time-course for the precision estimates of cued and uncued streams over 2.8 sec to 3.2 sec comparable to the SOA200-experiment. Another, faster, oscillation (3.75Hz) can be additionally observed with the same anti-phasic relation between cued and uncued stream. In Exp3c (SOA40) no stream-related differences over the tested intervals can be observed.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91183-fig4-v2.tif"/></fig><p>Collapsing across time throughout experiments 3a-c, the pattern of precision estimates, including a ratio of slightly less than 2, confirms the finding of a general uneven distribution of feature-based resources so far. This ratio of consistent resource allocation does also seem to be uninfluenced by the presentation of a luminance cue shortly after onset of the feature change.</p><p>Next, precision estimates were calculated for each SOA increment (t1...t6) in trials without and in trials with a brightness cue (cue-absent, cue-present trials). For cue-present trials, precision estimates were obtained as a function of whether the cued or the uncued stream was tested. <xref ref-type="fig" rid="fig4">Figure 4b</xref> exhibits the precision time-courses for the cued and uncued stream for cue-present trials as well as the separate precision time-course for cue-absent trials in experiments 3a-c.</p></sec><sec id="s2-4"><title>Experiment 3a</title><p>Precision estimates for cue-absent trials do not show any significant variation over the 200ms SOA increments (F(5,70) = 1.676, p=0.153, e=0.107). For cue-present trials, the precision estimates over time every 200ms do not vary, as revealed by a non-significant main effect of SOA (F(5,70) = 1.487, p=0.213, e=0.096). We find, however, a significant interaction between SOA and stream (F(5,70) = 2.495, p=0.039, e=0.151) in the cue-present trials, while the main effect of stream remains not significant (F(1,14) = 0.260, p=0.618, e=0.018). Hereby, precision estimates for cued and uncued streams differed at 3.2 sec (t(14) = â2.443, p=0.028, e=0.299) and 3.6 sec (t(14) = 3.001, p=0.010, e=0.391). The morphology of the precision time courses of cued and uncued color streams in cue-present trials suggest that the cue indeed induces a stream-specific phase reset, time-locked to the brightness increment. With time, the resource allocation then shifts systematically from the cued to the uncued stream. To further investigate the relation of resource allocation between the cued and uncued stream, both time-courses were Fourier-transformed, and the absolute phase-relation between the precision time courses of the cued and uncued stream was analyzed. At around 1Hz (0.94Hz and 1.25Hz) the phase difference between the precision time-courses for the cued and uncued stream showed a maximum phase-lock of 0.35 (0.94Hz) and 0.35 (1.25Hz). Crucially, the mean phase difference between the precision time-course of the cued and uncued stream is unimodally centered around 180Â° at 0.94Hz (v(14) = 4.95, p=0.035, m=199.87Â°) as well as at 1.25Hz (v(14) = 5.05, p=0.033, m=163.09Â°).</p></sec><sec id="s2-5"><title>Experiment 3b</title><p>The precision time-courses sampled at 100ms for cue-absent trials varies consistently over SOAs (F(5,70) = 2.87, p=0.021, e=0.17) (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). For cue-present trials, precision estimates differ between the cued and uncued stream (F(1,14) = 11.095, p=0.005, e=0.442) but not over time (F(5,70) = 0.456, p=0.771, e=0.032). The interaction between stream and SOA is not significant (F(5,70) = 1.228, p=0.305, e=0.081), although precision is higher when testing the cued compared to the uncued stream from 3.1 sec onwards (3.1: t(14) = â3.103, p=0.008, e=0.407; 3.2: t(14) = â2.074, p=0.057, e=0.235; 3.3: t(14) = â2.133, p=0.051, e=0.245). Please acknowledge the overall similarity of the time-course of precision estimates of the cued and uncued stream between 2800 and 3200ms when sampling every 200ms or 100ms. Although experiment 3a and 3b were performed by a different group of subjects, and covered a different temporal range, precision measures for both streams start to diverge consistently and substantially at about 3.1 sec after the brightness cue.</p><p>The Fourier-transformed precision time-courses exhibit a consistent phase difference between the cued and uncued stream at around 3.75Hz (PLV = 0.39), with the phase difference centering around 180Â° (v(14) = 5.12, p=0.031, m=211.51Â°) (<xref ref-type="fig" rid="fig4">Figure 4b</xref>).</p></sec><sec id="s2-6"><title>Experiment 3c</title><p>For SOA increments of 40ms no systematic variation in precision responses could be observed for the cue-absent trials over time (F(5,70) = 0.695, p=0.629, e=0.047). Cue-present trials as well did not exhibit any differences in precision as a function of SOA increment (F(5,70) = 1.242, p=0.299, e=0.081), stream (F(1,14) = 3.202, p=0.095, e=0.186), or interaction of both (F(5,70) = 1.345, p=0.256, e=0.088). The phase-lock for the phase differences between the Fourier-transformed time-courses of the cued and uncued stream peaked at around 4.68Hz (PLV = 0.18). The direction for the phase-differences at that frequency of 161.10Â° failed to be show a unimodal distribution around 180Â° (v(14) = 2.59, p=0.172) or 0Â° (v(14) = â2.59, p=0.828).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The reported experiments demonstrate that subjects are generally able to track two simultaneously and independently changing color streams over time. The results, however, show that at any moment, the precision of the representation of one stream is about twice as large as that of the other stream, suggesting that the attentional resources are unevenly distributed at a ratio of ~2:1. Critically, this ratio emerges consistently across different experimental conditions, different groups of subjects, and it does not depend on whether the color-values have to be reported sequentially in the same or separate trials. Finally, we find that the resource distribution between concurring color streams is not fixed but varies over the time at a speed of around 1 Hz, with resource allocation alternating between streams in anti-phase.</p><p>The alternation rate of ~1 Hz is substantially lower than oscillatory processes reported to underlie feature tracking in other work (<xref ref-type="bibr" rid="bib34">Re et al., 2019</xref>). <xref ref-type="bibr" rid="bib34">Re et al., 2019</xref> investigated the maintenance of two static color-values and argued for an oscillatory process at around 4 Hz with no representation-specific phase-shift. However, the internal feature-representations in this study did not traverse color space such that color representations had not to be updated continuously. Here, we show that when such updating is necessary, simultaneous tracking of two color-streams involves a dynamic allocation of attentional resources that runs in antiphase. This process operates at much slower speeds, in that it takes around 500ms to turn the bias for one representation to the other. Hence, we assume that the performance fluctuation observed here is qualitatively different from the 4 Hz fluctuation reported in <xref ref-type="bibr" rid="bib34">Re et al., 2019</xref>. The latter presumably reflects a modulation driven by a faster but non-specific attentional control mechanism, which has been proposed to arise from a general central sampler, that rhythmically distributes resources towards relevant objects in a top-down fashion (<xref ref-type="bibr" rid="bib42">VanRullen, 2016</xref>; <xref ref-type="bibr" rid="bib28">Landau et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Tomassini et al., 2017</xref>). The here observed alternation of resource allocation in the 1 Hz range is also lower than attentional cycling processes documented in the spatial domain (<xref ref-type="bibr" rid="bib27">Landau and Fries, 2012</xref>; <xref ref-type="bibr" rid="bib15">Fiebelkorn et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Kienitz et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">VanRullen et al., 2007</xref>). Here, performance fluctuations were found to cycle between spatially separated objects at a rate of 4 Hz (<xref ref-type="bibr" rid="bib15">Fiebelkorn et al., 2013</xref>), which was proposed to reflect a periodic reweighting of attentional prioritization of those objects. Similar oscillatory sampling processes have been described using visual search paradigms varying in complexity (<xref ref-type="bibr" rid="bib10">DuguÃ© and Vanrullen, 2014</xref>). Ongoing perceptual and attentional mechanisms were shown to facilitate feature- and conjunction searches by sampling the relevant spatial information in a sequential manner at frequencies within the alpha and theta range, respectively (<xref ref-type="bibr" rid="bib13">DuguÃ© et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">DuguÃ© et al., 2016</xref>). Attentional engagement is hereby indicated by a phase-reset within the relevant oscillatory mechanism (<xref ref-type="bibr" rid="bib11">DuguÃ© et al., 2015</xref>). Interestingly, very low-frequency oscillations around 2 Hz have been consistently reported. However, those frequencies represent the lower bound of spectral resolution in those studies (<xref ref-type="bibr" rid="bib10">DuguÃ© and Vanrullen, 2014</xref>; <xref ref-type="bibr" rid="bib13">DuguÃ© et al., 2017</xref>). Importantly, contrary to prior studies investigating oscillatory attentional engagements, the current work does not imply a temporal regularity (oscillation) to underlie the alternation between the two relevant color value representations.</p><p>In the present framework of feature-value-based tracking, a much slower rate of alternating between two feature representations is observed which, as outlined below, may reflect (1) the speed with which attention can reweight and sharpen feature representations and (2) the specific limitations of how attention is allocated in feature space. In both cases, the temporal structure of alternation would be variable and reflect the demands of individual feature-value selection.</p><sec id="s3-1"><title>Feature attention is inherently slow</title><p>Using SSVEP recordings, it was found that a color cue biases color attention slowly towards one of two superimposed color RDKs (<xref ref-type="bibr" rid="bib2">Andersen and MÃ¼ller, 2010</xref>; <xref ref-type="bibr" rid="bib17">Forschack et al., 2017</xref>), with strongest selectivity appearing 500â600ms after cue onset. Moreover, feature attention has been shown to rely on a sequence of spatiotemporal modulations in extrastriate visual cortex, unfolding as a coarse-to-fine tuning process over a period of 200ms â 400ms (<xref ref-type="bibr" rid="bib3">Bartsch et al., 2015</xref>; <xref ref-type="bibr" rid="bib4">Bartsch et al., 2017</xref>; <xref ref-type="bibr" rid="bib5">Bartsch et al., 2021</xref>). Sharpened color selectivity (stronger tuning) is only attained late in the modulation phase. Hence, resolving the tracked colors with high precision, as required in the present experiments, entails sharpened tuning towards one or the other color, resulting in a comparably slow change rate. The temporal profile of this systematic, yet non-oscillatory, alternation of feature-based resource allocation would be modulated by the discriminability of the present color value within a non-linear color space.</p></sec><sec id="s3-2"><title>Feature-values can only be attended one at a time</title><p>A 2:1 ratio of tracking precision between streams alternating at ~1 Hz implies a strong resource limit, effectively allowing only one stream to be attended with precision at a given time. This interpretation dovetails with studies showing that subjects can only be consciously aware of â or access - one non-spatial feature value within a feature dimension at any given point in time (<xref ref-type="bibr" rid="bib23">Huang et al., 2007</xref>; <xref ref-type="bibr" rid="bib22">Huang and Pashler, 2007</xref>; <xref ref-type="bibr" rid="bib24">Huang, 2010a</xref>; <xref ref-type="bibr" rid="bib25">Huang, 2010b</xref>; <xref ref-type="bibr" rid="bib20">Houtkamp and Roelfsema, 2009</xref>). A formal account of this limitation has been put forward by the Boolean map theory (<xref ref-type="bibr" rid="bib22">Huang and Pashler, 2007</xref>), which posits that attentional selection works on a labelled Boolean map representation of the input. This sets specific limits on the way feature attention can operate. Specifically, the Boolean map is the representational format required for conscious access to feature-values, such that it makes the feature-value reportable. Such representation, however, can only be established for one feature-value at a time. Multiple objects/locations defined by different feature-values can be represented, but then, the identity of the feature-values becomes inaccessible. Hence, the attempt to access multiple feature-values in parallel engenders a sequential process (feature-by-feature selection) (<xref ref-type="bibr" rid="bib22">Huang and Pashler, 2007</xref>; <xref ref-type="bibr" rid="bib32">Morales and Pashler, 1999</xref>; <xref ref-type="bibr" rid="bib21">Huang and Pashler, 2002</xref>), that makes selection a comparably slow operation (<xref ref-type="bibr" rid="bib23">Huang et al., 2007</xref>). <xref ref-type="bibr" rid="bib32">Morales and Pashler, 1999</xref> demonstrate that symmetry judgments that are to be made based on the spatial distribution of color patches are accomplished by switching between colors. They find that RT increases from ~1200ms for two-color displays to ~2000ms for four-color displays, suggesting that the time to switch between colors takes something around 400ms, which is well in line with the alternation time seen here. The limitation of being able to access only one feature-value at a time, may account for the alternation in accessing the color streams when tracking over time. Research investigating object-based tracking of different features (<xref ref-type="bibr" rid="bib6">Blaser et al., 2000</xref>) revealed that changing features of one objects can be easily tracked, whereas the simultaneous tracking of features of different objects results in poor performance. Importantly, analysis of the response pattern suggested that subjects did not divide but switched attention between objects when attempting to track the features of two objects simultaneously.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>All experiments were approved by the ethics-commission of the Otto-von-Guericke University (no. 141/20). All participants gave written informed consent and consent to publish prior to their participation.</p><sec id="s4-1"><title>Subjects</title><p>Eighteen subjects (15 female/ 3 male) with a mean age of 30.5 (SD = 7.09) participated in the first study. None of the subjects reported any psychological or neurological disorders and had normal or corrected-to-normal vision. All participants additionally confirmed correct color-perception. Subjects were monetarily compensated for their participation.</p></sec><sec id="s4-2"><title>General stimulus material</title><p>All stimulus material was generated using the Psychtoolbox for Matlab. Subjects were presented with a circular annulus with an outer radius of 5.66Â° consisting of two clouds of 900 colored dots each (0.2Â° in diameter), creating the perception of two superimposed objects occupying the same location (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The two objectsâ colors changed continuously throughout each trial.</p><p>The color space of the utilized monitor (Asus VG248QE) was calibrated using the measurements of a colorimeter (SpectroCAL, Cambridge Research Systems) from which the device-specific chromaticity space was calculated. The actual hues for the experiment were drawn from the normalized CIELUV space defined as a circle centered on the white point in a luminance plain of that space (u=0.1978, v=0.4683, 14.5 cd/m<sup>2</sup>). The circle subtended a radius (i.e. chromaticity) of c=0.0576 for a standard sRGB gamut under D65 illumination. Thus, color values within this 1-dimensional circular space varied continuously in hue (from 0Â° to 360Â°) while being matched for luminance and chroma.</p><p>During each trial the colors of both dot objects moved continuously through the circular hue space independently of each other at a speed of 1Â°/frame. Those color trajectories were calculated offline in the following manner: Each individual trajectory started at an initial random angular position, subsequently moving with 1Â°/frame for 480 frames (8 sec) along a unit circle. At random intervals in between 30 and 100 frames (0.5 sec â 1.66 sec) the direction along the circle could revert, creating random, unpredictable but continuous movements within the circular hue space. Sets of several hundred trajectories were calculated and subsequently paired into two color streams for each trial. The only criterion for each pairing was the maintenance of a minimum distance between both trajectories of 60Â° at each frame to avert a perceptual confusion of both colors. This strategy was employed to avoid any direct interdependence between the histories of each of the stream pairs, since although colors would never cross (always larger than 60Â°), the actual angular distance at which one or the other stream would revert direction was entirely unpredictable.</p><p>At the initial generation of the dot clouds at the beginning of each trial, the order of presentation of individual overlapping dots was randomized in order to avoid creating a perception of depth with one object being located in front of the other.</p><p>Since cognitive processes maintaining solely feature-based information are being investigated, one goal was to control for any spatial information that could potentially be utilized by the subject during the feature-based tracking of the two streams. First, the annulus has an inner radius of 2.20Â°, restricting the primary task to peripheral vision only. Furthermore, a strategy of âdot-flippingâ was employed: At every frame throughout the continuous hue change, 10 random dots of both streams interchanged identity (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), therefore removing any local spatial cues that might form during tracking, while leaving the color features (hue) and the amount of color information (900 dots each) intact.</p></sec><sec id="s4-3"><title>Stimulus analysis</title><p>In order to ensure that subjects were not able to predict or report object colors solely based on stimulus statistics, descriptions for the color streams within each trial across the entire length of 480frames (8 sec) for all 150 utilized trials were calculated. This includes the average amount of the combined hue-reversals amongst the two color-streams per trial as well as the overall distribution of hue distances between streams. Additionally, the amount of intersection between all the hue-values of the two streams should give an indicator of categorical separation of streams within hue-space for each trial. Next, the distributions of target colors (last presented hue before recall) for both streams were tested against a uniform distribution using Kolmogorov-Smirnov tests within each subject to investigate any potential bias for specific colors subjects have to report on. Note, that the distribution of actual target colors was specific for each subject, since although the same 150 pairs of color streams were used for all subjects, trial-length (the point at which hue-change seized and objects had to be reported on) varied randomly between 6â8 sec. Similarly, the distribution of hue-distances between the two target-hues of each trial were tested for uniformity for each subject using Kolmogorov-Smirnov tests.</p><p>On average 8.79 combined reversals (SD = 2.93) within the two color-streams were introduced randomly across the 8 sec timeframe. Such a reversal occurred on average after 0.7974 sec with a large variation of SD = 0.8228 sec, making them highly unpredictable. Across all trials and time points the angular distance between the two simultaneous color features was mostly evenly distributed between angles of 90Â° to 180Â° with the distances ranging between 60Â° and 90Â° being slightly less represented. This is to be expected with 60Â° being the lowest possible distance within hue-space the two color-streams could approach at any given time. Despite the introduction of a minimum possible inter-stream distance, absolute hue values between the streams were still fairly well shuffled across trial, in that a large portion of all hue-values of the two streams intersected for each trial (M=40.8%, SD = 32.8%).</p><p>The target hue-values (target colors at the time of report) were tested against a uniform distribution for each subject. All 300 targets were evenly distributed across the circular hue space in all but one subject (p=0.029), in which slightly more greenish (~180Â°) and purple (~240Â°) hues were present as targets. In all other 17 subjects, target hues were not different from a uniform distribution (0.335&lt;p&lt;0.993). Hue-distances between the two streams at the time of report exhibited an even distribution in between 90Â° and 180Â° degree in all (0.052&lt;p&lt;0.832) but two subjects (p=0.018, p=0.024), making statistical inferences for one of the target color based on the other highly unlikely.</p></sec><sec id="s4-4"><title>Experiment 1</title><sec id="s4-4-1"><title>Procedure</title><p>Subjects were placed in a dark shielded chamber with a 24â FullHD LED monitor (Asus VG248QE) placed 70 cm in front of them. Throughout the experiment a fixation dot (0.12Â° diameter) was present and subjects were required to maintain fixation for most of the paradigm. At the start of each trial two superimposed colored cloud objects appeared on a black background and continuous hue changes in both objects were introduced as outlined above. Subjects were asked to attend to both color streams simultaneously as accurate as possible throughout the hue change. The trial continued for 6â8 sec, at which point both streamsâ colors changed to an isoluminant uniform grey ([72 72 72] RGBâ¦white point of used color spaceâ¦devoid of any chroma). With the removal of the hue information, a colored ring (color-wheel) appeared within the inner diameter of the annulus (outer radius: 1.36Â°, inner radius: 0.95Â°) representing the entire available 1D-circular hue space as well as a white cursor (0.07Â°) placed at a random position on the edge of that circle (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Now subjects had to perform a precision estimation task by reporting the last perceived color for both streams (T<sub>a</sub> and T<sub>b</sub>) as precisely as possible by moving the cursor along the colored ring using a manual dial (SpaceMouse Compact, 3Dconnexion). The cursor position was recorded as response as soon as the top of the dial was pressed (R<sub>1</sub>). A second response was subsequently recorded the same way (R<sub>2</sub>). As soon as two consecutive color estimation responses were entered, feedback was given for two seconds by highlighting two additional cursor positions (grey) at the two target colors (T<sub>a</sub> and T<sub>b</sub>). After another 1 sec of blank screen the next trial started. Overall 150 trials were presented.</p></sec><sec id="s4-4-2"><title>Data analysis</title><p>The precision of the individualsâ responses was operationalized as the standard deviation of the angular differences between target color and reported color (T-R). Assuming that continuously changing values within one feature dimension (color) are maintained by separate representations drawing on independent cognitive resources analogous to various fixed and variable slot or precision models in working memory (<xref ref-type="bibr" rid="bib40">van den Berg et al., 2012</xref>; <xref ref-type="bibr" rid="bib31">Merkel et al., 2021</xref>), the overall distribution of angular differences for each subject in experiment 1 should be a mixture of two separate precision distributions. Each one with a specific deviation, quantifying the amount of allocated feature-based resource for one of the two color streams, respectively.</p><p>During each trial, subjects had to perform two reports (one for each target color). Each of the two responses are potentially associated with one of the two color streams since both are equally relevant, resulting in two ambiguous target-response allocations (and therefore 2 pairs of precision responses) for each trial ((T<sub>a</sub>-R<sub>1</sub> &amp; T<sub>b</sub>-R<sub>2</sub>) or (T<sub>b</sub>-R<sub>1</sub> &amp; T<sub>a</sub>-R<sub>2</sub>)). Therefore, assumptions have to be made about the subjectsâ intentions regarding the two given responses towards the two target colors. The post-hoc target-response assignments are non-trivial since both target values are equally relevant and their distance towards each other varies across trials. Additionally, response precision varies across trials and targets such that a higher resource allocation towards a specific target value can lead to a few poor responses due to natural fluctuations across trials. The pair of precision responses entered into the analysis was selected based on the minimum angular difference for the first response with any of the two targets (Min(T<sub>x</sub>-R<sub>1</sub>) &amp; (T<sub>~x</sub>-R<sub>2</sub>)) assuming that subjects performed the first report for one of the color targets usually with higher confidence and accuracy (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Thus, the first response was paired with whichever of the two targets was closest to that first response and the second response was paired with the remaining target. Note, that this assignment still allows for the accuracy (target-response distance) of the second response to be higher than the accuracy of the first response in any given trial. As a control, responses were paired with targets based on the overall minimum angular difference in one trial irrespective of the order of the given response (Min(T<sub>x</sub>-R<sub>y</sub>) &amp; (T<sub>~x</sub>-R<sub>~y</sub>)) yielding however the same pairings for most of the trials (94.78%+â3.03%).</p><p>The resulting response distribution for each subject containing 300 angular response differences (2 per trial) ranging from â180Â° to +180Â° was fitted with a mixture model of two von Mises distributions using an expectation maximization algorithm. Parameter estimation was restricted for each von Mises distribution being centered on 0Â°(+â10Â° to compensate for angular biases generated by random noise). Furthermore, each of the von Mises distribution was restricted to explain exactly 50% of the entire model, since 150 of the 300 angular differences are related to the responses towards one of the two color targets. The actual precision of the responses and therefore the precision of tracking for the two color-streams were modeled by the estimate of the standard deviations (sqrt(1/kappa)) for the two von Mises distributions (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p><p>In order to validate the two precision estimates for each subject, Monte-Carlo simulations based on 1000 response distributions were performed. The motivation was to confirm that performing that particular tracking task with a specific pair of precision values (or ground truth) causes our mixture-model to estimate that exact pair of precision values. Two sets of 150 target-response differences were calculated by taking the target angles of a particular subjects and adding response angles based on random draws of precision responses from two von Mises distribution with the previously estimated standard deviations for that subject. Thus, each of the 1000 simulations per subject yields a pair of precision estimates, for a theoretical observer who responds to each pair of targets with two responses drawn from independent precision distributions (ground truth) with the observed standard deviations.</p><p>The simulation thus creates a two-dimensional null-distribution for any given pair of precision parameters. This null-distribution was modeled for each subject using a two-dimensional (4-parameter) Gaussian (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). The position of the observed pair of precision parameters within the null-distribution determined therefore the deviation from the proposed model (significance p).</p><p>For each subject the pair of precision estimates could be ordered into a low and high precision value. Those were subsequently correlated to test whether there is an underlying relation between the amounts of resources allocated to multiple feature-based representations across subjects.</p></sec></sec><sec id="s4-5"><title>Experiment 2</title><sec id="s4-5-1"><title>Stimulus and procedure</title><p>Sixteen of the subjects participating in the first experiment took part in experiment 2 (14 female / 2 male, mean age 30.69 (SD = 7.53)). The current control experiment did not differ from the first paradigm except for one crucial point: At the end of each hue-changing phase of each trial, only one of the two streams changed its color to an isoluminant grey [72, 72, 72] while the other stream maintained the last hue of the color stream (<xref ref-type="fig" rid="fig1">Figure 1b</xref> â Exp2). At that point a color wheel was presented, as in the previous study, and subjects were required to move a cursor along that wheel using a manual dial to indicate the last perceived color of the now grey stream. Following a confirming press of the dial, subjects received feedback through a second cursor appearing in grey at the actual target-color location on the color wheel.</p><p>The same set of trajectories was used as in the previous study to generate the hue-changes. However, the length of each trial was again drawn from a uniform random distribution between 6 and 8 sec. A total of 150 trials were presented and for each of them an unambiguous target-response difference for the report of the last perceived color of the grey target-stream was recorded.</p></sec><sec id="s4-5-2"><title>Data analysis</title><p>The distribution of all 150 angular target-response differences was modeled to be derived from the representational precision of two separate feature-based cognitive resources. Hereby during each trial exactly one of the two resources (allocated to the two color-streams) are probed. The association of the reported color stream and one of the two resources cannot be known and is random. Therefore, across the experiment it is to be expected that around 50% of each of the two resources is going to be a target. A mixture model of two von Mises distributions was used to estimate the precision of maintaining each of the two feature representations and their allocated resources over time. Models were restricted in that each had to explain 50% of the variance of the data. Furthermore, both von Mises distributions were centered on 0Â°(+â10Â°). Cognitive resources allocated to each of the two feature representations were quantified as the estimated standard deviations (sqrt(1/kappa)) of the two fitted von Mises distributions.</p><p>Those precision estimates were again validated using Monte-Carlo simulations with the same logic as in the first experiment. For those simulations, 1000 distributions of 150 target-response differences were created from the sum of the actual target-colors for the particular subject and 75 random draws from two different von Mises distributions with the previously estimated standard deviations for that subject. Pairs of precision estimates from fitting the mixture model to each of the simulated target-response distributions constituted 2-dimensional null-distributions of precision values for each subject and, as in the previous analysis, determine the significance of the observed precision values for each subject. The relation between high and low precision values across subject was quantified using linear regression.</p></sec></sec><sec id="s4-6"><title>Experiment 3</title><sec id="s4-6-1"><title>Subjects</title><p>Forty-five subjects, who did not take part in any of the previous studies and were naÃ¯ve to the task, participated in the current set of experiments. For three iterations of the experiment in which different SOAs between color change onset and response were introduced, fifteen different subjects each participated. For the SOA-200ms version (Exp3a), mean age of the subjects was 25.07 (SD = 4.83) with eleven females and four males participating. Exp3b in which duration between color onset and response varied in steps of 100ms, nine females and 6 males participated with mean age of all subjects being 23.60 (SD = 2.64). For the 40 ms version (Exp3c), another nine females and six males with a mean age of 23.60 (SD = 2.77) participated.</p></sec><sec id="s4-6-2"><title>Stimulus and procedure</title><p>An annulus stimulus consisting of two overlapping set of dots was used with the same visual properties as in the previous set of experiments. Each trial started out with the hue of both streams being uniform grey ([72 72 72]) for 1â1.5 sec. Next, the two sets of dots appeared in different hues, which subsequently moved through color space. New pairs of trajectories along the hue circle were calculated offline with the same constraints as in the previous experiments. After the period of hue change, one of the targets turned grey again, at which point the color-wheel within the annulus appeared and subjects had to report the last perceived hue of the probed stream as precisely as possible using a dial.</p><p>Crucially, in the current design, the duration of the motion through hue space until one of the target streams was probed (turned grey) was introduced as an additional factor and varied along six SOAs (<xref ref-type="fig" rid="fig1">Figure 1b</xref> â Exp3). This way, the resource distribution amongst the two streams (as quantified by the precision estimates for the last perceived hue of the probed stream) can be analyzed as a function of a systematic SOA-variation between the color-change onset and response. As potentially relevant time-varying processes could be located at different frequencies within the spectral domain, three separate paradigms were conducted with 15 subjects each. Hereby, the factor âtimeâ had different sets of levels for each iteration of the experiment, potentially covering different spectra of precision-variation. In the SOA-40-version of the experiment, the target-hue was probed at 3.00,3.04,3.08,3.12,3.16 and 3.20 sec after hue-onset. The theoretical spectral limit for resolving any meaningful process can hereby by considered 5Hz-12.5Hz, given the Nyquist-frequency determined by the sampling (40ms) as well as the overall time-range (200ms). For the SOA-100-version, change-durations were spaced at 100ms intervals starting from 3 sec (3.0,3.1,3.2,3.3,3.4,3.5) with a frequency limit of 2Hz-5Hz. Finally in the SOA-200-version, the âtimeâ-levels spanned 1 sec from 3 to 4 sec (3.0,3.2,3.4,3.6,3.8,4.0) with a spectral limit of 1Hz-2.5Hz. Not one subject performed in different iterations of the same task.</p><p>Additionally, in half the trials a brief luminance change (cued trials) was introduced (+15 RGB) in one of the two color streams 200ms after hue-onset for 67ms. In the other half of trials that were uncued, no such luminance change occurred in any of the color streams. Within the cued trials, either the stream in which the cue appeared was probed at the end of the trial (cued stream) or the stream that was not cued (uncued stream). Both conditions in the cued trials were counterbalanced. Thus, the luminance cue itself did not carry any information about which stream was to be probed. The intention of the cue was to shift feature-based attention towards one of the two streams and introduce an object-specific phase-reset of the process involved in maintaining feature representations time-locked to the cue. Precision can now be recorded for reporting either the cued or uncued stream within the cued trials. Importantly, the luminance cue, while potentially modulating the temporal structure of the precision responses, is not suspected to change the average precision responses across the entire tested time range.</p><p>In order to gather enough responses to estimate reliable precision values for cued and uncued streams across all six timepoints, the experiment was partitioned for each subject into three separate sessions on 3 successive days. Across sessions, overall 36 trials per timepoint and stream (cued stream / uncued stream) in the cued trials and 72 trials per timepoint in the uncued trials were presented for a total of 864 trials. Subjectsâ task was to keep track of the two color-streams as precisely as possible and report the color of the probed (grey) stream at the end of each trial using a dial. They were told to not focus on any other aspect of the task. The appearance or significance of the luminance change was specifically not addressed by the experimenter throughout the sessions, but subjects were encouraged to simply ignore anything that did not concern their primary task if they mentioned it.</p></sec><sec id="s4-6-3"><title>Analysis</title><p>First, using von Mises mixture models, two pairs of precision estimates were calculated for each subject across SOAs and streams for the cued and uncued trials separately. For both trial-types (cued/uncued), the distribution of 432 target-response angular differences was fitted with the mixture model of two von-Mises distributions. The resulting deviation estimates quantified the low and high resource precisions for the trials in which one stream was cued and trials in which none of the streams were cued. This first analysis was used to replicate the previous finding of a general ratio of resource distribution amongst two relevant features within one dimension. In order to test, whether this ratio of resource allocation was altered by the appearance of the luminance-cue, the consistency of precision pairs for cued and uncued trials within each subject was determined using the sum of Euclidean distances between precision estimates for cued and uncued trials across subjects (see Exp2). This analysis was performed for each iteration of the experiment separately (SOA40, SOA100, SOA200). Linear regression models without intercept were used to quantify the ratio between high and low precision pairs for cued and uncued trials.</p><p>Next, the time-course of response precision for the cued and uncued trials for each subject was estimated by fitting a half-normal distribution to the absolute target-response differences at each timepoint of the uncued trials and at each timepoint for the cued stream and uncued stream of the cued trials. The half-normal function was used to disregard negligible counter-/clockwise hue response biases and improve the fit. Note, that no mixture-model was used in this analysis, as the cue allows for an identification and therefore unambiguous separation of the streams. The resulting standard deviations were z-normalized to account for large variations in general performance across subjects. Using multi-factorial rANOVAs, variations in response precision across trials(cued/uncued), streams(cued/uncued) and crucially SOA(t1...t6) were analyzed for the different iterations of the experiment (SOA40/SOA100/SOA200). rANOVAs were adjusted for multiple comparisons using the Greehouse-Geisser correction.</p><p>Finally, precision time-courses (along SOAs) for the uncued trials as well as the cued and uncued streams of the cued trials were Fourier-transformed. To analyze systematic precision variations over time for the two continuously maintained color streams, phase-differences were calculated between cued and uncued streams of the cued trials and tested for non-uniformity against 0Â° (in-phase) and 180Â° (counter-phase) using v-tests at frequencies exhibiting peak phase-lock.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con3"><p>Data curation, Formal analysis, Investigation</p></fn><fn fn-type="con" id="con4"><p>Supervision, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Supervision, Funding acquisition, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experiments were approved by the ethics-commission of the Otto-von-Guericke University (no. 141/20). All participants gave written informed consent and consent to publish prior to their participation.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91183-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All raw data the analyses are based on as well as analysis scripts are available online via the <ext-link ext-link-type="uri" xlink:href="https://osf.io/y3qst/">OSF repository</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Merkel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Multiple feature-value tracking</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/y3qst/">y3qst</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The current study was supported by the Deutsche Forschungsgemeinschaft grant SFB1436/B05.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Franconeri</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>How many objects can you track? Evidence for a resource-limited attentive tracking mechanism</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/7.13.14</pub-id><pub-id pub-id-type="pmid">17997642</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>SK</given-names></name><name><surname>MÃ¼ller</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Behavioral performance follows the time course of neural facilitation and suppression during cued shifts of feature-selective attention</article-title><source>PNAS</source><volume>107</volume><fpage>13878</fpage><lpage>13882</lpage><pub-id pub-id-type="doi">10.1073/pnas.1002436107</pub-id><pub-id pub-id-type="pmid">20643918</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartsch</surname><given-names>MV</given-names></name><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Stoppel</surname><given-names>CM</given-names></name><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Determinants of global color-based selection in human visual cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>2828</fpage><lpage>2841</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu078</pub-id><pub-id pub-id-type="pmid">24770709</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartsch</surname><given-names>MV</given-names></name><name><surname>Loewe</surname><given-names>K</given-names></name><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention to color sharpens neural population tuning via feedback processing in the human visual cortex hierarchy</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>10346</fpage><lpage>10357</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0666-17.2017</pub-id><pub-id pub-id-type="pmid">28947573</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartsch</surname><given-names>MV</given-names></name><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attention expedites target selection by prioritizing the neural processing of distractor features</article-title><source>Communications Biology</source><volume>4</volume><elocation-id>814</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-021-02305-9</pub-id><pub-id pub-id-type="pmid">34188169</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blaser</surname><given-names>E</given-names></name><name><surname>Pylyshyn</surname><given-names>ZW</given-names></name><name><surname>Holcombe</surname><given-names>AO</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Tracking an object through feature space</article-title><source>Nature</source><volume>408</volume><fpage>196</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1038/35041567</pub-id><pub-id pub-id-type="pmid">11089972</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busch</surname><given-names>NA</given-names></name><name><surname>Dubois</surname><given-names>J</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The phase of ongoing EEG oscillations predicts visual perception</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>7869</fpage><lpage>7876</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0113-09.2009</pub-id><pub-id pub-id-type="pmid">19535598</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>P</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Tracking multiple targets with multifocal attention</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>349</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.05.009</pub-id><pub-id pub-id-type="pmid">15953754</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>WY</given-names></name><name><surname>Howe</surname><given-names>PD</given-names></name><name><surname>Holcombe</surname><given-names>AO</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Resource demands of object tracking and differential allocation of the resource</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>75</volume><fpage>710</fpage><lpage>725</lpage><pub-id pub-id-type="doi">10.3758/s13414-013-0425-1</pub-id><pub-id pub-id-type="pmid">23359355</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuguÃ©</surname><given-names>L</given-names></name><name><surname>Vanrullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The dynamics of attentional sampling during visual search revealed by Fourier analysis of periodic noise interference</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1167/14.2.11</pub-id><pub-id pub-id-type="pmid">24525262</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuguÃ©</surname><given-names>L</given-names></name><name><surname>Marque</surname><given-names>P</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Theta oscillations modulate attentional search performance periodically</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>945</fpage><lpage>958</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00755</pub-id><pub-id pub-id-type="pmid">25390199</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuguÃ©</surname><given-names>L</given-names></name><name><surname>Roberts</surname><given-names>M</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Attention reorients periodically</article-title><source>Current Biology</source><volume>26</volume><fpage>1595</fpage><lpage>1601</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.04.046</pub-id><pub-id pub-id-type="pmid">27265395</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuguÃ©</surname><given-names>L</given-names></name><name><surname>Xue</surname><given-names>AM</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Distinct perceptual rhythms for feature and conjunction searches</article-title><source>Journal of Vision</source><volume>17</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.1167/17.3.22</pub-id><pub-id pub-id-type="pmid">28362897</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Humphreys</surname><given-names>G</given-names></name><name><surname>Ward</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Competitive brain activity in visual attention</article-title><source>Current Opinion in Neurobiology</source><volume>7</volume><fpage>255</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(97)80014-1</pub-id><pub-id pub-id-type="pmid">9142748</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiebelkorn</surname><given-names>IC</given-names></name><name><surname>Saalmann</surname><given-names>YB</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rhythmic sampling within and between objects despite sustained attention at a cued location</article-title><source>Current Biology</source><volume>23</volume><fpage>2553</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.10.063</pub-id><pub-id pub-id-type="pmid">24316204</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiebelkorn</surname><given-names>IC</given-names></name><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dynamic interplay within the frontoparietal network underlies rhythmic spatial attention</article-title><source>Neuron</source><volume>99</volume><fpage>842</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.038</pub-id><pub-id pub-id-type="pmid">30138590</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forschack</surname><given-names>N</given-names></name><name><surname>Andersen</surname><given-names>SK</given-names></name><name><surname>MÃ¼ller</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Global enhancement but local suppression in feature-based attention</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>619</fpage><lpage>627</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01075</pub-id><pub-id pub-id-type="pmid">27897668</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franconeri</surname><given-names>SL</given-names></name><name><surname>Jonathan</surname><given-names>SV</given-names></name><name><surname>Scimeca</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Tracking multiple objects is limited only by object spacing, not by speed, time, or capacity</article-title><source>Psychological Science</source><volume>21</volume><fpage>920</fpage><lpage>925</lpage><pub-id pub-id-type="doi">10.1177/0956797610373935</pub-id><pub-id pub-id-type="pmid">20534781</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukuda</surname><given-names>K</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Discrete capacity limits in visual working memory</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>177</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.03.005</pub-id><pub-id pub-id-type="pmid">20362427</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houtkamp</surname><given-names>R</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Matching of visual input to only one item at any one time</article-title><source>Psychological Research</source><volume>73</volume><fpage>317</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1007/s00426-008-0157-3</pub-id><pub-id pub-id-type="pmid">18665392</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Symmetry detection and visual attention: a âbinary-mapâ hypothesis</article-title><source>Vision Research</source><volume>42</volume><fpage>1421</fpage><lpage>1430</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(02)00059-7</pub-id><pub-id pub-id-type="pmid">12044748</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A Boolean map theory of visual attention</article-title><source>Psychological Review</source><volume>114</volume><fpage>599</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.3.599</pub-id><pub-id pub-id-type="pmid">17638498</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Treisman</surname><given-names>A</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Characterizing the limits of human visual awareness</article-title><source>Science</source><volume>317</volume><fpage>823</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1126/science.1143515</pub-id><pub-id pub-id-type="pmid">17690299</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>Characterizing the nature of visual conscious access: the distinction between features and locations</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>24</elocation-id><pub-id pub-id-type="doi">10.1167/10.10.24</pub-id><pub-id pub-id-type="pmid">20884489</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>The speed of feature-based attention: attentional advantage is slow, but selection is fast</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>36</volume><fpage>1382</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1037/a0018736</pub-id><pub-id pub-id-type="pmid">20718572</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kienitz</surname><given-names>R</given-names></name><name><surname>Schmiedt</surname><given-names>JT</given-names></name><name><surname>Shapcott</surname><given-names>KA</given-names></name><name><surname>Kouroupaki</surname><given-names>K</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Schmid</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Theta rhythmic neuronal activity and reaction times arising from cortical receptive field interactions during distributed attention</article-title><source>Current Biology</source><volume>28</volume><fpage>2377</fpage><lpage>2387</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.05.086</pub-id><pub-id pub-id-type="pmid">30017481</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>AN</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Attention samples stimuli rhythmically</article-title><source>Current Biology</source><volume>22</volume><fpage>1000</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.03.054</pub-id><pub-id pub-id-type="pmid">22633805</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>AN</given-names></name><name><surname>Schreyer</surname><given-names>HM</given-names></name><name><surname>van Pelt</surname><given-names>S</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distributed attention is implemented through theta-rhythmic gamma modulation</article-title><source>Current Biology</source><volume>25</volume><fpage>2332</fpage><lpage>2337</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.07.048</pub-id><pub-id pub-id-type="pmid">26279231</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The capacity of visual working memory for features and conjunctions</article-title><source>Nature</source><volume>390</volume><fpage>279</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1038/36846</pub-id><pub-id pub-id-type="pmid">9384378</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Stoppel</surname><given-names>CM</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Heinze</surname><given-names>H-J</given-names></name><name><surname>Hopf</surname><given-names>J-M</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spatio-temporal patterns of brain activity distinguish strategies of multiple-object tracking</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>28</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00455</pub-id><pub-id pub-id-type="pmid">23915053</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Bartsch</surname><given-names>MV</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Vellage</surname><given-names>A-K</given-names></name><name><surname>MÃ¼ller</surname><given-names>NG</given-names></name><name><surname>Hopf</surname><given-names>J-M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A direct neural measure of variable precision representations in visual working memory</article-title><source>Journal of Neurophysiology</source><volume>126</volume><fpage>1430</fpage><lpage>1439</lpage><pub-id pub-id-type="doi">10.1152/jn.00230.2021</pub-id><pub-id pub-id-type="pmid">34550022</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morales</surname><given-names>D</given-names></name><name><surname>Pashler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>No role for colour in symmetry perception</article-title><source>Nature</source><volume>399</volume><fpage>115</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/20103</pub-id><pub-id pub-id-type="pmid">10335840</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pylyshyn</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The role of location indexes in spatial perception: A sketch of the FINST spatial-index model</article-title><source>Cognition</source><volume>32</volume><fpage>65</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(89)90014-0</pub-id><pub-id pub-id-type="pmid">2752706</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Re</surname><given-names>D</given-names></name><name><surname>Inbar</surname><given-names>M</given-names></name><name><surname>Richter</surname><given-names>CG</given-names></name><name><surname>Landau</surname><given-names>AN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Feature-based attention samples stimuli rhythmically</article-title><source>Current Biology</source><volume>29</volume><fpage>693</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.01.010</pub-id><pub-id pub-id-type="pmid">30744973</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Object-based attention involves the sequential activation of feature-specific cortical modules</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>619</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1038/nn.3656</pub-id><pub-id pub-id-type="pmid">24561999</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scolari</surname><given-names>M</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perceptual expertise enhances the resolution but not the number of representations in working memory</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>15</volume><fpage>215</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.3758/pbr.15.1.215</pub-id><pub-id pub-id-type="pmid">18605506</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shim</surname><given-names>WM</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Jiang</surname><given-names>YV</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatial separation between targets constrains maintenance of attention on multiple objects</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>15</volume><fpage>390</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.3758/pbr.15.2.390</pub-id><pub-id pub-id-type="pmid">18488657</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>K</given-names></name><name><surname>Meng</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>K</given-names></name><name><surname>Luo</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Behavioral oscillations in attention: rhythmic Î± pulses mediated through Î¸ band</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4837</fpage><lpage>4844</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4856-13.2014</pub-id><pub-id pub-id-type="pmid">24695703</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomassini</surname><given-names>A</given-names></name><name><surname>Ambrogioni</surname><given-names>L</given-names></name><name><surname>Medendorp</surname><given-names>WP</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Theta oscillations locked to intended actions rhythmically modulate perception</article-title><source>eLife</source><volume>6</volume><elocation-id>e25618</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.25618</pub-id><pub-id pub-id-type="pmid">28686161</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Berg</surname><given-names>R</given-names></name><name><surname>Shin</surname><given-names>H</given-names></name><name><surname>Chou</surname><given-names>WC</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Variability in encoding precision accounts for visual short-term memory limitations</article-title><source>PNAS</source><volume>109</volume><fpage>8780</fpage><lpage>8785</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117465109</pub-id><pub-id pub-id-type="pmid">22582168</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The blinking spotlight of attention</article-title><source>PNAS</source><volume>104</volume><fpage>19204</fpage><lpage>19209</lpage><pub-id pub-id-type="doi">10.1073/pnas.0707316104</pub-id><pub-id pub-id-type="pmid">18042716</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual cycles</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>723</fpage><lpage>735</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.07.006</pub-id><pub-id pub-id-type="pmid">27567317</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91183.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2023.09.12.557201" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2023.09.12.557201"/></front-stub><body><p>This study presents an important finding on how human observers keep track of continuously changing feature values across two different streams but within the same dimension. The conclusion about the serial attentional resource allocation during parallel feature value tracking is informative to understand the function of visual cortical systems. The experimental evidence supporting the conclusion is convincing.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91183.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>South China Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2023.09.12.557201">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.09.12.557201v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Serial attentional resource allocation during parallel feature value tracking&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Xiling Zhang as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I suggest that the authors first explain the null difference between cued and un-cued conditions in Exp. 3s, and clarify how unbiased and adequate assign response 1 and response 2 to target 1 and target 2, respectively.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. I wonder if the classification of two possible target-response distances in Experiment 1 is a valid approach. The manipulation in Experiment 2 by randomly testing one of the two color streams seems better, but it is not clear how to separate the high-precision and low-precision distributions can be separated from a mixture distribution.</p><p>2. A ratio of 2:1 seems to be quite robust across different experiment setups. We need to be careful to see if this does not reflect a bias due to computational processes. Perhaps some simulations would help confirm it.</p><p>3. In Experiment 3a-c, the authors performed several versions of the experiment with different SOAs. The consistent phase difference between the cued and un-cued streams also varied with SOA (~1Hz for 200-ms SOA, 3.75 Hz for 1o0-ms SOA; 4.68 Hz for 40-ms SOA). It is not clear to me why the authors only made a conclusion based on 1Hz oscillation between two streams.</p><p>4. The authors should discuss more about the functional significance underlying the observation of &quot;unequally distributed attentional resources that alternated at 1 Hz&quot; with two attended feature values.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Below I am describing my concerns in more detail and making suggestions of how to address some of them.</p><p>1) The task instructs participants to &quot;keep track&quot; of two colored dot clouds and asks them to report the color after about 6-8 seconds on each trial. Certainly, there is some uncertainty with regards to when color is probed, yet it seems to me that participants can easily enough start attending to the color shortly before the test probe at the end (so, after about 6s), to be able to report the colors. Thus, the task itself does not truly require participants to track color values continuously for several seconds. The task can be performed by starting to pay attention to the colors only at the very end, shortly before the test probe (at least in Exp. 1 and 2). Also, as participants choose which color to report, it is not necessary for them to know which stream had which color â so &quot;swaps&quot; might be happening all the time; based on Exp. 1 it's entirely unclear which color stream they are intending to report.</p><p>Overall, this task design is very different from spatial attention tracking tasks that require participants to track target objects among identical distractors. The color tracking task in the current study tries to establish parallels between the spatial tracking literature, but it seems not clear at all where the parallels are given the differences in tasks. Spatial tracking tasks (multiple-object tracking, i.e.,) require selective and continuous tracking (e.g., Stormer et al., 2014), which is not true on the current color task that asks participants to report which colors they just saw.</p><p>Thus, it is unclear whether or to what extent this task tests &quot;tracking&quot; vs. simply perceiving and briefly remembering two colors once the test probe comes on, more akin to a visual working memory task with a short delay.</p><p>One potential additional analysis that could â at least to some extent â get at this might be to examine the response errors in relation to not just the very last color, but previous colors of that stream and examine recency effects/biases in people's color reports. If people are continuously tracking each color stream you'd expect relatively strong recency biases, consistent with &quot;serial dependence&quot; (Fischer and Whitney, 2014). Another paper on visual working memory for continuously color changing objects also seems relevant: Chung, Schurgin, Brady, 2023.</p><p>2) This relates to my previous point that questions the task itself but relates more directly to the main claim of a limited resource. Given participants are asked to report the color at the very end of the trial, this measure â in my view â might well reflect memory and decision-related processes, rather than attentional allocation. How can we tease these processes apart in the current task design? In some ways, it seems to me, that the probe at the end of the trial forces people to prioritize one color over the other, so the limits the authors are claiming might all happen at the response stage and not during tracking/viewing.</p><p>3) I am not sure I follow the logic of the correlation analysis and interpretation; what they seem to show is that the first and second responses are correlated, but I am not sure how we can conclude from this pattern that there is competition between attending to the two streams. Such a model would predict that a response on one trial trades off with the response on that same trial in particular, no? So, shouldn't these trade-offs be measured at a single trial level, rather than accuracy across people for two responses? It seems to me these correlations are consistent with a much simpler explanation that considers overall differences in performance between people (e.g., those people that have the best high-precision responses also have the best low-precision responses â thus, are overall more precise than other people), and that the second response is always a little worse than the first (probably due to longer memory delay). Please clarify the logic behind the analyses.</p><p>4) Experiment 3 used salient cues to one stream to direct participants' attention to that stream, and then asked participants to report the colors at different cue-probe intervals. I appreciated the higher sampling and larger unpredictability here f which likely introduced more of a feature tracking task than Exp. 1 and 2.</p><p>The authors find some differences in performance across the different SOAs, and in particular in Exp. 3A, find an interaction between cue condition and SOA tested, revealing higher performance at about 3.2s after the cue for the attended stream which then switches back to higher performance for the uncued stream. None of the other two experiments find interactions that would support that only one stream is monitored at a time. Given the authors had no a priori expectation of when such switches happen I would really like to see a replication of Exp. 3A to ensure that the one effect they observed here is not a false positive. Furthermore, while some temporal structure might be present in this data (if it replicates), making the additional step and concluding that this is an 'oscillation' is a far stretch. All I see in the data is that there is an increase in performance at 3.2s for cued which then disappears again. This is not evidence for an oscillation. Brookshire, in a recent paper published in Nature Human Behavior, discusses the problem of applying spectral analyses to data such as the one here, as these analyses are not only sensitive to periodic but also aperiodic structure. There are several additional analyses that can be done to test this (see Brookshire paper for more details).</p><p>5) All data is analyzed using a mixture model that is often used in visual working memory tasks that relies on strong assumptions, namely that response errors reflect a mixture of variably precise memories and guesses. While the mixture model has been quite popular in working memory research, recent studies challenge the model's assumptions and support a much more simple framework in which all response errors can be explained with a single parameter, memory strength, once the psychological similarity of the feature space is considered (e.g., Schurgin et al., 2020). Thus, I would strongly encourage the authors to report and analyze the mean error and standard deviation of the errors as indexes of performance in this task instead of modeling the data.</p><p>6) The discussion of how the results relate to the Boolean map theory was interesting and I appreciated how the authors linked their current results to that theory. I was wondering whether the authors are arguing that the bottleneck assessed in the current task is related to &quot;access&quot; of the colors, i.e., the ability to report the colors, or a limit of attentional allocation to early sensory representations.</p><p>References:</p><p>Brookshire, G. (2022). Putative rhythms in attentional switching can be explained by aperiodic temporal structure. Nature Human Behaviour, 6(9), 1280-1291.</p><p>Chung, Y.H., Schurgin, M.W. and Brady, T.F. The role of motion in visual working memory for dynamic stimuli: More lagged but more precise representations of moving objects.Â Atten Percept PsychophysÂ 85, 1387-1397 (2023).</p><p>Fischer, J., and Whitney, D. (2014). Serial dependence in visual perception. Nature neuroscience, 17(5), 738-743.</p><p>Schurgin, M. W., Wixted, J. T., and Brady, T. F. (2020). Psychophysical scaling reveals a unified theory of visual memory strength. Nature human behaviour, 4(11), 1156-1172.</p><p>StÃ¶rmer, V. S., Winther, G. N., Li, S. C., and Andersen, S. K. (2013). Sustained multifocal attentional enhancement of stimulus processing in early visual areas predicts tracking performance. Journal of Neuroscience, 33(12), 5346-5351.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91183.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I suggest that the authors first explain the null difference between cued and un-cued conditions in Exp. 3s</p></disp-quote><p>We further clarified in the methods the significance of the âcueâ as a luminance change in order to phase-reset the parallel feature-tracking in contrast to a stimulus designed to continuously enhance responses towards that particular color stream in which it occurs.</p><disp-quote content-type="editor-comment"><p>Clarify how unbiased and adequate assign response 1 and response 2 to target 1 and target 2, respectively.</p></disp-quote><p>We ran an additional simulation (shown under reviewer 3) to showcase the outcome of the two potential target-response pairing procedures in experiment 1, outlined above. These calculations suggest an overestimation of the precision ratio between the two streams using the best/worst assignment strategy. Additional clarifications have been added to the methods.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. I wonder if the classification of two possible target-response distances in Experiment 1 is a valid approach. The manipulation in Experiment 2 by randomly testing one of the two color streams seems better, but it is not clear how to separate the high-precision and low-precision distributions can be separated from a mixture distribution.</p></disp-quote><p>The reviewer raises the issues of response interference when testing both streams. In fact, the second experiment serves as a validation of the separation-strategy while avoiding such interference problems. To further clarify this, we provide an extended description of our reasoning behind target-response assignments for experiment 1 in the methods section. Additionally, the logic has been laid out in more detail for reviewer 1 as well as below for reviewer 3. Importantly, the ratio measured by both methods (report both streams/ report one random streams) are comparable.</p><p>Furthermore, the reviewer raises an important point about the validity of mixture-modeling using von-Mises distributions. Separating a single distribution of precision-responses (target-response differences) into two von-Mises distributions based on the assumption of two underlying continuous resources contributing equally to the sum of all precision-responses is a well-established technique, employed extensively in the working-memory literature (Zhang, 2008; Bays, 2009; Fougnie and Alvarez, 2011, van den Berg, 2012). In our case, precision quantifies the successful access to information of one of two independent color perceptions potentially maintained by feature-based attention.</p><disp-quote content-type="editor-comment"><p>2. A ratio of 2:1 seems to be quite robust across different experiment setups. We need to be careful to see if this does not reflect a bias due to computational processes. Perhaps some simulations would help confirm it.</p></disp-quote><p>We thank the reviewer for motivating an analysis about the computational validity of the performed mixture model. We ran a simulation for experiment 2 with the null-hypothesis of both streams receiving the same amount of cognitive resource (being equally accessible) at any point in time. Using our employed Monte-Carlo approach described in the paper we created a number of theoretical target-response distributions consisting of mixture responses originating from von-Mises distributions with equal standard deviation. Over 1000 simulations were run with deviations based on our measured precisions for each subject. For each of those subjects we assumed each stream to be maintained by a resource quantified by the mean of the two prior estimated deviations ([55.27,25.70]-&gt;40.49, etc.). Basically, we tested how the model would behave if the real precision data in figure 3 would be positioned on the diagonal x=y, i.e. ratio = 1.</p><p>Separating those equal distributions using our mixture model resulted in a range of ratios presented in the following figure. The maximum number of results equal a ratio of 1 and the median reaches a ratio of about 1.24, with less than 5% of the simulated results reaching a ratio of more than 1.928. We are therefore confident, that the data in our results show a real imbalance in cognitive resources across both streams throughout each trial. This analysis has been added to the manuscript.</p><disp-quote content-type="editor-comment"><p>3. In Experiment 3a-c, the authors performed several versions of the experiment with different SOAs. The consistent phase difference between the cued and un-cued streams also varied with SOA (~1Hz for 200-ms SOA, 3.75 Hz for 1o0-ms SOA; 4.68 Hz for 40-ms SOA). It is not clear to me why the authors only made a conclusion based on 1Hz oscillation between two streams.</p></disp-quote><p>The reviewer is correct, that we did focus on the 1Hz modulation of the phase difference in experiment 3. Experiment 3 was designed in three different SOA versions in order to be able to replicate significant phase differences across subject groups. This replication was successful for the 1Hz modulation only. Hence, we are confident of that specific finding. For 3b this effect emerges as a significant precision difference between the cued and uncued stream from 3.1sec onwards. In this case, the range of the SOAs do only cover one half of a full period. The 3.75Hz phase-lock found in experiment 3b however did not replicate in the 40ms version of the task.</p><p>Furthermore, the maximum phase-lock at 4.68Hz in the 40ms-version has been reported for the sake of detailed reporting of the results but is rather spurious, showing minimal phase-lock of 0.18 and no significant in-phase- or antiphase- behavior.</p><disp-quote content-type="editor-comment"><p>4. The authors should discuss more about the functional significance underlying the observation of &quot;unequally distributed attentional resources that alternated at 1 Hz&quot; with two attended feature values.</p></disp-quote><p>We expand in the discussion more on the potential attentional process alternating between to color representations within a certain time-frame. Additionally, we discuss the importance of the methodological and cognitive dissociation of oscillations in response to reviewer 3.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Below I am describing my concerns in more detail and making suggestions of how to address some of them.</p><p>1) The task instructs participants to &quot;keep track&quot; of two colored dot clouds and asks them to report the color after about 6-8 seconds on each trial. Certainly, there is some uncertainty with regards to when color is probed, yet it seems to me that participants can easily enough start attending to the color shortly before the test probe at the end (so, after about 6s), to be able to report the colors. Thus, the task itself does not truly require participants to track color values continuously for several seconds. The task can be performed by starting to pay attention to the colors only at the very end, shortly before the test probe (at least in Exp. 1 and 2). Also, as participants choose which color to report, it is not necessary for them to know which stream had which color â so &quot;swaps&quot; might be happening all the time; based on Exp. 1 it's entirely unclear which color stream they are intending to report.</p><p>Overall, this task design is very different from spatial attention tracking tasks that require participants to track target objects among identical distractors. The color tracking task in the current study tries to establish parallels between the spatial tracking literature, but it seems not clear at all where the parallels are given the differences in tasks. Spatial tracking tasks (multiple-object tracking, i.e.,) require selective and continuous tracking (e.g., Stormer et al., 2014), which is not true on the current color task that asks participants to report which colors they just saw.</p><p>Thus, it is unclear whether or to what extent this task tests &quot;tracking&quot; vs. simply perceiving and briefly remembering two colors once the test probe comes on, more akin to a visual working memory task with a short delay.</p></disp-quote><p>We clarified details of the experimental design of experiment 1 and 2 in the methods section. Importantly, subjects either had to report <italic>both</italic> streams (exp1) or <italic>one</italic> random stream (exp2). In both experiments, in order to perform well, subjects had to try to maintain both color information at any given time as accurately as possible. None of the tasks let them âchooseâ, which color to respond to. As a similar point is mentioned by the reviewer later on again: In the first two experiments, we presume that one attentional resource is split between color streams at the start of each trial to maintain its changing information. Importantly, we cannot know which stream receives which resource at the start of the trial (this is where the logic of the âresetâ-cue in Exp3 comes into play). Thus, the naming convention of âstream1â and âstream2â is arbitrary and only useful to make clear that two colors had to be attended to simultaneously. As the reviewer states, we cannot know which report corresponds to which target color (stream) in experiment 1. This is why we apply the described assignment-strategy that presumes that the first response is performed more confidently than the second in most cases. Since there is still a large amount of variability in this response strategy per trial we collapse all responses and separate them using a mixture model. We use the Monte-Carlo simulation in order to show that the estimated target-response precision pairing can be recreated using the assumption of responding to the âbetterâ maintained color stream first.</p><p>As an additional analysis, we also paired target and responses in a way to separate âbestâ and âworstâ responses independent of the sequence of response. Interestingly, the Monte-Carlo simulation with this assumption did systematically overestimate the precision ratio relative to the initial precision pairs for each subject (see <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>: left side depicts the analysis as seen in figure 3a of the manuscript; right side shows precisions estimates for exp1 under separation based on best/worst strategy (blue dots depict precision estimates and ellipses the confidences)). This means that only analyzing best and worst precisions would often pair a response with a target, that would not be the intended target but just randomly be closer to that particular target in this trial.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91183-sa2-fig1-v2.tif"/></fig><p>For example, let R1 pair with T1 and R2 with T2 at a given trial, with T1 = 0Â° and T2 = 90Â°. Let R1 be fairly accurate with R1 = 30Â° (diff = 30Â°) and R2 inaccurate with R2 = 20Â° (diff = 70Â°). Due to the random target configurations however, the âbestâ/âworstâ strategy would automatically pair R2 with T1 (diff = 20Â°) and R1 with T2 (diff = 60Â°), leading to an overestimation of the ratio. Importantly, the second and third experiment were designed with one response required per trial as to avoid the mentioned problem of target-response assignment.</p><disp-quote content-type="editor-comment"><p>One potential additional analysis that could â at least to some extent â get at this might be to examine the response errors in relation to not just the very last color, but previous colors of that stream and examine recency effects/biases in people's color reports. If people are continuously tracking each color stream you'd expect relatively strong recency biases, consistent with &quot;serial dependence&quot; (Fischer and Whitney, 2014). Another paper on visual working memory for continuously color changing objects also seems relevant: Chung, Schurgin, Brady, 2023.</p></disp-quote><p>This is a very intriguing idea proposed by the reviewer. Indeed, we could assume, that the color representation we have access to at any given point in time âlacksâ behind the continuous color stream. Especially, since we propose a fairly slow model of alternating between the two streams. We therefore could look at precision as a function of time.</p><p>The problem with the current dataset, however, is that the sequence of color-trajectory presentations across the experiment for each subject was randomized, such that we can not recover the specific history for the color streams for any given precision response but only the very last target color. If we would know whether the color âcame fromâ clockwise or counterclockwise, we could look for a bias of the precision distribution indicating a temporal lack of the representation reported. However, this is a very intriguing idea that we will keep in mind for future studies.</p><disp-quote content-type="editor-comment"><p>2) This relates to my previous point that questions the task itself but relates more directly to the main claim of a limited resource. Given participants are asked to report the color at the very end of the trial, this measure â in my view â might well reflect memory and decision-related processes, rather than attentional allocation. How can we tease these processes apart in the current task design? In some ways, it seems to me, that the probe at the end of the trial forces people to prioritize one color over the other, so the limits the authors are claiming might all happen at the response stage and not during tracking/viewing.</p></disp-quote><p>The reviewer is right in that every precision task is a mixture of variations in attentional processing but also decision related processes. In the current task however, there should not be any systematic decision-related variation between responding to one stream vs responding to the other. Especially for the 2<sup>nd</sup> and 3<sup>rd</sup> experiment, in which subjects do not know, which of the two streams is being probed and therefore no prioritization does occur. Also, one has to see that an eventual prioritizing effect of probing one stream is orthogonal to the distributional variation of precision within the probed stream. In other words, if color prioritization due to probing at the end of a trial accounts for the performance variation, there should be no 2:1 precision variation among the probed colors, as they would all be equally prioritized above the unprobed color stream.</p><p>The design for the 2<sup>nd</sup> and 3<sup>rd</sup> experiment was chosen apart from avoiding the problem of target-response assignments, in order to exclude variance due to memory-related decay of the second response.</p><disp-quote content-type="editor-comment"><p>3) I am not sure I follow the logic of the correlation analysis and interpretation; what they seem to show is that the first and second responses are correlated, but I am not sure how we can conclude from this pattern that there is competition between attending to the two streams. Such a model would predict that a response on one trial trades off with the response on that same trial in particular, no? So, shouldn't these trade-offs be measured at a single trial level, rather than accuracy across people for two responses? It seems to me these correlations are consistent with a much simpler explanation that considers overall differences in performance between people (e.g., those people that have the best high-precision responses also have the best low-precision responses â thus, are overall more precise than other people), and that the second response is always a little worse than the first (probably due to longer memory delay). Please clarify the logic behind the analyses.</p></disp-quote><p>The reviewerâs interpretation is exactly correct. All we wanted to show with the correlation analysis is, that the ratio between the estimated precisions for both resources are comparable across subjects. This âtrade-offâ is hereby independent of the overall precision of responses. Subjects can be very accurate in responding to the targets but would still show a bias by responding better to one of the streams compared to the other. Importantly, this bias is not constant (as with the suggested memory delay which would be relevant only in the first experiment) as would be indicated by a similar precision offset, but by a constant ratio (1.7-1.9).</p><disp-quote content-type="editor-comment"><p>4) Experiment 3 used salient cues to one stream to direct participants' attention to that stream, and then asked participants to report the colors at different cue-probe intervals. I appreciated the higher sampling and larger unpredictability here f which likely introduced more of a feature tracking task than Exp. 1 and 2.</p><p>The authors find some differences in performance across the different SOAs, and in particular in Exp. 3A, find an interaction between cue condition and SOA tested, revealing higher performance at about 3.2s after the cue for the attended stream which then switches back to higher performance for the uncued stream. None of the other two experiments find interactions that would support that only one stream is monitored at a time. Given the authors had no a priori expectation of when such switches happen I would really like to see a replication of Exp. 3A to ensure that the one effect they observed here is not a false positive. Furthermore, while some temporal structure might be present in this data (if it replicates), making the additional step and concluding that this is an 'oscillation' is a far stretch. All I see in the data is that there is an increase in performance at 3.2s for cued which then disappears again. This is not evidence for an oscillation. Brookshire, in a recent paper published in Nature Human Behavior, discusses the problem of applying spectral analyses to data such as the one here, as these analyses are not only sensitive to periodic but also aperiodic structure. There are several additional analyses that can be done to test this (see Brookshire paper for more details).</p></disp-quote><p>Please see the public section of the revision regarding the temporal structure of our proposed mechanism. We do absolutely agree, and emphasize in our paper, that attention in this case is not an oscillatory process but an alternation of conscious access towards one of two changing feature values over time as proposed by the Boolean map theory. We use Fourier-analysis based testing in order to merely quantify the time-frame in which these alternations occur, but not to imply oscillations and regularity. Two variations of experiment 3 do show time-based variations over a similar time-span with 3a over one period across 1 second and 3b over half a period across 500ms.</p><disp-quote content-type="editor-comment"><p>5) All data is analyzed using a mixture model that is often used in visual working memory tasks that relies on strong assumptions, namely that response errors reflect a mixture of variably precise memories and guesses. While the mixture model has been quite popular in working memory research, recent studies challenge the model's assumptions and support a much more simple framework in which all response errors can be explained with a single parameter, memory strength, once the psychological similarity of the feature space is considered (e.g., Schurgin et al., 2020). Thus, I would strongly encourage the authors to report and analyze the mean error and standard deviation of the errors as indexes of performance in this task instead of modeling the data.</p></disp-quote><p>We absolutely agree on the mentioned limitations of the mixture model in terms of variance separation into memory performance and guesses. However, here we do not make strong assumptions about the composition of variation explained by the model. We take âprecisionâ as a general reflection of a continuous but variable attentional resource. We do not go into interpretations regarding sources of âerrorâ but are content with the notion that a variety of errors contribute to larger or smaller precisions over a large amount of trials.</p><p>Experiments 2 and 3 require subjects to perform exactly one response per trial from which two precision distributions are estimated from. Therefore, non-modeled standard deviations would only give you the overall precision responses over all trials and all color streams and not be useful in distinguishing separate attentional resources for two changing color values. In experiments 3a-c we do report standard deviations without using a mixture model since we do have an unambiguous target-response assignment for cued and uncued streams.</p><disp-quote content-type="editor-comment"><p>6) The discussion of how the results relate to the Boolean map theory was interesting and I appreciated how the authors linked their current results to that theory. I was wondering whether the authors are arguing that the bottleneck assessed in the current task is related to &quot;access&quot; of the colors, i.e., the ability to report the colors, or a limit of attentional allocation to early sensory representations.</p></disp-quote><p>This is a very interesting question discussed extensively by Huang and Pashler. The Boolean map theory posits, that we can only select in the sense of conscious access and reportability one non-spatial feature like color at any given moment in time. Hence, our interpretation is that serial resource allocation arises from limits of conscious access to changing color values and not from limits of early sensory representation.</p></body></sub-article></article>