<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">97083</article-id><article-id pub-id-type="doi">10.7554/eLife.97083</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97083.2</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Short Report</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Frequency-specific cortico-subcortical interaction in continuous speaking and listening</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Abbasi</surname><given-names>Omid</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2169-8498</contrib-id><email>abbasi@wwu.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Steingräber</surname><given-names>Nadine</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Chalas</surname><given-names>Nikos</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kluger</surname><given-names>Daniel S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0691-794X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Gross</surname><given-names>Joachim</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3994-1006</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00pd74e08</institution-id><institution>Institute for Biomagnetism and Biosignal Analysis, University of Münster</institution></institution-wrap><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00pd74e08</institution-id><institution>Otto-Creutzfeldt-Center for Cognitive and Behavioral Neuroscience, University of Münster</institution></institution-wrap><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP97083</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-03-05"><day>05</day><month>03</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-03-08"><day>08</day><month>03</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.05.583572"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-17"><day>17</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97083.1"/></event></pub-history><permissions><copyright-statement>© 2024, Abbasi et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Abbasi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-97083-v1.pdf"/><abstract><p>Speech production and perception involve complex neural dynamics in the human brain. Using magnetoencephalography, our study explores the interaction between cortico-cortical and cortico-subcortical connectivities during these processes. Our connectivity findings during speaking revealed a significant connection from the right cerebellum to the left temporal areas in low frequencies, which displayed an opposite trend in high frequencies. Notably, high-frequency connectivity was absent during the listening condition. These findings underscore the vital roles of cortico-cortical and cortico-subcortical connections within the speech production and perception network. The results of our new study enhance our understanding of the complex dynamics of brain connectivity during speech processes, emphasizing the distinct frequency-based interactions between various brain regions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>MEG</kwd><kwd>speech production</kwd><kwd>speech perception</kwd><kwd>cerebellum</kwd><kwd>brain connectivity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>GR 2024/5-1</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>GR 2024/11-1</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>GR 2024/12-1</award-id><principal-award-recipient><name><surname>Gross</surname><given-names>Joachim</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Distinct frequency-based interactions between cortical and subcortical regions highlight the cerebellum’s crucial role in speech production and perception.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Human communication can be described as two dynamic systems (i.e., speaker and listener) that are coupled via sensory information (<xref ref-type="bibr" rid="bib45">Silbert et al., 2014</xref>) and operate according to principles of active inference by minimizing prediction errors (<xref ref-type="bibr" rid="bib20">Friston and Frith, 2015</xref>). Within this framework, the speaker’s predictive processing model steers speech production, allowing adjustments in volume, speed, or articulation based on proprioceptive and auditory feedback. Similarly, the listener’s predictive processing model generates anticipations about upcoming speech which are continually updated and compared with incoming sensory data. The implementation of this model in the human brain was shown to be associated with brain rhythms (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>; <xref ref-type="bibr" rid="bib8">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="bib37">Park et al., 2015</xref>).</p><p>Brain rhythms have been extensively investigated during continuous speech perception. A consistent finding in these studies is the synchronization of frequency-specific brain activity, with the rhythmic amplitude modulation in continuous speech (<xref ref-type="bibr" rid="bib24">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib26">Gross et al., 2013b</xref>). The exploration of the brain networks that underpin speech perception has also revealed a stronger causal influence from higher-order brain regions (such as the left inferior frontal gyrus and left motor areas) to the auditory cortex for intelligible speech compared to unintelligible speech (<xref ref-type="bibr" rid="bib37">Park et al., 2015</xref>). In the domain of speech production, both invasive (<xref ref-type="bibr" rid="bib30">Llorens et al., 2011</xref>; <xref ref-type="bibr" rid="bib36">Ozker et al., 2022</xref>; <xref ref-type="bibr" rid="bib39">Riès et al., 2017</xref>) and non-invasive (<xref ref-type="bibr" rid="bib21">Ganushchak et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Janssen et al., 2020</xref>) electrophysiological studies have made significant contributions due to their superior temporal resolution compared to fMRI. However, it is important to note that invasive recordings offer only limited spatial coverage from specific recording sites in patients. Magnetoencephalography (MEG) has been used by several research teams to explore the intricate relationship between brain frequency-specific dynamics and speech production. Ruspantini et al. observed notable coherence between MEG activity originating from the motor cortex and Electromyography (EMG) activity in lip muscles, with a peak frequency of approximately 2–3 Hz (<xref ref-type="bibr" rid="bib41">Ruspantini et al., 2012</xref>). In a separate investigation conducted by Alexandrou et al., they identified high-frequency band modulation (60–90 Hz) within bilateral temporal, frontal, and parietal brain regions (<xref ref-type="bibr" rid="bib7">Alexandrou et al., 2017</xref>). In our recent study, we also explored speech production and perception using MEG to map the cortical networks engaged during these two experimental conditions (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>). We reported significant connectivity from the motor area to superior temporal gyrus (STG) in lower-frequency bands (up to beta) during speaking, and the reverse pattern in high gamma frequencies (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>).</p><p>Subcortical areas also contribute in updating the brain’s internal model during speech production and perception. Thalamic nuclei serve as central nodes for circuits associated with language processing, interacting with the frontal cortex, basal ganglia, cerebellum, and dopaminergic groups. Notably, the motor-related thalamic nuclei link the basal ganglia and cerebellum with the frontal cortex, contributing to both motor and cognitive aspects of language (<xref ref-type="bibr" rid="bib10">Barbas et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Silveri, 2021</xref>). Relatedly, several previous studies have reported the cerebellum’s role in several cognitive functions such as speech production and perception (<xref ref-type="bibr" rid="bib23">Giraud et al., 2007</xref>; <xref ref-type="bibr" rid="bib47">Skipper and Lametti, 2021</xref>) movement coordination, timing, motor programming, speech motor control, and sensory prediction (<xref ref-type="bibr" rid="bib38">Parrell et al., 2017</xref>). Lesion studies, neuroimaging investigations, and brain stimulation studies have consistently linked the cerebellum to critical aspects of timing and predictive processing in speech perception (<xref ref-type="bibr" rid="bib6">Ackermann, 2008</xref>; <xref ref-type="bibr" rid="bib31">Manto et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Parrell et al., 2017</xref>). Temporo-cerebellar coupling has been suggested to support the continuous updating of internal models of the auditory environment (<xref ref-type="bibr" rid="bib28">Kotz et al., 2014</xref>). In a more recent study, Stockert et al. reported bidirectional temporo-cerebellar anatomical connectivity (<xref ref-type="bibr" rid="bib48">Stockert et al., 2021</xref>). The authors suggested that these anatomical temporo-cerebellar connections may support the encoding and modeling of rapidly modulated auditory spectro-temporal patterns.</p><p>However, to the best of our knowledge no study has so far investigated spectrally resolved, directed functional coupling between cortical and subcortical brain areas during listening and speaking. In this MEG study, we intend to investigate the specific frequency-related connections between both cortical and subcortical brain regions while participants engaged in speaking and listening tasks. Participants answered seven questions, each lasting 60 s (speaking condition), and also listened to recordings of their own speech from the previous speaking session (listening condition; see Methods for details). Initially, we identified the cortical and subcortical brain regions involved in speech perception and production through meta-analysis studies. Subsequently, we examined the brain network and directed connections within it to shed light on the cognitive processes underlying listening and speaking. This involved a direct examination of frequency-specific communication channels for both feedforward and feedback signaling during speech production and perception using multivariate Granger causality. Our connectivity findings confirmed the participation of subcortical regions such as the thalamus and cerebellum in both speech production and perception. Our analysis using directed Granger causality revealed stronger connectivity from the cerebellum to the auditory area at slower frequencies (below 40 Hz). This connectivity pattern might reflect the influence of cerebellar predictions on auditory processing. Conversely, connectivity in the opposite direction, from the auditory area to the cerebellum, was stronger at faster frequencies (above 40 Hz). This pattern could potentially be indicative of prediction error signals being relayed back to the cerebellum for updating its predictions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this study, we investigated the connectivity between brain regions that are involved in speech production and perception. OurROI-based (Region of Interest) analysis utilized the automatic meta-analysis provided by <ext-link ext-link-type="uri" xlink:href="https://neurosynth.org/">neurosynth.org</ext-link> using the term ‘speech’ that resulted in 642 fMRI studies highlighting the brain areas involved in speech production and perception. We extracted the MNI coordinates of active areas from the resulting neurosynth statistical map, resulting in 14 parcels. For each voxel in a given parcel, the beamformer-derived time-series for all the three source orientations were subjected to singular value decomposition (SVD). In every parcel, we used the three strongest components for further analysis. For estimating pairwise functional connectivity between all parcels (<italic>n</italic> = 14), we used multivariate nonparametric Granger causality (mGC; <xref ref-type="bibr" rid="bib42">Schaum et al., 2021</xref>) and related them to the hallmarks of predictive processing models during continuous speaking and listening (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). The method uses three-dimensional representations of parcel activity and thus estimates connectivity more accurately than traditional one-dimensional representations (<xref ref-type="bibr" rid="bib13">Chalas et al., 2022</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Methodological pipeline.</title><p>Thirty participants answered seven given questions (60 s each; <italic>speaking</italic> condition) as well as listened to audio-recordings of their own voice from previous sessions (<italic>listening</italic> condition) while magnetoencephalography (MEG) data were recorded. Artifacts were removed from the recorded MEG data (<xref ref-type="bibr" rid="bib3">Abbasi et al., 2021</xref>). Individual MRIs were used to estimate source models per participant which were interpolated to a template volumetric grid. Relevant areas in speech production and perception networks were identified from <ext-link ext-link-type="uri" xlink:href="https://neurosynth.org/">Neurosynth.org</ext-link> platform. Fourteen corresponding anatomical parcels in HCP and AAL atlases were identified: L-FOP (1), R-PEF+6v (2), L-SCEF (3), R-SCEF (4), L-3b (5), R-3b (6), L-STS (7), L-TPOJ1 (8), L-A5 (9), R-A5 (10), L-TH (11), R-CB-Crus2 (12), L-CB-6 (13), and R-CB-6 (14). For each identified parcel, estimated source time-series were extracted. Next, using a blockwise approach, we considered the first three singular value decomposition (SVD) components of each parcel as a block and estimated the connectivity between each pair of parcels using a multivariate nonparametric Granger causality approach (mGC; <xref ref-type="bibr" rid="bib42">Schaum et al., 2021</xref>). In this study, the connectivity results are presented using connectogram plots. In the connectograms, nodes represent the brain areas and edges represent the strength and direction of the connections between them. The thickness of the edges indicates the magnitude of the <italic>t</italic>-values, while color indicates the directionality of the connectivity. In other words, when node A connects to node B, the edge will have the same color as node A, and vice versa when node B connects to node A. Note that only significant connections are shown in the connectograms (p &lt; 0.05, cluster correction). For instance in the illustrated connectogram, the purple edge between L-CB6 and R-PEF shows significant connectivity from L-CB6 to R-PEF. R = right, L = left.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97083-fig1-v1.tif"/></fig><p>The calculation of mGC between two parcels yielded two spectra reflecting both directions (A-&gt;B and B-&gt;A). Next, we computed the directed asymmetry index (DAI) for each pair of spectra (<xref ref-type="bibr" rid="bib11">Bastos et al., 2015</xref>). This measure captures the predominant direction of mGC between two parcels based on the relative difference between both directions (see ‘Methods’ for more information). A positive DAI indicates the dominant directionality from A toward B, while a negative DAI indicates the opposite directionality. First, we used group statistics to identify brain areas where DAI values in specific frequency bands in the speaking as well as listening conditions differed significantly from zero. We performed group statistics from 0 to 100 Hz. Next, we defined the following canonical frequency bands: Delta/Theta (0–7 Hz), alpha (7–13 Hz), beta (15–30 Hz), gamma (30–60 Hz), and high gamma (60–90 Hz).</p><p>Using connectogram plots, we visualized the connectivity between the selected 14 ROIs and examined neural mechanisms associated with speech production and perception, as well as predictive processing models. The number of outgoing and incoming edges for each node was also calculated using the brain connectivity toolbox and represented as the node strength (<xref ref-type="bibr" rid="bib40">Rubinov and Sporns, 2010</xref>). The upper row of <xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates the strength for three different nodes in the speaking condition. In L-SCEF (pre-supplementary motor area) and R-CB6 (right cerebellum lobule VI), the number of outgoing edges in low frequencies is higher compared to incoming edges. However, we observed a reverse pattern in high frequencies. Interestingly, in the left STG (L-A5), there are more incoming connections compared to outgoings in low frequencies, and this changes in the higher frequencies, with more outgoing connections seen in this area. This result indicates that in low frequencies, the sensorimotor area and cerebellum predominantly transmit information, while in higher frequencies, they are more involved in receiving it. On the other hand, the left STG receives information in low frequencies and transmits it in high frequencies. The connectograms in the middle parts of <xref ref-type="fig" rid="fig2">Figure 2</xref> illustrate the connections of the left sensorimotor area (left plots; L-3b and L-SCEF) and the right cerebellum (right plots) to other cortical and subcortical parcels in low (top: 7–20 Hz) and high (bottom: 60–90 Hz) frequencies during the speaking condition. In low frequencies, both the sensorimotor area and the right cerebellum primarily send information to other parcels, including the left and right temporal regions, while in the high frequencies, these nodes primarily receive information. Interestingly, our connectivity results illustrate that the top–down effects from higher-order cortical areas to lower-order areas during speaking occur in distinct frequency bands. <xref ref-type="fig" rid="fig2">Figure 2</xref> (lower panels) depicts that the strongest top–down connectivity from the left pre-supplementary motor area (pre-SMA; L-SCEF) to lower-order cortical areas such as L-A5 occur in the theta band (peak at 5–7 Hz). However, the top–down connectivity from L-3b to L-A5 was strongest in two distinct theta (peak at 7 Hz) and beta (peak at 21 Hz) frequency bands. Moreover, these results revealed significant bottom–up connectivity from lower-order cortical areas, namely L-A5 to left sensorimotor area (L-3b and L-SCEF) in high-frequency bands. As we previously showed (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>), this striking reversal indicates a dissociation of bottom–up and top–down information flow during speaking in distinct frequency bands. Signals communicated top–down are predominantly transmitted in low-frequency ranges, while those communicated bottom–up are transmitted in high-frequency ranges. Notably, <xref ref-type="fig" rid="fig2">Figure 2</xref> (bottom-right panel) also depicts directional connectivity from the right cerebellum lobule VI (R-CB6) to the left STG (L-A5) in frequencies below 50 Hz. In contrast, in frequencies above 50 Hz, we observed reverse signaling from the left STG to the right cerebellum during speaking.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Connectivity analysis between the sensorimotor, cerebellum, and superior temporal gyrus (STG).</title><p>Upper panels illustrate the strength for L-SCEF, L-A5, and R-CB6 nodes in the speaking condition. In the middle part, connectograms illustrate connections between the left sensorimotor area (left plots) and the right cerebellum (right plots) during speaking to other cortical and subcortical parcels at low frequencies (top: 7–20 Hz) and high frequencies (bottom: 60–90 Hz). Lower panels illustrate spectrally resolved directed asymmetry index (DAI) from L-3b (left spectra), L-SCEF (middle spectra), and R-CB6 (right spectra) to L-A5. The results represented in the whole figure are significant connectivity patterns that passed a cluster-based permutation test (p &lt;.05, cluster correction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97083-fig2-v1.tif"/></fig><p>The connectograms in <xref ref-type="fig" rid="fig3">Figure 3</xref> provide a comprehensive overview of significant connectivities across various frequency bands among the selected fourteen brain areas during speaking and listening conditions. Specifically highlighting the first column’s connectograms during speaking, we observe comparable connectivity patterns to those of the left sensorimotor area. Connections from the right higher-order cortical regions, such as R-SCEF and R-3b, toward lower-order areas like R-A5, are noticeable in low-frequency bands (&lt;30 Hz). Conversely, for higher frequencies (&gt;30 Hz), we noticed the reverse directional connections, from lower- to higher-order brain regions. Furthermore, <xref ref-type="fig" rid="fig4">Figure 4 (upper panel)</xref> reveals strong top–down connectivity from bilateral 3b and left FOP (frontal opercular area) to bilateral A5 and left STS (superior temporal sulcus), particularly prominent in two distinct frequency bands: theta (peak at 7 Hz) and beta (peak at 21 Hz). Additionally, similar to the left hemisphere, our results in <xref ref-type="fig" rid="fig4">Figure 4 (upper panel)</xref> show significant bottom–up connectivity from lower-order cortical areas (bilateral A5 and left STS) to higher-order cortical regions (bilateral 3b and bilateral SCEF) across both low- and high-frequency bands.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Connectivity analysis.</title><p>Significant connectivity between 14 ROIs involved in speech production and perception. A cluster-based permutation test was used to detect significant connectivity patterns. The results of statistical analysis revealed significant connectivities between different brain areas during speaking (first column of connectograms), listening (second column of connectograms), and the comparison between speaking and listening conditions (third column of connectograms) across various frequency bands. In the connectograms, nodes represent the brain areas and edges represent the strength and direction of the connections between them. The thickness of the edges indicates the magnitude of the <italic>t</italic>-values, while color indicates the directionality of the connectivity. In other words, when node A connects to node B, the edge will have the same color as node A, and vice versa when node B connects to node A. Note that only significant connections are shown (p &lt; 0.05, cluster correction).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97083-fig3-v1.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Connectivity results.</title><p>Spectrally resolved directed asymmetry index (DAI) between all pairs of ROIs. A cluster-based permutation test was used to detect significant connectivity patterns. The results of statistical analysis revealed significant connectivities between different brain areas during speaking (top), listening (middle), and the comparison between speaking and listening conditions (bottom). The directionality is from <italic>y</italic>- to <italic>x</italic>-axis. Note that significant values are highlighted with increased line width (p &lt; 0.05, cluster correction). <italic>x</italic>- and <italic>y</italic>-axes represent frequency (Hz) and <italic>t</italic>-values, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97083-fig4-v1.tif"/></fig><p>We observed a similar pattern during the listening condition compared to the speaking condition, including reversed directionality between low and high frequencies (<xref ref-type="fig" rid="fig3">Figure 3</xref>, second column). However, both top–down effects in the low-frequency band and bottom–up effects in the high-frequency band were less pronounced during listening compared to speaking. The third column in <xref ref-type="fig" rid="fig3">Figure 3</xref> as well as the third panel of <xref ref-type="fig" rid="fig4">Figure 4</xref> confirm these results by showing the direct comparisons of DAI connectivity between speaking and listening. This figure illustrates significant differences in connectivity between speaking and listening conditions in different frequency bands. Specifically, stronger top–down signals originating from higher-order cortical areas (such as bilateral SCEF and 3b) toward lower-order brain areas, such as bilateral A5 and left STS were observed during speaking compared to listening. Conversely, the individual spectrally resolved DAI plots for the listening conditions, along with the connectograms, indicate the presence of significant connectivity solely from low-order areas (such as bilateral A5) toward higher-order cortical areas (bilateral SCEF and 3b) during the speaking condition, as opposed to the listening condition. More detailed spectrally resolved DAI between all ROIs and both conditions are illustrated in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>Notably, we found significant connections between subcortical and cortical areas, indicating their involvement in speech production and perception networks. <xref ref-type="fig" rid="fig4">Figure 4</xref> depicts the connectivity from the right cerebellum to other cortical and subcortical parcels. In contrast with what we reported for the speaking condition, during listening, there is only a significant connectivity in low frequency to the left temporal area but not a reverse connection in the high frequencies. However, a direct comparison between speaking and listening conditions did not reveal a significant difference. Our analysis also revealed robust connectivity between the right cerebellum and the left parietal cortex, evident in both speaking and listening conditions, with stronger connectivity observed during speaking. Notably, <xref ref-type="fig" rid="fig4">Figure 4</xref> depicts information flows from the cerebellum to the parietal areas in low-frequency ranges. There is also an intriguing connection between the left thalamus and right cerebellum, which reveals a distinct pattern of connectivity. Specifically, we found significant connectivity from the cerebellum to the thalamus in the low-frequency range during speaking, while the opposite pattern was observed in the high-frequency range (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Finally, we examined the relationship between the connectivity patterns from the selected cortical parcels to the right cerebellum and the coupling of the speech envelope with the oscillations in the STG. The speech–brain coupling was computed using a multivariate mutual information (MI) approach presented in our recently published study (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>). We investigated the correlations between the directional connectivity indices and MI values across participants for each parcel and frequency band in speaking condition. Our analysis showed significant negative correlations between the RCB6 to L_A5 connectivity and the speech–STG coupling in the theta band (at 130 ms lag) for speaking condition (<xref ref-type="fig" rid="fig5">Figure 5</xref>; p &lt; 0.05).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Correlation between speech–brain coupling and Granger causality.</title><p>This plot depicts the relationship between speech–brain coupling in the left superior temporal gyrus (STG) at theta frequency (4–8 Hz) with a positive lag of 130 ms and (right cerebellum to left STG) Granger causality (GC) during the speaking condition. The negative correlation indicates that weaker speech–brain coupling in the theta band is associated with stronger directional information flow from the right cerebellar lobule VI to the left temporal areas in the theta frequency band.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97083-fig5-v1.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present study utilized multivariate Granger causality analysis to investigate the frequency-specific neural representations of the internal forward model during continuous speaking and listening. Our results revealed significant connectivity from bilateral higher-order cortical areas to bilateral auditory areas (STG and STS) in lower-frequency bands (up to beta) during speaking, and in the opposite direction in gamma frequencies. Notably, subcortical areas also contributed to the internal forward model, with directional communication observed from the right cerebellum lobule VI (R-CB6) to the left auditory area below 30 Hz. Conversely, in frequencies above 30 Hz, we observed reverse signaling from the left auditory areas to the right cerebellum.</p><sec id="s3-1"><title>Directed connectivity in speech production and perception</title><p>In this study, we aimed to investigate directional connectivity in speech production and perception. Using the automatic meta-analysis provided by <ext-link ext-link-type="uri" xlink:href="https://neurosynth.org/">neurosynth.org</ext-link>, we included cortical and subcortical brain regions involved in these processes. Our multivariate Granger causality analysis suggests a potential role for predictive coding implemented through distinct frequency channels in the auditory–motor domain during both continuous speaking and listening. This analysis revealed stronger connectivity from higher-order brain regions (including the cerebellum) to the auditory area at slower frequencies (below 40 Hz). This connectivity pattern might reflect the influence of these regions’ predictions on auditory processing. Conversely, the analysis showed stronger connectivity in the opposite direction, from the auditory area to these regions, at faster frequencies (above 40 Hz). This pattern could potentially be indicative of prediction error signals being relayed back for updating predictions. Our study builds upon our previous work, which demonstrated top–down signaling in low frequencies from higher-order cortical areas to STG and STS and the reverse pattern in higher frequencies. Our findings are also supported by previous studies showing distinct frequency channels for feedforward and feedback communication between two hierarchically different auditory areas (<xref ref-type="bibr" rid="bib19">Fontolan et al., 2014</xref>) and demonstrating that prediction errors are represented in gamma power while predictions are represented in lower-frequency beta power (<xref ref-type="bibr" rid="bib44">Sedley et al., 2016</xref>).</p><p>Based on our meta-analysis results, several subcortical areas such as cerebellum and thalamus are involved in speech production and perception. Previous studies have supported the cerebellum’s role in speech production and perception (<xref ref-type="bibr" rid="bib23">Giraud et al., 2007</xref>; <xref ref-type="bibr" rid="bib47">Skipper and Lametti, 2021</xref>). Our connectivity analysis findings demonstrate the involvement of the cerebellum in the implementation of predictive coding within speech networks. Specifically, our results indicate directed connectivity from the right cerebellum to left temporal areas (L-A5 and L-STS) in lower frequencies, and the reverse direction in higher frequencies, supporting the role of the cerebellum in predictive processing. Similar to the connections between higher-order cortical areas and temporal areas, feedback signaling between the cerebellum and temporal areas occurs in low frequencies (below 40 Hz), likely conveying timing information for upcoming speech. In contrast, feedforward signaling occurs in gamma frequency (above 60 Hz) from temporal areas to the cerebellum, facilitating the update of sensory predictions. Comparison between speech production and perception conditions revealed stronger feedback signaling from the cerebellum to temporal areas during speech production, aligning with the nature of updating sensory predictions in this context. Our findings closely align with recent studies that have also investigated the role of the cerebellum in predictive internal modeling mechanisms in speech production and perception (<xref ref-type="bibr" rid="bib48">Stockert et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Todorović et al., 2024</xref>). These studies suggest that the combined yet distinct activation of temporal, parietal, and cerebellar regions during internal and external monitoring points toward their involvement in auditory and somatosensory targets and continuous updating of auditory environmental models. Moreover, temporo-cerebellar coupling may underlie the precise encoding of temporal structure and support the ongoing optimization of spectro-temporal models of the auditory environment within a network comprising the prefrontal cortex, temporal cortex, and cerebellum (<xref ref-type="bibr" rid="bib28">Kotz et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Stockert et al., 2021</xref>; <xref ref-type="bibr" rid="bib51">Todorović et al., 2024</xref>).</p><p>Our findings of significant negative correlations between right cerebellar connectivity to the left temporal area and speech–STG coupling in the theta band during speech production resonate with our prior work (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>). In our previous study, we reported a similar negative correlation involving theta-range speech–brain coupling in the left auditory area and top–down beta connectivity from motor areas. These observations parallel existing research on sensory attenuation, where the brain predicts the sensory outcomes of self-generated actions, resulting in reduced cortical responses (<xref ref-type="bibr" rid="bib33">Martikainen et al., 2005</xref>). In earlier studies, we associated this phenomenon with the modulation of beta-range directional couplings originating from motor cortices toward bilateral auditory regions, indicating predictive processes (<xref ref-type="bibr" rid="bib2">Abbasi and Gross, 2020</xref>). Now, our new results introduce the cerebellum as another key node in this sensory attenuation mechanism, expanding our understanding of how the brain anticipates and minimizes sensory responses during speech production.</p><p>The similarities observed between the feedback and feedforward signaling from the cerebellum and higher-order cortical areas to the temporal areas suggest a shared contribution in predicting the sensory consequences of generated speech. It is proposed that cortico-cerebellar and cortico-cortical predictions interact in speech networks, with the cerebellum potentially involved in predicting well-learned speech, while the cortex flexibly applies predictions in novel contexts (<xref ref-type="bibr" rid="bib6">Ackermann, 2008</xref>; <xref ref-type="bibr" rid="bib5">Ackermann et al., 1998</xref>; <xref ref-type="bibr" rid="bib47">Skipper and Lametti, 2021</xref>). Notably, the results of our previous findings (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>) together with the current results presented in this study align with recent studies comparing speech production and perception, which highlight that these processes may rely on the same neural representations but are differentiated in their temporal dynamics (<xref ref-type="bibr" rid="bib22">Giglio et al., 2024</xref>; <xref ref-type="bibr" rid="bib16">Fairs et al., 2021</xref>). Moreover, the current findings deepen our understanding of the role of the cerebellum in speech production and perception and align with previous research highlighting its involvement in predictive processing mechanisms.</p><p>We also observed significant connectivity between the right cerebellum and the left parietal cortex, specifically in the low-frequency ranges, which peaked in the alpha range. This result is consistent with previous research demonstrating the alpha band connectivity between parietal and temporal areas during speech production and perception (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>). We and others previously suggested that the parietal cortex might control and modulate the alpha rhythms in the early auditory areas which serves as a multidimensional filter (<xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>; <xref ref-type="bibr" rid="bib29">Lakatos et al., 2019</xref>). Moreover, our current results reveal that during speaking and listening, the parietal cortex not only receives inputs from higher cortical areas such as the motor cortex, but also receives inputs from the cerebellum. These inputs enable the parietal cortex to encode predicted sensory consequences during speaking and provide top–down signals to early auditory areas. Additionally, our new findings demonstrate stronger connectivity from the cerebellum to the parietal areas during the speaking condition compared to the listening condition. This enhanced connectivity may be attributed to the selective inhibition of auditory signals during speaking, as predicted by the internal predictive coding model (<xref ref-type="bibr" rid="bib12">Cao et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Floegel et al., 2020</xref>).</p><p>The thalamus is another subcortical structure involved in speech perception and production which appears to play a significant role as it serves as a crucial intermediary in the cortico-subcortical neural network involving the cerebellum (<xref ref-type="bibr" rid="bib10">Barbas et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Silveri, 2021</xref>). Our findings reinforce this notion, revealing significant connectivity from the right cerebellum to the left thalamus in the low-frequency band, with the opposite direction observed in the high-frequency band during speaking. These connectivity patterns align with the results observed between the right cerebellum and other cortical areas, providing further support for the thalamus’s role in interconnecting cortical and subcortical structures, facilitating communication and coordination in various stages of verbal production (<xref ref-type="bibr" rid="bib14">Crosson, 2013</xref>). It is important to acknowledge that, while MEG’s spatial resolution is inherently lower for deep brain regions like the cerebellum compared to cortical areas, there is a large body of evidence (such as <xref ref-type="bibr" rid="bib9">Attal and Schwartz, 2013</xref>) demonstrating its capability to record signals from these regions, including the thalamus and cerebellum.</p><p>Our latest findings underscore that both speech production and perception involve distinct frequency channels for predictive processing, emphasizing the role of feedback and feedforward signaling in supporting this aspect of the predictive coding framework. Notably, we have also unveiled the anticipated role of the cerebellum within this framework. However, definitive conclusions should be drawn with caution given recent studies raising concerns about the notion that top–down and bottom–up signals can only be transmitted via separate frequency channels (<xref ref-type="bibr" rid="bib17">Ferro et al., 2021</xref>; <xref ref-type="bibr" rid="bib43">Schneider et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Vinck et al., 2023</xref>). Therefore, further investigation is warranted to thoroughly assess the alignment between our results and the contribution of the predictive coding framework in speech production and perception.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Thirty native German-speaking participants (15 males, mean age 25.1 ± 2.8 years [<italic>M</italic> ± SD], range 20–32 years) were recruited for this study. Prior written informed consent was obtained before measurements and participants received monetary compensation after partaking. The study was approved by the local ethics committee and conducted in accordance with the Declaration of Helsinki. This study additionally re-analyzes data previously collected for a study published in <xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>.</p></sec><sec id="s4-2"><title>Recordings</title><p>MEG, EMG, and speech signals were recorded simultaneously. The speech recording had a sampling rate of 44.1 kHz, whereas the MEG, a 275-channel, whole-head sensor system (OMEGA 275, VSM Medtech Ltd, Vancouver, BC, Canada) was sampled with 1200 Hz.</p><p>In order not to cause any artifacts by the microphone used for capturing audio data, it was placed at a distance of 155 cm from the participants mouth. Three pairs of EMG surface electrodes were placed after tactile inspection to find the correct location to capture muscle activity from the m. genioglossus, m. orbicularis oris, and m. zygomaticus major (for exact location see Figure 1 in <xref ref-type="bibr" rid="bib3">Abbasi et al., 2021</xref>). One pair of electrodes was used for each muscle with about 1 cm between electrodes. A low-pass online filter with a 300-Hz cut-off was applied to the recorded MEG and EMG data.</p></sec><sec id="s4-3"><title>Paradigm</title><p>Participants were asked to sit relaxed while performing the given tasks and to keep their eyes on a white fixation cross. The experiment was split in two separate parts: The first one consisted of answering given questions, each for 60 s, thus recording overt speech. Participants had to answer seven questions covering neutral topics, such as ‘What does a typical weekend look like for you?’. A color change from white to blue fixation cross indicated the beginning of the time period in which participants should speak and the end was marked by a color change back to white. The second part focused on perceiving speech in the way that participants listened to their own answers from part one. The list of questions as well as further details of the paradigm presented can be found in <xref ref-type="bibr" rid="bib4">Abbasi et al., 2023</xref>.</p></sec><sec id="s4-4"><title>Preprocessing and data analysis</title><p>Prior to data analysis, MEG data were visually inspected. No jump artifacts or bad channels were detected. A discrete Fourier transform filter was applied to eliminate 50 Hz line noise from the continuous MEG and EMG data. Moreover, EMG data were highpass-filtered at 20 Hz and rectified. Continuous head position and rotation were extracted from the fiducial coils placed at anatomical landmarks (nasion, left, and right ear canals). MEG, EMG, and head movement signals were downsampled to 256 Hz and segmented to non-overlapping 60 s trials corresponding to each of their overt answers. In the preprocessing and data analysis steps, custom-made scripts in Matlab R2020 (The Mathworks, Natick, MA, USA) in combination with the Matlab-based FieldTrip toolbox (<xref ref-type="bibr" rid="bib35">Oostenveld et al., 2011</xref>) were used in accord with current MEG guidelines (<xref ref-type="bibr" rid="bib25">Gross et al., 2013a</xref>).</p></sec><sec id="s4-5"><title>Artifact rejection</title><p>For removing the speech-related artifacts we used the pipeline presented in <xref ref-type="bibr" rid="bib3">Abbasi et al., 2021</xref>. In a nutshell, the artifact rejection comprises four major steps: (1) Head movement-related artifact was initially reduced by incorporating the head position time-series into the general linear model using regression analysis (<xref ref-type="bibr" rid="bib49">Stolk et al., 2013</xref>). (2) To further remove the residual artifact, SVD was used to estimate the spatial subspace (components) containing the speech-related artifact from the MEG data. (3) Artifactual components were detected via visual inspections and MI analysis and then removed from the single-trial data (<xref ref-type="bibr" rid="bib1">Abbasi et al., 2016</xref>). (4) Finally, all remaining components were back-transformed to the sensor level.</p></sec><sec id="s4-6"><title>Source localization</title><p>For source localization we aligned individual T1-weighted anatomical MRI scans with the digitized head shapes using the iterative closest point algorithm. Then, we segmented the MRI scans and generated single-shell volume conductor models (<xref ref-type="bibr" rid="bib34">Nolte, 2003</xref>), and used this to create forward models. For group analyses, individual MRIs were linearly transformed to an MNI template provided by Fieldtrip. Next, the linearly constrained minimum variance algorithm was used to compute time-series for each voxel on a 5-mm grid. The time-series were extracted for each dipole orientation, resulting in three time-series per voxel. The reduced version of the HCP brain atlas as well as AAL atlas were applied on the source space time-series in order to reduce the dimensionality of the data, resulting in 230 cortical parcels (<xref ref-type="bibr" rid="bib50">Tait et al., 2020</xref>) and 116 subcortical parcels, respectively. Since the HCP atlas only covers cortical areas we used the AAL atlas for subcortical areas. Finally, we extracted the first three components of an SVD of time-series from all dipoles in this parcel, explaining most of the variance.</p></sec><sec id="s4-7"><title>ROI selection</title><p>In this study, we focused on the brain regions that are involved in speech production and perception. In order to find the areas involved in speech production and perception network, we utilized the automatic meta-analysis provided by <ext-link ext-link-type="uri" xlink:href="https://neurosynth.org/">neurosynth.org</ext-link> using the term ‘speech’ that resulted in a meta-analysis of 642 fMRI studies highlighting the brain areas involved in speech production and perception (<xref ref-type="bibr" rid="bib53">Yarkoni et al., 2011</xref>). We extracted the MNI coordinates of active areas from the presented map, found their corresponding voxels and identified the respective parcels on HCP and AAL atlases where these voxels are located. This resulted in 14 cortical and subcortical parcels (see <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Selected ROI labels from HCP and AAL atlases.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">1</th><th align="left" valign="bottom">L-FOP</th><th align="left" valign="bottom">Left frontal opercular area</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">R-PEF+6v</td><td align="left" valign="bottom">Right premotor eyefield + ventral area 6</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">L-SCEF</td><td align="left" valign="bottom">Left supplementary and cingulate eyefield</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">R-SCEF</td><td align="left" valign="bottom">Right supplementary and cingulate eyefield</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">L-3b</td><td align="left" valign="bottom">Left primary somatosensory cortex</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">R-3b</td><td align="left" valign="bottom">Right primary somatosensory cortex</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">L-STS</td><td align="left" valign="bottom">Left superior temporal sulcus</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">L-TPOJ1</td><td align="left" valign="bottom">Left TemporoParietoOccipital Junction 1</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">L-A5</td><td align="left" valign="bottom">Left auditory complex 5</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="left" valign="bottom">R-A5</td><td align="left" valign="bottom">Right auditory complex 5</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom">L-TH</td><td align="left" valign="bottom">Left thalamus</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">R-CB-Cruss2</td><td align="left" valign="bottom">Right cerebellum crus 2</td></tr><tr><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom">L-CB-6</td><td align="left" valign="bottom">Left cerebellum 6</td></tr><tr><td align="char" char="." valign="bottom">14</td><td align="left" valign="bottom">R-CB-6</td><td align="left" valign="bottom">Right cerebellum 6</td></tr></tbody></table></table-wrap></sec><sec id="s4-8"><title>Connectivity analysis</title><p>We performed connectivity analysis by using an mGC approach (<xref ref-type="bibr" rid="bib42">Schaum et al., 2021</xref>; <xref ref-type="bibr" rid="bib15">Dhamala et al., 2008</xref>). We computed the mGC to determine the directionality of functional coupling between all the detected involved parcels, in pairwise steps, during speech production and perception. Initially, the source signals were divided into trials of 4 s, with 500 ms overlap. We used the fast Fourier transform in combination with multitapers (2 Hz smoothing) to compute the cross-spectral density matrix of the trials. Next, using a blockwise approach, we considered the first three SVD components of each parcel as a block and estimated the connectivity between STG and other parcels. Finally, we computed the directed influence asymmetry index (DAI) defined by <xref ref-type="bibr" rid="bib11">Bastos et al., 2015</xref> as<disp-formula id="equ1"><mml:math id="m1"><mml:mi>D</mml:mi><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>→</mml:mo><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>→</mml:mo><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Therefore, a positive DAI for a given frequency indicates that the selected parcel conveys feedforward influences to STG in this frequency, and a negative DAI indicates feedback influences. Note that for the connectivity analysis, we used MEG data with 1200 Hz sampling rate without downsampling.</p></sec><sec id="s4-9"><title>Statistical analysis</title><p>We determined significant connectivity patterns (DAI values) in both speaking and listening conditions using nonparametric cluster-based permutation tests (<xref ref-type="bibr" rid="bib32">Maris and Oostenveld, 2007</xref>). First, we estimated the statistical contrast of connectivities during speaking and listening compared to zero for each parcel and participant. Second, the DAI values in the speaking condition were contrasted with DAI values in the listening condition at the group level. The statistical analysis was conducted for different frequency bands (Delta/Theta (0–7 Hz), alpha (7–13 Hz), beta (15–30 Hz), gamma (30–60 Hz), and high gamma (60–90 Hz)) using a dependent-samples <italic>t</italic>-test. We used a cluster-based correction to account for multiple comparisons across frequencies and parcels. We performed 5000 permutations and set the critical alpha value at 0.05.</p></sec><sec id="s4-10"><title>Speech–brain coupling</title><p>For each parcel, we calculated the complex-valued spectral estimates of the three SVD components using multitaper analysis with ±2 Hz spectral smoothing on 2-s windows with 50% overlap. Subsequently, we estimated the MI between the speech envelope and the combined spectral information from all three time-series using Gaussian Copula MI. This approach resulted in a single MI value per parcel, reflecting speech–brain coupling specifically at the chosen 130 ms lag.</p></sec><sec id="s4-11"><title>Correlation analysis</title><p>To examine the potential relationship between our connectivity findings from all the parcels to R-CB6 and speech–STG coupling, we conducted nonparametric cluster-based permutation tests. Our first step was to calculate top–down connectivity values for selected parcels and frequency bands, then to compute speech–STG couplings for each frequency band. To account for multiple comparisons across parcels, we employed the Pearson method implemented in the ft_statfun_correlationT function in Fieldtrip, with cluster-based correction. Our analysis was repeated for different frequency bands. Therefore, our results are not corrected across frequencies. We performed 5000 permutations and set the critical alpha value at 0.05.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Data curation, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Prior written informed consent was obtained before measurements and participants received monetary compensation after partaking. The study was approved by the local ethics committee and conducted in accordance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-97083-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Custom-made scripts used for data analysis are publicly available at link: <ext-link ext-link-type="uri" xlink:href="https://osf.io/9fq47/">https://osf.io/9fq47/</ext-link>. Additionally, an example dataset is included at the same repository to facilitate understanding of the data structure. The raw data contains participants' audio speech files and is protected by data privacy laws, preventing public sharing. However, we are committed to responsible data sharing and can make the raw data available upon reasonable request, subject to data privacy regulations. To access the raw data, interested researchers can contact the corresponding author of this manuscript by informal email. They only need to briefly explain their research goals and the specific data subsets they require. No formal proposal is required.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Abbasi</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Preprocessed and anonymised data set example as well as source codes of speech perception and production study!</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/9fq47/">9fq47</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We acknowledge support by the Open Access Publication Fund of University of Münster. JG was also supported by the DFG (GR 2024/5-1, GR 2024/11-1, and GR 2024/12-1).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbasi</surname><given-names>O</given-names></name><name><surname>Hirschmann</surname><given-names>J</given-names></name><name><surname>Schmitz</surname><given-names>G</given-names></name><name><surname>Schnitzler</surname><given-names>A</given-names></name><name><surname>Butz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rejecting deep brain stimulation artefacts from MEG data using ICA and mutual information</article-title><source>Journal of Neuroscience Methods</source><volume>268</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.010</pub-id><pub-id pub-id-type="pmid">27090949</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbasi</surname><given-names>O</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Beta-band oscillations play an essential role in motor-auditory interactions</article-title><source>Human Brain Mapping</source><volume>41</volume><fpage>656</fpage><lpage>665</lpage><pub-id pub-id-type="doi">10.1002/hbm.24830</pub-id><pub-id pub-id-type="pmid">31639252</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbasi</surname><given-names>O</given-names></name><name><surname>Steingräber</surname><given-names>N</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Correcting MEG Artifacts Caused by Overt Speech</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>682419</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.682419</pub-id><pub-id pub-id-type="pmid">34168536</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbasi</surname><given-names>O</given-names></name><name><surname>Steingräber</surname><given-names>N</given-names></name><name><surname>Chalas</surname><given-names>N</given-names></name><name><surname>Kluger</surname><given-names>DS</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spatiotemporal dynamics characterise spectral connectivity profiles of continuous speaking and listening</article-title><source>PLOS Biology</source><volume>21</volume><elocation-id>e3002178</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002178</pub-id><pub-id pub-id-type="pmid">37478152</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackermann</surname><given-names>H</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Daum</surname><given-names>I</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Does the cerebellum contribute to cognitive aspects of speech production? A functional magnetic resonance imaging (fMRI) study in humans</article-title><source>Neuroscience Letters</source><volume>247</volume><fpage>187</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/s0304-3940(98)00328-0</pub-id><pub-id pub-id-type="pmid">9655624</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackermann</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cerebellar contributions to speech production and speech perception: psycholinguistic and neurobiological perspectives</article-title><source>Trends in Neurosciences</source><volume>31</volume><fpage>265</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.02.011</pub-id><pub-id pub-id-type="pmid">18471906</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandrou</surname><given-names>AM</given-names></name><name><surname>Saarinen</surname><given-names>T</given-names></name><name><surname>Mäkelä</surname><given-names>S</given-names></name><name><surname>Kujala</surname><given-names>J</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The right hemisphere is highlighted in connected natural speech production and perception</article-title><source>NeuroImage</source><volume>152</volume><fpage>628</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.006</pub-id><pub-id pub-id-type="pmid">28268122</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.05.003</pub-id><pub-id pub-id-type="pmid">22682813</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attal</surname><given-names>Y</given-names></name><name><surname>Schwartz</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Assessment of subcortical source localization using deep brain activity imaging model with minimum norm operators: a MEG study</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e59856</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0059856</pub-id><pub-id pub-id-type="pmid">23527277</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbas</surname><given-names>H</given-names></name><name><surname>García-Cabezas</surname><given-names>MÁ</given-names></name><name><surname>Zikopoulos</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Frontal-thalamic circuits associated with language</article-title><source>Brain and Language</source><volume>126</volume><fpage>49</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.10.001</pub-id><pub-id pub-id-type="pmid">23211411</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>Bosman</surname><given-names>CA</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Dowdall</surname><given-names>JR</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual areas exert feedforward and feedback influences through distinct frequency channels</article-title><source>Neuron</source><volume>85</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.018</pub-id><pub-id pub-id-type="pmid">25556836</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>L</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The role of brain oscillations in predicting self-generated sounds</article-title><source>NeuroImage</source><volume>147</volume><fpage>895</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.001</pub-id><pub-id pub-id-type="pmid">27818209</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalas</surname><given-names>N</given-names></name><name><surname>Daube</surname><given-names>C</given-names></name><name><surname>Kluger</surname><given-names>DS</given-names></name><name><surname>Abbasi</surname><given-names>O</given-names></name><name><surname>Nitsch</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multivariate analysis of speech envelope tracking reveals coupling beyond auditory cortex</article-title><source>NeuroImage</source><volume>258</volume><elocation-id>119395</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119395</pub-id><pub-id pub-id-type="pmid">35718023</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosson</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Thalamic mechanisms in language: a reconsideration based on recent findings and concepts</article-title><source>Brain and Language</source><volume>126</volume><fpage>73</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.06.011</pub-id><pub-id pub-id-type="pmid">22831779</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhamala</surname><given-names>M</given-names></name><name><surname>Rangarajan</surname><given-names>G</given-names></name><name><surname>Ding</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Analyzing information flow in brain networks with nonparametric Granger causality</article-title><source>NeuroImage</source><volume>41</volume><fpage>354</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.02.020</pub-id><pub-id pub-id-type="pmid">18394927</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairs</surname><given-names>A</given-names></name><name><surname>Michelas</surname><given-names>A</given-names></name><name><surname>Dufour</surname><given-names>S</given-names></name><name><surname>Strijkers</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The same ultra-rapid parallel brain dynamics underpin the production and perception of speech</article-title><source>Cerebral Cortex Communications</source><volume>2</volume><elocation-id>tgab040</elocation-id><pub-id pub-id-type="doi">10.1093/texcom/tgab040</pub-id><pub-id pub-id-type="pmid">34296185</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferro</surname><given-names>D</given-names></name><name><surname>van Kempen</surname><given-names>J</given-names></name><name><surname>Boyd</surname><given-names>M</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Directed information exchange between cortical layers in macaque V1 and V4 and its modulation by selective attention</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2022097118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2022097118</pub-id><pub-id pub-id-type="pmid">33723059</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floegel</surname><given-names>M</given-names></name><name><surname>Fuchs</surname><given-names>S</given-names></name><name><surname>Kell</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Differential contributions of the two cerebral hemispheres to temporal and spectral speech feedback control</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2839</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-16743-2</pub-id><pub-id pub-id-type="pmid">32503986</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Liegeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The contribution of frequency-specific activity to hierarchical information processing in the human auditory cortex</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>4694</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms5694</pub-id><pub-id pub-id-type="pmid">25178489</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active inference, communication and hermeneutics</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>68</volume><fpage>129</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.025</pub-id><pub-id pub-id-type="pmid">25957007</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganushchak</surname><given-names>LY</given-names></name><name><surname>Christoffels</surname><given-names>IK</given-names></name><name><surname>Schiller</surname><given-names>NO</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The use of electroencephalography in language production research: a review</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>208</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00208</pub-id><pub-id pub-id-type="pmid">21909333</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giglio</surname><given-names>L</given-names></name><name><surname>Ostarek</surname><given-names>M</given-names></name><name><surname>Sharoh</surname><given-names>D</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Diverging neural dynamics for syntactic structure building in naturalistic speaking and listening</article-title><source>PNAS</source><volume>121</volume><elocation-id>e2310766121</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2310766121</pub-id><pub-id pub-id-type="pmid">38442171</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Lund</surname><given-names>TE</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name><name><surname>Laufs</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Endogenous cortical rhythms determine cerebral specialization for speech perception and production</article-title><source>Neuron</source><volume>56</volume><fpage>1127</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.09.038</pub-id><pub-id pub-id-type="pmid">18093532</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Barnes</surname><given-names>GR</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Hillebrand</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Maess</surname><given-names>B</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Good practice for conducting and reporting MEG research</article-title><source>NeuroImage</source><volume>65</volume><fpage>349</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.10.001</pub-id><pub-id pub-id-type="pmid">23046981</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hoogenboom</surname><given-names>N</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001752</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id><pub-id pub-id-type="pmid">24391472</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janssen</surname><given-names>N</given-names></name><name><surname>van der Meij</surname><given-names>M</given-names></name><name><surname>López-Pérez</surname><given-names>PJ</given-names></name><name><surname>Barber</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Exploring the temporal dynamics of speech production with EEG and group ICA</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>3667</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-60301-1</pub-id><pub-id pub-id-type="pmid">32111868</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kotz</surname><given-names>SA</given-names></name><name><surname>Stockert</surname><given-names>A</given-names></name><name><surname>Schwartze</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cerebellum, temporal predictability and the updating of a mental model</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>369</volume><elocation-id>20130403</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0403</pub-id><pub-id pub-id-type="pmid">25385781</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A new unifying account of the roles of neuronal entrainment</article-title><source>Current Biology</source><volume>29</volume><fpage>R890</fpage><lpage>R905</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.07.075</pub-id><pub-id pub-id-type="pmid">31550478</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llorens</surname><given-names>A</given-names></name><name><surname>Trébuchon</surname><given-names>A</given-names></name><name><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Alario</surname><given-names>FX</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Intra-cranial recordings of brain activity during language production</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>375</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00375</pub-id><pub-id pub-id-type="pmid">22207857</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manto</surname><given-names>M</given-names></name><name><surname>Bower</surname><given-names>JM</given-names></name><name><surname>Conforto</surname><given-names>AB</given-names></name><name><surname>Delgado-García</surname><given-names>JM</given-names></name><name><surname>da Guarda</surname><given-names>SNF</given-names></name><name><surname>Gerwig</surname><given-names>M</given-names></name><name><surname>Habas</surname><given-names>C</given-names></name><name><surname>Hagura</surname><given-names>N</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name><name><surname>Mariën</surname><given-names>P</given-names></name><name><surname>Molinari</surname><given-names>M</given-names></name><name><surname>Naito</surname><given-names>E</given-names></name><name><surname>Nowak</surname><given-names>DA</given-names></name><name><surname>Oulad Ben Taib</surname><given-names>N</given-names></name><name><surname>Pelisson</surname><given-names>D</given-names></name><name><surname>Tesche</surname><given-names>CD</given-names></name><name><surname>Tilikete</surname><given-names>C</given-names></name><name><surname>Timmann</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Consensus paper: roles of the cerebellum in motor control--the diversity of ideas on cerebellar involvement in movement</article-title><source>Cerebellum</source><volume>11</volume><fpage>457</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1007/s12311-011-0331-9</pub-id><pub-id pub-id-type="pmid">22161499</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martikainen</surname><given-names>MH</given-names></name><name><surname>Kaneko</surname><given-names>K</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Suppressed responses to self-triggered sounds in the human auditory cortex</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>299</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh131</pub-id><pub-id pub-id-type="pmid">15238430</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Physics in Medicine and Biology</source><volume>48</volume><fpage>3637</fpage><lpage>3652</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/48/22/002</pub-id><pub-id pub-id-type="pmid">14680264</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozker</surname><given-names>M</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A cortical network processes auditory error signals during human speech production to maintain fluency</article-title><source>PLOS Biology</source><volume>20</volume><elocation-id>e3001493</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001493</pub-id><pub-id pub-id-type="pmid">35113857</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parrell</surname><given-names>B</given-names></name><name><surname>Agnew</surname><given-names>Z</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name><name><surname>Houde</surname><given-names>J</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Impaired feedforward control and enhanced feedback control of speech in patients with cerebellar degeneration</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>9249</fpage><lpage>9258</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3363-16.2017</pub-id><pub-id pub-id-type="pmid">28842410</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riès</surname><given-names>SK</given-names></name><name><surname>Dhillon</surname><given-names>RK</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>King-Stephens</surname><given-names>D</given-names></name><name><surname>Laxer</surname><given-names>KD</given-names></name><name><surname>Weber</surname><given-names>PB</given-names></name><name><surname>Kuperman</surname><given-names>RA</given-names></name><name><surname>Auguste</surname><given-names>KI</given-names></name><name><surname>Brunner</surname><given-names>P</given-names></name><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Lin</surname><given-names>JJ</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Crone</surname><given-names>NE</given-names></name><name><surname>Dronkers</surname><given-names>NF</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatiotemporal dynamics of word retrieval in speech production revealed by cortical high-frequency band activity</article-title><source>PNAS</source><volume>114</volume><fpage>E4530</fpage><lpage>E4538</lpage><pub-id pub-id-type="doi">10.1073/pnas.1620669114</pub-id><pub-id pub-id-type="pmid">28533406</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinov</surname><given-names>M</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Complex network measures of brain connectivity: uses and interpretations</article-title><source>NeuroImage</source><volume>52</volume><fpage>1059</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.003</pub-id><pub-id pub-id-type="pmid">19819337</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruspantini</surname><given-names>I</given-names></name><name><surname>Saarinen</surname><given-names>T</given-names></name><name><surname>Belardinelli</surname><given-names>P</given-names></name><name><surname>Jalava</surname><given-names>A</given-names></name><name><surname>Parviainen</surname><given-names>T</given-names></name><name><surname>Kujala</surname><given-names>J</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Corticomuscular coherence is tuned to the spontaneous rhythmicity of speech at 2-3 Hz</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3786</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3191-11.2012</pub-id><pub-id pub-id-type="pmid">22423099</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaum</surname><given-names>M</given-names></name><name><surname>Pinzuti</surname><given-names>E</given-names></name><name><surname>Sebastian</surname><given-names>A</given-names></name><name><surname>Lieb</surname><given-names>K</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Mobascher</surname><given-names>A</given-names></name><name><surname>Jung</surname><given-names>P</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Tüscher</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Right inferior frontal gyrus implements motor inhibitory control via beta-band oscillations in humans</article-title><source>eLife</source><volume>10</volume><elocation-id>e61679</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61679</pub-id><pub-id pub-id-type="pmid">33755019</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>M</given-names></name><name><surname>Broggini</surname><given-names>AC</given-names></name><name><surname>Dann</surname><given-names>B</given-names></name><name><surname>Tzanou</surname><given-names>A</given-names></name><name><surname>Uran</surname><given-names>C</given-names></name><name><surname>Sheshadri</surname><given-names>S</given-names></name><name><surname>Scherberger</surname><given-names>H</given-names></name><name><surname>Vinck</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A mechanism for inter-areal coherence through communication based on connectivity and oscillatory power</article-title><source>Neuron</source><volume>109</volume><fpage>4050</fpage><lpage>4067</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.09.037</pub-id><pub-id pub-id-type="pmid">34637706</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Gander</surname><given-names>PE</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Kovach</surname><given-names>CK</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Kawasaki</surname><given-names>H</given-names></name><name><surname>Howard</surname><given-names>MA</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural signatures of perceptual inference</article-title><source>eLife</source><volume>5</volume><elocation-id>e11476</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11476</pub-id><pub-id pub-id-type="pmid">26949254</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Simony</surname><given-names>E</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Coupled neural systems underlie the production and comprehension of naturalistic narrative speech</article-title><source>PNAS</source><volume>111</volume><fpage>E4687</fpage><lpage>E4696</lpage><pub-id pub-id-type="doi">10.1073/pnas.1323812111</pub-id><pub-id pub-id-type="pmid">25267658</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silveri</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Contribution of the cerebellum and the basal ganglia to language production: speech, word fluency, and sentence construction-evidence from pathology</article-title><source>Cerebellum</source><volume>20</volume><fpage>282</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1007/s12311-020-01207-6</pub-id><pub-id pub-id-type="pmid">33120434</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Lametti</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Speech perception under the tent: a domain-general predictive role for the cerebellum</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>1517</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01729</pub-id><pub-id pub-id-type="pmid">34496370</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stockert</surname><given-names>A</given-names></name><name><surname>Schwartze</surname><given-names>M</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Anwander</surname><given-names>A</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Temporo-cerebellar connectivity underlies timing constraints in audition</article-title><source>eLife</source><volume>10</volume><elocation-id>e67303</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67303</pub-id><pub-id pub-id-type="pmid">34542407</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Todorovic</surname><given-names>A</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Online and offline tools for head movement compensation in MEG</article-title><source>NeuroImage</source><volume>68</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.11.047</pub-id><pub-id pub-id-type="pmid">23246857</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tait</surname><given-names>L</given-names></name><name><surname>Özkan</surname><given-names>A</given-names></name><name><surname>Szul</surname><given-names>MJ</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical Source Imaging of Resting-State MEG with a High Resolution Atlas: An Evaluation of Methods</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.12.903302</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorović</surname><given-names>S</given-names></name><name><surname>Anton</surname><given-names>JL</given-names></name><name><surname>Sein</surname><given-names>J</given-names></name><name><surname>Nazarian</surname><given-names>B</given-names></name><name><surname>Chanoine</surname><given-names>V</given-names></name><name><surname>Rauchbauer</surname><given-names>B</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name><name><surname>Runnqvist</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Cortico-cerebellar monitoring of speech sequence production</article-title><source>Neurobiology of Language</source><volume>5</volume><fpage>701</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1162/nol_a_00113</pub-id><pub-id pub-id-type="pmid">39175789</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Uran</surname><given-names>C</given-names></name><name><surname>Spyropoulos</surname><given-names>G</given-names></name><name><surname>Onorato</surname><given-names>I</given-names></name><name><surname>Broggini</surname><given-names>AC</given-names></name><name><surname>Schneider</surname><given-names>M</given-names></name><name><surname>Canales-Johnson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Principles of large-scale neural interactions</article-title><source>Neuron</source><volume>111</volume><fpage>987</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.015</pub-id><pub-id pub-id-type="pmid">37023720</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title><source>Nature Methods</source><volume>8</volume><fpage>665</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id><pub-id pub-id-type="pmid">21706013</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97083.2.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Incomplete</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>Abbasi and colleagues use Granger causality to explore the cortico-subcortical dynamics during speaking and listening. They find <bold>valuable</bold> evidence for bi-directional connectivity in distinct frequency bands as a function of behaviour, but currently offer <bold>incomplete</bold> support for the validity of their analyses and the predictive coding interpretation of their results.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97083.2.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Abbasi et al. assess in this MEG study the directed connectivity of both cortical and subcortical regions during continuous speech production and perception. The authors observed bidirectional connectivity patterns between speech-related cortical areas as well as subcortical areas in production and perception. Interestingly, they found in speaking low-frequency connectivity from subcortical (the right cerebellum) to cortical (left superior temporal) areas, while connectivity from the cortical to subcortical areas was in the high frequencies. In listening a similar cortico-subcortical connectivity pattern was observed for the low frequencies, but the reversed connectivity in the higher frequencies was absent.</p><p>The work by Abbasi and colleagues addresses a relevant, novel topic, namely understanding the brain dynamics between speaking and listening. This is important because traditionally production and perception of speech and language are investigated in a modality-specific manner. To have a more complete understanding of the neurobiology underlying these different speech behaviors, it is key to also understand their similarities and differences. Furthermore, to do so, the authors utilize state-of-the-art directed connectivity analyses on MEG measurements, providing a quite detailed profile of cortical and subcortical interactions for the production and perception of speech. Importantly, and perhaps most interesting in my opinion, is that the authors find evidence for frequency-specific directed connectivity, which is (partially) different between speaking and listening. This could suggest that both speech behaviors rely (to some extent) on similar cortico-cortical and cortico-subcortical networks, but different frequency-specific dynamics.</p><p>These elements mentioned above (investigation of both production and perception, both cortico-cortical and cortico-subcortical connectivity is considered, and observing frequency-specific connectivity profiles within and between speech behaviors), make for important novel contributions to the field. Notwithstanding these strengths, I find that they are especially centered on methodology and functional anatomical description, but that precise theoretical contributions for neurobiological and cognitive models of speech are less transparent. This is in part because the study compares speech production and perception in general, but no psychophysical or psycholinguistic manipulations are considered. I also have some critical questions about the design which may pose some confounds in interpreting the data, especially with regard to comparing production and perception.</p><p>(1) While the cortico-cortical and cortico-subcortical connectivity profiles highlighted in this study and the depth of the analyses are impressive, what these data mean for models of speech processing remains on the surface. This is in part due, I believe, to the fact that the authors have decided to explore speaking and listening in general, without targeting specific manipulations that help elucidate which aspects of speech processing are relevant for the particular connectivity profiles they have uncovered. For example, the frequency-specific directed connectivity is it driven by low-level psychophysical attributes of the speech or by more cognitive linguistic properties? Does it relate to the monitoring of speech, timing information, and updating of sensory predictions? Without manipulations trying to target one or several of these components, as some of the referenced work has done (e.g., Floegel et al., 2020; Stockert et al., 2021; Todorović et al., 2023), it is difficult to draw concrete conclusions as to which representations and/or processes of speech are reflected by the connectivity profiles. An additional disadvantage of not having manipulations within each speech behavior is that it makes the comparison between listening and speaking harder. That is, speaking and listening have marked input-output differences which likely will dominate any comparison between them. These physically driven differences (or similarities for that matter; see below) can be strongly reduced by instead exploring the same manipulations/variables between speaking and listening. If possible (if not to consider for future work), it may be interesting to score psychophysical (e.g., acoustic properties) or psycholinguistic (e.g., lexical frequency) information of the speech and see whether and how the frequency-specific connectivity profiles are affected by it.</p><p>(2) Recent studies comparing the production and perception of language may be relevant to the current study and add some theoretical weight since their data and interpretations for the comparisons between production and perception fit quite well with the observations in the current work. These studies highlight that language processes between production and perception, specifically lexical and phonetic processing (Fairs et al., 2021), and syntactic processing (Giglio et al., 2024), may rely on the same neural representations, but are differentiated in their (temporal) dynamics upon those shared representations. This is relevant because it dispenses with the classical notion in neurobiological models of language where production and perception rely on (partially) dissociable networks (e.g., Price, 2010). Rather those data suggest shared networks where different language behaviors are dissociated in their dynamics. The speech results in this study nicely fit and extend those studies and their theoretical implications.</p><p>(3) The authors align the frequency-selective connectivity between the right cerebellum and left temporal speech areas with recent studies demonstrating a role for the right cerebellum for the internal modelling in speech production and monitoring (e.g., Stockert et al., 2021; Todorović et al., 2023). This link is indeed interesting, but it does seem relevant to point out that at a more specific scale, it does not concern the exact same regions between those studies and the current study. That is, in the current study the frequency-specific connectivity with temporal regions concerns lobule VI in the right cerebellum, while in the referenced work it concerns Crus I/II. The distinction seems relevant since Crus I/II has been linked to the internal modelling of more cognitive behavior, while lobule VI seems more motor-related and/or contextual-related (e.g., D'Mello et al., 2020; Runnqvist et al., 2021; Runnqvist, 2023).</p><p>(4) On the methodological side, my main concern is that for the listening condition, the authors have chosen to play back the speech produced by the participants in the production condition. Both the fixed order as well as hearing one's own speech as listening condition may produce confounds in data interpretation, especially with regard to the comparison between speech production and perception. Could order effects impact the observed connectivity profiles, and how would this impact the comparison between speaking and listening? In particular, I am thinking of repetition effects present in the listening condition as well as prediction, which will be much more elevated for the listening condition than the speaking condition. The fact that it also concerns their own voice furthermore adds to the possible predictability confound (e.g., Heinks-Maldonado et al., 2005). In addition, listening to one's speech which just before has been articulated may, potentially strategically even, enhance inner speech and &quot;mouthing&quot; in the participants, hereby thus engaging the production mechanism. Similarly, during production, the participants already hear their own voice (which serves as input in the subsequent listening condition). Taken together, both similarities or differences between speaking and listening connectivity may have been due to or influenced by these order effects, and the fact that the different speech behaviors are to some extent present in both conditions.</p><p>(5) The ability of the authors to analyze the spatiotemporal dynamics during continuous speech is a potentially important feat of this study, given that one of the reasons that speech production is much less investigated compared to perception concerns motor and movement artifacts due to articulation (e.g., Strijkers et al., 2010). Two questions did spring to mind when reading the authors' articulation artifact correction procedure: If I understood correctly, the approach comes from Abbasi et al. (2021) and is based on signal space projection (SSP) as used for eye movement corrections, which the authors successfully applied to speech production. However, in that study, it concerned the repeated production of three syllables, while here it concerns continuous speech of full words embedded in discourse. The articulation and muscular variance will be much higher in the current study compared to three syllables (or compared to eye movements which produce much more stable movement potentials compared to an entire discourse). Given this, I can imagine that corrections of the signal in the speaking condition were likely substantial and one may wonder (1) how much signal relevant to speech production behavior is lost?; (2) similar corrections are not necessary for perception, so how would this marked difference in signal processing affect the comparability between the modalities?</p><p>References:</p><p>- Abbasi, O., Steingräber, N., &amp; Gross, J. (2021). Correcting MEG artifacts caused by overt speech. Frontiers in Neuroscience, 15, 682419.</p><p>- D'Mello, A. M., Gabrieli, J. D., &amp; Nee, D. E. (2020). Evidence for hierarchical cognitive control in the human cerebellum. Current Biology, 30(10), 1881-1892.</p><p>- Fairs, A., Michelas, A., Dufour, S., &amp; Strijkers, K. (2021). The same ultra-rapid parallel brain dynamics underpin the production and perception of speech. Cerebral Cortex Communications, 2(3), tgab040.</p><p>- Floegel, M., Fuchs, S., &amp; Kell, C. A. (2020). Differential contributions of the two cerebral hemispheres to temporal and spectral speech feedback control. Nature Communications, 11(1), 2839.</p><p>- Giglio, L., Ostarek, M., Sharoh, D., &amp; Hagoort, P. (2024). Diverging neural dynamics for syntactic structure building in naturalistic speaking and listening. Proceedings of the National Academy of Sciences, 121(11), e2310766121.</p><p>- Heinks‐Maldonado, T. H., Mathalon, D. H., Gray, M., &amp; Ford, J. M. (2005). Fine‐tuning of auditory cortex during speech production. Psychophysiology, 42(2), 180-190.</p><p>- Price, C. J. (2010). The anatomy of language: a review of 100 fMRI studies published in 2009. Annals of the new York Academy of Sciences, 1191(1), 62-88.</p><p>- Runnqvist, E., Chanoine, V., Strijkers, K., Pattamadilok, C., Bonnard, M., Nazarian, B., ... &amp; Alario, F. X. (2021). Cerebellar and cortical correlates of internal and external speech error monitoring. Cerebral Cortex Communications, 2(2), tgab038.</p><p>- Runnqvist, E. (2023). Self-monitoring: The neurocognitive basis of error monitoring in language production. In Language production (pp. 168-190). Routledge.</p><p>- Stockert, A., Schwartze, M., Poeppel, D., Anwander, A., &amp; Kotz, S. A. (2021). Temporo-cerebellar connectivity underlies timing constraints in audition. Elife, 10, e67303.</p><p>- Strijkers, K., Costa, A., &amp; Thierry, G. (2010). Tracking lexical access in speech production: electrophysiological correlates of word frequency and cognate effects. Cerebral cortex, 20(4), 912-928.</p><p>- Todorović, S., Anton, J. L., Sein, J., Nazarian, B., Chanoine, V., Rauchbauer, B., ... &amp; Runnqvist, E. (2023). Cortico-cerebellar monitoring of speech sequence production. Neurobiology of Language, 1-21.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97083.2.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors re-analyse MEG data from a speech production and perception study and extend their previous Granger causality analysis to a larger number of cortical-cortical and in particular cortical-subcortical connections. Regions of interest were defined by means of a meta-analysis using Neurosynth.org and connectivity patterns were determined by calculating directed influence asymmetry indices from the Granger causality analysis results for each pair of brain regions. Abbasi et al. report feedforward signals communicated via fast rhythms and feedback signals via slow rhythms below 40 Hz, particularly during speaking. The authors highlight one of these connections between the right cerebellum lobule VI and auditory association area A5, where in addition the connection strength correlates negatively with the strength of speech tracking in the theta band during speaking (significant before multiple comparison correction). Results are interpreted within a framework of active inference by minimising prediction errors.</p><p>While I find investigating the role of cortical-subcortical connections in speech production and perception interesting and relevant to the field, I am not yet convinced that the methods employed are fully suitable to this endeavour or that the results provide sufficient evidence to make the strong claim of dissociation of bottom-up and top-down information flow during speaking in distinct frequency bands.</p><p>Strengths:</p><p>The investigation of electrophysiological cortical-subcortical connections in speech production and perception is interesting and relevant to the field. The authors analyse a valuable dataset, where they spent a considerable amount of effort to correct for speech production-related artefacts. Overall, the manuscript is well-written and clearly structured.</p><p>Weaknesses:</p><p>The description of the multivariate Granger causality analysis did not allow me to fully grasp how the analysis was performed and I hence struggled to evaluate its appropriateness.</p><p>Knowing that (1) filtered Granger causality is prone to false positives and (2) recent work demonstrates that significant Granger causality can simply arise from frequency-specific activity being present in the source but not the target area without functional relevance for communication (Schneider et al. 2021) raises doubts about the validity of the results, in particular with respect to their frequency specificity. These doubts are reinforced by what I perceive as an overemphasis on results that support the assumption of specific frequencies for feedforward and top-down connections, while findings not aligning with this hypothesis appear to be underreported. Furthermore, the authors report some main findings that I found difficult to reconcile with the data presented in the figures. Overall, I feel the conclusions with respect to frequency-specific bottom-up and top-down information flow need to be moderated and that some of the reported findings need to be checked and if necessary corrected.</p><p>Major points</p><p>(1) I think more details on the multivariate GC approach are needed. I found the reference to Schaum et al., 2021 not sufficient to understand what has been done in this paper. Some questions that remained for me are:</p><p>(i) Does multivariate here refer to the use of the authors' three components per parcel or to the conditioning on the remaining twelve sources? I think the latter is implied when citing Schaum et al., but I'm not sure this is what was done here?</p><p>If it was not: how can we account for spurious results based on indirect effects?</p><p>(ii) Did the authors check whether the GC of the course-target pairs was reliably above the bias level (as Schaum et. al. did for each condition separately)? If not, can they argue why they think that their results would still be valid? Does it make sense to compute DAIs on connections that were below the bias level? Should the data be re-analysed to take this concern into account?</p><p>(iii) You may consider citing the paper that introduced the non-parametric GC analysis (which Schaum et al. then went on to apply): Dhamala M, Rangarajan G, Ding M. Analyzing Information Flow in Brain Networks with Nonparametric Granger Causality. Neuroimage. 2008; 41(2):354-362. https://doi.org/10.1016/j.neuroimage.2008.02. 020</p><p>(2) GC has been discouraged for filtered data as it gives rise to false positives due to phase distortions and the ineffectiveness of filtering in the information-theoretic setting as reducing the power of a signal does not reduce the information contained in it (Florin et al., 2010; Barnett and Seth, 2011; Weber et al. 2017; Pinzuti et al., 2020 - who also suggest an approach that would circumvent those filter-related issues). With this in mind, I am wondering whether the strong frequency-specific claims in this work still hold.</p><p>(3) I found it difficult to reconcile some statements in the manuscript with the data presented in the figures:</p><p>(i) Most notably, the considerable number of feedforward connections from A5 and STS that project to areas further up the hierarchy at slower rhythms (e.g. L-A5 to R-PEF, R-Crus2, L CB6 L-Tha, L-FOP and L-STS to R-PEF, L-FOP, L-TOPJ or R-A5 as well as R-STS both to R-Crus2, L-CB6, L-Th) contradict the authors' main message that 'feedback signals were communicated via slow rhythms below 40 Hz, whereas feedforward signals were communicated via faster rhythms'. I struggled to recognise a principled approach that determined which connections were highlighted and reported and which ones were not.</p><p>(ii) &quot;Our analysis also revealed robust connectivity between the right cerebellum and the left parietal cortex, evident in both speaking and listening conditions, with stronger connectivity observed during speaking. Notably, Figure 4 depicts a prominent frequency peak in the alpha band, illustrating the specific frequency range through which information flows from the cerebellum to the parietal areas.&quot; There are two peaks discernible in Figure 4, one notably lower than the alpha band (rather theta or even delta), the other at around 30 Hz. Nevertheless, the authors report and discuss a peak in the alpha band.</p><p>(iii) In the abstract: &quot;Notably, high-frequency connectivity was absent during the listening condition.&quot; and p.9 &quot;In contrast with what we reported for the speaking condition, during listening, there is only a significant connectivity in low frequency to the left temporal area but not a reverse connection in the high frequencies.&quot;</p><p>While Fig. 4 shows significant connectivity from R-CB6 to A5 in the gamma frequency range for the speaking, but not for the listening condition, interpreting comparisons between two effects without directly comparing them is a common statistical mistake (Makin and Orban de Xivry). The spectrally-resolved connectivity in the two conditions actually look remarkably similar and I would thus refrain from highlighting this statement and indicate clearly that there were no significant differences between the two conditions.</p><p>(iv) &quot;This result indicates that in low frequencies, the sensory-motor area and cerebellum predominantly transmit information, while in higher frequencies, they are more involved in receiving it.&quot;</p><p>I don't think that this statement holds in its generality: L-CB6 and R-3b both show strong output at high frequencies, particularly in the speaking condition. While they seem to transmit information mainly to areas outside A5 and STS these effects are strong and should be discussed.</p><p>(4) &quot;However, definitive conclusions should be drawn with caution given recent studies raising concerns about the notion that top-down and bottom-up signals can only be transmitted via separate frequency channels (Ferro et al., 2021; Schneider et al., 2021; Vinck et al., 2023).&quot;</p><p>I appreciate this note of caution and think it would be useful if it were spelled out to the reader why this is the case so that they would be better able to grasp the main concerns here. For example, Schneider et al. make a strong point that we expect to find Granger-causality with a peak in a specific frequency band for areas that are anatomically connected when the sending area shows stronger activity in that band than the receiving one, simply because of the coherence of a signal with its own linear projection onto the other area. The direction of a Granger causal connection would in that case only indicate that one area shows stronger activity than the other in the given frequency band. I am wondering to what degree the reported connectivity pattern can be traced back to regional differences in frequency-specific source strength or to differences in source strength across the two conditions.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97083.2.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In the current paper, Abbasi et al. aimed to characterize and compare the patterns of functional connectivity across frequency bands (1 Hz - 90 Hz) between regions of a speech network derived from an online meta-analysis tool (Neurosynth.org) during speech production and perception. The authors present evidence for complex neural dynamics from which they highlight directional connectivity from the right cerebellum to left superior temporal areas in lower frequency bands (up to beta) and between the same regions in the opposite direction in the (lower) high gamma range (60-90 Hz). Abbasi et al. interpret their findings within the predictive coding framework, with the cerebellum and other &quot;higher-order&quot; (motor) regions transmitting top-down sensory predictions to &quot;lower-order&quot; (sensory) regions in the lower frequencies and prediction errors flowing in the opposite direction (i.e., bottom-up) from those sensory regions in the gamma band. They also report a negative correlation between the strength of this top-down functional connectivity and the alignment of superior temporal regions to the syllable rate of one's speech.</p><p>Strengths:</p><p>(1) The comprehensive characterization of functional connectivity during speaking and listening to speech may be valuable as a first step toward understanding the neural dynamics involved.</p><p>(2) The inclusion of subcortical regions and connectivity profiles up to 90Hz using MEG is interesting and relatively novel.</p><p>(3) The analysis pipeline is generally adequate for the exploratory nature of the work.</p><p>Weaknesses:</p><p>(1) The work is framed as a test of the predictive coding theory as it applies to speech production and perception, but the methodological approach is not suited to this endeavor.</p><p>(2) Because of their theoretical framework, the authors readily attribute roles or hierarchy to brain regions (e.g., higher- vs lower-order) and cognitive functions to observed connectivity patterns (e.g., feedforward vs feedback, predictions vs prediction errors) that cannot be determined from the data. Thus, many of the authors' claims are unsupported.</p><p>(3) The authors' theoretical stance seems to influence the presentation of the results, which may inadvertently misrepresent the (otherwise perfectly valid; cf. Abbasi et al., 2023) exploratory nature of the study. Thus, results about specific regions are often highlighted in figures (e.g., Figure 2 top row) and text without clear reasons.</p><p>(4) Some of the key findings (e.g., connectivity in opposite directions in distinct frequency bands) feature in a previous publication and are, therefore, interesting but not novel.</p><p>(5) The quantitative comparison between speech production and perception is interesting but insufficiently motivated.</p><p>(6) Details about the Neurosynth meta-analysis and subsequent selection of brain regions for the functional connectivity analyses are incomplete. Moreover, the use of the term 'Speech' in Neurosynth seems inappropriate (i.e., includes irrelevant works, yielding questionable results). The approach of using separate meta-analyses for 'Speech production' and 'Speech perception' taken by Abbasi et al. (2023) seems more principled. This approach would result, for example, in the inclusion of brain areas such as M1 and the BG that are relevant for speech production.</p><p>(7) The results involving subcortical regions are central to the paper, but no steps are taken to address the challenges involved in the analysis of subcortical activity using MEG. Additional methodological detail and analyses would be required to make these results more compelling. For example, it would be important to know what the coverage of the MEG system is, what head model was used for the source localization of cerebellar activity, and if specific preprocessing or additional analyses were performed to ensure that the localized subcortical activity (in particular) is valid.</p><p>(8) The results and methods are often detailed with important omissions (a speech-brain coupling analysis section is missing) and imprecisions (e.g., re: Figure 5; the Connectivity Analysis section is copy-pasted from their previous work), which makes it difficult to understand what is being examined and how. (It is also not good practice to refer the reader to previous publications for basic methodological details, for example, about the experimental paradigm and key analyses.) Conversely, some methodological details are given, e.g., the acquisition of EMG data, without further explanation of how those data were used in the current paper.</p><p>(9) The examination of gamma functional connectivity in the 60 - 90 Hz range could be better motivated. Although some citations involving short-range connectivity in these frequencies are given (e.g., within the visual system), a more compelling argument for looking at this frequency range for longer-range connectivity may be required.</p><p>(10) The choice of source localization method (linearly constrained minimum variance) could be explained, particularly given that other methods (e.g. dynamic imaging of coherent sources) were specifically designed and might potentially be a better alternative for the types of analyses performed in the study.</p><p>(11) The mGC analysis needs to be more comprehensively detailed for the reader to be able to assess what is being reported and the strength of the evidence. Relatedly, first-level statistics (e.g., via estimation of the noise level) would make the mGC and DAI results more compelling.</p><p>(12) Considering the exploratory nature of the study, it is essential for other researchers to continue investigating and validating the results presented in the current manuscript. Thus, it is concerning that data and scripts are not fully and openly available. Data need not be in its raw state to be shared and useful, which circumvents the stated data privacy concerns.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97083.2.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Abbasi</surname><given-names>Omid</given-names></name><role specific-use="author">Author</role><aff><institution>University of Münster</institution><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Steingräber</surname><given-names>Nadine</given-names></name><role specific-use="author">Author</role><aff><institution>University of Münster</institution><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Chalas</surname><given-names>Nikos</given-names></name><role specific-use="author">Author</role><aff><institution>University of Münster</institution><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Kluger</surname><given-names>Daniel S</given-names></name><role specific-use="author">Author</role><aff><institution>University of Münster</institution><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Gross</surname><given-names>Joachim</given-names></name><role specific-use="author">Author</role><aff><institution>University of Muenster</institution><addr-line><named-content content-type="city">Muenster</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Abbasi et al. assess in this MEG study the directed connectivity of both cortical and subcortical regions during continuous speech production and perception. The authors observed bidirectional connectivity patterns between speech-related cortical areas as well as subcortical areas in production and perception. Interestingly, they found in speaking low-frequency connectivity from subcortical (the right cerebellum) to cortical (left superior temporal) areas, while connectivity from the cortical to subcortical areas was in the high frequencies. In listening a similar cortico-subcortical connectivity pattern was observed for the low frequencies, but the reversed connectivity in the higher frequencies was absent.</p><p>The work by Abbasi and colleagues addresses a relevant, novel topic, namely understanding the brain dynamics between speaking and listening. This is important because traditionally production and perception of speech and language are investigated in a modality-specific manner. To have a more complete understanding of the neurobiology underlying these different speech behaviors, it is key to also understand their similarities and differences. Furthermore, to do so, the authors utilize state-of-the-art directed connectivity analyses on MEG measurements, providing a quite detailed profile of cortical and subcortical interactions for the production and perception of speech. Importantly, and perhaps most interesting in my opinion, is that the authors find evidence for frequency-specific directed connectivity, which is (partially) different between speaking and listening. This could suggest that both speech behaviors rely (to some extent) on similar cortico-cortical and cortico-subcortical networks, but different frequency-specific dynamics.</p><p>These elements mentioned above (investigation of both production and perception, both cortico-cortical and cortico-subcortical connectivity is considered, and observing frequency-specific connectivity profiles within and between speech behaviors), make for important novel contributions to the field. Notwithstanding these strengths, I find that they are especially centered on methodology and functional anatomical description, but that precise theoretical contributions for neurobiological and cognitive models of speech are less transparent. This is in part because the study compares speech production and perception in general, but no psychophysical or psycholinguistic manipulations are considered. I also have some critical questions about the design which may pose some confounds in interpreting the data, especially with regard to comparing production and perception.</p><p>(1) While the cortico-cortical and cortico-subcortical connectivity profiles highlighted in this study and the depth of the analyses are impressive, what these data mean for models of speech processing remains on the surface. This is in part due, I believe, to the fact that the authors have decided to explore speaking and listening in general, without targeting specific manipulations that help elucidate which aspects of speech processing are relevant for the particular connectivity profiles they have uncovered. For example, the frequency-specific directed connectivity is it driven by low-level psychophysical attributes of the speech or by more cognitive linguistic properties? Does it relate to the monitoring of speech, timing information, and updating of sensory predictions? Without manipulations trying to target one or several of these components, as some of the referenced work has done (e.g., Floegel et al., 2020; Stockert et al., 2021; Todorović et al., 2023), it is difficult to draw concrete conclusions as to which representations and/or processes of speech are reflected by the connectivity profiles. An additional disadvantage of not having manipulations within each speech behavior is that it makes the comparison between listening and speaking harder. That is, speaking and listening have marked input-output differences which likely will dominate any comparison between them. These physically driven differences (or similarities for that matter; see below) can be strongly reduced by instead exploring the same manipulations/variables between speaking and listening. If possible (if not to consider for future work), it may be interesting to score psychophysical (e.g., acoustic properties) or psycholinguistic (e.g., lexical frequency) information of the speech and see whether and how the frequency-specific connectivity profiles are affected by it.</p></disp-quote><p>We thank the reviewer for pointing this out. The current study is indeed part of a larger project investigating the role of the internal forward model in speech perception and production. In the original, more comprehensive study, we also included a masked condition where participants produced speech as usual, but their auditory perception was masked. This allowed us to examine how the internal forward model behaves when it doesn't receive the expected sensory consequences of generated speech. However, for the current study, we focused solely on data from the speaking and listening conditions due to its specific research question. We agree that further manipulations would be interesting. However, for this study our focus was on natural speech and we avoided other manipulations (beyond masked speech) so that we can have sufficiently long recording time for the main speaking and listening conditions.</p><disp-quote content-type="editor-comment"><p>(2) Recent studies comparing the production and perception of language may be relevant to the current study and add some theoretical weight since their data and interpretations for the comparisons between production and perception fit quite well with the observations in the current work. These studies highlight that language processes between production and perception, specifically lexical and phonetic processing (Fairs et al., 2021), and syntactic processing (Giglio et al., 2024), may rely on the same neural representations, but are differentiated in their (temporal) dynamics upon those shared representations. This is relevant because it dispenses with the classical notion in neurobiological models of language where production and perception rely on (partially) dissociable networks (e.g., Price, 2010). Rather those data suggest shared networks where different language behaviors are dissociated in their dynamics. The speech results in this study nicely fit and extend those studies and their theoretical implications.</p></disp-quote><p>We thank the reviewer for the suggestion and we will include these references and the points made by the reviewer in our revised manuscript.</p><disp-quote content-type="editor-comment"><p>(3) The authors align the frequency-selective connectivity between the right cerebellum and left temporal speech areas with recent studies demonstrating a role for the right cerebellum for the internal modelling in speech production and monitoring (e.g., Stockert et al., 2021; Todorović et al., 2023). This link is indeed interesting, but it does seem relevant to point out that at a more specific scale, it does not concern the exact same regions between those studies and the current study. That is, in the current study the frequency-specific connectivity with temporal regions concerns lobule VI in the right cerebellum, while in the referenced work it concerns Crus I/II. The distinction seems relevant since Crus I/II has been linked to the internal modelling of more cognitive behavior, while lobule VI seems more motor-related and/or contextual-related (e.g., D'Mello et al., 2020; Runnqvist et al., 2021; Runnqvist, 2023).</p></disp-quote><p>We thank the reviewer for their insightful comment. The reference was intended to provide evidence for the role of the cerebellum in internal modelling in speech. We do not claim that we have the spatial resolution with MEG to reliably spatially resolve specific parts of the cerebellum.</p><disp-quote content-type="editor-comment"><p>(4) On the methodological side, my main concern is that for the listening condition, the authors have chosen to play back the speech produced by the participants in the production condition. Both the fixed order as well as hearing one's own speech as listening condition may produce confounds in data interpretation, especially with regard to the comparison between speech production and perception. Could order effects impact the observed connectivity profiles, and how would this impact the comparison between speaking and listening? In particular, I am thinking of repetition effects present in the listening condition as well as prediction, which will be much more elevated for the listening condition than the speaking condition. The fact that it also concerns their own voice furthermore adds to the possible predictability confound (e.g., Heinks-Maldonado et al., 2005). In addition, listening to one's speech which just before has been articulated may, potentially strategically even, enhance inner speech and &quot;mouthing&quot; in the participants, hereby thus engaging the production mechanism. Similarly, during production, the participants already hear their own voice (which serves as input in the subsequent listening condition). Taken together, both similarities or differences between speaking and listening connectivity may have been due to or influenced by these order effects, and the fact that the different speech behaviors are to some extent present in both conditions.</p></disp-quote><p>This is a valid point raised by the reviewer. By listening to their own previously produced speech, our participants might have anticipated and predicted the sentences easier. However, during designing our experiment, we tried to lower the chance of this anticipation by several steps. First, participants were measured in separate sessions for speech production and perception tasks. There were always several days' intervals between performing these two conditions. Secondly, our questions were mainly about a common/general topic. Consequently, participants may not remember their answers completely.</p><p>Importantly, using the same stimulus material for speaking and listening guaranteed that there was no difference in the low-level features of the material for both conditions that could have affected the results of our statistical comparison.</p><p>Due to bone conduction, hearing one’s unaltered own speech from a recording may seem foreign and could lead to unwanted emotional reactions e.g. embarrassment, so participants were asked whether they heard their own voice in a recording already (e.g. from a self-recorded voice-message in WhatsApp) which most of them confirmed. Participants were also informed that they were going to hear themselves during the measurement to further reduce unwanted psychophysiological responses.</p><disp-quote content-type="editor-comment"><p>(5) The ability of the authors to analyze the spatiotemporal dynamics during continuous speech is a potentially important feat of this study, given that one of the reasons that speech production is much less investigated compared to perception concerns motor and movement artifacts due to articulation (e.g., Strijkers et al., 2010). Two questions did spring to mind when reading the authors' articulation artifact correction procedure: If I understood correctly, the approach comes from Abbasi et al. (2021) and is based on signal space projection (SSP) as used for eye movement corrections, which the authors successfully applied to speech production. However, in that study, it concerned the repeated production of three syllables, while here it concerns continuous speech of full words embedded in discourse. The articulation and muscular variance will be much higher in the current study compared to three syllables (or compared to eye movements which produce much more stable movement potentials compared to an entire discourse). Given this, I can imagine that corrections of the signal in the speaking condition were likely substantial and one may wonder (1) how much signal relevant to speech production behavior is lost?; (2) similar corrections are not necessary for perception, so how would this marked difference in signal processing affect the comparability between the modalities?</p></disp-quote><p>One of the results of our previous study (Abbasi et al., 2021) was that the artefact correction was not specific to individual syllables but generalised across syllables. Also, the repeated production of syllables was associated with substantial movements of the articulators mimicking those observed during naturalistic speaking. We therefore believe that the artefact rejection is effective during speaking. We also checked this by investigating speech related coherence in brain parcels in spatial proximity to the articulators.In our previous study we also show that the correction method retains neural activity to a very large degree. We are therefore confident that speaking and listening conditions can be compared and that the loss of true signals from correcting the speaking data will be minor.</p><disp-quote content-type="editor-comment"><p>References:</p><list list-type="bullet"><list-item><p>Abbasi, O., Steingräber, N., &amp; Gross, J. (2021). Correcting MEG artifacts caused by overt speech. Frontiers in Neuroscience, 15, 682419.</p></list-item></list><list list-type="bullet"><list-item><p>D'Mello, A. M., Gabrieli, J. D., &amp; Nee, D. E. (2020). Evidence for hierarchical cognitive control in the human cerebellum. Current Biology, 30(10), 1881-1892.</p></list-item></list><list list-type="bullet"><list-item><p>Fairs, A., Michelas, A., Dufour, S., &amp; Strijkers, K. (2021). The same ultra-rapid parallel brain dynamics underpin the production and perception of speech. Cerebral Cortex Communications, 2(3), tgab040.</p></list-item></list><list list-type="bullet"><list-item><p>Floegel, M., Fuchs, S., &amp; Kell, C. A. (2020). Differential contributions of the two cerebral hemispheres to temporal and spectral speech feedback control. Nature Communications, 11(1), 2839.</p></list-item></list><list list-type="bullet"><list-item><p>Giglio, L., Ostarek, M., Sharoh, D., &amp; Hagoort, P. (2024). Diverging neural dynamics for syntactic structure building in naturalistic speaking and listening. Proceedings of the National Academy of Sciences, 121(11), e2310766121.</p></list-item></list><list list-type="bullet"><list-item><p>Heinks‐Maldonado, T. H., Mathalon, D. H., Gray, M., &amp; Ford, J. M. (2005). Fine‐tuning of auditory cortex during speech production. Psychophysiology, 42(2), 180-190.</p></list-item></list><list list-type="bullet"><list-item><p>Price, C. J. (2010). The anatomy of language: a review of 100 fMRI studies published in 2009. Annals of the new York Academy of Sciences, 1191(1), 62-88.</p></list-item></list><list list-type="bullet"><list-item><p>Runnqvist, E., Chanoine, V., Strijkers, K., Pattamadilok, C., Bonnard, M., Nazarian, B., ... &amp; Alario, F. X. (2021). Cerebellar and cortical correlates of internal and external speech error monitoring. Cerebral Cortex Communications, 2(2), tgab038.</p></list-item></list><list list-type="bullet"><list-item><p>Runnqvist, E. (2023). Self-monitoring: The neurocognitive basis of error monitoring in language production. In Language production (pp. 168-190). Routledge.</p></list-item></list><list list-type="bullet"><list-item><p>Stockert, A., Schwartze, M., Poeppel, D., Anwander, A., &amp; Kotz, S. A. (2021). Temporo-cerebellar connectivity underlies timing constraints in audition. Elife, 10, e67303.</p></list-item></list><list list-type="bullet"><list-item><p>Strijkers, K., Costa, A., &amp; Thierry, G. (2010). Tracking lexical access in speech production: electrophysiological correlates of word frequency and cognate effects. Cerebral cortex, 20(4), 912-928.</p></list-item></list><list list-type="bullet"><list-item><p>Todorović, S., Anton, J. L., Sein, J., Nazarian, B., Chanoine, V., Rauchbauer, B., ... &amp; Runnqvist, E. (2023). Cortico-cerebellar monitoring of speech sequence production. Neurobiology of Language, 1-21.</p></list-item></list><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors re-analyse MEG data from a speech production and perception study and extend their previous Granger causality analysis to a larger number of cortical-cortical and in particular cortical-subcortical connections. Regions of interest were defined by means of a meta-analysis using Neurosynth.org and connectivity patterns were determined by calculating directed influence asymmetry indices from the Granger causality analysis results for each pair of brain regions. Abbasi et al. report feedforward signals communicated via fast rhythms and feedback signals via slow rhythms below 40 Hz, particularly during speaking. The authors highlight one of these connections between the right cerebellum lobule VI and auditory association area A5, where in addition the connection strength correlates negatively with the strength of speech tracking in the theta band during speaking (significant before multiple comparison correction). Results are interpreted within a framework of active inference by minimising prediction errors.</p><p>While I find investigating the role of cortical-subcortical connections in speech production and perception interesting and relevant to the field, I am not yet convinced that the methods employed are fully suitable to this endeavour or that the results provide sufficient evidence to make the strong claim of dissociation of bottom-up and top-down information flow during speaking in distinct frequency bands.</p><p>Strengths:</p><p>The investigation of electrophysiological cortical-subcortical connections in speech production and perception is interesting and relevant to the field. The authors analyse a valuable dataset, where they spent a considerable amount of effort to correct for speech production-related artefacts. Overall, the manuscript is well-written and clearly structured.</p><p>Weaknesses:</p><p>The description of the multivariate Granger causality analysis did not allow me to fully grasp how the analysis was performed and I hence struggled to evaluate its appropriateness.Knowing that (1) filtered Granger causality is prone to false positives and (2) recent work demonstrates that significant Granger causality can simply arise from frequency-specific activity being present in the source but not the target area without functional relevance for communication (Schneider et al. 2021) raises doubts about the validity of the results, in particular with respect to their frequency specificity. These doubts are reinforced by what I perceive as an overemphasis on results that support the assumption of specific frequencies for feedforward and top-down connections, while findings not aligning with this hypothesis appear to be underreported. Furthermore, the authors report some main findings that I found difficult to reconcile with the data presented in the figures. Overall, I feel the conclusions with respect to frequency-specific bottom-up and top-down information flow need to be moderated and that some of the reported findings need to be checked and if necessary corrected.</p><p>Major points</p><p>(1) I think more details on the multivariate GC approach are needed. I found the reference to Schaum et al., 2021 not sufficient to understand what has been done in this paper. Some questions that remained for me are:</p><p>(i) Does multivariate here refer to the use of the authors' three components per parcel or to the conditioning on the remaining twelve sources? I think the latter is implied when citing Schaum et al., but I'm not sure this is what was done here?</p><p>If it was not: how can we account for spurious results based on indirect effects?</p></disp-quote><p>Yes, multivariate refers to the three components.</p><disp-quote content-type="editor-comment"><p>(ii) Did the authors check whether the GC of the course-target pairs was reliably above the bias level (as Schaum et. al. did for each condition separately)? If not, can they argue why they think that their results would still be valid? Does it make sense to compute DAIs on connections that were below the bias level? Should the data be re-analysed to take this concern into account?</p></disp-quote><p>We performed statistics on DAI and believe that this is a valid approach. We argue that random GC effects would not survive our cluster-corrected statistics.</p><disp-quote content-type="editor-comment"><p>(iii) You may consider citing the paper that introduced the non-parametric GC analysis (which Schaum et al. then went on to apply): Dhamala M, Rangarajan G, Ding M. Analyzing Information Flow in Brain Networks with Nonparametric Granger Causality. Neuroimage. 2008; 41(2):354-362. https://doi.org/10.1016/j.neuroimage.2008.02. 020</p></disp-quote><p>Thanks, we will add this reference in the revised version.</p><disp-quote content-type="editor-comment"><p>(2) GC has been discouraged for filtered data as it gives rise to false positives due to phase distortions and the ineffectiveness of filtering in the information-theoretic setting as reducing the power of a signal does not reduce the information contained in it (Florin et al., 2010; Barnett and Seth, 2011; Weber et al. 2017; Pinzuti et al., 2020 - who also suggest an approach that would circumvent those filter-related issues). With this in mind, I am wondering whether the strong frequency-specific claims in this work still hold.</p></disp-quote><p>This must be a misunderstanding. We are aware of the problem with GC on filtered data. But GC was here computed on broadband data and not in individual frequency bands.</p><disp-quote content-type="editor-comment"><p>(3) I found it difficult to reconcile some statements in the manuscript with the data presented in the figures:</p><p>(i) Most notably, the considerable number of feedforward connections from A5 and STS that project to areas further up the hierarchy at slower rhythms (e.g. L-A5 to R-PEF, R-Crus2, L CB6 L-Tha, L-FOP and L-STS to R-PEF, L-FOP, L-TOPJ or R-A5 as well as R-STS both to R-Crus2, L-CB6, L-Th) contradict the authors' main message that 'feedback signals were communicated via slow rhythms below 40 Hz, whereas feedforward signals were communicated via faster rhythms'. I struggled to recognise a principled approach that determined which connections were highlighted and reported and which ones were not.</p><p>(ii) &quot;Our analysis also revealed robust connectivity between the right cerebellum and the left parietal cortex, evident in both speaking and listening conditions, with stronger connectivity observed during speaking. Notably, Figure 4 depicts a prominent frequency peak in the alpha band, illustrating the specific frequency range through which information flows from the cerebellum to the parietal areas.&quot; There are two peaks discernible in Figure 4, one notably lower than the alpha band (rather theta or even delta), the other at around 30 Hz. Nevertheless, the authors report and discuss a peak in the alpha band.</p><p>(iii) In the abstract: &quot;Notably, high-frequency connectivity was absent during the listening condition.&quot; and p.9 &quot;In contrast with what we reported for the speaking condition, during listening, there is only a significant connectivity in low frequency to the left temporal area but not a reverse connection in the high frequencies.&quot;</p><p>While Fig. 4 shows significant connectivity from R-CB6 to A5 in the gamma frequency range for the speaking, but not for the listening condition, interpreting comparisons between two effects without directly comparing them is a common statistical mistake (Makin and Orban de Xivry). The spectrally-resolved connectivity in the two conditions actually look remarkably similar and I would thus refrain from highlighting this statement and indicate clearly that there were no significant differences between the two conditions.</p><p>(iv) &quot;This result indicates that in low frequencies, the sensory-motor area and cerebellum predominantly transmit information, while in higher frequencies, they are more involved in receiving it.&quot;</p><p>I don't think that this statement holds in its generality: L-CB6 and R-3b both show strong output at high frequencies, particularly in the speaking condition. While they seem to transmit information mainly to areas outside A5 and STS these effects are strong and should be discussed.</p></disp-quote><p>We appreciate the reviewer's thoughtful comments. We acknowledge that not all connectivity patterns strictly adhere to the initial observation regarding feedback and feedforward communication. It's true that our primary focus was on interactions between brain regions known to be crucial for speech prediction, including auditory, somatosensory, and cerebellar areas. However, we also presented connectivity patterns across other regions to provide a more comprehensive picture of the speech network. We believe this broader perspective can be valuable for future research directions.</p><p>Regarding the reviewer's observation about the alpha band peak in Figure 4, we agree that a closer examination reveals the connectivity from right cerebellum to the left parietal is in a wider low frequency range. We will refrain from solely emphasizing the alpha band and acknowledge the potential contribution of lower frequencies to cerebellar-parietal communication.</p><p>We also appreciate the reviewer highlighting the need for a more nuanced interpretation of the listening condition connectivity compared to the speaking condition. The reviewer is correct in pointing out that while Figure 4 suggests a high-frequency connectivity from L-A5 to R-CB only in the speaking condition, a direct statistical comparison between conditions might not reveal a significant difference. We will revise the manuscript to clarify this point.</p><p>Finally, a closer examination of Figure 3 revealed that the light purple and dark green edges in the speaking condition for R-CB6 and L-3b suggest outgoing connections at low frequencies, while other colored edges indicate information reception at high frequencies. We acknowledge that exceptions to this directional pattern might exist and warrant further investigation in future studies.</p><disp-quote content-type="editor-comment"><p>(4) &quot;However, definitive conclusions should be drawn with caution given recent studies raising concerns about the notion that top-down and bottom-up signals can only be transmitted via separate frequency channels (Ferro et al., 2021; Schneider et al., 2021; Vinck et al., 2023).&quot;</p><p>I appreciate this note of caution and think it would be useful if it were spelled out to the reader why this is the case so that they would be better able to grasp the main concerns here. For example, Schneider et al. make a strong point that we expect to find Granger-causality with a peak in a specific frequency band for areas that are anatomically connected when the sending area shows stronger activity in that band than the receiving one, simply because of the coherence of a signal with its own linear projection onto the other area. The direction of a Granger causal connection would in that case only indicate that one area shows stronger activity than the other in the given frequency band. I am wondering to what degree the reported connectivity pattern can be traced back to regional differences in frequency-specific source strength or to differences in source strength across the two conditions.</p></disp-quote><p>This is indeed an important point. That is why we are discussing our results with great caution and specifically point the reader to the relevant literature. We are indeed thinking about a future study where we investigate this connectivity using other connectivity metrics and a detailed consideration of power.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>In the current paper, Abbasi et al. aimed to characterize and compare the patterns of functional connectivity across frequency bands (1 Hz - 90 Hz) between regions of a speech network derived from an online meta-analysis tool (Neurosynth.org) during speech production and perception. The authors present evidence for complex neural dynamics from which they highlight directional connectivity from the right cerebellum to left superior temporal areas in lower frequency bands (up to beta) and between the same regions in the opposite direction in the (lower) high gamma range (60-90 Hz). Abbasi et al. interpret their findings within the predictive coding framework, with the cerebellum and other &quot;higher-order&quot; (motor) regions transmitting top-down sensory predictions to &quot;lower-order&quot; (sensory) regions in the lower frequencies and prediction errors flowing in the opposite direction (i.e., bottom-up) from those sensory regions in the gamma band. They also report a negative correlation between the strength of this top-down functional connectivity and the alignment of superior temporal regions to the syllable rate of one's speech.</p><p>Strengths:</p><p>(1) The comprehensive characterization of functional connectivity during speaking and listening to speech may be valuable as a first step toward understanding the neural dynamics involved.</p><p>(2) The inclusion of subcortical regions and connectivity profiles up to 90Hz using MEG is interesting and relatively novel.</p><p>(3) The analysis pipeline is generally adequate for the exploratory nature of the work.</p><p>Weaknesses:</p><p>(1) The work is framed as a test of the predictive coding theory as it applies to speech production and perception, but the methodological approach is not suited to this endeavor.</p></disp-quote><p>We agree that we cannot provide definite evidence for predictive coding in speech production and perception and we believe that we do not make that claim in the manuscript. However, our results are largely consistent with what can be expected based on predictive coding theory.</p><disp-quote content-type="editor-comment"><p>(2) Because of their theoretical framework, the authors readily attribute roles or hierarchy to brain regions (e.g., higher- vs lower-order) and cognitive functions to observed connectivity patterns (e.g., feedforward vs feedback, predictions vs prediction errors) that cannot be determined from the data. Thus, many of the authors' claims are unsupported.</p></disp-quote><p>We will revise the manuscript to more clearly differentiate our results (e.g. directed Granger-Causality from A to B) from their interpretation (potentially indicating feedforward or feedback signals).</p><disp-quote content-type="editor-comment"><p>(3) The authors' theoretical stance seems to influence the presentation of the results, which may inadvertently misrepresent the (otherwise perfectly valid; cf. Abbasi et al., 2023) exploratory nature of the study. Thus, results about specific regions are often highlighted in figures (e.g., Figure 2 top row) and text without clear reasons.</p></disp-quote><p>Our connectograms reveal a multitude of results that we hope is interesting to the community. At the same time the wealth of findings poses a problem for describing them. We did not see a better way then to highlight specific connections of interest.</p><disp-quote content-type="editor-comment"><p>(4) Some of the key findings (e.g., connectivity in opposite directions in distinct frequency bands) feature in a previous publication and are, therefore, interesting but not novel.</p></disp-quote><p>We actually see this as a strength of the current manuscript. The computation of connectivity is here extended to a much larger sample of brain areas. It is reassuring to see that the previously reported results generalise to other brain areas.</p><disp-quote content-type="editor-comment"><p>(5) The quantitative comparison between speech production and perception is interesting but insufficiently motivated.</p></disp-quote><p>We thank the reviewer for this comment. We have addressed that in detail in response to the point (1&amp;4) of reviewer 1.</p><disp-quote content-type="editor-comment"><p>(6) Details about the Neurosynth meta-analysis and subsequent selection of brain regions for the functional connectivity analyses are incomplete. Moreover, the use of the term 'Speech' in Neurosynth seems inappropriate (i.e., includes irrelevant works, yielding questionable results). The approach of using separate meta-analyses for 'Speech production' and 'Speech perception' taken by Abbasi et al. (2023) seems more principled. This approach would result, for example, in the inclusion of brain areas such as M1 and the BG that are relevant for speech production.</p></disp-quote><p>We agree that there are inherent limitations in automated meta-analysis tools such as Neurosynth. Papers are used in the meta-analysis that might not be directly relevant. However, Neurosynth has proven its usefulness over many years and has been used in many studies. We also agree that our selection of brain areas is not complete. But Granger Causality analysis of every pair of ROIs leads to complex results and we had to limit our selection of areas.</p><disp-quote content-type="editor-comment"><p>(7) The results involving subcortical regions are central to the paper, but no steps are taken to address the challenges involved in the analysis of subcortical activity using MEG. Additional methodological detail and analyses would be required to make these results more compelling. For example, it would be important to know what the coverage of the MEG system is, what head model was used for the source localization of cerebellar activity, and if specific preprocessing or additional analyses were performed to ensure that the localized subcortical activity (in particular) is valid.</p></disp-quote><p>There is a large body of evidence demonstrating that MEG can record signals from deep brain areas such as thalamus and cerebellum including Attal &amp; Schwarz 2013, Andersen et al, Neuroimage 2020; Piastra et al., 2020; Schnitzler et al., 2009. These and other studies provide evidence that state-of-the-art recording (with multichannel SQUID systems) and analysis is sufficient to allow reconstruction of subcortical areas. However, spatial resolution is clearly reduced for these deep areas. We will add a statement in the revised manuscript to acknowledge this limitation.</p><disp-quote content-type="editor-comment"><p>(8) The results and methods are often detailed with important omissions (a speech-brain coupling analysis section is missing) and imprecisions (e.g., re: Figure 5; the Connectivity Analysis section is copy-pasted from their previous work), which makes it difficult to understand what is being examined and how. (It is also not good practice to refer the reader to previous publications for basic methodological details, for example, about the experimental paradigm and key analyses.) Conversely, some methodological details are given, e.g., the acquisition of EMG data, without further explanation of how those data were used in the current paper.</p></disp-quote><p>We will revise the relevant sections of the manuscript.</p><disp-quote content-type="editor-comment"><p>(9) The examination of gamma functional connectivity in the 60 - 90 Hz range could be better motivated. Although some citations involving short-range connectivity in these frequencies are given (e.g., within the visual system), a more compelling argument for looking at this frequency range for longer-range connectivity may be required.</p></disp-quote><p>Given previous evidence of connectivity in the gamma band we think that it would be a weakness to exclude this frequency band from analysis.</p><disp-quote content-type="editor-comment"><p>(10) The choice of source localization method (linearly constrained minimum variance) could be explained, particularly given that other methods (e.g. dynamic imaging of coherent sources) were specifically designed and might potentially be a better alternative for the types of analyses performed in the study.</p></disp-quote><p>Both LCMV and DICS are beamforming methods. We used LCMV because we wanted used Granger Causality which requires broadband signals. DICS would only provide frequency-specific band-limited signals.</p><disp-quote content-type="editor-comment"><p>(11) The mGC analysis needs to be more comprehensively detailed for the reader to be able to assess what is being reported and the strength of the evidence. Relatedly, first-level statistics (e.g., via estimation of the noise level) would make the mGC and DAI results more compelling.</p></disp-quote><p>We perform group-level cluster-based statistics on mGC while correcting for multiple comparisons across frequency bands and brain parcels and report only significant results. This is an established approach that is routinely used in this type of studies.</p><disp-quote content-type="editor-comment"><p>(12) Considering the exploratory nature of the study, it is essential for other researchers to continue investigating and validating the results presented in the current manuscript. Thus, it is concerning that data and scripts are not fully and openly available. Data need not be in its raw state to be shared and useful, which circumvents the stated data privacy concerns.</p></disp-quote><p>We acknowledge the reviewer's concern regarding the full availability of the dataset. Due to privacy limitations on the collected data, we are unable to share it publicly at this time. However, to promote transparency and enable further exploration, we have provided the script used for data analysis and an example dataset. This example dataset should provide a clear understanding of the data structure and variables used in the analysis. Additionally, we are happy to share the complete dataset upon request from research teams interested in performing in-depth secondary analyses.</p></body></sub-article></article>