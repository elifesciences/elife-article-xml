<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">76803</article-id><article-id pub-id-type="doi">10.7554/eLife.76803</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Brain representations of motion and position in the double-drift illusion</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-267945"><name><surname>Steinberg</surname><given-names>Noah J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1337-1479</contrib-id><email>noah.steinberg@nih.gov</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-110093"><name><surname>Roth</surname><given-names>Zvi N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2173-1625</contrib-id><email>zvi.roth@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-37017"><name><surname>Movshon</surname><given-names>J Anthony</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0274-9213</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-99508"><name><surname>Merriam</surname><given-names>Elisha</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2787-566X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>Laboratory of Brain and Cognition, National Institute of Mental Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04mhzgx49</institution-id><institution>School of Psychological Sciences, Faculty of Social Sciences, Tel Aviv University</institution></institution-wrap><addr-line><named-content content-type="city">Tel Aviv</named-content></addr-line><country>Israel</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute, Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>29</day><month>05</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e76803</elocation-id><history><date date-type="received" iso-8601-date="2022-01-05"><day>05</day><month>01</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-03-28"><day>28</day><month>03</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-01-27"><day>27</day><month>01</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.01.25.477714"/></event></pub-history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-76803-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-76803-figures-v1.pdf"/><abstract><p>In the ‘double-drift’ illusion, local motion within a window moving in the periphery of the visual field alters the window’s perceived path. The illusion is strong even when the eyes track a target whose motion matches the window so that the stimulus remains stable on the retina. This implies that the illusion involves the integration of retinal signals with non-retinal eye-movement signals. To identify where in the brain this integration occurs, we measured BOLD fMRI responses in visual cortex while subjects experienced the double-drift illusion. We then used a combination of univariate and multivariate decoding analyses to identify (1) which brain areas were sensitive to the illusion and (2) whether these brain areas contained information about the illusory stimulus trajectory. We identified a number of cortical areas that responded more strongly during the illusion than a control condition that was matched for low-level stimulus properties. Only in area hMT+ was it possible to decode the illusory trajectory. We additionally performed a number of important controls that rule out possible low-level confounds. Concurrent eye tracking confirmed that subjects accurately tracked the moving target; we were unable to decode the illusion trajectory using eye position measurements recorded during fMRI scanning, ruling out explanations based on differences in oculomotor behavior. Our results provide evidence for a perceptual representation in human visual cortex that incorporates extraretinal information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fmri</kwd><kwd>visual cortex</kwd><kwd>illusion</kwd><kwd>double drift</kwd><kwd>smooth pursuit</kwd><kwd>motion perception</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>ZIAMH002966</award-id><principal-award-recipient><name><surname>Merriam</surname><given-names>Elisha</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The 'double-drift' illusion involves integration of retinal and non-retinal signals in the human visual cortex, providing evidence for a perceptual representation that incorporates extraretinal information.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neurons throughout visual cortex encode the location of visual stimuli on the retina, suggesting that the visual system uses a primarily retina-centered reference frame. Yet visual perception is stable across frequent eye movements that displace the retinal image. This observation has led to the idea that the brain maintains a world-centered or ’spatiotopic' representation that is invariant to changes in eye position. This idea has perhaps received its strongest support from monkey single-unit recording studies showing that neurons in the ventral intraparietal area (VIP) in macaque monkeys exhibit receptive fields that do not change position when the eyes move (<xref ref-type="bibr" rid="bib8">Duhamel et al., 1997</xref>). Observations of spatiotopic representation have also been reported in a number of human brain imaging studies. For example, visually evoked responses in both human MT/MST (hMT+) and the lateral occipital complex (LOC) have been reported to be invariant to changes in eye position (<xref ref-type="bibr" rid="bib7">d’Avossa et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">McKyton and Zohary, 2007</xref>). These brain imaging studies suggest a broad agreement in the brain’s representation of space in monkey and human visual cortex.</p><p>Spatiotopic encoding has not been observed in all fMRI studies, however. For example, a number of studies have measured spatial receptive fields for a range of eye positions and found that receptive fields change position when the eyes move, suggesting that the brain uses a retinotopic reference frame (<xref ref-type="bibr" rid="bib9">Gardner et al., 2008</xref>; <xref ref-type="bibr" rid="bib11">Golomb and Kanwisher, 2012</xref>; <xref ref-type="bibr" rid="bib20">Merriam et al., 2013</xref>). The discrepancy between these studies and reports of spatiotopic representations have not been fully resolved. One suggestion is that the reference frame for stimulus encoding depends on cognitive or task demands. For example, <xref ref-type="bibr" rid="bib5">Crespi et al., 2011</xref> reported that the reference frame of visual responses can shift from retinotopic to spatiotopic depending on the attentional state of the observer. Behavioral studies have reported that spatiotopic representations become more prominent in tasks requiring sequences of eye movements, suggesting that spatiotopic coordinates are built-up over time (<xref ref-type="bibr" rid="bib27">Poletti et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Sun and Goldberg, 2016</xref>). Together, these observations suggest that reference frames can be dynamic and depend on a variety of factors, such as visual context or the specific task (<xref ref-type="bibr" rid="bib33">Steinberg et al., 2022</xref>).</p><p>In the current study, we used a version of the double-drift illusion to investigate a fundamental paradox of spatiotopic visual processing. The double-drift illusion occurs when a combination of local motion and an orthogonal global motion trajectory causes a strong perception of illusory drift away from the veridical trajectory. The illusion can be strikingly large so that the stimulus appears to deviate by as much as 45° away from the veridical motion path (<xref ref-type="bibr" rid="bib35">Tse and Hsieh, 2006</xref>; <xref ref-type="bibr" rid="bib30">Shapiro et al., 2010</xref>; <xref ref-type="bibr" rid="bib16">Lisi and Cavanagh, 2015</xref>). A recent study revealed that the illusion persists even during smooth pursuit when the stimulus is stabilized on the retina (<xref ref-type="bibr" rid="bib4">Cavanagh and Tse, 2019</xref>). This pursuit version of the double-drift illusion highlights the paradox of spatiotopic processing: even though the stimulus is at a constant position on the retina, it is perceived to change position in world-centered coordinates. Here, we asked if this illusion could provide insight into spatiotopic encoding in the brain. We hypothesized that several regions in occipital and parietal cortex are involved in computing the illusory percept. A number of brain areas encode stable stimulus position during pursuit eye movements (i.e., ‘real position’ cells) (<xref ref-type="bibr" rid="bib22">Nau et al., 2018</xref>). Moreover, several of these areas have been implicated in spatiotopic processing (<xref ref-type="bibr" rid="bib7">d’Avossa et al., 2007</xref>). If this hypothesis is correct, we predict that activity in extrastriate cortex will reflect the illusory motion path instead of the veridical stimulus path.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We tested whether fMRI BOLD activity in human visual cortex reflects the perceived spatial position of a visual stimulus that remained at a constant retinal location. We measured BOLD activity during a version of the double-drift illusion in which the perceived location of the stimulus differed from its actual location by several degrees (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Double-drift illusion during smooth pursuit.</title><p>(<bold>A</bold>) Leftward drift illusion. Participants made smooth pursuit eye movements, tracking the target as it moved vertically in tandem with a Gabor stimulus. Both the gabor and target moved for 12 seconds. Conjunction of local motion (grating phase drift) and global motion (displacement of the Gaussian envelope) produces an illusion in which the Gabor appears to drift several degrees to the left of its actual trajectory, even when smooth pursuit eye movements stabilize the Gabor on the retina. (<bold>B</bold>) Rightward drift illusion. Conjunction of local and global motion produces illusion of a rightward Gabor trajectory. (<bold>C</bold>) No-illusion control condition. Randomly updated grating phase does not produce illusory stimulus trajectory. All three stimulus conditions contain the same net motion energy and involved the same pursuit eye movements, yet are associated with strongly different percepts.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76803-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Mean number of voxels in each region of interest, for each experiment.</title><p>For experiments 1, 2, and 4, colored portions of bars represent mean number of voxels identified in Stim (blue) and eye (yellow) localizers, their overlap (orange), and the remaining voxels (purple). For experiment 3, colored portions of bars represent mean number of voxels used for decoding (blue, top 50% voxels ranked by R<sup>2</sup>) and those that were not used for decoding (orange).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76803-fig1-figsupp1-v1.tif"/></fig></fig-group><p>To determine whether BOLD activity contained information about the visual illusion, we trained a classifier to decode blocks of illusory trials from blocks of trials in which no illusion was perceived. Using leave-one-run-out cross-validation, we found that responses in multiple cortical areas were sensitive to the double-drift illusion (<xref ref-type="fig" rid="fig2">Figure 2</xref>); the classifier could accurately decode illusory drift in all four visual area regions of interest (ROIs) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, left).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Modulation of fMRI response amplitude during double-drift illusion.</title><p>(<bold>A</bold>) Stimulus localizer-evoked activity in cortical regions representing stimulus location (center), eye movement localizer-evoked diffuse activity in visual cortex, extending well beyond stimulus representation. Data from a single participant in the stimulus localizer shown on an inflated cortical surface (left) and a flattened patch of the occipital lobe (center). Data from the same subject in the eye-movement localizer shown on the right. Boundaries of retinotopic visual areas identified according to an anatomical template. Color indicates the phase of the response. Yellow hues indicate a response in phase with the onset of the stimulus (center) or onset of smooth pursuit (right). (<bold>B</bold>) Time course of fMRI response from voxels identified in the stimulus localizer, from three cortical areas exhibiting a larger response for the double-drift illusion than during a no-illusion control condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76803-fig2-v1.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Stimulus location information encoding during double-drift illusion.</title><p>(<bold>A</bold>) Accuracy of discriminating the double-drift illusion from a control condition that was matched for net motion energy. Participants either attended the peripheral stimulus and reported the presence of the illusion (Expt 1, left), or attended the fovea and reported a luminance decrement at fixation (Expt 2, right). (<bold>B</bold>) Accuracy of discriminating rightward vs. leftward drift illusion paths in Expt 2 (attend fixation) based on fMRI responses in voxels selected to match the retinotopic location of the stimulus (left) and voxels selected based on responses to pursuit eye movements (right). (<bold>C</bold>) Decoding accuracy for independent replication and control experiments (Expt 3). Left, decoding illusory drift paths, replicating results of Expt 2. Right, decoding local-motion only control conditions, which did not produce a drift illusion. Vertical lines extend from minimum to maximum bootstrap decoding accuracy. Horizontal lines denote median bootstrap decoding accuracy. Maroon dot, p&lt;0.01; orange dot, p&lt;0.05; gray dot, nonsignificant (p&gt;0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76803-fig3-v1.tif"/></fig><p>A number of different factors could lead to accurate decoding of the double-drift illusion. One possibility is that the decoder was sensitive to neural activity related to computing the location of the stimulus. Alternatively, it is possible that the perception of the illusion attracted spatial attention, and the classifier was picking up on attentional differences between illusory and non-illusory conditions. To control for this second possibility, we repeated the experiment, but had participants perform a demanding task at fixation that required sustained attention (<xref ref-type="bibr" rid="bib12">Haladjian et al., 2018</xref>). The fixation task minimized differences in spatial attention to the stimulus across conditions. We again tested whether the classifier could discriminate the double-drift illusion from the control condition. While overall decoding accuracy was slightly reduced in this experiment, we found that decoding accuracy remained robust and significant in LO, hMT+, and V3A/B, but not in early visual cortex (EVC) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right), consistent with other recent observations (<xref ref-type="bibr" rid="bib17">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Ho and Schwarzkopf, 2021</xref>). Participants were not attending the stimulus; therefore, these results cannot be attributed to differences in spatial attention. Instead, we conclude that the classifier was sensitive to information related to encoding the perceived position of the stimulus during the illusion.</p><p>The critical test in this study is whether BOLD fMRI activity in visual cortex can discriminate between different illusory paths. We tested whether a classifier could decode the drift path of the illusion. Of all the visual areas tested, only area hMT+ could discriminate leftward from rightward illusory paths (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, left). Because the stimulus remained at a constant retinal location, the ability to discriminate the illusory motion path suggests a non-retinotopic representation of stimulus position.</p><p>We next tested alternative explanations for the ability to discriminate motion trajectory in MT+. It is conceivable that decoding of the drift path was due to subtle differences in smooth pursuit eye movements, rather than encoding of the stimulus position. Specifically, we wondered if perceiving the illusion caused a change in oculomotor behavior, which could in turn result in decodable differences in fMRI activity. Under this alternative explanation, the ability to decode the trajectory of the illusion would be a secondary consequence of any difference in oculomotor behavior between illusory conditions. We conducted the following analyses to rule out this explanation. First, we repeated the classification analysis, this time using only voxels that were selective for smooth pursuit eye movements, as identified in a separate pursuit control experiment. In this analysis, we specifically excluded voxels that responded in the stimulus localizer (see ‘Smooth pursuit control experiment’). We reasoned that voxels that responded in the pursuit localizer should be most sensitive to any differences in pursuit eye movements in the main experiment. Note that this logic should apply, regardless of whether these voxels are selective for pursuit eye movements, or to the visual consequences of retinal slip during pursuit (i.e., during catch-up saccades). We found that responses in these voxels do not carry information that distinguishes the drift paths, in any of the ROIs (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, right). Results from this control analysis suggest that the information being utilized by the classifier is not due to differences in pursuit eye movements. Second, in a subset of subjects, we repeated the fMRI experiment but with concurrent eye tracking (see ‘Eye tracking data with concurrent fMRI’). In this subset of subjects, we replicated our main fMRI results (decoding the illusion trajectory from hMT+ responses), but we were unable to decode the illusory trajectory from the eye position measurements alone, indicating that there was no information contained in the oculomotor behavior related to perceiving the illusion (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Moreover, we quantified microsaccade characteristics (amplitude and direction) and found no reliable differences between illusory conditions (illusion vs. no-illusion), or between the direction of the illusion (left vs. right). We conclude that oculomotor behavior was unlikely an underlying cause of the fMRI findings reported here.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Eye position measurements did not reflect the trajectory of the illusion.</title><p>(<bold>A</bold>) Eye position measured during blocks of double-drift illusion; one representative subject averaged over all blocks in a scan session. Traces show stable fixation during the first 12 s followed by 12 s of vertical smooth pursuit. Eye position did not differ between leftward and rightward illusion. (<bold>B</bold>) Eye position during blocks of local motion only trials. (<bold>C</bold>) Polar histogram of microsaccades direction during leftward and rightward double-drift illusion and the two motion conditions. (<bold>D</bold>) Histogram of saccade amplitude during the two illusion conditions and the two motion conditions. (<bold>E</bold>) Decoding accuracies, using the horizontal and vertical eye position measurements to train and test a linear classifier to discriminate the direction of local motion. The bar labeled 'illusion' indicates accuracy for decoding trajectory during the illusion; the bar labeled 'fixation' indicates decoding during fixation (when no illusion was perceived); bar labeled 'local motion' indicates decoding during the local-motion only condition (during fixation). Horizontal gray dashed line denotes chance decoding for binary decision (50%). Horizontal black dashed line denotes 95% confidence interval of null distribution estimated using a permutation test.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-76803-fig4-v1.tif"/></fig><p>While the two illusory drift paths in our experiment were carefully balanced for net motion energy (i.e., a combination of a vertical global trajectory and horizontal local motion), we wondered if the ability of the classifier to discriminate leftward and rightward illusion drift paths could be due to the difference in temporal sequence of events within the trial (e.g., leftward followed by rightward motion, and vice versa). To test this possibility, we scanned another group of participants in an experiment (Expt 3) in which we included both illusion conditions from Expt 2, and two control conditions that contained the same local motion, but no global trajectory (and no smooth pursuit eye movements). For the illusory double-drift conditions, we again found that drift path was decodable in hMT+, replicating the results from Exp 2 in an independent group of participants (<xref ref-type="fig" rid="fig3">Figure 3C</xref>, left). However, the classifier was unable to decode the conditions containing local motion alone (i.e., discriminating left-followed-by-right from right-followed-by-leftward; <xref ref-type="fig" rid="fig3">Figure 3C</xref>, right). This result demonstrates that information about illusory motion paths in hMT+ is not due to local motion of the stimulus alone.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We found that fMRI BOLD responses in several visual cortical areas could reliably discriminate the double-drift illusion from a control condition that was matched for motion energy. In EVC, this result could be explained by attentional effects associated with perceiving the illusion, since when attention was directed away from the illusion, decoding in EVC dropped to chance. Beyond early visual cortex, several areas (hMT+, LO, and V3A/B) exhibited significant decoding of the illusion itself, even when controlling for spatial attention. Moreover, responses in hMT+ could also discriminate the illusory drift path, suggesting that retinal and extraretinal information are integrated in hMT+ and used to construct the spatiotopic perception experienced during the illusion. A number of control experiments indicate that these findings cannot be attributed to low-level stimulus or oculomotor factors. Our results may indicate non-retinal stimulus position encoding occurs in human extrastriate visual cortex.</p><sec id="s3-1"><title>Source of illusory drift path information</title><p>What is the source of decodable drift path information? One possibility is related to a coarse-scale map for direction of motion, which has been observed throughout visual cortex, including all of the areas included in our study (<xref ref-type="bibr" rid="bib37">Wang et al., 2014</xref>). The coarse-scale map for direction of motion in early visual areas (V1/V2/V3) is thought to result from an aperture-inward bias: larger responses were observed for motion away from the aperture edge (<xref ref-type="bibr" rid="bib37">Wang et al., 2014</xref>). In contrast, the coarse-scale map observed in hMT+ did not depend on the aperture boundary, but instead consisted of a bias for motion toward the fovea (<xref ref-type="bibr" rid="bib37">Wang et al., 2014</xref>). Could this fovea-centered bias explain the ability to decode the path of the double-drift illusion? In the current study, a fovea-centered bias would predict a leftward preference across voxels within hMT+ since the stimulus was always in the right visual field and leftward motion would be toward the fovea. However, the two illusory conditions (<xref ref-type="fig" rid="fig1">Figure 1A and B</xref>) had identical net amounts of leftward and rightward local motion, and identical proportions of time of leftward and rightward illusory drift paths. We think it is hence unlikely that a net motion bias toward the fovea in hMT + accounts for the observed results.</p><p>An alternative account is that differences in BOLD activity to the two illusory drift paths arise because of the topographic organization within hMT+ (<xref ref-type="bibr" rid="bib14">Huk et al., 2002</xref>; <xref ref-type="bibr" rid="bib1">Amano et al., 2009</xref>). The rightward drift path begins with illusory drift up-and-to-the-right, which increases the perceived eccentricity of the Gabor (<xref ref-type="bibr" rid="bib16">Lisi and Cavanagh, 2015</xref>). The path continues with drift down-and-to-the-left, which brings the perceived position back to the original position. This cycle repeats throughout the block of trials. In contrast, the leftward drift path begins with illusory drift up-and-to-the-left, which decreases the perceived eccentricity of the Gabor, and continues with drift down-and-to-the-right, bringing the perceived position back to the original position. Thus, the average eccentricity of the perceived drift path is higher during rightward drift and lower during leftward drift. This shift in the perceived eccentricity of the stimulus could result in slightly different patterns of activity in hMT+, and this difference could underlie the ability to decode the illusion drift path.</p><p>This second account depends on there being an explicit representation of the perceived position of a stimulus in hMT+, while position encoding in EVC is entirely veridical. Since veridical position did not differ for rightward and leftward drift paths; the classifier was unable to decode the drift path from activity in EVC. Consistent with this account, one fMRI study (<xref ref-type="bibr" rid="bib18">Maus et al., 2013</xref>) has reported that BOLD activity in hMT+ reflects the illusory position during motion-induced position shift. Activity throughout visual cortex is known to encode stimulus position in retinal, not spatiotopic, coordinates (<xref ref-type="bibr" rid="bib9">Gardner et al., 2008</xref>). However, it remains unknown whether retinotopic coding is also universal in visual cortex for motion illusions. If the second account is accurate, our data may imply a difference between EVC and downstream areas in the spatial encoding of illusory motion.</p></sec><sec id="s3-2"><title>Spatiotopic coordinates in visual cortex</title><p>The double-drift illusion results from combining local motion of the Gabor with a global trajectory of the envelope. In the smooth-pursuit variant of the illusion, the envelope only has a trajectory when defined in spatiotopic coordinates, since the stimulus remains at a constant retinal location. With a stable position of the stimulus on the retina, the presence of the illusion suggests some degree of spatiotopic processing in the brain (<xref ref-type="bibr" rid="bib36">Turi and Burr, 2012</xref>). This could be accomplished by the formation of an explicit spatiotopic reference frame (<xref ref-type="bibr" rid="bib8">Duhamel et al., 1997</xref>; d'<xref ref-type="bibr" rid="bib7">d’Avossa et al., 2007</xref>; <xref ref-type="bibr" rid="bib5">Crespi et al., 2011</xref>). Alternatively this could be accomplished through a computation by which a retinotopic input is combined with an eye position gain field (<xref ref-type="bibr" rid="bib20">Merriam et al., 2013</xref>). Our data do not speak to which of these two possibilities is more likely.</p></sec><sec id="s3-3"><title>Decoding the drift illusion beyond hMT+</title><p>In addition to decoding the illusory drift path, patterns of activity in multiple visual areas enabled classification of the perception of the illusion. When subjects were attending the stimulus, illusion decoding was possible in all the visual areas that we studied, raising the possibility that the illusion attracted attention, resulting in a higher BOLD response during illusory blocks (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). When subjects were attending to a task at fixation (Exp 2), the illusion could still be decoded from activity in LO, V3A/B, and hMT+, but not from EVC. Previous fMRI studies have claimed that the locus of attention can affect the apparent reference frame in which a stimulus is encoded (<xref ref-type="bibr" rid="bib5">Crespi et al., 2011</xref>). It is unclear, however, whether attention and task indeed change the spatial reference frame, or instead affect global response amplitudes (<xref ref-type="bibr" rid="bib29">Roth et al., 2020</xref>), which may constitute an additive signal obfuscating measurement of the underlying reference frame. In Exp 1, subjects performed a task on the stimulus, and so attention was likely directed toward the stimulus. In an earlier fMRI study on the double-drift illusion (<xref ref-type="bibr" rid="bib17">Liu et al., 2019</xref>), subjects also performed a task on the stimulus, and while the tasks were different in the two experiments, in both cases attention was focused on the stimulus. Our results in Exp 2, in which subjects attended fixation, and not the stimulus, demonstrate that attention and task do have an impact on the spatial encoding of the double-drift illusion, and highlights the importance of controlling the attentional state of the observer when studying visual reference frames (<xref ref-type="bibr" rid="bib5">Crespi et al., 2011</xref>).</p></sec><sec id="s3-4"><title>Disentangling spatiotopic representations and remapping</title><p>Two potential mechanisms have been suggested for the visual system’s ability to preserve a stable percept across saccades. The first is a spatiotopic representation, relying on afferent signals that update across saccades. This can be thought of as a combination of two representations: a retinotopic representation of the visual world, and a representation of gaze direction in the world. The two are integrated to form our perception of the outside world, independent of changes in direction of gaze. The second mechanism is remapping. During (or a brief moment prior to) a saccade, receptive fields shift to where they will naturally be positioned after the saccade. This shift, or remapping, ensures that neurons activity before and after the saccade will reflect the same region in the visual field. After the saccade, the receptive field returns to its natural retinotopic position. Behavioral investigations into mechanisms for visual stability across eye movements have found evidence that both spatiotopic representations and receptive field remapping underly visual stability (<xref ref-type="bibr" rid="bib27">Poletti et al., 2013</xref>), with the relative contribution of each mechanism depending on the number of intervening saccades. After a single saccade, receptive field remapping is the primary mechanism underlying visual stability, whereas spatiotopic representations become prominent after multiple saccades (<xref ref-type="bibr" rid="bib34">Sun and Goldberg, 2016</xref>). It is therefore possible that fMRI studies exploring spatiotopic representations could in fact probe retinotopic coding that is updated by remapping across saccades.</p><p>The version of the double-drift illusion employed in the current study did not require saccadic eye movements, making it unlikely that perisaccadic remapping contributed to our results. Remapping can take place during saccades since saccades are discrete events separated in time, leaving time for both the shift and the return. However, shifting the receptive field cannot be used for continuous gradual changes such as smooth pursuit. A receptive field shift in the direction of the planned motion before the beginning of the pursuit would not correct for the rest of the pursuit, and furthermore, there would be no opportunity to shift back. We find it plausible to assume, therefore, that the remapping mechanism is relevant only for saccades. Note that subjects may perform saccades during the pursuit (e.g., catch-up saccades) that could be corrected by remapping, but the pursuit itself cannot be corrected by remapping. Therefore, a spatiotopic signal robust to smooth pursuit provides evidence for a different correction mechanism, namely a spatiotopic representation. From this, our results suggest that stimulus position was encoded in a spatiotopic representation.</p></sec><sec id="s3-5"><title>Relationship to a previous study of the double-drift illusion</title><p>A recent study <xref ref-type="bibr" rid="bib17">Liu et al., 2019</xref> used a decoding approach to identify brain activity reflecting the percept during a version of the double-drift illusion that did not include smooth pursuit. A classifier was used to decode both the veridical direction of a diagonally moving Gabor patch, and the illusion direction during the double-drift illusion. Briefly, they found that both veridical motion and illusory motion direction could be decoded from visual cortex, but a classifier trained on veridical motion could not decode illusory motion and vice versa, suggesting differences between the patterns of activity in the two conditions. Instead, cross-decoding was possible primarily in prefrontal cortex, suggesting that activity in PFC reflects the perceived motion direction.</p><p>The results reported by <xref ref-type="bibr" rid="bib17">Liu et al., 2019</xref> are surprising on several accounts. First, decoding accuracy in all significant cortical regions were more or less uniform in accuracy (&gt;70%) (see their Figure 4B and their Figure 6B), something rarely observed in full-brain decoding analyses. Second, when the double-drift experiment was repeated with the exact same stimuli, the brain regions that showed significant decoding changed while the decoding accuracy remained the same (compare their Figure 4A with their Figure 6A). Finally, when the veridical motion stimuli were changed slightly, the pattern of regions supporting decoding changed substantially (compare their Figure 4B with their Figure 6B), as did the regions supporting cross-decoding between veridical motion and illusory motion (compare their Figure 4C with their Figure 6C). These findings raise important questions regarding the results reported in Liu et al. and suggest that multiple factors may influence the ability to decode an illusory motion path, such as spatial attention, as we have demonstrated in our study.</p><p>Regardless, the results of Liu et al. do not have direct bearing on which reference frame was used to encode the stimulus location, which is the topic of the current study. Because in Liu et al. subjects were fixating on the Gabor, the encoding of the illusion could have been in either retinal or spatiotopic coordinates. In contrast, in our study, the stimulus must have been encoded in spatiotopic coordinates. However, one potentially interesting extension of the cross-decoding approach would be to train the decoder on a version of the illusion involving fixation (as in Liu et al.), but then test the decoder on the illusion during pursuit (as in the current study). If perceived motion direction is represented in spatiotopic coordinates in both cases, one would expect the classifier to succeed in cross-decoding. However, if spatiotopic coding is used during pursuit (as we have shown here) but not during fixation, this cross-decoding should fail.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Data were acquired from 19 healthy participants (11 females, age range 23–34 y, mean 25.8 y) with normal or corrected-to-normal vision. Experiments were conducted with the written consent of each observer. The consent and experimental protocol were in compliance with the safety guidelines for MRI research and were approved by the Institutional Review Board of the National Institutes of Health. Of the 19 participants, 12 were scanned in multiple sessions and in multiple experimental conditions. 9, 12, and 5 participants participated in Exp 1, 2, and 3, respectively.</p></sec><sec id="s4-2"><title>Stimuli</title><p>A Gabor pattern, consisting of a vertically oriented sinusoidal grating (spatial frequency of 1 cycle/°) within a Gaussian envelope (standard deviation of 1 dva), moved back and forth along a linear, vertical trajectory with a length of 8°. The trajectory length was varied slightly (±1°) across subjects to accommodate the restricted field of view within the scanner. The Gabor moved according to a linear velocity profile (10°/s) that was smoothed slightly at the top and bottom of the Gabor’s path where it changed direction to facilitate accurate pursuit eye movements. The Gabor’s internal grating moved orthogonally relative to its trajectory with a speed of 6.66 Hz, reversing its direction at the two endpoints of the trajectory. Participants fixated a target (0.2 dva, white dot with a black outer rim) that was positioned 9–12 dva to the left of the Gabor envelope and moved smoothly alongside it. Participants were instructed to pursue the target. Pursuit accuracy was confirmed during the behavioral experiment for each subject prior to the fMRI experiment.</p></sec><sec id="s4-3"><title>Pre-scan behavioral experiment</title><p>Prior to the first scanning session, participants viewed the double drift stimulus in a behavioral experiment and were asked to judge the angle of the illusion. Participants viewed the illusion in 12 s blocks while pursuing a target that moved smoothly alongside the stimulus. The Gabor completed eight traversals per block. At the end of each block, the Gabor was replaced with a vertical line, aligned to the physical (vertical) path of the Gabor’s trajectory. Participants manipulated the angle of the line with the keyboard in order to match the perceived path of the Gabor.</p></sec><sec id="s4-4"><title>Eye tracking data with concurrent fMRI (Expt 4)</title><p>Eye tracking concurrent with 7T fMRI scanning was performed in a replication of Expt 3. We reran the experimental same experimental protocol described for (Expt 3, see below) with simultaneous eye tracking (Eyelink, 1000 Hz) on five new participants. The raw eye traces were corrected for missing data during blinks and mean centered, but not further preprocessing was performed (i.e., the eye traces were not temporally smoothed). We then used a support vector machine (SVM) classifier to the azimuth and elevation data (concatenated together to produce two vectors) together using the cosmoMVPA toolbox (<xref ref-type="bibr" rid="bib25">Oosterhof et al., 2016</xref>). Decoding was attempted over trajectories/traversals of the stimulus (6 traversals × 8 runs = 48 exemplars). Three separate classifiers were designed: first, we trained and tested a classifier on eye data during the fixation portion of the trials with the visual illusion where we did not expect the traces to discriminate between conditions. This analysis served as a control. Second, we trained and tested a classifier using eye data from the illusion trials, just as we did using the fMRI data. We reasoned that any differences in the horizontal or vertical displacement of eye position during the illusion could be used to support accurate decoding. Therefore, to isolate the effect of local motion alone, we built a third classifier using eye position during the trials with only local motion. For all tests, a permutation test (1000 instances) was performed to determine the 95th percentile. Any differences in the horizontal or vertical displacement of eye position during the illusion could be used to support accurate decoding. Therefore, to isolate the effect of local motion alone, we built a classifier using eye position during the trials with only local motion. For all tests, a permutation test (1000 instances) was performed to determine the 95th percentile.</p><p>All subjects invariably make small saccades during smooth pursuit eye movements. These are typically catch-up saccades that correct for small inaccuracies in pursuit gain. It is possible that the illusion changed saccadic characteristics, such as the number, size, or direction of saccades. We therefore performed an analysis on the magnitude and direction of saccades. Saccades were typically small and did not differ in any between any of the four experimental conditions, two illusory conditions (left and right) and the two local motion-only conditions. A two-way ANOVA (illusion-vs-no-illusion × direction) was performed across all subjects and did not reveal any significant differences or interactions.</p></sec><sec id="s4-5"><title>Experimental conditions and fMRI design</title><sec id="s4-5-1"><title>Expt 1: Attend to drift illusion path</title><p>The main fMRI experiment consisted of a randomized blocked design with three stimulus conditions, all of which involved the same vertical Gabor envelope trajectory: (1) perceived leftward drift path (internal local motion leftward during upward trajectory; rightward during downward trajectory); (2) perceived rightward drift path (internal local motion rightward during upward trajectory; leftward during downward trajectory); and (3) no-illusion control condition (randomized internal local motion, updated at 60 Hz). In all three conditions, participants pursued a fixation dot that moved smoothly and predictably alongside the Gabor, so that the Gabor remained at a constant retinal location throughout the experiment. Under conditions 1 and 2, the Gabor’s perceived drift path differed from its actual trajectory by several degrees. In condition 3, the Gabor’s perceived path matched its actual trajectory (i.e., there was no illusion). Each of the three conditions contained the same global and local motion energy and required the same smooth pursuit eye movements.</p><p>Each Gabor traversal lasted for 1.5 s (750 ms up, 750 ms down). The Gabor completed eight traversals in a 12 s block. The three conditions were randomly interleaved, and experimental blocks alternated with 12 s blocks of fixation in which both the pursuit target and the Gabor remained stationary, with no internal motion. Participants were instructed to press one of three buttons at the end of each experimental block indicating the direction of the illusion (‘1’ for leftward drift illusion, ‘2’ for no illusion, and ‘3’ for rightward drift). The double-drift illusion is typically strong and unambiguous, even during smooth pursuit eye movements. Accordingly, participants performed the attend-to-stimulus task with nearly 100% accuracy. Each fMRI run lasted for 288 s and included four blocks of each of the three conditions in a randomized order.</p></sec><sec id="s4-5-2"><title>Expt 2: Attend to fixation target</title><p>Experimental design and stimuli were identical to Expt 1, except for additional luminance decrements of the fixation target. Participants’ task was to press a button when they detected the brief (250 ms) luminance decrement. Luminance decrements were determined using an adaptive, 1-up-2-down staircase procedure (<xref ref-type="bibr" rid="bib15">Levitt, 1971</xref>) prior to scanning, producing a detection rate of approximately 70% outside the scanner. We found that subjects behavior improved inside the scanner and detected the luminance changes with greater than 90% accuracy. The Gabor stimulus was not relevant to the task and subjects were not instructed to attend to it. Each fMRI run lasted for 288 s and included four blocks of each of the three conditions in a randomized order.</p></sec><sec id="s4-5-3"><title>Expt 3: Local motion control</title><p>This experiment controlled for differences in the pattern of local internal motion in Expts 1 and 2. While the two illusory drift paths (leftward drift, rightward drift) were balanced for net local and global motion energy, they differed in the order of motion direction. In the leftward drift illusion condition, local motion started leftward (for 750 ms) and was followed by rightward motion (for 750 ms). Vice versa for the rightward drift illusion condition. Expt 3 consisted of 4 conditions. Conditions 1 and 2 were identical to the two illusory conditions in Expts 1 and 2. However, in this experiment there were two additional control conditions in which participants viewed the same patterns of local motion as in conditions 1 and 2, but the Gabor did not move across the screen (no global motion trajectory), nor were there smooth pursuit eye movements. Instead, participants fixated a stationary target alongside a stationary Gabor containing internal motion to the left and right. In condition 3, the order of local motion was the same as in condition 1 (leftward followed by rightward). Condition 4 matched the local motion of condition 2. Participants were instructed to press a button when they detected the brief (250 ms) luminance decrement. Each fMRI run lasted for 288 s and included 3 blocks of each of the 4 conditions in randomized order.</p></sec><sec id="s4-5-4"><title>Expt 4: Concurrent eye tracking</title><p>This experiment was identical to Expt 3, except for two important differences. First, the experiment was performed with concurrent high-resolution eye tracking. Second, we ran the Luminance decrements task using an adaptive, 1-up-2-down staircase procedure (<xref ref-type="bibr" rid="bib15">Levitt, 1971</xref>) for an extended period of time inside the scanner before the start of the experiment. Once the luminance decriment thresholds were stable at 70% inside the scanner, we fixed the decrement step size for the rest of the experiment. Analysis of behavioral performance inside the scanner confirmed that performance was as intended for each of the six subjects was as intended (66, 70, 78, 80, 75, and 71%). This was important for ensuring high task demands, so that subjects could not simultaneously attend the Gabor stimulus.</p></sec><sec id="s4-5-5"><title>Stimulus-only localizer experiment</title><p>In each scanning session, participants were scanned in a stimulus-only localizer experiment in which the stimulus appeared and disappeared in a two-condition block alternation protocol (9 s on, 9 s off; 14 blocks per fMRI run, lasting 252 s). Participants maintained fixation on a stationary target. A vertical Gabor stimulus appeared at the same size and eccentricity as in the main experiment. The Gabor contained internal local motion that changed direction randomly every 250 ms. After 9 s, the stimulus disappeared and participants continued to fixate. Three stimulus-only runs were included in each scanning session, one run at the beginning of the session, one in the middle, and a third run at the end of the session. Participants did not perform a behavioral task during the stimulus-only experiment.</p></sec><sec id="s4-5-6"><title>Eye-movement localizer experiment</title><p>In each scanning session, participants were also scanned in an eye-movement localizer experiment in which participants tracked a moving fixation dot that was identical to the double-drift illusion experiments, except that there was no peripheral Gabor stimulus. Responses to pursuit eye movements were measured in a two-condition block alternation protocol (9 s pursuit, 9 s fixation; 14 blocks per fMRI run, each lasting 252 s). Two eye-movement localizer runs were included in each scanning session, one run at the beginning of the session and a second run at the end of the session. Participants did not perform a behavioral task during eye-movement localizer experiment.</p></sec></sec><sec id="s4-6"><title>Experimental setup: Behavioral</title><p>Stimuli were generated using MATLAB (MathWorks, MA) and MGL (<xref ref-type="bibr" rid="bib10">Gardner et al., 2018</xref>) on a Macintosh computer, and presented on a 61-inch screen (BenQ XL242OZ) positioned 57 cm away from the participant. Participants were seated in a darkened room and were head stabilized by a chin rest. An Eyelink 1000 eye-tracking system was used to measure binocular eye position at 1000 Hz. Eye-tracking calibration was performed at the beginning of the session and repeated intermittently throughout the session to ensure that eye tracking accuracy remained within 1° of visual angle throughout the experiment.</p></sec><sec id="s4-7"><title>Experimental setup: fMRI</title><p>Stimuli were generated using MATLAB (MathWorks) and MGL (<xref ref-type="bibr" rid="bib10">Gardner et al., 2018</xref>) on a Macintosh computer. Stimuli were displayed via a PLUS U2-1200 LCD projector (resolution: 1024 × 768 pixels; refresh rate: 60 Hz) onto a back-projection screen in the bore of the magnet. Participants viewed the display through an angled mirror at a viewing distance of approximately 58 cm, producing a field of view of 20.5° × 16.1°.</p><p>fMRI data were acquired from participants on a research-dedicated Siemens 7T Magnetom scanner using a 32-channel head coil, located in the Clinical Research Center on the National Institutes of Health campus (Bethesda, MD). Functional imaging was conducted with 56 slices oriented parallel to the calcarine sulcus covering the posterior half of the brain: TR: 1500 ms; TE 23 ms; FA: 55°; voxel size: 1.2 × 1.2 × 1.2 mm with 10% gap between slices; grid size: 160 × 160 voxels. Multiband factor 2, GRAPPA/iPAT factor 3. The slices covered all of the occipital and parietal lobes, and the posterior portion of the temporal lobe. For Expt 5, voxel size was increased to 1.8 × 1.8 × 1.8 mm, with 0% gap, Multiband factor 2, and GRAPPA/iPAT factor 2.</p><p>For each participant, a high-resolution anatomy of the entire brain was acquired by co-registering and averaging between 2 and 8 T1-weighted anatomical volumes (magnetization-prepared rapid-acquisition gradient echo, or MP2RAGE; TR: 2500 ms; TE: 3.93 ms; FA: 8°; voxel size: 0.7 × 0.7 × 0.7 mm; grid size: 256 × 256 voxels). The averaged anatomical volume was used for co-registration across scanning sessions and for gray-matter segmentation and cortical flattening. Functional scans were acquired using T2*-weighted, gradient recalled echo-planar imaging to measure blood oxygen level-dependent (BOLD) changes in image intensity (<xref ref-type="bibr" rid="bib24">Ogawa et al., 1990</xref>). The in-plane anatomical was aligned to the high-resolution anatomical volume using a robust image registration algorithm (<xref ref-type="bibr" rid="bib23">Nestares and Heeger, 2000</xref>).</p><p>Prior to the first experimental functional run of each session, 30 volumes were acquired with identical scanning parameters and slice prescription as the subsequent functional runs, except for the phase encoding direction which was reversed. This single reverse phase-encoded run was used to estimate the susceptibility-induced off-resonance field using a method similar to that described in <xref ref-type="bibr" rid="bib2">Andersson et al., 2003</xref> as implemented in FSL (<xref ref-type="bibr" rid="bib32">Smith et al., 2004</xref>). This estimate was then used to correct the spatial distortions in each subsequent run in the session.</p></sec><sec id="s4-8"><title>fMRI preprocessing and analysis</title><p>The anatomical volume acquired in each scanning session was aligned to the high-resolution anatomical volume of the same participant’s brain using a robust image registration algorithm (<xref ref-type="bibr" rid="bib23">Nestares and Heeger, 2000</xref>). Head movement within and across scans was compensated using standard procedures (<xref ref-type="bibr" rid="bib23">Nestares and Heeger, 2000</xref>). The time series from each voxel was divided by its mean to convert from arbitrary intensity units to percent modulation and high-pass filtered (cutoff = 0.01 Hz) to remove low-frequency noise and drift (<xref ref-type="bibr" rid="bib31">Smith et al., 1999</xref>).</p></sec><sec id="s4-9"><title>ROI definition</title><p>ROIs were defined according to an anatomical template (<xref ref-type="bibr" rid="bib3">Benson and Winawer, 2018</xref>). Such templates are inherently imprecise because of inter-subject variability. Each subject has a slightly different anatomy and a slightly different location of functional ROIs. The smaller an ROI the more severe this imprecision becomes relative to the ROI size. To mitigate this problem, we combined nearby regions to form larger ROIs which we analyzed. V1, V2, and V3 were combined to form an early visual cortex ROI (EVC); LO1 and LO2 were combined into LO; V3A and V3B were combined into V3A/B; TO1 and TO2 were combined to form hMT+, corresponding to MT and MST.</p></sec><sec id="s4-10"><title>Data analysis for stimulus-only and eye-movement localizers</title><p>The three stimulus localizer scans were averaged together, and the two eye-movement scans were averaged together. The first cycle of each averaged time series was discarded, leaving 13 cycles. Each individual voxel’s time course was fitted to a cosine with a period matching the cycle duration of 12 volumes (18 s). Each voxel was then assigned the correlation coefficient and phase of the best-fitting cosine. Voxels with a correlation coefficient greater than 0.2 were considered active in the localizer.</p></sec><sec id="s4-11"><title>GLM analysis</title><p>BOLD fMRI time series were averaged across all voxels within each ROI. Trials were then divided into two conditions: double-drift illusion (either leftward or rightward) and no illusion. Each condition was modeled by 16 predictors, one for each time point in the 24 s following the beginning of the trial. Deconvolution was performed by multiplying the pseudoinverse of the condition predictor matrix with the time series (<xref ref-type="bibr" rid="bib6">Dale, 1999</xref>). This procedure yields two hemodynamic response functions for each ROI. The average of the two response functions was used as the ROI’s hemodynamic response function for the next analysis step. Next, a design matrix was constructed for each ROI, with a single HRF function modeling each 12 s block. Response amplitudes were then computed by taking the pseudoinverse of this design matrix and multiplying it with the single voxel time series.</p><p>The goal of this analysis was to estimate a response amplitude for each voxel on each scanning run. The first step was to estimate a single hemodynamic response function for each ROI and each participant. This was accomplished by averaging across all voxels within the ROI, and then using deconvolution (<xref ref-type="bibr" rid="bib9">Gardner et al., 2008</xref>) to estimate a hemodynamic response function (collapsing across the different conditions) over a 24 s period following the beginning of the block. Next, this single hemodynamic response was used to create a design matrix, treating each of the conditions independently. Response amplitudes were then computed by taking the pseudoinverse of this design matrix and multiplying it with the time series for each individual voxel within the ROI. This procedure allowed for differences in the shape of the hemodynamic response across different ROIs and across different participants.</p></sec><sec id="s4-12"><title>Decoding analysis</title><p>In multivariate classification analysis of fMRI data, each condition is represented by a set of points in multidimensional space, with dimensionality equal to the number of voxels and each point corresponding to a single measurement. Accurate decoding is possible when the responses corresponding to different conditions form distinct clusters within this high-dimensional space (<xref ref-type="bibr" rid="bib26">Pereira et al., 2009</xref>). We measured the amplitude of the fMRI response during 12 s blocks of trials, in which each block consisted of eight up-down traversals of the double-drift illusion. We took the beta weight from the GLM analysis (see above) as the amplitude of the response during each 12 s block as a single input to the classification analysis. These response amplitudes were stacked across blocks within a run, and across runs within a session, forming an m × n matrix, with m being the number of voxels in the region of interest and n being the number of repeated measurements in the session. The value of n was typically 64 (for 14 of 19 participants), and ranged from 48 (1 participant) to 96 (4 participants). We only included voxels with a GLM R<sup>2</sup> in the top 50th percentile. Prior to decoding each voxel’s beta weights were z-scored. Decoding was performed with a maximum likelihood classifier using the MATLAB function ‘classify’ with the option ‘diagLinear’ (<xref ref-type="bibr" rid="bib28">Roth et al., 2018</xref>). Decoding accuracy was computed using leave-one-run out cross-validation. The m × n data matrix was partitioned along the n dimension (repeated measurements) into training and testing sets, in which the training set consisted of the blocks from all but one of the runs, and the testing set included the blocks (from all three conditions) from the left out run. Because the data in the training and testing sets were drawn from different runs in the same session, they were statistically independent. The training set was used to estimate the parameters (multivariate means and variances) of the maximum-likelihood classifier. The testing set was then used for decoding. Decoding accuracy was determined as the proportion of the test examples that the classifier was able to correctly assign to one of the two illusory drift paths. Illusion decoding was performed separately for left vs no-illusion, and for right vs no-illusion, and then averaged across both.</p><p>The leave-one-run-out cross-validation procedure resulted in a single decoding accuracy estimate per ROI per session. A non-parametric permutation test was used to evaluate the significance of this decoding accuracy. Specifically, we constructed a distribution of accuracies expected under the null hypothesis that there is no difference between the two illusory drift paths. To generate a null distribution decoding accuracy, we permuted the block labels for each run and repeated the leave-one-run-out decoding analysis. Repeating this randomization 1000 times yielded a distribution of accuracies expected under the null hypothesis. Accuracies computed using the unrandomized training data were then considered statistically significant when decoding accuracy was higher than the 95th percentile of the null distribution (p&lt;0.05, one-tailed permutation test).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Supervision, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Clinical trial registration ClinicalTrials.gov identifier: NCT00001360.</p></fn><fn fn-type="other"><p>All participants granted informed consent under an NIH Institutional Review Board approved protocol (93-M-0170, ClinicalTrials.gov identifier: NCT00001360).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-76803-transrepform1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code are publicly available on Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/elimerriam/doubleDrift">https://github.com/elimerriam/doubleDrift</ext-link> copy archived at <xref ref-type="bibr" rid="bib21">Merriam, 2024</xref>) and the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/3ZX8P">https://doi.org/10.17605/OSF.IO/3ZX8P</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>ZN</given-names></name><name><surname>Steinberg</surname><given-names>NJ</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data and code for &quot;Brain representations of motion and position in the double drift illusion&quot;</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/3ZX8P</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Intramural Research Program of the National Institutes of Health (ZIAMH002966), National Institute of Mental Health Clinical Study Protocol 93M-0170, NCT00001360.</p><p>We would like to thank Francisco Pereira, June Kim, and Cassie Jones for assisting with this study.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amano</surname><given-names>K</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual field maps, population receptive field sizes, and visual field coverage in the human MT+ complex</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>2704</fpage><lpage>2718</lpage><pub-id pub-id-type="doi">10.1152/jn.00102.2009</pub-id><pub-id pub-id-type="pmid">19587323</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Skare</surname><given-names>S</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian analysis of retinotopic maps</article-title><source>eLife</source><volume>7</volume><elocation-id>e40224</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.40224</pub-id><pub-id pub-id-type="pmid">30520736</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>P</given-names></name><name><surname>Tse</surname><given-names>PU</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The vector combination underlying the double-drift illusion is based on motion in world coordinates: evidence from smooth pursuit</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/19.14.2</pub-id><pub-id pub-id-type="pmid">31826247</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crespi</surname><given-names>S</given-names></name><name><surname>Biagi</surname><given-names>L</given-names></name><name><surname>d’Avossa</surname><given-names>G</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name><name><surname>Tosetti</surname><given-names>M</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spatiotopic coding of BOLD signal in human visual cortex depends on spatial attention</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e21661</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0021661</pub-id><pub-id pub-id-type="pmid">21750720</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Optimal experimental design for event-related fMRI</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:2/3&lt;109::AID-HBM7&gt;3.0.CO;2-W</pub-id><pub-id pub-id-type="pmid">10524601</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>d’Avossa</surname><given-names>G</given-names></name><name><surname>Tosetti</surname><given-names>M</given-names></name><name><surname>Crespi</surname><given-names>S</given-names></name><name><surname>Biagi</surname><given-names>L</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatiotopic selectivity of BOLD responses to visual motion in human area MT</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>249</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1038/nn1824</pub-id><pub-id pub-id-type="pmid">17195842</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Graf</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Spatial invariance of visual receptive fields in parietal cortex neurons</article-title><source>Nature</source><volume>389</volume><fpage>845</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1038/39865</pub-id><pub-id pub-id-type="pmid">9349815</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Maps of visual space in human occipital cortex are retinotopic, not spatiotopic</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>3988</fpage><lpage>3999</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5476-07.2008</pub-id><pub-id pub-id-type="pmid">18400898</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Larsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>MGL: visual Psychophysics stimuli and experimental design package</data-title><version designator="2.0">2.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1299496">https://doi.org/10.5281/zenodo.1299496</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golomb</surname><given-names>JD</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Higher level visual cortex represents retinotopic, not spatiotopic, object location</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>2794</fpage><lpage>2810</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr357</pub-id><pub-id pub-id-type="pmid">22190434</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haladjian</surname><given-names>HH</given-names></name><name><surname>Lisi</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motion and position shifts induced by the double-drift stimulus are unaffected by attentional load</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>80</volume><fpage>884</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.3758/s13414-018-1492-0</pub-id><pub-id pub-id-type="pmid">29476332</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>ML</given-names></name><name><surname>Schwarzkopf</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>V1 encodes the perceived position of static but not moving objects</article-title><source>Open Science Framework</source><volume>01</volume><elocation-id>5ymv</elocation-id><pub-id pub-id-type="doi">10.31219/osf.io/c5ymv</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Dougherty</surname><given-names>RF</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Retinotopy and functional subdivision of human areas MT and MST</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>7195</fpage><lpage>7205</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-16-07195.2002</pub-id><pub-id pub-id-type="pmid">12177214</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Transformed up-down methods in psychoacoustics</article-title><source>The Journal of the Acoustical Society of America</source><volume>49</volume><elocation-id>467</elocation-id><pub-id pub-id-type="pmid">5541744</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisi</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dissociation between the perceptual and saccadic localization of moving objects</article-title><source>Current Biology</source><volume>25</volume><fpage>2535</fpage><lpage>2540</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.021</pub-id><pub-id pub-id-type="pmid">26412133</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Tse</surname><given-names>PU</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural correlates of the conscious perception of visual location lie outside visual cortex</article-title><source>Current Biology</source><volume>29</volume><fpage>4036</fpage><lpage>4044</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.033</pub-id><pub-id pub-id-type="pmid">31761706</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maus</surname><given-names>GW</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Motion-dependent representation of space in area MT+</article-title><source>Neuron</source><volume>78</volume><fpage>554</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.010</pub-id><pub-id pub-id-type="pmid">23664618</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKyton</surname><given-names>A</given-names></name><name><surname>Zohary</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Beyond retinotopic mapping: the spatial representation of objects in the human lateral occipital complex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>1164</fpage><lpage>1172</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl027</pub-id><pub-id pub-id-type="pmid">16818474</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Modulation of visual responses by gaze direction in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>9879</fpage><lpage>9889</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0500-12.2013</pub-id><pub-id pub-id-type="pmid">23761883</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Merriam</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>doubleDrift</data-title><version designator="swh:1:rev:2769b4f88537c53aa8087999b5ace7794ff0e286">swh:1:rev:2769b4f88537c53aa8087999b5ace7794ff0e286</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:af7532efbeb9fb5d582322ce948f853d453e9cb3;origin=https://github.com/elimerriam/doubleDrift;visit=swh:1:snp:f3fd0159a1d6ba17fbfee77d7b92a2338eff4c03;anchor=swh:1:rev:2769b4f88537c53aa8087999b5ace7794ff0e286">https://archive.softwareheritage.org/swh:1:dir:af7532efbeb9fb5d582322ce948f853d453e9cb3;origin=https://github.com/elimerriam/doubleDrift;visit=swh:1:snp:f3fd0159a1d6ba17fbfee77d7b92a2338eff4c03;anchor=swh:1:rev:2769b4f88537c53aa8087999b5ace7794ff0e286</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Schindler</surname><given-names>A</given-names></name><name><surname>Bartels</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Real-motion signals in human early visual cortex</article-title><source>NeuroImage</source><volume>175</volume><fpage>379</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.04.012</pub-id><pub-id pub-id-type="pmid">29649561</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nestares</surname><given-names>O</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Robust multiresolution alignment of MRI brain volumes</article-title><source>Magnetic Resonance in Medicine</source><volume>43</volume><fpage>705</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1002/(sici)1522-2594(200005)43:5&lt;705::aid-mrm13&gt;3.0.co;2-r</pub-id><pub-id pub-id-type="pmid">10800036</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogawa</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>TM</given-names></name><name><surname>Kay</surname><given-names>AR</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Brain magnetic resonance imaging with contrast dependent on blood oxygenation</article-title><source>PNAS</source><volume>87</volume><fpage>9868</fpage><lpage>9872</lpage><pub-id pub-id-type="doi">10.1073/pnas.87.24.9868</pub-id><pub-id pub-id-type="pmid">2124706</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cosmomvpa: Multi-modal multivariate pattern analysis of neuroimaging data in matlab/gnu octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Mitchell</surname><given-names>T</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title><source>NeuroImage</source><volume>45</volume><fpage>S199</fpage><lpage>S209</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.11.007</pub-id><pub-id pub-id-type="pmid">19070668</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Optimal multimodal integration in spatial localization</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>14259</fpage><lpage>14268</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0523-13.2013</pub-id><pub-id pub-id-type="pmid">23986259</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>ZN</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stimulus vignetting and orientation selectivity in human visual cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e37241</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37241</pub-id><pub-id pub-id-type="pmid">30106372</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>ZN</given-names></name><name><surname>Ryoo</surname><given-names>M</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Task-related activity in human visual cortex</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000921</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000921</pub-id><pub-id pub-id-type="pmid">33156829</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>A</given-names></name><name><surname>Lu</surname><given-names>ZL</given-names></name><name><surname>Huang</surname><given-names>CB</given-names></name><name><surname>Knight</surname><given-names>E</given-names></name><name><surname>Ennis</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Transitions between central and peripheral vision create spatial/temporal distortions: a hypothesis concerning the perceived break of the curveball</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e13296</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0013296</pub-id><pub-id pub-id-type="pmid">20967247</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AM</given-names></name><name><surname>Lewis</surname><given-names>BK</given-names></name><name><surname>Ruttimann</surname><given-names>UE</given-names></name><name><surname>Ye</surname><given-names>FQ</given-names></name><name><surname>Sinnwell</surname><given-names>TM</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Duyn</surname><given-names>JH</given-names></name><name><surname>Frank</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Investigation of low frequency drift in fMRI signal</article-title><source>NeuroImage</source><volume>9</volume><fpage>526</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1006/nimg.1999.0435</pub-id><pub-id pub-id-type="pmid">10329292</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AT</given-names></name><name><surname>Williams</surname><given-names>AL</given-names></name><name><surname>Singh</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Negative BOLD in the visual cortex: evidence against blood stealing</article-title><source>Human Brain Mapping</source><volume>21</volume><fpage>213</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1002/hbm.20017</pub-id><pub-id pub-id-type="pmid">15038003</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinberg</surname><given-names>NJ</given-names></name><name><surname>Roth</surname><given-names>ZN</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Spatiotopic and retinotopic memory in the context of natural images</article-title><source>Journal of Vision</source><volume>22</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1167/jov.22.4.11</pub-id><pub-id pub-id-type="pmid">35323869</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>LD</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Corollary discharge and oculomotor proprioception: Cortical mechanisms for spatially accurate vision</article-title><source>Annual Review of Vision Science</source><volume>2</volume><fpage>61</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035407</pub-id><pub-id pub-id-type="pmid">28532350</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname><given-names>PU</given-names></name><name><surname>Hsieh</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The infinite regress illusion reveals faulty integration of local and global motion signals</article-title><source>Vision Research</source><volume>46</volume><fpage>3881</fpage><lpage>3885</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.06.010</pub-id><pub-id pub-id-type="pmid">16879854</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turi</surname><given-names>M</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spatiotopic perceptual maps in humans: evidence from motion adaptation</article-title><source>Proceedings. Biological Sciences</source><volume>279</volume><fpage>3091</fpage><lpage>3097</lpage><pub-id pub-id-type="doi">10.1098/rspb.2012.0637</pub-id><pub-id pub-id-type="pmid">22535785</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>HX</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Motion direction biases and decoding in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>12601</fpage><lpage>12615</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1034-14.2014</pub-id><pub-id pub-id-type="pmid">25209297</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76803.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.01.25.477714" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.25.477714"/></front-stub><body><p>This important and elegant imaging experiment in humans shows that visual area hMT+, but not other candidate brain areas, signal the perceived motion path in a visual drift illusion. Using a convincing computational decoding approach, the results indicate a perceptual representation of the illusory position in space for moving stimuli even when the actual retinal position of the stimulus is kept stable. Such a representation and the underlying neural mechanisms are of broad importance for our understanding of the neural basis of sensory perception.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76803.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Morrone</surname><given-names>Maria Concetta</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ad39j10</institution-id><institution>University of Pisa</institution></institution-wrap><country>Italy</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.25.477714">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.01.25.477714v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you very much for submitting your article &quot;Neural Basis of The Double Drift Illusion&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Kristine Krug as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Corentin Gaillard (Reviewer #1); Maria Concetta Morrone (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions (for the authors):</p><p>There are three main areas, the reviewers have identified that need further clarification and some limited additional analyses. They are summarised here. Please use the recommendations to the authors below for the more detailed questions that need addressing.</p><p>1) Motivation of the particular study and discussion of the results in relation to the existing literature needs expansion for the reader to be able to better evaluate the specific contribution of this particular study.</p><p>- The authors motivate the study by saying that there have been conflicting results about which brain areas are involved in spatiotopic coding, but they did not give an indication about why there might be conflicting results or why the current study is suitable to address the previous discrepancies. Is this study simply adding another observation to the existing body of literature, or does it go beyond previous studies in a critical theoretical way, especially also with regard to Liu et al. 2019 (Current Biology)?</p><p>- A more nuanced discussion of the more controversial literature on retinotopic vs. spatiotopic visual coding and where the current work is situated in introduction and discussion. The authors seem to confirm the importance of attention, and that could be made more explicit. In addition, there is much evidence both for retinotopy and spatiotopy which is simply overlooked.</p><p>- How do the current results relate to the literature on a role of hMT+ in global motion perception? Is this a potential alternative interpretation of the results and if not, why not.</p><p>2) The results and methods require more detailed explanation and some limited additional data, in particular with regards to:</p><p>- the eye movement controls</p><p>- the ROI definitions</p><p>- the decoding method</p><p>- some of the more marginal statistical results</p><p>3) The reviewers would like to see a direct, quantitative comparison of the decoding for different motion directions of the drift illusion with and without attention (i.e. the attention comparison in 3A but for decoding left-right trajectory).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) lines 289-298: Eye movement control: &quot;Pursuit accuracy was confirmed during the behavioral experiment or each subject prior to the fMRI experiment.&quot; A weakness of the paper is the lack of eye movement measurements during the MR scan. Changes in eye movement for the combination of different combinations of up- and downwards drift with directions of motion could potentially provide an explanation of the source of the perceptual signals.</p><p>(i) The authors should report the detailed behavioural data from their eye movement controls. For instance, show up and down pursuit trajectories separately when combined with left or right stimulus motion to exclude eye movement drift as a signal source and test whether they are different.</p><p>(ii) This control is also relevant to the last point from the discussion (line 256) whether the authors are dealing with (a) a retinotopic representation associated with saccade driven remapping of receptive field or (b) a spatiotopic representation.</p><p>The authors defend the second argument based on the fact that their task does not require saccades (therefore no remapping), but there could still be catch-up saccades during the smooth pursuit phase and there is also no control for micro-saccades.</p><p>2) The authors want to rule out the hypothesis that differences in pursuit eye movements could account for the decoding performance. In figure 3B right, they claim that hMT eye movement voxels did not contain information about the illusionary trajectory but:</p><p>Accuracy for that decoding is 0.53+-0.8, p-value 0.052, which is just not significant. If you look at the table, For Expt 1, second condition, LO based decoding is at 0.5260 +- 0.10 and has p-value of 0.047. This could arise from the fact that the decoded categories are not the same and from changes in 95%CI, but given the number of comparisons and very close results, Expt 2 is a little bit less convincing than either Exit 1or 3 with their elegant designs.</p><p>This requires a more detailed and differentiated discussion of the underlying statistics.</p><p>The authors should also show (in a supplement) the distribution of the eye movement related voxels they excluded/analysed separately in the different subjects.</p><p>3) In a previous (neurophysiology) study, the authors made the point that V5/MT signals in macaques were not coding a global motion percept (Hedges et al. 2011). A critical point there was the size of stimulus relative to the receptive field size in V5/MT. In reference to that, a more detailed discussion would be helpful, in terms of the extent to which the receptive fields in hMT+ encode spatial position in this paradigm and the size of their stimulus relative to hMT+ receptive field sizes might shed light on the underlying neural mechanism.</p><p>In general, it would be good to see more discussion of how the current study is situated relative to other studies that suggested (or not) a role of V5/MT with regard to perceptual signalling.</p><p>4) More experimental detail in some areas would be helpful to the reader (see also points 1-3)</p><p>- Abstract: Include type details of analysis used.</p><p>- eye movement data from behavioural experiments (supplement)</p><p>- number and distribution of eye movement related voxels.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>One major concern, which could confuse readers, is the treatment of previous research of the authors. The opening line is: &quot;The primate visual system is retinotopic: neurons throughout visual cortex encode the location of visual stimuli on the retina (Gardner et al., 2008)&quot;: a simple statement of undisputed fact, with no qualification, no mention that several other papers, before and after Gardner, have reported different data and drawn different conclusions. Later in the introduction we hear that there is indeed a controversy, with a possible explanation for the discrepant results (attentional focus). Later still their results are presented, strongly supporting a non-retinotopic representation in hMT+. Then in the discussion, the first sentence is reiterated: &quot;Activity throughout visual cortex is known to encode stimulus position in retinal, not spatiotopic, coordinates (Gardner et al., 2008)&quot;, with no discussion of the discrepancy between the current results and this simple, undisputed &quot;historical fact&quot;.</p><p>This is very hard to follow. However much one is attached to one's own work, it cannot be considered Gospel truth and everything else noise. This becomes particularly bizarre when the &quot;noise&quot; is consistent with the current research, and the previous publication not. I think the readers deserve a discussion on the discrepancy, and how best to move forward. The authors seem to confirm the importance of attention, and that could be made more explicit (but see methodological criticisms). In addition, there is much evidence both for retinotopy and spatiotopy which is simply overlooked.</p><p>Perhaps the title could be improved, to reflect the actual conclusions of the paper (non-retinal motion response in MT). It does not really discover the &quot;Neural basis for the double-drift illusion&quot;, as it specifically examines only the condition when observers track the stimulus, stabilizing it on the retina. It also does not speak to the main result, spatiotopic cortical representation.</p><p>The results and methods presentation also requires a more detailed explanation. At present many analyses are unclear or use the wrong methodology.</p><p>1. Definition of ROIs. It is unclear if the stimuli used to define the map of figure 2A is moving vertically and if the %BOLD of figure 2B is taken for the extended ROI defined by the atlas as stated in the methods, or are simply the average of the significant foci of figure 2A. Given the maximum activity of 1% I am inclined to say that it is the response of only the significant voxels in the ROIs. Usually, peripheral stimuli do not produce such large activity. But if so the analysis is subject to circularity!</p><p>In the methods they state that given the great variability of segmentation with atlas they pooled together early visual areas. This is very peculiar, given the much greater variability in the segmentation of LO1 and LO2. Why not use a better and complete atlas, like the Glasser? Why not show a map of the average across all subjects?</p><p>2. The decoding methods are very compressed and many details are not available. Normally for any decoding strategy the average activity is normalize so the decoding is not biased by the different mean. Has this been done?</p><p>Decoding responses to stimuli that have a different average power of signal is trivial, so the results of figure 3A that compare two stimuli with greatly different energy should be eliminated. In any case the different stimulus power between stimuli in Figure 1C and the other two should be measured and discussed.</p><p>3. The attention control is important, given the previous dispute, but it should be run between the two different motion directions. The question is: in the unattended condition can the direction of the illusory motion been decoded?</p><p>4. Also eye-movements is a crucial aspect, but not so much for pursuit given that hopefully the subject were fixating, but for drift. The direction of eye drift could bias the decoding results. However, the comparison of decoding between sets of voxels of different numerosity is a potential problem. The stimulus voxels should be reduced in size to match the other ROI.</p><p>To conclude, I think the work is interesting and important, worth eventual publication in a good journal. However, it needs a major rewrite, detailing better important technical details, reviewing existing literature with a less egocentric bias, and discussing better the apparent conflicts between this paper and previously published studies, including those of the authors.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Recommendations</p><p>1. Regarding major comment 3 -- I believe there are a couple of things the authors can do to increase confidence that eye movements are not driving the results. (1) Redo the decoding from stimulus selective voxels while excluding all pursuit selective voxels. (2) If it is not possible to measure (or ideally, control) eye position in the scanner, a behavioral version of the &quot;attend to fixation&quot; experiment could be performed with eye tracking to determine whether smooth pursuit eye movements are affected by the illusory drift direction.</p><p>2. It would be helpful if the authors could more thoroughly report the behavioral results for both tasks, including performance levels and threshold values on the fixation task to demonstrate that the task was demanding and the staircase was working well.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76803.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions (for the authors):</p><p>There are three main areas, the reviewers have identified that need further clarification and some limited additional analyses. They are summarised here. Please use the recommendations to the authors below for the more detailed questions that need addressing.</p><p>1) Motivation of the particular study and discussion of the results in relation to the existing literature needs expansion for the reader to be able to better evaluate the specific contribution of this particular study.</p><p>- The authors motivate the study by saying that there have been conflicting results about which brain areas are involved in spatiotopic coding, but they did not give an indication about why there might be conflicting results or why the current study is suitable to address the previous discrepancies. Is this study simply adding another observation to the existing body of literature, or does it go beyond previous studies in a critical theoretical way, especially also with regard to Liu et al. 2019 (Current Biology)?</p></disp-quote><p>Here, the Reviewers are making two separate points. The first point regards the earlier controversy regarding whether or not human area MT contains an explicit map-like representation of absolution spatial position that is invariant to eye position (i.e., a spatiotopic representation, rather than a retinotopic representation). We have revised the text in several places to summarize this literature and better contextualize our results given the previous work in this field.</p><p>The second point regards the novelty of our results in light of a previous study on the double drift illusion (Liu et al., 2019). Liu et al. addressed a different question. In that study, subjects fixated for the entire duration of the experiment, so the issue of visual reference frame was not addressed; Liu et al. instead focused on the representation of stimulus position during fixation (regardless of reference frame) and found significant decoding in far anterior frontal regions. We have now clarified this important distinction in the manuscript.</p><disp-quote content-type="editor-comment"><p>- A more nuanced discussion of the more controversial literature on retinotopic vs. spatiotopic visual coding and where the current work is situated in introduction and discussion. The authors seem to confirm the importance of attention, and that could be made more explicit. In addition, there is much evidence both for retinotopy and spatiotopy which is simply overlooked.</p></disp-quote><p>We have addressed this comment by substantially editing the Discussion.</p><disp-quote content-type="editor-comment"><p>- How do the current results relate to the literature on a role of hMT+ in global motion perception? Is this a potential alternative interpretation of the results and if not, why not.</p></disp-quote><p>Please see our response below to Reviewer 1, Comment 1ii.</p><disp-quote content-type="editor-comment"><p>2) The results and methods require more detailed explanation and some limited additional data, in particular with regards to:</p><p>- the eye movement controls</p><p>- the ROI definitions</p><p>- the decoding method</p><p>- some of the more marginal statistical results</p></disp-quote><p>We address each of these points in considerable detail below where they appear in the Reviewers comments.</p><disp-quote content-type="editor-comment"><p>3) The reviewers would like to see a direct, quantitative comparison of the decoding for different motion directions of the drift illusion with and without attention (i.e. the attention comparison in 3A but for decoding left-right trajectory).</p></disp-quote><p>Please see response to Reviewer 2 below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) lines 289-298: Eye movement control: &quot;Pursuit accuracy was confirmed during the behavioral experiment or each subject prior to the fMRI experiment.&quot; A weakness of the paper is the lack of eye movement measurements during the MR scan. Changes in eye movement for the combination of different combinations of up- and downwards drift with directions of motion could potentially provide an explanation of the source of the perceptual signals.</p></disp-quote><p>We now include eye tracking data.</p><p>We have addressed the Reviewer's comment by repeating the fMRI experiment in a new group of subjects in which we were able to also obtain concurrent, high-quality eye tracking. When we initially conducted the experiment, it was not possible to perform eye tracking in the 7T scanner at NIH. Because of this limitation, we were forced to depend on careful eye tracking in a pre-scan behavioral experiment. But in the ensuing period of time, we have developed a protocol for obtaining high quality eye tracking with an Eyelink 1000 mounted in the bore of the scanner. Now that we have the ability to collect concurrent eye tracking, we repeated the fMRI experiment and found that our main fMRI result replicated (i.e, it was possible to decode the direction of the illusion from fMRI responses in hMT+). Additional, the concurrent fMRI eye tracking enabled us to make four important observations (see new Figure 4):</p><p>First, subjects maintained stable fixation when the target was stationary during fixation and accurately pursued the vertically moving target during illusion (Figure 4). This analysis confirms that the drifting Gabor remained at a relatively fixed position on the retina during the illusory period.</p><p>Second, there were no differences in microsaccades between any of the conditions. We quantified the direction, amplitude, and frequency of all saccades for each condition. While we did observe small rightward microsaccades, none of the microsaccade characteristics differed between conditions. The rightward microsaccades may have been due to the sustained eccentric leftward fixation. Or, it may have been due to attention to the right visual field stimulus (despite the foveal attention task). Or it may have reflected the known horizontal microsaccade bias. Regardless, we do not believe our fMRI results are related to microsaccades because these small saccades did not differ across condition.</p><p>Finally, we wondered if small not-easily-quantified ocular deviations could have differed between conditions, and somehow result in differences in fMRI activity picked up by the decoding analysis. To test for this possibility, we trained a classier to discriminate condition based on the raw eye traces (just as we did in the main fMRI data analysis). But unlike the fMRI analysis, we found that it was not possible to decode the direction of the illusion from the eye traces themselves.</p><p>We conclude that the ability to decode the illusion from fMRI responses were not due to differences in eye movements caused by the illusion.</p><disp-quote content-type="editor-comment"><p>(i) The authors should report the detailed behavioural data from their eye movement controls. For instance, show up and down pursuit trajectories separately when combined with left or right stimulus motion to exclude eye movement drift as a signal source and test whether they are different.</p></disp-quote><p>See comment above.</p><disp-quote content-type="editor-comment"><p>(ii) This control is also relevant to the last point from the discussion (line 256) whether the authors are dealing with (a) a retinotopic representation associated with saccade driven remapping of receptive field or (b) a spatiotopic representation.</p><p>The authors defend the second argument based on the fact that their task does not require saccades (therefore no remapping), but there could still be catch-up saccades during the smooth pursuit phase and there is also no control for micro-saccades.</p></disp-quote><p>The reviewer correctly points out that our argument against remapping depends on there being no saccades during the illusion, yet subjects were making microsaccades and catch-up saccades during the pursuit phase of the illusion (our new eye tracking data confirm this). Could remapping during these small saccades produce the illusion? While an intriguing possibility, we think this is unlikely. The small saccades would have to be very regular and consistent to produce such a continuous percept. Nonetheless, we now discuss this possibility.</p><disp-quote content-type="editor-comment"><p>2) The authors want to rule out the hypothesis that differences in pursuit eye movements could account for the decoding performance. In figure 3B right, they claim that hMT eye movement voxels did not contain information about the illusionary trajectory but:</p><p>Accuracy for that decoding is 0.53+-0.8, p-value 0.052, which is just not significant. If you look at the table, For Expt 1, second condition, LO based decoding is at 0.5260 +- 0.10 and has p-value of 0.047. This could arise from the fact that the decoded categories are not the same and from changes in 95%CI, but given the number of comparisons and very close results, Expt 2 is a little bit less convincing than either Exit 1or 3 with their elegant designs.</p><p>This requires a more detailed and differentiated discussion of the underlying statistics.</p></disp-quote><p>Here the Reviewer makes the valid point that one cannot conclude anything from the lack of significance in a particular ROI. We completely agree. It has been convincingly demonstrated that with sufficient SNR, significant effects can be observed throughout the brain, even for the simplest tasks (Gonzalez-Castillo et al., PNAS, 2012), and hence we have little doubt that with infinite scanning, we would eventually observe significant decoding even in the pursuit-selective voxels (and elsewhere too).</p><p>We aren't claiming that regions that exhibit selectivity for smooth pursuit <italic>don't</italic> contain information about the illusion. Rather, we emphasize that regions that are selective for the stimulus <italic>do</italic> contain information about the illusion. This is important because it is precisely these stimulus-related voxels that represent the position of the stimulus during fixation, and hence should also be affected by a shift in perceived position of the stimulus during the illusion.</p><disp-quote content-type="editor-comment"><p>The authors should also show (in a supplement) the distribution of the eye movement related voxels they excluded/analysed separately in the different subjects.</p></disp-quote><p>We have added a flat patch of occipital cortex to Figure 2 showing the spatial distribution of both stimulus-selective and pursuit-selective voxels based on the two localizer scans for one example subject. Stimulus-selective voxels were located where one would expect based on retinotopy. Pursuit-selective voxels were generally located in more peripheral portions of the retinotopic map. Smooth pursuit voxels were also generally more spatially diffuse, with a larger number of responsive voxels. These pursuit voxels may reflect the retinal stimulation caused by the edge of the screen moving on the retina as subjects moved their gaze vertically.</p><disp-quote content-type="editor-comment"><p>3) In a previous (neurophysiology) study, the authors made the point that V5/MT signals in macaques were not coding a global motion percept (Hedges et al. 2011). A critical point there was the size of stimulus relative to the receptive field size in V5/MT. In reference to that, a more detailed discussion would be helpful, in terms of the extent to which the receptive fields in hMT+ encode spatial position in this paradigm and the size of their stimulus relative to hMT+ receptive field sizes might shed light on the underlying neural mechanism.</p><p>In general, it would be good to see more discussion of how the current study is situated relative to other studies that suggested (or not) a role of V5/MT with regard to perceptual signalling.</p></disp-quote><p>The reviewer points out that MT has been shown to code local rather than global motion for stimuli in which there were both local and global motion that were distinct from each other. Such distinction between local and global motion existed in our stimuli where the local motion was horizontal and the global motion was vertical. We interpret the reviewer’s comment to suggest that perhaps global motion signaling was involved in processing the double drift stimulus. However, we point out that the global motion component was the same for all illusory conditions. Hence, we think it unlikely that mechanisms that support global motion perception underlie this particular illusion.</p><disp-quote content-type="editor-comment"><p>4) More experimental detail in some areas would be helpful to the reader (see also points 1-3)</p><p>- Abstract: Include type details of analysis used.</p></disp-quote><p>We have expanded the abstract to include many more details about the analyses.</p><disp-quote content-type="editor-comment"><p>- eye movement data from behavioural experiments (supplement)</p></disp-quote><p>In the revision, we now include eye movement data collected at the same time as fMRI data.</p><disp-quote content-type="editor-comment"><p>- number and distribution of eye movement related voxels.</p></disp-quote><p>This is now included as part of Figure 2 and in Figure 1—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>One major concern, which could confuse readers, is the treatment of previous research of the authors. The opening line is: &quot;The primate visual system is retinotopic: neurons throughout visual cortex encode the location of visual stimuli on the retina (Gardner et al., 2008)&quot;: a simple statement of undisputed fact, with no qualification, no mention that several other papers, before and after Gardner, have reported different data and drawn different conclusions. Later in the introduction we hear that there is indeed a controversy, with a possible explanation for the discrepant results (attentional focus). Later still their results are presented, strongly supporting a non-retinotopic representation in hMT+. Then in the discussion, the first sentence is reiterated: &quot;Activity throughout visual cortex is known to encode stimulus position in retinal, not spatiotopic, coordinates (Gardner et al., 2008)&quot;, with no discussion of the discrepancy between the current results and this simple, undisputed &quot;historical fact&quot;.</p><p>This is very hard to follow. However much one is attached to one's own work, it cannot be considered Gospel truth and everything else noise. This becomes particularly bizarre when the &quot;noise&quot; is consistent with the current research, and the previous publication not. I think the readers deserve a discussion on the discrepancy, and how best to move forward. The authors seem to confirm the importance of attention, and that could be made more explicit (but see methodological criticisms). In addition, there is much evidence both for retinotopy and spatiotopy which is simply overlooked.</p></disp-quote><p>We recognize that we were too selective in our review of the literature on retinotopic organization in the early visual pathway, and we have consequently reframed both the Intro and Discussion to acknowledge that extra retinal factors can modulate the retinotopic maps that are routinely observed in early visual cortex.</p><disp-quote content-type="editor-comment"><p>Perhaps the title could be improved, to reflect the actual conclusions of the paper (non-retinal motion response in MT). It does not really discover the &quot;Neural basis for the double-drift illusion&quot;, as it specifically examines only the condition when observers track the stimulus, stabilizing it on the retina. It also does not speak to the main result, spatiotopic cortical representation.</p></disp-quote><p>We have changed the title to be more closely aligned with the result: <bold><italic>Brain representations of motion and position in the double drift illusion</italic>.</bold></p><disp-quote content-type="editor-comment"><p>The results and methods presentation also requires a more detailed explanation. At present many analyses are unclear or use the wrong methodology.</p><p>1. Definition of ROIs. It is unclear if the stimuli used to define the map of figure 2A is moving vertically and if the %BOLD of figure 2B is taken for the extended ROI defined by the atlas as stated in the methods, or are simply the average of the significant foci of figure 2A. Given the maximum activity of 1% I am inclined to say that it is the response of only the significant voxels in the ROIs. Usually, peripheral stimuli do not produce such large activity. But if so the analysis is subject to circularity!</p></disp-quote><p>We have clarified in the Methods how voxels were selected. Briefly, we used a conjunction of an anatomical atlas and functional localizers. The two functional localizers (the ‘stimulus-alone’ and ‘pursuit-alone’ conditions) were completely independent of the main experiment (different runs, different experimental protocol), so none of the analyses were circular in any way. This has all now been clarified in the revision.</p><disp-quote content-type="editor-comment"><p>In the methods they state that given the great variability of segmentation with atlas they pooled together early visual areas. This is very peculiar, given the much greater variability in the segmentation of LO1 and LO2. Why not use a better and complete atlas, like the Glasser? Why not show a map of the average across all subjects?</p></disp-quote><p>The Reviewer is referring to our explanation in the Methods section in which we explained why we combined multiple Atlas-defined ROI's into a larger, more general ROI. Fundamentally, the issue has to do with the spatial accuracy of all of the human cortical atlases that currently exist. All atlases, including the Glasser atlas, involve the registration of individual subject data to a template. There are spatial errors that are inherent in this procedure — and again, that’s true of all atlases. The spatial errors largely stem from the variability across subjects in sulcal anatomy and the degree to which functional areal boundaries follow anatomic landmarks. These factors result in a fundamental limit in the accuracy of any atlas to accurately identify a particular cortical area. To circumvent this limitation, we simply pooled voxels across adjacent atlas-defined ROIs in V1, V2, and V3 to make a general ‘early visual cortex’ ROI, and TO1 and TO2 to make a single hMT+ ROI. We have published with both ROI criteria previously (e.g., Roth et al. <italic>PLOS Bio</italic>, 2021; Burlingham et al., <italic>eLife</italic>, 2022).</p><disp-quote content-type="editor-comment"><p>2. The decoding methods are very compressed and many details are not available. Normally for any decoding strategy the average activity is normalize so the decoding is not biased by the different mean. Has this been done?</p></disp-quote><p>We have added a number of important details to the methods section that were indeed lacking in the initial submission. For example, we now describe that stimulus-related and eye-movement voxels were identified by localizer coherence &gt; 0.2. (<italic>Methods: Data analysis for stimulus-only and eye-movement localizers)</italic>. For decoding, we only used voxels with R<sup>2</sup> in top 50<sup>th</sup> percentile. Betas were z-scored for each voxel. (<italic>Methods: Decoding analysis</italic>).</p><p>Some fMRI studies using MVPA subtract off the average activity across the ROI prior to building and testing the decoder. But we feel this approach is not appropriate because including the mean cannot bias decoding toward one condition or another (as long as the experimental design is well balanced, which was the case in our experiment). Moreover, subtracting the mean can, in fact, lower sensitivity to real effects. For example, decodable information may be present in the ROI mean response amplitude, or it may be present in the spatial pattern of activity across voxels. Both results are valid, but analyses pipelines involving mean-subtraction are only sensitive to the latter.</p><disp-quote content-type="editor-comment"><p>Decoding responses to stimuli that have a different average power of signal is trivial, so the results of figure 3A that compare two stimuli with greatly different energy should be eliminated. In any case the different stimulus power between stimuli in Figure 1C and the other two should be measured and discussed.</p></disp-quote><p>We don't agree with the reviewer's assertion that the stimuli have different average power. They are matched for contrast energy, which is the way that we and most others think about the 'power' of the stimulus. We are, of course, studying a motion representation — we therefore had to manipulate <italic>motion</italic> energy; it is the core of our experiment. It may not have been clear to the reviewer why we designed the experiment this way. But we contend that there was no other way to design the stimulus while still producing the illusion.</p><disp-quote content-type="editor-comment"><p>3. The attention control is important, given the previous dispute, but it should be run between the two different motion directions. The question is: in the unattended condition can the direction of the illusory motion been decoded?</p></disp-quote><p>We ran the analysis that the reviewer suggested (attempting to decode the direction of the illusion when subjects were not performing a detection task on the pursuit target). In this condition, we found no reliable information about the direction of the illusion. This is somewhat surprising. One explanation for this is that the attentional control in Expt 2-4 served to stabilize attentional state, thereby improving the overall signal-to-noise ratio of the measurement. In Expt 1, subjects simply reported whether or not they perceived the illusion, and hence we cannot be certain of where their attention was directed. We conclude that uncontrolled attention may have obfuscated the signal needed to train the decoder to discriminate the illusion trajectory.</p><disp-quote content-type="editor-comment"><p>4. Also eye-movements is a crucial aspect, but not so much for pursuit given that hopefully the subject were fixating, but for drift. The direction of eye drift could bias the decoding results.</p></disp-quote><p>Please see our response above. In a newly added experiment with eye tracking during the fMRI task, we now show that we cannot decode the different conditions from the eye traces themselves (Figure 4).</p><disp-quote content-type="editor-comment"><p>However, the comparison of decoding between sets of voxels of different numerosity is a potential problem. The stimulus voxels should be reduced in size to match the other ROI.</p></disp-quote><p>We agree with the Reviewer's points regarding ROI size. As Figure 2 now shows, there were many more voxels in the eye movement localizer. And yet, we were unable to decode the direction of the illusion from these ROIs. Hence, we do not think that the ability to decode from the stimulus-selective voxels is due to differences in ROI size.</p><disp-quote content-type="editor-comment"><p>To conclude, I think the work is interesting and important, worth eventual publication in a good journal. However, it needs a major rewrite, detailing better important technical details, reviewing existing literature with a less egocentric bias, and discussing better the apparent conflicts between this paper and previously published studies, including those of the authors.</p></disp-quote><p>We thank the reviewer for the helpful comments and hope that the revision addresses these points satisfactorily.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Recommendations</p><p>1. Regarding major comment 3 -- I believe there are a couple of things the authors can do to increase confidence that eye movements are not driving the results. (1) Redo the decoding from stimulus selective voxels while excluding all pursuit selective voxels. (2) If it is not possible to measure (or ideally, control) eye position in the scanner, a behavioral version of the &quot;attend to fixation&quot; experiment could be performed with eye tracking to determine whether smooth pursuit eye movements are affected by the illusory drift direction.</p></disp-quote><p>To be certain that errors in smooth pursuit weren’t driving the results, we repeated the experiment with concurrent eye tracking in 5 subjects during 7T fMRI scanning. We attempted to decode the direction of the illusion directly from the eye traces, reasoning that any difference in oculomotor behavior (such as, for example, differences pursuit gain or catchup saccades) would enable decoding of the two conditions. We found that there were no differences in eye tracking between the two illusory conditions (see Figure 4)</p><p>We repeated the decoding analysis (as shown in Figure 3) after excluding pursuit selective voxels in hMT+. In all cases, decoding in hMT+ was still significant, albeit less robust (see our response to Major Comment 3, above). We believe this is because there were pursuit-selective voxels in hMT+ that overlap with stimulus selective voxels, which is not surprising given the large population receptive fields of voxels in hMT+ and the small size of the retinotopic map. Excluding pursuit-selective voxels yielded a smaller population with which to conduct the analysis.</p><disp-quote content-type="editor-comment"><p>2. It would be helpful if the authors could more thoroughly report the behavioral results for both tasks, including performance levels and threshold values on the fixation task to demonstrate that the task was demanding and the staircase was working well.</p></disp-quote><p>In addressing the reviewer’s question, we realized that performance on the attention task during scanning was much higher than we expected (&gt;90% accuracy). We think that this may have been because we ran the staircase after putting subjects in the scanner but prior to running the main experiment, at which point we locked the stimulus parameters. Subjects may have become acclimated to the scanning environment and their performance may have thus improved. To address this issue, we reran the experiment in a new group of 5 subjects (see results in Figure 4). We ran the staircase for a much longer period of time at the start of each session until their thresholds were stable at 70% correct. In these new scan sessions, we also had concurrent eye tracking, addressing other concerns brought by the reviewers comments (discussed above). In these new data (see supp Figure 1), behavioral accuracy was on average 70% over the course of the entire session, as expected by the 1-up 2-down staircase that was run prior to the main experiment, so we are confident that subjects were directing attention to the fixation task. The main finding was replicated: we were able to decode the direction of the illusion in hMT+. And there was no decodable information about the direction of the illusion in the eye traces.</p></body></sub-article></article>