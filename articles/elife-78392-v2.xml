<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78392</article-id><article-id pub-id-type="doi">10.7554/eLife.78392</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-274454"><name><surname>Fox</surname><given-names>Kai J</given-names></name><email>kaifox@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-272295"><name><surname>Birman</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3748-6289</contrib-id><email>dbirman@uw.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="pa1">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-73545"><name><surname>Gardner</surname><given-names>Justin L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2220-5050</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>‡</label><p>Department of Biological Structure, University of Washington, Washington, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>15</day><month>05</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e78392</elocation-id><history><date date-type="received" iso-8601-date="2022-03-05"><day>05</day><month>03</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-05-12"><day>12</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-03-07"><day>07</day><month>03</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.03.04.483026"/></event></pub-history><permissions><copyright-statement>© 2023, Fox, Birman et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Fox, Birman et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78392-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-78392-figures-v2.pdf"/><abstract><p>Attention allows us to focus sensory processing on behaviorally relevant aspects of the visual world. One potential mechanism of attention is a change in the gain of sensory responses. However, changing gain at early stages could have multiple downstream consequences for visual processing. Which, if any, of these effects can account for the benefits of attention for detection and discrimination? Using a model of primate visual cortex we document how a Gaussian-shaped gain modulation results in changes to spatial tuning properties. Forcing the model to use only these changes failed to produce any benefit in task performance. Instead, we found that gain alone was both necessary and sufficient to explain category detection and discrimination during attention. Our results show how gain can give rise to changes in receptive fields which are not necessary for enhancing task performance.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual attention</kwd><kwd>convolutional neural network</kwd><kwd>CNN</kwd><kwd>receptive field</kwd><kwd>spatial attention</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001906</institution-id><institution>Washington Research Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Birman</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001818</institution-id><institution>Research to Prevent Blindness</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gardner</surname><given-names>Justin L</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011238</institution-id><institution>Lions Clubs International Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gardner</surname><given-names>Justin L</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Hellman Fellows Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gardner</surname><given-names>Justin L</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>T32EY07031</award-id><principal-award-recipient><name><surname>Birman</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Simple modifications to early stages of the visual hierarchy, such as gain changes, can induce complex effects on later stages, but only gain is both necessary and sufficient to explain enhanced perception during spatial attention.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Deploying goal-directed spatial attention towards visual locations allows observers to detect targets with higher accuracy (<xref ref-type="bibr" rid="bib29">Hawkins et al., 1990</xref>), faster reaction times (<xref ref-type="bibr" rid="bib54">Posner, 1980</xref>), and higher sensitivity (<xref ref-type="bibr" rid="bib58">Sagi and Julesz, 1986</xref>) providing humans and non-human primates with a mechanism to select and prioritize spatial visual information (<xref ref-type="bibr" rid="bib11">Carrasco, 2011</xref>). These enhanced behavioral responses are accompanied by both an increase in the gain of sensory responses near attended locations (<xref ref-type="bibr" rid="bib17">Connor et al., 1996</xref>; <xref ref-type="bibr" rid="bib42">McAdams and Maunsell, 1999</xref>) and changes in the shape and size of receptive fields, typically shrinking and shifting towards the target of attention (<xref ref-type="bibr" rid="bib5">Ben Hamed et al., 2002</xref>; <xref ref-type="bibr" rid="bib71">Womelsdorf et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>; <xref ref-type="bibr" rid="bib35">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">van Es et al., 2018</xref>). These changes in neural representation are thought to contribute to behavioral enhancement, but because both gain and changes in spatial properties co-occur in biological systems, it is not possible to disentangle them. Computational models of the visual system allow us to design experiments to independently examine the effects of such changes (<xref ref-type="bibr" rid="bib40">Lindsay and Miller, 2018</xref>; <xref ref-type="bibr" rid="bib20">Eckstein et al., 2000</xref>).</p><p>Shrinkage and shift of receptive fields toward attended targets has been observed in both single unit (<xref ref-type="bibr" rid="bib71">Womelsdorf et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>) and population (<xref ref-type="bibr" rid="bib35">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Fischer and Whitney, 2009</xref>; <xref ref-type="bibr" rid="bib67">van Es et al., 2018</xref>) activity, and has been suggested to lead to behavioral enhancement through a variety of possible mechanisms (<xref ref-type="bibr" rid="bib4">Anton-Erxleben and Carrasco, 2013</xref>). For example, receptive field changes might magnify the cortical representation of attended regions (<xref ref-type="bibr" rid="bib46">Moran and Desimone, 1985</xref>), select for relevant information (<xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>; <xref ref-type="bibr" rid="bib62">Sprague and Serences, 2013</xref>), reduce uncertainty about spatial position (<xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>), increase spatial discriminability (<xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Fischer and Whitney, 2009</xref>), or change estimates of perceptual size (<xref ref-type="bibr" rid="bib2">Anton-Erxleben et al., 2007</xref>). Compression of visual space is also observed just prior to saccades and thought to shift receptive fields towards the saccade location (<xref ref-type="bibr" rid="bib73">Zirnsak et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Colby and Goldberg, 1999</xref>; <xref ref-type="bibr" rid="bib44">Merriam et al., 2007</xref>) and maintain a stable representation of visual space (<xref ref-type="bibr" rid="bib38">Kusunoki and Goldberg, 2003</xref>; <xref ref-type="bibr" rid="bib65">Tolias et al., 2001</xref>; <xref ref-type="bibr" rid="bib56">Ross et al., 1997</xref>; <xref ref-type="bibr" rid="bib19">Duhamel et al., 1992</xref>).</p><p>Shrinkage and shift of receptive fields has also been hypothesized to occur as a side effect of increasing gain of neural responses (<xref ref-type="bibr" rid="bib35">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Compte and Wang, 2006</xref>), thus raising the question of which of these physiological effects could be responsible for enhanced perception. When gain is asymmetric across a receptive field, the overall effect will be to shift the receptive field location towards the side with the largest gain. Similarly, asymmetric gain can be expected to change spatial tuning properties such as the size and structure of the receptive field. These concomitant changes of receptive field size, location, and structure could improve perceptual performance through the mechanisms described above, or could be an epiphenomenological consequence of increasing gain. Increasing gain by itself has also been hypothesized to be a mechanism for improving perceptual performance, because response gain can increase the signal-to-noise ratio and make responses to different stimuli more discriminable (<xref ref-type="bibr" rid="bib42">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib14">Cohen and Newsome, 2008</xref>). Moreover, larger responses for attended stimuli can act to select relevant information when read-out through winner-take-all mechanisms (<xref ref-type="bibr" rid="bib39">Lee et al., 1999</xref>; <xref ref-type="bibr" rid="bib52">Pelli, 1985</xref>; <xref ref-type="bibr" rid="bib53">Pestilli et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Palmer et al., 2000</xref>; <xref ref-type="bibr" rid="bib28">Hara et al., 2014</xref>).</p><p>We took a modeling approach to ask what effects gain changes incur on spatial receptive field structure when introduced at the earliest stage of visual processing and to ask which effects would improve behavioral performance. We modified a convolutional neural network (CNN) trained on ImageNet categorization to test various hypotheses by implementing them as elements of the model architecture. CNN architectures can be designed to closely mimic the primate visual hierarchy (<xref ref-type="bibr" rid="bib72">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref>). Training ‘units’ in these networks to categorize images leads to visual filters that show a striking qualitative resemblance to the filters observed in early visual cortex (<xref ref-type="bibr" rid="bib36">Krizhevsky et al., 2012</xref>) and the pattern of activity of these units when presented with natural images is sufficient to capture a large portion of the variance in neural activity in the retina (<xref ref-type="bibr" rid="bib43">McIntosh et al., 2016</xref>), in early visual cortex (<xref ref-type="bibr" rid="bib8">Cadena et al., 2019</xref>), and in later areas (<xref ref-type="bibr" rid="bib27">Güçlü and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="bib13">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="bib21">Eickenberg et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib72">Yamins et al., 2014</xref>). Cortical responses and neural network activity also share a correlation structure across natural image categories (<xref ref-type="bibr" rid="bib63">Storrs et al., 2020</xref>). These properties of CNNs make them a useful tool which we can use to indirectly study visual cortex, probing activity and behavior in ways that are impractical in humans and non-human primates (<xref ref-type="bibr" rid="bib40">Lindsay and Miller, 2018</xref>).</p><p>Using simulations based on a CNN observer model we found that gain changes introduced at the earliest stage in visual processing improved task performance with a magnitude comparable to that measured in human subjects. While these gain changes also induced changes in receptive field location, size, and spatial structure similar to that reported in physiological measurements, these changes were neither necessary nor sufficient for improving model task performance. More specifically, we designed a simple cued object-detection task and measured improved human performance on trials with focal attention. Using CORnet-Z (<xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref>), a CNN whose architecture was designed to maximize similarity with the primate visual stream, we measured a similar improvement in detection performance when a Gaussian gain augmented inputs coming from a ‘cued’ location. We found that the network mirrored the physiology of human and non-human primates: units shifted their center-of-mass toward the locus of attention and shrank in size, all in a gain-dependent manner. We isolated each of these physiological changes to determine which, if any, could account for the benefits to performance. A model with only gain reproduced the benefits of cued attention while models with only receptive field shifts, shrinkage, or only changes in receptive field structure were unable to provide any benefit to task performance. These results held for both an object detection task and a category discrimination task. Gain applied or removed at the last stage of processing in the CNN observer model demonstrated that gain was both necessary and sufficient to account for the benefits in task performance of the model.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We characterized the ability of human observers to detect objects in a grid of four images, with or without prior information about the object’s possible location (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Observers were given a written category label, for example ‘ferris wheel’, and shown five exemplar images of that category (Category intro, <xref ref-type="fig" rid="fig1">Figure 1a</xref>). This was followed by a block of 80 trials in which observers tried to detect the presence or absence of the target category among the four images in the grid (Each trial, <xref ref-type="fig" rid="fig1">Figure 1a</xref>). Half of the 80 trials had focal cues and 50% of the focal (and distributed) trials included a target image. On focal trials, a cue indicated with 100% validity the grid quadrant that could contain a target while on distributed trials no information was given as to where an image of the target category could appear. Distractor images were randomly sampled from the nineteen non-target image categories. Stimulus durations were sampled uniformly from 1 (8.3 ms), 2 (16.7), 4 (33.3), 8 (66.7), 16 (133.3), or 32 (267.7) frames (Stimulus, 8.3ms per frame, <xref ref-type="fig" rid="fig1">Figure 1a</xref>). Image grids were masked before and after stimulus presentation by shuffling the pixel locations in the stimulus images, ensuring that the luminance during each trial remained constant. Observers had 2 s to make a response and each trial was followed by a 0.25 s inter-trial interval. Observers completed one training block on an unused category prior to data collection.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Cued object detection task.</title><p>(<bold>a</bold>) Observers were asked to perform object detection with or without a spatial cue. At the start of a block, observers were shown five examples of the target category. This was followed by 80 trials: 40 with a spatial cue indicating the possible target quadrant and 40 with no prior information. Stimulus presentation was pre and post-masked. The stimuli consisted of a composite grid of four individual object exemplars. The target category was present in 50% of trials and always in the cued location on focal trials. Human observers used a keyboard to make a fast button response to indicate the target presence before moving on to the next trial. (<bold>b</bold>) Human observers showed a substantial improvement in performance when given a focal cue indicating the quadrant at which the target might appear. Vertical line at 64ms indicates the duration at which the best-fit <inline-formula><mml:math id="inf1"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> curve for the Distributed condition matched the CNN observer model performance without gain. Markers indicate the median and error bars the 95% confidence intervals (n=7 observers).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig1-v2.tif"/></fig><p>Human observers improved their performance on this detection task when given a focal cue indicating the potential location of a target (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). We quantified human performance by computing sensitivity, d’, as a function of stimulus duration separately for focal and distributed conditions. Across all observers the <inline-formula><mml:math id="inf2"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> function was best fit as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>163.6</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf3"><mml:mi>α</mml:mi></mml:math></inline-formula> scaled the function for the focal condition. At a stimulus duration of 8.3ms (one frame) observers were near chance performance regardless of cueing condition. On distributed trials observers exceeded threshold performance (<inline-formula><mml:math id="inf4"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) at a stimulus duration of 155ms, 95% CI [135, 197]. For focal trials, the same threshold was reached with only a 38ms [32, 43] stimulus duration, demonstrating a substantial performance benefit of the focal cue. We found that <inline-formula><mml:math id="inf5"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> in the focal condition was higher than in the distributed condition, average increase across observers <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1.67</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> [1.57, 1.74].</p><p>Using a drift diffusion model, we found that the majority of this performance benefit came from improved perceptual sensitivity, rather than speed-accuracy trade off. We assessed this by fitting a drift diffusion model to the reaction time and choice data (<xref ref-type="bibr" rid="bib69">Wagenmakers et al., 2007</xref>). Drift diffusion models assume that responses are generated by a diffusion process in which evidence accumulates over time toward a bound. We used the equations in <xref ref-type="bibr" rid="bib69">Wagenmakers et al., 2007</xref> to transform each observer’s percent correct, mean reaction time, and reaction time variance for the twenty categories and two focal conditions into drift rate, bound separation, and non-decision time. The drift rate parameter is designed to isolate the effect of external input, the non-decision time reflects an internal delay before stimulus processing begins, and the bound separation is a proxy for how conservative observers are. Comparing the drift rate parameter we observed a similar effect to what was described above for <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>: the average drift rate across observers in the focal condition was 1.61×, 95% CI [1.39, 1.77] the drift rate in the distributed condition. This suggests that the majority of the performance gain observed in the <inline-formula><mml:math id="inf8"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> parameter came from increased stimulus information. We did find that the other parameters of the drift diffusion model were also sensitive to duration and condition, but in opposite directions. We found larger bound separation at longer stimulus durations and on focal trials (focal bound-separation 1.57× distributed [1.37, 1.75]), consistent with observers being more conservative on trials where more information was available. But this increase in cautiousness was offset by a shorter non-decision time on focal trials (0.26 s) compared to distributed (0.38, [0.34, 0.41]).</p><p>Having shown that a spatial cue provides human observers with increased stimulus information in this task, we next sought to show that a neural network model of the human visual stream could replicate this behavior under similar conditions. We used a convolutional neural network (CNN) model, CORnet-Z (<xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref>), a neural network designed to mimic primate V1, V2, V4, and IT and optimized to perform object recognition for images at a similar scale to our task. CORnet-Z is a four-layer CNN with repeated convolutional, rectified linear units (ReLU), and pooling (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). We used pretrained weights which were optimized for object categorization on ImageNet (<xref ref-type="bibr" rid="bib18">Deng et al., 2009</xref>). To perform our category detection task, we added a fully connected output layer for each category and trained the weights of that layer to predict the presence of the twenty object categories selected for this study, thus creating a neural network observer model, that is a model designed to idealize the computations human observers perform in the four-quadrant object detection task. We applied the observer model to a task analogous to the one human observers performed (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). The prediction layers added to the end of the model provided independent readouts for the presence or absence of the different target categories (Linear classifier, <xref ref-type="fig" rid="fig2">Figure 2c</xref>). These output layers were trained on a held out set of full-size images from each category. On a separate held out validation set of 100 images, the trained prediction layers achieved a median AUC of 0.90, range [0.77, 0.96].</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural network observer model.</title><p>(<bold>a</bold>) Using a Gaussian gain the neural network observer was able to replicate the benefit of spatial attention for human observers. Human performance is shown at a stimulus duration of 64ms which provided the closest match to the convolutional neural network (CNN) performance without gain. Black markers indicate the median by category and error bars the 95% confidence intervals (n=20 categories). (<bold>b</bold>) The Gaussian gain was implemented by varying the maximum strength of a multiplicative gain map applied to the ‘cued’ quadrant. (<bold>c</bold>) The gain was applied prior to the first layer of the CNN. The neural network observer model consisted of a four layer CNN with linear classifiers applied to the output layer. Individual classifiers were trained on examples of each object category. (<bold>d</bold>) Each of the four convolutional layers consisted of a convolution operation, a rectified linear unit, and max pooling. Unit activations were measured at the output of each layer.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig2-v2.tif"/></fig><p>To examine the computational mechanisms that could underlie the performance benefit of focal cues we added a multiplicative Gaussian gain centered at the location of the cued image (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, Gaussian width 56 px). We applied this gain at the first layer of the model and tested various strengths of gain.</p><p>To align the human and model performance we took the performance of the model in the distributed condition (Distributed, <xref ref-type="fig" rid="fig2">Figure 2a</xref>) and found the stimulus duration at which observers in the distributed condition of the human data matched this performance level (64ms, <xref ref-type="fig" rid="fig1">Figure 1b</xref>). We then scaled up the amplitude of the Gaussian gain incrementally and found that we could mimic the performance enhancement of human spatial attention by setting the maximum of the Gaussian gain field to approximately <inline-formula><mml:math id="inf9"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula>. The model with this level of gain had a median AUC across categories of 0.80, 95% CI [0.77, 0.82] compared to 0.71 [0.67, 0.72] without gain and a median AUC improvement of 0.09 [0.08, 0.12] within each category.</p><p>The gain strengths necessary to induce an increase in task performance in the neural network observer model were relatively large compared to the gain due to directed attention observed in measurements of single unit (<xref ref-type="bibr" rid="bib41">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib66">Treue and Martínez Trujillo, 1999</xref>) and population (<xref ref-type="bibr" rid="bib6">Birman and Gardner, 2019</xref>) activity. We attribute this difference to the lack of any non-linear “winner-take-all” type of activation in the CNN. In the primate visual system, it is thought that non-linearities such as exponentiation and normalization can accentuate response differences (<xref ref-type="bibr" rid="bib55">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib9">Carandini and Heeger, 2012</xref>) and act as a selection mechanism for sensory signals (<xref ref-type="bibr" rid="bib53">Pestilli et al., 2011</xref>). We tested whether similar non-linear mechanisms would allow for smaller gain strengths to be amplified to the range needed by our model. This was tested by raising the activations of units to an exponent before re-normalizing the activation of all units at the output of each layer (see Materials and methods for details). This has the effect of amplifying active units and further suppressing inactive ones. Using this approach, we found that a relatively small gain of 1.1× combined with an exponent of 3.8 led to a significantly larger effective gain of 1.37× after just one layer (<xref ref-type="fig" rid="fig3">Figure 3j</xref>). This form of non-linearity is consistent with the finding that static output non-linearities in single units range from about 2–4 (<xref ref-type="bibr" rid="bib24">Gardner et al., 1999</xref>; <xref ref-type="bibr" rid="bib1">Albrecht and Hamilton, 1982</xref>; <xref ref-type="bibr" rid="bib61">Sclar et al., 1990</xref>; <xref ref-type="bibr" rid="bib30">Heeger, 1992</xref>) and suggests a plausible physiological mechanism by which the larger gains predicted by our model could be implemented. Repeated use of exponentiation and normalization in successive layers of the visual system could produce an even larger effective gain. To avoid training a new convolutional neural network (CNN) and possibly violate the close relationship between the primate visual system and the CNN we studied, we continued our analysis without introducing an exponentiation and normalization step.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Effects of Gaussian gain on neural network units.</title><p>(<bold>a</bold>) The Gaussian gain applied to Layer 1 units caused the measured receptive field (RF) of units in Layer 4 to shift (black ellipse, original; brown ellipse, with gain) toward the locus of attention (black ×). (<bold>b</bold>) A 2D spatial map demonstrates the effects of Gaussian gain in Layer 4: shift of RF center position (black arrows), shrinking RF size near the attended locus (blue colors) and an expansion of size near the gain boundaries (red colors). (<bold>c</bold>) <inline-formula><mml:math id="inf10"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> map of the output layer before averaging, showing the change in AUC caused by the addition of Gaussian gain. Each pixel’s ΔAUC is computed by projecting the activations at that location for composite grids with target present and absent on the decision axis and then calculating the difference in AUC between a model with and without Gaussian gain. The map demonstrates that units overlapping the borders of the composite grid have the largest change in information content when Gaussian gain is applied. (<bold>d,e</bold>) Scatter plots demonstrate that each layer magnifies the effect of the gain on RF shift and RF size. The RF shift percentages are the ratio of pixel shift at the peak of the curve relative to the average receptive field size, measured as the full-width at half-maximum. (<bold>f</bold>) Later layers do not magnify the effective gain (shown for an 11× gain), which stays constant across layers. (<bold>g</bold>) Gain strength influences the size of RF position shifts, RF size (<bold>h</bold>), and effective gain (<bold>i</bold>). (<bold>j</bold>) Adding an additional non-linear normalizing exponent at the output of each layer allows for much smaller gains to be magnified across layers. Markers in all panels indicate individual sampled units from the model. Lines show the LOESS fit for visualization.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig3-v2.tif"/></fig><p>The Gaussian gain could have its effect on the neural network observer model’s performance by increasing the activation strength of units with receptive fields near the locus of attention. These changes in activation strength might directly modify behavior, or work indirectly through mechanisms such as changes in receptive field size, location, or spatial tuning. We observed all of these effects in our model (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To measure receptive fields we computed the derivative of each unit with respect to the input image and then fit these with a 2D Gaussian (see Materials and methods for details). We found that the gain caused receptive fields to shift and shrink toward the locus of attention (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>). The information provided by individual units in the model also changed, increasing for units on the border of the cued quadrant (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). The receptive field shift and shrinkage were magnified in deeper layers of the model (<xref ref-type="fig" rid="fig3">Figure 3d and e</xref>) consistent with physiological observations (<xref ref-type="bibr" rid="bib35">Klein et al., 2014</xref>). The gain in activation strength propagated through the network without modification (<xref ref-type="fig" rid="fig3">Figure 3f</xref>). To measure the effective gain experienced by the layer four units (<xref ref-type="fig" rid="fig3">Figure 3i</xref>), we computed the ratio of the standard deviations of unit activations at the output of each layer (<xref ref-type="fig" rid="fig2">Figure 2d</xref>) with and without gain applied. All three observed effects: receptive field shift, shrinkage and expansion, and effective gain were directly related to the gain strength at the input layer (<xref ref-type="fig" rid="fig3">Figure 3g–i</xref>). All of these changes have been proposed as mechanisms that could account for the behavioral benefits of attention (<xref ref-type="bibr" rid="bib4">Anton-Erxleben and Carrasco, 2013</xref>; <xref ref-type="bibr" rid="bib46">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>; <xref ref-type="bibr" rid="bib62">Sprague and Serences, 2013</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Fischer and Whitney, 2009</xref>; <xref ref-type="bibr" rid="bib2">Anton-Erxleben et al., 2007</xref>). We designed models to try to isolate these effects with the goal of testing their independent contributions to task performance.</p><p>We next sought to test whether receptive field shifts alone could account for the behavioral benefits of the neural network observer model. To do this, we built a model variant that could shift receptive fields without introducing gain. To develop an intuition for how this could affect perceptual reports, consider a CNN with just four units in a 2 × 2 grid with each unit having its receptive field centered on one image in the composite. When shown a composite grid of four images, a logistic regression using the output of these four units would receive one quarter the information it expects from being trained on full size images. Shifting the receptive fields of the three non-target units to overlap more with the cued image could add additional task-relevant information to the output, much as was observed for units with receptive fields overlapping multiple images in the Gaussian gain attention model (<xref ref-type="fig" rid="fig3">Figure 3c</xref>).</p><p>We designed a variant of our model that could be used to test the hypothesis that receptive field shifts alone are responsible for the behavioral enhancement (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In this model, we re-wired the units in the first layer to reproduce the effect of Gaussian gain. The re-wiring was designed so that receptive fields in the fourth layer matched their shift with the Gaussian gain model (<xref ref-type="fig" rid="fig3">Figure 3g</xref>). To mimic those shifts, we changed the connections between the input image pixels and layer one (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). This manipulation worked as designed and changed the receptive field locations and size (<xref ref-type="fig" rid="fig4">Figure 4b–d</xref>) but since no gain was added to the model, the overall responsiveness of units remained constant (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). Because receptive field shifts due to gain are not the result of actual rewiring it is unsurprising that the shift and shrinkage in this model variant are only qualitatively matched to those caused by the original Gaussian gain. Note that the effective gain of individual units in layer four <italic>did</italic> change for individual images, a result of each unit receiving different inputs, but the average change across images was zero.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Receptive field shift model.</title><p>(<bold>a</bold>) To mimic the effects of the Gaussian gain on receptive field position without inducing gain in the model we re-assigned the inputs to units in Layer 1. This re-assignment was performed so that the pattern of receptive field shift in Layer 4 would match what was observed when the Gaussian gain was applied. (<bold>b</bold>) The observed pattern of receptive field shifts and shrinkage is shown for a sample of units in layer 4, qualitatively matching the effects of the Gaussian gain. (<bold>c</bold>) RF shift is shown for sampled units (markers) and the LOESS fit (solid lines) compared to the effect in the Gaussian gain model (dotted lines). (<bold>d</bold>) Conventions as in c for the RF size change. (<bold>e</bold>) Conventions as in (<bold>c,d</bold>) for the effective gain of units. (<bold>f</bold>) The behavioral effect of shifting receptive fields is shown to be null on average across categories when compared to the effect of Gaussian gain. Large markers indicate the median performance, small markers the individual categories (n=20), and error bars the 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig4-v2.tif"/></fig><p>We found that the model with receptive field shifts but no gain had no effect on task performance, demonstrating that receptive field shifts are not key for the improvement in task performance observed with Gaussian gain (<xref ref-type="fig" rid="fig4">Figure 4f</xref>). The model imitating shifts from <inline-formula><mml:math id="inf11"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> Gaussian gain had a median AUC across categories of 0.71, 95% CI [0.66, 0.73] compared to 0.71 [0.67, 0.72] in the baseline model, median change in AUC of –0.01 [-0.02, 0.01] within each category.</p><p>Another way to understand the possible effect of the Gaussian gain on task performance is to note that the spatial tuning profile of units is ‘shifted’ towards the locus of attention: sensitivity is enhanced closer to the locus of attention, but the receptive field itself has not truly moved in the manner studied by the previous model. If different parts of a receptive field receive asymmetric gain, as expected for Gaussian gain then the local structure of the receptive field has been changed (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). We designed another model variant to test the hypothesis that these local changes in receptive field structure might be sufficient to explain changes in task performance without inducing receptive field shifts or gain. To implement this model at layer <inline-formula><mml:math id="inf12"><mml:mi>L</mml:mi></mml:math></inline-formula>, we examined the effect of the Gaussian gain on each unit (green differential gain, <xref ref-type="fig" rid="fig5">Figure 5a</xref>). We normalized this differential gain within each unit’s receptive field to prevent any overall gain effect and re-scaled the unit’s kernel accordingly. Overall, this manipulation of each unit’s kernel preserved a portion of the receptive field shift effect in a gain-dependent manner but guaranteed that there was no effective gain.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Receptive field structure model.</title><p>(<bold>a</bold>) We adjusted the kernels of each convolutional neural network (CNN) unit according to the effect of a Gaussian gain, subtly shifting the the sensitivity within individual units. To avoid inducing a gain change, we then normalized each units output such that the sum-of-squares of the weights was held constant, ensuring the local gain at that unit remained at 1×. This model was implemented individually at each layer, replicating the effect of a Gaussian gain of 1.1×–11× as well as at all layers at once. (<bold>b–f</bold>) conventions as in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig5-v2.tif"/></fig><p>The receptive field structure model was designed to only change the spatial tuning of individual units without inducing gain, which naturally caused some shifts in the measured receptive field size and location (solid lines and markers, <xref ref-type="fig" rid="fig5">Figure 5b–d</xref>) but these were smaller than the effects observed under Gaussian gain (dashed lines). The normalization prevented the model from introducing any spatial pattern of gain change (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). Note that there were still small changes in overall sensitivity of units in this model, for example, the <inline-formula><mml:math id="inf13"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> model had an average gain of 1.08, 95% CI [1.07, 1.09] across all units, which we attribute to the fact that inputs to a unit may exhibit correlations due to spatial structure. These receptive field changes and small gain effects were distinct from those observed under Gaussian gain (<xref ref-type="fig" rid="fig5">Figure 5c–e</xref>).</p><p>The receptive field structure model, like the shift model, was unable to account for the behavioral effects of the Gaussian gain. No matter where in the model we changed the receptive field structure, and even when applied at all layers, the average performance across categories remained flat (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). Compared to the median distributed AUC across categories of 0.71 [0.67, 0.72], the sensitivity model applied to all layers had a median AUC across categories of 0.69 [0.65, 0.72] when imitating gain of <inline-formula><mml:math id="inf14"><mml:mrow><mml:mn>1.1</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula>, 0.70 [0.65, 0.72] for <inline-formula><mml:math id="inf15"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> gain, 0.69 [0.65, 0.71] for <inline-formula><mml:math id="inf16"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> and 0.66 [0.63, 0.69] for <inline-formula><mml:math id="inf17"><mml:mrow><mml:mn>11</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula>. Each of these conditions resulted in a median AUC change within category of –0.02 [-0.03, 0.00], –0.01 [-0.03, 0.00], -0.02 [-0.04,–0.01], and -0.04 [-0.05,–0.03], respectively. When applied to early layers, we observed a slight drop in performance, which we attribute to how this model directly alters the kernels in the CNN. These changes break the assumption that the CNN kernels at each layer are consistent with those that were optimized when the model weights were trained.</p><p>The Gaussian gain also caused units to shrink and expand their receptive fields across the visual field (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). These changes might modify the information content received at the output layer, improving or hurting performance. We designed a model variant to test the hypothesis that shrinkage and expansion of receptive fields, without shift or gain, might be sufficient to explain the behavioral effect (<xref ref-type="fig" rid="fig6">Figure 6</xref>). To implement this model, we took the observed change in receptive field size at layer 4 and then re-scaled the connections between layers three and four to mimic the observed effect. Because the kernels were scaled in space this manipulation has no effect on effective gain or receptive field position. Specifically, we approximated the shrinkage of the sampled units using a parameterized equation (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>) that provides a shrinkage factor for every unit in the model (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). We then re-wired the connections between layer three and four using linear interpolation to approximate the necessary change in scaling.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Shrinkage model.</title><p>(<bold>a</bold>) To create shrinkage at layer 4 matched with the effects observed under Gaussian gain we re-assigned the connections between layers 3 and 4 according to a parameterized approximation of the shrinkage effect as a function of distance from the locus of attention. This re-scaling of connections changed the size of receptive fields without moving them in space or modifying their gain. (<bold>b–f</bold>) conventions as in previous figures.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig6-v2.tif"/></fig><p>After re-wiring, units’ receptive fields retained the same overall position, but were scaled to qualitatively match the observed effects under Gaussian gain (<xref ref-type="fig" rid="fig6">Figure 6b–d</xref>). The size changes do not match perfectly with those under Gaussian gain because we enforced symmetry in two ways: first, by parameterizing the shrinkage and expansion we enforced symmetry around the locus of attention, and second, because the observed receptive field changes were often asymmetric but we implemented a symmetric linear scaling. These simplifications were necessary to reduce the complexity of implementation. By design, the shrinkage and expansion effects scaled by attention strength (<xref ref-type="fig" rid="fig6">Figure 6d</xref>) and induced neither gain-dependent shifts (<xref ref-type="fig" rid="fig6">Figure 6c</xref>) or effective gain (<xref ref-type="fig" rid="fig6">Figure 6e</xref>).</p><p>The shrinkage model was unable to account for improved task performance with Gaussian gain. The average performance across categories remained flat (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). Compared to the median distributed AUC across categories of 0.71 [0.67, 0.72], the shrinkage model applied to all layers had a median AUC across categories of 0.70 [0.66, 0.72] when imitating gain of <inline-formula><mml:math id="inf18"><mml:mrow><mml:mn>1.1</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula>, 0.70 [0.66, 0.71] for <inline-formula><mml:math id="inf19"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> gain, 0.69 [0.65, 0.70] for <inline-formula><mml:math id="inf20"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> and 0.68 [0.65, 0.70] for <inline-formula><mml:math id="inf21"><mml:mrow><mml:mn>11</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula>. Each of these conditions resulted in a median AUC change within category of -0.01 [-0.01,–0.00], -0.01 [-0.01,–0.01], -0.02 [-0.02,–0.01], and –0.02 [–0.03, –0.01], respectively. We again observed drops in performance, which we attribute to how the kernels have been altered.</p><p>Having ruled out that receptive field shift, shrinkage, or changes in spatial tuning could account for the improved task performance in our neural network observer, we next designed a model to amplify signals in the cued quadrant without these other effects and found that this model was able to explain the improved task performance observed with cued attention. In the original Gaussian gain model, an asymmetry in gain was introduced in the receptive fields of the units, causing size and location changes in the receptive fields. To remove this effect, we flattened the gain within the cued quadrant (<xref ref-type="fig" rid="fig7">Figure 7a</xref>) by setting the gain at each pixel to the average of the Gaussian gain across the entire quadrant. By itself, this change has the unintended consequence that units partly overlapping the cued quadrant will still shift in a gain-dependent manner. To remove this effect, we split the CNN feature maps into the four quadrants and computed these separately with padding and concatenated the results. This forces all units in the model to receive information about only a single quadrant. These manipulations did result in shifts in receptive field location and size for units at the borders (<xref ref-type="fig" rid="fig7">Figure 7b–d</xref>), but by design these were independent of the gain strength.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Gain-only model.</title><p>(<bold>a</bold>) To create a gain effect without modifying the receptive fields of units, we applied a flattened gain field, with the gain set to the average of the original Gaussian gain for each attention strength. The flat gain alone causes units to shift their receptive field at the boundary between the four stimulus quadrants. To modify gain while ensuring shifts were gain-independent, we computed the four quadrants separately with zero padding and then concatenated the results. (<bold>b–f</bold>) Conventions as in previous figures.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig7-v2.tif"/></fig><p>Using the gain-only model we were able to reproduce the improved task performance of the original Gaussian gain (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The gain-only model induced the same pattern of receptive field shift and size change at all gain strengths (<xref ref-type="fig" rid="fig7">Figure 7b–d</xref>) and a flat effective gain within the cued quadrant (<xref ref-type="fig" rid="fig7">Figure 7e</xref>). We found that increasing the strength of a flat gain was sufficient to capture the full performance improvement of the original model (<xref ref-type="fig" rid="fig7">Figure 7f</xref>). The median AUC across categories of the <inline-formula><mml:math id="inf22"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> flat gain model was 0.78, 95% CI [0.76, 0.83] compared to 0.80 [0.77, 0.82] for the <inline-formula><mml:math id="inf23"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> Gaussian gain model. The confidence intervals in flat gain and Gaussian gain performance overlapped at all gain strengths, with a difference of 0.00 [-0.00, 0.02] at 1.1× gain, –0.01 [-0.02, 0.00] at 2× gain, –0.01 [-0.02, 0.00] at 4× gain, and 0.02 [0.00, 0.04] at 11× gain.</p><p>Having found that the improved task performance could be explained not by receptive field changes, but instead by the change in the overall gain, we asked whether gain propagated through the network was both necessary and sufficient to explain this effect. To test necessity and sufficiency, we ran the task images through the Gaussian gain model (first row, <xref ref-type="fig" rid="fig8">Figure 8a</xref>) and measured the effective gain propagated to units in the final layer output (7 × 7 × 512, before averaging). We averaged these effective gains over features to obtain a propagated gain map (Layer 4 feature map, 7 × 7, <xref ref-type="fig" rid="fig8">Figure 8b</xref>). To test the hypothesis that this propagated gain was sufficient to account for improved performance in the task, we re-applied it to the output layer of a model with no gain applied to the inputs.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Gain is both necessary and sufficient to explain the improved task performance due to cued attention.</title><p>(<bold>a</bold>) To test necessity and sufficiency of gain on performance, we propagated the effect of Gaussian gain through the model and measured the effective gain at the output layer. (<bold>b</bold>) We averaged the effective gain across features to obtain a ‘propagated gain map’. To test sufficiency, we multiplied the output of a model with no true gain by the propagated gain map. To test necessity, we divided the output of a model with true gain by the propagated gain map. (<bold>c</bold>) Multiplying the output by the propagated gain recovered the effect of Gaussian gain, while dividing removed this effect, confirming that gain was both necessary and sufficient to account for the change in task performance. Grey markers show the individual category performance, black markers the median across categories (n=20) and error bars the 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Direct readout from the cued quadrant improves performance alone, with no additional improvement from gain.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Gain propagation can account for changes in discrimination task performance due to Gaussian gain.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-78392-fig8-figsupp2-v2.tif"/></fig></fig-group><p>We found that the propagated gain map, when used to multiply the outputs of a model with no Gaussian gain (Multiply by propagated gain, <xref ref-type="fig" rid="fig8">Figure 8a</xref>) was sufficient to induce task performance benefits similar to Gaussian gain applied to the input (Propagated gain vs. Gaussian gain, <xref ref-type="fig" rid="fig8">Figure 8c</xref>). The median AUC across categories using the propagated gain map was 0.79, 95% CI [0.76, 0.84], compared to 0.71 [0.67, 0.72] in the distributed model. There was a small difference between the Gaussian gain and the effect of the propagated gain map -0.02 [–0.03, 0.01], within the 95% confidence interval for no difference. This difference could be attributed to changes in receptive field structure in the Gaussian gain condition, but we attribute it instead to differences between the propagated gain map and the effect of the Gaussian gain. The propagated gain manipulation was constructed from the average effective gain of units across all task stimuli. Because of this, the gain map did not exactly reproduce the effect of gain on an image-by-image basis.</p><p>To test the hypothesis that gain was necessary to account for the behavioral effect, we divided the final layer activations by the propagated gain map (Divide by propagated gain, <xref ref-type="fig" rid="fig8">Figure 8a</xref>). We found that the behavioral effect of an early gain was mostly reversed by this manipulation (Removed gain vs. Distributed, <xref ref-type="fig" rid="fig8">Figure 8c</xref>). The median AUC across categories after dividing out the propagated gain was 0.72, 95% CI [0.68, 0.75], compared to 0.71 [0.67, 0.72] in the distributed condition. Dividing by the propagated map did not perfectly reverse the effects of the full Gaussian gain. When compared to the distributed baseline, we found a median within category AUC advantage for Gaussian gain with division of 0.02 [0.01, 0.03]. These small residual differences are likely due to the combined effects of changes in spatial receptive field properties that are not reversed by the division of the propagated gain map.</p><p>Note that in the final readout of our model, we assumed that explicit spatial information was lost, as we averaged activations across the <inline-formula><mml:math id="inf24"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> convolutional units in the final pooling layer. However, some evidence in ventral temporal cortex suggests that there is spatial information available (<xref ref-type="bibr" rid="bib60">Schwarzlose et al., 2008</xref>; <xref ref-type="bibr" rid="bib10">Carlson et al., 2011</xref>), so we tested a model read out which retained spatial information and found that the necessary and sufficiency results did not show qualitative changes. A model trained to use the full <inline-formula><mml:math id="inf25"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> output had marginally worse performance than the model built with the average encodings, achieving a median AUC across categories of 0.68 [0.63, 0.71] in the distributed condition and 0.78 [0.75, 0.82] in the focal condition with <inline-formula><mml:math id="inf26"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> Gaussian gain at the first layer. We attribute the small difference in task performance compared to the average model to worse generalization: on the validation set the <inline-formula><mml:math id="inf27"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> model showed a median drop in AUC across categories of –0.02, range [–0.10, 0.00] compared to the average-pooled readouts.</p><p>We repeated the propagated gain manipulations in the <inline-formula><mml:math id="inf28"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> readout model to confirm the necessary and sufficiency results would not change when the model retained spatial information in the final readout. Both the necessity and sufficiency tests showed similar results when using the full output: the average increase in AUC when using the propagated gain map was 0.09, 95% CI [0.08, 0.10] for the full output model, compared to 0.09 [0.07, 0.10] for the average pooled model and the average change in AUC (compared to the distributed condition) when dividing out the propagated gain map from a model with Gaussian gain applied was 0.01 [-0.01, 0.02] for the full output model, compared to 0.02 [0.01, 0.03] for the average pooled model.</p><p>Improvements in task performance with a Gaussian gain could come from changes in signal discriminability, but also could come from the network being better able to suppress irrelevant visual information. That is, increasing the gain could act to strengthen signals from the relevant target and suppress signals from irrelevant locations. To see how much suppressing irrelevant visual information alone could improve task performance, we designed a neural network observer model which explicitly read out from the top-left <inline-formula><mml:math id="inf29"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> quadrant of the layer 4 output, instead of the average pooled <inline-formula><mml:math id="inf30"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> output. As expected, the task performance of this model with no additional gain is already elevated (Distributed, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>), because the readout now implicitly acts as a form of spatial cueing. The performance of the 4×4 readout was still not at ceiling (ΔAUC between training validation set and distributed images = –0.07, 95% CI [-0.06,–0.09]). Thus, the performance enhancement due to the Gaussian gain appears to act similarly to an explicit manipulation which suppresses irrelevant information.</p><p>Theoretical considerations would suggest that moving receptive fields into the target quadrant should further improve performance even when the readout is already spatially specific, because these additional receptive fields can add new information (<xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>). We found that this was not true for the amount of shift induced by the 4× Gaussian gain, chosen to match the magnitude of the human behavioral benefits of spatial attention. To demonstrate this, we applied a 4× Gaussian gain to the <inline-formula><mml:math id="inf31"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> readout model and found no further increase in performance beyond what was achieved by shifting the readout (Gaussian gain vs. 4 × 4 Readout, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). Gain applied to the output of the model also provided no additional benefit (Propagated Gain vs. 4 × 4 Readout, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>), supporting the interpretation that the gain acts as a selection mechanism with no effect in the absence of irrelevant distractors.</p><p>Finally, the observer model solved a detection task where both criterion and sensitivity contribute to performance and we reported task performance as AUC to avoid confounding these factors. A more explicit test is to use a criterion-free discrimination task to evaluate the effects of gain on task performance. (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2a</xref>). We therefore designed a category discrimination task in which the neural network observer model determined which of two composite grids included the target category at a specified location (always top-left). Baseline performance (Baseline, <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2b</xref>) was considered as the performance of the model when no information about which location is cued for discrimination was provided. Note that because the discrimination location always included the target and all other locations had equal probability of including the target category, chance performance was greater than 50%. To compute task performance, the previously trained fully connected category target readout was compared across the two composite grids and the composite with the larger response chosen as the model’s response. We then applied a Gaussian gain at the cued location and found that discrimination task performance improved in a similar manner to the detection task (Gaussian Gain, <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2b</xref>). Using the propagated gain manipulation, we confirmed that the gain was both necessary and sufficient for improvements in model task performance (Removed Gain vs. Baseline and Propagated Gain vs. Gaussian Gain, <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2b</xref>).</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Human observers are more accurate when trying to detect or discriminate objects at a cued location. Our results demonstrate that this behavioral benefit can also be observed in a neural network model of visual cortex when a Gaussian gain is applied over the pixels of a ‘cued’ object. By modeling attentional modulation as gain at the earliest stage of the neural network, we were able to observe similar effects on spatial receptive fields to what is seen in human physiology. When using a gain strength set to match model and human performance enhancement during spatial attention, we documented shifts of receptive fields towards the center of the Gaussian gain field, shrinkage of receptive fields, and changes to the spatial structure in units at later stages of the model. These changes in model receptive field properties were similar in magnitude and characteristics to changes in single-unit (<xref ref-type="bibr" rid="bib71">Womelsdorf et al., 2006</xref>; <xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>) and population (<xref ref-type="bibr" rid="bib35">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Fischer and Whitney, 2009</xref>; <xref ref-type="bibr" rid="bib67">van Es et al., 2018</xref>) receptive fields reported from physiological measurements.</p><p>To determine which, if any, of these changes to receptive field properties were the source of improved task performance in the model, we built a series of neural network observer models in which we isolated receptive field shifts, shrinkage, and structural changes from the direct effect of gain. To assess these changes in a way that could provide information about the human visual system, we matched the scale of the shifts, shrinkage, and structural changes to the effect size observed in the Gaussian gain model with the gain strength best matched to human performance. In the shift-only model, we re-wired units to move receptive fields without introducing gain and found that this produced no improvements in task performance. In the shrinkage model, we changed the size of units without changing their gain or position, and again found no improvements in task performance. In the receptive field structure model, we modified the sensitivity profile of individual receptive fields to mimic the effects of gain, without changing their gain, position, or size, but again found no improvements in task performance. It was only by applying a gain while keeping receptive field properties stable that we were able to reproduce the improvements in task performance.</p><p>Our results suggest that spatial gain implemented by neural populations in visual cortex can be sufficient to induce behavioral effects of attention for both detection and discrimination even without the concomitant changes in downstream receptive field properties. That is, increasing response magnitudes through gain changes can act to select relevant visual information when coupled with a max or soft-max pooling to suppress irrelevant visual information which has a lower response magnitude (<xref ref-type="bibr" rid="bib39">Lee et al., 1999</xref>; <xref ref-type="bibr" rid="bib53">Pestilli et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Hara et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Pelli, 1985</xref>). Although increasing gain can have downstream effects on receptive field properties such as changes in position, size and spatial structure, our results suggest that these may be secondary effects and only a consequence of applying gain, rather than the cause of the behavioral improvements as others have suggested (<xref ref-type="bibr" rid="bib4">Anton-Erxleben and Carrasco, 2013</xref>; <xref ref-type="bibr" rid="bib46">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>; <xref ref-type="bibr" rid="bib62">Sprague and Serences, 2013</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Fischer and Whitney, 2009</xref>; <xref ref-type="bibr" rid="bib2">Anton-Erxleben et al., 2007</xref>). In addition, we found that gain had no additional impact when the readout was already spatially specific, reinforcing the interpretation that gain and selection of relevant information are intertwined.</p><p>We used an image-computable model of the computational steps from sensory input to decision making which allowed us to formally test hypotheses (<xref ref-type="bibr" rid="bib26">Gardner and Merriam, 2021</xref>) about how different attentional mechanisms could impact task performance. In our case, the advantage of this approach is that the model architecture allowed us to examine how gain at the earliest stages of processing causes changes in spatial receptive field properties: any time a gain occurs in an asymmetrical manner across a receptive field, downstream units will experience an apparent shift as well as shrinkage or expansion. We know from the large literature exploring the physiology of attention that receptive field shifts are correlated with spatial attention (<xref ref-type="bibr" rid="bib4">Anton-Erxleben and Carrasco, 2013</xref>; <xref ref-type="bibr" rid="bib3">Anton-Erxleben et al., 2009</xref>; <xref ref-type="bibr" rid="bib2">Anton-Erxleben et al., 2007</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Fischer and Whitney, 2009</xref>; <xref ref-type="bibr" rid="bib71">Womelsdorf et al., 2006</xref>). Several authors have proposed that enhanced task performance is a result of increasing information capacity by reducing spatial uncertainty about position (<xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>) or enhancing discriminability (<xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>). However, if changes in spatial receptive field properties are the consequence of gain changes (<xref ref-type="bibr" rid="bib35">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Compte and Wang, 2006</xref>), then it raises the question of whether these receptive field changes actually help to improve task performance. Our modeling approach allowed us to examine the theoretical impact of each change associated with gain systematically and quantify that these provided no benefit to detection or discrimination task performance. Nevertheless, previous modeling work has demonstrated that shift and shrinkage, in particular, can increase the resolution and redundancy of receptive field coverage (<xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib64">Theiss et al., 2022</xref>). Our results differ along two important dimensions: first, we scaled the magnitude of shift, shrinkage, and sensitivity changes to those induced by Gaussian gain (at a gain strength that was matched to human performance) and these were in general equal to or smaller in magnitude to what was observed in these previous papers. This leaves open the possibility that larger shifts or shrinkage could have a more direct impact on task performance. We also note that our results were based only on simulations for category detection and discrimination tasks. It may be the case that other tasks which depend more directly on spatial coding, such as judgements of visual position, could exhibit larger benefits from shifts and shrinkage of receptive fields (<xref ref-type="bibr" rid="bib32">Kay et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Vo et al., 2017</xref>).</p><p>Whether our conclusions can generalize to the behavior of attentional gain in biological neural circuits is limited both by how well the neural network observer model approximates the functioning of those neural circuits and by the model’s ability to predict behavior. There are several reasons to suggest that the model captures relevant properties of both object recognition and the primate visual system. We chose to analyze a CNN whose architecture was designed to reflect the primate visual system. This has been evaluated by comparing the similarity of CNN unit activity against measurements of single unit activity in the primate visual cortex (<xref ref-type="bibr" rid="bib59">Schrimpf et al., 2018</xref>). After training, the image features that the CNN units become selective for align closely with those that activate single units in visual cortex (<xref ref-type="bibr" rid="bib72">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Carter et al., 2019</xref>). In addition, the designers of the architecture we used (CORnet), <xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref> optimized for ‘core object recognition’, detecting a dominant object during a viewing duration of natural fixation (100–200ms) in the central visual field (10 deg). We re-used core object recognition in our human object detection task and projected our composites in a 10 degree square aperture to obtain similar perceptual characteristics. In the analysis of our task we showed that distributed performance was similar for humans and the CNN at a stimulus presentation of 65ms, confirming that the intended design of CORnet generalized to the new dataset and task that we used.</p><p>While CORnet was designed to map individual visual cortex regions onto the different layers of the CNN, it differs from the visual system in that it is a completely feed-forward model. It is well-known that the visual system has recurrence both within and between visual areas (<xref ref-type="bibr" rid="bib22">Felleman and Van Essen, 1991</xref>). Computational modeling has suggested that recurrence can affect how gain and additive offsets change down stream receptive field location and size, in particular enhancing these effects beyond receptive field boundaries (<xref ref-type="bibr" rid="bib16">Compte and Wang, 2006</xref>). These considerations suggest that more realistic models could have even stronger downstream effects on spatial receptive field properties then what we have documented in a purely feed-forward network. In computational models, recurrent connections are often unfolded into feed-forward layers, effectively making a recurrent model a deeper convolutional model (<xref ref-type="bibr" rid="bib48">Nayebi et al., 2018</xref>). Although we did not test deeper architectures in our analysis, we expect that the general principles we described should hold for models with more layers and therefore also for models with recurrent connections. An intriguing follow-up direction would be to extend the modeling described here to reaction time tasks, where a recurrent architecture allows for modeling of temporal dynamics and where diffusion models have been found to provide a useful parameterization of how bottom-up and top-down signals contribute to sensory responses over time (<xref ref-type="bibr" rid="bib33">Kay and Yeatman, 2017</xref>).</p><p>CORnet is also missing many intermediate areas of the visual system (notably area V3) (<xref ref-type="bibr" rid="bib70">Wandell and Winawer, 2011</xref>) as well as an explicit gain control mechanism such as divisive normalization (<xref ref-type="bibr" rid="bib9">Carandini and Heeger, 2012</xref>) which might account for the large gain necessary in our model to produce human-like performance enhancements. These differences mean that the exact strength of the gain signal we observed cannot be mapped directly onto physiology. In particular, while we apply gain at the earliest stage of the model, we do not wish to imply that such a large gain is seen with attention in the LGN inputs to V1 (<xref ref-type="bibr" rid="bib49">O’Connor et al., 2002</xref>). Nor do we imply that the gain in various stages of our model should directly map on to the gain observed in physiological measurements, which have tended to highlight larger gain changes in intermediate areas like V4 and MT (<xref ref-type="bibr" rid="bib66">Treue and Martínez Trujillo, 1999</xref>; <xref ref-type="bibr" rid="bib42">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib45">Moore and Armstrong, 2003</xref>) compared to earlier areas. Instead, in our model, the 4× gain should be interpreted as both an explicit increase in gain as well as an implicit gain due to the effects of normalization (<xref ref-type="bibr" rid="bib55">Reynolds and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib9">Carandini and Heeger, 2012</xref>). While normalization models have traditionally been studied in single layer models, our work extends this general approach to consider downstream effects of gain on RF properties. We assessed how these effects might interact in our CNN by demonstrating that a physiologically plausible gain of 1.1×, when accentuated by a divisive gain control mechanism (<xref ref-type="bibr" rid="bib31">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Carandini and Heeger, 2012</xref>) and amplified across multiple visual areas (many of which are not included in the CORnet model), could have produced the magnitude of effects necessary for human-level improvements in task performance. This smaller gain is more consistent with neural recordings in primates, where gain changes on the order 20–40% (1.2–1.4×) have been measured (<xref ref-type="bibr" rid="bib47">Motter, 1993</xref>; <xref ref-type="bibr" rid="bib41">Luck et al., 1997</xref>; <xref ref-type="bibr" rid="bib66">Treue and Martínez Trujillo, 1999</xref>).</p><p>We chose to model gain at the earliest possible point in the system to understand how signal changes propagate through the visual hierarchy and modify receptive field structure. Physiological measurements have found evidence for early gain (<xref ref-type="bibr" rid="bib42">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib47">Motter, 1993</xref>; <xref ref-type="bibr" rid="bib41">Luck et al., 1997</xref>), but it is equally possible that the gain is applied at a late stage close to decision making and signal gains early in visual cortex are a result of backward projections to these areas (<xref ref-type="bibr" rid="bib7">Buffalo et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Moore and Armstrong, 2003</xref>). The propagated gain analysis confirms that gain signals with spatial specificity arriving at later stages in processing (<xref ref-type="bibr" rid="bib45">Moore and Armstrong, 2003</xref>) would have similar effects on task performance.</p><p>To solve the demands of goal-directed visual attention, the human brain has multiple potential mechanisms available. To select for relevant and suppress irrelevant information, sensory responses can be amplified or the tuning of neurons and populations can be shifted to enhance some signals at the cost of others. In addition, these bottom-up sensory changes can be combined with changes in how sensory representations are read out or communicated to downstream regions. In biological systems, these mechanisms are intertwined: as we have shown, changes to early sensory signals will have complex effects on the later stages that are used for readout. In an idealized model, the changes that would have the most effect on the readout would be computed by approximating their gradients on the decision axis (<xref ref-type="bibr" rid="bib40">Lindsay and Miller, 2018</xref>). However, these gradients are typically computed in models through back-propagation (<xref ref-type="bibr" rid="bib57">Rumelhart et al., 1986</xref>), and it is not known whether or how similar gradients can be computed in biological systems. Here, we have shown using a state-of-the-art model of the visual system that when the neural network observer is matched with human performance during spatial attention some mechanisms can improve task performance, while others cannot. In the limit, shift, shrinkage, and tuning changes in receptive fields must have an impact on sensory representations and therefore on performance. But our results show that in a neural network model and at the scale expected in the primate visual system during goal-directed behavior, these are not sufficient to produce the expected effects of spatial attention on task performance. Instead, gain combined with a nonlinear selection mechanism meets the demands imposed by goal-directed visual attention. New techniques that allow for targeting interventions to defined populations of neurons raise the possibility of manipulating gain and top-down signaling to determine the effect on downstream neural response properties and behavior. Such interventions would allow for testing the main prediction of our model: that spatial visual attention relies primarily on changes in gain and not concomitant downstream effects to spatial receptive field properties.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Human observers</title><p>Seven observers were observers for the experiments (1 female, 6 male, mean age 22 y, range 19–24). All observers except one (who was an author) were naïve to the intent. No observers were excluded during the initial training sessions (see eye-tracking below). Observers completed 1600 trials in two 60-min sessions. Observers wore lenses to correct vision to normal if needed. Procedures were approved in advance by the Stanford Institutional Review Board on human participants research and all observers gave prior written informed consent before participating (Protocol IRB-32120).</p></sec><sec id="s4-2"><title>Hardware setup for human observers</title><p>Visual stimuli were generated using MATLAB (The Mathworks, Inc) and MGL (<xref ref-type="bibr" rid="bib25">Gardner et al., 2018</xref>). Stimuli were displayed at 60 cm viewing distance on a 22.5 inch VIEWPixx LCD display (resolution of 1900x1200, refresh-rate of 120 Hz) and responses collected via keyboard. Experiments were performed in a darkened room where extraneous sources of light were minimized.</p><p>Eye-tracking was performed using an infrared video-based eye-tracker at 500 Hz (Eyelink 1000; SR Research). Calibration was performed at the start of each session to get a validation accuracy of less than 1 degree average offset from expected, using a thirteen-point calibration procedure. During training, trials were initiated by fixating the central cross for 0.5 s and canceled on-line when an observer’s eye position moved more than 1.5 degree away from the center of the fixation cross for more than 0.3 s. Observers were excluded prior to data collection if we were unable to calibrate the eye tracker to an error of less than 1 degree of visual angle or if their canceled trial rate did not drop to near zero. All observers passed these criteria. During data collection the online cancellation was disabled and trials were excluded if observers made a saccade outside of fixation (&gt;1.5 deg) during the stimulus period.</p></sec><sec id="s4-3"><title>Experimental design</title><p>We compared the ability of humans and neural networks to detect objects in a grid of four images covering 10 degrees of visual angle (224 px). Given a grid of images, the observers were asked to identify whether or not a particular target category was present. On half of the trials we gave observers prior information telling them which of the four grid locations could contain the object (100% valid cue). This focal condition was compared with a distributed condition, in which no information was provided about which grid location could contain the target object. For humans, the prior in the focal condition was a spatial cue, a visual pointer to one corner of the grid. For the neural network, the prior for the focal condition was implemented by a mechanistic change in the model architecture, which differed according to the model of attention being tested. For the neural network, note that in the distributed condition the model is analogous to one in which the focal cue is implemented by a Gaussian of infinite width.</p><p>To verify that our results were not specific to detection, we also examined the ability of a neural network observer model to perform a category discrimination task. To perform the discrimination, we compared the classifier outputs from two composite grids. These grids were constructed such that one of the two grids always contained an image of the target category (A) in the top-left location and the other contained an image from the non-target category (B). The remaining distractors images were randomly sampled from the A and B categories with 50% probability. In the focal cue condition, the model architecture was modified to implement a model of attention.</p></sec><sec id="s4-4"><title>Stimuli: object detection task</title><p>In the object detection task, the stimuli presented to both humans and the neural network observer model were composed of four base images arranged in a grid (henceforth a ‘composite grid’). Each base image contained an exemplar of one of 21 ImageNet (<xref ref-type="bibr" rid="bib18">Deng et al., 2009</xref>) categories. Composite grids always contained images from four different categories. The base images were cropped to be square, and resized to <inline-formula><mml:math id="inf32"><mml:mrow><mml:mn>122</mml:mn><mml:mo>×</mml:mo><mml:mn>122</mml:mn></mml:mrow></mml:math></inline-formula> pixels, making each composite grid <inline-formula><mml:math id="inf33"><mml:mrow><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:math></inline-formula> pixels. We pulled 929 images from each of 21 ImageNet categories: analog clock (renamed to ‘clock’), artichoke, bakery (renamed to ‘baked goods’), banana, bathtub, bonsai tree (renamed to ‘tree’), cabbage butterfly, coffee, computer, Ferris wheel, football helmet, garden spider (renamed to ‘spider’), greenhouse, home theater, long-horned beetle (renamed to ‘beetle’), mortar, padlock, paintbrush, seashore, stone wall, and toaster. These base images were usually representative of their category. However, many included other distracting elements (people, text, strong reflections, etc). Two authors (KF and DB) selected 100 base images for each category absent of distracting elements (low-distraction base images) to be used for the human task. From these low-distraction base images, we set aside 5 to use as exemplars when introducing the category to human participants.</p><p>To create the human stimulus set we generated composite grids for each of the 20 target categories. Each category required 80 composite grids: 40 including target objects and 40 without. We therefore needed 40 base images from the target category and 280 (<inline-formula><mml:math id="inf34"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>40</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) base images from the non-target categories. We sampled all images from the low-distraction base images. Targets were placed 10 times in each of the four corners.</p><p>The neural network observer model was trained and tested on an expanded stimulus set. We set aside 50 base images for each category to train the linear classifiers (see Linear Classifiers, below). The approach was otherwise identical to that described above, but 829 composite grids were created with a target and 829 without, and the composites were assembled from the full set of 929 base images. Because CNN models are translation invariant we formed all target composites with the target image in the NW corner, to simplify analysis.</p></sec><sec id="s4-5"><title>Stimuli: category discrimination task</title><p>The stimuli in the category discrimination task were also composite grids of four images. However, these composites were constructed to only include images from a target pair of categories (called ‘A’ and ‘B’ and generated from 20 of the 21 ImageNet categories, as displayed in <xref ref-type="table" rid="table1">Table 1</xref>). Pairs of composites were generated, consisting of an ‘A’ stimulus and a ‘B’ stimulus with the corresponding category in the top left target grid position. The other three locations were filled with distractor images sampled pseudorandomly from the A or B category. Target images were not repeated across composites, but did appear in other stimuli as distractors. We generated 900 images per category pair, 450 with an A target and 450 with a B target.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Category pairs for the discrimination task.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Pair</th><th align="left" valign="bottom">Category A</th><th align="left" valign="bottom">Category B</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">0</td><td align="left" valign="bottom">Ferris wheel</td><td align="left" valign="bottom">Analog clock</td></tr><tr><td align="char" char="." valign="bottom">1</td><td align="left" valign="bottom">Artichoke</td><td align="left" valign="bottom">Bakery</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="left" valign="bottom">Banana</td><td align="left" valign="bottom">Bathtub</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="left" valign="bottom">Cabbage butterfly</td><td align="left" valign="bottom">Coffee</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="left" valign="bottom">Computer</td><td align="left" valign="bottom">Football helmet</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="left" valign="bottom">Garden spider</td><td align="left" valign="bottom">Greenhouse</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="left" valign="bottom">Home theatre</td><td align="left" valign="bottom">Long-horned beetle</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="left" valign="bottom">Mortar</td><td align="left" valign="bottom">Padlock</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="left" valign="bottom">Paintbrush</td><td align="left" valign="bottom">Seashore</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="left" valign="bottom">Stone wall</td><td align="left" valign="bottom">Toaster</td></tr></tbody></table></table-wrap></sec><sec id="s4-6"><title>Human object detection task</title><p>Human observers performed blocks of trials in which they had to report the presence or absence of a specified category in composite grids. At the start of each block we showed the human observers the words ‘Search for:’ followed by the name of the current target category (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, Category). They were then shown five held-out (i.e. not shown in the task) exemplar base images to gain familiarity with the target category (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, Examples) and advanced through these with a self-paced button click. This was followed by individual trials of the task. At all times, a fixation cross (0.5 deg diameter, white) was visible at the center of the screen in front of a black circle (1 deg diameter). This fixation region obscured the center of the composite grid, but made maintaining fixation easier for observers. At the start of each trial, the pixels of the current composite grid were scrambled to create a luminance-matched visual mask. This was displayed until an observer maintained fixation for 0.3 s (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, ‘Fixation’). Once fixation was acquired a cue was shown for 0.75 s, informing the observer about whether the trial was focal (in which case the possible target location was indicated) or distributed (four possible target locations indicated). The focal cue was a 0.25 deg length white line pointing toward the cued corner of the grid. The distributed cue was four 0.25 deg length white lines pointing toward all four corners of the grid. Distributed and focal cues were presented in pseudo-randomized order throughout each block. The cue was followed by a 0.75 s inter-stimulus interval (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, Delay) before the composite grid (10 × 10 deg) was shown for either 1 (8.3 ms), 2 (16.7), 4 (33.3), 8 (66.7), 16 (133.3), or 32 (266.7) video frames (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, Stimulus). The mask then replaced the stimulus and observers were given 2 s to make a response (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, Response), pressing the ‘1’ key for target present or the ‘2’ key for absent. Feedback was given by changing the fixation cross color to green for correct and red for incorrect until the 2 s period elapsed. A 0.25 s inter-trial interval separated trials.</p><p>Observers completed one training block (the ‘tree’ category) as practice before data collection began. They then completed each category block (40 focal trials with 20 target present and 20 target absent, and 40 distributed trials with 20 target present and 20 target absent) before moving on to the next category. Block order was pseudo-randomized for each observer. Each block took about five minutes to complete and a break was provided between blocks, as needed. In total, the experiment took about 2 hr, split into two 1-hr sessions on different days.</p></sec><sec id="s4-7"><title>Neural network observer model</title><p>We modeled the ventral visual pathway using CORnet-Z, a convolutional neural network (CNN) proposed by <xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref>. The model consists of four convolutional layers producing feature maps of decreasing spatial resolution (<xref ref-type="table" rid="table2">Table 2</xref>). The model which we used was pre-trained on ImageNet by the original authors (<xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref>). At the last convolutional layer we took the average over the spatial dimensions of each feature map to create the neural network’s representation (512-dimensional vector) of the input image.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>CORnet-Z structure.</title><p>Average receptive field (RF) full-width at half-maximum (FWHM) is measured using ellipses fit to the backpropagated gradients of units in a convolutional layer with respect to the input image pixels. 22.4 pixels corresponds to one degree of visual angle (<xref ref-type="bibr" rid="bib37">Kubilius et al., 2018</xref>).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Layer Type</th><th align="left" valign="bottom">Kernel Size</th><th align="left" valign="bottom">Output Shape</th><th align="left" valign="bottom">FWHM (px, deg)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Input</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">224 × 224 × 3</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">V1 Block</td><td align="left" valign="bottom">conv, stride = 2</td><td align="left" valign="bottom">7×7</td><td align="left" valign="bottom">112 × 112 × 64</td><td align="left" valign="bottom">11 (0.5)</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom"/><td align="left" valign="bottom">56 × 56 × 64</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">max pool</td><td align="left" valign="bottom">2×2</td><td align="left" valign="bottom">56 × 56 × 64</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">V2 Block</td><td align="left" valign="bottom">conv</td><td align="left" valign="bottom">3×3</td><td align="left" valign="bottom">56 × 56 ×128</td><td align="left" valign="bottom">26.8 (1.21)</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom"/><td align="left" valign="bottom">28 × 28 ×128</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">max pool</td><td align="left" valign="bottom">2×2</td><td align="left" valign="bottom">28 × 28 ×128</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">V4 Block</td><td align="left" valign="bottom">conv</td><td align="left" valign="bottom">3×3</td><td align="left" valign="bottom">28 × 28 × 256</td><td align="left" valign="bottom">55.6 (2.52)</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom"/><td align="left" valign="bottom">14 × 14 × 256</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">max pool</td><td align="left" valign="bottom">2×2</td><td align="left" valign="bottom">14 × 14 × 256</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">IT Block</td><td align="left" valign="bottom">conv</td><td align="left" valign="bottom">3×3</td><td align="left" valign="bottom">14 × 14 × 512</td><td align="left" valign="bottom">111.4 (5.06)</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom"/><td align="left" valign="bottom">7 × 7 × 512</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">max pool</td><td align="left" valign="bottom">2×2</td><td align="left" valign="bottom">7 × 7 × 512</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Encodings</td><td align="left" valign="bottom">avg. pool</td><td align="left" valign="bottom"/><td align="left" valign="bottom">1 × 1 × 512</td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap></sec><sec id="s4-8"><title>Linear classifiers: object detection task</title><p>To allow the neural network observer model to perform an object detection task, we trained a set of linear classifiers on the model output to predict the presence or absence of each of the twenty target categories. Each of these fully connected layers received as input the (512-dimensional) feature output from the CNN and projected these to a scalar output. Weights were fit using logistic regression with L2 regularization, using <italic>scikit-learn</italic> and the <italic>LIBLINEAR</italic> package (<xref ref-type="bibr" rid="bib51">Pedregosa et al., 2011</xref>). We trained the classifiers on a held out set of base images not used to generate the task grids, using 50 images with the target present and 50 images with the target absent. Training was evaluated on an independent validation set of 100 images, median AUC 0.90, range [0.77, 0.96].</p><p>To test model performance in the detection task the observer model was presented with each of the composite grids in the full image set and the output of the target category’s classifier was computed. We report the model’s area under the curve (AUC) as a measure of performance. The AUC is computed from the area under the curve defined by plotting the false positive rate against the true positive rate across the full range of possible thresholds (0–1). We used the <italic>scikit-learn</italic> implementation to calculate the AUC for each model. The AUC can be interpreted as the average probability that target images will be ranked higher by the logistic regression compared to non-target images, with a value of 0.5 indicating chance performance and a value of 1 indicating perfect discrimination. In a signal detection framework, an AUC of 0.75 corresponds approximately to a d’ of 1.</p></sec><sec id="s4-9"><title>Linear classifiers: category discrimination task</title><p>To allow the neural network observer model to perform a category discrimination task, we repeated the linear classifier training described above, adding a final step in which the classifier outputs were compared for two composites. The composite grid producing a higher output was marked as containing the target category. The classifiers were trained on a held out set of base images not used to generate the task grids. Because this task is criterion-free, we report the model’s accuracy as a measure of performance. Note that even in the distributed condition the model performance exceeds chance: this is because in any set of category pair composites the proportion of grid positions with a target will always be higher when the target image is fixed to one category. On average across images the proportion of A images in the A targets will be <inline-formula><mml:math id="inf35"><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf36"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-10"><title>Spatial attention: Gaussian gain model</title><p>To introduce Gaussian gain as a mechanism for spatial attention, we multiplied the pixel intensity of the input image at row <inline-formula><mml:math id="inf37"><mml:mi>r</mml:mi></mml:math></inline-formula> and column <inline-formula><mml:math id="inf38"><mml:mi>c</mml:mi></mml:math></inline-formula> by the magnitude of a 2-dimensional Gaussian, using the following equation:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> set the row and column location for the center of the gain field and <inline-formula><mml:math id="inf41"><mml:mi>β</mml:mi></mml:math></inline-formula> controls the strength, that is the multiplicative factor at the peak of the Gaussian. The Gaussian was centered in the cued quadrant and <inline-formula><mml:math id="inf42"><mml:mi>σ</mml:mi></mml:math></inline-formula> was set to 56 pixels (approx 2.5 degrees). We explored four values of <inline-formula><mml:math id="inf43"><mml:mi>β</mml:mi></mml:math></inline-formula>: 1.1, 2, 4, and 11.</p></sec><sec id="s4-11"><title>Quantifying the effects of gain on receptive fields and activations</title><p>To reduce computational requirements we randomly sampled 300 units per layer (1200 total units) for receptive field analysis, with higher density near the attended locus.</p><p>To determine the location and size of the receptive field of each CNN unit, we computed the derivative of their activation with respect to the pixels in the input image. This derivative was taken across a batch of 40 task images evenly distributed across categories. The magnitude of derivatives with respect to the red, green and blue channels were summed to create a sensitivity map. Receptive field location and size were estimated by fitting a 2D Gaussian distribution. The fit was performed by treating the sensitivity map as an unnormalized probability distribution and choosing the Gaussian with the same mean and covariance matrix as that distribution. Receptive field location was measured as the mean of the Gaussian fit. We report the full-width at half-maximum for the receptive field size.</p><p>To measure the effect of gain on the activation and information content of CNN units, we computed the effective gain and the change in AUC across the sampled units. We defined effective gain as the ratio between the standard deviation of a unit’s activity after applying an attention mechanism compared to before. We computed the effective gain across all features and all stimuli. To compute the change in AUC, we measured the average change along the prediction layers’ decision axes for each feature map location in layer 4 between the distributed and focal conditions. More specifically, for each category and each location in the <inline-formula><mml:math id="inf44"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> feature map, we passed the 512-dimensional encoding vector onto that category’s prediction layer just as we did for the 512-dimensional vector after average pooling. This resulted in two distributions of confidence scores along the prediction layer’s decision axis (one each for target present and absent), the AUC of which describes the relative amount of information contained in that feature map location pertaining to discrimination of target present and absent conditions. We then took the difference of AUCs between focal and distributed conditions averaged across categories in each location.</p></sec><sec id="s4-12"><title>Nonlinear normalization</title><p>In order to test the ability of ‘winner-take-all’ normalization to amplify small gains, we isolated the first layer of the CNN, and applied nonlinear normalization with exponent <inline-formula><mml:math id="inf45"><mml:mi>ξ</mml:mi></mml:math></inline-formula>. More precisely, if the output feature map of the first layer had size <inline-formula><mml:math id="inf46"><mml:mi>M</mml:mi></mml:math></inline-formula> rows by <inline-formula><mml:math id="inf47"><mml:mi>N</mml:mi></mml:math></inline-formula> columns by <inline-formula><mml:math id="inf48"><mml:mi>C</mml:mi></mml:math></inline-formula> channels and activations <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we calculated the normalized outputs:<disp-formula id="equ3"> <label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To measure the resulting amplified gain, we applied a small Gaussian gain between <inline-formula><mml:math id="inf50"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf51"><mml:mrow><mml:mn>1.1</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> to the input image in the same manner as in the full Gaussian gain model. We then measured the ratio of average effective gain for units contained entirely within the gain field against the average effective gain of units entirely outside the attention gain field, for various values of <inline-formula><mml:math id="inf52"><mml:mi>ξ</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-13"><title>Spatial attention: shift-only model</title><p>In the Gaussian gain model, we applied the gain at layer 1 and observed changes in the model’s detection performance at the output layers. We took a parallel approach here to design a model that could mimic the receptive field shifts at layer 4 (induced by gain at layer 1) while producing no systematic effect on response gain. To cause the layer 4 units to observe different parts of the input image, we shifted the connections between pixels in the input image and first layer. We preserved all other connections, so layer 4 units of the neural network continued to receive information from the same layer 1 units.</p><p>To obtain the size of the necessary connection shifts we created a ‘shift map’ in input image space by measuring the distance and direction that layer 4 units moved when the Gaussian gain was applied. To make this measurement, we took each input image pixel location <inline-formula><mml:math id="inf53"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and calculated the average receptive field shift of the 20 sampled layer 4 units with the closest receptive field centers without attention. Because we used a sampling procedure and not the full set of layer 4 units we weighted the sampled units by their Euclidean distance from the target pixel. To reduce noise in the shift map, we applied a Gaussian blur with <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> pixels. Using the shift map, we then re-assigned the connections from the input image to the layer 1 units. The simplest way to to implement this involved swapping the activation of each layer 1 unit with the activation of the unit at its shifted location. For example, if unit <inline-formula><mml:math id="inf55"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>75</mml:mn><mml:mo>,</mml:mo><mml:mn>75</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> was shifted by <inline-formula><mml:math id="inf56"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> we assigned it the activation of the unit at <inline-formula><mml:math id="inf57"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>65</mml:mn><mml:mo>,</mml:mo><mml:mn>65</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. To deal with decimal shifts we performed linear interpolation using neighboring units.</p></sec><sec id="s4-14"><title>Spatial attention: receptive field structure</title><p>In the receptive field structure model we aimed to mimic the spatial tuning changes induced by the Gaussian gain at a particular layer but without changing the effective gain of units. To accomplish this, we first computed the true gain propagated to the target layer <inline-formula><mml:math id="inf58"><mml:mi>L</mml:mi></mml:math></inline-formula> by scaling the Gaussian gain map to the size of layer <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>’s feature map. With this change alone the weights of units closer to the locus of attention are scaled more than the weights farther from the locus, introducing differential gain. To avoid a change in the overall scale of units’ weights, we re-scaled the kernel to match the L2-norm (sum-of-squares) of the original kernel weights.</p><p>To summarize, suppose that layer <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>’s feature map is <inline-formula><mml:math id="inf61"><mml:mi>t</mml:mi></mml:math></inline-formula> times the size of the input image so that a unit at row <inline-formula><mml:math id="inf62"><mml:mi>r</mml:mi></mml:math></inline-formula> and column <inline-formula><mml:math id="inf63"><mml:mi>c</mml:mi></mml:math></inline-formula> of the layer <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> feature map has an effective effective gain of <inline-formula><mml:math id="inf65"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> under the Gaussian gain model. Then if <inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the original weight vector of a unit in the unraveled convolution at layer <inline-formula><mml:math id="inf67"><mml:mi>L</mml:mi></mml:math></inline-formula> whose input vector <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> contains the activations of post-ReLU units of layer <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and if the row-column positions in the <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> feature map of the unit described by <italic>a</italic><sub><italic>i</italic></sub> is <inline-formula><mml:math id="inf71"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, then the replacement weight vector in the sensitivity shift model is given by the vector <inline-formula><mml:math id="inf72"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, whose entries are:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-15"><title>Spatial attention: shrinkage model</title><p>In the shrinkage model we aimed to mimic the receptive field size changes observed at layer 4 under Gaussian gain, without causing changes in receptive field location or gain. To achieve this, we assigned a shrinkage factor to each layer 4 unit and rewired its connections to layer 3 accordingly.</p><p>Shrinkage factors <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were determined by the distance <inline-formula><mml:math id="inf74"><mml:mi>d</mml:mi></mml:math></inline-formula> between the locus of attention in input image space and the unit’s spatial location in the feature map projected back onto the input image. This distance was converted to a shrinkage factor by a function chosen to model the properties of the receptive field size change pattern observed under <inline-formula><mml:math id="inf75"><mml:mrow><mml:mn>11</mml:mn><mml:mi>x</mml:mi><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> Gaussian gain at layer 4 (<xref ref-type="fig" rid="fig3">Figure 3e</xref>), namely<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mspace width="thinmathspace"/><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>2.44</mml:mn><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mn>112</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2.89</mml:mn><mml:mfrac><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mn>112</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf76"><mml:mi>β</mml:mi></mml:math></inline-formula> determines the overall strength of the effect, and ranged from 0.1 to 0.4 in our analyses. A shrinkage factor of 0 indicated no change in receptive field size, while a shrinkage factor of 1 indicated shrinkage to zero radius.</p><p>Given a shrinkage factor, we re-weighted the connections of each layer 4 unit to produce an approximate shrunken convolutional kernel for that unit. The linearity of convolution provides an equivalence between re-weighting connections from layer 3 to layer 4 and replacing those connections with new ones to units in a virtual continuous layer 3 feature map formed by linear interpolation between activations in the true layer 3 feature map. We therefore were able to calculate the new weights for each layer 4 unit based on a length-9 array of floating-point locations on the layer 3 feature map (all CORnet-Z kernels are <inline-formula><mml:math id="inf77"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>). Given the original wiring locations <inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for a unit with distance <inline-formula><mml:math id="inf80"><mml:mi>d</mml:mi></mml:math></inline-formula>, the new location corresponding to input <inline-formula><mml:math id="inf81"><mml:mi>i</mml:mi></mml:math></inline-formula> was chosen to be<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>9</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and similarly for <inline-formula><mml:math id="inf82"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula>. Using the linearity of convolution, each new virtual input location <inline-formula><mml:math id="inf83"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is equivalent (for a linearly interpolated feature map) to a weighted combination of connections to the four feature map locations surrounding <inline-formula><mml:math id="inf84"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, calculated by rounding <inline-formula><mml:math id="inf85"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf86"><mml:mi>y</mml:mi></mml:math></inline-formula> coordinates up or down. The resultant 36 (<inline-formula><mml:math id="inf87"><mml:mrow><mml:mn>9</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) connections were then simplified by combining connections from the same layer 3 unit to yield a re-weighted convolution kernel.</p></sec><sec id="s4-16"><title>Spatial attention: Gain-only model</title><p>We designed a model which could effect gain without receptive field shift by flattening the gain in the cued quadrant. Receptive field shift occurs when there is a differential gain across the receptive field of a unit. To get rid of this, you can put a flat gain over the cued quadrant. This naive approach has the problem that units that overlap two quadrants will still shift and shrink according to the strength of the gain. To prevent these units from shifting in a manner correlated to the gain we separated the CNN feature maps into four parts corresponding to the four image quadrants, ran the model forward with zero padding around each quadrant, and then concatenated the results back together. This ensured that each unit experienced a flat gain across its inputs and that as gain increased units near the quadrant boundaries did not experience gain-dependent receptive field shift or shrinkage.</p></sec><sec id="s4-17"><title>Necessary and sufficient test</title><p>To obtain a propagated gain map in the final layer output we applied the Gaussian gain to the start of the neural network observer model and measured the average effective gain of the <inline-formula><mml:math id="inf88"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula> layer 4 output units across a representative sample of images. We call this the ‘propagated gain map’, since it represents the effect of the input gain on the output layers. We tested necessity by dividing the network output by the map for a model with gain applied and we tested sufficiency by multiplying the outputs from a no-gain model.</p></sec><sec id="s4-18"><title>Readout from target quadrant</title><p>To test the behavior of the neural network observer model with spatially-specific readout from the last convolution layer (Layer 4), we masked the output of that layer to the linear prediction layers in the object detection task. To apply the mask, we zeroed activations of units outside the top-left <inline-formula><mml:math id="inf89"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> of layer 4 (full dimensions <inline-formula><mml:math id="inf90"><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula>). The same linear prediction layers and stimuli were used as in the necessary and sufficient test, and the same four conditions were tested: no gain, early Gaussian gain, and with a propagated gain map applied and divided out at layer 4.</p></sec><sec id="s4-19"><title>Behavioral analysis</title><p>We analyzed the human behavioral data by binning trials according to their duration and computing sensitivity <inline-formula><mml:math id="inf91"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> from the equation:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf92"><mml:mi>Z</mml:mi></mml:math></inline-formula> is the inverse of the cumulative normal distribution and <inline-formula><mml:math id="inf93"><mml:mi>H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> are the hit and false alarm rate, respectively. We fit a logarithmic function to the <inline-formula><mml:math id="inf95"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> data using the equation:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>∗</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf96"><mml:mi>t</mml:mi></mml:math></inline-formula> is the stimulus duration and <inline-formula><mml:math id="inf97"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf98"><mml:mi>κ</mml:mi></mml:math></inline-formula> are parameters that control the shape of the logarithmic function.</p><p>To compare human and model performance, we can also convert between <inline-formula><mml:math id="inf99"><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> and the area under the curve (AUC) by the equation:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-20"><title>Confidence intervals</title><p>All error bars are calculated by bootstrapping the given statistic with <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula> and reported as the 95% confidence interval.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Investigation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Procedures were approved in advance by the Stanford Institutional Review Board on human participants research and all observers gave prior written informed consent before participating (Protocol IRB-32120).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-78392-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The images and composite grids used in this study as well as the code necessary to replicate our analyses are available in the Open Science Framework with the identifier <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/AGHQK">https://doi.org/10.17605/OSF.IO/AGHQK</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>KJ</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Gain, not changes in spatial receptive field properties, improves task performance in a neural network attention model</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/AGHQK</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Josh Wilson for help with data collection and Eline Kupers and Maggie Henderson for early discussions.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>DG</given-names></name><name><surname>Hamilton</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Striate cortex of monkey and cat: contrast response function</article-title><source>Journal of Neurophysiology</source><volume>48</volume><fpage>217</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1152/jn.1982.48.1.217</pub-id><pub-id pub-id-type="pmid">7119846</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Henrich</surname><given-names>C</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Attention changes perceived size of moving visual patterns</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>5</elocation-id><pub-id pub-id-type="doi">10.1167/7.11.5</pub-id><pub-id pub-id-type="pmid">17997660</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Stephan</surname><given-names>VM</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention reshapes center-surround receptive field structure in Macaque cortical area mt</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2466</fpage><lpage>2478</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp002</pub-id><pub-id pub-id-type="pmid">19211660</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attentional enhancement of spatial resolution: linking behavioural and neurophysiological evidence</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1038/nrn3443</pub-id><pub-id pub-id-type="pmid">23422910</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Graf</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual receptive field modulation in the lateral Intraparietal area during attentive fixation and free gaze</article-title><source>Cerebral Cortex</source><volume>12</volume><fpage>234</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.3.234</pub-id><pub-id pub-id-type="pmid">11839598</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A flexible Readout mechanism of human sensory representations</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3500</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11448-7</pub-id><pub-id pub-id-type="pmid">31375665</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buffalo</surname><given-names>EA</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Landman</surname><given-names>R</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A backward progression of Attentional effects in the ventral stream</article-title><source>PNAS</source><volume>107</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907658106</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep Convolutional models improve predictions of Macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Normalization as a Canonical neural computation</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name><name><surname>Fonteijn</surname><given-names>H</given-names></name><name><surname>Verstraten</surname><given-names>FAJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spatial coding and Invariance in object-selective cortex</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>47</volume><fpage>14</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2009.08.015</pub-id><pub-id pub-id-type="pmid">19833329</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual attention: the past 25 years</article-title><source>Vision Research</source><volume>51</volume><fpage>1484</fpage><lpage>1525</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.04.012</pub-id><pub-id pub-id-type="pmid">21549742</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>S</given-names></name><name><surname>Armstrong</surname><given-names>Z</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name><name><surname>Johnson</surname><given-names>I</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Activation Atlas</article-title><source>Distill</source><volume>4</volume><elocation-id>e15</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00015</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparison of deep neural networks to Spatio-temporal cortical Dynamics of human visual object recognition reveals Hierarchical correspondence</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Context-dependent changes in functional circuitry in visual area mt</article-title><source>Neuron</source><volume>60</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.08.007</pub-id><pub-id pub-id-type="pmid">18940596</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Space and attention in Parietal cortex</article-title><source>Annual Review of Neuroscience</source><volume>22</volume><fpage>319</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.22.1.319</pub-id><pub-id pub-id-type="pmid">10202542</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Tuning curve shift by attention modulation in cortical neurons: a computational study of its mechanisms</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>761</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj021</pub-id><pub-id pub-id-type="pmid">16135783</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Preddie</surname><given-names>DC</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Responses in area V4 depend on the spatial relationship between stimulus and attention</article-title><source>Journal of Neurophysiology</source><volume>75</volume><fpage>1306</fpage><lpage>1308</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.75.3.1306</pub-id><pub-id pub-id-type="pmid">8867139</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>ImageNet: A large-scale hierarchical image database</article-title><conf-name>2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops</conf-name><conf-loc>Miami, FL</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The updating of the representation of visual space in Parietal cortex by intended eye movements</article-title><source>Science</source><volume>255</volume><fpage>90</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1126/science.1553535</pub-id><pub-id pub-id-type="pmid">1553535</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckstein</surname><given-names>MP</given-names></name><name><surname>Thomas</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name><name><surname>Shimozaki</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A signal detection model predicts the effects of set size on visual search accuracy for feature, conjunction, triple conjunction, and Disjunction displays</article-title><source>Perception &amp; Psychophysics</source><volume>62</volume><fpage>425</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.3758/BF03212096</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title><source>NeuroImage</source><volume>152</volume><fpage>184</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.001</pub-id><pub-id pub-id-type="pmid">27777172</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed Hierarchical processing in the Primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention NARROWS position tuning of population responses in V1</article-title><source>Current Biology</source><volume>19</volume><fpage>1356</fpage><lpage>1361</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.06.059</pub-id><pub-id pub-id-type="pmid">19631540</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Anzai</surname><given-names>A</given-names></name><name><surname>Ohzawa</surname><given-names>I</given-names></name><name><surname>Freeman</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Linear and Nonlinear contributions to orientation tuning of simple cells in the cat’s Striate cortex</article-title><source>Visual Neuroscience</source><volume>16</volume><fpage>1115</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1017/s0952523899166112</pub-id><pub-id pub-id-type="pmid">10614591</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Larsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>MGL: visual Psychophysics stimuli and experimental design package</data-title><version designator="2.0">2.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1299497">https://doi.org/10.5281/zenodo.1299497</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Population models, not analyses, of human Neuroscience measurements</article-title><source>Annual Review of Vision Science</source><volume>7</volume><fpage>225</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-093019-111124</pub-id><pub-id pub-id-type="pmid">34283926</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hara</surname><given-names>Y</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differing effects of attention in single-units and populations are well predicted by heterogeneous tuning and the normalization model of attention</article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00012</pub-id><pub-id pub-id-type="pmid">24600380</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname><given-names>HL</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Mouloua</surname><given-names>M</given-names></name><name><surname>Downing</surname><given-names>CJ</given-names></name><name><surname>Woodward</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Visual attention modulates signal Detectability</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>16</volume><fpage>802</fpage><lpage>811</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.16.4.802</pub-id><pub-id pub-id-type="pmid">2148593</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Half-squaring in responses of cat Striate cells</article-title><source>Visual Neuroscience</source><volume>9</volume><fpage>427</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1017/S095252380001124X</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural Dynamics of Attentional selection in natural scenes</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id><pub-id pub-id-type="pmid">27733605</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention reduces spatial uncertainty in human ventral temporal cortex</article-title><source>Current Biology</source><volume>25</volume><fpage>595</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.12.050</pub-id><pub-id pub-id-type="pmid">25702580</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Yeatman</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bottom-up and top-down computations in word-and face-selective cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22341</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22341</pub-id><pub-id pub-id-type="pmid">28226243</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not Unsupervised, models may explain it cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>BP</given-names></name><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attraction of position preference by spatial attention throughout human visual cortex</article-title><source>Neuron</source><volume>84</volume><fpage>227</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.047</pub-id><pub-id pub-id-type="pmid">25242220</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep Convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CORnet: Modeling the Neural Mechanisms of Core Object Recognition</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/408385</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The time course of Perisaccadic receptive field shifts in the lateral Intraparietal area of the monkey</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>1519</fpage><lpage>1527</lpage><pub-id pub-id-type="doi">10.1152/jn.00519.2002</pub-id><pub-id pub-id-type="pmid">12612015</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DK</given-names></name><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Braun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attention activates winner-take-all competition among visual filters</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>375</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1038/7286</pub-id><pub-id pub-id-type="pmid">10204546</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How biological attention mechanisms improve task performance in a large-scale visual system model</article-title><source>eLife</source><volume>7</volume><elocation-id>e38105</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38105</pub-id><pub-id pub-id-type="pmid">30272560</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Neural mechanisms of spatial selective attention in areas V1, V2, and V4 of Macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>24</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.1.24</pub-id><pub-id pub-id-type="pmid">9120566</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in Macaque cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00431.1999</pub-id><pub-id pub-id-type="pmid">9870971</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McIntosh</surname><given-names>LT</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Baccus</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning models of the retinal response to natural scenes</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1369</fpage><lpage>1377</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Genovese</surname><given-names>CR</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remapping in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>1738</fpage><lpage>1755</lpage><pub-id pub-id-type="doi">10.1152/jn.00189.2006</pub-id><pub-id pub-id-type="pmid">17093130</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selective gating of visual signals by Microstimulation of frontal cortex</article-title><source>Nature</source><volume>421</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1038/nature01341</pub-id><pub-id pub-id-type="pmid">12540901</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Selective attention gates visual processing in the Extrastriate cortex</article-title><source>Science</source><volume>229</volume><fpage>782</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1126/science.4023713</pub-id><pub-id pub-id-type="pmid">4023713</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Focal attention produces spatially selective processing in visual cortical areas V1, V2, and V4 in the presence of competing stimuli</article-title><source>Journal of Neurophysiology</source><volume>70</volume><fpage>909</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.70.3.909</pub-id><pub-id pub-id-type="pmid">8229178</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task-driven Convolutional recurrent models of the visual system</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>5290</fpage><lpage>5301</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Fukui</surname><given-names>MM</given-names></name><name><surname>Pinsk</surname><given-names>MA</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Attention modulates responses in the human lateral Geniculate nucleus</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1203</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1038/nn957</pub-id><pub-id pub-id-type="pmid">12379861</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>J</given-names></name><name><surname>Verghese</surname><given-names>P</given-names></name><name><surname>Pavel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The Psychophysics of visual search</article-title><source>Vision Research</source><volume>40</volume><fpage>1227</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00244-8</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Uncertainty explains many aspects of visual contrast detection and discrimination</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><elocation-id>1508</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.2.001508</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Attentional Enhancement via selection and pooling of early sensory responses in human visual cortex</article-title><source>Neuron</source><volume>72</volume><fpage>832</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.025</pub-id><pub-id pub-id-type="pmid">22153378</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Orienting of attention</article-title><source>The Quarterly Journal of Experimental Psychology</source><volume>32</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/00335558008248231</pub-id><pub-id pub-id-type="pmid">7367577</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The normalization model of attention</article-title><source>Neuron</source><volume>61</volume><fpage>168</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.002</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>J</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Compression of visual space before Saccades</article-title><source>Nature</source><volume>386</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/386598a0</pub-id><pub-id pub-id-type="pmid">9121581</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-Propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sagi</surname><given-names>D</given-names></name><name><surname>Julesz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Enhanced detection in the aperture of focal attention during simple discrimination tasks</article-title><source>Nature</source><volume>321</volume><fpage>693</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1038/321693a0</pub-id><pub-id pub-id-type="pmid">3713853</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarzlose</surname><given-names>RF</given-names></name><name><surname>Swisher</surname><given-names>JD</given-names></name><name><surname>Dang</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The distribution of category and location information across object-selective regions in human visual cortex</article-title><source>PNAS</source><volume>105</volume><fpage>4447</fpage><lpage>4452</lpage><pub-id pub-id-type="doi">10.1073/pnas.0800431105</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sclar</surname><given-names>G</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>Lennie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Coding of image contrast in central visual pathways of the Macaque monkey</article-title><source>Vision Research</source><volume>30</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(90)90123-3</pub-id><pub-id pub-id-type="pmid">2321355</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention modulates spatial priority maps in the human occipital, Parietal and frontal Cortices</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1879</fpage><lpage>1887</lpage><pub-id pub-id-type="doi">10.1038/nn.3574</pub-id><pub-id pub-id-type="pmid">24212672</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Diverse Deep Neural Networks All Predict Human IT Well, after Training and Fitting</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.07.082743</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theiss</surname><given-names>JD</given-names></name><name><surname>Bowen</surname><given-names>JD</given-names></name><name><surname>Silver</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Spatial attention enhances crowded stimulus Encoding across modeled receptive fields by increasing redundancy of feature representations</article-title><source>Neural Computation</source><volume>34</volume><fpage>190</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01447</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name><name><surname>Tehovnik</surname><given-names>EJ</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name><name><surname>Schiller</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Eye movements modulate visual receptive fields of V4 neurons</article-title><source>Neuron</source><volume>29</volume><fpage>757</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00250-1</pub-id><pub-id pub-id-type="pmid">11301034</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Martínez Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in Macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Es</surname><given-names>DM</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial sampling in human visual cortex is modulated by both spatial and feature-based attention</article-title><source>eLife</source><volume>7</volume><elocation-id>e36928</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36928</pub-id><pub-id pub-id-type="pmid">30526848</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vo</surname><given-names>VA</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial tuning shifts increase the Discriminability and Fidelity of population codes in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3386</fpage><lpage>3401</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-16.2017</pub-id><pub-id pub-id-type="pmid">28242794</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><name><surname>Van Der Maas</surname><given-names>HLJ</given-names></name><name><surname>Grasman</surname><given-names>RPPP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An Ez-diffusion model for response time and accuracy</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>14</volume><fpage>3</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.3758/BF03194023</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Imaging Retinotopic maps in the human brain</article-title><source>Vision Research</source><volume>51</volume><fpage>718</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2010.08.004</pub-id><pub-id pub-id-type="pmid">20692278</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Pieper</surname><given-names>F</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamic shifts of visual receptive fields in cortical area mt by spatial attention</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1156</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/nn1748</pub-id><pub-id pub-id-type="pmid">16906153</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-Optimized Hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zirnsak</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Noudoost</surname><given-names>B</given-names></name><name><surname>Xu</surname><given-names>KZ</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual space is compressed in Prefrontal cortex before eye movements</article-title><source>Nature</source><volume>507</volume><fpage>504</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1038/nature13149</pub-id><pub-id pub-id-type="pmid">24670771</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78392.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Serences</surname><given-names>John T</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.03.04.483026" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.03.04.483026"/></front-stub><body><p>This manuscript combines human behavioral experiments using a categorization task and a convolutional neural network model to test different mechanisms that may support attention-related improvements in perception. Through carefully controlled manipulations of computational architecture and parameters, the authors dissociate the effects of tuning gain vs. tuning shifts. They conclude that increases in gain are the primary means by which attention improves behavioral performance.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78392.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>University of California, San Diego</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Compte</surname><given-names>Albert</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/054vayn55</institution-id><institution>IDIBAPS</institution></institution-wrap><country>Spain</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Kay</surname><given-names>Kendrick N</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>University of Minnesota</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.03.04.483026">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.03.04.483026v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Behavioral benefits of spatial attention explained by multiplicative gain, not receptive field shifts, in a neural network model&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Floris de Lange as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Albert Compte (Reviewer #1); Kendrick N Kay (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>We were all very enthusiastic about the approach and the findings. During our discussions, we collectively focused on several issues regarding the generality of the conclusions and the methods used to arrive at them. Below is a list of the main issues:</p><p>1) The Detection vs discrimination. Having at least one other task would allow you to determine if the gain story is specific to detection or if the type of modulation depends on the task. In either case, results from another task would be interesting and would greatly improve the generality and theoretical impact of the claims.</p><p>2) Justifying the spatial invariance of the readout mechanism and potentially testing other schemes.</p><p>3) What happens when gain isn't just injected into &quot;V1&quot;, but into other layers? For example, Lindsay has shown that this matters in the context of CNNs performing attention tasks, and the rationale for injecting a change so early in the system isn't entirely clear.</p><p>4) Others, like Compte and Wang 2006, have also explored this question using other architectures. Given that the CORnet-z model does not include recurrent dynamics within each layer, it will be important to discuss and consider the implications of the chosen network and implications for the generality of the conclusions.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) The model requires a more detailed presentation in the text (now lines 93,94). A succinct description of how it is built should be provided so the reader does not have to look up the references for a simple, broad-picture understanding. It is important to mention that the model has been built and trained previously based on the same set of images that will be used here, and the weights are available, so the only further training done here is the linear classifier on the model outputs to detect image categories upon presentation of &quot;composite&quot; grids. It took a while for me to understand this and to see the difference between base images and composite images. There is also a confusing thing about the neural model in Figure 2d: it pictures the ReLU occurring after the max pooling step, but in Figure 1 of Kubilius et al. (2018) it occurs before the max pooling step. The sentence (&quot;Unit activations were measured after the convolution, prior to the max pooling step&quot;) in the caption of Figure 2 is also confusing: how is unit activity being measured before the ReLU non-linearity that transforms inputs into output rates?</p><p>2) The manuscript emphasizes 3 different RF modulations: scaling, shift, and shrinkage, but then only two of them are specifically isolated with network simulations. RF shrinkage is not addressed. This is not really a problem, but it would be nice to have full &quot;symmetry&quot; in the manuscript. Is there a specific reason why this modulation can not be assessed to make it more parallel to the rest?</p><p>3) References are required in lines 145 (&quot;…behavioral benefits of attention&quot;) and 250 (&quot;…as others have suggested&quot;).</p><p>4) In general, scatter plots in the figures should indicate what are the individual data points presented. I think in some cases are units, in other cases they are categories. This should be clearly indicated.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic>1. On the issue of the readout.As mentioned in the public review, it appears that the conclusions are heavily dependent on the choice of readout mechanism. Thus, it seems a bit premature at this point to make strong general conclusions about which tuning properties are critical for behavioral improvements. Many recent studies have shown spatial tuning (i.e. large but limited RF sizes) in the high-level visual cortex; hence, in the neural network observer model, it is not clear why the readout (linear classification) is performed on unit activity that has been summed fully across visual space.</p><p>Now, it is an open interesting empirical question whether perceptual decisions from the visual system are based on fully spatially invariant units. But note that if that were the case, isn't one counterintuitive implication that our perceptual decisions can be corrupted by visual stimuli far away from the relevant decision zone?</p><p>If we entertain the possibility that the readout can be from units that have spatial specificity, then it remains to be seen what sort of modeling outcomes there might be.</p><p>Even though the authors show, in the context of their particular model, that tuning shifts do not produce the required behavioral improvements, it seems that in general, it should be possible in theory for shifts to provide the required benefits. For example, if the focal cue is on stimuli in the upper right quadrant, if all receptive fields in the model covering the other quadrants were fully shifted to the upper right quadrant, wouldn't the readout from the model obviously improve?</p><p>With regards to the linear classifiers implemented by the authors, it seems there are many important details that need to get spelled out. Things like how exactly was the final predicted category obtained? Was a 20-way classification scheme used, or some winner-take-all scheme from 20 separate binary classifiers used? Was there any regularization used to fit the weights of the logistic regression? If not, how can we be confident that the amount of training data was sufficient? Details on the classification methods are important for interpreting how upstream computational changes impact the behavioral output from the model.</p><p>2. Reframing and/or exploring other models.</p><p>In light of the uncertainties about how general the conclusions can be from the modeling results, the authors may wish to consider refocusing/reframing their work. Instead of attempting to make strong general conclusions about the relationship between behavior and computational mechanisms, the author might reframe their work into more of a computational and theory-building exercise.</p><p>In particular, the authors show interesting insights regarding how a Gaussian applied to an early layer of a CNN can have effects that propagate through the network and which manifest as complex, rich RF tuning changes elsewhere in the model (e.g. as the authors state, &quot;RF shifts are a result of the signal gain…&quot;) (see also Compte Cerebral Cortex 2006). The distinction between architecture (internal computational mechanisms) and what might manifest at the level of input-output behavior of a unit is an important one, as the authors discuss (p. 17, lines 257-268). Certainly, a number of previous studies have adopted the latter mindset. Note that doing so is not necessarily inconsistent with the former &quot;mechanistic&quot; mindset (they are just different modeling stances one can take). Emphasizing and generating insights from adopting a &quot;mechanistic&quot; mindset is valuable research output.</p><p>Thus, one suggestion is to reframe the work to focus more on this insight and think about the theoretical ramifications (and/or concrete experimental predictions) that might stem from this. Related to this effort might be to consider other models. For example, it would be interesting to understand the similarities and differences between the 'attentional field' model (Reynolds, Heeger) and the current model. As another example, it would be interesting to consider models in which the &quot;Gaussian gain&quot; is applied at the end (deep layers) and effects propagate backwards, as opposed to being applied at the beginning and effects propagating forwards.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model&quot; for further consideration by <italic>eLife</italic>.</p><p>Reviewer #2 and I have re-evaluated your revised submission and we both agree that it is much improved and makes a strong contribution to the literature. There are a few remaining issues that need to be addressed that are captured in Reviewer #2's comments, and I'd ask you to address them before a final decision is made. Please let me know if you have any questions, and thanks.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I have reviewed the revisions and the authors' responses.</p><p>Overall, the revisions are solid and substantial. The additions to the paper are useful, in particular, the new analyses exploring exactly how spatial information is included, restricted, or &quot;averaged over&quot;, which provide insights into the potential equivalency of the gain effects and the selection (or suppression) of irrelevant information. In addition, the new framing and discussion provide helpful clarity over the original manuscript and make for a more straightforward read. The computational modeling performed in this paper represents a novel and well-executed foray into a challenging area. While it does not necessarily definitively resolve the issues at stake, I believe it provides and demonstrates an interesting and non-trivial approach.</p><p>A few critical comments on the revisions:</p><p>The authors state, &quot;At larger scales or in other tasks there are theoretical reasons to expect that task performance will improve due to these effects&quot;. In this sentence, the referent of &quot;these effects&quot; is a bit unclear. Also, I am not sure what 'larger scales' refers to. Finally, it would be useful to give specific examples of what types of 'other tasks' might be implied by the sentence here.</p><p>In response to the reviews, the authors added some detail on the linear classifier methods, but the text is still not sufficient. More detail on exactly what was implemented seems important. For example, details on the &quot;under the curve&quot; quantification. In addition, perhaps there is a misspecification of the methods: As the authors state, &quot;Weights were fit using logistic regression with an L2 loss and no regularization, using scikit-learn and the LIBLINEAR package (Pedregosa et al., 2011).&quot; However, if I understand correctly, for each category, there are effectively 512 parameters that need to get learned and only 100 instances to train these parameters. Hence, if no regularization were used, this presumably would lead to a very noisy (overfit) solution. Perhaps the authors meant that they used L2 regularization (and the default settings for the regularization hyperparameter in the scikit-learn package)? Note that logistic regression implies a probabilistic formulation of the classification problem, and would seem to suggest that the authors don't actually mean that they used an L2 loss (which implies additive Gaussian noise). Hopefully, these important analysis choices and details can be clarified and do not have larger ramifications for the rest of the paper.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.78392.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>We were all very enthusiastic about the approach and the findings. During our discussions, we collectively focused on several issues regarding the generality of the conclusions and the methods used to arrive at them. Below is a list of the main issues:</p><p>1) The Detection vs discrimination. Having at least one other task would allow you to determine if the gain story is specific to detection or if the type of modulation depends on the task. In either case, results from another task would be interesting and would greatly improve the generality and theoretical impact of the claims.</p></disp-quote><p>We agree that generalization to a discrimination task is worth testing. To ensure our results generalize better we ran our analysis against such a task using an analog to a two-interval forced choice task and found qualitatively similar results to the detection task [Figure 8 Supplement 2].</p><disp-quote content-type="editor-comment"><p>2) Justifying the spatial invariance of the readout mechanism and potentially testing other schemes.</p></disp-quote><p>Agreed that the assumption of spatial invariance in the original model is important to test. To ensure that our results were not affected by the choice to read out from a spatially collapsed final layer, we now report two new analyses: first, the result for a model trained to read out from the full spatial output (7 x 7 x 512) [292-308] and second, the result for a model trained to read out only from the target quadrant while masking out the other four. The results from these analyses support our initial conclusions, please see the text for the rationale and results [Figure 8 Supplement 1].</p><disp-quote content-type="editor-comment"><p>3) What happens when gain isn't just injected into &quot;V1&quot;, but into other layers? For example, Lindsay has shown that this matters in the context of CNNs performing attention tasks, and the rationale for injecting a change so early in the system isn't entirely clear.</p></disp-quote><p>We’ve clarified in the text that the propagated gain test which is applied at the last layer of the CNN (Figure 8) demonstrates that for this particular model the choice of layer only changes whether the receptive field properties are changed, not the task performance [274-283]. We’ve also added a more complete discussion of the rationale for injecting gain into the earlier layers [442-448] and the Lindsay paper specifically [453-456].</p><disp-quote content-type="editor-comment"><p>4) Others, like Compte and Wang 2006, have also explored this question using other architectures. Given that the CORnet-z model does not include recurrent dynamics within each layer, it will be important to discuss and consider the implications of the chosen network and implications for the generality of the conclusions.</p></disp-quote><p>We’ve added a more complete discussion of this issue in the discussion [411-424].</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) The model requires a more detailed presentation in the text (now lines 93,94). A succinct description of how it is built should be provided so the reader does not have to look up the references for a simple, broad-picture understanding. It is important to mention that the model has been built and trained previously based on the same set of images that will be used here, and the weights are available, so the only further training done here is the linear classifier on the model outputs to detect image categories upon presentation of &quot;composite&quot; grids. It took a while for me to understand this and to see the difference between base images and composite images. There is also a confusing thing about the neural model in Figure 2d: it pictures the ReLU occurring after the max pooling step, but in Figure 1 of Kubilius et al. (2018) it occurs before the max pooling step. The sentence (&quot;Unit activations were measured after the convolution, prior to the max pooling step&quot;) in the caption of Figure 2 is also confusing: how is unit activity being measured before the ReLU non-linearity that transforms inputs into output rates?</p></disp-quote><p>We’ve added a better description of the model in both the methods [564-590] and results [109-113]. We thank the reviewer for pointing out the error in the figure and text concerning the order of the relu and maxpool operations and have fixed these.</p><disp-quote content-type="editor-comment"><p>2) The manuscript emphasizes 3 different RF modulations: scaling, shift, and shrinkage, but then only two of them are specifically isolated with network simulations. RF shrinkage is not addressed. This is not really a problem, but it would be nice to have full &quot;symmetry&quot; in the manuscript. Is there a specific reason why this modulation can not be assessed to make it more parallel to the rest?</p></disp-quote><p>We very much appreciate the suggestion to make the paper symmetrical and have added a new figure and series of analyses showing the effects of shrinkage [Figure 6, 222-245].</p><disp-quote content-type="editor-comment"><p>3) References are required in lines 145 (&quot;…behavioral benefits of attention&quot;) and 250 (&quot;…as others have suggested&quot;).</p></disp-quote><p>Citations have been added for each statement.</p><disp-quote content-type="editor-comment"><p>4) In general, scatter plots in the figures should indicate what are the individual data points presented. I think in some cases are units, in other cases they are categories. This should be clearly indicated.</p></disp-quote><p>We’ve clarified whether data points indicate units or categories for all figures.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. On the issue of the readout.</p><p>As mentioned in the public review, it appears that the conclusions are heavily dependent on the choice of readout mechanism. Thus, it seems a bit premature at this point to make strong general conclusions about which tuning properties are critical for behavioral improvements. Many recent studies have shown spatial tuning (i.e. large but limited RF sizes) in the high-level visual cortex; hence, in the neural network observer model, it is not clear why the readout (linear classification) is performed on unit activity that has been summed fully across visual space.</p></disp-quote><p>We agree that it is important to consider models with some spatial tuning at the decision stage to more accurately model ventral temporal cortical inputs that are expected to have such tuning. We have added a model variant to address this issue and have arrived at similar conclusions, see [292-301] for details.</p><disp-quote content-type="editor-comment"><p>Now, it is an open interesting empirical question whether perceptual decisions from the visual system are based on fully spatially invariant units. But note that if that were the case, isn't one counterintuitive implication that our perceptual decisions can be corrupted by visual stimuli far away from the relevant decision zone?</p></disp-quote><p>Yes, we agree. We would clarify though that we are considering both visual processing and the decision stage [see 424-440]. At the decision stage, uncertainty over which visual signal to base a decision on is a problem that attention has been proposed to solve (e.g. Pelli, 1985). In this context, set size effects in which increasing the size of stimulus arrays across the visual field increases response times (even for covert search), can be viewed as the effect of uncertainty introduced by visual stimuli far from a search target. Or, for example in our own work, in which cueing one of four spatially separate targets improves contrast discrimination thresholds compared to distributing attention (Pestilli et al., 2011), even though distractors are far from the post-cued decision target.</p><disp-quote content-type="editor-comment"><p>If we entertain the possibility that the readout can be from units that have spatial specificity, then it remains to be seen what sort of modeling outcomes there might be.</p></disp-quote><p>Please see [309-319] where we show that the modeling outcomes for considering spatially specific responses does not qualitatively change our conclusions.</p><disp-quote content-type="editor-comment"><p>Even though the authors show, in the context of their particular model, that tuning shifts do not produce the required behavioral improvements, it seems that in general, it should be possible in theory for shifts to provide the required benefits. For example, if the focal cue is on stimuli in the upper right quadrant, if all receptive fields in the model covering the other quadrants were fully shifted to the upper right quadrant, wouldn't the readout from the model obviously improve?</p></disp-quote><p>We do agree that, in principle, tuning shifts could provide required benefits. We would like to clarify that we are considering the tuning shifts that occur because of downstream effects of increasing gain of a magnitude required to match human performance benefits with attention cueing. In our reframing of the manuscript (see below), we have substantially rewritten the text to clarify this particular point, and that our conclusion is *not* that tuning shifts of any magnitude won’t have behavioral effects [394-395 and 441-467], but is specific to those that occur with gain changes. We have also specifically tested the case pointed out above in which only receptive fields covering the target are considered and receptive field shifts of the magnitude found for the gain change are considered, We find that the magnitude of these changes is not enough to induce task performance benefits for the observer model. See [320-328].</p><disp-quote content-type="editor-comment"><p>With regards to the linear classifiers implemented by the authors, it seems there are many important details that need to get spelled out. Things like how exactly was the final predicted category obtained? Was a 20-way classification scheme used, or some winner-take-all scheme from 20 separate binary classifiers used? Was there any regularization used to fit the weights of the logistic regression? If not, how can we be confident that the amount of training data was sufficient? Details on the classification methods are important for interpreting how upstream computational changes impact the behavioral output from the model.</p></disp-quote><p>These details have now been added in the results [117-119] and methods [570-590]. We believe that the training data was sufficient because the validation set showed stable classification accuracy [118-119].</p><disp-quote content-type="editor-comment"><p>2. Reframing and/or exploring other models.</p><p>In light of the uncertainties about how general the conclusions can be from the modeling results, the authors may wish to consider refocusing/reframing their work. Instead of attempting to make strong general conclusions about the relationship between behavior and computational mechanisms, the author might reframe their work into more of a computational and theory-building exercise.</p><p>In particular, the authors show interesting insights regarding how a Gaussian applied to an early layer of a CNN can have effects that propagate through the network and which manifest as complex, rich RF tuning changes elsewhere in the model (e.g. as the authors state, &quot;RF shifts are a result of the signal gain…&quot;) (see also Compte Cerebral Cortex 2006). The distinction between architecture (internal computational mechanisms) and what might manifest at the level of input-output behavior of a unit is an important one, as the authors discuss (p. 17, lines 257-268). Certainly, a number of previous studies have adopted the latter mindset. Note that doing so is not necessarily inconsistent with the former &quot;mechanistic&quot; mindset (they are just different modeling stances one can take). Emphasizing and generating insights from adopting a &quot;mechanistic&quot; mindset is valuable research output.</p><p>Thus, one suggestion is to reframe the work to focus more on this insight and think about the theoretical ramifications (and/or concrete experimental predictions) that might stem from this. Related to this effort might be to consider other models. For example, it would be interesting to understand the similarities and differences between the 'attentional field' model (Reynolds, Heeger) and the current model. As another example, it would be interesting to consider models in which the &quot;Gaussian gain&quot; is applied at the end (deep layers) and effects propagate backwards, as opposed to being applied at the beginning and effects propagating forwards.</p></disp-quote><p>Thank you for this helpful suggestion. We have substantially modified the manuscript throughout to reframe as suggested. In particular, please note the new title, abstract, changes in the introduction [36-38, 49-52, 59-64] and re-written discussion. We also have extended our discussion of the alternate model (what we call propagated gain model) of adding gain at the last stages (end, deep layers) of the CNN [274-328]. The RH models consider the effects of gain in a similar fashion but only on a single layer model, which we now mention in the discussion [435-437]</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I have reviewed the revisions and the authors' responses.</p><p>Overall, the revisions are solid and substantial. The additions to the paper are useful, in particular, the new analyses exploring exactly how spatial information is included, restricted, or &quot;averaged over&quot;, which provide insights into the potential equivalency of the gain effects and the selection (or suppression) of irrelevant information. In addition, the new framing and discussion provide helpful clarity over the original manuscript and make for a more straightforward read. The computational modeling performed in this paper represents a novel and well-executed foray into a challenging area. While it does not necessarily definitively resolve the issues at stake, I believe it provides and demonstrates an interesting and non-trivial approach.</p><p>A few critical comments on the revisions:</p><p>The authors state, &quot;At larger scales or in other tasks there are theoretical reasons to expect that task performance will improve due to these effects&quot;. In this sentence, the referent of &quot;these effects&quot; is a bit unclear. Also, I am not sure what 'larger scales' refers to. Finally, it would be useful to give specific examples of what types of 'other tasks' might be implied by the sentence here.</p></disp-quote><p>We agree that these additional details help clarify this important discussion point. We have clarified the meaning of each term and specifically mention judgments of spatial position as an example of a task that should theoretically show a larger benefit from shift/shrinkage of receptive fields. [new text at lines 460-472].</p><disp-quote content-type="editor-comment"><p>In response to the reviews, the authors added some detail on the linear classifier methods, but the text is still not sufficient. More detail on exactly what was implemented seems important. For example, details on the &quot;under the curve&quot; quantification.</p></disp-quote><p>We have added additional detail to this section, explaining how the AUC is calculated and how it can be interpreted [lines 681-690].</p><disp-quote content-type="editor-comment"><p>In addition, perhaps there is a misspecification of the methods: As the authors state, &quot;Weights were fit using logistic regression with an L2 loss and no regularization, using scikit-learn and the LIBLINEAR package (Pedregosa et al., 2011).&quot; However, if I understand correctly, for each category, there are effectively 512 parameters that need to get learned and only 100 instances to train these parameters. Hence, if no regularization were used, this presumably would lead to a very noisy (overfit) solution. Perhaps the authors meant that they used L2 regularization (and the default settings for the regularization hyperparameter in the scikit-learn package)? Note that logistic regression implies a probabilistic formulation of the classification problem, and would seem to suggest that the authors don't actually mean that they used an L2 loss (which implies additive Gaussian noise). Hopefully, these important analysis choices and details can be clarified and do not have larger ramifications for the rest of the paper.</p></disp-quote><p>We thank the reviewer for raising these important concerns, which were partly due to a typo. We have fixed the typo to clarify that L2 regularization was used in training the output layers, specifically to reduce the risk of overfitting to the training data [Methods lines 673-680]. We have also made it more clear that the output layers were evaluated on a held-out validation set (median AUC 0.9), providing further evidence that the models are not overfit [Results lines 194-195] and we have copied this information into the methods to make it easier to find [Methods lines 679-680]</p></body></sub-article></article>