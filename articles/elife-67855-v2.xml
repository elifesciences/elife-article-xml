<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">67855</article-id><article-id pub-id-type="doi">10.7554/eLife.67855</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-228645"><name><surname>Goffinet</surname><given-names>Jack</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6729-0848</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-228646"><name><surname>Brudner</surname><given-names>Samuel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6043-9328</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-8890"><name><surname>Mooney</surname><given-names>Richard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3308-1367</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-157159"><name><surname>Pearson</surname><given-names>John</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9876-7837</contrib-id><email>john.pearson@duke.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Computer Science, Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Center for Cognitive Neurobiology, Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Neurobiology, Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Department of Biostatistics &amp; Bioinformatics, Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>Department of Electrical and Computer Engineering, Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewing Editor</role><aff><institution>Cornell University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>05</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e67855</elocation-id><history><date date-type="received" iso-8601-date="2021-02-24"><day>24</day><month>02</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-05-12"><day>12</day><month>05</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Goffinet et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Goffinet et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-67855-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-67855-figures-v2.pdf"/><abstract><p>Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-by-moment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-dimensional vocal behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>zebra finch</kwd><kwd>autoencoder</kwd><kwd>statistics</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH117778</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01-NS118424</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name><name><surname>Pearson</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01-DC013826</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name><name><surname>Pearson</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01-NS099288</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009633</institution-id><institution>Eunice Kennedy Shriver National Institute of Child Health and Human Development</institution></institution-wrap></funding-source><award-id>F31-HD098772</award-id><principal-award-recipient><name><surname>Brudner</surname><given-names>Samuel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Modern machine learning methods offer new techniques for analyzing complex vocal behavior like ultrasonic mouse calls and birdsong.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Quantifying the behavior of organisms is of central importance to a wide range of fields including ethology, linguistics, and neuroscience. Yet given the variety and complex temporal structure of many behaviors, finding concise yet informative descriptions has remained a challenge. Vocal behavior provides a paradigmatic example: audio data are notoriously high dimensional and complex, and despite intense interest from a number of fields, and significant progress, many aspects of vocal behavior remain poorly understood. A major goal of these various lines of inquiry has been to develop methods for the quantitative analysis of vocal behavior, and these efforts have resulted in several powerful approaches that enable the automatic or semi-automatic analysis of vocalizations (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>; <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Sainburg et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Tchernichovski et al., 2000</xref>; <xref ref-type="bibr" rid="bib31">Mandelblat-Cerf and Fee, 2014</xref>; <xref ref-type="bibr" rid="bib33">Mets and Brainard, 2018</xref>; <xref ref-type="bibr" rid="bib27">Kollmorgen et al., 2020</xref>; <xref ref-type="bibr" rid="bib19">Holy and Guo, 2005</xref>).</p><p>Key to this approach has been the existence of software packages that calculate acoustic features for each unit of vocalization, typically a syllable (<xref ref-type="bibr" rid="bib4">Burkett et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>; <xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Chabout et al., 2015</xref>). For example, Sound Analysis Pro (SAP), focused on birdsong, calculates 14 features for each syllable, including duration, spectral entropy, and goodness of pitch, and uses the set of resulting metrics as a basis for subsequent clustering and analysis (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>). More recently, MUPET and DeepSqueak have applied a similar approach to mouse vocalizations, with a focus on syllable clustering (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>). Collectively, these and similar software packages have helped facilitate numerous discoveries, including circadian patterns of song development in juvenile birds (<xref ref-type="bibr" rid="bib10">Derégnaucourt et al., 2005</xref>), cultural evolution among isolate zebra finches (<xref ref-type="bibr" rid="bib12">Fehér et al., 2009</xref>), and differences in ultrasonic vocalizations (USVs) between mouse strains (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>).</p><p>Despite these insights, this general approach suffers from several limitations: first, handpicked acoustic features are often highly correlated, and these correlations can result in redundant characterizations of vocalization. Second, an experimenter-driven approach may exclude features that are relevant for communicative function or, conversely, may emphasize features that are not salient or capture negligible variation in the data. Third, there is no diagnostic approach to determine when enough acoustic features have been collected: Could there be important variation in the vocalizations that the chosen features simply fail to capture? Lastly and most generally, committing to a syllable-level analysis necessitates a consistent definition of syllable boundaries, which is often difficult in practice. It limits the types of structure one can find in the data and is often difficult to relate to time series such as neural data, for which the relevant timescales are believed to be orders of magnitude faster than syllable rate.</p><p>Here, we address these shortcomings by applying a data-driven approach based on variational autoencoders (VAEs) (<xref ref-type="bibr" rid="bib25">Kingma and Welling, 2013</xref>; <xref ref-type="bibr" rid="bib39">Rezende et al., 2014</xref>) to the task of quantifying vocal behavior in two model species: the laboratory mouse (<italic>Mus musculus</italic>) and the zebra finch (<italic>Taeniopygia guttata</italic>). The VAE is an unsupervised modeling approach that learns from data of a pair of probabilistic maps, an ‘encoder’ and a ‘decoder,’ capable of compressing the data into a small number of latent variables while attempting to preserve as much information as possible. In doing so, it discovers features that best capture variability in the data, offering a nonlinear generalization of methods like Principal Components Analysis (PCA) and Independent Components Analysis (ICA) that adapts well to high-dimensional data like natural images (<xref ref-type="bibr" rid="bib8">Dai et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Higgins et al., 2017</xref>). By applying this technique to collections of single syllables, encoded as time-frequency spectrograms, we looked for latent spaces underlying vocal repertoires across individuals, strains, and species, asking whether these data-dependent features might reveal aspects of vocal behavior overlooked by traditional acoustic metrics and provide more principled means for assessing differences among these groups.</p><p>Our contributions are fourfold: first, we show that the VAE’s learned acoustic features outperform common sets of handpicked features in a variety of tasks, including capturing acoustic similarity, representing a well-studied effect of social context on zebra finch song, and comparing the USVs of different mouse strains. Second, using learned latent features, we report new results concerning both mice and zebra finches, including the finding that mouse USV syllables do not appear to cluster into distinct subtypes, as is commonly assumed, but rather form a broad continuum. Third, we present a novel approach to characterizing stereotyped vocal behavior that does not rely on syllable boundaries, one which we find is capable of quantifying subtle changes in behavioral variability on tens-of-milliseconds timescales. Lastly, we demonstrate that the VAE’s learned acoustic features accurately reflect the relationship between songbird tutors and pupils. In all, we show that data-derived acoustic features confirm and extend findings gained by existing approaches to vocal analysis and offer distinct advantages over handpicked acoustic features in several critical applications.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>VAEs learn a low-dimensional space of vocal features</title><p>We trained a VAE <xref ref-type="bibr" rid="bib25">Kingma and Welling, 2013</xref>; <xref ref-type="bibr" rid="bib39">Rezende et al., 2014</xref> to learn a probabilistic mapping between vocalizations and a latent feature space. Specifically, we mapped single-syllable spectrogram images (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>384</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> pixels) to vectors of latent features (<inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></inline-formula>) and back to the spectrogram space (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). As with most VAE methods, we parameterized both the encoder and decoder using convolutional neural networks, which provide useful inductive biases for representing regularly sampled data such as images or spectrograms. The two maps are jointly trained to maximize a lower bound on the probability of the data given the model (see Materials and methods). As in other latent variable models, we assume each observed spectrogram can be explained by an unobserved ‘latent’ variable situated in some ‘latent space.’ As visualized in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, the result is a continuous latent space that captures the complex geometry of vocalizations. Each point in this latent space represents a single spectrogram image, and trajectories in this latent space represent sequences of spectrograms that smoothly interpolate between start and end syllables (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Although we cannot visualize the full 32-dimensional latent space, methods like PCA and the UMAP algorithm <xref ref-type="bibr" rid="bib8">Dai et al., 2018</xref> allow us to communicate results in an informative and unsupervised way. The VAE training procedure can thus be seen as a compression algorithm that represents each spectrogram as a collection of 32 numbers describing data-derived vocal features. In what follows, we will show that these features outperform traditional handpicked features on a wide variety of analysis tasks.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Variational autoencoders (VAEs) learn a latent acoustic feature space.</title><p>(<bold>a</bold>) The VAE takes spectrograms as input (left column), maps them via a probabilistic ‘encoder’ to a vector of latent dimensions (middle column), and reconstructs a spectrogram via a ‘decoder’ (right column). The VAE attempts to ensure that these probabilistic maps match the original and reconstructed spectrograms as closely as possible. (<bold>b</bold>) The resulting latent vectors can then be visualized via dimensionality reduction techniques like principal components analysis. (<bold>c</bold>) Interpolations in latent space correspond to smooth syllable changes in spectrogram space. A series of points (dots) along a straight line in the inferred latent space is mapped, via the decoder, to a series of smoothly changing spectrograms (right). This correspondence between inferred features and realistic dimensions of variation is often observed when VAEs are applied to data like natural images (<xref ref-type="bibr" rid="bib25">Kingma and Welling, 2013</xref>; <xref ref-type="bibr" rid="bib39">Rezende et al., 2014</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Variational autoencoder network architecture.</title><p>The architecture outlined above was used for all training runs. The looping arrows at the right of the encoder and decoder denote repeated sequences of layer types, not recurrent connections. For training details, see Materials and methods. For implementation details, see <ext-link ext-link-type="uri" xlink:href="https://github.com/pearsonlab/autoencoded-vocal-analysis">https://github.com/pearsonlab/autoencoded-vocal-analysis</ext-link> (<xref ref-type="bibr" rid="bib14">Goffinet, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:55b90c22ca93685ad74019af02876995eea38daa;origin=https://github.com/pearsonlab/autoencoded-vocal-analysis;visit=swh:1:snp:c4354df2f3139cd46149d743296dcc324bf6b85b;anchor=swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904">swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904</ext-link>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Finally, we note that, while the VAE is compressive—that is, it discards some data—this is both necessary in practice and often desirable. First, necessity: as noted above, nearly all current methods reduce raw audio waveforms to a manageable number of features for purposes of analysis. This is driven in part by the desire to distill these complex sounds into a small collection of interpretable features, but it also stems from the needs of statistical testing, which suffers drastic loss of power for high-dimensional data without advanced methods. Second, this compression, as we will show, is often beneficial as it facilitates visualization and analyses of large collections of vocalizations in new ways. Thus, we view the VAE and its compression-based approach as complementary to both other dimension-reduction techniques like PCA and traditional acoustic signal processing as a method for learning structure from complex data.</p></sec><sec id="s2-2"><title>Learned features capture and expand upon typical acoustic features</title><p>Most previous approaches to analyzing vocalizations have focused on tabulating a predetermined set of features such as syllable duration or entropy variance that are used for subsequent processing and analysis (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>; <xref ref-type="bibr" rid="bib4">Burkett et al., 2015</xref>). We thus asked whether the VAE learned feature space simply recapitulated these known features or also captured new types of information missed by traditional acoustic metrics. To address the first question, we trained a VAE on a publicly available collection of mouse USVs (31,440 total syllables [<xref ref-type="bibr" rid="bib51">Van Segbroeck M et al., 2019</xref>]), inferred latent features for each syllable, and colored the results according to three acoustic features—frequency bandwidth, maximum frequency, and duration—calculated by the analysis program MUPET (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>). As <xref ref-type="fig" rid="fig2">Figure 2a–c</xref> shows, each acoustic feature appears to be encoded in a smooth gradient across the learned latent space, indicating that information about each has been preserved. In fact, when we quantified this pattern by asking how much variance in a wide variety of commonly used acoustic metrics could be accounted for by latent features (see Materials and methods), we found that values ranged from 64% to 95%, indicating that most or nearly all traditional features were captured by the latent space (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for individual acoustic features). Furthermore, we found that, when the analysis was reversed, commonly used acoustic features were not able to explain as much variance in the VAE latent features, indicating a prediction asymmetry between the two sets (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). That is, the learned features carry most of the information available in traditional features, as well as unique information missed by those metrics.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Learned acoustic features capture and expand upon traditional features.</title><p>(<bold>a–c</bold>) UMAP projections of latent descriptions of mouse ultrasonic vocalizations (USVs) colored by three traditional acoustic features. The smoothly varying colors show that these traditional acoustic features are represented by gradients within the latent feature space. (<bold>d</bold>) Many traditional features are highly correlated. When applied to the mouse USVs from (<bold>a</bold>) to (<bold>c</bold>), the acoustic features compiled by the analysis program MUPET have high correlations, effectively reducing the number of independent measurements made. (<bold>e</bold>) To better understand the representational capacity of traditional and latent acoustic features, we used each set of features to predict the other and vice versa (see Materials and methods). We find that, across software programs, the learned latent features were better able to predict the values of traditional features than vice versa, suggesting that they have a higher representational capacity. Central line indicates median, upper and lower box the 25th and 75th percentiles, respectively. Whiskers indicate 1.5 times the interquartile range. Feature vector dimensions: MUPET, 9; DeepSqueak, 10; SAP, 13; mouse latent, 7; zebra finch latent, 5. (<bold>f</bold>) As another test of representational capacity, we performed PCA on the feature vectors to determine the effective dimensionality of the space spanned by each set of features (see Materials and methods). We find in all cases that latent features require more principal components to account for the same portion of feature variance, evidence that latent features span a higher dimensional space than traditional features applied to the same datasets. Colors are as in (<bold>e</bold>). Latent features with colors labeled ‘MUPET’ and ‘DeepSqueak’ refer to the <italic>same</italic> set of latent features, truncated at different dimensions corresponding to the number of acoustic features measured by MUPET and DeepSqueak, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Variance explained by traditional and latent features.</title><p>Left column: named acoustic feature variance explained by latent features. Right column: latent acoustic feature variance explained by named acoustic features. Values reported are the results of k-nearest neighbor classification averaged over five shuffled test/train folds (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>The variational autoencoder (VAE) learns a parsimonious set of acoustic features.</title><p>When trained on mouse syllables (from <xref ref-type="fig" rid="fig2">Figure 2a</xref>), the VAE makes use of only 7 of 32 latent dimensions. When trained on zebra finch syllables (from <xref ref-type="fig" rid="fig5">Figure 5a</xref>), the VAE makes use of only 5 of 32 latent dimensions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Correlations among traditional and latent features.</title><p>Traditional acoustic features are highly correlated. Left column: pairwise absolute correlations between named acoustic features when applied to the datasets in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Right column: pairwise absolute correlations of latent features for the same datasets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Reproducibility of variational autoencoder (VAE) latent features.</title><p>The VAE produces similar latent features when retraining on identical data and disjoint splits of the same dataset. Top row: pairwise latent distances of 5000 random pairs of syllables under separately trained VAEs. Bottom row: the distribution of errors when predicting the latent means of one VAE from the other using linear regression. Errors are normalized relative to the root-mean-square (RMS) distance from the mean in latent space so that -1 corresponds to 10% of the RMS distance from the mean. For predicting a multivariate <italic>Y</italic> from a multivariate <italic>X</italic>, we report a multivariate <italic>R</italic><sup>2</sup> value: <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. first column: Two VAEs are trained on the set of mouse syllables from <xref ref-type="fig" rid="fig2">Figure 2a-c</xref>. Second column: Two VAEs are trained on the set of zebra finch syllables from <xref ref-type="fig" rid="fig4">Figure 4a-c</xref>. Third column: Two VAEs are trained on two disjoint halves of the mouse syllables from <xref ref-type="fig" rid="fig2">Figure 2a-c</xref>. Fourth column: Two VAEs are trained on two disjoint halves of the zebra finch syllables from <xref ref-type="fig" rid="fig4">Figure 4a-c</xref>. Note that retraining produces less consistent results for zebra finch syllables, possibly because the relative orientations and positions of well-separated clusters is underdetermined by the data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>The effect of time stretch and frequency spacing parameters.</title><p>Top row: two variational autoencoder (VAEs) are trained on the mouse ultrasonic vocalization (USV) syllables from <xref ref-type="fig" rid="fig4">Figure 4d</xref>, one with time stretching to expand the spectrograms of short syllables (see Materials and methods) and one without. When using the learned latent features from each model to predict the values of acoustic features calculated by MUPET and DeepSqueak, we observe a small but consistent performance gain using time stretching (left column). Additionally, we find a good correspondence between pairwise Euclidean distances in the two learned feature spaces (right column), indicating the two latent spaces have similar geometries. Middle row: repeating the same comparison with zebra finch song syllables from <xref ref-type="fig" rid="fig4">Figure 4a-c</xref>, we find fairly consistent pairwise distances across the two latent spaces (right) and no substantial effect on the performance of predicting acoustic features calculated by SAP (left). Bottom row: two VAEs trained on the same zebra finch song syllables, one with linearly spaced spectrogram frequency bins and the other with mel-spaced frequency bins, have fairly consistent pairwise distances across the two latent spaces (right) and no substantial effect on the performance of predicting acoustic features calculated by SAP (left).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Removing noise from single mouse ultrasonic vocalization (USV) recordings (see Recordings).</title><p>Above is a UMAP projection of all detected USV syllables. The false positives (red) cluster fairly well, so they were removed from further analysis. Of the 17,400 total syllables detected, 15,712 (blue) remained after removing the noise cluster.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig2-figsupp6-v2.tif"/></fig></fig-group><p>We thus attempted to compare the effective representational capacity of the VAE to current best approaches in terms of the dimensionalities of their respective feature spaces. We begin by noting that the VAE, although trained with a latent space of 32 dimensions, converges on a parsimonious representation that makes use of only 5–7 dimensions, with variance apportioned roughly equally between these (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>; <xref ref-type="bibr" rid="bib8">Dai et al., 2018</xref>). For the handpicked features, we normalized each feature independently by z-score to account for scale differences. For comparison purposes, we applied the same normalization step to the learned features, truncated the latent dimension to the number of handpicked features, and calculated the cumulative feature variance as a function of number of principal components (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). In such a plot, shallow linear curves are preferred since this indicates that variance is apportioned roughly equally among principal components and the effective dimensionality of the space is large. Equivalently, this means that the eigenvalue spectrum of the feature correlation matrix is close to the identity. As <xref ref-type="fig" rid="fig2">Figure 2f</xref> thus makes clear, the spaces spanned by the learned latent features have comparatively higher effective dimension than the spaces spanned by traditional features, suggesting that the learned features have a higher representational capacity. While the three software packages we tested (SAP [<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>], MUPET [<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>], DeepSqueak [<xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>]) measure upwards of 14 acoustic features per syllable, we find that these features often exhibit high correlations (<xref ref-type="fig" rid="fig2">Figure 2d</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), effectively reducing the number of independent measurements made. While correlations among features are not necessarily undesirable, they can complicate subsequent statistical testing because nonlinear relationships among features violate the assumptions of many statistical tests. VAE features, which allow for nonlinear warping, avoid this potential difficulty.</p><p>The degree to which the learned features capture novel information can also be demonstrated by considering their ability to encode a notion of spectrogram similarity since this is a typical use to which they are put in clustering algorithms (although see <xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref> for an alternative approach to clustering). We tested this by selecting query spectrograms and asking for the closest spectrograms as represented in both the DeepSqueak acoustic feature space and the VAE’s learned latent space. As <xref ref-type="fig" rid="fig3">Figure 3</xref> shows, DeepSqueak feature space often fails to return similar spectrograms, whereas the learned latent space reliably produces close matches (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for comparisons to metrics in spectrogram space, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for a representative sample using all feature sets, and <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> for more details on nearest neighbors returned by DeepSqueak feature space). This suggests that the learned features better characterize local variation in the data by more accurately arranging nearest neighbors.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Latent features better represent acoustic similarity.</title><p>Top row: example spectrograms; middle row: nearest neighbors in latent space; bottom row: nearest neighbors in DeepSqueak feature space.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Nearest neighbors returned by distance metrics in spectrogram space exhibit failure modes not found in latent space.</title><p>Top block: selected query spectrograms, their nearest neighbors in latent space (Euclidean metric), and their nearest neighbors in spectrogram space (<italic>L</italic><sub>1</sub> metric). Middle block: same comparison with the <italic>L</italic><sub>2</sub> metric in spectrogram space. Bottom block: same comparison with the cosine metric in spectrogram space.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Representative sample of nearest neighbors returned by several feature spaces.</title><p>Top block: given 20 random zebra finch syllable spectrograms, we find nearest neighbors in five feature spaces: variational autoencoder (VAE) latent space, Sound Analysis Pro feature space, and spectrogram space (Manhattan [L1], Euclidean [L2], and cosine metrics). All methods consistently find nearest neighbors of the same syllable type. Bottom block: given 20 random mouse syllable spectrograms, we find nearest neighbors in six feature spaces: VAE latent space, MUPET feature space, DeepSqueak feature space, and spectrogram space (Manhattan, Euclidean, and cosine metrics). Most methods return mostly similar spectrograms. However, latent features more consistently return good matches than other methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Investigating poor DeepSqueak feature nearest neighbors from <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Top row: selected query spectrograms from <xref ref-type="fig" rid="fig3">Figure 3</xref>. Lower rows: nearest neighbor spectrograms returned by various feature spaces: latent features, DeepSqueak features <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref> (standardized, but not whitened), the linear projection of DeepSqueak features that best predicts latent features, whitened DeepSqueak features, and the subset of DeepSqueak features excluding frequency standard deviation, sinuosity, and frequency modulation, the three features most poorly predicted by latent features (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). All three variants of the DeepSqueak feature set return more visually similar nearest neighbors than the original DeepSqueak feature set for some but not all query spectrograms. In particular, the remaining poor nearest neighbors returned by the linear projection of DeepSqueak features most predictive of latent features suggest that DeepSqueak features are insufficient to capture the full acoustic complexity of ultrasonic vocalization syllables.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig3-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Latent spaces facilitate comparisons between vocal repertoires</title><p>Many experimental designs require quantifying differences between sets of vocalizations. As a result, the ability of a feature set to distinguish between syllables, individuals, and groups poses a key test of the VAE-based approach. Here, we apply the VAE latent features to several comparison problems for which handpicked features are often used.</p><p>A common comparison in birdsong research is that between female-directed and undirected song. It is well-established that directed song is more stereotyped and slightly faster than undirected song (<xref ref-type="bibr" rid="bib46">Sossinka and Böhner, 1980</xref>). We thus asked whether the learned features could detect this effect. In <xref ref-type="fig" rid="fig4">Figure 4a</xref>, we plot the first two principal components of acoustic features calculated by the Sound Analysis Pro software package (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>) for both directed and undirected renditions of a single zebra finch song syllable. We note a generally diffuse arrangement and a subtle leftward bias in the directed syllables compared to the undirected syllables. <xref ref-type="fig" rid="fig4">Figure 4b</xref> displays the same syllables with respect to the first two principal components of the VAE’s latent features, showing a much more concentrated distribution of directed syllables relative to undirected syllables (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for all syllables). In fact, when we quantify this reduction of variability across all feature-space dimensions and song syllables (see Materials and methods), learned latent features consistently report greater variability reductions than SAP-generated features (<xref ref-type="fig" rid="fig4">Figure 4c</xref>; SAP: 0–20%, VAE: 27–37%), indicating that latent features are more sensitive to this effect. Additionally, we find that latent features outperform SAP features in the downstream tasks of predicting social context (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>), mouse strain (<xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>), and mouse identity (<xref ref-type="table" rid="app1table3">Appendix 1—table 3</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Latent features better capture differences in sets of vocalizations.</title><p>(<bold>a</bold>) The first two principal components in SAP feature space of a single zebra finch song syllable, showing differences in directed and undirected syllable distributions. (<bold>b</bold>) The first two principal components of latent syllable features, showing the same comparison. Learned latent features more clearly indicate differences between the two conditions by clustering directed syllables together. (<bold>c</bold>) Acoustic variability of each song syllable as measured by SAP features and latent features (see Methods). Latent features more clearly represent the constriction of variability in the directed context. Spectrogram scale bars denote 100ms. (<bold>d</bold>) A UMAP projection of the latent means of USV syllables from two strains of mice, showing clear differences in their vocal repertoires. (<bold>e</bold>) Similarity matrix between syllable repertoires for each of the 40 reccording sessions from (<bold>d</bold>). Lighter values correspond to more similar syllable repertoires (lower Maximum Mean Discrepancy (MMD)). (<bold>f</bold>) t-SNE representation of similarities between syllable repertoires, where distance metric is estimated MMD. The dataset, which is distinct from that represented in (d) and (e), contains 36 individuals, 118 recording sessions, and 156,180 total syllables. Color indicates individual mice and scatterpoints of the same color represent repertoires recorded on different days. Distances between points represent the similarity in vocal repertoires, with closer points more similar. We note that the major source of repertoire variability corresponds to genetic background, corresponding to the two distinct clusters (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). A smaller level of variability can be seen across individuals in the same clusters. Individual mice have repertoires with even less variability, indicated by the close proximity of most repertoires for each mouse.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Latent features better represent constricted variability of female-directed zebra finch song.</title><p>At top is a single rendition of a male zebra finch’s song motif, with individual syllables labeled (A–F). The top row of scatterplots shows each syllable over many directed (blue) and undirected (purple) renditions, plotted with respect to the first two principal components of the Sound Analysis Pro acoustic feature space. The bottom row of scatterplots shows the same syllables plotted with respect to the first two principal components of latent feature space. The difference in distributions between the two social contexts is displayed more clearly in the latent feature space, especially for non-harmonic syllables (D–F).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>An ‘atlas’ of mouse ultrasonic vocalizations (USVs).</title><p>This screenshot shows an interactive version of <xref ref-type="fig" rid="fig4">Figure 4d</xref> in which example spectrograms are displayed as tooltips when a cursor hovers over the plot. A version of this plot is hosted at: <ext-link ext-link-type="uri" xlink:href="https://pearsonlab.github.io/research.html#mouse_tooltip">https://pearsonlab.github.io/research.html#mouse_tooltip</ext-link>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Details of <xref ref-type="fig" rid="fig4">Figure 4f</xref>.</title><p>(a) t-SNE representation of similarities between syllable repertoires, where distance metric is estimated maximum mean discrepancy (MMD) between latent syllable distributions (reproduction of <xref ref-type="fig" rid="fig4">Figure 4f</xref>). Each scatter represents the ultrasonic vocalization (USV) syllable repertoire of a single recording session. Recordings of the same mice across different days are connected and colored identically. Distances between points represent the similarity in vocal repertoires, with closer points more similar. Note that most mice have similar repertoires across days, indicated by the close proximity of connected scatterpoints. (<bold>b</bold>) The same plot as (<bold>a</bold>), colored by the genetic background of each mouse. Note that the two primary clusters of USV syllable repertoires correspond to two distinct sets of genetic backgrounds. (<bold>c</bold>) The full pairwise MMD matrix between USV repertoires from individual recording sessions. The dataset contains 36 individuals, 118 recording sessions, and 156,180 total syllables. The two main clusters separating the PV:Cre and Chat:Cre mice from the other backgrounds are apparent as the large two-by-two checkerboard pattern. Colors at the top and left sides indicate individual and genetic background information, with colors matching those in panels (<bold>a</bold>) and (<bold>b</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig4-figsupp3-v2.tif"/></fig></fig-group><p>Similarly, we can ask whether latent features are able to capture differences between groups of individuals. In <xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>, the authors compared USVs of 12 strains of mice using a clustering-based approach. Here, we perform an alternative version of this analysis using two mouse strains (C57/BL6 and DBA/2) from a publicly available dataset that were included in this earlier study. <xref ref-type="fig" rid="fig4">Figure 4d</xref> shows a UMAP projection of the 31,440 detected syllables, colored by mouse strain. Visualized with UMAP, clear differences between the USV distributions are apparent. While in contrast to traditional acoustic features such as ‘mean frequency’ individual VAE latent features (vector components) are generally less interpretable, when taken together with an ‘atlas’ of USV shapes derived from this visualization (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), we can develop an intuitive understanding of the differences between the USVs of the two strains: the C57 mice mostly produce noisy USVs, while the DBA mice produce a much greater variety, including many short low-frequency syllables that C57s rarely produce.</p><p>Given these results, we asked whether these strain differences are evident at the level of individual 6.5 min recording sessions. To compare distributions of syllables without making restrictive parametric assumptions, we employed maximum mean discrepancy (MMD), a difference measure between pairs of distributions (<xref ref-type="bibr" rid="bib15">Gretton et al., 2012</xref>). We estimated MMD between the distributions of latent syllable encodings for each pair of recording sessions (see Materials and methods) and visualized the result as a distance matrix (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). Here, lighter values indicate more similar syllable repertoires. We note that, in general, values are brighter when comparing repertoires within strains than when comparing across strains, consistent with the hypothesis of inter-strain differences. We also note some substructure, including a well-defined cluster within the C57 block (annotated).</p><p>Finally, we used a much larger library of female-directed mouse USVs (36 individuals, 2–4 20-min recording sessions each, 40 total hours of audio, 156,000 syllables) to investigate the diversity and stability of syllable repertoires. We repeated the above procedure, estimating MMD for each pair of recording sessions (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>), and then computed a t-distributed Stochastic Neighbor Embedding t-SNE layout of the recording sessions with estimated MMD as the distance metric to visualize the distribution of syllable repertoires (see Materials and methods). In <xref ref-type="fig" rid="fig4">Figure 4f</xref>, each recording session is represented by a scatterpoint, and recordings of the same individual are connected and displayed in the same color. We note an overall organization of syllables into two clusters, corresponding to the genetic backgrounds of the mice (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). Furthermore, we note that almost all recordings of the same individuals are co-localized, indicating that within-subject differences in syllable repertoire are smaller than those between individuals. Although it has been previously shown that a deep convolutional neural network can be trained to classify USV syllables according to mouse identity with good accuracy (<xref ref-type="bibr" rid="bib20">Ivanenko et al., 2020</xref>, Figure S1), here we find that repertoire features learned in a wholly unsupervised fashion achieve similar results, indicating that mice produce individually stereotyped, stable vocal repertoires.</p></sec><sec id="s2-4"><title>Latent features fail to support cluster substructure in USVs</title><p>Above, we have shown that, by mapping complex sets of vocalizations to low-dimensional latent representations, VAEs allow us to visualize the relationships among elements in mouse vocal repertoires. The same is likewise true for songbirds such as the zebra finch, <italic>T. guttata</italic>. <xref ref-type="fig" rid="fig5">Figure 5</xref> compares the geometry of learned latent spaces for an individual of each species as visualized via UMAP. As expected, the finch latent space exhibits well-delineated clusters corresponding to song syllables (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). However, as seen above, mouse USVs clump together in a single quasi-continuous mass (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This raises a puzzle since the clustering of mouse vocalizations is often considered well-established in the literature (<xref ref-type="bibr" rid="bib19">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib4">Burkett et al., 2015</xref>; <xref ref-type="bibr" rid="bib54">Woehr, 2014</xref>; <xref ref-type="bibr" rid="bib6">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Hertz et al., 2020</xref>) and is assumed in most other analyses of these data (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>). Clusters of mouse USVs are used to assess differences across strains (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>), social contexts (<xref ref-type="bibr" rid="bib6">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Hammerschmidt et al., 2012</xref>), and genotypes (<xref ref-type="bibr" rid="bib13">Gaub et al., 2010</xref>), and the study of transition models among clusters of syllables has given rise to models of syllable sequences that do not readily extend to the nonclustered case (<xref ref-type="bibr" rid="bib19">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib6">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Hertz et al., 2020</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Bird syllables clearly cluster, but mouse ultrasonic vocalizations (USVs) do not.</title><p>(<bold>a</bold>) UMAP projection of the song syllables of a single male zebra finch (14,270 syllables). (<bold>b</bold>) UMAP projection of the USV syllables of a single male mouse (17,400 syllables). (<bold>c</bold>) The same UMAP projection as in (<bold>b</bold>), colored by MUPET-assigned labels. (<bold>d</bold>) Mean silhouette coefficient (an unsupervised clustering metric) for latent descriptions of zebra finch song syllables and mouse syllables. The dotted line indicates the null hypothesis of a single covariance-matched Gaussian noise cluster fit by the same algorithm. Each scatterpoint indicates a cross-validation fold, and scores are plotted as differences from the null model. Higher scores indicate more clustering. (<bold>e</bold>) Interpolations (horizontal series) between distinct USV shapes (left and right edges) demonstrating the lack of data gaps between putative USV clusters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Evaluation of clustering metrics on vocalization for different cluster numbers.</title><p>Three unsupervised clustering metrics evaluated on the latent description of zebra finch song syllables (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) and mouse ultrasonic vocalization syllables (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) as the number of components, <inline-formula><mml:math id="inf4"><mml:mi>k</mml:mi></mml:math></inline-formula>, varies from 2 to 12. Clustering metrics are reported relative to moment-matched Gaussian noise (see Materials and methods) with a possible sign change so that higher scores indicate more clustering.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Evaluation of clustering metrics on different mouse ultrasonic vocalization (USV) feature sets.</title><p>Three unsupervised clustering metrics evaluated on latent and MUPET features of mouse USV syllables (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) as the number of components, <italic>k</italic>, varies from 2 to 12. Clustering metrics are reported relative to moment-matched Gaussian noise (see Materials and methods) with a possible sign change so that higher scores indicate more clustering. Latent features are consistently judged to produce better clustering than MUPET feature. Additionally, MUPET features are consistently judged to be less clustered than moment-matched Gaussian noise. Compare to <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Evaluation of clustering metrics on different zebra finch syllable feature sets.</title><p>Three unsupervised clustering metrics evaluated on latent and SAP features of zebra finch song syllables (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) as the number of components, <italic>k</italic>, varies from 2 to 12. Clustering metrics are reported relative to moment-matched Gaussian noise (see Materials and methods) with a possible sign change so that higher scores indicate more clustering. Both feature sets admit clusters that are consistently judged to be more clustered than moment-matched Gaussian noise.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Reliability of clustering for zebra finch syllables and mouse ultrasonic vocalizations (USVs).</title><p>Repeated clustering with Gaussian mixture models (GMMs) produces reliable clusters for zebra finch syllable latent features with six clusters, but not for mouse syllable latent features with more than two clusters. Both sets of syllable latent descriptions (zebra finch, 14,270 syllables; mouse, 15,712 syllables) are repeatedly split into thirds. The first and second splits are used to train GMMs (full covariance, best of five fits, fit via expectation maximization), which are used to predict labels on the third split. Given these predicted labels and a matching of labels between the two GMMs, a syllable can be considered consistently labeled if it is assigned the same label class by the two GMMs. The Hungarian method is used to find the label matching that maximizes the portion of consistently labeled syllables. Top row: the portion of consistently labeled syllables for 20 repetitions of this procedure is shown for varying numbers of clusters, <italic>k</italic>. Note that zebra finch syllables achieve near-perfect consistency for six clusters, the number of clusters found by hand labeling (syllables A–F in <xref ref-type="fig" rid="fig6">Figure 6c</xref>), and this consistency degrades with more clusters. By contrast, the consistency of mouse USV clusters is poor. Somewhat surprisingly, the clustering is very consistent for two clusters (<italic>k</italic> = 2). To test whether this is a trivial effect of rarely used clusters, we calculated the entropy of the empirical label distributions (bottom row). We find in each case, and specifically the case, that the empirical distribution entropy is close to the maximum possible entropy (plotted as dashed horizontal lines), indicating that all clusters are frequently used. While consistent with mouse USVs forming two clusters, this result is not sufficient proof of syllable clustering or even bimodality. Yet, for cluster identity to be a practical syllable descriptor, clusters should be readily identifiable from the data. Thus the combination of VAE-identified latent features and Gaussian clusters does not appear to be a suitable description of mouse USVs for <italic>k</italic> &gt; 2 clusters. Note that the portion of consistently labeled zebra finch syllables for <italic>k</italic> &lt; 6 clusters form well-defined bands at multiples of <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>6</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, indicating that cluster structure is so well defined by the data that the GMMs do not split individual clusters across multiple Gaussian components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig5-figsupp4-v2.tif"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 5.</label><caption><title>Absence of continuous interpolations between zebra finch song syllables.</title><p>Each row displays two random zebra finch syllables of different syllable types at either end and an attempted smooth interpolation between the two. Interpolating spectrograms are those with the closest latent features along a linear interpolation in latent space. Note the discontinuous jump in each attempted interpolation, which is expected given that adult zebra finch syllables are believed to be well-clustered. Compare with <xref ref-type="fig" rid="fig5">Figure 5e</xref>, which shows continuous variation in mouse ultrasonic vocalizations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig5-figsupp5-v2.tif"/></fig></fig-group><p>We therefore asked whether mouse USVs do, in fact, cluster or whether, as the latent space projection suggests, they form a single continuum. In principle, this is impossible to answer definitively because, without the benefit of ground truth labels, clustering is an unsupervised classification task. Moreover, there is little consensus among researchers as to the best method for assessing clustering and where the cutoff between clustered and nonclustered data lies (<xref ref-type="bibr" rid="bib21">Jain et al., 1999</xref>). In practice, new clustering algorithms are held to function well when they outperform previous approaches and produce sensible results on data widely agreed on to be clustered. Thus, while it is clear that zebra finch song syllables should be and are clustered by the VAE (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), we can only ask whether clustering is a more or less satisfying account of the mouse data in <xref ref-type="fig" rid="fig5">Figure 5b</xref>.</p><p>To address this question, we performed a series of analyses to examine the clustering hypothesis from complementary angles. First, we asked how clusters detected by other analysis approaches correspond to regions in the latent space. As shown in <xref ref-type="fig" rid="fig5">Figure 5c</xref>, clusters detected by MUPET roughly correspond to regions of the UMAP projection, with some overlap between clusters (e.g., purple and blue clusters) and some noncontiguity of single clusters (red and orange clusters). That is, even though clusters do broadly label different subsets of syllables, they also appear to substantially bleed into one another, unlike the finch song syllables in <xref ref-type="fig" rid="fig5">Figure 5a</xref>. However, it might be objected that <xref ref-type="fig" rid="fig5">Figure 5b</xref> displays the UMAP projection, which only attempts to preserve local relationships between nearest neighbors and is not to be read as an accurate representation of the latent geometry. Might the lack of apparent clusters result from distortions produced by the projection to two dimensions? To test this, we calculated several unsupervised clustering metrics on full, unprojected latent descriptions of zebra finch and mouse syllables. By these measures, both bird syllables and mouse USVs were more clustered than moment-matched samples of Gaussian noise, a simple null hypothesis, but mouse USVs were closer to the null than to birdsong on multiple goodness-of-clustering metrics (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Additionally, we find that MUPET acoustic features admit uniformly poorer clusters than latent features as quantified by the same metrics (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplements 2</xref> and <xref ref-type="fig" rid="fig5s3">3</xref>). On a more practical note, we compared the consistency of cluster labels assigned by Gaussian mixture models (GMMs) trained on disjoint subsets of zebra finch and mouse latent syllable descriptions to determine how well the structure of the data determines cluster membership across repeated fittings (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>). We find near-perfectly consistent assignments of zebra finch syllables into six clusters and much less consistent clusters for mouse syllables for more than two clusters. Finally, we tested whether the data contained noticeable gaps between syllables in different clusters. If syllable clusters are well-defined, there should not exist smooth sequences of datapoints connecting distinct examples. However, we find that even the most acoustically disparate syllables can be connected with a sequence of syllables exhibiting more-or-less smooth acoustic variation (<xref ref-type="fig" rid="fig5">Figure 5e</xref>), in contrast to zebra finch syllables (<xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>). Thus, even though clustering may not constitute the best account of mouse USV syllable structure, learned latent features provide useful tools to both explore and quantify the acoustic variation within and across species.</p></sec><sec id="s2-5"><title>Measuring acoustic variability over tens of milliseconds</title><p>The results above have shown that data-derived latent features represent more information about syllables than traditional metrics and can successfully capture differences within and between individuals and groups. Here, we consider how a related approach can also shed light on the short-time substructure of vocal behavior.</p><p>The analysis of syllables and other discrete segments of time is limited in at least two ways. First, timing information, such as the lengths of gaps between syllables, is ignored. Second, experimenters must choose the unit of analysis (syllable, song motif, bout), which has a significant impact on the sorts of structure that can be identified (<xref ref-type="bibr" rid="bib23">Kershenbaum et al., 2016</xref>). In an attempt to avoid these limitations, we pursued a complementary approach, using the VAE to infer latent descriptions of fixed duration audio segments, irrespective of syllable boundaries. Similar to the shotgun approach to gene sequencing (<xref ref-type="bibr" rid="bib52">Venter et al., 1998</xref>) and a related method of neural connectivity inference (<xref ref-type="bibr" rid="bib47">Soudry et al., 2015</xref>), we trained the VAE on randomly sampled segments of audio, requiring that it learn latent descriptions sufficient to characterize any given time window during the recording. That is, this ‘shotgun-VAE’ approach encouraged the autoencoder to find latent features sufficient to ‘glue’ continuous sequences back together from randomly sampled audio snippets.</p><p><xref ref-type="fig" rid="fig6">Figure 6a</xref> shows a UMAP projection of latent features inferred from fixed-duration segments from a subset of the mouse USVs shown in <xref ref-type="fig" rid="fig5">Figure 5b</xref>. While this projection does display some structure (silence on the right, shorter to longer syllables arranged from right to left), there is no evidence of stereotyped sequential structure (see also <xref ref-type="video" rid="fig6video1">Figure 6—videos 1</xref> and <xref ref-type="video" rid="fig6video2">2</xref>). In contrast, <xref ref-type="fig" rid="fig6">Figure 6b</xref> shows the same technique applied to bouts of zebra finch song, with the song represented as a single well-defined strand coursing clockwise from the bottom to the top left of the projection. Other notable features are the loop on the left containing repeated, highly variable introductory notes that precede and often join song renditions and a ‘linking note’ that sometimes joins song motifs. Most importantly, such a view of the data clearly illustrates not only stereotypy but variability: introductory notes are highly variable, but so are particular syllables (<bold>B, E</bold>) in contrast to others (<bold>C, F</bold>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>A shotgun variational autoencoder approach learns low dimensional latent representations of subsampled, fixed-duration spectrograms and captures short-timescale variability in behavior.</title><p>(<bold>a</bold>) A UMAP projection of 100,000 200-ms windows of mouse ultrasonic vocalizations (cp. <xref ref-type="fig" rid="fig4">Figure 4a</xref>). (<bold>b</bold>) A UMAP projection of 100,000 120-ms windows of zebra finch song (cp. <xref ref-type="fig" rid="fig4">Figure 4b</xref>). Song progresses counterclockwise on the right side, while more variable, repeated introductory notes form a loop on the left side. (<bold>c</bold>) A single rendition of the song in (<bold>b</bold>). (<bold>d</bold>) The song’s first principal component in latent space, showing both directed (cyan) and undirected (purple) renditions. (<bold>e</bold>) In contrast to a syllable-level analysis, the shotgun approach can measure zebra finch song variability in continuous time. Song variability in both directed (cyan) and undirected (purple) contexts is plotted (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Effect of window duration on shotgun variational autoencoder (VAE).</title><p>Qualitatively similar shotgun VAE latent projections are achieved with a wide range of window durations. (<bold>a</bold>–<bold>d</bold>) UMAP projections of 100,000 windows of mouse ultrasonic vocalizations, with window durations of 100, 200, 300, and 400 ms. Compare with <xref ref-type="fig" rid="fig6">Figure 6a</xref>. (<bold>e</bold>–<bold>h</bold>) UMAP projections of 100,000 windows of zebra finch song motifs, with window durations of 60, 120, 180, and 240 ms. Compare with <xref ref-type="fig" rid="fig6">Figure 6b</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Effect of time warping on shotgun variational autoencoder.</title><p>Non-timewarped version of <xref ref-type="fig" rid="fig6">Figure 6c-e</xref>. As in <xref ref-type="fig" rid="fig6">Figure 6d</xref>, there is reduced variability in the first latent principal component for directed song compared to undirected song. The overall variability reduction is quantified by the variability index (see Materials and methods), reproducing the reduced variability of directed song found in <xref ref-type="fig" rid="fig6">Figure 6e</xref>. Note that the directed traces lag the undirected traces at the beginning of the motif and lead at the end of the motif due to their faster tempo, which is uncorrected in this version of the analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig6-figsupp2-v2.tif"/></fig><media id="fig6video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-67855-fig6-video1.mp4"><label>Figure 6—video 1.</label><caption><title>An animated version of <xref ref-type="fig" rid="fig6">Figure 6b</xref>.</title><p>Recorded USVs from asingle male mouse are played while the corresponding latent features are visualized by a moving star in a UMAP projection of latent space. The recording is slowed by a factor of four and additionally pitchshifted downward by a factor of two so that the vocalizations areaudible.</p></caption></media><media id="fig6video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-67855-fig6-video2.mp4"><label>Figure 6—video 2.</label><caption><title>An animated version of <xref ref-type="fig" rid="fig6">Figure 6a</xref>.</title><p>A recorded song boutfrom a single male zebra finch is played while the corresponding latent features are visualized by a moving star in a UMAP projectionof latent space.</p></caption></media></fig-group><p>Following this, we asked whether the shotgun VAE method could be used to assess the phenomenon of reduced variability in directed birdsong (<xref ref-type="bibr" rid="bib46">Sossinka and Böhner, 1980</xref>). We examined the song portion of <xref ref-type="fig" rid="fig6">Figure 6b</xref> in both directed and undirected conditions, warping each in time to account for well-documented differences in rhythm and tempo. We then trained a VAE on randomly sampled 80 ms portions of the warped spectrograms. As a plot of the first principal component of the latent space shows (<xref ref-type="fig" rid="fig4">Figure 4d</xref>), the VAE is able to recover the expected reduction in directed song variability on a tens-of-milliseconds timescale relevant to the hypothesized neural underpinnings of the effect (<xref ref-type="bibr" rid="bib11">Fee and Goldberg, 2011</xref>). This result recapitulates similar analyses that have focused on harmonic and tonal syllables like A and B in <xref ref-type="fig" rid="fig4">Figure 4c</xref> (<xref ref-type="bibr" rid="bib22">Kao and Brainard, 2006</xref>), but the shotgun VAE method is applicable to all syllables, yielding a continuous estimate of song variability (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). Thus, not only do VAE-derived latent features capture structural properties of syllable repertoires, the shotgun VAE approach serves to characterize continuous vocal dynamics as well.</p></sec><sec id="s2-6"><title>Latent features capture song similarity</title><p>Above, we saw how a subsampling-based ‘shotgun VAE’ approach can capture fine details of zebra finch vocal behavior modulated by social context. However, a principal reason songbirds are studied is their astonishing ability to copy song. A young male zebra finch can successfully learn to sing the song of an older male over the course of multiple months after hearing only a handful of song renditions. At least three methods exist for quantifying the quality of song learning outcomes, with two using handpicked acoustic features (<xref ref-type="bibr" rid="bib48">Tchernichovski et al., 2000</xref>; <xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>; <xref ref-type="bibr" rid="bib31">Mandelblat-Cerf and Fee, 2014</xref>) and another using Gaussian distributions in a similarity space based on power spectral densities to represent syllable categories (<xref ref-type="bibr" rid="bib33">Mets and Brainard, 2018</xref>). We reiterate that handpicked acoustic features are sensitive to experimenter choices, with some acoustic features like pitch only defined for certain kinds of sounds. Additionally, restrictive parametric assumptions limit the potential sensitivity of a method. By contrast, the VAE learns a compact feature representation of complete spectrograms and MMD provides a convenient nonparametric difference measure between distributions. Therefore, as a final assessment of the VAE’s learned features, we asked whether latent features reflect the similarity of tutor and pupil songs.</p><p>To assess the VAE’s ability to capture effects of song learning, we trained a syllable VAE and a shotgun VAE on song motifs of 10 paired adult zebra finch tutors and adult zebra finch pupils. As <xref ref-type="fig" rid="fig7">Figure 7a</xref> shows for the syllable-level analysis, most syllables form distinct clusters in a latent UMAP embedding, with many tutor/pupil syllable pairs sharing a cluster (<xref ref-type="video" rid="fig7video1">Figure 7—video 1</xref>). For a specific tutor/pupil pair shown in <xref ref-type="fig" rid="fig7">Figure 7b</xref>, we highlight three corresponding syllables from the two motifs. The first and third syllables (C and E) are well-copied from tutor to pupil, but the pupil’s copy of the second syllable (syllable D) does not contain the high-frequency power present in the first half of the tutor’s syllable. This discrepancy is reflected in the latent embedding, with the two versions of syllable D corresponding to distinct clusters. We quantified the quality of copying by calculating MMD between each pair of tutor and pupil syllables, shown in <xref ref-type="fig" rid="fig7">Figure 7c</xref>. A dark band of low MMD values along the diagonal indicates well-copied syllables and syllable orderings for most birds.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Latent features capture similarities between tutor and pupil song.</title><p>(<bold>a</bold>) Latent UMAP projection of the song syllables of 10 zebra finch tutor/pupil pairs. Note that many tutor and pupil syllables cluster together, indicating song copying. (<bold>b</bold>) Example song motifs from one tutor/pupil pair. Letters A-E indicate syllables. Syllables Syllables “C” and “E” are well-copied, but the pupil’s rendition of syllable “D” does not have as much high-frequency power as the tutor’s rendition. The difference between these two renditions is captured in the latent UMAP projection below. Additionally, the pupil sings a concatenated version of the tutor’s syllables “A” and “B,” a regularity that a syllable-level analysis cannot capture. Thus, the pupil’s syllable “AB” does not appear near tutor syllable “B” in the UMAP projection. Scale bar denotes 100ms. (<bold>c</bold>) Quality of song copying, estimated by maximum mean discrepancy (MMD) between every pair of tutor and pupil syllables. The dark band of low MMD values near the diagonal indicates good song copying. Syllables are shown in motif order. (<bold>d</bold>) Latent UMAP projection of shotgun VAE latents (60ms windows) for the song motifs of 10 zebra finch tutor/pupil pairs. Song copying is captured by the extent to which pupil and tutor strands co-localize. (<bold>e</bold>) Top: Detail of the UMAP projection in panel d shows a temporary split in pupil and tutor song strands spanning the beginning of syllable “D” with poorly copied high-frequency power. Labeled points correspond to the motif fragments marked in panel b. Bottom: Details of the syllable and shotgun MMD matrices in panels c and f. Note high MMD values for the poorly copied “D” syllable. Additionally, the syllable-level analysis reports high MMD betwen the pupil’s fused “AB” and syllable and tutor’s“A” and “B” syllables, though the shotgun VAE approach reports high similarity (low MMD) between pupil andtutor throughout these syllables. (<bold>f</bold>) MMD between pupil and tutor shotgun VAE latents indexed by continuous-valued time-in-motif quantifies song copying on fine timescales. The dark bands near the diagonal indicate well-copied stretches of song. The deviation of MMD values from a rank-one matrix is displayed for visual clarity (see <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> for details). Best viewed zoomed in.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Effect of modified UMAP distance matrix on shotgun variational autoencoder (VAE) from <xref ref-type="fig" rid="fig7">Figure 7d,e</xref>.</title><p>(<bold>a</bold>) A UMAP projection of shotgun VAE latent means with a standard Euclidean metric. Colors represent birds as in <xref ref-type="fig" rid="fig7">Figure 7</xref>. (<bold>b</bold>) A UMAP projection of the same latent means with a modified metric to discourage strands from the same motif rendition from splitting (see Materials and methods). This is a reproduction of <xref ref-type="fig" rid="fig7">Figure 7d</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Full and rank-one estimates of maximum mean discrepancy (MMD) matrix.</title><p>For the shotgun variational autoencoder pupil/tutor analysis presented in <xref ref-type="fig" rid="fig7">Figure 7</xref>, an MMD matrix (<bold>a</bold>) is decomposed into a rank-one component (<bold>b</bold>) and a residual component (<bold>c</bold>). The residual component is shown in <xref ref-type="fig" rid="fig7">Figure 7f</xref> for visual clarity. The decomposition is performed by MAP estimation assuming a flat prior on the rank-one matrix and independent Laplace errors on the residual component.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-67855-fig7-figsupp2-v2.tif"/></fig><media id="fig7video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-67855-fig7-video1.mp4"><label>Figure 7—video 1.</label><caption><title>An animated version of <xref ref-type="fig" rid="fig7">Figure 7d</xref>.</title></caption></media></fig-group><p>To complement the previous analysis, we performed an analogous analysis using the shotgun VAE approach. After training the VAE on 60 ms chunks of audio drawn randomly from each song motif, we projected the learned latent features into two dimensions using UMAP (<xref ref-type="fig" rid="fig7">Figure 7d</xref>) with a modified distance to prevent motif ‘strands’ from breaking (see Materials and methods, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). As expected, we see a close correspondence between pupil and tutor motif strands, with only a single tutor/pupil pair (pink) not overlapping. In fact, out of the roughly 3.6 million possible pairings of pupils and tutors, it is easily apparent which is the true pairing. Additionally, we find that the finer details are informative. For instance, the poorly copied syllable ‘D’ from <xref ref-type="fig" rid="fig7">Figure 7b</xref> corresponds to a temporary divergence of the pupil strand from the tutor strand, reflected by large MMD values for both shotgun and syllable analyses (<xref ref-type="fig" rid="fig7">Figure 7e</xref>). Additionally, we find that the shotgun VAE approach accurately judges the pupil’s fused ‘AB’ syllable to be similar to the tutor’s ‘A’ and ‘B’ syllables, in contrast to the syllable-level analysis (<xref ref-type="fig" rid="fig7">Figure 7e</xref>, bottom). We quantified the quality of copying in continuous time for all tutor/pupil pairs by calculating MMD between the distributions of song latents corresponding to distinct times within motifs. Deviations from a simple rank-one structure are shown in <xref ref-type="fig" rid="fig7">Figure 7f</xref> (see <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> for the original MMD matrix). Consistent with <xref ref-type="fig" rid="fig7">Figure 7c</xref>, a dark band near the diagonal indicates good copying, quantifying the quality of copying in much more detail than a syllable-level analysis could provide.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The complexity and high dimensionality of vocal behavior have posed a persistent challenge to the scientific study of animal vocalization. In particular, comparisons of vocalizations across time, individuals, groups, and experimental conditions require some means of characterizing the similarity of selected groups of vocal behaviors. Feature vector-based approaches and widespread software tools have gone a long way toward addressing this challenge and providing meaningful scientific insights, but the reliance of these methods on handpicked features leaves open the question of whether other feature sets might better characterize vocal behavior.</p><p>Here, we adopt a data-driven approach, demonstrating that features learned by the VAE, an unsupervised learning method, outperform frequently used acoustic features across a variety of common analysis tasks. As we have shown, these learned features are both more parsimonious (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), capture more variability in the data (<xref ref-type="fig" rid="fig2">Figure 2e and f</xref>), and better characterize vocalizations as judged by nearest neighbor similarity (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Moreover, these features easily facilitate comparisons across sessions (<xref ref-type="fig" rid="fig4">Figure 4f</xref>), social contexts (<xref ref-type="fig" rid="fig4">Figure 4a–c</xref>), and individuals (<xref ref-type="fig" rid="fig4">Figures 4d–f</xref>–<xref ref-type="fig" rid="fig7">7</xref>), quantifying not only differences in mean vocal behavior (<xref ref-type="fig" rid="fig4">Figure 4d</xref>), but also in vocal variability (<xref ref-type="fig" rid="fig4">Figure 4c</xref>).</p><p>This data-driven approach is closely related to previous studies that have applied dimensionality reduction algorithms (UMAP [<xref ref-type="bibr" rid="bib32">McInnes et al., 2018</xref>] and t-SNE [<xref ref-type="bibr" rid="bib30">Lvd and Hinton, 2008</xref>]) to spectrograms to aid in syllable clustering of birdsong (<xref ref-type="bibr" rid="bib42">Sainburg et al., 2019</xref>) and visualize juvenile song learning in the zebra finch (<xref ref-type="bibr" rid="bib27">Kollmorgen et al., 2020</xref>). Additionally, a related recent publication (<xref ref-type="bibr" rid="bib43">Sainburg et al., 2020</xref>) similarly described the application of UMAP to vocalizations of several more species and the application of the VAE to generate interpolations between birdsong syllables for use in playback experiments. Here, by contrast, we restrict use of the UMAP and t-SNE dimensionality reduction algorithms to visualizing latent spaces inferred by the VAE and use the VAE as a general-purpose tool for quantifying vocal behavior, with a focus on cross-species comparisons and assessing variability across groups, individuals, and experimental conditions.</p><p>Moreover, we have argued above that, despite conventional wisdom, clustering is not the best account of the diversity of mouse vocal behavior. We argued this on the basis of multiple converging lines of evidence, but note three important qualifications: first, the huge variety of vocal behavior among rodents (<xref ref-type="bibr" rid="bib2">Berryman, 1976</xref>; <xref ref-type="bibr" rid="bib19">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib36">Novakowski, 1969</xref>; <xref ref-type="bibr" rid="bib41">Sadananda et al., 2008</xref>; <xref ref-type="bibr" rid="bib44">Smith et al., 1977</xref>; <xref ref-type="bibr" rid="bib34">Miller and Engstrom, 2007</xref>) suggests the possibility of clustered vocal behavior in some mouse strains not included in our data. Second, it is possible that the difference in clustered and nonclustered data depends crucially on dataset size. If real syllables even occasionally fall between well-defined clusters, a large enough dataset might lightly ‘fill in’ true gaps. Conversely, even highly clustered data may look more or less continuous given an insufficient number of samples per cluster. While this is not likely given the more than 15,000 syllables in <xref ref-type="fig" rid="fig5">Figure 5</xref>, it is difficult to rule out in general. Finally, our purely signal-level analysis of vocal behavior cannot address the possibility that a continuous distribution of syllables could nevertheless be perceived categorically. For example, swamp sparrows exhibit categorical neural and behavioral responses to changes in syllable duration (<xref ref-type="bibr" rid="bib38">Prather et al., 2009</xref>). Nonetheless, we argue that, without empirical evidence to this effect in rodents, caution is in order when interpreting the apparent continuum of USV syllables in categorical terms.</p><p>Lastly, we showed how a ‘shotgun VAE’ approach can be used to extend our approach to the quantification of moment-by-moment vocal variability. In previous studies, syllable variability has only been quantified for certain well-characterized syllables like harmonic stacks in zebra finch song (<xref ref-type="bibr" rid="bib22">Kao and Brainard, 2006</xref>). Our method, by contrast, provides a continuous variability measure for all syllables (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). This is particularly useful for studies of the neural basis of this vocal variability, which is hypothesized to operate on millisecond to tens-of-milliseconds timescales (<xref ref-type="bibr" rid="bib11">Fee and Goldberg, 2011</xref>).</p><p>Nonetheless, as a data-driven method, our approach carries some drawbacks. Most notably, the VAE must be trained on a per-dataset basis. This is more computationally intensive than calculating typical acoustic features (≈1 hr training times on a GPU) and also prevents direct comparisons across datasets unless they are trained together in a single model. Additionally, the resulting learned features, representing nonlinear, nonseparable acoustic effects, are somewhat less interpretable than named acoustic features like duration and spectral entropy. However, several recent studies in the VAE literature have attempted to address this issue by focusing on the introduction of covariates (<xref ref-type="bibr" rid="bib45">Sohn et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Louizos et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Khemakhem et al., 2019</xref>) and ‘disentangling’ approaches that attempt to learn independent sources of variation in the data (<xref ref-type="bibr" rid="bib18">Higgins et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Burgess et al., 2018</xref>), which we consider to be promising future directions. We also note that great progress in generating raw audio has been made in the past few years, potentially enabling similar approaches that bypass an intermediate spectrogram representation (<xref ref-type="bibr" rid="bib1">Avd et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Kong et al., 2020</xref>).</p><p>Finally, we note that while our focus in this work is vocal behavior, our training data are simply syllable spectrogram images. Similar VAE approaches could also be applied to other kinds of data summarizable as images or vectors. The shotgun VAE approach could likewise be applied to sequences of such vectors, potentially revealing structures like those in <xref ref-type="fig" rid="fig6">Figure 6b</xref>. More broadly, our results suggest that data-driven dimensionality reduction methods, particularly modern nonlinear, overparameterized methods, and the latent spaces that come with them, offer a promising avenue for the study of many types of complex behavior.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal statement</title><p>All experiments were conducted according to protocols approved by the Duke University Institutional Animal Care and Use Committee (mice: A171-20-08, birds: A172-20-08).</p></sec><sec id="s4-2"><title>Recordings</title><p>Recordings of C57BL/6 and DBA/2 mice were obtained from the MUPET Wiki (<xref ref-type="bibr" rid="bib51">Van Segbroeck M et al., 2019</xref>). These recordings are used in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s5">5</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>, <xref ref-type="fig" rid="fig4">Figure 4d–e</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, <xref ref-type="fig" rid="fig5">Figure 5e</xref>, and Appendix 1. Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires (<xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>).</p><p>Additional recordings of female-directed mouse USVs are used in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>. These recordings comprise 36 male mice from various genetic backgrounds over 118 recording sessions of roughly 20 minutes each (≈40 total hours, 156,180 [<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>] total syllables). USVs were recorded with an ultrasonic microphone (Avisoft, CMPA/CM16), amplified (Presonus TubePreV2), and digitized at 300 kHz (Spike 7, CED). A subset of these recordings corresponding to a single individual (17,400 syllables) is further used in <xref ref-type="fig" rid="fig5">Figures 5b–d</xref>–<xref ref-type="fig" rid="fig6">6a</xref>, <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>, and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. Because these recordings contained more noise than the first set of C57/DBA recordings, we removed false-positive syllables by training the VAE on all detected syllables, projecting latent syllables to two dimensions, and then removing syllables contained within the resulting cluster of noise syllables, with 15,712 syllables remaining (see <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>).</p><p>A single male zebra finch was recorded over a 2-day period (153–154 days post-hatch [dph]) in both female-directed and undirected contexts (14,270 total syllables, 1100 directed). Song was recorded with Sound Analysis Pro 2011.104 (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>) in a soundproof box. These recordings are used in <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>, <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>, <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>, <xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, and <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><p>For <xref ref-type="fig" rid="fig7">Figure 7</xref>, we selected 10 adult, normally reared birds from different breeding cages in our colony. Until at least 60 dph, each of these pupil birds had interacted with only one adult male, the tutor from his respective breeding cage. We recorded the adult (&gt;90 dph) vocalizations of these pupil birds for 5–12 days each with Sound Analysis Pro 2011.104 (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>), then recorded their respective tutors under the same conditions for 5–12 days each. These recordings are used in <xref ref-type="fig" rid="fig7">Figure 7</xref> and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>.</p></sec><sec id="s4-3"><title>Software comparisons</title><p>We compared our VAE method to three widely used vocal analysis packages: MUPET 2.0 (<xref ref-type="bibr" rid="bib50">Van Segbroeck et al., 2017</xref>), DeepSqueak 2.0 (<xref ref-type="bibr" rid="bib7">Coffey et al., 2019</xref>) (for mouse USVs), and SAP 2011.104 (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>) (for birdsong), each with default parameter settings. MUPET clusters were found using the minimum number of clusters (10). DeepSqueak features were generated using the DeepSqueak ‘import from MUPET’ feature.</p><sec id="s4-3-1"><title>Audio segmenting</title><p>For all mouse USV datasets, individual syllable onsets and offsets were detected using MUPET with default parameter settings. The shotgun VAE analysis in <xref ref-type="fig" rid="fig6">Figure 6</xref> was restricted to manually defined regions (bouts) of vocalization. In this figure, zebra finch songs were segmented semi-automatically: first, we selected four representative song motifs from each individual. Then we converted these to spectrograms using a Short Time Fourier Transform with Hann windows of length 512 and overlap of 256, averaged these spectrograms, and blurred the result using a Gaussian filter with 0.5 pixel standard deviation. The result was a song template used to match against the remaining data. Specifically, we looked for local maxima in the normalized cross-correlation between the template and each audio file. Matches corresponded to local maxima with cross-correlations above 1.8 median absolute deviations from the median, calculated on a per-audio-file basis. A spectrogram was then computed for each match. All match spectrograms were then projected to two dimensions using UMAP (<xref ref-type="bibr" rid="bib32">McInnes et al., 2018</xref>), from which a single well-defined cluster, containing stereotyped song, was retained. Zebra finch syllable onsets and offsets were then detected using SAP (<xref ref-type="bibr" rid="bib49">Tchernichovski and Mitra, 2004</xref>) on this collection of song renditions using default parameter settings. After segmentation, syllable spectrograms were projected to two dimensions using UMAP, and eight well-defined clusters of incorrectly segmented syllables were removed, leaving six well-defined clusters of song syllables. For <xref ref-type="fig" rid="fig7">Figure 7</xref>, song motifs were hand-labeled for approximately 10 min of song-rich audio per animal. These labels were used to train an automated segmentation tool, <italic>vak</italic> 0.3.1 (<xref ref-type="bibr" rid="bib35">Nicholson and Cohen, 2020</xref>), for each animal. Trained <italic>vak</italic> models were used to automatically label motifs in the remaining audio data for each animal. Automatic segmentation sometimes divided single motifs or joined multiple motifs. To correct for these errors, short (&lt;50 ms) gaps inside motifs were eliminated. After this correction, putative motif segments with durations outside 0.4–1.5 s were discarded. Syllables segments were derived from a subset of the <italic>vak</italic> motif segments by aligning the motif amplitude traces and manually determining syllable boundaries, resulting in 75,430 total syllable segments.</p></sec><sec id="s4-3-2"><title>Spectrograms</title><p>Spectrograms were computed using the log modulus of a signal’s Short Time Fourier Transform, computed using Hann windows of length 512 and overlap of 256 for bird vocalization, and length 1024 and overlap 512 for mouse vocalization. Sample rates were 32 kHz for bird vocalization and 250 kHz for mouse vocalization, except for the recordings in <xref ref-type="fig" rid="fig3">Figure 3f</xref>, which were sampled at a rate of 300 kHz. The resulting time/frequency representation was then interpolated at desired frequencies and times. Frequencies were mel-spaced from 0.4 to 8 kHz for bird recordings in <xref ref-type="fig" rid="fig7">Figure 7</xref>, mel-spaced from 0.4 to 10 kHz for all other bird recordings, and linearly spaced from 30 to 110 kHz for mouse recordings. For both species, syllables longer than <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>⁢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> were discarded. Additionally, short syllables were stretched in time in a way that preserved relative duration, but encouraged the VAE to represent fine temporal details. Specifically, a syllable of duration <inline-formula><mml:math id="inf7"><mml:mi>t</mml:mi></mml:math></inline-formula> was stretched by a factor of <inline-formula><mml:math id="inf8"><mml:msqrt><mml:mfrac><mml:msub><mml:mi>t</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mi>t</mml:mi></mml:mfrac></mml:msqrt></mml:math></inline-formula>. See <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref> for a comparison to linear frequency spacing for zebra finches and no time stretching for mice and zebra finches. The resulting spectrograms were then clipped to manually tuned minimum and maximum values. The values were then linearly stretched to lie in the interval [0,1]. The resulting spectrograms were 128 × 128 = 16,384 pixels, with syllables shorter than <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>t</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> zero-padded symmetrically.</p></sec></sec><sec id="s4-4"><title>Model training</title><p>Our VAE is implemented in PyTorch (v1.1.0) and trained to maximize the standard evidence lower bound (ELBO) objective using the reparameterization trick and ADAM optimization (<xref ref-type="bibr" rid="bib25">Kingma and Welling, 2013</xref>; <xref ref-type="bibr" rid="bib39">Rezende et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Paszke et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Kingma and Ba, 2014</xref>). The encoder and decoder are deep convolutional neural networks with fixed architecture diagrammed in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. The latent dimension was fixed to 32, which was found to be sufficient for all training runs. The approximate posterior was parameterized as a normal distribution with low rank plus diagonal covariance: <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> , where µ is the latent mean, u is a 32 × 1 covariance factor, and <inline-formula><mml:math id="inf11"><mml:mi>d</mml:mi></mml:math></inline-formula> was the latent diagonal, a vector of length 32. The observation distribution was parameterized as <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where µ was the output of the decoder. All activation functions were Rectified Linear Units. Learning rate was set to 10<sup>-3</sup> and batch size was set to 64.</p></sec><sec id="s4-5"><title>Comparison of VAE and handpicked features</title><p>For each analysis tool (MUPET, DeepSqueak, SAP), we assembled two feature sets: one calculated by the comparison tool (e.g., MUPET features) and one a matched VAE set. For the first set, each feature calculated by the program was z-scored and all components with nonzero variance were retained (9/9, 10/10, and 13/14 components for MUPET, DeepSqueak, and SAP, respectively). For the second set, we trained a VAE on all syllables, computed latent means of these via the VAE encoder, and removed principal components containing less than 1% of the total feature variance (7, 5, and 5 out of 32 components retained for MUPET, DeepSqueak, and SAP syllables, respectively). Each feature set was used as a basis for predicting the features in the other set using <inline-formula><mml:math id="inf13"><mml:mi>k</mml:mi></mml:math></inline-formula>-nearest neighbors regression with <inline-formula><mml:math id="inf14"><mml:mi>k</mml:mi></mml:math></inline-formula> set to 10 and nearest neighbors determined using Euclidean distance in the assembled feature spaces. The variance-explained value reported is the average over five shuffled train/test folds (<xref ref-type="fig" rid="fig2">Figure 2e</xref>).</p><p>Unlike latent features, traditional features do not come equipped with a natural scaling. For this reason, we z-scored traditional features to avoid tethering our analyses to the identities of particular acoustic features involved. Then, to fairly compare the effective dimensionalities of traditional and acoustic features in <xref ref-type="fig" rid="fig2">Figure 2d</xref>, we thus also z-scored the latent features as well, thereby disregarding the natural scaling of the latent features. PCA was then performed on the resulting scaled feature set.</p></sec><sec id="s4-6"><title>Birdsong variability index</title><p>For <xref ref-type="fig" rid="fig4">Figure 4c</xref>, given a set <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of feature vectors of <inline-formula><mml:math id="inf16"><mml:mi>n</mml:mi></mml:math></inline-formula> syllables, we defined a variability index for the data as follows:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">I</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:msub><mml:mi mathvariant="normal">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is proportional to a robust estimator of the variance of the data around <inline-formula><mml:math id="inf18"><mml:mi>z</mml:mi></mml:math></inline-formula>:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:munder accentunder="true"><mml:mi>median</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:munder></mml:mpadded><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We calculate the above metric for every combination of syllable (A–F), feature set (SAP-generated vs. VAE-generated), and social context (directed vs. undirected) and report the variability index of the directed condition relative to the variability index of the undirected condition (<xref ref-type="fig" rid="fig4">Figure 4c</xref>).</p><p>For <xref ref-type="fig" rid="fig6">Figure 6e</xref>, we would ideally use the variability index defined above, but <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is expensive to compute for each datapoint, as required in (1). Thus, we use an approximate center point defined by the median along each <italic>coordinate</italic>: <inline-formula><mml:math id="inf20"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup><mml:mo>≡</mml:mo><mml:mrow><mml:mi>median</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where the superscript here represents the ith coordinate of <inline-formula><mml:math id="inf21"><mml:mi>z</mml:mi></mml:math></inline-formula>. That is, <inline-formula><mml:math id="inf22"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> contains the medians of the marginal distributions. This value is calculated for each combination of time point and social context (directed vs. undirected) and plotted in <xref ref-type="fig" rid="fig6">Figure 6e</xref>.</p></sec><sec id="s4-7"><title>Maximum mean discrepancy</title><p>We used the MMD integral probability metric to quantify differences in sets of syllables (<xref ref-type="bibr" rid="bib15">Gretton et al., 2012</xref>). Given random variables <inline-formula><mml:math id="inf23"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mi>y</mml:mi></mml:math></inline-formula>, MMD is defined over a function class <inline-formula><mml:math id="inf25"><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo form="prefix" movablelimits="true">sup</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf27"><mml:mi class="ltx_font_mathcaligraphic">ℱ</mml:mi></mml:math></inline-formula> was taken to the set of functions on the unit ball in a reproducing kernel Hilbert space with fixed spherical Gaussian kernel. For <xref ref-type="fig" rid="fig4">Figure 4e–f</xref>, the kernel width <inline-formula><mml:math id="inf28"><mml:mi>σ</mml:mi></mml:math></inline-formula> was chosen to be the median distance between points in the aggregate sample, a common heuristic (<xref ref-type="bibr" rid="bib15">Gretton et al., 2012</xref>). In <xref ref-type="fig" rid="fig7">Figure 7</xref>, the kernel bandwidth was chosen to be 25% of the median distance between points in the aggregate sample in order to focus on finer differences in distributions. For <xref ref-type="fig" rid="fig4">Figure 4e</xref>, we obtained 20 approximately 6.5 min recordings of male C57BL/6 mice and 20 approximately 6.5 min recordings of male DBA/2 mice (see Recordings). Latent means of USVs from a single recording were treated as independent and identically distributed draws from a recording-specific USV distribution, and MMD was estimated using these latent means. In <xref ref-type="fig" rid="fig4">Figure 4e</xref>, the MMD values are plotted as a matrix and the order of rows was obtained by agglomerative clustering. In <xref ref-type="fig" rid="fig4">Figure 4f</xref> and <xref ref-type="fig" rid="fig7">Figure 7</xref>, the same procedure was followed. For <xref ref-type="fig" rid="fig4">Figure 4f</xref>, a t-SNE was computed for each recording session, with the distance between recording sessions taken to be the estimated MMD between them (see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for the MMD matrix).</p></sec><sec id="s4-8"><title>Unsupervised clustering metrics</title><p>We used three unsupervised clustering metrics to assess the quality of clustering for both zebra finch and mouse syllables: the mean silhouette coefficient (<xref ref-type="bibr" rid="bib40">Rousseeuw, 1987</xref>), the Calinski–Harabasz Index (<xref ref-type="bibr" rid="bib5">Caliński and Harabasz, 1974</xref>), and the Davies–Bouldin Index (<xref ref-type="bibr" rid="bib9">Davies and Bouldin, 1979</xref>). For each species (zebra finch and mouse), we partitioned the data for 10-fold cross-validation (train on 9/10, test on 1/10 held out). For a null comparison, for each of 10% disjoint subsets of the data, we created a synthetic Gaussian noise dataset matched for covariance and number of samples. These synthetic noise datasets were then used to produce the dotted line in <xref ref-type="fig" rid="fig5">Figure 5d</xref>.</p><p>For each data split, we clustered using a GMM with full covariance using Expectation Maximization on the training set. We then evaluated each clustering metric on the test set. The number of clusters, <inline-formula><mml:math id="inf29"><mml:mi>k</mml:mi></mml:math></inline-formula>, was set to six in <xref ref-type="fig" rid="fig5">Figure 5d</xref>, but qualitatively similar results were obtained when <inline-formula><mml:math id="inf30"><mml:mi>k</mml:mi></mml:math></inline-formula> was allowed to vary between 2 and 12 (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Reported values in <xref ref-type="fig" rid="fig5">Figure 5d</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> are the differences in unsupervised metrics on real data and Gaussian noise for each cross-validation fold, with a possible sign change to indicate higher values as more clustered.</p></sec><sec id="s4-9"><title>Shotgun VAE</title><p>To perform the analysis in <xref ref-type="fig" rid="fig6">Figure 6a–b</xref>, regions of active vocalization were defined manually for both species (22 min of mouse recordings, 2 min of zebra finch recordings). Zebra finch bouts containing only calls and no song motifs were excluded. For both species, the duration of audio chunks was chosen to be roughly as long as the longest syllables (zebra finch: 120 ms; mouse: 200 ms). No explicit training set was made. Rather, onsets and offsets were drawn uniformly at random from the set of fixed-duration segments and the corresponding spectrograms were computed on a per-datapoint basis. Thus, the VAE likely never encountered the same spectrogram twice, encouraging it to learn the underlying time series.</p><p>To perform the variability reduction analysis in <xref ref-type="fig" rid="fig6">Figure 6d–e</xref>, song renditions were collected (see Audio segmenting) and a spectrogram was computed for each. The whole collection of spectrograms was then jointly warped using piecewise-linear time warping (<xref ref-type="bibr" rid="bib53">Williams et al., 2019</xref>). Fixed-duration training spectrograms were made by interpolating normal spectrograms (as described in Spectrograms) at linearly spaced time points in warped time, generally corresponding to nonlinearly spaced time points in real time. As above, spectrograms were made during training on a per-datapoint basis. After training the VAE on these spectrograms, latent means were collected for 200 spectrograms for each song rendition, linearly spaced in warped time from the beginning to the end of the song bout. Lastly, for each combination of condition (directed vs. undirected song) and time point, the variability index described above was calculated. A total of 186 directed and 2227 undirected song renditions were collected and analyzed.</p><p>To generate the shotgun VAE training set for <xref ref-type="fig" rid="fig7">Figure 7</xref>, 2000 <italic>vak</italic>-labeled motifs were selected from each animal (see Audio segmenting). A single 60 ms window was drawn from each motif to create a training set of 40,000 total spectrogram windows drawn from the 20-animal cohort. After training a VAE on this dataset, the hand-labeled motif segments used to train <italic>vak</italic> models (see Audio segmenting) were segmented into overlapping 60 ms windows that spanned each motif with an 8 ms step size between successive windows (52,826 total windows). These windows were reduced with the trained VAE and their latent means subsequently analyzed.</p></sec><sec id="s4-10"><title>Modified UMAP</title><p>Although the standard UMAP embedding of shotgun VAE latents from single-finch datasets generates points along smoothly varying strands (see <xref ref-type="fig" rid="fig6">Figure 6b</xref>), UMAP typically broke motifs into multiple strand-like pieces in the 20-animal dataset from <xref ref-type="fig" rid="fig7">Figure 7</xref>. To encourage embeddings that preserve the neighbor relationship of successive windows, we modified the distance measure underlying the UMAP. First, we computed the complete pairwise Euclidean distance matrix between all windows in latent space. Then, we artificially decreased the distance between successive windows from the same motif by multiplying corresponding distance matrix entries by 10<sup>-3</sup>. This precomputed distance matrix was then passed to UMAP as a parameter. See <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> for a comparison of the two UMAP projections.</p></sec><sec id="s4-11"><title>Data and code availability statement</title><p>The latest version of Autoencoded Vocal Analysis, the Python package used to generate, plot, and analyze latent features, is available online: <ext-link ext-link-type="uri" xlink:href="https://github.com/pearsonlab/autoencoded-vocal-analysis">https://github.com/pearsonlab/autoencoded-vocal-analysis</ext-link> (<xref ref-type="bibr" rid="bib14">Goffinet, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:55b90c22ca93685ad74019af02876995eea38daa;origin=https://github.com/pearsonlab/autoencoded-vocal-analysis;visit=swh:1:snp:c4354df2f3139cd46149d743296dcc324bf6b85b;anchor=swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904">swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904</ext-link>). Mouse and zebra finch recordings used in this study are archived on the Duke Digital Repository: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7924/r4gq6zn8w">https://doi.org/10.7924/r4gq6zn8w</ext-link>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, software, validation, Supervision, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Investigation, Methodology, validation, Supervision, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing – review and editing, Investigation, Funding acquisition, Project administration, resources, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Writing – review and editing, Investigation, Methodology, Funding acquisition, resources, Supervision, Visualization, Writing – original draft</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All data generated in conjunction for this study were generated by experiments performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols of Duke University, protocol numbers A171-20-08 and A172-20-08.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-67855-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Dataset 1: Online, publicly available MUPET dataset: ~5GB Available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki">https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki</ext-link> Figs: 2, 3, 4d-e.</p><p>Dataset 2: Single zebra finch data: ~200-400 MB of audio generated as part of work in progress in Mooney Lab. Figs: 2e-f, 4a-c, 5a, 5d, 6b-e.</p><p>Dataset 3: Mouse USV dataset: ~30-40 GB of audio generated as part of work in progress in Mooney Lab. Figs: 4f.</p><p>Dataset 5: This is a subset of dataset 3, taken from a single mouse: ~1GB of audio. Figs: 5b-e, 6a.</p><p>Dataset 6: 10 zebra finch pupil/tutor pairs: ~60 GB of audio generated as part of work in progress in Mooney Lab. Figs: 7.</p><p>Datasets 2-6 are archived in the Duke Digital Repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7924/r4gq6zn8w">https://doi.org/10.7924/r4gq6zn8w</ext-link>).</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Brudner</surname><given-names>S</given-names></name><name><surname>Goffinet</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Data from: Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</data-title><source>Duke Digital Repository</source><pub-id pub-id-type="doi">10.7924/r4gq6zn8w</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH grants R01-DC013826 (RM, JP), R01-NS118424 (RM, JP), R01-NS099288 (RM), R01-MH117778 (RM), and F31-HD098772 (SB) and by a hardware grant from the NVIDIA corporation.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Avd</surname><given-names>O</given-names></name><name><surname>Dieleman</surname><given-names>S</given-names></name><name><surname>Zen</surname><given-names>H</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Kalchbrenner</surname><given-names>N</given-names></name><name><surname>Senior</surname><given-names>A</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Wavenet: A Generative Model for Raw Audio</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berryman</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Guinea-pig vocalizations: their structure, causation and function</article-title><source>Zeitschrift Für Tierpsychologie</source><volume>41</volume><fpage>80</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0310.1976.tb00471.x</pub-id><pub-id pub-id-type="pmid">961122</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>CP</given-names></name><name><surname>Higgins</surname><given-names>I</given-names></name><name><surname>Pal</surname><given-names>A</given-names></name><name><surname>Matthey</surname><given-names>L</given-names></name><name><surname>Watters</surname><given-names>N</given-names></name><name><surname>Desjardins</surname><given-names>G</given-names></name><name><surname>Lerchner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Understanding Disentangling in -VAE</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.02494">https://arxiv.org/abs/1906.02494</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkett</surname><given-names>ZD</given-names></name><name><surname>Day</surname><given-names>NF</given-names></name><name><surname>Peñagarikano</surname><given-names>O</given-names></name><name><surname>Geschwind</surname><given-names>DH</given-names></name><name><surname>White</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Voice: A semi-automated pipeline for standardizing vocal analysis across models</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>10237</elocation-id><pub-id pub-id-type="doi">10.1038/srep10237</pub-id><pub-id pub-id-type="pmid">26018425</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caliński</surname><given-names>T</given-names></name><name><surname>Harabasz</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>A dendrite method for cluster analysis</article-title><source>Communications in Statistics-Theory and Methods</source><volume>3</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1080/03610927408827101</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Sarkar</surname><given-names>A</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Male mice song syntax depends on social contexts and influences female preferences</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id><pub-id pub-id-type="pmid">25883559</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffey</surname><given-names>KR</given-names></name><name><surname>Marx</surname><given-names>RG</given-names></name><name><surname>Neumaier</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deepsqueak: A deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title><source>Neuropsychopharmacology</source><volume>44</volume><fpage>859</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id><pub-id pub-id-type="pmid">30610191</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Aston</surname><given-names>J</given-names></name><name><surname>Hua</surname><given-names>G</given-names></name><name><surname>Wipf</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Connections with robust PCA and the role of emergent sparsity in variational autoencoder models</article-title><source>The Journal of Machine Learning Research</source><volume>19</volume><fpage>1573</fpage><lpage>1614</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>DL</given-names></name><name><surname>Bouldin</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>A cluster separation measure</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>1</volume><fpage>224</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1979.4766909</pub-id><pub-id pub-id-type="pmid">21868852</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derégnaucourt</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Fehér</surname><given-names>O</given-names></name><name><surname>Pytte</surname><given-names>C</given-names></name><name><surname>Tchernichovski</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>How sleep affects the developmental learning of bird song</article-title><source>Nature</source><volume>433</volume><fpage>710</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1038/nature03275</pub-id><pub-id pub-id-type="pmid">15716944</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname><given-names>MS</given-names></name><name><surname>Goldberg</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A hypothesis for basal ganglia-dependent reinforcement learning in the songbird</article-title><source>Neuroscience</source><volume>198</volume><fpage>152</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.069</pub-id><pub-id pub-id-type="pmid">22015923</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fehér</surname><given-names>O</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Saar</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Tchernichovski</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>De novo establishment of wild-type song culture in the zebra finch</article-title><source>Nature</source><volume>459</volume><fpage>564</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1038/nature07994</pub-id><pub-id pub-id-type="pmid">19412161</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaub</surname><given-names>S</given-names></name><name><surname>Groszer</surname><given-names>M</given-names></name><name><surname>Fisher</surname><given-names>SE</given-names></name><name><surname>Ehret</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The structure of innate vocalizations in Foxp2-deficient mouse pups</article-title><source>Genes, Brain and Behavior</source><volume>9</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1111/j.1601-183X.2010.00570.x</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Goffinet</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Autoencoded-vocal-analysis</data-title><version designator="swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904">swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:55b90c22ca93685ad74019af02876995eea38daa;origin=https://github.com/pearsonlab/autoencoded-vocal-analysis;visit=swh:1:snp:c4354df2f3139cd46149d743296dcc324bf6b85b;anchor=swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904">https://archive.softwareheritage.org/swh:1:dir:55b90c22ca93685ad74019af02876995eea38daa;origin=https://github.com/pearsonlab/autoencoded-vocal-analysis;visit=swh:1:snp:c4354df2f3139cd46149d743296dcc324bf6b85b;anchor=swh:1:rev:f512adcae3f4c5795558e2131e54c36daf23b904</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gretton</surname><given-names>A</given-names></name><name><surname>Borgwardt</surname><given-names>KM</given-names></name><name><surname>Rasch</surname><given-names>MJ</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A kernel two-sample test</article-title><source>Journal of Machine Learning Research</source><volume>13</volume><fpage>723</fpage><lpage>773</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Radyushkin</surname><given-names>K</given-names></name><name><surname>Ehrenreich</surname><given-names>H</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The structure and usage of female and male mouse ultrasonic vocalizations reveal only minor differences</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e41133</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0041133</pub-id><pub-id pub-id-type="pmid">22815941</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>S</given-names></name><name><surname>Weiner</surname><given-names>B</given-names></name><name><surname>Perets</surname><given-names>N</given-names></name><name><surname>London</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal structure of mouse courtship vocalizations facilitates syllable labeling</article-title><source>Communications Biology</source><volume>3</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s42003-020-1053-7</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>I</given-names></name><name><surname>Matthey</surname><given-names>L</given-names></name><name><surname>Pal</surname><given-names>A</given-names></name><name><surname>Burgess</surname><given-names>C</given-names></name><name><surname>Glorot</surname><given-names>X</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>beta-VAE</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning basic visual concepts with a constrained variational framework</article-title><source>ICLR</source><volume>2</volume><elocation-id>6</elocation-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holy</surname><given-names>TE</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ultrasonic songs of male mice</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id><pub-id pub-id-type="pmid">16248680</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivanenko</surname><given-names>A</given-names></name><name><surname>Watkins</surname><given-names>P</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007918</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007918</pub-id><pub-id pub-id-type="pmid">32569292</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>AK</given-names></name><name><surname>Murty</surname><given-names>MN</given-names></name><name><surname>Flynn</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Data clustering</article-title><source>ACM Computing Surveys</source><volume>31</volume><fpage>264</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1145/331499.331504</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>MH</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Lesions of an avian basal ganglia circuit prevent context-dependent changes to song variability</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>1441</fpage><lpage>1455</lpage><pub-id pub-id-type="doi">10.1152/jn.01138.2005</pub-id><pub-id pub-id-type="pmid">16723412</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kershenbaum</surname><given-names>A</given-names></name><name><surname>Blumstein</surname><given-names>DT</given-names></name><name><surname>Roch</surname><given-names>MA</given-names></name><name><surname>Ç</surname><given-names>A</given-names></name><name><surname>Backus</surname><given-names>G</given-names></name><name><surname>Bee</surname><given-names>MA</given-names></name><name><surname>Bohn</surname><given-names>K</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Carter</surname><given-names>G</given-names></name><name><surname>Cäsar</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Acoustic sequences in non-human animals: a tutorial review and prospectus</article-title><source>Biological Reviews</source><volume>91</volume><fpage>13</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1111/brv.12160</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Khemakhem</surname><given-names>I</given-names></name><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Hyvärinen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Variational Autoencoders and Nonlinear ICA: A Unifying Framework</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.04809">https://arxiv.org/abs/1907.04809</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auto-Encoding Variational Bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kollmorgen</surname><given-names>S</given-names></name><name><surname>Hahnloser</surname><given-names>RH</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nearest neighbours reveal fast and slow components of motor learning</article-title><source>Nature</source><volume>577</volume><fpage>526</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1892-x</pub-id><pub-id pub-id-type="pmid">31915383</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Z</given-names></name><name><surname>Ping</surname><given-names>W</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>K</given-names></name><name><surname>Catanzaro</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Diffwave: A Versatile Diffusion Model for Audio Synthesis</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2009.09761">https://arxiv.org/abs/2009.09761</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Louizos</surname><given-names>C</given-names></name><name><surname>Swersky</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name><name><surname>Zemel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The Variational Fair Autoencoder</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.00830">https://arxiv.org/abs/1511.00830</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lvd</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing data using T-SNE</article-title><source>Journal of Machine Learning Research</source><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mandelblat-Cerf</surname><given-names>Y</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An automated procedure for evaluating song imitation</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e96484</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0096484</pub-id><pub-id pub-id-type="pmid">24809510</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Umap: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">https://arxiv.org/abs/1802.03426</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mets</surname><given-names>DG</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An automated approach to the quantitation of vocalizations and vocal learning in the songbird</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006437</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006437</pub-id><pub-id pub-id-type="pmid">30169523</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>JR</given-names></name><name><surname>Engstrom</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Vocal stereotypy and singing behavior in baiomyine mice</article-title><source>Journal of Mammalogy</source><volume>88</volume><fpage>1447</fpage><lpage>1465</lpage><pub-id pub-id-type="doi">10.1644/06-MAMM-A-386R.1</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>D</given-names></name><name><surname>Cohen</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>VAK</data-title><publisher-name>0.3</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novakowski</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>The influence of vocalization on the behavior of beaver, Castor canadensis Kuhl</article-title><source>American Midland Naturalist</source><volume>81</volume><fpage>198</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.2307/2423661</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Automatic differentiation in PyTorch</data-title><publisher-name>NeurIPS 2017 Autodiff Workshop</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prather</surname><given-names>JF</given-names></name><name><surname>Nowicki</surname><given-names>S</given-names></name><name><surname>Anderson</surname><given-names>RC</given-names></name><name><surname>Peters</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural correlates of categorical perception in learned vocal communication</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>221</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1038/nn.2246</pub-id><pub-id pub-id-type="pmid">19136972</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rezende</surname><given-names>DJ</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Stochastic Backpropagation and Variational Inference in Deep Latent Gaussian Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/14014082">https://arxiv.org/abs/14014082</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title><source>Journal of Computational and Applied Mathematics</source><volume>20</volume><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadananda</surname><given-names>M</given-names></name><name><surname>Wöhr</surname><given-names>M</given-names></name><name><surname>Schwarting</surname><given-names>RK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Playback of 22-kHz and 50-kHz ultrasonic vocalizations induces differential c-fos expression in rat brain</article-title><source>Neuroscience Letters</source><volume>435</volume><fpage>17</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2008.02.002</pub-id><pub-id pub-id-type="pmid">18328625</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>Theilman</surname><given-names>B</given-names></name><name><surname>Thielk</surname><given-names>M</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Parallels in the sequential organization of birdsong and human speech</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3636</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11605-y</pub-id><pub-id pub-id-type="pmid">31406118</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>Thielk</surname><given-names>M</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008228</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008228</pub-id><pub-id pub-id-type="pmid">33057332</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>WJ</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name><name><surname>Oppenheimer</surname><given-names>EC</given-names></name><name><surname>Devilla</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Vocalizations of the black-tailed prairie dog, Cynomys ludovicianus</article-title><source>Animal Behaviour</source><volume>25</volume><fpage>152</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(77)90078-1</pub-id><pub-id pub-id-type="pmid">855948</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>K</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Yan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning structured output representation using deep conditional generative models</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>3483</fpage><lpage>3491</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sossinka</surname><given-names>R</given-names></name><name><surname>Böhner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Song types in the zebra finch Poephila guttata castanotis</article-title><source>Zeitschrift Für Tierpsychologie</source><volume>53</volume><fpage>123</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0310.1980.tb01044.x</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soudry</surname><given-names>D</given-names></name><name><surname>Keshri</surname><given-names>S</given-names></name><name><surname>Stinson</surname><given-names>P</given-names></name><name><surname>Mh</surname><given-names>O</given-names></name><name><surname>Iyengar</surname><given-names>G</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Efficient s’’shotgun’’inference of neural connectivity from highly sub-sampled activity data</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004464</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004464</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name><name><surname>Ho</surname><given-names>CE</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A procedure for an automated measurement of song similarity</article-title><source>Animal Behaviour</source><volume>59</volume><fpage>1167</fpage><lpage>1176</lpage><pub-id pub-id-type="doi">10.1006/anbe.1999.1416</pub-id><pub-id pub-id-type="pmid">10877896</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Mitra</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Sound Analysis Pro User Manual</source><publisher-loc>New York</publisher-loc><publisher-name>CCNY</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Segbroeck</surname><given-names>M</given-names></name><name><surname>Knoll</surname><given-names>AT</given-names></name><name><surname>Levitt</surname><given-names>P</given-names></name><name><surname>Narayanan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MUPET Mouse Ultrasonic Profile ExTraction: a signal processing tool for rapid and unsupervised analysis of ultrasonic vocalizations</article-title><source>Neuron</source><volume>94</volume><fpage>465</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id><pub-id pub-id-type="pmid">28472651</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Van Segbroeck M</collab><collab>Knoll AT</collab><collab>Levitt P</collab><collab>Narayanan S</collab></person-group><year iso-8601-date="2019">2019</year><data-title>MUPET Wiki</data-title><version designator="ae7dc9e">ae7dc9e</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki">https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Venter</surname><given-names>JC</given-names></name><name><surname>Adams</surname><given-names>MD</given-names></name><name><surname>Sutton</surname><given-names>GG</given-names></name><name><surname>Kerlavage</surname><given-names>AR</given-names></name><name><surname>Smith</surname><given-names>HO</given-names></name><name><surname>Hunkapiller</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Shotgun Sequencing of the Human Genome</source><publisher-loc>Washington, D.C</publisher-loc><publisher-name>American Association for the Advancement of Science</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Fisher</surname><given-names>T</given-names></name><name><surname>Wilson</surname><given-names>CD</given-names></name><name><surname>Brann</surname><given-names>DH</given-names></name><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shusterman</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discovering Precise Temporal Patterns in Large-Scale Neural Recordings through Robust and Interpretable Time Warping</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/661165</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woehr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Ultrasonic vocalizations in Shank mouse models for autism spectrum disorders: detailed spectrographic analyses and developmental profiles</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>43</volume><fpage>199</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.03.021</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Comparison of feature sets on the downstream task of predicting finch social context.</title><p>(directed vs. undirected context) given acoustic features of single syllables.Classification accuracy, in percent, averaged over five disjoint, class-balanced splits of the data is reported. Empirical standard deviation is shown in parentheses. Euclidean distance is used for nearest neighbor classifiers. Each SAP acoustic feature is independently z-scored as a preprocessing step. Latent feature dimension is truncated when &gt;99% of the feature variance is explained. Random forest (RF) classifiers use 100 trees and the Gini impurity criterion. The multilayer perceptron (MLP) classifiers are two-layer networks with a hidden layer size of 100, ReLU activations, and an L2 weight regularization parameter ‘alpha,’ trained with ADAM optimization with a learning rate of 10<sup>-3</sup> for 200 epochs. D denotes the dimension of each feature set, with Gaussian random projections used to decrease the dimension of spectrograms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="6" valign="bottom">Predicting finch social context (<xref ref-type="fig" rid="fig4">Figure 4a–c</xref>)</th></tr><tr><th align="left" rowspan="2" valign="bottom"/><th align="left" colspan="3" valign="bottom">Spectrogram</th><th align="left" valign="bottom">SAP</th><th align="left" valign="bottom">Latent</th></tr><tr><th align="left" valign="bottom">D = 10</th><th align="left" valign="bottom">D = 30</th><th align="left" valign="bottom">D = 100</th><th align="left" valign="bottom">D = 13</th><th align="left" valign="bottom">D = 5</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>k</italic>-NN (<inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td align="char" char="." valign="bottom">92.5 (0.2)</td><td align="char" char="." valign="bottom">95.3 (0.1)</td><td align="char" char="." valign="bottom"><bold>97.3 (0.3)</bold></td><td align="char" char="." valign="bottom">93.0 (0.3)</td><td align="char" char="." valign="bottom">96.9 (0.2)</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf32"><mml:mi>k</mml:mi></mml:math></inline-formula>-NN (<inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td align="char" char="." valign="bottom">93.0 (0.2)</td><td align="char" char="." valign="bottom">95.3 (0.2)</td><td align="char" char="." valign="bottom">97.1 (0.3)</td><td align="char" char="." valign="bottom">93.2 (0.1)</td><td align="char" char="." valign="bottom">96.7 (0.2)</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf34"><mml:mi>k</mml:mi></mml:math></inline-formula>-NN (<inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td align="char" char="." valign="bottom">92.7 (0.1)</td><td align="char" char="." valign="bottom">94.2 (0.2)</td><td align="char" char="." valign="bottom">96.0 (0.2)</td><td align="char" char="." valign="bottom">92.8 (0.1)</td><td align="char" char="." valign="bottom">96.3 (0.1)</td></tr><tr><td align="left" valign="bottom">RF (depth = 10)</td><td align="char" char="." valign="bottom">92.6 (0.1)</td><td align="char" char="." valign="bottom">92.7 (0.1)</td><td align="char" char="." valign="bottom">93.1 (0.1)</td><td align="char" char="." valign="bottom">92.8 (0.1)</td><td align="char" char="." valign="bottom">94.9 (0.2)</td></tr><tr><td align="left" valign="bottom">RF (depth = 15)</td><td align="char" char="." valign="bottom">92.7 (0.1)</td><td align="char" char="." valign="bottom">93.2 (0.2)</td><td align="char" char="." valign="bottom">93.6 (0.1)</td><td align="char" char="." valign="bottom">93.6 (0.2)</td><td align="char" char="." valign="bottom">96.1 (0.1)</td></tr><tr><td align="left" valign="bottom">RF (depth = 20)</td><td align="char" char="." valign="bottom">92.8 (0.1)</td><td align="char" char="." valign="bottom">93.4 (0.1)</td><td align="char" char="." valign="bottom">93.8 (0.1)</td><td align="char" char="." valign="bottom">93.8 (0.2)</td><td align="char" char="." valign="bottom">96.4 (0.2)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.1)</td><td align="char" char="." valign="bottom">92.8 (0.4)</td><td align="char" char="." valign="bottom">95.4 (0.4)</td><td align="char" char="." valign="bottom"><bold>97.6 (0.3)</bold></td><td align="char" char="." valign="bottom">92.9 (0.1)</td><td align="char" char="." valign="bottom">95.7 (0.1)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.01)</td><td align="char" char="." valign="bottom">92.9 (0.3)</td><td align="char" char="." valign="bottom">95.4 (0.3)</td><td align="char" char="." valign="bottom"><bold>97.5 (0.2)</bold></td><td align="char" char="." valign="bottom">93.1 (0.2)</td><td align="char" char="." valign="bottom">96.2 (0.1)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.001)</td><td align="char" char="." valign="bottom">92.7 (0.6)</td><td align="char" char="." valign="bottom">95.2 (0.5)</td><td align="char" char="." valign="bottom"><bold>97.5 (0.2)</bold></td><td align="char" char="." valign="bottom">93.0 (0.2)</td><td align="char" char="." valign="bottom">96.3 (0.0)</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Comparison of feature sets on the downstream task of predicting mouse strain.</title><p>(C57 vs.DBA) given acoustic features of single syllables. Classification accuracy, in percent, averaged over five disjoint, class-balanced splits of the data is reported. Empirical standard deviation is shown in parentheses. Euclidean distance is used for nearest neighbor classifiers. Each MUPET and DeepSqueak acoustic feature is independently z-scored as a preprocessing step. Latent features dimension is truncated when &gt;99% of the feature variance is explained. Random forest (RF) classifiers use 100 trees and the Gini impurity criterion. The multilayer perceptron (MLP) classifiers are two-layer networks with a hidden layer size of 100, ReLU activations, and an L2 weight regularization parameter ‘alpha,’ trained with ADAM optimization with a learning rate of 10<sup>-3</sup> for 200 epochs. D denotes the dimension of each feature set, with Gaussian random projections used to decrease the dimension of spectrograms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="7" valign="bottom">Predicting mouse strain (<xref ref-type="fig" rid="fig4">Figure 4d–e</xref>)</th></tr><tr><th align="left" rowspan="2" valign="bottom"/><th align="left" colspan="3" valign="bottom">Spectrogram</th><th align="left" valign="bottom">MUPET</th><th align="left" valign="bottom">DeepSqueak</th><th align="left" valign="bottom">Latent</th></tr><tr><th align="left" valign="bottom">D = 10</th><th align="left" valign="bottom">D = 30</th><th align="left" valign="bottom">D = 100</th><th align="left" valign="bottom">D = 9</th><th align="left" valign="bottom">D = 10</th><th align="left" valign="bottom">D = 7</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf36"><mml:mi>k</mml:mi></mml:math></inline-formula>-NN (<inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td align="char" char="." valign="bottom">68.1 (0.2)</td><td align="char" char="." valign="bottom">76.4 (0.3)</td><td align="char" char="." valign="bottom">82.3 (0.5)</td><td align="char" char="." valign="bottom">86.1 (0.2)</td><td align="char" char="." valign="bottom">79.0 (0.3)</td><td align="char" char="." valign="bottom">89.8 (0.2)</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf38"><mml:mi>k</mml:mi></mml:math></inline-formula>-NN (<inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td align="char" char="." valign="bottom">71.0 (0.3)</td><td align="char" char="." valign="bottom">78.2 (0.1)</td><td align="char" char="." valign="bottom">82.7 (0.6)</td><td align="char" char="." valign="bottom">87.0 (0.1)</td><td align="char" char="." valign="bottom">80.7 (0.3)</td><td align="char" char="." valign="bottom"><bold>90.7 (0.4)</bold></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf40"><mml:mi>k</mml:mi></mml:math></inline-formula>-NN (<inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>)</td><td align="char" char="." valign="bottom">72.8 (0.3)</td><td align="char" char="." valign="bottom">78.5 (0.2)</td><td align="char" char="." valign="bottom">81.3 (0.5)</td><td align="char" char="." valign="bottom">86.8 (0.2)</td><td align="char" char="." valign="bottom">81.0 (0.2)</td><td align="char" char="." valign="bottom"><bold>90.3 (0.4)</bold></td></tr><tr><td align="left" valign="bottom">RF (depth = 10)</td><td align="char" char="." valign="bottom">72.8 (0.2)</td><td align="char" char="." valign="bottom">76.6 (0.2)</td><td align="char" char="." valign="bottom">79.1 (0.3)</td><td align="char" char="." valign="bottom">87.4 (0.5)</td><td align="char" char="." valign="bottom">81.2 (0.4)</td><td align="char" char="." valign="bottom">88.1 (0.5)</td></tr><tr><td align="left" valign="bottom">RF (depth = 15)</td><td align="char" char="." valign="bottom">73.1 (0.3)</td><td align="char" char="." valign="bottom">78.0 (0.3)</td><td align="char" char="." valign="bottom">80.5 (0.2)</td><td align="char" char="." valign="bottom">87.9 (0.4)</td><td align="char" char="." valign="bottom">82.1 (0.3)</td><td align="char" char="." valign="bottom">89.6 (0.4)</td></tr><tr><td align="left" valign="top">RF (depth = 20)</td><td align="char" char="." valign="top">73.2 (0.2)</td><td align="char" char="." valign="top">78.3 (0.2)</td><td align="char" char="." valign="top">80.7 (0.3)</td><td align="char" char="." valign="top">87.9 (0.4)</td><td align="char" char="." valign="top">81.9 (0.3)</td><td align="char" char="." valign="top">89.6 (0.4)</td></tr><tr><td align="left" valign="top">MLP (α = 0.1)</td><td align="char" char="." valign="top">72.4 (0.3)</td><td align="char" char="." valign="top">79.1 (0.4)</td><td align="char" char="." valign="top">84.5 (0.3)</td><td align="char" char="." valign="top">87.8 (0.2)</td><td align="char" char="." valign="top">82.1 (0.4)</td><td align="char" char="." valign="top"><bold>90.1 (0.3)</bold></td></tr><tr><td align="left" valign="top">MLP (α = 0.01)</td><td align="char" char="." valign="top">72.3 (0.4)</td><td align="char" char="." valign="top">78.6 (0.3)</td><td align="char" char="." valign="top">82.9 (0.4)</td><td align="char" char="." valign="top">88.1 (0.3)</td><td align="char" char="." valign="top">82.4 (0.4)</td><td align="char" char="." valign="top"><bold>90.0 (0.4)</bold></td></tr><tr><td align="left" valign="top">MLP (α = 0.001)</td><td align="char" char="." valign="top">72.4 (0.4)</td><td align="char" char="." valign="top">78.5 (0.8)</td><td align="char" char="." valign="top">82.8 (0.1)</td><td align="char" char="." valign="top">87.9 (0.2)</td><td align="char" char="." valign="top">82.4 (0.3)</td><td align="char" char="." valign="top"><bold>90.4 (0.3)</bold></td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Comparison of feature sets on the downstream task of predicting mouse identity given acoustic features of single syllables.</title><p>Classification accuracy, in percent, averaged over five disjoint, class-balanced splits of the data is reported. A class-weighted log-likelihood loss is targeted to help correct for class imbalance. Empirical standard deviation is shown in parentheses. Each MUPET acoustic feature is independently z-scored as a preprocessing step. Latent feature principal components are truncated when &gt;99% of the feature variance is explained. The multilayer perceptron (MLP) classifiers are two-layer networks with a hidden layer size of 100, ReLU activations, and an L2 weight regularization parameter ‘alpha,’ trained with ADAM optimization with a learning rate of 10<sup>-3</sup> for 200 epochs. Chance performance is 2.8% for top-1 accuracy and 13.9% for top-5 accuracy. D denotes the dimension of each feature set, with Gaussian random projections used to decrease the dimension of spectrograms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="6" valign="bottom">Predicting mouse identity (<xref ref-type="fig" rid="fig4">Figure 4f</xref>)</th></tr><tr><th align="left" rowspan="2" valign="bottom"/><th align="left" colspan="3" valign="bottom">Spectrogram</th><th align="left" valign="bottom">MUPET</th><th align="left" valign="bottom">Latent</th></tr><tr><th align="left" valign="bottom">D = 10</th><th align="left" valign="bottom">D = 30</th><th align="left" valign="bottom">D = 100</th><th align="left" valign="bottom">D = 9</th><th align="left" valign="bottom">D = 8</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>Top-1 accuracy</italic></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">MLP (α = 0.01)</td><td align="char" char="." valign="bottom">9.9 (0.2)</td><td align="char" char="." valign="bottom">14.9 (0.2)</td><td align="char" char="." valign="bottom">20.4 (0.4)</td><td align="char" char="." valign="bottom">14.7 (0.2)</td><td align="char" char="." valign="bottom">17.0 (0.3)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.001)</td><td align="char" char="." valign="bottom">10.8 (0.1)</td><td align="char" char="." valign="bottom">17.3 (0.4)</td><td align="char" char="." valign="bottom"><bold>25.3 (0.3)</bold></td><td align="char" char="." valign="bottom">19.0 (0.3)</td><td align="char" char="." valign="bottom">22.7 (0.5)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.0001)</td><td align="char" char="." valign="bottom">10.7 (0.2)</td><td align="char" char="." valign="bottom">17.3 (0.3)</td><td align="char" char="." valign="bottom"><bold>25.1 (0.3)</bold></td><td align="char" char="." valign="bottom">20.6 (0.4)</td><td align="char" char="." valign="bottom">24.0 (0.2)</td></tr><tr><td align="left" valign="bottom"><italic>Top-5 accuracy</italic></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">MLP (α = 0.01)</td><td align="char" char="." valign="bottom">36.6 (0.4)</td><td align="char" char="." valign="bottom">45.1 (0.5)</td><td align="char" char="." valign="bottom">55.0 (0.3)</td><td align="char" char="." valign="bottom">46.5 (0.3)</td><td align="char" char="." valign="bottom">49.9 (0.4)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.001)</td><td align="char" char="." valign="bottom">38.6 (0.2)</td><td align="char" char="." valign="bottom">50.7 (0.6)</td><td align="char" char="." valign="bottom"><bold>62.9 (0.4)</bold></td><td align="char" char="." valign="bottom">54.0 (0.2)</td><td align="char" char="." valign="bottom">59.2 (0.6)</td></tr><tr><td align="left" valign="bottom">MLP (α = 0.0001)</td><td align="char" char="." valign="bottom">38.7 (0.5)</td><td align="char" char="." valign="bottom">50.8 (0.3)</td><td align="char" char="." valign="bottom"><bold>63.2 (0.4)</bold></td><td align="char" char="." valign="bottom">57.3 (0.4)</td><td align="char" char="." valign="bottom">61.6 (0.4)</td></tr></tbody></table></table-wrap></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.67855.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewing Editor</role><aff><institution>Cornell University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewer</role><aff><institution>Cornell University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Tchernichovski</surname><given-names>Ofer</given-names></name><role>Reviewer</role><aff><institution>Hunter College, City University of New York</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Linderman</surname><given-names>Scott W</given-names></name><role>Reviewer</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Animal vocalizations are notoriously complex and difficult to categorize. Traditionally, sounds are transformed into spectrograms, which are then segmented into syllables and analyzed according to hand-selected features such as pitch, amplitude and frequency modulation. Here, the authors take a new approach: they use variational autoencoders to analyze vocalizations from songbirds and mice and find that they can quantify the similarity between distinct utterances. This approach will complement existing sound analysis methods to further our understanding of animal social behavior.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 4 peer reviewers, including Jesse H Goldberg as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Ofer Tchernichovski (Reviewer #3); Scott W Linderman (Reviewer #4).</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic> (but note caveat below).</p><p>The reviewers mostly agreed that the VAE is a potentially interesting approach to categorizing vocalization data, and there was enthusiasm about the codebase available in github. Some major problems that arose in review were (1) lack of strong behavioral insights; (2) lack of clarity about data pre-processing – and how this would affect results; and (3) concern about novelty given the widespread use of VAEs for similar problems.</p><p>We would, in principle, be open to considering a revised version of this manuscript if the relatively long list of concerns by reviewers 2 and 4 were adequately addressed and if the VAE approach could perform similarity-score metrics (as requested by reviewer 3).</p><p><italic>Reviewer #1:</italic></p><p>The authors proposed the use of variational autoencoder (VAE) to vocal recordings of model organisms (mouse and zebra finch) to better capture the acoustic features that are missed by conventional acoustic metrics. This manuscript explores the effectiveness of the VAE approach from two perspectives: (i) a motifs based clustering which seeks to match the acoustics data against several predetermined template and (ii) an unsupervised clustering based on randomly segmented vocal recordings. These approaches involve the generation of a collection of images from spectrograms that are then fed to variational encoders to estimate the number of latent variables. With these latent variables, the authors then employed UMAP to visualize the variation within the dataset. The analyses are well conducted and will be useful for broad range of scientists investigating animal vocalizations.</p><p>i. From the zebra finch's discussion, this approach performs well in clustering the song syllables based on the four motifs predefined and at the same time could delineate the differences between directed and undirected songs as compared to previous acoustic metrics. While the authors provided a comparison between the ability of SAP acoustics features and VAE latent features in differentiating the directed and undirected birdsong, a comparison between the clustering of song syllables with the use of SAP features and VAE latent feature was not offered (Figure 5a). It would be interesting to see a side-by-side comparison of the 'Goodness of Clustering' metric in this VAE approach vs SAP.</p><p>ii. As for the randomly segmented vocal recordings, this method could generate a manifold where different acoustic features were situated in different regions and offer a continuous variability measure for all syllables. It is worth noting that the vocal recordings were randomly segmented based on the length of one syllable. What happens when the length is extended beyond that? Will the manifold produced look like the one shown in Figure 6?</p><p>iii. As for the mouse vocal recordings, the VAE approach generates a continuum-like points cloud that performs reasonably well in differentiating different acoustic features of the mouse vocalizations albeit the lack of boundaries in separating them. Could the smooth variation of points be due to the sampling rate? The mouse vocalizations were sampled at a much higher rate (10x) as compared to bird vocalizations. I would expect a greater resolution for the mouse data and thus the VAE can capture more subtle differences between the vocalizations, yielding a continuum-like points cloud. Of course these sampling rate differences are justified because of the different spectral properties of birdsong and mouse USVs – but a simple sentence or two about how sampling rates may affect these analyses would be useful..</p><p><italic>Reviewer #2:</italic></p><p>Authors introduce a new method for behavioral analysis of vocalizations that supposedly improves on weaknesses of handpicked features, which is that they miss variability and introduce undesirable correlations. They apply VAEs to bird and mouse vocalizations and perform analysis of the latent space. They show that VAEs outperform handpicked features on some simple analysis tasks including time-resolved analysis of variability.</p><p>Overall, even if it is correct that their method outperforms traditional-features based analysis, I don't think this insight is in any way relevant. Essentially, it is like saying: here is a bad way of doing things, and we offer one that is slightly better but still much worse than the gold standard. The problem is that there are many methods out there for doing things right from the start, so I cannot see a need for VAEs for the problems addressed. The problem with VAEs is that they are compressive (not representing all aspects of the data), which is why they are less desirable than methods that operate on equivalent representations of the raw data. If you are given money to buy groceries, why would you throw away part of it before you enter the store?</p><p>Overall it seems authors have not much to argue for other than their opposition to handpicked features. They pretend not much is out there other than these features for behavioral analysis, which does not reflect the wealth of methods out there. They have not even given appropriate credit to the real usefulness of the handpicked-feature approach, which is interpretability (which their method lacks). For example, one main utility of handpicked features is that some are learned. Songbirds have been shown to learn pitch from a tutor, so pitch is relevant. The same cannot be said about their latent features: which one of these do birds learn from a tutor?</p><p>Also, correlations among features that authors criticize as a bug can be seen as a feature, it provides insights into the aspects of their vocalizations that animals cannot control independently.</p><p>I could also not detect any new behavioral insights. The only real strength in their manuscript I could detect is that their method allows them to visually segregate directed from undirected song (Figure 4b), vocalizations from different mice strains (Figure 4f), and songbird syllables (Figure 5a). Thus, their method could be a suitable pre-processing for clustering efforts. Also, on a positive note, their method also produces some interesting looking plots (Figure 6b) of to-be identified utility.</p><p>L15 Authors claim that finding concise metrics remains a challenge, despite the large number of concise metrics ranging from Euclidean, Hamming, cosine, Wasserstein, Jensen-Shannon, Bhattacharia, Levenstein, ROUGE, etc. Surprisingly, their paper deals with the least concise metric imaginable, a deep autoencoder with thousands of parameters!</p><p>L17 Also not clear what would substantiate the claim that vocal behavior remains poorly understood.</p><p>L19 They mention several powerful approaches to enable automatic analysis of vocalizations but cite none.</p><p>L20 Given the wished-for generality of the problem they would like to study, it sounds strange that key to a successful approach would be a software package. Software packages are the last stage of analysis tool development.</p><p>L26 [10] did not discover overnight consolidation of learned birdsong as claimed, but instead 'inappropriate consolidation', which is challenged by a more recent analysis in [26].</p><p>L29 Authors criticize 'correlations among features', claiming these could result in redundant characterizations of vocalizations. Implicitly they argue for features that do not correlate, e.g. independent component analysis. But again, no citations, no following up on the embarked ideas. Correlations could actually be a feature of the behavior itself (and quite interesting to study).</p><p>I do not believe that mouse USV syllables form a continuum of syllables. Because authors did not find clusters, this does not mean that they are not there. Rather than trying to find shortcomings (e.g. of their method or the number of samples analyzed), authors generalize from a negative finding to inexistence. By their rejection of vocal clustering, they also ignore previous results showing such clustering [18, 4, 48, 6, 16]. Quite audacious. Is the moon still there when I close my eyes?</p><p>In the caption of Figure 2d, authors state 'an ideal representation would exhibit minimal off-diagonal correlation', i.e., ICA is ideal. Why do we need VAEs then if ICA is ideal?</p><p>Caption Figure 2e, the representational capacity will depend on the number of features, which is not reported. Same for Figure 2f, the more features used, the more principal components will be needed, so this may be a trivial effect of unequal number of features.</p><p>With regards to Figure 2f, it is not even clear from their analysis whether for a given fixed dimensionality, VAEs encode more variance than simple PCA, and if so, at what cost on memory (principal components vs auto-encoder network). For example, in the original Hinton paper in Science, the outcome of this analysis was rather surprising (VAEs are not clearly outperforming PCA in terms of representational capacity).</p><p>Last paragraph of Page 4, other than some pretty pictures (Figure S4) there is no (numerical) evidence for their claims of superiority of their latent features.</p><p>L115-125 and Figure 4: This is an ill-advised analysis. Why would one choose SAP features to detect changes in song? It is obvious that change detection requires the most sensitive analysis possible, so why would one perform compression beforehand? Same goes for their latent features. Even if it is better than SAP, why would one choose it and not the raw data itself (and a suitable standard metric)?</p><p>Same for L 126-L135 on data in mice.</p><p>L 173-L187: Authors argue about clustering failures of MUPET using UMAP representations, ignoring the fact that UMAP provides a faulty account of true distance. Their analysis of latent features is a step in the right direction, but falls short of analysis of the raw data (or an equivalent representation, e.g. https://asa.scitation.org/doi/abs/10.1121/1.4731466 and others).</p><p>L214: Their method is not useful for analyzing moment-by-moment variability of song, because they need to pre-process songs by 'warping each in time to account for well-documented differences in rhythm and tempo', which is the only problem that would complicate a more rigorous analysis.<italic>Reviewer #3:</italic></p><p>This manuscript presents a new implementation of variational autoencoder machine learning algorithm (VAE) for the analysis of animal vocalization. The new method is impressive and powerful compared to existing methods. I also like the AVA Python program package, which is well documented. Results demonstrate that AVA can capture important biological differences between vocalization e.g., between directed and undirected songs in birds, and identify similar syllables in mouse song. It can clusters syllables to types in adult zebra finches. The evidence for the lack of clusters in the mouse song are strong and convincing, and of important implications.</p><p>The principal weakness of the manuscript, in its present form, is that only insufficient evidence are provided to allow judging how AVA can perform in more difficult tasks, for which software like SAP is often used. For example, can AVA perform robust tutor-pupil similarity measurement in zebra finches? Can it identify clusters in young zebra finches? There is also no serious attempt to show replicability across different training sets. Once these concerns are address I feel that the manuscript should be appropriate for publication.</p><p>1. Detecting similarly as in Figure S5 across random repetition of the same zebra finch syllable spectrograms is not convincing enough. It is important to show how well can AVA performs when comparing tutor and pupil zebra finch songs. This testing should include examples of tutor pupil zebra finch songs (some with high similarity and others with lower similarity) should be plotted using UMAP projection as in Figure 6b.</p><p>2. It is relatively easy to detect clusters in adult zebra finch songs, but to study song learning it is often needed to cluster song syllables in young birds. SAP can often detect clusters in 50-day old birds. I wonder if AVA can detect clusters even earlier? This would be a very convincing demonstration to the power and usability of the new approach. Again, it is critical to show how AVA behaves with presented with more challenging tasks.</p><p>3. One issue that bothers me a lot is how the specific training of the algorithm might affect the outcomes. Say for example that lab 1 trained AVA with one dataset, and lab 2 trained AVA with a second dataset. But assume that both datasets were randomly sampled from the same population of birds. How comparable the results would be? For example will a similarity measurement of the same tutor and pupil would be comparable across the labs who trained AVA independently?</p><p>4. I like the &quot;higher representational capacity&quot; of the new method, but sometimes &quot;with much wisdom comes much sorrow&quot;: higher representation capacity can potentially cause trouble if it makes the method too sensitive to things we do not care about. At this level, I would like to see some evidence for robustness to noise. For example, it should be easy to test how sensitive AVA is for small differences in recording conditions, say, to recording with in a sound attenuation chamber while door is open or closed?</p><p><italic>Reviewer #4:</italic></p><p>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires.</p><p>The authors use variational autoencoders (VAEs) to learn a low-dimensional representations to spectrograms of bird song an mouse unltrasonic vocalizations (USVs). They find these representations of vocal behavior to be useful for studying social interactions and differences between strains of mice. Further investigations suggest that mouse USVs do not cluster as nicely as previously thought, and rather span a continuous manifold in feature space. Finally, VAEs trained on random snippets of the spectrogram highlight variability (and stereotypy) in zebra finch songs, in contrast to the unstructured (or highly variable) space of mouse USVs.</p><p>The proposed model operates in the frequency domain, consuming snippets of time-warped spectrograms. Given that one of the central claims of this paper is the superiority of unsupervised methods for feature extraction, I think these preprocessing steps warrant further consideration. For example, the 2D convolutions in the VAE must implicitly leverage the fact that neighboring frequencies are adjacent in the 128x128 ``images,' but does the choice of frequency spacing (mel-spaced for song birds vs linearly-spaced for mouse USVs) affect the learned representations? How important is the time-warping to downstream representations and analyses? The spectral preprocessing also complicates the ``shotgun' analysis in Figure 6. Each point in the VAE latent space corresponds to a window of time rather than a single frame. How do the projections change as you vary the window size?</p><p>Motivated in part by these concerns, some recent approaches like WaveNet (van den Oord et al., 2016) have directly modeled raw waveform data. The sampling rates used for human speech and music (16kHz) are lower than those necessary for USVs, but the same principles should apply. For example, a pre-trained WaveNet with minimal fine-tuning for song bird or mouse USV could yield a very competitive generative model of vocalizations and offer a different representation of this behavior. The comparison may be beyond the scope of this paper, but I think it is worthy of discussion.</p><p>Overall, this paper offers a nice application of nonlinear latent variable models to vocal behavior data. The techniques themselves are not particularly novel – variational autoencoders have been widely used in the machine learning community for over five years now – and the finding that learned features can outperform handcrafted ones has been shown across many domains. Given the wealth of works on sequential VAEs for time-series data, I think the novelty of the shotgun VAE is somewhat overstated. In my view, the main contribution lies in the codebase (I looked through the Github repo and was quite impressed!), the analysis pipeline, and the proof-of-concept. That is why I think it is especially important to assess the sensitivity of these results to various design choices that went into the pipeline, including the very first choices about how to preprocess the raw waveform data into time-warped and windowed spectrograms.</p><p>Other comments:</p><p>– Figure 1a: It's not clear why the length 32 vector is appearing as a square matrix here.</p><p>– Please label which dataset (song bird or mouse) the point clouds and spectrograms are coming from in all of the figures. A consistent color scheme could help.</p><p>– Figure 2f only has three solid lines. Where is the plot of explained variance in MUPET features by VAE features?</p><p>– The paragraph starting on line 97 paints an overly optimistic view of VAEs. Designing deep generative models that can reliably disentangle latent factors is still an active area of research, as is model selection.</p><p>– Figures 3, S4, and S5 suggest that nearest neighbor reconstruction with DeepSqueak (and other handcrafted features) is surprisingly bad. Are you using just the Euclidean distance in DeepSqueak feature space? Did you whiten the DeepSqueak features before computing distances? Can you explain why it DeepSqueak is failing so dramatically?</p><p>– Throughout, the spectrograms fail to indicate the time window or frequency bands.</p><p>– Figure 4a+b aim to show that SAP do not separate directed and undirected vocalizations as well as latent features do, but is this information simply not present in the first two PCs? A classification accuracy assessment would be more convincing.</p><p>– The 2D embedding in Figure 4f is confusing to me. Why not just show the full distance matrix from Figure S8, but with the lines to indicate which rows/columns belong to each mouse? That figure gives considerably more information than the tSNE embedding, in my opinion. In particular, it looks like there is a solid group of C57 mice that are very similar to DBA mice, as measured by MMD. The use of tSNE seems rather arbitrary and lossy. Moreover, the colors in Figure 4f convey little information beyond identity, when there seems to be lots of extra info about strain that could be conveyed.</p><p>– There are no axis labels or titles in Figure 5a-c, just random clouds of points.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Jesse H Goldberg as the Reviewing Editor and Reviewer #1. and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Ofer Tchernichovski (Reviewer #2); Scott W Linderman (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission. This was deemed a strong resubmission and only one relatively minor issue related to Figure 7 is necessary for revision.</p><p>Essential revisions:</p><p>1. Figure 7 does not show clearly enough how the SG approach overcomes the problem of fused syllables (A and B). Figure 7c should somehow indicate the similarity in the fused AB vs A,B area. The issue is that it is not easy to see how the color code correspond to specific areas in the sonograms presented. An overlay panel could help here.</p><p><italic>Reviewer #1:</italic></p><p>In this paper, the authors addressed the reviewers' concerns and expanded extensively on the utility of variational autoencoder (VAE). The authors included an extra section discussing VAE 's capability in handling more complicated scenarios by studying the tutor and pupil song learning experiment. One can readily visualize the differences between tutor and pupil syllables via the latent embeddings. Although the latent features could be hard to interpret, one could view it as an initial exploratory analysis in identifying possible acoustic structure discrepancies. The authors also included additional data benchmarking latent features against conventional acoustics features for classification tasks and offered a more in-depth study comparing the clustering of song syllables using traditional acoustic features and VAE latent features. Moreover, they discussed the effect of time stretch and frequency spacing parameters on SAP features prediction and VAE's replicability issue for completeness.</p><p>The new Figure 7 showing tutor-pupil analyses is a welcome addition to the paper.</p><p>While it remains uncertain if this method will actually supersede others in quantifying finch and/or mouse datasets, this paper could, at minimum, provide a case study of advantages and disadvantages for using the VAE approach for vocalization datasets.</p><p><italic>Reviewer #2:</italic></p><p>This study applies an unsupervised learning approach for assessing acoustic similarity and for classifying animal vocalizations. Investigation focuses on mice vocalization and song learning in zebra finches. The method demonstrate an impressive capacity to map and compare vocal sounds in both species and to assess vocal learning. It has clear advantages upon existing methods. It is still an open question to what extent this approach can successfully capture vocal development during early stages of song learning. In particular, the learned latent features have no simple interpretation in production and perception of vocal sounds, which future studies will need to address.</p><p>Two remaining issues:</p><p>1. figure 7 does not show clearly enough how the SG approach overcomes the problem of fused syllables (A and B). Figure 7c should somehow indicate the similarity in the fused AB vs A,B area. The issue is that it is not easy to see how the color code correspond to specific areas in the sonograms presented. An overlay panel could help here.</p><p>2. The lack of song development analysis is still an issue.</p><p><italic>Reviewer #3:</italic></p><p>I thank the authors for their detailed responses. They have satisfactorily addressed all of my concerns.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.67855.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>The reviewers mostly agreed that the VAE is a potentially interesting approach to categorizing vocalization data, and there was enthusiasm about the codebase available in github. Some major problems that arose in review were (1) lack of strong behavioral insights; (2) lack of clarity about data pre-processing – and how this would affect results; and (3) concern about novelty given the widespread use of VAEs for similar problems.</p></disp-quote><p>We thank all four reviewers for their detailed and thoughtful critiques. In our substantially revised manuscript, we have attempted to address each of the concerns above by adding a major new analysis and numerous supplementary analyses that we believe help to round out our discussion and demonstrate the utility of the VAE as a tool for vocal analysis. In particular:</p><p>In response to a suggestion by Reviewer 3, we have included a significant new analysis comparing the similarity of zebra finch tutor and pupil song, now Figure 7. We believe these new results both demonstrate an improvement on existing methods and clearly point the way toward addressing point (1) above, since assessing similarity between tutor and pupil song constitutes a key method of behavioral analysis in birdsong research.</p><p>1. In response to suggestions by Reviewers 1, 3, and 4, we have performed several new analyses (included as Supplementary Figures 4, 5, 8, 11, 13, 14, 15, 17, and 18 and Supplementary Tables 1-3) that examine the effects of preprocessing choices, the reliability of training, and the quantitative performance of VAE features for downstream tasks. We discuss each of these more fully below; collectively, these work to address the concerns raised under point (2) above.</p><p>2. In response to concerns raised by Reviewers 2 and 4, we have revised the text to better place our work in the context of both vocal analysis and machine learning more broadly (point (3)). We clarify that, while the VAE has been applied to many kinds of problems, establishing its usefulness for a new audience is nonetheless valuable (as the reviewers’ comments attest) and necessitates a careful comparison with existing approaches to build confidence in the method.</p><p>Taken together, we believe these revisions both address each of the key concerns listed above and greatly improve the rigor and reach of the work. Replies to individual reviewers follow. Original comments are displayed in <italic>italics</italic>, our response, as here, in bold. Text added or altered in the revised manuscript is in blue.</p><disp-quote content-type="editor-comment"><p>We would, in principle, be open to considering a revised version of this manuscript if the relatively long list of concerns by reviewers 2 and 4 were adequately addressed and if the VAE approach could perform similarity-score metrics (as requested by reviewer 3).</p><p>Reviewer #1:</p><p>The authors proposed the use of variational autoencoder (VAE) to vocal recordings of model organisms (mouse and zebra finch) to better capture the acoustic features that are missed by conventional acoustic metrics. This manuscript explores the effectiveness of the VAE approach from two perspectives: (i) a motifs based clustering which seeks to match the acoustics data against several predetermined template and (ii) an unsupervised clustering based on randomly segmented vocal recordings. These approaches involve the generation of a collection of images from spectrograms that are then fed to variational encoders to estimate the number of latent variables. With these latent variables, the authors then employed UMAP to visualize the variation within the dataset. The analyses are well conducted and will be useful for broad range of scientists investigating animal vocalizations.</p><p>i. From the zebra finch's discussion, this approach performs well in clustering the song syllables based on the four motifs predefined and at the same time could delineate the differences between directed and undirected songs as compared to previous acoustic metrics. While the authors provided a comparison between the ability of SAP acoustics features and VAE latent features in differentiating the directed and undirected birdsong, a comparison between the clustering of song syllables with the use of SAP features and VAE latent feature was not offered (Figure 5a). It would be interesting to see a side-by-side comparison of the 'Goodness of Clustering' metric in this VAE approach vs SAP.</p></disp-quote><p>We thank the reviewer for this suggestion. Results from this analysis are shown in Figure S13, which shows latent features produce better clusters than MUPET features, and Figure S14, which shows that SAP features also admit well-clustered clusters. In fact, on two of the three metrics we examine, SAP features produce better clusters than VAE features, while on the third, VAE features perform best. We believe these results illustrate two points:</p><p>1. When data cleanly separate into clusters — as they do in the bird data — multiple methods perform well, with the best-performing method depending on what measure one wants to optimize. That is, tight, well-isolated clusters satisfy multiple (semantically overlapping) notions of what clustering entails.</p><p>2. When data do not obviously cluster, the particular definition of clustering, as well as the assumptions of clustering methods, play a much larger role. As a result, MUPET features produce worse clusters than VAE features (by the metrics investigated). We unpack this more fully in our response to Reviewer 2 below.</p><disp-quote content-type="editor-comment"><p>ii. As for the randomly segmented vocal recordings, this method could generate a manifold where different acoustic features were situated in different regions and offer a continuous variability measure for all syllables. It is worth noting that the vocal recordings were randomly segmented based on the length of one syllable. What happens when the length is extended beyond that? Will the manifold produced look like the one shown in Figure 6?</p></disp-quote><p>This is an excellent question. Results are presented in Figure S17. Clearly, the manifolds are not identical, since (a) changing the size of the window introduces a change in the amount of data latent features must represent, and (b) the UMAP projection does not necessarily preserve distances or orientation, only local relationships. Nonetheless, features of the song like the linking note spiral and the song motif, including the correct ordering of syllables, are still present, consistent with the fact that the shotgun VAE does preserve local similarities in the data.</p><disp-quote content-type="editor-comment"><p>iii. As for the mouse vocal recordings, the VAE approach generates a continuum-like points cloud that performs reasonably well in differentiating different acoustic features of the mouse vocalizations albeit the lack of boundaries in separating them. Could the smooth variation of points be due to the sampling rate? The mouse vocalizations were sampled at a much higher rate (10x) as compared to bird vocalizations. I would expect a greater resolution for the mouse data and thus the VAE can capture more subtle differences between the vocalizations, yielding a continuum-like points cloud. Of course these sampling rate differences are justified because of the different spectral properties of birdsong and mouse USVs – but a simple sentence or two about how sampling rates may affect these analyses would be useful..</p></disp-quote><p>The answer to the specific question here is simple — while the raw data were sampled at different rates, the spectrogram data on which the VAE operates were of the same dimension (128 x 128 after preprocessing) in both bird and mouse — but raises an important issue regarding clustering: Is the quasi-continuum we see in Figure 5b,c the result of <italic>some kind</italic> of “fill in” behavior (as we ourselves raise in ll. 318-328)? Might there be some feature of the VAE that favors an unclustered latent representation?</p><p>This is an important reason for including both bird and mouse data, as we have done in the manuscript. Under the same set of analyses, bird data reliably and cleanly cluster, while the mouse data are, even under the most generous interpretation, ambiguous. Moreover, our data set sizes are not drastically larger than those used by other groups (again, we use MUPET’s own benchmark data), which argues against the idea that clustering is stymied by the size of the data set. But of course, as Reviewers 1and 2 both point out, UMAP is a visualization strategy, one not guaranteed to preserve metric information, which may lead to misleading displays. This is why we have performed all analysis on latent features (<italic>not</italic> UMAP representations) and attempted to tackle the problem from multiple directions, as we further explain in response to Reviewer 2 below.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Authors introduce a new method for behavioral analysis of vocalizations that supposedly improves on weaknesses of handpicked features, which is that they miss variability and introduce undesirable correlations. They apply VAEs to bird and mouse vocalizations and perform analysis of the latent space. They show that VAEs outperform handpicked features on some simple analysis tasks including time-resolved analysis of variability.</p><p>Overall, even if it is correct that their method outperforms traditional-features based analysis, I don't think this insight is in any way relevant. Essentially, it is like saying: here is a bad way of doing things, and we offer one that is slightly better but still much worse than the gold standard. The problem is that there are many methods out there for doing things right from the start, so I cannot see a need for VAEs for the problems addressed. The problem with VAEs is that they are compressive (not representing all aspects of the data), which is why they are less desirable than methods that operate on equivalent representations of the raw data. If you are given money to buy groceries, why would you throw away part of it before you enter the store?</p></disp-quote><p>We believe it would be helpful to clarify here: our claim is not that traditional feature based analysis is a “bad” way of doing things. Clearly, this approach, whatever its limitations, has proven tremendously fruitful and continues to be the standard of practice in much of the vocalization community. Our goal is to suggest the VAE as an additional tool that offers improvements in several cases where feature-based methods have encountered difficulty, as our analyses show.</p><p>We must also confess some confusion as to which particular gold standard the reviewer is referencing, and this makes it difficult for us to reply on this point. Moreover, we are unsure how some degree of data compression is to be avoided. Feature-based methods are all a form of compression — they select some aggregate measures of the data while eschewing others — as are spectrograms, which throw away phase data. If nothing else, standard statistical tests each reduce the entire data to a single summary statistic for hypothesis testing.</p><p>To be clear: when a given analysis question can be answered by a method operating on raw data (with no intermediate compressed representation), we are not, <italic>a priori</italic>, advocating against this. But in the literature, there are many studies in which feature based methods remain the approach of choice (Derégnaucourt et al. (2005), Gaub et al. (2010), Hammerschmidt et al. (2012), Holy and Guo (2005), Chabout et al. (2015), Woehr (2014)), and we would argue that the present manuscript shows that VAE methods may offer advantages in these cases. It is difficult to say much more than this without a particular alternative analysis in view.</p><disp-quote content-type="editor-comment"><p>Overall it seems authors have not much to argue for other than their opposition to handpicked features. They pretend not much is out there other than these features for behavioral analysis, which does not reflect the wealth of methods out there. They have not even given appropriate credit to the real usefulness of the handpicked-feature approach, which is interpretability (which their method lacks). For example, one main utility of handpicked features is that some are learned. Songbirds have been shown to learn pitch from a tutor, so pitch is relevant. The same cannot be said about their latent features: which one of these do birds learn from a tutor?</p><p>Also, correlations among features that authors criticize as a bug can be seen as a feature, it provides insights into the aspects of their vocalizations that animals cannot control independently.</p></disp-quote><p>We apologize for some lack of clarity on several points relevant to the reviewer’s concerns. In the interest of brevity and clarity, we have not attempted a comprehensive review of methods for the analysis of animal vocalization. Nor do we claim that the VAE could or should supersede all of these. Instead, we have focused on the feature-based approach, which overwhelmingly remains the standard of practice in the field and forms the basis of the analysis packages (SAP, MUPET, DeepSqueak) against which we provide extensive comparison.</p><p>We can also offer more clarity on two specific points the reviewer raises here:</p><p>1. While the learned latent features are not interpretable in the sense that they precisely correspond to constructs like pitch and maximum frequency well-known in signal processing, the point of panels 1a-c and Figure S1 is that this information is nonetheless present in the latent features in a straightforward way. That is, simple linear combinations of the latent features often correlate well (if imperfectly) with the traditional metrics of interest (Figure S1), in addition to capturing some information missed by these features.</p><p>The reviewer’s point about pitch is an important one, which we might rephrase as follows: it is not so much that juveniles learn pitch <italic>per se</italic> from a tutor as that juveniles are engaged in a complex, multi-feature problem of song learning, successful completion of which involves changes in pitch. That is, pitch forms only one axis along which behavior is changing, and our preference for describing the more complex learning problem in these terms is our own familiarity with this construct, both quantitatively and experientially. And while we do not wish to dispute the reasonability of this, as our new tutor/pupil song comparison shows (Figure 7), latent features precisely capture the similarity of the resulting songs, which has proven difficult to do with existing feature-based methods.</p><p>On decorrelation of features: As the reviewer notes, decorrelation of learned features is not, <italic>a priori</italic>, a bug, and the correlations among traditional features could possibly be put to interesting uses. In our analysis of feature decorrelation, our primary concern is for subsequent <italic>statistical</italic> testing, where nonlinear relationships among features violate the assumptions of many statistical tests and may lead to erroneous results. If traditional features were linearly related to one another, then a simple PCA or whitening operation, as is often done, would be sufficient to remedy this, but this does not appear to hold in real data (Figure S8). VAE features, which allow for nonlinear warping, avoid this. We have attempted to clarify this in the text (ll. 115-117):</p><p>“While correlations among features are not necessarily undesirable, they can complicate subsequent statistical testing because nonlinear relationships among features violate the assumptions of many statistical tests. VAE features, which allow for nonlinear warping, avoid this potential difficulty.”</p><disp-quote content-type="editor-comment"><p>I could also not detect any new behavioral insights. The only real strength in their manuscript I could detect is that their method allows them to visually segregate directed from undirected song (Figure 4b), vocalizations from different mice strains (Figure 4f), and songbird syllables (Figure 5a). Thus, their method could be a suitable pre-processing for clustering efforts. Also, on a positive note, their method also produces some interesting looking plots (Figure 6b) of to-be identified utility.</p></disp-quote><p>We apologize that what we believe to be novel behavioral insights were not better delineated. Briefly:</p><p>1. Mice produce stereotyped syllable repertoires that are stable across days (Figures 4f and S11, Table S3), but there is no evidence that the <italic>sequences</italic> in which these syllables are produced are stereotyped (Figure 6a).</p><p>2. We argue that the best account of USV distributions is an unclustered, quasicontinuum account (Figure 5; more on this below).</p><p>3. We show that zebra finch directed song is less variable in all syllable types, not just the tonal or harmonic syllables for which it has been established. That is, the VAE allows this to be observed and quantified for <italic>all</italic> syllables (Figures 4a-c), and this on sub-syllable time scales (6c-e).</p><p>4. We demonstrate a novel VAE-based method of computing tutor-pupil similarity in birdsong and quantify it in Figure 7.</p><disp-quote content-type="editor-comment"><p>L15 Authors claim that finding concise metrics remains a challenge, despite the large number of concise metrics ranging from Euclidean, Hamming, cosine, Wasserstein, Jensen-Shannon, Bhattacharia, Levenstein, ROUGE, etc. Surprisingly, their paper deals with the least concise metric imaginable, a deep autoencoder with thousands of parameters!</p></disp-quote><p>We apologize for the confusion. We are here using “metrics” in the more colloquial sense of “useful measurements,” not the mathematical construct the reviewer seems to be referencing. This has been fixed in the text, which now reads (ll. 16-17):</p><p>“[G]iven the variety and complex temporal structure of many behaviors, finding concise yet informative descriptions has remained a challenge.”</p><disp-quote content-type="editor-comment"><p>L19 They mention several powerful approaches to enable automatic analysis of vocalizations but cite none.</p></disp-quote><p>Thank you. This has been fixed (ll. 19-21).</p><disp-quote content-type="editor-comment"><p>L20 Given the wished-for generality of the problem they would like to study, it sounds strange that key to a successful approach would be a software package. Software packages are the last stage of analysis tool development.</p></disp-quote><p>The primary purpose of this work is to introduce and establish the VAE as a useful tool for vocal analysis. While we would contend that this is <italic>conceptually</italic> independent from any particular software implementation, we also note that, <italic>practically</italic>, code helps. We provide code as a means of disseminating these techniques, which Reviewers 3 and 4 applauded as providing additional value.</p><disp-quote content-type="editor-comment"><p>L26 [10] did not discover overnight consolidation of learned birdsong as claimed, but instead 'inappropriate consolidation', which is challenged by a more recent analysis in [26].</p></disp-quote><p>Thank you for this correction. The text has been edited to read (ll. 26-29):</p><p>“Collectively, these and similar software packages have helped facilitate numerous discoveries, including circadian patterns of song development in juvenile birds [10], …”</p><disp-quote content-type="editor-comment"><p>L29 Authors criticize 'correlations among features', claiming these could result in redundant characterizations of vocalizations. Implicitly they argue for features that do not correlate, e.g. independent component analysis. But again, no citations, no following up on the embarked ideas. Correlations could actually be a feature of the behavior itself (and quite interesting to study).</p></disp-quote><p>While we agree that correlations among features might provide interesting information, they can also reflect simple mathematical relationships among the feature sets chosen (x and x<sup>3</sup> are strongly correlated by construction). However, as we have attempted to clarify under point 2 in our response above, such correlations can prove highly statistically problematic when attempting to test hypotheses about, e.g., changes in sets of vocal features across days, animals, or experimental conditions.</p><disp-quote content-type="editor-comment"><p>I do not believe that mouse USV syllables form a continuum of syllables. Because authors did not find clusters, this does not mean that they are not there. Rather than trying to find shortcomings (e.g. of their method or the number of samples analyzed), authors generalize from a negative finding to inexistence. By their rejection of vocal clustering, they also ignore previous results showing such clustering [18, 4, 48, 6, 16]. Quite audacious. Is the moon still there when I close my eyes?</p></disp-quote><p>We believe the reviewer is voicing here a warranted skepticism of our most controversial claim. And of course the absence of evidence is not evidence of absence. Yet, as we lay out in the manuscript (ll. 187-193), there is no absolute correct answer to the question of whether some particular data cluster, only more or less satisfying accounts. Reviewer 3 was convinced by our presentation, while others may remain skeptical. In favor of our conclusion, we note:</p><p>1. We argue from three directions: (a) the lack of apparent clustering for USVs as compared to bird song syllables (Figures 5a-b, S12); (b) the markedly worse performance of clustering algorithms (as quantified by several methods) in the mouse as compared to bird case (Figures 5c, S12-S15); (c) the existence of interpolating sequences within the data between qualitatively different USV syllables (Figures 5d, S16). Of these three, the last does not depend at all on VAE features and clearly fails for the bird song example.</p><p>2. Several of the works cited by ourselves and the reviewer ([4,6,16]) simply <italic>assume</italic> that mouse USVs cluster or that clustering offers a useful quantitative account of the behavior. One work [18] provides evidence of syllable clustering on the basis of a single-dimensional time/frequency curve description of USVs using a relatively small dataset of 750 syllables. We believe our analysis complements this approach by considering a more complete syllable description applied to much larger collections of syllables (17,400 syllables in Figures 5b-c). One of these works [48] reports a single bimodally-distributed acoustic feature, peak frequency, and refers to corresponding modes as clusters, but does not explicitly argue for clustering. To the best of our knowledge, our work provides the most comprehensive investigation of the USV syllable clustering hypothesis.</p><p>3. Many of the same works that lay claim to clustering use traditional features similar to MUPET features to cluster. To explicitly test whether MUPET features produce better behaved clusters than learned latent features, we added the analysis depicted in Figure S13. We found that MUPET features actually produce much worse clusters than latent features. In fact, when quantified by three unsupervised clustering metrics, MUPET feature-derived clusters are judged to be <italic>less</italic> clustered than moment-matched Gaussian noise.</p><p>4. On a more practical note, we have added Figure S15, which explores the consistency of syllable clusters under independent clusterings of different data splits. If cluster structure is well-determined by the data, we would expect syllables to be reliably segregated into the same clusters. This is indeed what we find for six clusters of zebra finch syllables, the number of clusters found by hand-labeling. However, for mouse syllables, we find poor consistency for more than two clusters.</p><p>5. Thus, while we cannot definitively prove the absence of clusters (nor do we know how to make such a claim rigorous), we have shown by a series of analyses that the USV data we analyze do not readily conform to a clustering account in the way bird song syllables do. Not only are the results markedly worse quantitatively, we show that there are sequences of USVs <italic>in the data</italic> that connect even the most disparate syllable types. We believe this calls into question the <italic>utility</italic> of a clustering account of USVs.</p><p>6. Finally, we have attempted in our original manuscript to nuance our conclusion (ll. 318-328), discussing multiple reasons we might have failed to find clusters. These include being open to the possibility of different results in other species/strains,</p><p>data set size, and a distinction between mathematical clustering and categorical perception.</p><disp-quote content-type="editor-comment"><p>In the caption of Figure 2d, authors state 'an ideal representation would exhibit minimal off-diagonal correlation', i.e., ICA is ideal. Why do we need VAEs then if ICA is ideal?</p></disp-quote><p>While ICA results in independence, it does so by choosing linear combinations of the original input vectors. The VAE can be viewed in one way as a nonlinear generalization of this demixing operation. So while ICA might be ideal if linear independence were the only desideratum, it must usually be paired with some form of dimension reduction, which the VAE will perform in a way that preserves more information.</p><p>Nonetheless, we agree that the point is confusing in the way it was stated. In the revised text, we have clarified (Figure 2 caption):</p><p>“When applied to the mouse USVs from a-c, the acoustic features compiled by the analysis program MUPET have high correlations, effectively reducing the number of independent measurements made.”</p><disp-quote content-type="editor-comment"><p>Caption Figure 2e, the representational capacity will depend on the number of features, which is not reported. Same for Figure 2f, the more features used, the more principal components will be needed, so this may be a trivial effect of unequal number of features.</p><p>With regards to Figure 2f, it is not even clear from their analysis whether for a given fixed dimensionality, VAEs encode more variance than simple PCA, and if so, at what cost on memory (principal components vs auto-encoder network). For example, in the original Hinton paper in Science, the outcome of this analysis was rather surprising (VAEs are not clearly outperforming PCA in terms of representational capacity).</p></disp-quote><p>Thank you. We have added the number of features to the caption of Figure 2e. In Figure 2f, we agree, and have truncated the latent feature dimension to match the number of traditional features for a fairer comparison.</p><p>“Reducing the dimensionality of data with neural networks” (Hinton and Salakhutdinov, 2006) concerns deterministic autoencoders, which we do not believe are directly relevant to this work.</p><disp-quote content-type="editor-comment"><p>Last paragraph of Page 4, other than some pretty pictures (Figure S4) there is no (numerical) evidence for their claims of superiority of their latent features.</p></disp-quote><p>While we would argue that Figures 3 and S6 (previously S4) demonstrate clear qualitative improvements in nearest neighbor recovery, we have also included in our revised submission additional details regarding the failure of DeepSqueak features in identifying nearest neighbors (Figure S8) and several comparisons between the utility of VAE features and other feature sets for downstream classification tasks (Tables S1-S3). As those results demonstrate, VAE features clearly outperform handpicked feature sets.</p><disp-quote content-type="editor-comment"><p>L115-125 and Figure 4: This is an ill-advised analysis. Why would one choose SAP features to detect changes in song? It is obvious that change detection requires the most sensitive analysis possible, so why would one perform compression beforehand? Same goes for their latent features. Even if it is better than SAP, why would one choose it and not the raw data itself (and a suitable standard metric)? Same for L 126-L135 on data in mice.</p></disp-quote><p>We appreciate the reviewer raising this point. In situations with many more features than data points, learning in a generalizable way that does not severely overfit the data requires extra assumptions, for example sparsity or other informative prior beliefs. Dimension reduction can be seen as an attempt to cope with this fundamental limitation: if we can distill the relevant variation in our data into a much lower dimensional space, then we can re-enter the classical regime with more data points than features where off-the-shelf regression and classification are known to work well.</p><p>We also note that many highly influential analyses of song learning have relied on SAP features to detect changes in song (Tchernichovski et al. “Dynamics of the vocal imitation process: how a zebra finch learns its song” (2001); Ravbar et al. “Vocal exploration is locally regulated during song learning” (2012); Vallentin et al. &quot;Inhibition protects acquired song segments during vocal learning in zebra finches.&quot; (2016); and many others).</p><disp-quote content-type="editor-comment"><p>L 173-L187: Authors argue about clustering failures of MUPET using UMAP representations, ignoring the fact that UMAP provides a faulty account of true distance. Their analysis of latent features is a step in the right direction, but falls short of analysis of the raw data (or an equivalent representation, e.g. https://asa.scitation.org/doi/abs/10.1121/1.4731466 and others).</p></disp-quote><p>First, all of our clustering comparisons are performed on features (MUPET, SAP, VAE, etc.), with UMAP only used for visualization, as stated in the manuscript. And we agree with the reviewer that the UMAP representation cannot tell the whole story. Our UMAP plot simply serves the purpose of providing a contrast between mouse and bird syllables and motivating further analyses.</p><p>As for the reviewer’s other concern, we <italic>think</italic> the reviewer is suggesting that clustering might better be performed on the raw data, and that failure to find clusters in the USV case might be attributable to dimension reduction. To this objection, we first note that the studies we cite that report clustering of USVs (with which the reviewer appears to agree) all use a feature-based clustering approach. It is hard to see how both (a) featurebased clustering methods are illegitimate for USVs and (b) reports of USV clustering based on features can be trusted.</p><p>Moreover, we are unsure what clustering based on raw data in this case might mean. Even our spectrograms, which ignore phase data, comprise tens of thousands of data points per observation, and clustering data in dimensions this high is known to be borderline meaningless without strong priors or dimension reduction (cf. Aggarwal, Hinneberg, Keim (2001); Bouveyron and Brunet-Saumard (2014)). The reviewer may have one of these specific techniques in mind, but we also note that the highly structured nature of spectrograms implies that invariances in the interpretation of the data (like small pixel shifts in time) may not be respected by a naive clustering of raw data.</p><disp-quote content-type="editor-comment"><p>L214: Their method is not useful for analyzing moment-by-moment variability of song, because they need to pre-process songs by 'warping each in time to account for well-documented differences in rhythm and tempo', which is the only problem that would complicate a more rigorous analysis.</p></disp-quote><p>We apologize for some lack of clarity in our description. While time warping does improve our ability to align and thus compare syllables, the difference is not nearly so large as might be imagined. Figure S18 reproduces Figures 6d-e without time warping applied. More to the point, we are a bit confused by the contention that, apart from time warping, the moment-by-moment analysis of variability in song is trivial. We were unaware of any published results on this problem. Indeed several other reviewers noted this new analysis as a real advance beyond the current state of the art.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This manuscript presents a new implementation of variational autoencoder machine learning algorithm (VAE) for the analysis of animal vocalization. The new method is impressive and powerful compared to existing methods. I also like the AVA Python program package, which is well documented. Results demonstrate that AVA can capture important biological differences between vocalization e.g., between directed and undirected songs in birds, and identify similar syllables in mouse song. It can clusters syllables to types in adult zebra finches. The evidence for the lack of clusters in the mouse song are strong and convincing, and of important implications.</p><p>The principal weakness of the manuscript, in its present form, is that only insufficient evidence are provided to allow judging how AVA can perform in more difficult tasks, for which software like SAP is often used. For example, can AVA perform robust tutor-pupil similarity measurement in zebra finches? Can it identify clusters in young zebra finches? There is also no serious attempt to show replicability across different training sets. Once these concerns are address I feel that the manuscript should be appropriate for publication.</p></disp-quote><p>We appreciate the reviewer’s positive assessment of the work. Based on the reviewer’s suggestions, we have implemented several new analyses detailed below. We hope the reviewer agrees that these significantly enhance the work by demonstrating the robustness and applicability of the VAE.</p><disp-quote content-type="editor-comment"><p>1. Detecting similarly as in Figure S5 across random repetition of the same zebra finch syllable spectrograms is not convincing enough. It is important to show how well can AVA performs when comparing tutor and pupil zebra finch songs. This testing should include examples of tutor pupil zebra finch songs (some with high similarity and others with lower similarity) should be plotted using UMAP projection as in Figure 6b.</p></disp-quote><p>This is an excellent suggestion. To address this, we have added an entirely new analysis (Figure 7) to test how well the VAE-based approach performs detecting tutor and pupil song similarity. In this analysis we take 10 pairs of zebra finch tutors and pupils and inspect the VAE’s learned representations using both syllable-based and shotgun VAE approaches. For both approaches, we find highly similar learned representations for the tutor and pupil pairs (Figure 7a,d). Additionally, we find that the VAE represents finer scale variations in the quality of song copying (Figure 7b,e) and demonstrate that the maximum mean discrepancy (MMD) metric proposed earlier in the manuscript is effective for quantifying the quality of song copying. We believe this constitutes a novel advance on current practice for quantifying tutor/pupil similarity in zebra finch song.</p><disp-quote content-type="editor-comment"><p>2. It is relatively easy to detect clusters in adult zebra finch songs, but to study song learning it is often needed to cluster song syllables in young birds. SAP can often detect clusters in 50-day old birds. I wonder if AVA can detect clusters even earlier? This would be a very convincing demonstration to the power and usability of the new approach. Again, it is critical to show how AVA behaves with presented with more challenging tasks.</p></disp-quote><p>We agree with the reviewer that the ability to cluster adult songs is no surprise. We also agree that identifying syllables in young birds is a much more interesting test. Indeed, as part of a separate collaboration between our labs, we have been pursuing these questions. In practice, we find the major obstacle to detecting syllable clusters in very young birds is our ability to reliably segment syllables. In order to bypass this difficulty, we are currently using the shotgun VAE approach for analyzing the song of very young birds. Encouragingly, despite the segmenting difficulties, we find that the syllable-based VAE analysis identifies some structure in the song of a 50-day-old juvenile, including a clear separation between calls and subsong.</p><p>Largely the same structure is evident five days earlier, when the bird is 45 days old.</p><disp-quote content-type="editor-comment"><p>3. One issue that bothers me a lot is how the specific training of the algorithm might affect the outcomes. Say for example that lab 1 trained AVA with one dataset, and lab 2 trained AVA with a second dataset. But assume that both datasets were randomly sampled from the same population of birds. How comparable the results would be? For example will a similarity measurement of the same tutor and pupil would be comparable across the labs who trained AVA independently?</p></disp-quote><p>We share this concern and note two enframing points: First, replicability will be a function of what question is subsequently asked of the VAE. For instance, any rotation of features in latent space is equally valid, so there is no guarantee – in fact, it is unlikely – that “latent feature 1” will be the same across training runs. However, other analyses that are invariant to this degree of freedom may replicate across retraining. Second, producing replicable results across training runs of neural network models constitutes an interesting and important research question in machine learning, one on which we have a separate manuscript in preparation. That is, while we believe this is an important and interesting point, a proper treatment of it is beyond the scope of this work.</p><p>To address the reviewer’s suggestion, we added Figure S5 to investigate the similarity of latent features across training runs, both using identical training data and disjoint subsets of data. For mouse USV syllables, we find very good correspondence in both cases in terms of pairwise distances and linear predictions of one set of latent features from the other. For zebra finch syllables, we find poorer pairwise distances but fairly accurate linear predictions in both cases. Taken together with Figure S15, which shows near-perfect identification of zebra finch syllable clusters, we believe the poorer performance of zebra finch syllable alignment compared to mouse syllable alignment is driven by the inconsistency of the between-cluster structure, the relative positions and orientations of syllable clusters, which is underdetermined by the data.</p><disp-quote content-type="editor-comment"><p>4. I like the &quot;higher representational capacity&quot; of the new method, but sometimes &quot;with much wisdom comes much sorrow&quot;: higher representation capacity can potentially cause trouble if it makes the method too sensitive to things we do not care about. At this level, I would like to see some evidence for robustness to noise. For example, it should be easy to test how sensitive AVA is for small differences in recording conditions, say, to recording with in a sound attenuation chamber while door is open or closed?</p></disp-quote><p>This is well put and an important point to address. Because the VAE is trained to capture the distribution in the training data, it will represent the largest sources of variation in the features it learns. Unfortunately, this method has no built-in notion of “relevant” versus “irrelevant” features. However, in the experiments reported here, systematically varying each of the latent features does produce changes in syllable shapes that appear meaningful. That is, they do not appear to capture artifacts. However, if artifacts were present non-uniformly in the data, it is very likely that the VAE would encode this. This could be addressed in two ways: (a) better data preprocessing, which would remove the artifact; (b) approaches like domain adaptation and other methods of de-biasing machine learning that work by identifying sources of variance (like race or gender in lending decisions) that systems should ignore (see, for example, Louizos et al. “The variational fair autoencoder” (2015)). We believe this will be an interesting avenue to pursue in future work.</p><disp-quote content-type="editor-comment"><p>Reviewer #4:</p><p>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires.</p><p>The authors use variational autoencoders (VAEs) to learn a low-dimensional representations to spectrograms of bird song an mouse unltrasonic vocalizations (USVs). They find these representations of vocal behavior to be useful for studying social interactions and differences between strains of mice. Further investigations suggest that mouse USVs do not cluster as nicely as previously thought, and rather span a continuous manifold in feature space. Finally, VAEs trained on random snippets of the spectrogram highlight variability (and stereotypy) in zebra finch songs, in contrast to the unstructured (or highly variable) space of mouse USVs.</p><p>The proposed model operates in the frequency domain, consuming snippets of time-warped spectrograms. Given that one of the central claims of this paper is the superiority of unsupervised methods for feature extraction, I think these preprocessing steps warrant further consideration. For example, the 2D convolutions in the VAE must implicitly leverage the fact that neighboring frequencies are adjacent in the 128x128 ``images,' but does the choice of frequency spacing (mel-spaced for song birds vs linearly-spaced for mouse USVs) affect the learned representations? How important is the time-warping to downstream representations and analyses? The spectral preprocessing also complicates the ``shotgun' analysis in Figure 6. Each point in the VAE latent space corresponds to a window of time rather than a single frame. How do the projections change as you vary the window size?</p></disp-quote><p>This is an excellent point. In practice, preprocessing can be as important as the model itself in deriving usable results. In response to the reviewer’s questions, we have performed several new analyses, including:</p><p>1. In Figure S4, we compare latent representations of mel vs. linear frequency spacing for zebra finch syllables and stretched vs. non-stretched syllables for both mouse and zebra finch. For zebra finch syllables, we find both comparisons produce latent features that encode hand-picked features to a similar extent and also display reasonable consistent pairwise distances (R<sup>2</sup>≈0.4). For mouse syllables, we find that the time stretch does provide a small but consistent benefit in terms of latent features representing hand-picked features and also fairly consistent pairwise distances with or without the time stetch (R<sup>2</sup>=0.69).</p><p>2. We have repeated the shotgun VAE analysis for temporal windows of varying size. The results are in S17. While there are obvious but trivial differences (overall rotations), major features like the linking note and song motif of bird song maps are reproduced, as well as the coarse structure of mouse USVs. However, fine details, particularly in the non-metric UMAP projection, do change, and these highlight the need, as mentioned in our response to Reviewer 3 above, for identifying downstream analyses that do not depend on small details of these maps.</p><disp-quote content-type="editor-comment"><p>Motivated in part by these concerns, some recent approaches like WaveNet (van den Oord et al., 2016) have directly modeled raw waveform data. The sampling rates used for human speech and music (16kHz) are lower than those necessary for USVs, but the same principles should apply. For example, a pre-trained WaveNet with minimal fine-tuning for song bird or mouse USV could yield a very competitive generative model of vocalizations and offer a different representation of this behavior. The comparison may be beyond the scope of this paper, but I think it is worthy of discussion.</p></disp-quote><p>We agree that this is both a really interesting direction and worth discussing. We have added the following text to the manuscript (ll. 341-342):</p><p>“We also note that great progress in generating raw audio has been made in the past few years, potentially enabling similar approaches that bypass an intermediate spectrogram representation (va den Oord et al., 2016, Kong et al., 2020).”</p><disp-quote content-type="editor-comment"><p>Overall, this paper offers a nice application of nonlinear latent variable models to vocal behavior data. The techniques themselves are not particularly novel – variational autoencoders have been widely used in the machine learning community for over five years now – and the finding that learned features can outperform handcrafted ones has been shown across many domains.</p></disp-quote><p>We agree. Our purpose here is to introduce these methods to many in the neuroscience community who’ve not been aware of their utility and to demonstrate that they produce useful and novel results in this particular domain.</p><disp-quote content-type="editor-comment"><p>Given the wealth of works on sequential VAEs for time-series data, I think the novelty of the shotgun VAE is somewhat overstated.</p></disp-quote><p>This is fair. Incorporating some of the sequential VAE methods is an interesting direction for future work. We have modified the text to temper the claim of novelty for this approach. Nevertheless, we feel the subsampling-based approach offers an intuitive and accessible single-parameter stand-in for what are often substantially more powerful (but complicated) time series models.</p><disp-quote content-type="editor-comment"><p>In my view, the main contribution lies in the codebase (I looked through the Github repo and was quite impressed!), the analysis pipeline, and the proof-of-concept. That is why I think it is especially important to assess the sensitivity of these results to various design choices that went into the pipeline, including the very first choices about how to preprocess the raw waveform data into time-warped and windowed spectrograms.</p></disp-quote><p>We concur. To recap our efforts along these lines, we have included S4, which investigates the effect of spectrogram frequency spacing and time stretching on the resulting latent features.</p><disp-quote content-type="editor-comment"><p>Other comments:</p><p>– Figure 1a: It's not clear why the length 32 vector is appearing as a square matrix here.</p></disp-quote><p>Thank you. This has now been fixed.</p><disp-quote content-type="editor-comment"><p>– Please label which dataset (song bird or mouse) the point clouds and spectrograms are coming from in all of the figures. A consistent color scheme could help.</p></disp-quote><p>Thank you for this suggestion. Green and brown/orange are used to indicate mouse vocalization (as in Figure 4d) and purple and cyan are used to indicate zebra finch vocalization (as in Figure 4a).</p><disp-quote content-type="editor-comment"><p>– Figure 2f only has three solid lines. Where is the plot of explained variance in MUPET features by VAE features?</p></disp-quote><p>We regret the confusion. In the original submission, only two lines were shown because there was a single set of latent features corresponding to the same syllables as both the MUPET and DeepSqueak features. In response to comments from reviewer 2, however, we have now truncated the single set of latent features to match the dimensions of the MUPET and DeepSqueak feature sets, resulting in separate lines. The caption for the figure now contains additional text:</p><p>“Latent features with colors labeled “MUPET” and “DeepSqueak” refer to the <italic>same</italic> set of latent features, truncated at different dimensions corresponding to the number of acoustic features measured by MUPET and DeepSqueak, respectively.”</p><disp-quote content-type="editor-comment"><p>– The paragraph starting on line 97 paints an overly optimistic view of VAEs. Designing deep generative models that can reliably disentangle latent factors is still an active area of research, as is model selection.</p></disp-quote><p>The reviewer is correct that many of these problems still constitute pressing research questions. We have omitted this mention of disentangling, instead adding a reference to (Dai et al., 2018), which specifically analyzes sparse latent representations in VAEs.</p><disp-quote content-type="editor-comment"><p>- Figures 3, S4, and S5 suggest that nearest neighbor reconstruction with DeepSqueak (and other handcrafted features) is surprisingly bad. Are you using just the Euclidean distance in DeepSqueak feature space? Did you whiten the DeepSqueak features before computing distances? Can you explain why it DeepSqueak is failing so dramatically?</p></disp-quote><p>This is an excellent question. For Figures 3 and S7 (previously S5), we used Euclidean distance with standardized but not whitened DeepSqueak features (ll. 423-430). We have added Figure S8 to investigate the failure of the DeepSqueak feature set, in particular, in returning reasonable nearest neighbors by constructing alternative versions of the feature set. In one version we whitened DeepSqueak features, in the next we considered the feature set without the three features most poorly predicted by latent features (from Figure S1), and lastly, we considered the linear projection of DeepSqueak feature space most predictive of the VAE’s features. All three feature sets return more suitable nearest neighbors for <italic>some</italic> of the query spectrograms shown from Figure 3, but the improvements are slight and none are as close as the latent feature nearest neighbors. In particular, the poor performance of the last feature set, an attempt to bring DeepSqueak features in line with latent features, suggests that DeepSqueak features are not sufficient to capture the full range of mouse USV syllables.</p><disp-quote content-type="editor-comment"><p>– Throughout, the spectrograms fail to indicate the time window or frequency bands.</p></disp-quote><p>While we agree that this information is useful, we are somewhat concerned about the additional visual clutter to figures that need to display many spectrograms. These numbers are clearly stated in the methods.</p><disp-quote content-type="editor-comment"><p>– Figure 4a+b aim to show that SAP do not separate directed and undirected vocalizations as well as latent features do, but is this information simply not present in the first two PCs? A classification accuracy assessment would be more convincing.</p></disp-quote><p>We thank the reviewer for this suggestion. To address this, we performed additional analyses using different feature sets for downstream classification tasks (Tables S1-3). In particular, classifiers trained to predict social context performed better when using latent features than when using SAP features (Tables S1).</p><disp-quote content-type="editor-comment"><p>– The 2D embedding in Figure 4f is confusing to me. Why not just show the full distance matrix from Figure S8, but with the lines to indicate which rows/columns belong to each mouse? That figure gives considerably more information than the tSNE embedding, in my opinion. In particular, it looks like there is a solid group of C57 mice that are very similar to DBA mice, as measured by MMD. The use of tSNE seems rather arbitrary and lossy. Moreover, the colors in Figure 4f convey little information beyond identity, when there seems to be lots of extra info about strain that could be conveyed.</p></disp-quote><p>We appreciate the suggestion. We have elected to retain the t-SNE embedding in the main figure, since it makes certain features more available at a glance, and provides an analogous presentation to our visualizations of syllables, but we have also added Figure S11 to include the full MMD matrix with easily visualized individual and strain information.</p><disp-quote content-type="editor-comment"><p>– There are no axis labels or titles in Figure 5a-c, just random clouds of points.</p></disp-quote><p>Thank you. We have fixed this.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. Figure 7 does not show clearly enough how the SG approach overcomes the problem of fused syllables (A and B). Figure 7c should somehow indicate the similarity in the fused AB vs A,B area. The issue is that it is not easy to see how the color code correspond to specific areas in the sonograms presented. An overlay panel could help here.</p></disp-quote><p>We appreciate the suggestion. Figure 7e now contains an inset illustrating similarity between the fused AB syllable from the pupil and the individual A and B syllables from the tutor. When assessed by MMD, the VAE trained on syllables judges the fused syllable to be distinct from both A and B, while the shotgun VAE shows a clear similarity (dark line) between A, B, and AB.</p></body></sub-article></article>