<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106628</article-id><article-id pub-id-type="doi">10.7554/eLife.106628</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106628.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Hierarchical encoding of natural sound mixtures in ferret auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Landemard</surname><given-names>Agnès</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6081-1014</contrib-id><email>agnes.landemard@hotmail.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Bimbard</surname><given-names>Célian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6380-5856</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Boubenec</surname><given-names>Yves</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0106-6947</contrib-id><email>boubenec@ens.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>Laboratoire des systèmes perceptifs, Département d’études cognitives, École normale supérieure, PSL University</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>UCL Institute of Ophthalmology, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>09</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP106628</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-03-24"><day>24</day><month>03</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-02-17"><day>17</day><month>02</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.15.637892"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-05-19"><day>19</day><month>05</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106628.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-08-21"><day>21</day><month>08</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106628.2"/></event></pub-history><permissions><copyright-statement>© 2025, Landemard et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Landemard et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106628-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-106628-figures-v1.pdf"/><abstract><p>Extracting relevant auditory signals from complex natural scenes is a fundamental challenge for the auditory system. Sounds from multiple sources overlap in time and frequency. In particular, dynamic ‘foreground’ sounds are often masked by more stationary ‘background’ sounds. Human auditory cortex exhibits a hierarchical organization where background-invariant representations are progressively enhanced along the processing stream, from primary to non-primary regions. However, we do not know whether this organizational principle is conserved across species and which neural mechanisms drive this invariance. To address these questions, we investigated background invariance in ferret auditory cortex using functional ultrasound imaging, which enables large-scale, high-resolution recordings of hemodynamic responses. We measured responses across primary, secondary, and tertiary auditory cortical regions as ferrets passively listened to mixtures of natural sounds and their components in isolation. We found a hierarchical gradient of background invariance, mirroring findings in humans: responses in primary auditory cortex reflected contributions from both foreground and background sounds, while background invariance increased in higher-order auditory regions. Using a spectrotemporal filter-bank model, we found that in ferrets this hierarchical structure could be largely explained by tuning to low-order acoustic features. However, this model failed to fully account for background invariance in human non-primary auditory cortex, suggesting that additional, higher-order mechanisms are crucial for background segregation in humans.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ferret</kwd><kwd>auditory cortex</kwd><kwd>natural sounds</kwd><kwd>background invariance</kwd><kwd>cross-species</kwd><kwd>cortical hierarchy</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-17- EURE-0017</award-id><principal-award-recipient><name><surname>Boubenec</surname><given-names>Yves</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-10-IDEX-0001-02</award-id><principal-award-recipient><name><surname>Boubenec</surname><given-names>Yves</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004795</institution-id><institution>Institut Universitaire de France</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Boubenec</surname><given-names>Yves</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>To parse foreground and background sounds, the auditory cortex of humans and ferrets share a similar hierarchical organization, but the underlying computational mechanisms are fundamentally different.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In the intricate auditory landscapes of our surroundings, the brain quickly extracts meaningful information from a cacophony of overlapping sources. Humans are remarkably good at detecting speech in complex noise: two people in a bar can easily have a conversation despite the ambient chatter. This ability is thought to rely on the different properties of background and foreground sounds, which are mixed in a single waveform at the level of the cochlea. Dynamic ‘foreground’ sounds, such as speech or vocalizations, fluctuate over short timescales, and these fluctuations convey new information over time (<xref ref-type="bibr" rid="bib43">Singh and Theunissen, 2003</xref>). In contrast, stationary ‘background’ sounds, like ambient chatter or the rustle of wind, fluctuate over longer periods, and are thus more predictable and less vital for signaling sudden events. They can be synthesized realistically based solely on their time-averaged acoustic statistics (<xref ref-type="bibr" rid="bib23">McWalter and McDermott, 2018</xref>; <xref ref-type="bibr" rid="bib21">McDermott and Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="bib22">McWalter and Dau, 2017</xref>), which the brain uses to perceptually fill short acoustic gaps (<xref ref-type="bibr" rid="bib24">McWalter and McDermott, 2019</xref>). Despite these differences in the nature of foreground and background sounds, we still lack a general understanding of where background invariance occurs in the brain and what neural mechanisms underlie it.</p><p>In humans, multiple stages of processing build up background-invariant representations. In fMRI recordings of auditory cortex, non-primary areas are more background-invariant than primary areas (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>). In addition, intracranial cortical responses show rapid adaptation to new background noise, which improves representations of foreground speech (<xref ref-type="bibr" rid="bib15">Khalighinejad et al., 2019</xref>). This effect is stronger for neurons in non-primary auditory cortex. This progressive invariance is unlikely to be fully explained by the parallel hierarchical organization of speech and music processing in humans (<xref ref-type="bibr" rid="bib29">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Norman-Haignere and McDermott, 2018</xref>) since it is still present when excluding speech and music from foreground sounds (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>). Moreover, background invariance appears to be independent of selective attention (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>; <xref ref-type="bibr" rid="bib15">Khalighinejad et al., 2019</xref>). Thus, the hierarchical organization of background invariance may not be specific to humans but rather a generic principle shared with other animals.</p><p>Background invariance can also be found in non-human animals. In fact, neurons that exhibit background-invariant responses are present throughout the auditory pathway in multiple species, increasingly from subcortical nuclei to the auditory cortex (<xref ref-type="bibr" rid="bib44">Souffi et al., 2020</xref>; <xref ref-type="bibr" rid="bib33">Rabinowitz et al., 2013</xref>). Background-invariant representations in auditory cortex are necessary for behavioral discrimination of natural sounds in noise (<xref ref-type="bibr" rid="bib45">Town et al., 2023</xref>). Neurons with foreground-invariant responses are also found in the non-human auditory cortex (<xref ref-type="bibr" rid="bib1">Bar-Yosef and Nelken, 2007</xref>; <xref ref-type="bibr" rid="bib28">Ni et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Hamersky et al., 2025</xref>), but a critical transformation of representations within the cortical hierarchy could specifically enhance foreground representations (<xref ref-type="bibr" rid="bib39">Schneider and Woolley, 2013</xref>; <xref ref-type="bibr" rid="bib36">Saderi et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Carruthers et al., 2015</xref>). However, these studies differ by species, regions, state (awake vs. anesthetized), type of background noise (synthetic vs. natural), and signal-to-noise ratio. Thus, it remains unclear whether a cortical gradient in background invariance, as seen in humans, is present in other animals.</p><p>Mechanistically, background invariance could come from the simple tuning properties of neurons. Auditory cortical neurons are known to be tuned to frequency and spectrotemporal modulations (<xref ref-type="bibr" rid="bib26">Miller et al., 2002</xref>; <xref ref-type="bibr" rid="bib41">Shamma, 2009</xref>), which are acoustic features that differ between background and foreground sounds (<xref ref-type="bibr" rid="bib9">Elie and Theunissen, 2015</xref>). In ferrets, this low-level spectrotemporal tuning can explain large-scale responses to natural sounds in both primary and non-primary regions of auditory cortex (<xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref>). Thus, differences in background invariance across regions could result from regional differences in such low-order tuning (<xref ref-type="bibr" rid="bib27">Moore et al., 2013</xref>; <xref ref-type="bibr" rid="bib13">Ivanov et al., 2022</xref>). In humans, this tuning can explain responses in primary auditory cortex (<xref ref-type="bibr" rid="bib38">Santoro et al., 2014</xref>), but not in non-primary fields (<xref ref-type="bibr" rid="bib30">Norman-Haignere and McDermott, 2018</xref>). Consequently, the sensitivity of human non-primary cortex to acoustic high-order acoustic features may underlie the hierarchical gradient in background invariance. However, it remains untested whether background invariance in the auditory cortex relies on neural mechanisms that are conserved across species.</p><p>Here, we asked (1) whether background invariance is hierarchically organized in the auditory cortex of a non-human mammal, the ferret, and (2) whether hierarchical background invariance could be explained by tuning to low-order acoustic properties that differentiate foreground-like and background-like natural sounds. We defined foreground and background sounds based on an acoustic criterion of stationarity. We measured large-scale hemodynamic responses in ferrets’ auditory cortex as they passively listened to isolated and combined foregrounds and backgrounds. Leveraging the spatial resolution and coverage of functional ultrasound imaging (fUSI), we showed that background invariance increased from primary to secondary and tertiary areas of auditory cortex. We could predict these effects using a model based on frequency and spectrotemporal modulations, suggesting that the organization of background invariance in the ferret brain can be derived from neural spectrotemporal tuning. Finally, we showed that the same model could not explain as well the patterns of background invariance found in human auditory cortex, suggesting that additional mechanisms underlie background invariance in humans.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Using fUSI, we measured hemodynamic responses in the auditory cortex of ferrets passively listening to continuous streams of natural sounds. We used three types of stimuli: foregrounds, backgrounds, and combinations of those. We use those terms to refer to sounds differing in their stationarity, under the assumption that stationary sounds carry less information than non-stationary sounds, and are thus typically ignored. To classify natural sounds into these categories, we computed each sound’s stationarity, defined as the stability in time of a sound’s acoustic statistics (<xref ref-type="bibr" rid="bib23">McWalter and McDermott, 2018</xref>). Backgrounds were chosen as the most stationary and foregrounds as the least stationary sounds (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>). Different sound segments were concatenated to form continuous streams (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In the isolated condition, a sound change occurred every 9.6 s. In the mixture condition, the same sequences overlapped with a lag of 4.8 s between the foreground and background streams, leading to a change in one or the other stream every 4.8 s.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Hemodynamic activity reflects encoding of foregrounds and backgrounds.</title><p>(<bold>A</bold>) Stationarity for foregrounds (<italic>squares</italic>) and backgrounds (<italic>diamonds</italic>). (<bold>B</bold>) Sound presentation paradigm, with example cochleagrams. We created continuous streams by concatenating 9.6 s foreground (<italic>cold colors</italic>) and background segments (<italic>warm colors</italic>) following the illustrated design. Each foreground (resp. background) stream was presented in isolation and with two different background (resp. foreground) streams. (<bold>C</bold>) We measured cerebral blood volume (CBV) in coronal slices (<italic>blue plane</italic>) of the ferret auditory cortex (<italic>black outline</italic>) with functional ultrasound imaging. We imaged the whole auditory cortex through successive slices across several days. Baseline blood volume for an example slice is shown, where two sulci are visible, as well as penetrating arterioles. <italic>D</italic>: dorsal, <italic>V</italic>: ventral, <italic>M</italic>: medial, <italic>L</italic>: lateral. (<bold>D</bold>) Changes in CBV aligned to sound changes, averaged across all (including non-responsive) voxels and all ferrets, as well as across all sounds within each condition (normalized to silent baseline). Shaded area represents standard error of the mean across sound segments. (<bold>E</bold>) Test-retest cross-correlation for each condition. Voxel responses for two repeats of sounds are correlated with different lags. Resulting matrices are then averaged across all responsive voxels (ΔCBV &gt; 2.5%).</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>List of sounds used in ferret experiments.</title><p>Each column corresponds to a different run.</p></caption><media mimetype="application" mime-subtype="pdf" xlink:href="elife-106628-fig1-data1-v1.pdf"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig1-v1.tif"/></fig><p>Hemodynamic responses stably encoded sound identity after transient responses. We continuously measured hemodynamic activity in ferret auditory cortex (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) throughout the sound sequences and normalized cerebral blood volume (CBV) relative to blocks of silent baseline. Sound changes elicited transient increases in activity before reaching a sustained level (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Sustained activity was lower for backgrounds in isolation than for foregrounds and mixtures. We asked whether sound encoding was stable over time despite these changes in amplitude. We focused on the sound-specific activity of responsive voxels (ΔCBV &gt; 2.5%) by subtracting their average timecourse across all sounds in each category. For each voxel, we cross-correlated vectors of responses to sounds of each category across two repetitions (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). Voxels stably encoded sounds of each category throughout their duration despite the observed variation in response amplitude. Thus, we summarized voxels’ activity by their time-averaged activity over an interval spanning 2.4–4.8 s after sound change.</p><sec id="s2-1"><title>Invariance to background sounds is hierarchically organized in ferret auditory cortex</title><p>We investigated the spatial organization of sound responses throughout the whole ferret auditory cortex. We found that reliable sound-evoked responses were confined to the central part of the ventral gyrus of the auditory cortex (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). Hereafter, we focus on reliable sound-responsive voxels (test–retest correlation &gt; 0.3 for sounds in at least one condition). We assigned voxels to three regions of interest in ferret auditory cortex (<xref ref-type="fig" rid="fig2">Figure 2C</xref>): MEG (primary), dPEG (secondary), and VP (tertiary, see <xref ref-type="bibr" rid="bib7">Elgueda et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref>; <xref ref-type="bibr" rid="bib34">Radtke-Schuller, 2018</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Invariance to background sounds is hierarchically organized in ferret auditory cortex.</title><p>(<bold>A</bold>) Map of average response for an example hemisphere (ferret L). Responses are expressed in percent changes in cerebral blood volume (CBV) relative to baseline activity, measured in periods of silence. Values are averaged across depth to obtain this surface view of auditory cortex. (<bold>B</bold>) Map of test-retest reliability. In the following maps, only reliably responding voxels are displayed (test–retest &gt; 0.3 for at least one category of sounds) and the transparency of surface bins in the maps is determined by the number of (reliable) voxels included in the average. (<bold>C</bold>) Map of considered regions of interest (ROIs), based on anatomical landmarks. The arrows indicate the example slices shown in (D) (<italic>orange</italic>: primary; <italic>green</italic>: non-primary example). (<bold>D</bold>) Responses to isolated and combined foregrounds. <italic>Bottom</italic>: responses to mixtures and foregrounds in isolation, for example, voxels (<italic>left</italic>: primary; <italic>right:</italic> non-primary). Each dot represents the voxel’s time-averaged response to every foreground (<italic>x-axis</italic>) and mixture (<italic>y-axis</italic>), averaged across two repetitions. r indicates the value of the Pearson correlation. <italic>Top</italic>: maps show invariance, defined as noise-corrected correlation between mixtures and foregrounds in isolation, for the example voxel’s slice with values overlaid on anatomical images representing baseline CBV. Example voxels are shown with white squares. (<bold>E</bold>) Map of background invariance for the same hemisphere (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for other ferrets). (<bold>F</bold>) Quantification of background invariance for each ROI. Colored circles indicate median values across all voxels of each ROI, across animals. Gray dots represent median values across the voxels of each ROI for each animal. The size of each dot is proportional to the number of voxels across which the median is taken. The thicker line corresponds to the example ferret L. ***: <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$  p \leq 0.001 $\end{document}</tex-math></alternatives></inline-formula> for comparing the average background invariance across animals for pairs of ROIs, obtained by a permutation test of voxel ROI labels within each animal. (<bold>G–I</bold>) Same as (D–F) for foreground invariance (comparing mixtures to backgrounds in isolation). <italic>AEG</italic>, anterior ectosylvian gyrus; <italic>MEG</italic>, medial ectosylvian gyrus; <italic>dPEG</italic>, dorsal posterior ectosylvian gyrus; <italic>VP</italic>, ventral posterior auditory field.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Table of statistics for comparison across regions.</title><p>Values of background and foreground invariance for each ROI (MEG, dPEG, and VP), for different conditions: actual and predicted data, or restricted to voxels tuned to low (&lt; 8 Hz) or high (&gt; 8 Hz) temporal modulations. For each metric, we provide the median across voxels of each ROI for each animal (B, L, and R), as well as the <italic>p</italic>-values obtained to test the difference across pairs of regions. Metrics are also provided for the average across all animals (<italic>all</italic>). Significant <italic>p</italic>-values (<inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$p \lt 0.05$\end{document}</tex-math></alternatives></inline-formula>) are highlighted in bold font.</p></caption><media mimetype="application" mime-subtype="pdf" xlink:href="elife-106628-fig2-data1-v1.pdf"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Invariance dynamics.</title><p>For each voxel, we computed the Pearson correlation between the vectors of trial-averaged responses to mixtures and foregrounds (<bold>A</bold>) or backgrounds (<bold>B</bold>) with different lags. We then averaged these matrices across all responsive voxels to obtain the cross-correlation matrices shown here. The matrices here are not noise-corrected.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Maps for all ferrets.</title><p>(<bold>A</bold>) Maps of mean response, test–retest reliability, true and predicted background and foreground invariance, for all recorded hemispheres. In the invariance maps, only reliable voxels are shown. (<bold>B</bold>) Comparison of metrics shown in (A) across primary (MEG) and non-primary regions (dPEG, VP), for voxels selected for prediction analyses (test-retest &gt; 0 for each category, and &gt; 0.3 for at least one category).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig2-figsupp2-v1.tif"/></fig></fig-group><p>Invariance to background sounds was stronger in non-primary than in primary auditory cortex. To probe invariance to background sounds for each voxel, we compared responses to mixtures and isolated foregrounds across sound segments (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Responses to mixtures and isolated foregrounds tended to be more similar for voxels in non-primary regions. We quantified this effect by computing a noise-corrected version of the Pearson correlation between responses to mixtures and foregrounds for each voxel, which we call background invariance (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>). Background invariance emerged shortly after a sound change and was stable throughout the duration of each sound snippet (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). When going up the hierarchy, correlations between mixtures and foregrounds increased (tertiary (VP) &gt; secondary (dPEG) &gt; primary (MEG), <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula> with a permutation test), indicating a stronger invariance to background sounds in higher-order areas (<xref ref-type="fig" rid="fig2">Figure 2E and F</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). While background invariance was overall highest in VP, the differences within non-primary areas were more variable across animals (see <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>).</p><p>To test whether foregrounds were processed differently than backgrounds, and at which stage, we also compared responses to mixtures and backgrounds in isolation. We defined a ‘foreground invariance’, symmetric to the background invariance, by correlating responses to mixtures and isolated backgrounds across sound segments (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). In primary auditory cortex (MEG), foreground invariance was slightly lower than background invariance, although this difference was not significant (<inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.063</mml:mn></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$  p=0.063$\end{document}</tex-math></alternatives></inline-formula>, obtained by randomly permuting the sounds’ background and foreground labels, 1000 times). However, foreground invariance tended to decrease from primary to non-primary areas (<xref ref-type="fig" rid="fig2">Figure 2H and I</xref>, VP &lt; dPEG: <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>, dPEG &lt; MEG: <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.136</mml:mn></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$  p=0.136$\end{document}</tex-math></alternatives></inline-formula> with a permutation test). As a consequence, foreground invariance became significantly lower than background invariance in both dPEG (<inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>) and VP (<inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>).</p></sec><sec id="s2-2"><title>A model of auditory processing predicts hierarchical differences</title><p>By definition, foregrounds and backgrounds are separated in the acoustic domain. Low-level acoustic features computed from their waveforms can well discriminate both categories. To show this, we used a standard two-stage filter-bank model of auditory processing (<xref ref-type="bibr" rid="bib4">Chi et al., 2005</xref>). This model (1) computes a sound’s cochleagram, (2) convolves the resulting cochleagram through a bank of spectrotemporal modulation filters, tuned to a range of frequencies and spectrotemporal modulations (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Foregrounds and backgrounds differed in their pattern of energy in the modulation space (<xref ref-type="fig" rid="fig3">Figure 3B and C</xref>). In particular, the axis of temporal modulation was highly discriminative: foregrounds tended to have higher energy in the low rates (&lt; 8 Hz), and backgrounds in higher rates (&gt; 8 Hz).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Simple spectrotemporal tuning explains spatial organization of background invariance.</title><p>(<bold>A</bold>) Presentation of the two-stage filter-bank, or spectrotemporal model. Cochleagrams (shown for an example foreground and background) are convolved through a bank of spectrotemporal modulation filters. (<bold>B</bold>) Energy of foregrounds and backgrounds in spectrotemporal modulation space, averaged across all frequency bins. (<bold>C</bold>) Average difference of energy between foregrounds and backgrounds in the full acoustic feature space (frequency * temporal modulation * spectral modulation). (<bold>D</bold>) We predicted time-averaged voxel responses using sound features derived from the spectrotemporal model presented in (A) with ridge regression. For each voxel, we thus obtain a set of weights for frequency and spectrotemporal modulation features, as well as cross-validated predicted responses to all sounds. (<bold>E</bold>) Average model weights for MEG. (<bold>F</bold>) Maps of preferred frequency, temporal and spectral modulation based on the fit model. To calculate the preferred value for each feature, we marginalized the weight matrix over the two other dimensions. (<bold>G</bold>) Average differences of weights between voxels of each non-primary (dPEG and VP) and primary (MEG) region. (<bold>H</bold>) Background invariance (<italic>left</italic>) and foreground invariance (<italic>right</italic>) for voxels tuned to low (&lt; 8 Hz) or high (&gt; 8 Hz) temporal modulation rates within each region of interest (ROI). Colored circles indicate median value across all voxels of each ROI, across animals. Gray dots represent median values across the voxels of each ROI for each animal. **: <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.01</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$  p \leq0.01$\end{document}</tex-math></alternatives></inline-formula>, ***: <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$  p \leq 0.001$\end{document}</tex-math></alternatives></inline-formula> for comparing the average background invariance across animals for voxels tuned to low vs. high rates, obtained by a permutation test of tuning within each animal.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Tuning to acoustic features for all ferrets.</title><p>Maps of preferred values for each dimension of acoustic space, obtained by marginalizing the fitted weight matrix over other dimensions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig3-figsupp1-v1.tif"/></fig></fig-group><p>In ferret auditory cortex, large-scale cortical responses to natural sounds are well predicted by the spectrotemporal model, even when controlling for stimuli correlations (<xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref>). To test whether differences in neural invariance could be explained by tuning to low-order acoustic features, we predicted the voxels’ responses using this model. We derived voxels weights for model features using cross-validated ridge regression of trial-averaged responses to all sounds (<xref ref-type="fig" rid="fig3">Figure 3D and E</xref>).</p><p>Tuning to frequency and spectrotemporal modulations showed large-scale spatial structure. Through the model weights, we first explored whether tuning to acoustic features was topographically organized. We extracted the voxels’ preferred frequency, spectral and temporal modulations by marginalizing over the other features. We retrieved the tonotopic organization of the auditory cortex, confirming our anatomical segmentation into functional areas (<xref ref-type="bibr" rid="bib7">Elgueda et al., 2019</xref>). The voxels’ preferred spectral and temporal modulations were also spatially organized (<xref ref-type="fig" rid="fig3">Figure 3F</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for all ferrets). To directly examine the differences of tuning between non-primary and primary areas, we computed the differences in weights between dPEG vs. MEG and VP vs. MEG, throughout the whole three-dimensional acoustic space (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). The weights of voxels in non-primary areas differed from those in primary areas in a complex manner, in contrast to the simple separation of the acoustic space between foregrounds and backgrounds (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Compared to MEG, dPEG was preferentially tuned to high frequencies and higher spectral modulations (<xref ref-type="fig" rid="fig3">Figure 3G</xref>, top), while VP tuning was biased toward low frequencies and intermediate spectrotemporal modulations (<xref ref-type="fig" rid="fig3">Figure 3G</xref>, bottom).</p><p>We sought to assess whether these differences in tuning underlaid differences in background invariance across regions. The model included a range of realistic temporal rates, and this axis was the most informative to discriminate foregrounds from backgrounds (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Thus, we first compared the invariance properties of voxels tuned to low or high temporal modulation rates. Within each region, voxels tuned to lower modulation rates displayed higher background invariance and lower foreground invariance than those tuned to higher rates (<xref ref-type="fig" rid="fig3">Figure 3H</xref>, <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.004</mml:mn></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$  p \lt 0.004$\end{document}</tex-math></alternatives></inline-formula> within each region of interest (ROI) by permuting voxels’ tuning, 1000 times). However, within each tuning group, differences were still present across regions (primary vs. non-primary, <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula> within voxels tuned to high rates and within voxels tuned to low rates, see <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref> for more detailed comparisons between regions), confirming that tuning to temporal modulation was not the only mechanism at play.</p><p>To take into account tuning across all dimensions, we then directly looked at the invariance of responses predicted from the spectrotemporal model. The model was able to predict voxel responses accurately (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) and recapitulated invariance properties across single voxels (<xref ref-type="fig" rid="fig4">Figure 4A–C and E–G</xref>). To verify how well the model could explain the observed invariance, we directly compared measured and predicted invariance across voxels for each animal, regardless of ROI (<xref ref-type="fig" rid="fig4">Figure 4C and G</xref>). Predictions explained a large fraction of the reliable variability in background invariance (mean noise-corrected correlation between measured and predicted invariance: <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.65</mml:mn><mml:mo>±</mml:mo><mml:mn>0.007</mml:mn></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$  r=0.65\pm 0.007$\end{document}</tex-math></alternatives></inline-formula> standard error of the mean across animals) and foreground invariance (mean <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.43</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$  r=0.43\pm 0.04$\end{document}</tex-math></alternatives></inline-formula>). Predicted responses recapitulated the gradient from primary to non-primary areas for background invariance (<xref ref-type="fig" rid="fig4">Figure 4D</xref>; MEG &lt; dPEG, <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.022</mml:mn></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$  p=0.022$\end{document}</tex-math></alternatives></inline-formula> ; dPEG &lt; VP, <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>, permutation test) and foreground invariance (<xref ref-type="fig" rid="fig4">Figure 4H</xref>; MEG &gt; dPEG: <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.014</mml:mn></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$  p=0.014$\end{document}</tex-math></alternatives></inline-formula>, dPEG &gt; VP: <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>, permutation test). Results were similar if the model was fit solely on isolated sounds, excluding mixtures from the training set (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>A model of auditory processing predicts hierarchical differences in ferret auditory cortex.</title><p>Same as in <xref ref-type="fig" rid="fig2">Figure 2</xref> using cross-validated predictions from the spectrotemporal model. (<bold>A</bold>) Predicted responses to mixtures and foregrounds in isolation for example voxels (<italic>left</italic>: primary; <italic>right</italic>: non-primary). Each dot represents the voxel’s predicted response to foregrounds (<italic>x-axis</italic>) and mixtures (<italic>y-axis</italic>). r indicates the value of the Pearson correlation. Maps above show predicted invariance values for the example voxel’s slice overlaid on anatomical images representing baseline cerebral blood volume (CBV). Example voxels are shown with white squares. (<bold>B</bold>) Maps of predicted background invariance, defined as the correlation between predicted responses to mixtures and foregrounds in isolation. (<bold>C</bold>) Binned scatter plot representing predicted vs. measured background invariance across voxels. Each line corresponds to the median across voxels for one animal, using 0.1 bins of measured invariance. (<bold>D</bold>) Predicted background invariance for each region of interest (ROI). Colored circles indicate median value across all voxels of each ROI, across animals. Gray dots represent median values across the voxels of each ROI, for each animal. The size of each dot is proportional to the number of voxels across which the median is done. The thicker line corresponds to example ferret L. *: <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$  p \leq 0.05$\end{document}</tex-math></alternatives></inline-formula>; ***: <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$  p \leq 0.001$\end{document}</tex-math></alternatives></inline-formula> for comparing the average predicted background invariance across animals for pairs of ROIs, obtained by a permutation test of voxel ROI labels within each animal. (<bold>E–H</bold>) Same as (A–D) for predicted foreground invariance, that is, comparing predicted responses to mixtures and backgrounds in isolation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Assessment and effect of model prediction accuracy across species.</title><p>(<bold>A</bold>) Map of model prediction accuracy (correlation between measured and cross-validated predicted responses) for the example ferret. (<bold>B</bold>) Histogram of prediction accuracy across voxels of each region, for ferrets. (<bold>C</bold>) Comparison of prediction accuracy vs. test–retest reliability across voxels. (<bold>D</bold>) Median predicted background invariance across voxels grouped in bins of observed prediction accuracy, in ferrets. Each thin line corresponds to the median across voxels within one subject for one region. Thick lines correspond to averages across subjects. (<bold>E</bold>). Same, for predicted foreground invariance. (<bold>F–I</bold>). Same as (<bold>B–E</bold>), for humans.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Predicting from a model fitted on isolated sounds only.</title><p>(<bold>A</bold>) Predicted background invariance by region, with weights fitted using all sounds including mixtures (reproduced from <xref ref-type="fig" rid="fig4">Figure 4B</xref>). (<bold>B</bold>) Predicted background invariance by region, with weights fitted on the isolated sounds only (excluding mixtures). (<bold>C, D</bold>) Same as (A, B), for predicted foreground invariance. (<bold>E–H</bold>) Same as (A–D), for humans. *: <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$  p \leq 0.05$\end{document}</tex-math></alternatives></inline-formula>; ***: <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$  p \leq 0.001$\end{document}</tex-math></alternatives></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig4-figsupp2-v1.tif"/></fig></fig-group><p>Thus, hierarchical differences in invariance were largely explained by differences in tuning to frequency and spectrotemporal modulation between regions.</p></sec><sec id="s2-3"><title>Species difference in background invariance</title><p>In ferrets, background invariance is hierarchically organized, and this organization can largely be explained by simple spectrotemporal tuning. Is it also the case for the humans’ hierarchical organization of invariance? To test this, we compared background invariance previously obtained in humans with what we found in ferrets. We used a comparable published dataset (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>), in which combinations of foregrounds and backgrounds were presented to human subjects, in isolation and in mixtures. The BOLD signal from the auditory cortex was acquired with fMRI (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Applying our analyses to this dataset, we reproduced the results of <xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref> and showed that background invariance was stronger in non-primary than in primary auditory cortex (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>, non-primary &gt; primary, <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>, permutation test on the average across seven subjects). In parallel, we found that foreground invariance decreased from primary to non-primary areas (<xref ref-type="fig" rid="fig5">Figure 5F and G</xref>, non-primary &lt; primary, <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>, permutation test). As in ferrets, foreground and background invariance values were similar in primary auditory cortex (<inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.577</mml:mn></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$  p=0.577$\end{document}</tex-math></alternatives></inline-formula>, permutation test), but significantly differed in non-primary auditory cortex (<inline-formula><alternatives><mml:math id="inf26"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$  p=0.002$\end{document}</tex-math></alternatives></inline-formula>, permutation test).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The spectrotemporal model is a poor predictor of human background invariance.</title><p>(<bold>A</bold>) We replicated our analyses with a dataset of a similar experiment measuring fMRI responses in human auditory cortex (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>). We compared responses in primary and non-primary auditory cortex, as delineated in <xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>. (<bold>B</bold>) Responses to mixtures and foregrounds in isolation for example voxels (<italic>left</italic>: primary; <italic>right</italic>: non-primary). Each dot represents the voxel’s response to foregrounds (<italic>x-axis</italic>) and mixtures (<italic>y-axis</italic>), averaged across repetitions. r indicates the value of the Pearson correlation. (<bold>C</bold>) Quantification of background invariance measured for each region of interest (ROI). Colored circles indicate median value across all voxels of each ROI, across subjects. Gray dots represent median values for each ROI and subject. The size of each dot is proportional to the number of (reliable) voxels across which the median is done. *:<inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$  p \leq 0.05$\end{document}</tex-math></alternatives></inline-formula>; ***:<inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$  p \leq 0.001$\end{document}</tex-math></alternatives></inline-formula> for comparing the average predicted background invariance across subjects for pairs of ROIs, obtained by a permutation test of voxel ROI labels within each subject. (<bold>D</bold>) Binned scatter plot representing predicted vs measured background invariance across voxels. Each line corresponds to the median across voxels for one subject, using 0.1 bins of measured invariance. (<bold>D</bold>) Same as (C) for responses predicted from the spectrotemporal model. (<bold>F–I</bold>) Same as (B–E) for foreground invariance, that is, comparing predicted responses to mixtures and backgrounds in isolation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Spectrotemporal tuning properties for humans.</title><p>(<bold>A</bold>) Average difference of energy between foregrounds and backgrounds used in human experiments, in the acoustic feature space (frequency * temporal modulation * spectral modulation). (<bold>B</bold>) Average model weights for human primary auditory cortex. (<bold>C</bold>) Average differences of weights between voxels of human non-primary vs. primary auditory cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Invariance metrics are not affected by differences in test–retest reliability across regions.</title><p>(<bold>A</bold>) Background invariance across voxels grouped in bins of test–retest reliability (averaged across sound categories). (<bold>B</bold>) Same, for foreground invariance. Thin lines show the median across voxels within regions of interest (ROIs) of each animal. Thick lines show the median across voxels of an ROI, across all animals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106628-fig5-figsupp2-v1.tif"/></fig></fig-group><p>We next tested whether the spectrotemporal model could predict human voxel responses. The model performed similarly in humans and ferrets (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.42</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$  r=0.42\pm 0.01$\end{document}</tex-math></alternatives></inline-formula> standard error of the mean across subjects, humans vs ferrets: <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0667</mml:mn></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$  p=0.0667$\end{document}</tex-math></alternatives></inline-formula>, Wilcoxon rank-sum test). However, the model explained less of the measured background invariance in humans than in ferrets (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, noise-corrected correlation between measured and predicted invariance: <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.23</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$  r=0.23\pm 0.04$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0167</mml:mn></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$  p=0.0167$\end{document}</tex-math></alternatives></inline-formula> compared to ferrets, <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0156</mml:mn></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$  p=0.0156$\end{document}</tex-math></alternatives></inline-formula> compared to 0, Wilcoxon rank-sum test). While predicted background invariance differed significantly between primary and non-primary regions (<xref ref-type="fig" rid="fig5">Figure 5E</xref>, primary &lt; non-primary, <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$  p=0.001$\end{document}</tex-math></alternatives></inline-formula>, permutation test), the predicted difference was much smaller than observed. Furthermore, the model failed to predict the levels of foreground invariance in both primary and non-primary human auditory cortex (<xref ref-type="fig" rid="fig5">Figure 5H and I</xref>, noise-corrected correlation between measured and predicted invariance: <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.17</mml:mn><mml:mo>±</mml:mo><mml:mn>0.06</mml:mn></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$  r=-0.17\pm 0.06$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0167</mml:mn></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$  p=0.0167$\end{document}</tex-math></alternatives></inline-formula> compared to ferrets, <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0312</mml:mn></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$  p=0.0312$\end{document}</tex-math></alternatives></inline-formula> compared to 0, Wilcoxon rank-sum test). Thus, the spectrotemporal model performed worse in explaining the patterns of cortical invariance in humans than in ferrets. This suggests that additional higher-order mechanisms contribute to background segregation in humans.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we investigated large-scale background and foreground invariance in auditory cortex using mixtures of natural sounds. In ferrets, we found a hierarchical gradient of invariance, where primary cortical areas encode both background and foreground sounds, while non-primary regions show increasing invariance to background sounds. A spectrotemporal filter-bank model could account for a large portion of the hierarchical changes in ferrets, suggesting that low-level tuning to frequency and modulation properties explain how stationary ‘background’ and more dynamic ‘foreground’ signals are segregated in hemodynamic signals. When we applied the same model to human data, we found that although humans also display a hierarchical increase in background invariance, the model failed to capture a substantial fraction of the observed effect in non-primary regions. This suggests that mechanisms beyond simple spectrotemporal tuning are at play in human auditory cortex.</p><p>Our interpretation of foreground and background neural representations relies on several methodological choices that make the problem tractable but introduce specific limitations. First, this study defined foregrounds and backgrounds solely based on their acoustic stationarity, rather than perceptual judgments. This choice allowed us to isolate the contribution of acoustic factors in a simplified setting. Within this controlled framework, we show that acoustic features of foreground and background sounds drive their separation in the brain and the hierarchical extraction of foreground sound features. Second, we used blood volume as a proxy for local neuronal activity. Thus, our signal ignores any heterogeneity that might exist at the level of local neuronal populations. However, our main findings are related to the large-scale organization of cortical responses and how they relate to those of humans. For this purpose, the functional spatial resolution of our signal, driven by the spatial resolution of neurovascular coupling, should be adapted. In addition, using hemodynamic signals provides a much better comparison with human fMRI data, where the same limitations are present.</p><p>Our results show that foregrounds and backgrounds are similarly represented in primary auditory cortex, both in ferrets and humans; at this stage, we found no significant bias toward foregrounds. This is in line with previous work in non-human animals, including birds, where neurons in various regions up to primary auditory cortex display a continuum of background-invariant and foreground-invariant responses (<xref ref-type="bibr" rid="bib28">Ni et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Bar-Yosef and Nelken, 2007</xref>; <xref ref-type="bibr" rid="bib39">Schneider and Woolley, 2013</xref>). In ferrets, primary auditory cortex has been found to over-represent backgrounds in mixtures compared to foregrounds (<xref ref-type="bibr" rid="bib11">Hamersky et al., 2025</xref>). In contrast, we found a slight, non-significant bias toward foregrounds in primary regions. This difference could be driven by a difference in timescales, as we looked at slower timescales in which adaptation might be more present, reducing the strength of background encoding. In humans, we found a much smaller gap between background and foreground invariance in primary auditory cortex, which was not predicted by the spectrotemporal model. Additionally, more closely controlled experiments would be needed to confirm and understand this species difference.</p><p>The encoding of background in primary regions could play a functional role in background denoising at later stages of processing. Studies show complex synergetic interactions between target sounds and background noise in primary auditory cortex, even with simple stimuli. On the one hand, white noise can sharpen neurons’ tuning curves, enhancing tone discrimination (<xref ref-type="bibr" rid="bib5">Christensen et al., 2019</xref>), and improve the representations of frequency sweeps (<xref ref-type="bibr" rid="bib20">Malone et al., 2017</xref>). On the other hand, background information can be more easily decoded from population responses when presented with a target sound than when in isolation (<xref ref-type="bibr" rid="bib20">Malone et al., 2017</xref>). Computationally, background-related information could facilitate further extraction of foregrounds. For instance, in a convolutional neural network trained to visually recognize digits in different types of noise, when local feedback is implemented, early layers encode noise properties, while later layers represent clean signals (<xref ref-type="bibr" rid="bib18">Lindsay et al., 2022</xref>). Similarly, primary auditory areas could be recruited to maintain background representations, enabling downstream cortical regions to use these representations to specifically suppress background information and enhance foreground representations (<xref ref-type="bibr" rid="bib12">Hicks and McDermott, 2024</xref>).</p><p>We found that the main difference regarding background invariance between humans and ferrets was in how well invariance could be explained by a model based on low-level acoustic features. There could be multiple reasons for this difference.</p><p>First, methodological differences could contribute to the observed species difference. In our ferret experiment, sounds were presented continuously and in longer segments, an experimental design made possible by fUSI but impractical for fMRI due to scanner noise. However, this is unlikely to have a major impact, as our analyses focused on the sustained response component, similar to the fMRI experiment. Furthermore, while fUSI and fMRI both provide hemodynamic signals, they differ in their physiological basis (CBV vs. BOLD). Studies comparing CBV-weighted and BOLD fMRI indicate that CBV provides a more spatially and temporally precise signal than BOLD (<xref ref-type="bibr" rid="bib42">Silva et al., 2007</xref>), improving localization of neural activity. In addition, fMRI has a worse spatial resolution than fUSI (here, 2 vs. 0.1 mm voxels). However, this difference in resolution compensates for the difference in brain size between humans and ferrets. In our previous work, we showed that a large fraction of cortical responses to natural sounds could be predicted from one species to the other using these methods (<xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref>). Last, to minimize potential biases, we applied the same criteria for voxel inclusion and noise-correction procedures across datasets. Thus, while minor differences in data quality and experimental protocols exist, they are unlikely to fully account for the species differences we observed.</p><p>Second, ferret and human paradigms differed by their control of attention. Ferrets were passively listening to the sounds while human participants had to perform a simple loudness-based task to ensure attention to the sounds. Even though specific auditory attention is not necessary for the gradient in invariance (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>), the task ensured that subjects remained engaged, while this was not the case for ferrets. General engagement might still affect auditory representations via top-down processes (<xref ref-type="bibr" rid="bib8">Elhilali et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Saderi et al., 2021</xref>; <xref ref-type="bibr" rid="bib40">Schwartz and David, 2018</xref>). Thus, we cannot exclude that both species possess a similar feedforward acoustic analysis (well modeled by simple spectrotemporal filters) as well as more context-driven networks in non-primary auditory cortex, which were more strongly engaged in humans than in ferrets due to experimental procedures. In addition, most of the sounds included in our study likely have more relevance for humans compared to ferrets (see <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>). Despite including ferret vocalizations and environmental sounds that are more ecologically relevant for ferrets, it is not clear whether ferrets would behaviorally categorize foregrounds and backgrounds as humans do. Examining how ferrets naturally orient or respond to foreground and background sounds under more ecologically valid conditions, potentially with free exploration or spontaneous listening paradigms, could help address this issue.</p><p>Third, human non-primary regions could host higher-order computations that cannot be predicted by simple acoustic features and are not present in ferrets. Human and non-human primates have a complex functional architecture in the auditory belt and parabelt regions (<xref ref-type="bibr" rid="bib10">Hackett, 2010</xref>), likely supporting elaborate computations. These computations might be related to conspecific calls and required for advanced vocal communication. In particular, the unique importance of speech for humans has shaped auditory circuits throughout the brain. Neural circuits in human non-primary auditory cortex encode higher-order acoustic features, such as phonemic patterns, which might help to segregate meaningful content from background noise (<xref ref-type="bibr" rid="bib29">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Mesgarani et al., 2014</xref>). Last, other processes might critically shape background-invariant representations in humans, such as dynamic adaptation to backgrounds that could be faster in humans than in ferrets (<xref ref-type="bibr" rid="bib15">Khalighinejad et al., 2019</xref>).</p><p>The divergence in background invariance mechanisms between humans and ferrets may reflect differences in the depth and complexity of their auditory cortical hierarchies. Our previous work (<xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref>) showed that large-scale representations of natural sounds are not clearly distinct between primary and non-primary auditory regions in the ferret cortex. However, <xref ref-type="bibr" rid="bib35">Sabat et al., 2025</xref> found a hierarchical organization of temporal integration in ferrets, reminiscent of humans (<xref ref-type="bibr" rid="bib31">Norman-Haignere et al., 2022</xref>). In the present study, we found a gradient of background invariance that was similarly structured as in humans, yet relying on different mechanisms. Altogether, this raises a fundamental question: Does the ferret auditory cortex correspond to the different early auditory fields in the human core regions, with deeper hierarchical stages simply being absent? Or do these differences reflect evolutionary divergence, with partly shared computational principles and homologous hierarchy, but additional cortical modules in humans, enabling for instance more sophisticated background segregation or dedicated speech and music processing (<xref ref-type="bibr" rid="bib30">Norman-Haignere and McDermott, 2018</xref>)? Addressing these questions will require a finer characterization of cross-species homologies, leveraging high-resolution functional imaging and electrophysiological recordings to uncover whether hierarchical transformations in auditory processing follow a conserved or species-specific trajectory.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal preparation</title><p>Experiments were performed in three female ferrets (B, L, and R), across one or both hemispheres (left for all ferrets + right for R). Experiments were approved by the French Ministry of Agriculture (protocol authorization: 21022) and strictly comply with the European directives on the protection of animals used for scientific purposes (2010/63/EU). Animal preparation and fUS imaging were performed as in <xref ref-type="bibr" rid="bib2">Bimbard et al., 2018</xref>. Briefly, a metal headpost was surgically implanted on the skull under anesthesia. After recovery from surgery, a craniotomy was performed over auditory cortex and then sealed with an ultrasound-transparent polymethylpentene (TPX) cover, embedded in an implant of dental cement. Animals could then recover for 1 week, with unrestricted access to food, water, and environmental enrichment.</p><p>fUSI data were collected using an Iconeus One system (Iconeus, Paris, France) and a linear IcoPrime ultrasonic probe (15 MHz central frequency, 70% bandwidth, 0.110 mm pitch, 128 elements). Complex frames are acquired at 500 Hz. To remove tissue motion, a spatio-temporal clutter filter is applied using sets of 200 contiguous frames (<xref ref-type="bibr" rid="bib6">Demené et al., 2015</xref>). The power Doppler signal is then obtained by taking the mean power over these 200 frames, thus getting an effective 2.5 Hz sampling rate. The power Doppler signal is approximately proportional to CBV (<xref ref-type="bibr" rid="bib19">Macé et al., 2011</xref>).</p><p>In each recording day, we imaged activity in response to the entire sound set sequentially in two coronal slices. The probe was angled at ~30–35° relative to the vertical axis in order for it to be roughly parallel to the surface of the brain. Through several recording sessions, we covered primary to tertiary regions of auditory cortex. Coronal slices were 14 mm wide and spaced 0.4 mm apart. The resolution of each voxel was 0.1 * 0.1 * ~0.4 mm (the latter dimension, called elevation, is dependent on the depth of the voxel).</p></sec><sec id="s4-2"><title>Sound presentation</title><p>Ferrets were awake, head-fixed, and passively listening to different streams of natural sounds: foregrounds in isolation, backgrounds in isolation, or mixtures of both.</p><sec id="s4-2-1"><title>Sound stimuli</title><p>To choose foreground vs. background sounds, we used a previously established method based on sounds’ stationarity (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>; <xref ref-type="bibr" rid="bib24">McWalter and McDermott, 2019</xref>). We reproduce here a description of this algorithm from <xref ref-type="bibr" rid="bib24">McWalter and McDermott, 2019</xref>.</p><p>To quantify the stationarity of real-world sound recordings, we computed the standard deviation of the texture statistics measured across successive segments of a signal for a variety of segment lengths (0.125, 0.25, 0.5, 1, and 2 s). For each segment length, we performed the following steps. First, we divided a signal into adjacent segments of the specified duration and measured the statistics in each segment. Second, we computed the standard deviation across the segments. Third, we averaged the standard deviation across statistics within each statistic class (envelope mean, envelope coefficient of variation, envelope skewness, envelope correlations, modulation power, and modulation correlations). Fourth, we normalized the average standard deviation for a class by its median value across sounds (to put the statistic classes on comparable scales, and to compensate for any differences between statistics in intrinsic variability). Fifth, we averaged the normalized values across statistics classes. Sixth, we averaged the resulting measure for the five segment lengths to yield a single measure of variability for each real-world sound recording. Finally, we took the negative logarithm of this quantity, yielding the stationarity measure.</p><p>We then defined arbitrary thresholds to define foregrounds and backgrounds, so that both groups were well separated in the stationarity axis (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Sounds were taken from <xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref> and <xref ref-type="bibr" rid="bib29">Norman-Haignere et al., 2015</xref> or downloaded from online resources (see <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref> for the full list of sounds).</p></sec><sec id="s4-2-2"><title>Sound design and presentation</title><p>Foreground and background streams were composed of sequences of six different sound snippets of 9.6 s duration, repeated twice, so that the same snippet would occur at two distinct times of the stream (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). For each condition, six such streams were created by drawing randomly from a pool of 36 individual snippets. Foreground and background streams were presented in isolation and in mixtures. In mixtures, both streams were overlaid with a delay of 4.8 s so that a change, either of foreground or background, occurred every 4.8 s. Each foreground stream was presented with two different background streams, and vice versa. In this design, each foreground snippet was presented with four different backgrounds, and in two history contexts.</p><p>Recording sessions were composed of three ‘runs’ in which two foreground streams and two background streams were presented in all possible combinations, each preceded by 20 s of silence to establish baseline activity. These runs were presented twice. The order of sounds within each run was fixed, but the relative order of runs was randomized for each recording session. A list of sounds used is available in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p>Sounds were presented at 65 dB SPL. In mixtures, foregrounds and backgrounds were presented with a similar gain (0 dB SNR). Each snippet was normalized by its root mean square prior to integration in the sequence.</p></sec></sec><sec id="s4-3"><title>Data analysis</title><sec id="s4-3-1"><title>Denoising and normalization</title><p>To correct for large movements of the brain or deformations throughout the session, we used NoRMcorre, a non-rigid motion correction algorithm developed for two-photon imaging (<xref ref-type="bibr" rid="bib32">Pnevmatikakis and Giovannucci, 2017</xref>).</p><p>Data were then denoised with canonical correlation analysis (CCA) to remove components shared between regions inside and outside of the brain, as described in <xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref> with the difference that data was not recentered beforehand.</p><p>We normalized the signal in each voxel by subtracting and then dividing by average baseline activity. Baseline activity was estimated during 20 s segments of silence occurring every ~20 min, before each run. The obtained value was called <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi><mml:mi>V</mml:mi></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$  \Delta CBV$\end{document}</tex-math></alternatives></inline-formula> and expressed in percent changes in CBV.</p></sec><sec id="s4-3-2"><title>Combining different recordings</title><p>The whole dataset consisted of multiple slices, each recorded in a different recording session. Slices to image on a given day were chosen at random to avoid any systematic bias. Responses were consistent across neighboring slices recorded on different sessions, as shown by the maps of average responses (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) where any spatial continuity across different slices must reflect a true underlying signal in the absence of common noise.</p></sec><sec id="s4-3-3"><title>Cross-correlation matrices</title><p>For each voxel, we computed the Pearson correlation between the vectors of responses to sounds of a given category (either foregrounds, backgrounds, or mixtures) for two repeats, and with different lags. We then averaged these matrices across all voxels to obtain the cross-correlation matrices shown in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. In the rest of the analyses, we use time-averaged responses (2.4–4.8 s after sound change). This range was chosen as the biggest window we could use (to improve SNR) while minimizing contamination from the previous or next sound (indeed, blood volume typically lags neuronal activity by 1.5–2 s). In <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, we show similar cross-correlation matrices between responses to mixtures and either foregrounds or backgrounds, to confirm that background and foreground invariance are also stable in time.</p><p>We computed the voxelwise test–retest correlation by correlating the vector of time-averaged responses to sounds across two repeats, for each category (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We then focused on voxels with a test–retest correlation above 0.3 in at least one category (foregrounds, backgrounds, or mixtures) and above 0 in all categories (which ensures that the noise correction can be applied).</p></sec><sec id="s4-3-4"><title>Voxel-wise correlations</title><p>For each voxel, we defined background invariance (resp. foreground invariance) as the noise-corrected correlation between mixtures and corresponding foregrounds (resp. backgrounds), across sounds. We excluded the first and last sound snippet of each block, in which either foreground or background was missing in half of the snippet (see design in <xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>We used a noise-corrected version of Pearson correlation to take into account differences in test-retest reliability across voxels. Specifically, the noise-corrected correlation should provide an estimate of the correlation we would measure in the limit of infinite data (<xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>). The noise-corrected correlation values were obtained using the following equation:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>∗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  r_{noise-corrected}(\boldsymbol{x},\boldsymbol{y}) = \frac{0.5*(corr(\boldsymbol{x}_1,\boldsymbol{y}_1)+corr(\boldsymbol{x}_2,\boldsymbol{y}_2))}{\sqrt{corr(\boldsymbol{x}_1,\boldsymbol{x}_2)*corr(\boldsymbol{y}_1,\boldsymbol{y}_2)}} $$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf39"><mml:msub><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><tex-math id="inft39">\begin{document}$\boldsymbol{x}_{1}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf40"><mml:msub><mml:mi mathvariant="bold-italic">𝒙</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><tex-math id="inft40">\begin{document}$\boldsymbol{x}_{2}$\end{document}</tex-math></alternatives></inline-formula> (resp. <inline-formula><alternatives><mml:math id="inf41"><mml:msub><mml:mi mathvariant="bold-italic">𝒚</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><tex-math id="inft41">\begin{document}$\boldsymbol{y}_{1}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf42"><mml:msub><mml:mi mathvariant="bold-italic">𝒚</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><tex-math id="inft42">\begin{document}$\boldsymbol{y}_{2}$\end{document}</tex-math></alternatives></inline-formula>) are two repetitions of <inline-formula><alternatives><mml:math id="inf43"><mml:mi mathvariant="bold-italic">𝒙</mml:mi></mml:math><tex-math id="inft43">\begin{document}$\boldsymbol{x}$\end{document}</tex-math></alternatives></inline-formula> (resp. <inline-formula><alternatives><mml:math id="inf44"><mml:mi mathvariant="bold-italic">𝒚</mml:mi></mml:math><tex-math id="inft44">\begin{document}$\boldsymbol{y}$\end{document}</tex-math></alternatives></inline-formula>).</p><p>After this correction, the differences we observed between brain regions were present regardless of voxels’ test–retest reliability, or noise level (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Thus, potential differences in vasculature across regions are unlikely to affect our results.</p></sec><sec id="s4-3-5"><title>ROI analyses</title><p>We manually defined ROIs by combining anatomical markers and sound responsiveness. We found CBV responses to be organized in distinct patches in auditory cortex (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Using anatomical landmarks (<xref ref-type="bibr" rid="bib34">Radtke-Schuller, 2018</xref>), we defined boundaries between the different functional regions from primary (MEG, medial ectosylvian gyrus) to secondary (dPEG, dorsal ectosylvian gyrus) and tertiary (VP, ventral posterior auditory field).</p></sec><sec id="s4-3-6"><title>Map display</title><p>For surface views, we averaged values over the depth of the slice. The transparency (alpha value) of each surface bin was determined by the number of voxels included in the average. Thus, white sections in maps correspond to regions in which no reliable voxels were recorded. All maps in the main figures show the left auditory cortex of ferret L. Maps for other ferrets are shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</p></sec><sec id="s4-3-7"><title>Cochlear and spectrotemporal modulation energy estimation</title><p>Spectrotemporal modulation energy was calculated by measuring the strength of modulations in filtered cochleagrams (which highlight modulations at a particular temporal rate and/or spectral scale) (<xref ref-type="bibr" rid="bib4">Chi et al., 2005</xref>). Specifically, modulation strength was computed as the standard deviation across time of each frequency channel of the filtered cochleagram, using the same 400 ms windows as for fUSI data. The bank of spectrotemporal filters used was the same as in <xref ref-type="bibr" rid="bib16">Landemard et al., 2021</xref> (scales: 0, 0.25, 0.5, 1, 2, 4, 8 cyc/oct; rates: 0.5, 1, 2, 4, 8, 16, 32, 64 Hz). Modulation energy was then averaged in 10 frequency bins (centers: 27, 50, 91, 167, 307, 1032, 3470 Hz).</p><p>We then averaged modulation energy using the same temporal window as for fUSI sound-evoked data (2.4–4.8 s after sound change), after shifting it by -0.8 s to account for hemodynamic delays. We thus obtained a feature matrix (frequency * temporal modulation * spectral modulation) for each sound segment.</p><p>We fit a linear model using ridge regression to predict voxel responses from sound features. The feature matrix was z-scored as part of the regression. We used sixfold cross-validation: we split the sound set in six folds (keeping blocks of temporally adjacent sounds in the same fold). One fold was left out and the others were used to fit the weights. The lambda hyperparameter was defined using an inner cross-validation loop. The fitted weights were used to predict responses to left-out sounds. We thus built a matrix of cross-validated predicted responses and applied the same analyses as were applied to the true data (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Prediction accuracy was defined as the Pearson correlation between measured and cross-validated predicted responses (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>To examine the fitted weights, we averaged the weights obtained in all cross-validation folds and multiplied them by the standard deviation of the feature matrix. To extract voxels’ preferred value over one feature axis (e.g. frequency), we first averaged the weight matrix along the other feature axes (e.g. spectral and temporal modulation) and used the value of the feature for which the maximum was reached. For plotting the tuning maps and the analysis of voxels tuned to low or high rates, we only considered voxels with average prediction accuracy in left-out folds higher than 0.3.</p></sec><sec id="s4-3-8"><title>Human analyses</title><p>To directly compare our results with those obtained in humans, we used the dataset from <xref ref-type="bibr" rid="bib14">Kell and McDermott, 2019</xref>. Specifically, we used data from Experiment 1 which provided the closest match to our experimental conditions, and only considered the last seven subjects that heard both the foregrounds and the backgrounds in isolation, in addition to the mixtures. In brief, subjects were presented with 30 natural foregrounds, 30 natural backgrounds, and mixtures of both. Foregrounds and backgrounds were determined based on a criterion of stationarity, similar to our study. For each subject, backgrounds and foregrounds were matched at random (so that the mixtures were different for each subject). Each sound was presented four times. The SNR (ratio of power of foreground vs. background sound) was -6 dB (vs. 0 dB in our case). To encourage attention to each stimulus, participants performed a sound intensity discrimination task on the stimuli. We used a similar criterion for voxel inclusion and conducted the same analyses as for ferrets. Ridge regression was performed using 10-fold cross-validation. For each subject, (foreground, background, mixture) triplets were kept together in the same fold.</p></sec><sec id="s4-3-9"><title>Statistical testing</title><p>To test the difference between background and foreground invariance for each ROI, we ran a permutation test. We built the null distribution by randomly permuting foreground and background labels for each sound and computing invariance for each voxel, 1000 times. We then compared the median difference between background and foreground invariance across voxels, to the values obtained in this null distribution and used the proportion of null values higher than the observed difference to obtain our <italic>p</italic>-value.</p><p>To compute statistical significance between ROIs, we performed a permutation test. We repeated the following procedure for each pair of ROI (MEG vs. dPEG, dPEG vs. VP). We built a null distribution by shuffling ROI labels for voxels belonging to the pair of ROIs within each animal, 1000 times. We then compared the observed average across animals of the difference of median correlations between ROIs across animals to this null distribution and used the proportion of null values more extreme than the observed difference to obtain our <italic>p</italic>-value. We also computed a <italic>p</italic>-value for each ferret separately (see <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>). Finally. we ran this test by first focusing on voxels tuned to high or low rates within each ROI.</p><p>To test for differences in invariance between voxels tuned to low (&lt; 8 Hz) or high (&gt; 8 Hz) temporal modulation rates within each ROI, we used a similar permutation test in which we compared the difference of invariance between voxels tuned to low vs. high rates to a null distribution in which we shuffled the tuning values across voxels within each animal and ROI.</p><p>To compare results between ferrets and humans, we performed Wilcoxon rank-sum tests between ferrets and human subjects for different metrics: median prediction accuracy across voxels, median noise-corrected correlation between measured and predicted invariance. For the latter, we also tested the difference to zero using a sign-rank test.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Experiments were approved by the French Ministry of Agriculture (protocol authorization: 21022) and strictly comply with the European directives on the protection of animals used for scientific purposes (2010/63/EU).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-106628-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data is publicly available on Zenodo: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.15800440">https://doi.org/10.5281/zenodo.15800440</ext-link>. Code to reproduce the figures and analyses presented in this manuscript is available on Github: <ext-link ext-link-type="uri" xlink:href="https://github.com/agneslandemard/naturalsoundmixtures">https://github.com/agneslandemard/naturalsoundmixtures</ext-link> (copy archived at <xref ref-type="bibr" rid="bib17">Landemard, 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Landemard</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>fUS imaging of ferret auditory cortex during passive listening to natural sound mixtures</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.15800440</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Shihab Shamma and Sam V Norman-Haignere for fruitful discussions, Alex JE Kell for providing and helping with the human data, Balkis Cadi and Lynda Bourguignon for technical and administrative support throughout the project. This work was supported by the Institut Universitaire de France, ANR-17-EURE-0017, ANR-10-IDEX-0001–02, as well as an AMX doctoral fellowship to AL.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Yosef</surname><given-names>O</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The effects of background noise on the neural responses to natural sounds in cat primary auditory cortex</article-title><source>Frontiers in Computational Neuroscience</source><volume>1</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.10.003.2007</pub-id><pub-id pub-id-type="pmid">18946525</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Demene</surname><given-names>C</given-names></name><name><surname>Girard</surname><given-names>C</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multi-scale mapping along the auditory hierarchy using high-resolution functional UltraSound in the awake ferret</article-title><source>eLife</source><volume>7</volume><elocation-id>e35028</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.35028</pub-id><pub-id pub-id-type="pmid">29952750</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carruthers</surname><given-names>IM</given-names></name><name><surname>Laplagne</surname><given-names>DA</given-names></name><name><surname>Jaegle</surname><given-names>A</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Mwilambwe-Tshilobo</surname><given-names>L</given-names></name><name><surname>Natan</surname><given-names>RG</given-names></name><name><surname>Geffen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Emergence of invariant representation of vocalizations in the auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>2726</fpage><lpage>2740</lpage><pub-id pub-id-type="doi">10.1152/jn.00095.2015</pub-id><pub-id pub-id-type="pmid">26311178</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>RK</given-names></name><name><surname>Lindén</surname><given-names>H</given-names></name><name><surname>Nakamura</surname><given-names>M</given-names></name><name><surname>Barkat</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>White noise background improves tone discrimination by suppressing cortical tuning curves</article-title><source>Cell Reports</source><volume>29</volume><fpage>2041</fpage><lpage>2053</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.10.049</pub-id><pub-id pub-id-type="pmid">31722216</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demené</surname><given-names>C</given-names></name><name><surname>Deffieux</surname><given-names>T</given-names></name><name><surname>Pernot</surname><given-names>M</given-names></name><name><surname>Osmanski</surname><given-names>B-F</given-names></name><name><surname>Biran</surname><given-names>V</given-names></name><name><surname>Gennisson</surname><given-names>J-L</given-names></name><name><surname>Sieu</surname><given-names>L-A</given-names></name><name><surname>Bergel</surname><given-names>A</given-names></name><name><surname>Franqui</surname><given-names>S</given-names></name><name><surname>Correas</surname><given-names>J-M</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name><name><surname>Baud</surname><given-names>O</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spatiotemporal clutter filtering of ultrafast ultrasound data highly increases doppler and fultrasound sensitivity</article-title><source>IEEE Transactions on Medical Imaging</source><volume>34</volume><fpage>2271</fpage><lpage>2285</lpage><pub-id pub-id-type="doi">10.1109/TMI.2015.2428634</pub-id><pub-id pub-id-type="pmid">25955583</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elgueda</surname><given-names>D</given-names></name><name><surname>Duque</surname><given-names>D</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>State-dependent encoding of sound and behavioral meaning in a tertiary region of the ferret auditory cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>447</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0317-8</pub-id><pub-id pub-id-type="pmid">30692690</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Xiang</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Interaction between attention and bottom-up saliency mediates the representation of foreground and background in an auditory scene</article-title><source>PLOS Biology</source><volume>7</volume><elocation-id>e1000129</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000129</pub-id><pub-id pub-id-type="pmid">19529760</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>JE</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Meaning in the avian auditory cortex: neural representation of communication calls</article-title><source>The European Journal of Neuroscience</source><volume>41</volume><fpage>546</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1111/ejn.12812</pub-id><pub-id pub-id-type="pmid">25728175</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackett</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Organization and correspondence of the auditory cortex of humans and nonhuman primates</article-title><source>Evolution of Nervous Systems</source><volume>4</volume><fpage>109</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/B0-12-370878-8/00012-4</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamersky</surname><given-names>GR</given-names></name><name><surname>Shaheen</surname><given-names>LA</given-names></name><name><surname>Espejo</surname><given-names>ML</given-names></name><name><surname>Wingert</surname><given-names>JC</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Reduced neural responses to natural foreground versus background sounds in the auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>45</volume><elocation-id>e0121242024</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0121-24.2024</pub-id><pub-id pub-id-type="pmid">39837664</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hicks</surname><given-names>JM</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Noise schemas aid hearing in noise</article-title><source>PNAS</source><volume>121</volume><elocation-id>e2408995121</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2408995121</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivanov</surname><given-names>AZ</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Walker</surname><given-names>KMM</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cortical adaptation to sound reverberation</article-title><source>eLife</source><volume>11</volume><elocation-id>e75090</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.75090</pub-id><pub-id pub-id-type="pmid">35617119</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Invariance to background noise as a signature of non-primary auditory cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3958</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11710-y</pub-id><pub-id pub-id-type="pmid">31477711</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalighinejad</surname><given-names>B</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Adaptation of the human auditory cortex to changing background noise</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2509</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10611-4</pub-id><pub-id pub-id-type="pmid">31175304</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landemard</surname><given-names>A</given-names></name><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Demené</surname><given-names>C</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Distinct higher-order representations of natural sounds in human and ferret auditory cortex</article-title><source>eLife</source><volume>10</volume><elocation-id>e65566</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65566</pub-id><pub-id pub-id-type="pmid">34792467</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Landemard</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Naturalsoundmixtures</data-title><version designator="swh:1:rev:5f80d2d375aa634ea5641f47ebcd97b4d994e59c">swh:1:rev:5f80d2d375aa634ea5641f47ebcd97b4d994e59c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:91a273eeec0634c8f6afd01017d9bccc9e39295c;origin=https://github.com/agneslandemard/naturalsoundmixtures;visit=swh:1:snp:14385a89cdff430cdb2900f1365d69fb548185f0;anchor=swh:1:rev:5f80d2d375aa634ea5641f47ebcd97b4d994e59c">https://archive.softwareheritage.org/swh:1:dir:91a273eeec0634c8f6afd01017d9bccc9e39295c;origin=https://github.com/agneslandemard/naturalsoundmixtures;visit=swh:1:snp:14385a89cdff430cdb2900f1365d69fb548185f0;anchor=swh:1:rev:5f80d2d375aa634ea5641f47ebcd97b4d994e59c</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Bio-inspired neural networks implement different recurrent visual processing strategies than task-trained ones do</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.03.07.483196</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macé</surname><given-names>E</given-names></name><name><surname>Montaldo</surname><given-names>G</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name><name><surname>Baulac</surname><given-names>M</given-names></name><name><surname>Fink</surname><given-names>M</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional ultrasound imaging of the brain</article-title><source>Nature Methods</source><volume>8</volume><fpage>662</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1641</pub-id><pub-id pub-id-type="pmid">21725300</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malone</surname><given-names>BJ</given-names></name><name><surname>Heiser</surname><given-names>MA</given-names></name><name><surname>Beitel</surname><given-names>RE</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Background noise exerts diverse effects on the cortical encoding of foreground sounds</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>1034</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1152/jn.00152.2017</pub-id><pub-id pub-id-type="pmid">28490644</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title><source>Neuron</source><volume>71</volume><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.032</pub-id><pub-id pub-id-type="pmid">21903084</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McWalter</surname><given-names>R</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cascaded amplitude modulations in sound texture perception</article-title><source>Frontiers in Neuroscience</source><volume>11</volume><elocation-id>485</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2017.00485</pub-id><pub-id pub-id-type="pmid">28955191</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McWalter</surname><given-names>R</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adaptive and selective time averaging of auditory scenes</article-title><source>Current Biology</source><volume>28</volume><fpage>1405</fpage><lpage>1418</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.049</pub-id><pub-id pub-id-type="pmid">29681472</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McWalter</surname><given-names>R</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Illusory sound texture reveals multi-second statistical completion in auditory scene analysis</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5096</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12893-0</pub-id><pub-id pub-id-type="pmid">31704913</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>LM</given-names></name><name><surname>Escabí</surname><given-names>MA</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>516</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1152/jn.00395.2001</pub-id><pub-id pub-id-type="pmid">11784767</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>RC</given-names></name><name><surname>Lee</surname><given-names>T</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Noise-invariant neurons in the avian auditory cortex: hearing the song in noise</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002942</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002942</pub-id><pub-id pub-id-type="pmid">23505354</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname><given-names>R</given-names></name><name><surname>Bender</surname><given-names>DA</given-names></name><name><surname>Shanechi</surname><given-names>AM</given-names></name><name><surname>Gamble</surname><given-names>JR</given-names></name><name><surname>Barbour</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contextual effects of noise on vocalization encoding in primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>713</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1152/jn.00476.2016</pub-id><pub-id pub-id-type="pmid">27881720</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical pathways for music and speech revealed by hypothesis-free voxel decomposition</article-title><source>Neuron</source><volume>88</volume><fpage>1281</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id><pub-id pub-id-type="pmid">26687225</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural responses to natural and model-matched stimuli reveal distinct computations in primary and nonprimary auditory cortex</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2005127</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2005127</pub-id><pub-id pub-id-type="pmid">30507943</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>Long</surname><given-names>LK</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Irobunda</surname><given-names>I</given-names></name><name><surname>Merricks</surname><given-names>EM</given-names></name><name><surname>Feldstein</surname><given-names>NA</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multiscale temporal integration organizes hierarchical computation in human auditory cortex</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>455</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01261-y</pub-id><pub-id pub-id-type="pmid">35145280</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Giovannucci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>NoRMCorre: An online algorithm for piecewise rigid motion correction of calcium imaging data</article-title><source>Journal of Neuroscience Methods</source><volume>291</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.07.031</pub-id><pub-id pub-id-type="pmid">28782629</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Constructing noise-invariant representations of sound in the auditory pathway</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001710</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001710</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Cyto- and Myeloarchitectural Brain Atlas of the Ferret (Mustela Putorius) in MRI Aided Stereotaxic Coordinates</source><publisher-name>Springer cham</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-76626-3</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sabat</surname><given-names>M</given-names></name><name><surname>Gouyette</surname><given-names>H</given-names></name><name><surname>Gaucher</surname><given-names>Q</given-names></name><name><surname>López Espejo</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Neurons in auditory cortex integrate information within constrained temporal windows that are invariant to the stimulus context and information rate</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2025.02.14.637944</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saderi</surname><given-names>D</given-names></name><name><surname>Buran</surname><given-names>BN</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Streaming of repeated noise in primary and secondary fields of auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3783</fpage><lpage>3798</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2105-19.2020</pub-id><pub-id pub-id-type="pmid">32273487</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saderi</surname><given-names>D</given-names></name><name><surname>Schwartz</surname><given-names>ZP</given-names></name><name><surname>Heller</surname><given-names>CR</given-names></name><name><surname>Pennington</surname><given-names>JR</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dissociation of task engagement and arousal effects in auditory cortex and midbrain</article-title><source>eLife</source><volume>10</volume><elocation-id>e60153</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.60153</pub-id><pub-id pub-id-type="pmid">33570493</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santoro</surname><given-names>R</given-names></name><name><surname>Moerel</surname><given-names>M</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding of natural sounds at multiple spectral and temporal resolutions in the human auditory cortex</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003412</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003412</pub-id><pub-id pub-id-type="pmid">24391486</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>DM</given-names></name><name><surname>Woolley</surname><given-names>SMN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sparse and background-invariant coding of vocalizations in auditory scenes</article-title><source>Neuron</source><volume>79</volume><fpage>141</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.038</pub-id><pub-id pub-id-type="pmid">23849201</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>ZP</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Focal suppression of distractor sounds by selective attention in auditory cortex</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>323</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx288</pub-id><pub-id pub-id-type="pmid">29136104</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Auditory perception through the cortical analysis of spectrotemporal modulations</article-title><source>The Journal of the Acoustical Society of America</source><volume>125</volume><elocation-id>2564</elocation-id><pub-id pub-id-type="doi">10.1121/1.4783707</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname><given-names>AC</given-names></name><name><surname>Koretsky</surname><given-names>AP</given-names></name><name><surname>Duyn</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Functional MRI impulse response for BOLD and CBV contrast in rat somatosensory cortex</article-title><source>Magnetic Resonance in Medicine</source><volume>57</volume><fpage>1110</fpage><lpage>1118</lpage><pub-id pub-id-type="doi">10.1002/mrm.21246</pub-id><pub-id pub-id-type="pmid">17534912</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>3394</fpage><lpage>3411</lpage><pub-id pub-id-type="doi">10.1121/1.1624067</pub-id><pub-id pub-id-type="pmid">14714819</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souffi</surname><given-names>S</given-names></name><name><surname>Lorenzi</surname><given-names>C</given-names></name><name><surname>Varnet</surname><given-names>L</given-names></name><name><surname>Huetz</surname><given-names>C</given-names></name><name><surname>Edeline</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Noise-sensitive but more precise subcortical representations coexist with robust cortical encoding of natural vocalizations</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5228</fpage><lpage>5246</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2731-19.2020</pub-id><pub-id pub-id-type="pmid">32444386</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Town</surname><given-names>SM</given-names></name><name><surname>Poole</surname><given-names>KC</given-names></name><name><surname>Wood</surname><given-names>KC</given-names></name><name><surname>Bizley</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Reversible inactivation of ferret auditory cortex impairs spatial and nonspatial hearing</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>749</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1426-22.2022</pub-id><pub-id pub-id-type="pmid">36604168</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106628.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>King</surname><given-names>Andrew J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This paper presents <bold>valuable</bold> findings on the processing of sound mixtures in the auditory cortex of ferrets, a species widely used for studies of auditory processing. Using the convenient and relatively high-resolution method of functional ultrasound imaging, the authors provide <bold>convincing</bold> evidence that background noise invariance emerges across the auditory cortical processing hierarchy. They also draw informative comparisons with previously published fMRI data obtained in humans. This work will be of interest to researchers studying the auditory cortex and the neural mechanisms underlying auditory scene analysis and hearing in noise.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106628.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This is a very interesting paper addressing the hierarchical nature of the mammalian auditory system. The authors use an unconventional technique to assess brain responses -- functional ultrasound imaging (fUSI). This measures blood volume in cortex at a relatively high spatial resolution. They present dynamic and stationary sounds in isolation and together, and show that the effect of the stationary sounds (relative to the dynamic sounds) on blood volume measurements decreases as one ascends the auditory hierarchy. Since the dynamic/stationary nature of sounds is related to their perception as foreground/background sounds, this suggests that neurons in higher levels of the cortex may be increasingly invariant to background sounds.</p><p>The study is interesting, well conducted and well written. In the revised manuscript, the authors have addressed all the points I raised in my review.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106628.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Noise invariance is an essential computation in sensory systems for stable perception across a wide range of contexts. In this paper, Landemard et al. perform functional ultrasound imaging across primary, secondary and tertiary auditory cortex in ferrets to uncover the mesoscale organization of background invariance in auditory cortex. Consistent with previous work, they find that background invariance increases throughout the cortical hierarchy. Importantly, they find that background invariance is largely explained by progressive changes in spectro-temporal tuning across cortical stations which are biased towards foreground sound features. To test if these results are broadly relevant, they then re-analyze human fMRI data and find that spectro-temporal tuning fails to explain background invariance in human auditory cortex.</p><p>Strengths:</p><p>(1) Novelty of approach: Though the authors have published on this technique previously, functional ultrasound imaging offers unprecedented temporal and spatial resolution in a species where large-scale calcium imaging is not possible and electrophysiological mapping would take weeks or months. Combining mesoscale imaging with a clever stimulus paradigm, they address a fundamental question in sensory coding.</p><p>(2) Quantification and execution: the results are generally clear and well supported by statistical quantification.</p><p>(3) Elegance of modeling: The spectrotemporal model presented here is explained clearly and most importantly, provides a compelling framework for understanding differences in background invariance across cortical areas.</p><p>Comments on revised version:</p><p>The authors have addressed all of my previous concerns and their publicly shared data is easy to view, this is a nice contribution to the field.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106628.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This paper investigates invariance to natural background noise in the auditory cortex of ferrets and humans. The authors first replicate, in ferrets, a finding from human neuroimaging showing that invariance to background noise increases along the cortical hierarchy (i.e. from primary to non-primary auditory cortex). Next, the authors ask whether this pattern of invariance could be explained by differences in tuning to low-level acoustic features across primary and non-primary regions. The authors conclude that this tuning can explain the spatial organization of background invariance in ferrets, but not in humans. The conclusions of the paper are well supported by the data.</p><p>The paper is very straightforwardly written, with a generally clear presentation including well-designed and visually appealing figures. Not only does this paper provide an important replication in a non-human animal model commonly used in auditory neuroscience, but also it extends the original findings in three ways. First, the authors reveal a more fine-grained gradient of background invariance by showing that background invariance increases across primary, secondary and tertiary cortical regions. Second, the authors address a potential mechanism that might underlie this pattern of invariance by considering whether differences in tuning to frequency and spectrotemporal modulations across regions could account for the observed pattern of invariance. The spectrotemporal modulation encoding model used here is a well-established approach in auditory neuroscience and seems appropriate for exploring potential mechanisms underlying invariance in auditory cortex, particularly in ferrets. Third, the authors provide a more complete picture of invariance by additionally analyzing foreground invariance, a complementary measure not explored in the original study.</p><p>Comments on author revisions:</p><p>The authors have thoroughly addressed the concerns raised in my initial review.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106628.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Landemard</surname><given-names>Agnès</given-names></name><role specific-use="author">Author</role><aff><institution>École normale supérieure - PSL</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Bimbard</surname><given-names>Célian</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Boubenec</surname><given-names>Yves</given-names></name><role specific-use="author">Author</role><aff><institution>École normale supérieure - PSL</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1(Public review):</bold></p><p>(1) Changes in blood volume due to brain activity are indirectly related to neuronal responses. The exact relationship is not clear, however, we do know two things for certain: (a) each measurable unit of blood volume change depends on the response of hundreds or thousands of neurons, and (b) the time course of the volume changes are slow compared to the potential time course of the underlying neuronal responses. Both of these mean that important variability in neuronal responses will be averaged out when measuring blood changes. For example, if two neighbouring neurons have opposite responses to a given stimulus, this will produce opposite changes in blood volume, which will cancel each other out in the blood volume measurement due to (a). This is important in the present study because blood volume changes are implicitly being used as a measure of coding in the underlying neuronal population. The authors need to acknowledge that this is a coarse measure of neuronal responses and that important aspects of neuronal responses may be missing from the blood volume measure.</p></disp-quote><p>The reviewer is correct: we do not measure neuronal firing but use blood volume as a proxy for bulk local neuronal activity, which does not capture the richness of single neuron responses. This is why the paper focuses on large-scale spatial representations as well as cross-species comparison. For this latter purpose, fMRI responses are on par with our fUSI data, with both neuroimaging techniques showing the same weakness. We have now added this point to the discussion:</p><p>“Second, we used blood volume as a proxy for local neuronal activity. Thus, our signal ignores any heterogeneity that might exist at the level of local neuronal populations. However, our main findings are related to the large-scale organization of cortical responses and how they relate to those of humans. For this purpose, the functional spatial resolution of our signal, driven by the spatial resolution of neurovascular coupling, should be adapted. In addition, using hemodynamic signals provides a much better comparison with human fMRI data, where the same limitations are present.”</p><disp-quote content-type="editor-comment"><p>(2) More importantly for the present study, however, the effect of (b) is that any rapid changes in the response of a single neuron will be cancelled out by temporal averaging. Imagine a neuron whose response is transient, consisting of rapid excitation followed by rapid inhibition. Temporal averaging of these two responses will tend to cancel out both of them. As a result, blood volume measurements will tend to smooth out any fast, dynamic responses in the underlying neuronal population. In the present study, this temporal averaging is likely to be particularly important because the authors are comparing responses to dynamic (nonstationary) stimuli with responses to more constant stimuli. To a first approximation, neuronal responses to dynamic stimuli are themselves dynamic, and responses to constant stimuli are themselves constant. Therefore, the averaging will mean that the responses to dynamic stimuli are suppressed relative to the real responses in the underlying neurons, whereas the responses to constant stimuli are more veridical. On top of this, temporal following rates tend to decrease as one ascends the auditory hierarchy, meaning that the comparison between dynamic and stationary responses will be differently affected in different brain areas. As a result, the dynamic/stationary balance is expected to change as you ascend the hierarchy, and I would expect this to directly affect the results observed in this study.</p><p>It is not trivial to extrapolate from what we know about temporal following in the cortex to know exactly what the expected effect would be on the authors' results. As a first-pass control, I would strongly suggest incorporating into the authors' filterbank model a range of realistic temporal following rates (decreasing at higher levels), and spatially and temporally average these responses to get modelled cerebral blood flow measurements. I would want to know whether this model showed similar effects as in Figure 2. From my guess about what this model would show, I think it would not predict the effects shown by the authors in Figure 2. Nevertheless, this is an important issue to address and to provide control for.</p></disp-quote><p>We understand the reviewer’s concern about potential differences in response dynamics in stationary vs non-stationary sounds. It seems that the reviewer is concerned that responses to foregrounds may be suppressed in non-primary fields because foregrounds are not stationary, and non-primary regions could struggle to track and respond to these sounds. Nevertheless, we observed the contrary, with non-primary regions overrepresenting non-stationary (dynamic) sounds, over stationary ones. For this reason, we are inclined to think that this explanation cannot falsify our findings.</p><p>We understand the comment that temporal following rates might differ across regions in the auditory hierarchy and agree. In fact, we do show that tuning to temporal rates differs across regions and partly explains the differences in background invariance we observe. In this regard, we think the reviewer’s suggestion is already implemented by our spectrotemporal model, which incorporates the full range of realistic temporal following rates (up to 128 Hz). The temporal averaging is done as we take the output of the model (which varies continuously through time) and average it in the same window as we used for fUSI data. When we fit this model to the ferret data, we find that voxels in non-primary regions, especially VP (tertiary auditory cortex), tend to be more tuned to low temporal rates (Figure 2F, G), and that background invariance is stronger in voxels tuned to low rates. This is, however, not true in humans, suggesting that background invariance in humans relies on different computational mechanisms. We have added a sentence to clarify this: “The model included a range of realistic temporal rates and this axis was the most informative to discriminate foregrounds from backgrounds.”</p><disp-quote content-type="editor-comment"><p>(3) I do not agree with the equivalence that the authors draw between the statistical stationarity of sounds and their classification as foreground or background sounds. It is true that, in a common foreground/background situation - speech against a background of white noise - the foreground is non-stationary and the background is stationary. However, it is easy to come up with examples where this relationship is reversed. For example, a continuous pure tone is perfectly stationary, but will be perceived as a foreground sound if played loudly. Background music may be very non-stationary but still easily ignored as a background sound when listening to overlaid speech. Ultimately, the foreground/background distinction is a perceptual one that is not exclusively determined by physical characteristics of the sounds, and certainly not by a simple measure of stationarity. I understand that the use of foreground/background in the present study increases the likely reach of the paper, but I don't think it is appropriate to use this subjective/imprecise terminology in the results section of the paper.</p></disp-quote><p>We appreciate the reviewer’s comment that the classification of our sounds into foregrounds and backgrounds is not verified by any perceptual experiments. We use those terms to be consistent with the literature (McWalter and McDermott, 2018; McWalter and McDermott, 2019), including the paper we derived this definition from (Kell et al., 2019). These terms are widely used in studies where no perceptual or behavioral experiments are included, and even when animals are anesthetized. We have clarified and justified this choice in the beginning of the Results section:</p><p>“We used three types of stimuli: foregrounds, backgrounds, and combinations of those. We use those terms to refer to sounds differing in their stationarity, under the assumption that stationary sounds carry less information than non-stationary sounds, and are thus typically ignored.”</p><p>We have also added a paragraph in the discussion to emphasize the limits of this definition:</p><p>“First, this study defined foregrounds and backgrounds solely based on their acoustic stationarity, rather than perceptual judgments. This choice allowed us to isolate the contribution of acoustic factors in a simplified setting. Within this controlled framework, we show that acoustic features of foreground and background sounds drive their separation in the brain and the hierarchical extraction of foreground sound features.”</p><disp-quote content-type="editor-comment"><p>(4) Related to the above, I think further caveats need to be acknowledged in the study. We do not know what sounds are perceived as foreground or background sounds by ferrets, or indeed whether they make this distinction reliably to the degree that humans do. Furthermore, the individual sounds used here have not been tested for their foreground/background-ness. Thus, the analysis relies on two logical jumps - first, that the stationarity of these sounds predicts their foreground/background perception in humans, and second, that this perceptual distinction is similar in ferrets and humans. I don't think it is known to what degree these jumps are justified. These issues do not directly affect the results, but I think it is essential to address these issues in the Discussion, because they are potentially major caveats to our understanding of the work.</p></disp-quote><p>We agree with the reviewer that the foreground-background distinction might be different in ferrets. In anticipation of that issue, we had enriched the sound set with more ecologically relevant sounds, such as ferret and other animal vocalizations. Nevertheless, we have emphasized this limitation in addition to the limitation of our definition of foregrounds and backgrounds in the discussion:</p><p>“In addition, most of the sounds included in our study likely have more relevance for humans compared to ferrets (see table 1). Despite including ferret vocalizations and environmental sounds that are more ecologically relevant for ferrets, it is not clear whether ferrets would behaviorally categorize foregrounds and backgrounds as humans do. Examining how ferrets naturally orient or respond to foreground and background sounds under more ecologically valid conditions, potentially with free exploration or spontaneous listening paradigms, could help address this issue.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2(Public review);</bold></p><p>(1) Interpretation of the cerebral blood volume signal: While the results are compelling, more caution should be exercised by the authors in framing their results, given that they are measuring an indirect measure of neural activity, this is the difference between stating &quot;CBV in area MEG was less background invariant than in higher areas&quot; vs. saying &quot;MEG was less background invariant than other areas&quot;. Beyond framing, the basic properties of the CBV signal should be better explored:</p><p>a) Cortical vasculature is highly structured (e.g. Kirst et al.(2020) Cell). One potential explanation for the results is simply differences in vasculature and blood flow between primary and secondary areas of auditory cortex, even if fUS is sensitive to changes in blood flow, changes in capillary beds, etc (Mace et al., 2011) Nat. Methods.. This concern could be addressed by either analyzing spontaneous fluctuations in the CBV signal during silent periods or computing a signal-to-noise ratio of voxels across areas across all sound types. This is especially important given the complex 3D geometry of gyri and sulci in the ferret brain.</p></disp-quote><p>We agree with the reviewers that there could be differences in vasculature across subregions of the auditory cortex and note that this point would also be valid for the published human fMRI data. Nevertheless, even if small differences in vasculature were present, it is unlikely that they would affect our analyses and results, which are designed to be independent of local vascular density. First, we normalize the signal in each voxel using the silent periods, so that the absolute strength of the raw signal, or baseline blood volume in each voxel, is factored in our analysis. Second, we only focus on reliably responsive voxels in each region and do see comparable sound-evoked responses in all regions (Figure S2). Third, our analysis mostly relies on voxel-based correlation across sounds, which is independent of the mean and variance of the voxel responses. Differences in noise, measured through test-retest reliability, can affect values of correlation, which is why we used a noise-correction procedure. After this procedure, invariance does not depend on test-retest, and differences across regions are still seen when matching for test-retest (new Figure S7). Thus, we believe that differences in vascular architecture across regions are unlikely to affect our results. We added this point in the Methods section when discussing the noise-correction:</p><p>“After this correction, the differences we observed between brain regions were present regardless of voxels' test-retest reliability, or noise level (Figure S7). Thus, potential differences in vasculature across regions are unlikely to affect our results.”</p><disp-quote content-type="editor-comment"><p>b) Figure 1 leaves the reader uncertain what exactly is being encoded by the CBV signal, as temporal responses to different stimuli look very similar in the examples shown. One possibility is that the CBV is an acoustic change signal. In that case, sounds that are farther apart in acoustic space from previous sounds would elicit larger responses, which is straightforward to test. Another possibility is that the fUS signal reflects time-varying features in the acoustic signal (e.g. the low-frequency envelope). This could be addressed by cross-correlating the stimulus envelope with fUS waveform. The third possibility, which the authors argue, is that the magnitude of the fUS signal encodes the stimulus ID. A better understanding of the justification for only looking at the fUS magnitude in a short time window (2-4.8 s re: stimulus onset) would increase my confidence in the results.</p></disp-quote><p>We thank the reviewer for raising that point as it highlights that the layout of Figure 1 is misleading. While Figure 1B shows an example snippet of our sound streams, Figure 1D shows the average timecourse of CBV time-locked to a change in sound (foreground or background, isolated or in a mixture). This is the average across all voxels and sounds, aiming at illustrating the dynamics for the three broad categories. In Figure 1E however, we show the cross-validated cross-correlation of CBV across sounds (and different time lags). To obtain this, we compute for each voxel the response to each sound at each time lag, thus obtaining two vectors (size: number of sounds) per lag, one per repeat. Then, we correlate all these vectors across the two repeats, obtaining one cross-correlation matrix per voxel. We finally average these matrices across all voxels. The presence of red squares with high correlations demonstrates that the signal encodes sound identity, since CBV is more similar across two repeats of the same sound (e.g., in the foreground only matrix, 0-5 s vs 0-5 s), than two different sounds (0-5 s vs. 7-12 s). We modified the figure layout as well as the legend to improve clarity.</p><disp-quote content-type="editor-comment"><p>(2) Interpretation of the human data: The authors acknowledge in the discussion that there are several differences between fMRI and fUS. The results would be more compelling if they performed a control analysis where they downsampled the Ferret fUS data spatially and temporally to match the resolution of fMRI and demonstrated that their ferret results hold with lower spatiotemporal resolution.</p></disp-quote><p>We agree with the reviewer that the use of different techniques might come in the way of cross-species comparison. We already control for the temporal aspect by using the average of stimulus-evoked activity across time (note that due to scanner noise, sounds are presented cut into small pieces in the fMRI experiments). Regarding the spatial aspect, there are several things to consider. First, both species have brains of very different sizes, a factor that is conveniently compensated for by the higher spatial resolution of fUSI compared to fMRI (0.1 vs 2 mm). Downsampling to fMRI resolution would lead to having one voxel per region per slice, which is not feasible. We also summarize results with one value per region, which is a form of downsampling that is fairer across species. Furthermore, we believe that we already established in a previous study (Landemard et al, 2021 eLife) that fUSI and fMRI data are comparable signals. We indeed could predict human fMRI responses to most sounds from ferret fUSI responses to the same identical sounds. We clarified these points in the discussion:</p><p>“In addition, fMRI has a worse spatial resolution than fUSI (here, 2 vs. 0.1 mm voxels). However, this difference in resolution compensates for the difference in brain size between humans and ferrets. In our previous work, we showed that a large fraction of cortical responses to natural sounds could be predicted from one species to the other using these methods (Landemard et al., 2021).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>As mentioned above, interpretation of the invariance analyses using predictions from the spectrotemporal modulation encoding model hinges on the model's ability to accurately predict neural responses. Although Figure S5 suggests the encoding model was generally able to predict voxel responses accurately, the authors note in the introduction that, in human auditory cortex, this kind of tuning can explain responses in primary areas but not in non-primary areas (Norman-Haignere &amp; McDermott, PLOS Biol. 2018). Indeed, the prediction accuracy histograms in Figure S5C suggest a slight difference in the model's ability to predict responses in primary versus non-primary voxels. Additional analyses should be done to (a) determine whether the prediction accuracies are meaningfully different across regions and (b) examine whether controlling for prediction accuracy across regions (i.e., subselecting voxels across regions with matched prediction accuracy) affects the outcomes of the invariance analyses.</p></disp-quote><p>The reviewer is correct: the spectrotemporal model tends to perform less well in human non-primary cortex. We believe this does not contradict our results but goes in the same direction: while there is a gradient in invariance in both ferrets and humans, this gradient is predicted by the spectrotemporal model in ferrets, but not in humans (possibly indeed because predictions are less good in human non-primary auditory cortex). Regardless of the mechanism, this result points to a difference across species. In ferrets, we found a significantly better prediction accuracy in VP (p=0.001, permutation test) and no differences between MEG and dPEG (p=0.89). In humans, prediction accuracy was slightly higher in primary compared to non-primary auditory cortex, but this effect was not significant (p=0.076). In both species, when matching prediction accuracy between regions, the gradients in invariance were preserved. We have added these analyses to the manuscript (Figure S5).</p><disp-quote content-type="editor-comment"><p>A related concern is the procedure used to train the encoding model. From the methods, it appears that the model may have been fit using responses to both isolated and mixture sounds. If so, this raises questions about the interpretability of the invariance analyses. In particular, fitting the model to all stimuli, including mixtures, may inflate the apparent ability of the model to &quot;explain&quot; invariance, since it is effectively trained on the phenomenon it is later evaluated on. Put another way, if a voxel exhibits invariance, and the model is trained to predict the voxel's responses to all types of stimuli (both isolated sounds and mixtures), then the model must also show invariance to the extent it can accurately predict voxel responses, making the result somewhat circular. A more informative approach would be to train the encoding model only on responses to isolated sounds (or even better, a completely independent set of sounds), as this would help clarify whether any observed invariance is emergent from the model (i.e., truly a result of low-level tuning to spectrotemporal features) or simply reflects what it was trained to reproduce.</p></disp-quote><p>We thank the reviewer for this suggestion. We have run an additional prediction using only the sounds presented in isolation, which replicates our main results (new Figure S6). We have added this control to the manuscript:</p><p>“Results were similar if the model was fit solely on isolated sounds, excluding mixtures from the training set (Figure S6).”</p><disp-quote content-type="editor-comment"><p>Finally, the interpretation of the foreground invariance results remains somewhat unclear. In ferrets (Figure 2I), the authors report relatively little foreground invariance, whereas in humans (Figure 5G), most participants appear to show relatively high levels of foreground invariance in primary auditory cortex (around 0.6 or greater). However, the paper does not explicitly address these apparent crossspecies differences. Moreover, the findings in ferrets seem at odds with other recent work in ferrets (Hamersky et al. 2025 J. Neurosci.), which shows that background sounds tend to dominate responses to mixtures, suggesting a prevalence of foreground invariance at the neuronal level. Although this comparison comes with the caveat that the methods differ substantially from those used in the current study, given the contrast with the findings of this paper, further discussion would nonetheless be valuable to help contextualize the current findings and clarify how they relate to prior work.</p></disp-quote><p>We thank the reviewer for this point. While we found a trend for higher background invariance than foreground invariance in ferret primary auditory cortex, this difference was not significant and many voxels exhibit similar levels of background and foreground invariance (for example in Figure 2D, G). Thus, we do not think our results are inconsistent with Hamersky et al., 2025, though we agree the bias towards background sounds is not as strong in our data. This might indeed reflect differences in methodology, both in the signal that is measured (blood volume vs spikes), and the sound presentation paradigm. Our timescales are much slower and likely reflect responses post-adaptation, which might not be as true for Hamersky et al. We have added this point to the discussion, as well as a comment on the difference between ferrets and humans in foreground invariance in primary auditory cortex:</p><p>“In ferrets, primary auditory cortex has been found to over-represent backgrounds in mixtures compared to foregrounds (Hamersky et al., 2025). In contrast, we found a slight, non-significant bias towards foregrounds in primary regions. This difference could be driven by a difference in timescales, as we looked at slower timescales in which adaptation might be more present, reducing the strength of background encoding. In humans, we found a much smaller gap between background and foreground invariance in primary auditory cortex, which was not predicted by the spectrotemporal model. Additional, more closely controlled experiments would be needed to confirm and understand this species difference.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) In the introduction, explain the relationship between background/foreground and stationarity/non-stationarity, and thus why stationary/nonstationary stimuli could be used to probe differences in background/foreground processing.</p></disp-quote><p>We have added a sentence at the beginning of the results section to justify our choice (see public review).</p><disp-quote content-type="editor-comment"><p>(2) Avoid use of the background/foreground terminology in Results (and probably Methods).</p></disp-quote><p>For consistency with previous literature, we decided to keep this terminology, though imperfect. We further justified our choice in the beginning of the Results section (see previous point).</p><disp-quote content-type="editor-comment"><p>(3) In the Discussion, explain what the implications of the results are for background/foreground processing, and, importantly, highlight any caveats that result from stationarity not being a direct measure of background/foreground.</p></disp-quote><p>We added a paragraph in the Discussion to highlight this point choice (see public review).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) Figure 1: Showing a silent period in the examples would help in understanding the fUS signal.</p></disp-quote><p>In Figure 1D, we show the average timecourse of CBV time-locked to a change in sound (foreground or background, isolated or in a mixture). This is the average across all voxels and sounds. Thus, it would not be very informative to show an equivalent plot for a silent period, as it would look flat by definition. However, we updated the layout and legend of Figure 1 to make it clearer and avoid confusion.</p><disp-quote content-type="editor-comment"><p>(2) &quot;Responses were not homogenous&quot; - would make more sense to say something like &quot;responses were not spatially distributed&quot;.</p></disp-quote><p>We removed these words which were indeed not necessary: “We found that reliable soundevoked responses were confined to the central part of ventral gyrus of the auditory cortex.”</p><disp-quote content-type="editor-comment"><p>(3) Figure 2D: The maps shown in Figure 2D are difficult to understand for the noninitiated in fUS. At a minimum, labels should be added to indicate A-P, M-L, D-V. I cannot see the white square in the primary figure. An additional graphic would be helpful here to understand the geometry of the measurement.</p></disp-quote><p>We thank the reviewer for pointing out that reading these images is indeed an acquired skill. We added an annotated image of anatomy with indications of main features to guide the reader in Figure 1. We also added missing white squares.</p><disp-quote content-type="editor-comment"><p>(4) Figure 2F: Can the authors better justify why the summary statistic is shown for all three areas, but the individual data only compares primary vs. higher order?`</p></disp-quote><p>We now show individual data for all three areas.</p><disp-quote content-type="editor-comment"><p>(5) More methods information is needed to understand how recordings were stitched across days. Was any statistical modeling used to factor out the influence of day on overall response levels?</p></disp-quote><p>We simply concatenated voxels recorded across different sessions and days. The slices were sampled randomly to avoid any systematic effect. Because different slices were sampled in different sessions, any spatial structure spanning several slices is unlikely to be artefactual. For instance, the map of average responses in Figure 2A shows a high level of continuity of spatial patterns across slices. This indicates that this pattern reflects a true underlying organization rather than session-specific noise. It also shows that the overall response levels are not affected by the day or recording session. We added a section in the Methods (“Combining different recordings”) to clarify this point:</p><p>“The whole dataset consisted of multiple slices, each recorded in a different recording session. Slices to image on a given day were chosen at random to avoid any systematic bias. Responses were consistent across neighboring slices recorded on different sessions, as shown by the maps of average responses (Figure 2A, Figure S2) where any spatial continuity across different slices must reflect a true underlying signal in the absence of common noise.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>(1) Figures:</p><p>The figures are generally very well done and visually appealing. However, I have a few suggestions and questions.</p><disp-quote content-type="editor-comment"><p>a) In Figure 1G, the delta CBV ranges from 0.5 to 1.5, although in subsequent figures (e.g., Figure 2D), the range is much larger (-15 to 45). Is it possible that the first figure is a proportion rather than a percentage, or is there some other explanation for the massive difference in scale? Not being very familiar with this measure, it was confusing.</p></disp-quote></disp-quote><p>The same scale is used in both figures, the major difference being that in Figure 1D, we take the average over all voxels and sounds (for each category), which will include many nonresponsive voxels, and for responsive voxels, sounds that they do not respond a lot to. On the other hand, Figure 2D shows the response of a single, responsive voxel. Thus, the values it reaches for its preferred sounds (45%) are an extreme, which weighs only little in Figure 1D. We have changed the legend of Figure 1D to make this more explicit.</p><disp-quote content-type="editor-comment"><p>b) Similar to the first point, the strength of the correlations in the matrices of Figure 1E is very small (~ 0.05) compared to the test-retest reliabilities plotted in Figure 2B (~0.5). Again, I was confused by this large difference in scale.</p></disp-quote><p>Two main factors explain the difference in values between Figure 1E and Figure 2B. First, in Figure 1B, each correlation is done on the average activity in a window of 0.3 s, opposed to 2.4 s in Figure 2B. More averaging leads to better SNR, which inevitably leads to higher testretest correlations. Second, in Figure 1B, the cross-correlation matrices are averaged across all responsive voxels without any criterion for reliability. On the other hand, Figure 2B show example voxels with good test-retest reliability.</p><disp-quote content-type="editor-comment"><p>c) In Figure 2D, the example voxels are supposed to be shown in white. It appears that this example voxel is only shown for the non-primary voxel. Please be sure to add these voxels throughout the other panels and figures as well.</p></disp-quote><p>We fixed this mistake and added the example voxel in all panels.</p><disp-quote content-type="editor-comment"><p>d) Why do the invariance results (e.g., Figure 2F) for individual animals combine across dPEG and VP, while the overall results (across all animals) split things across all three regions? The results in Table 2 do, in fact, provide this data. Upon further examination of the data in Table 2, it seems like there is only a significant difference between background invariance between dPEG and VP for one of the two animals, and that this might be what drives the effect when pooling across all animals. This seems important to both show visually in the figure and to potentially discuss. There is still very clearly a difference between primary and non-primary, but whether there is a real difference between dPEG and VP seems more unclear.</p></disp-quote><p>We added the values for single animals in the plot and highlighted this limitation in the text:</p><p>“While background invariance was overall highest in VP, the differences within non-primary areas were more variable across animals (see table 2).”</p><disp-quote content-type="editor-comment"><p>e) Again, as in Figure 2F, the cross symbols seem like a bad choice as markers since the vertical components of the cross are suggestive of the error of the measurement. However, no error is actually plotted in these figures. I recommend using a different marker and including some measure of error in the invariance plots.</p></disp-quote><p>We replaced the crosses with circles to avoid confusion. The measure of error is provided by the representation of values for single animals.</p><disp-quote content-type="editor-comment"><p>f) The caption for Figure 4C states that each line corresponds to one animal, but does not precisely state what this line represents. Is this the median or something?</p></disp-quote><p>Each line indeed represents the median across voxels for one animal. We added this information to the legend.</p><disp-quote content-type="editor-comment"><p>g) In Figure 5, the captions for panels D and E are swapped.</p></disp-quote><p>This has now been corrected.</p><disp-quote content-type="editor-comment"><p>(2) Discussion:</p><p>(a) In the paragraph on methodological differences, it mentions that the fMRI voxel size is around 2 mm. This may be true in general, but given the comparison to Kell &amp; McDermott 2019, the voxel size should reflect that used in their study (1 mm).</p></disp-quote><p>The reviewer might refer to this sentence from the methods of Kell et al., 2019: “T1weighted anatomical images were collected in each participant (1-mm isotropic voxels) for alignment and cortical surface reconstruction.” However, this does not correspond to the resolution of the functional data, which is 2 mm, as mentioned a bit further in the Methods: “In-plane resolution was 2 × 2 mm (96 × 96 matrix), and slice thickness was 2.8 mm with a 10% gap, yielding an effective voxel size of 2 × 2 × 3.08 mm.”</p><disp-quote content-type="editor-comment"><p>(b) In the next paragraph on the control of attention, it mentions that attentional differences could play a role. However, in Kell &amp; McDermott 2019, they manipulated attention (attend visual versus attend auditory) and found that it did not substantially affect the observed pattern invariance. I suppose it could potentially affect the degree to which an encoding model could explain the invariance. This seems important, and given that the data was already collected, it could be worth it to analyze that data.</p></disp-quote><p>As the reviewer points out, Kell et al. 2019 ran an additional experiment in which they manipulated auditory vs. visual attention. However, the auditory task was just based on loudness and ensured that the participants were awake and paying attention to the stimuli, but not specifically to the foreground or background. This type of attention did not lead to changes in the observed patterns of invariance, which might have been the case for selective attention to backgrounds or foregrounds in the mixture. Given that these manipulations were not done in the ferret experiments, we chose to not include the analysis of this dataset in the scope of this paper. However, future work investigating that topic further would indeed be of interest.</p><disp-quote content-type="editor-comment"><p>(c) The mention of &quot;a convolutional neural network trained to recognize digits in noise&quot; should make more obvious that this is visual recognition rather than auditory recognition.</p></disp-quote><p>We clarified this sentence to make clear that the recognition is visual and not auditory: “For instance, in a convolutional neural network trained to visually recognize digits in different types of noise, when local feedback is implemented, early layers encode noise properties, while later layers represent clean signal.”</p><disp-quote content-type="editor-comment"><p>(d) Finally, one explanation of the results in the discussion is that &quot;primary auditory areas could be recruited to maintain background representations, enabling downstream cortical regions to use these representations to specifically suppress background information and enhance foreground representations.&quot; This &quot;background-related information&quot; being used to &quot;facilitate further extraction of foregrounds&quot; is similar to what is argued in Hicks &amp; McDermott PNAS 2024.</p></disp-quote><p>We thank the reviewer for suggesting this relevant reference and added it in this paragraph of the discussion.</p><disp-quote content-type="editor-comment"><p>(3) Methods:</p><p>In the &quot;Cross-correlation matrices&quot; section, it mentions that time-averaged responses from 2.4 to 4.8 s were used. It would be helpful to provide an explanation of why this particular time window was used. Additionally, I wondered whether one could look at adaptation type effects (e.g., that of Khalighinejad et al., 2019) or whether fUSI does not offer this kind of temporal precision?</p></disp-quote><p>The effects shown in Khalighinejad et al., 2019, are indeed likely too fast to be observed with our methods. However, there are still dynamics in the fUSI signal and in its invariance (Figure S1). Each individual combination of foreground and background is presented for 4.8 s (Figure 1B). Therefore, we chose the range 2.4-4.8 s as the biggest window we could use (to improve SNR) while minimizing contamination from the previous or next sound (indeed, blood volume typically lags neuronal activity by 1.5-2 s). We added this precision to the methods.</p><disp-quote content-type="editor-comment"><p>In the &quot;Human analyses&quot; section, it is very unclear which set of data was used from Kell &amp; McDermott 2019. For example, that paper contains 4 different experiments, none of which has 7 subjects. Upon closer reading, it seems that only 7 of the 11 participants from Experiment 1 also heard the background sounds in isolation (thus enabling the foreground invariance analyses). However, they stated that there were only 3 female participants in that experiment, while you state that you used data from 7 females. It would be helpful to double-check this and to more clearly state exactly which participants (i.e., from which experiment) were used and why (e.g., why not use data from Experiment 4 in the visual task/attention condition?).</p></disp-quote><p>We added a sentence to clarify which datasets were used: “Specifically, we used data from Experiment 1 which provided the closest match to our experimental conditions, and only considered the last 7 subjects that heard both the foregrounds and the backgrounds in isolation, in addition to the mixtures.”</p><p>It was a mistake to mention that it was all female, as the original dataset has 3 females and 8 males, of which we used 7 without any indication of their sex. Thus, we removed this mention from the text.</p><disp-quote content-type="editor-comment"><p>In the &quot;Statistical testing&quot; section, why were some tests done with 1000 permutations/shuffles while others were done with 2000?</p></disp-quote><p>We homogenized and used 1000 permutations/shuffles for all statistical tests.</p><disp-quote content-type="editor-comment"><p>(4) Miscellany:</p><p>(a) The Hamersky et al. 2023 preprint has recently been published (referenced in the public review), and so you could consider updating the reference.</p></disp-quote><p>This reference has now been updated.</p><disp-quote content-type="editor-comment"><p>(b) There are a few borderline statistical tests that could use a bit more nuance. For example (on page 4), &quot;In primary auditory cortex (MEG), there was no significant difference between values of foreground invariance and background invariance (p = 0.063, obtained by randomly permuting the sounds' background and foreground labels, 1000 times).&quot; This test is quite close to being significant, and this might be acknowledged.</p></disp-quote><p>We emphasized the trend to nuance the interpretation of these results: “In primary auditory cortex (MEG), foreground invariance was slightly lower than background invariance, although this difference was not significant (p=0.063, obtained by randomly permuting the sounds' background and foreground labels, 1000 times).”</p><disp-quote content-type="editor-comment"><p>(5) Potential typos:</p><p>(a) Should the title be &quot;natural sound mixtures&quot; instead of &quot;natural sounds mixtures&quot;?</p><p>(b) The caption for Figure 1 says &quot;We imaged the whole auditory through successive slices across several days.&quot; I believe this should the &quot;the whole auditory [cortex].&quot; (c) In the first paragraph of the discussion, there is a sentence ending in &quot;...are segregated in hemody-namic signal.&quot; I believe this should be &quot;hemody-namic signal.&quot;</p></disp-quote><p>These errors are now all corrected.</p></body></sub-article></article>