<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81679</article-id><article-id pub-id-type="doi">10.7554/eLife.81679</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Uncertainty alters the balance between incremental learning and episodic memory</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-285539"><name><surname>Nicholas</surname><given-names>Jonathan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2314-0765</contrib-id><email>jonathan.nicholas@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-41719"><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5029-1430</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-10220"><name><surname>Shohamy</surname><given-names>Daphna</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Psychology, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Mortimer B. Zuckerman Mind, Brain, Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hx57361</institution-id><institution>Department of Psychology, Princeton University</institution></institution-wrap><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hx57361</institution-id><institution>Princeton Neuroscience Institute, Princeton University</institution></institution-wrap><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>The Kavli Institute for Brain Science, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>02</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e81679</elocation-id><history><date date-type="received" iso-8601-date="2022-07-07"><day>07</day><month>07</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-12-01"><day>01</day><month>12</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-07-06"><day>06</day><month>07</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.07.05.498877"/></event></pub-history><permissions><copyright-statement>Â© 2022, Nicholas et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Nicholas et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81679-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-81679-figures-v2.pdf"/><abstract><p>A key question in decision-making is how humans arbitrate between competing learning and memory systems to maximize reward. We address this question by probing the balance between the effects, on choice, of incremental trial-and-error learning versus episodic memories of individual events. Although a rich literature has studied incremental learning in isolation, the role of episodic memory in decision-making has only recently drawn focus, and little research disentangles their separate contributions. We hypothesized that the brain arbitrates rationally between these two systems, relying on each in circumstances to which it is most suited, as indicated by uncertainty. We tested this hypothesis by directly contrasting contributions of episodic and incremental influence to decisions, while manipulating the relative uncertainty of incremental learning using a well-established manipulation of reward volatility. Across two large, independent samples of young adults, participants traded these influences off rationally, depending more on episodic information when incremental summaries were more uncertain. These results support the proposal that the brain optimizes the balance between different forms of learning and memory according to their relative uncertainties and elucidate the circumstances under which episodic memory informs decisions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>episodic memory</kwd><kwd>reinforcement learning</kwd><kwd>uncertainty</kwd><kwd>volatility</kwd><kwd>incremental learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1644869</award-id><principal-award-recipient><name><surname>Nicholas</surname><given-names>Jonathan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1822619</award-id><principal-award-recipient><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name><name><surname>Shohamy</surname><given-names>Daphna</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH121093</award-id><principal-award-recipient><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name><name><surname>Shohamy</surname><given-names>Daphna</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000925</institution-id><institution>John Templeton Foundation</institution></institution-wrap></funding-source><award-id>60844</award-id><principal-award-recipient><name><surname>Shohamy</surname><given-names>Daphna</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>People use uncertainty for effective arbitration between episodic memory and incremental learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Effective decision-making depends on using memories of past experiences to inform choices in the present. This process has been extensively studied using models of learning from trial-and-error, many of which rely on error-driven learning rules that in effect summarize experiences using a running average (<xref ref-type="bibr" rid="bib76">Sutton and Barto, 1998</xref>; <xref ref-type="bibr" rid="bib64">Rescorla and Wagner, 1972</xref>; <xref ref-type="bibr" rid="bib35">Houk et al., 1995</xref>). This sort of <italic>incremental learning</italic> provides a simple mechanism for evaluating actions without maintaining memory traces of each individual experience along the way and has rich links to conditioning behavior and putative neural mechanisms for error-driven learning (<xref ref-type="bibr" rid="bib73">Schultz et al., 1997</xref>). However, recent findings indicate that decisions may also be guided by the retrieval of individual events, a process often assumed to be supported by <italic>episodic memory</italic> (<xref ref-type="bibr" rid="bib3">Bakkour et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Plonsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Mason et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Bornstein et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib10">Bornstein and Norman, 2017</xref>; <xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Duncan and Shohamy, 2016</xref>; <xref ref-type="bibr" rid="bib45">Lee et al., 2015</xref>; <xref ref-type="bibr" rid="bib83">Wimmer and BÃ¼chel, 2020</xref>). Although theoretical work has suggested a role for episodic memory in initial task acquisition, when experience is sparse (<xref ref-type="bibr" rid="bib28">Gershman and Daw, 2017</xref>; <xref ref-type="bibr" rid="bib46">Lengyel and Dayan, 2007</xref>), the use of episodes may be much more pervasive as its influence has been detected empirically even in decision tasks that are well-trained and can be solved normatively using incremental learning alone (<xref ref-type="bibr" rid="bib60">Plonsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Bornstein et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Bornstein and Norman, 2017</xref>). The apparent ubiquity of episodic memory as a substrate for decision-making raises questions about the circumstances under which it is recruited and the implications for behavior.</p><p>How and when episodic memory is used for decisions relates to a more general challenge in cognitive control: understanding how the brain balances competing systems for decision-making. An overarching hypothesis is that the brain judiciously adopts different decision strategies in circumstances for which they are most suited; for example, by determining which system is likely to produce the most rewarding choices at the least cost. This general idea has been invoked to explain how the brain arbitrates between deliberative versus habitual decisions and previous work has suggested a key role for uncertainty in achieving a balance that maximizes reward (<xref ref-type="bibr" rid="bib17">Daw et al., 2005</xref>; <xref ref-type="bibr" rid="bib44">Lee et al., 2014</xref>). Moreover, imbalances in arbitration have been implicated in dysfunction such as compulsion (<xref ref-type="bibr" rid="bib29">Gillan et al., 2011</xref>; <xref ref-type="bibr" rid="bib81">Voon et al., 2015</xref>), addiction (<xref ref-type="bibr" rid="bib25">Ersche et al., 2016</xref>; <xref ref-type="bibr" rid="bib26">Everitt and Robbins, 2005</xref>), and rumination (<xref ref-type="bibr" rid="bib37">Hunter et al., 2022</xref>; <xref ref-type="bibr" rid="bib18">Dayan and Huys, 2008</xref>; <xref ref-type="bibr" rid="bib38">Huys et al., 2012</xref>).</p><p>Here, we hypothesized that uncertainty is used for effective arbitration between decision systems and tested this hypothesis by investigating the tradeoff between incremental learning and episodic memory. This is a particularly favorable setting in which to examine this hypothesis due to a rich prior literature theoretically analyzing, and experimentally manipulating, the efficacy of incremental learning in isolation. Studies of this sort typically manipulate the volatility, or frequency of change, of the environment, as a way of affecting uncertainty about incrementally learned quantities. In line with predictions made by statistical learning models, these experiments demonstrate that when the reward associated with an action is more volatile, people adapt by increasing their incremental learning rates (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Mathys et al., 2011</xref>; <xref ref-type="bibr" rid="bib55">OâReilly, 2013</xref>; <xref ref-type="bibr" rid="bib54">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Piray and Daw, 2020</xref>; <xref ref-type="bibr" rid="bib39">Kakade and Dayan, 2002</xref>; <xref ref-type="bibr" rid="bib87">Yu and Dayan, 2005</xref>). In this case, incrementally constructed estimates reflect a running average over fewer experiences, yielding both less accurate and more uncertain estimates of expected reward. We, therefore, reasoned that the benefits of incremental learning are most pronounced when incremental estimation can leverage many experiences or, in other words, when volatility is low. By contrast, when the environment is either changing frequently or has recently changed, estimating reward episodically by retrieving a single, well-matched experience should be relatively more favorable.</p><p>We tested this hypothesis using a choice task that directly pits these decision systems against one another (<xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>), while manipulating volatility. In particular, we (i) independently measured the contributions of episodic memory vs. incremental learning to choice and (ii) altered the uncertainty about incremental estimates using different levels of volatility. Two large online samples of healthy young adults completed three tasks. Results from the primary sample (n = 254) are reported in the main text; results from a replication sample (n = 223) are reported in the appendices (Appendix 1).</p><p>The main task of interest combined incremental learning and episodic memory, referred to throughout as the <italic>deck learning and card memory</italic> task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, middle panel). On each trial of this task, participants chose between two cards of a different color and received feedback following their choice. The cards appeared on each trial throughout the task, but their relative value changed over time (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In addition to the color of the card, each card also displayed an object. Critically, objects appeared on a card at most twice throughout the task, such that a chosen object could reappear between 9 and 30 trials after it was chosen the first time, and would deliver the same reward. Thus, participants could make decisions based on incremental learning of the average value of the decks or based on episodic memory for the specific value of an object which they only saw once before. Additionally, participants made choices across two environments: a <italic>high-volatility</italic> and a <italic>low-volatility</italic> environment. The environments differed in how often reversals in deck value occurred.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Study design and sample events.</title><p>(<bold>A</bold>) Participants completed three tasks in succession. The first was the <italic>deck learning</italic> task that consisted of choosing between two colored cards and receiving an outcome following each choice. One color was worth more on average at any given timepoint, and this mapping changed periodically. Second was the main task of interest, the <italic>deck learning and card memory</italic> task, which followed the same structure as the deck learning task but each card also displayed a trial-unique object. Cards that were chosen could appear a second time in the task after 9â30 trials and, if they reappeared, were worth the same amount, thereby allowing participants to use episodic memory for individual cards in addition to learning deck value from feedback. Outcomes ranged from $0 to $1 in increments of 20Â¢ in both of these tasks. Lastly, participants completed a <italic>subsequent memory</italic> task for objects that may have been seen in the deck learning and card memory task. Participants had to indicate whether they recognized an object and, if they did, whether they chose that object. If they responded that they had chosen the object, they were then asked if they remembered the value of that object. (<bold>B</bold>) Uncertainty manipulation within and across environments. Uncertainty was manipulated by varying the volatility of the relationship between cue and reward over time. Participants completed the task in two counterbalanced environments that differed in their relative volatility. The low-volatility environment featured half as many reversals in deck luckiness as the high-volatility environment. <italic>Top:</italic> the true value of the purple deck is drawn in gray for an example trial sequence. In purple and orange are estimated deck values from the reduced Bayesian model (<xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>). Trials featuring objects appeared only in the deck learning and card memory task. <italic>Bottom:</italic> uncertainty about deck value as estimated by the model is shown in gray. This plot shows relative uncertainty, which is the modelâs imprecision in its estimate of deck value.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig1-v2.tif"/></fig><p>In addition to the main task, participants also completed two other tasks in the experiment. First, participants completed a simple <italic>deck learning</italic> task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, left panel) to acclimate them to each environment and quantify the effects of uncertainty. This task included choices between two diamonds of a different color on each trial, without any trial-unique objects. Second, after the main task, participants completed a standard <italic>subsequent memory</italic> task (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right panel) designed to assess later episodic memory for objects encountered in the main task.</p><p>We predicted that greater uncertainty about incremental values would be related to increased use of episodic memory. The experimental design provided two opportunities to measure the impact of uncertainty: <italic>across</italic> conditions, by comparing between the high- and the low-volatility environments, and <italic>within</italic> condition, by examining how learning and choices were impacted by each reversal.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Episodic memory is used more under conditions of greater volatility</title><p>As noted above, participants completed two decision-making tasks. The <italic>deck learning</italic> task familiarized them with the underlying incremental learning task and established an independent measure of sensitivity to the volatility manipulation. The separate <italic>deck learning and card memory</italic> task measured the additional influence of episodic memory on decisions (<xref ref-type="fig" rid="fig1">Figure 1</xref>). In the deck learning task, participants chose between two decks with expected value (<inline-formula><mml:math id="inf1"><mml:mi>V</mml:mi></mml:math></inline-formula>) that reversed periodically across two environments, with one more volatile (reversals every 10 trials on average) and the other less volatile (reversals every 20 trials on average).</p><p>Participants were told that at any point in the experiment one of the two decks was âlucky,â meaning that its expected value (<inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> = 63Â¢) was higher than the other âunluckyâ deck (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> = 37Â¢). They were also told that which deck was currently lucky could reverse at any time, and that they would be completing the task in two environments that differed in how often these reversals occurred. We reasoned that, following each reversal, participants should be more uncertain about deck value and that this uncertainty should reduce with experience. Because the more volatile environment featured more reversals, in this condition subjects should have greater uncertainty about the deck value overall.</p><p>In the second deck learning and card memory task, each deck featured cards with trial-unique objects that could reappear once after being chosen and were worth an identical amount at each appearance. Here, participants were told that they could use their memory for the value of objects they recognized to guide their choices. They were also told that the relative level of volatility in each environment during the card learning task would be identical in this task. We predicted that decisions would be based more on object value when deck value was more volatile. Our logic was that episodic memory should be relied upon more strongly when incremental learning is less accurate and reliable due to frequent change. This, in turn, is because episodic memory is itself imperfect in practice, so participants face a nontrivial tradeoff between attempting episodic recall vs. relying on incremental learning when an object recurs. We, therefore, expected choices to be more reliant on episodic memory in the high- compared to the low-volatility environment.</p><p>We first examined whether participants were separately sensitive to each source of value in the deck learning and card memory task: the value of the objects (episodic) and of the decks (incremental). Controlling for average deck value, we found that participants used episodic memory for object value, evidenced by a greater tendency to choose high-valued old objects than low-valued old objects (<inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.621</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.527</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.713</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Likewise, controlling for object value, we also found that participants used incrementally learned value for the decks, evidenced by the fact that the higher-valued (lucky) deck was chosen more frequently on trials immediately preceding a reversal (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.038</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.038</mml:mn><mml:mo>,</mml:mo><mml:mn>0.113</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.056</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.02</mml:mn><mml:mo>,</mml:mo><mml:mn>0.134</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.088</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.009</mml:mn><mml:mo>,</mml:mo><mml:mn>0.166</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.136</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.052</mml:mn><mml:mo>,</mml:mo><mml:mn>0.219</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2B</xref>), that this tendency was disrupted by the reversals (<inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.382</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.465</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.296</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>), and by the quick recovery of performance on the trials following a reversal (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.175</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.258</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.095</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.106</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.18</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.029</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.084</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mn>0.158</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>â</mml:mo><mml:mn>0.006</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.129</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.071</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.184</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Evaluating the proportion of incremental and episodic choices.</title><p>(<bold>A</bold>) Participantsâ (n = 254) choices demonstrate sensitivity to the value of old objects. Group-level averages are shown as points and lines represent 95% confidence intervals. (<bold>B</bold>) Reversals in deck luckiness altered choice such that the currently lucky deck was chosen less following a reversal. The line represents the group-level average, and the band represents the 95% confidence interval. (<bold>C</bold>) On incongruent trials, choices were more likely to be based on episodic memory (e.g., high-valued objects chosen and low-valued objects avoided) in the high- compared to the low-volatility environment. Averages for individual subjects are shown as points, and lines represent the group-level average with a 95% confidence interval. (<bold>D</bold>) Median reaction time was longer for incongruent choices based on episodic memory compared to those based on incremental learning.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Recreation of <xref ref-type="fig" rid="fig2">Figure 2</xref> in the main text using the replication dataset.</title><p>(<bold>A</bold>) Participantsâ (n = 223) choices demonstrate sensitivity to the value of old objects. (<bold>B</bold>) Reversals in deck luckiness altered choice such that the currently lucky deck was chosen less following a reversal. (<bold>C</bold>) On incongruent trials, choices were more likely to be based on episodic memory in the high- compared to the low-volatility environment. (<bold>D</bold>) Reaction time was longer for incongruent choices based on episodic memory compared to those based on incremental learning.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig2-figsupp1-v2.tif"/></fig></fig-group><p>Having established that both episodic memory and incremental learning guided choices, we next sought to determine the impact of volatility on episodic memory for object value by isolating trials on which episodic memory was most likely to be used. To identify reliance on object value, we first focused on trials where the two sources of value information were incongruent: that is, trials for which the high-value deck featured an old object that was of low value (&lt;50Â¢) or the low-value deck featured an old object that was of high value (&gt;50Â¢). We then defined an <italic>episodic-based choice index</italic> (EBCI) by considering a choice as episodic if the old object was, in the first case, avoided or, in the second case, chosen. Consistent with our hypothesis, we found greater evidence for episodic choices in the high-volatility environment compared to the low-volatility environment (<inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.092</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.018</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mn>0.164</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2C</xref>). Finally, this analysis also gave us the opportunity to test differences in reaction time between incremental and episodic decisions. Decisions based on episodic value took longer (<inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>37.629</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>28.488</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>46.585</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2D</xref>), perhaps reflecting that episodic retrieval may take more time than retrieval of cached incremental value.</p></sec><sec id="s2-2"><title>Uncertainty about incremental values increases sensitivity to episodic value</title><p>The effects of environment described above provide a coarse index of overall differences in learning across conditions. To capture uncertainty about deck value on a trial-by-trial basis, we adopted a computational model that tracks uncertainty during learning. We then used this model to test our central hypothesis: that episodic memory is used more when posterior uncertainty about deck value is high. Our reasoning was that episodic memory should not only be deployed more when incremental learning is overall inaccurate due to frequent change, but also within either condition following recent change. We, therefore, predicted that, across both environments, participants would be more likely to recruit episodic memory following reversals in deck value, when uncertainty is at its highest.</p><p>We began by hierarchically fitting two classes of incremental learning models to the behavior on the deck learning task: a baseline model with a RescorlaâWagner (<xref ref-type="bibr" rid="bib64">Rescorla and Wagner, 1972</xref>) style update (RW) and a reduced Bayesian model (<xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>) (RB) that augments the RW learner with a variable learning rate, which it modulates by tracking ongoing uncertainty about deck value. This approach â which builds on a line of work applying Bayesian learning models to capture trial-by-trial modulation in uncertainty and learning rates in volatile environments (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Mathys et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="bib58">Piray and Daw, 2020</xref>; <xref ref-type="bibr" rid="bib39">Kakade and Dayan, 2002</xref>; <xref ref-type="bibr" rid="bib87">Yu and Dayan, 2005</xref>) â allowed us to first assess incremental learning free of any contamination due to competition with episodic memory. We then used the parameters fit to this task for each participant to generate estimates of subjective deck value and uncertainty around deck value, out of sample, in the deck learning and card memory task. These estimates were then used alongside episodic value to predict choices on incongruent trials in the deck learning and card memory task.</p><p>We first tested whether participants adjusted their rates of learning in response to uncertainty, both between environments and due to trial-wise fluctuations in uncertainty about deck value. We did this by comparing the ability of each combined choice model to predict participantsâ decisions out of sample. To test for effects between environments, we compared models that controlled learning with either a single free parameter (for RW, a learning rate <inline-formula><mml:math id="inf20"><mml:mi>Î±</mml:mi></mml:math></inline-formula>; for RB, a hazard rate <italic>H</italic> capturing the expected frequency of reversals) shared across both environments or models with a separate free parameter for each environment. To test for trial-wise effects within environments, we compared between RB and RW models: while RW updates deck value with a constant learning rate, RB tracks ongoing posterior uncertainty about deck value (called relative uncertainty, RU) and increases its learning rate when this quantity is high.</p><p>We also included two other models in our comparison to control for alternative learning strategies. The first was a contextual inference model (CI), which modeled deck value as arising from two switching contexts (either that one deck was lucky and the other unlucky or vice versa) rather than from incremental learning. The second was a RescorlaâWagner model that, like the RB model but unlike the RW models described above, learned only a single-value estimate (RW1Q). The details for all models can be found in Appendix 3.</p><p>Participants were sensitive to the volatility manipulation and also incorporated uncertainty into updating their beliefs about deck value. This is indicated by the fact that the RB combined choice model that included a separate hazard rate for each environment (RB2<italic>H</italic>) outperformed both RW models, the RB model with a single hazard rate, as well as other alternative learning models (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Further, across the entire sample, participants detected higher levels of volatility in the high-volatility environment, as indicated by the generally larger hazard rates recovered from this model in the high- compared to the low-volatility environment (<inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.033</mml:mn><mml:mo>,</mml:mo><mml:mn>0.048</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.081</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.067</mml:mn><mml:mo>,</mml:mo><mml:mn>0.097</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig3">Figure 3B</xref>). Next, we examined the modelâs ability to estimate uncertainty as a function of reversals in deck luckiness. Compared to an average of the four trials prior to a reversal, RU increased immediately following a reversal and stabilized over time (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.014</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.019</mml:mn><mml:mo>,</mml:mo><mml:mn>0.048</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.242</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.276</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.209</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.145</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.178</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.112</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.131</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.07</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.079</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.108</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.048</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig3">Figure 3C</xref>). As expected, RU was also, on average, greater in the high- compared to the low-volatility environment (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.015</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.012</mml:mn><mml:mo>,</mml:mo><mml:mn>0.018</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>). Lastly, we were interested in assessing the relationship between reaction time and RU as we expected that higher uncertainty may be reflected in more time needed to resolve decisions. In line with this idea, RU was strongly related to reaction time such that choices made under more uncertain conditions took longer (<inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.685</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.823</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>2.528</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Evaluating model fit and sensitivity to volatility.</title><p>(<bold>A</bold>) Expected log pointwise predictive density (ELPD) from each model was calculated from a 20-fold leave-N-subjects-out cross-validation procedure and is shown here subtracted from the best-fitting model. The best-fitting model was the reduced Bayesian (RB) model with two hazard rates (2H) and sensitivity to the interaction between old object value and relative uncertainty (RU) in the choice function. Error bars represent standard error around ELPD estimates. (<bold>B</bold>) Participants (n = 254) were sensitive to the relative level of volatility in each environment as measured by the hazard rate. Group-level parameters are superimposed on individual subject parameters. Error bars represent 95% posterior intervals. The true hazard rate for each environment is shown on the interior of the plot. (<bold>C</bold>) RU peaks on the trial following a reversal and is greater in the high- compared to the low-volatility environment. Lines represent group means, and bands represent 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Recreation of <xref ref-type="fig" rid="fig3">Figure 3</xref> in the main text using the replication dataset.</title><p>(<bold>A</bold>) The best-fitting model was again the reduced Bayesian (RB) model with two hazard rates (2H) and sensitivity to the interaction between old object value and relative uncertainty (RU) in the choice function. (<bold>B</bold>) Participants (n = 223) were affected by the relative level of volatility in each environment as measured by the hazard rate. Group-level parameters are superimposed on individual subject parameters. (<bold>C</bold>) RU peaks on the trial following a reversal and is greater in the high- compared to the low-volatility environment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig3-figsupp1-v2.tif"/></fig></fig-group><p>Having established that participants were affected by uncertainty around beliefs about deck value, we turned to examine our primary question: whether this uncertainty alters the use of episodic memory in choices. We first examined effects of RU on the episodic choice index, which measures choices consistent with episodic value on trials when it disagrees with incremental learning. This analysis verified that episodic memory was used more on incongruent trial decisions made under conditions of high RU (<inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.133</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>3.535</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). To more directly test the prediction that participants would use episodic memory when uncertainty is high, we included trial-by-trial estimates of RU in the RB2<italic>H</italic> combined choice model, which was augmented with an additional free parameter to capture any change with RU in the effect of episodic value on choice. Formally, this parameter measured an effect of the interaction between these two factors, and the more positive this term the greater the impact of increased uncertainty on the use of episodic memory. This new combined choice model further improved out-of-sample predictions (RB2<italic>H</italic>+RU, <xref ref-type="fig" rid="fig3">Figure 3A</xref>). As predicted, while both incremental and episodic value were used overall (<inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.502</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.428</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.583</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.150</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.101</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mn>0.20</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>), episodic value impacted choices more when RU was high (<inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.067</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.026</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.11</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4">Figure 4B</xref>) and more generally in the high- compared to the low-volatility environment (<inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.06</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.02</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.1</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>). This is consistent with the hypothesis that episodic value was relied on more when beliefs about incremental value were uncertain.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Evaluating effects of sensitivity to uncertainty on episodic choices.</title><p>(<bold>A</bold>) Participantsâ (n = 254) degree of episodic-based choice increased with greater relative uncertainty (RU) as predicted by the combined choice model. Points are group means, and error bars are 95% confidence intervals. (<bold>B</bold>) Estimates from the combined choice model. Participants were biased to choose previously seen objects regardless of their value and were additionally sensitive to their value. As hypothesized, this sensitivity was increased when RU was higher, as well as in the high- compared to the low-volatility environment. There was no bias to choose one deck color over the other, and participants were highly sensitive to estimated deck value. Group-level parameters are superimposed as bars on individual subject parameters represented as points. Error bars represent 95% posterior intervals around group-level parameters. Estimates are shown in standard units.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Recreation of <xref ref-type="fig" rid="fig4">Figure 4</xref> in the main text using the replication dataset.</title><p>(<bold>A</bold>) Participantsâ (n = 223) degree of episodic-based choice increases with greater relative uncertainty (RU). (<bold>B</bold>) Estimates from the combined choice model. Participants were biased to choose previously seen objects regardless of their value and were additionally sensitive to their value. As hypothesized, this sensitivity was increased when RU was higher as well as in the high- compared to the low-volatility environment. There was no bias to choose one deck color over the other, and participants were highly sensitive to estimated deck value.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>Results of relative uncertainty (RU) at encoding time on episodic-based choice in the main (<bold>AâC</bold>) and replication (<bold>DâF</bold>) sample.</title><p>(<bold>A</bold>) There was no relationship between RU at encoding and the degree to which participants (n = 254) based decisions on episodic value. (<bold>B</bold>) Estimates from the combined choice model including both effects of RU at retrieval time and RU at encoding time. Relative to the effect of the interaction between RU at retrieval time and old object value, the equivalent effect for RU at encoding time was small in the main sample. (<bold>C</bold>) Expected log pointwise predictive density for the combined choice model including only an effect of the interaction between RU at retrieval time and old object value (presented in the text) and the model also including the interaction between RU at encoding time and old object value. Including RU at encoding time did not improve model performance. (<bold>D</bold>) There was again no relationship between RU at encoding and episodic-based choice in the replication sample (n = 223). (<bold>E</bold>) In the replication sample, there was no effect of the interaction between RU at encoding and old object value on choice behavior. (<bold>F</bold>) Including RU at encoding time again did not improve model performance in the replication sample.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig4-figsupp2-v2.tif"/></fig></fig-group><p>The analyses above focus on uncertainty present at the time of retrieving episodic value because this is what we hypothesized would drive competition in the reliance on either system at choice time. However, in principle, reward uncertainty at the time an object is first encountered might also affect its encoding, and hence its subsequent use in episodic choice when later retrieved (<xref ref-type="bibr" rid="bib66">Rouhani et al., 2018</xref>). To address this possibility, we looked at the impact of RU resulting from the first time an old objectâs value was revealed on whether that object was later retrieved for a decision. Using our EBCI, there was no relationship between the use of episodic memory on incongruent trial decisions and RU at encoding (<inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.622</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.832</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>2.044</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2A</xref>). Similarly, we also examined effects of trial-by-trial estimates of RU at encoding time in the combined choice model by adding another free parameter that captured change with RU at encoding time in the effect of episodic value on choice. This parameter was added alongside the effect of RU at retrieval time (from the previous analysis). There was no effect on choice in either sample (main: <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.028</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.011</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.067</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; replication: <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.003</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.046</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.037</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> ; <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2B</xref>) and the inclusion of this parameter did not provide a better fit to subjectsâ choices than the combined choice model with only increased sensitivity due to RU at retrieval time (<xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2C</xref>).</p></sec><sec id="s2-3"><title>Episodic and incremental value sensitivity predicts subsequent memory performance</title><p>Having determined that decisions depended on episodic memory more when uncertainty about incremental value was higher, we next sought evidence for similar effects on the quality of episodic memory. Episodic memory is, of course, imperfect, and value estimates derived from episodic memory are therefore also uncertain. More uncertain episodic memory should then be disfavored while the influence of incremental value on choice is promoted instead. Although in this study we did not experimentally manipulate the strength of episodic memory, as our volatility manipulation was designed to affect the uncertainty of incremental estimates, we did measure memory strength in a subsequent memory test. Thus, we predicted that participants who base fewer decisions on object value and more decisions on deck value should have poorer subsequent memory for objects from the deck learning and card memory task.</p><p>We first assessed subsequent memory performance. Participantsâ recognition memory was well above chance (<inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.887</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1.782</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>1.989</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>), indicating a general ability to discriminate objects seen in the main task from those that were new. Recall for the value of previously seen objects was also well predicted by their true value (<inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.174</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.160</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.188</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>), providing further support that episodic memory was used to encode object value. To underscore this point, we sorted subsequent memory trials according to whether an object was seen on an episodic- or incremental-based choice, as estimated according to our EBCI, during the deck learning and card memory task. Not only were objects from episodic-based choices better remembered than those from incremental-based choices (<inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.192</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.072</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.322</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5">Figure 5A</xref>), but value recall was also improved for these objects (<inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>:</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.047</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.030</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.065</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Relationship between choice type and subsequent memory.</title><p>(<bold>A</bold>) Objects originally seen during episodic-based choices were better remembered than objects seen during incremental-based choices. Average hit rates for individual subjects (n = 254) are shown as points, bars represent the group-level average, and lines represent 95% confidence intervals. (<bold>B</bold>) The value of objects originally seen during episodic-based choices was better recalled than objects seen during incremental-based choices. Points represent average value memory for each possible object value, and error bars represent 95% confidence intervals. Lines are linear fits, and bands are 95% confidence intervals. (<bold>C</bold>) Participants with greater sensitivity to episodic value as measured by random effects in the combined choice model tended to better remember objects seen originally in the card learning and deck memory task. (<bold>D</bold>) Participants with greater sensitivity to incremental value tended to have worse memory for objects from the card learning and deck memory task. Points represent individual participants, lines are linear fits, and bands are 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Recreation of <xref ref-type="fig" rid="fig5">Figure 5</xref> in the main text using the replication dataset.</title><p>(<bold>A</bold>) Objects seen during episodic-based choices were better remembered than objects seen during incremental-based choices. (<bold>B</bold>) The value of objects originally seen during episodic-based choices was better recalled than objects seen during incremental-based choices. (<bold>C</bold>) Participants (n = 223) with greater sensitivity to episodic value tended to better remember objects from the deck learning and card memory task. (<bold>D</bold>) Participants with greater sensitivity to incremental value tended to have worse memory for objects from the card learning and deck memory task.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Effects of relative uncertainty (RU), changepoint probability (CPP), and absolute prediction error (APE) at encoding time on subsequent recognition and value memory in both the main and replication samples.</title><p>(<bold>A</bold>) Relationship between RU, CPP, and APE at encoding on recognition memory in the main sample (n = 254). RU outperformed both of these variables at predicting recognition memory (right). (<bold>B</bold>) Relationship between RU, CPP, and APE at encoding on value memory in the main sample. RU outperformed both of these variables at predicting value memory (right). (<bold>C</bold>) Relationship between RU, CPP, and APE at encoding on recognition memory in the replication sample (n = 223). Neither of these variables performed better than RU at predicting recognition memory (right). (<bold>D</bold>) Relationship between RU, CPP, and APE at encoding on value memory in the main sample. RU outperformed both of these variables at predicting value memory (right). Note that here value memory is visualized as a hit rate (where 1 is accurate value memory and 0 is inaccurate value memory) in order to show x-axis variables on a continuous scale, but analyses were conducted using the true value responses. Points are group means binned by x-axis variables at encoding, and error bars are 95% confidence intervals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81679-fig5-figsupp2-v2.tif"/></fig></fig-group><p>We next leveraged the finer-grained estimates of sensitivity to episodic value from the learning model to ask whether, across participants, individuals who were estimated to deploy episodic value more during the deck learning and card memory task also performed better on the subsequent memory test. In line with the idea that episodic memory quality also impacts the relationship between incremental learning and episodic memory, participants with better subsequent recognition memory were more sensitive to episodic value (<inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.373</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.273</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.478</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5">Figure 5C</xref>), and these same participants were less sensitive to incremental value (<inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.276</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.383</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.17</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5">Figure 5D</xref>). This result provides further evidence for a tradeoff between episodic memory and incremental learning. It also provides preliminary support for a broader version of our hypothesis, which is that uncertainty about value provided by either memory system arbitrates the balance between them.</p><p>Lastly, the subsequent memory task also provided us with the opportunity to replicate other studies that have found that prediction error and its related quantities enhance episodic memory across a variety of tasks and paradigms (<xref ref-type="bibr" rid="bib66">Rouhani et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Rouhani and Niv, 2021</xref>; <xref ref-type="bibr" rid="bib1">Antony et al., 2021</xref>; <xref ref-type="bibr" rid="bib6">Ben-Yakov et al., 2022</xref>). We predicted that participants should have better subsequent memory for objects encoded under conditions of greater uncertainty. While not our primary focus, we found support for this prediction across both samples (see Appendix 2, <xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>).</p></sec><sec id="s2-4"><title>Replication of the main results in a separate sample</title><p>We repeated the tasks described above in an independent online sample of healthy young adults (n = 223) to test the replicability and robustness of our findings. We replicated all effects of environment and RU on episodic-based choice and subsequent memory (see Appendix 1 and figure supplements for details).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Research on learning and value-based decision-making has focused on how the brain summarizes experiences by error-driven incremental learning rules that, in effect, maintain the running average of many experiences. While recent work has demonstrated that episodic memory also contributes to value-based decisions (<xref ref-type="bibr" rid="bib3">Bakkour et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Plonsky et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Mason et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Bornstein et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib10">Bornstein and Norman, 2017</xref>; <xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Duncan and Shohamy, 2016</xref>; <xref ref-type="bibr" rid="bib45">Lee et al., 2015</xref>; <xref ref-type="bibr" rid="bib83">Wimmer and BÃ¼chel, 2020</xref>), many open questions remain about the circumstances under which episodic memory is used. We used a task that directly contrasts episodic and incremental influences on decisions and found that participants traded these influences off rationally, relying more on episodic information when incremental summaries were less reliable, that is, more uncertain and based on fewer experiences. We also found evidence for a complementary modulation of this episodic-incremental balance by episodic memory quality, suggesting that more uncertain episodic-derived estimates may reduce reliance on episodic value. Together, these results indicate that reward uncertainty modulates the use of episodic memory in decisions, suggesting that the brain optimizes the balance between different forms of learning according to volatility in the environment.</p><p>Our findings add empirical data to previous theoretical and computational work, which has suggested that decision-making can greatly benefit from episodic memory for individual estimates when available data are sparse. This most obviously arises early in learning a new task, but also in task transfer, high-dimensional or non-Markovian environments, and (as demonstrated in this work) during conditions of rapid change (<xref ref-type="bibr" rid="bib46">Lengyel and Dayan, 2007</xref>; <xref ref-type="bibr" rid="bib8">Blundell, 2016</xref>; <xref ref-type="bibr" rid="bib69">Santoro et al., 2016</xref>). We investigate these theoretical predictions in the context of human decision-making, testing whether humans rely more heavily on episodic memory when incremental summaries comprising multiple experiences are relatively poor. We operationalize this tradeoff in terms of uncertainty, exemplifying a more general statistical scheme for arbitrating between different decision systems by treating them as estimators of action value.</p><p>There is precedent for this type of uncertainty-based arbitration in the brain, with the most well-known being the tradeoff between model-free learning and model-based learning (<xref ref-type="bibr" rid="bib17">Daw et al., 2005</xref>; <xref ref-type="bibr" rid="bib41">Keramati et al., 2011</xref>). Control over decision-making by model-free and model-based systems has been found to shift in accordance with the accuracy of their respective predictions (<xref ref-type="bibr" rid="bib44">Lee et al., 2014</xref>), and humans adjust their reliance on either system in response to external conditions that provide a relative advantage to one over the other (<xref ref-type="bibr" rid="bib74">Simon and Daw, 2011</xref>; <xref ref-type="bibr" rid="bib43">Kool et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Otto et al., 2013</xref>). Tracking uncertainty provides useful information about when inaccuracy is expected and helps to maximize utility by deploying whichever system is best at a given time. Our results add to these findings and expand their principles to include episodic memory in this tradeoff. This may be especially important given that human memory is resource limited and prone to distortion (<xref ref-type="bibr" rid="bib70">Schacter et al., 2011</xref>) and forgetting (<xref ref-type="bibr" rid="bib24">Ebbinghaus, 2013</xref>). Notably, in our task, an observer equipped with perfect episodic memory would always benefit from using it to make decisions. Yet, as our findings show, participants vary in their episodic memory abilities, and this memory capacity is related to the extent to which episodic memory is used to guide decisions.</p><p>One intriguing possibility is that there is more than just an analogy between the incremental-episodic balance studied here and previous work on model-free versus model-based competition. Incremental error-driven learning coincides closely with model-free learning in other settings (<xref ref-type="bibr" rid="bib73">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib17">Daw et al., 2005</xref>) and, although it has been proposed that episodic control constitutes a âthird wayâ (<xref ref-type="bibr" rid="bib46">Lengyel and Dayan, 2007</xref>), it is possible that behavioral signatures of model-based learning might instead arise from episodic control via covert retrieval of individual episodes (<xref ref-type="bibr" rid="bib28">Gershman and Daw, 2017</xref>; <xref ref-type="bibr" rid="bib32">Hassabis and Maguire, 2009</xref>; <xref ref-type="bibr" rid="bib71">Schacter et al., 2012</xref>; <xref ref-type="bibr" rid="bib80">Vikbladh et al., 2017</xref>), which contain much of the same information as a cognitive map or world model. While this study assesses single-event episodic retrieval more overtly, an open question for future work is whether the extent to which these same processes, and ultimately the same episodic-incremental tradeoff, might also explain model-based choice as it has been operationalized in other decision tasks. A related line of work has emphasized a similar role for working memory in maintaining representations of individual trials for choice (<xref ref-type="bibr" rid="bib14">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib85">Yoo and Collins, 2022</xref>; <xref ref-type="bibr" rid="bib15">Collins, 2018</xref>; <xref ref-type="bibr" rid="bib16">Collins and Frank, 2018</xref>). Given the capacity constraints of working memory, we think it unlikely that working memory can account for the effects shown here, which involve memory for dozens of trial-unique stimuli maintained over tens of trials.</p><p>Our findings also help clarify the impacts of uncertainty, novelty, and prediction error on episodic memory. Recent studies found that new episodes are more likely to be encoded under novel circumstances while prior experiences are more likely to be retrieved when conditions are familiar (<xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Duncan and Shohamy, 2016</xref>; <xref ref-type="bibr" rid="bib21">Duncan et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Hasselmo, 2006</xref>). Shifts between these states of memory are thought to be modulated by oneâs focus on internal or external sources of information (<xref ref-type="bibr" rid="bib20">Decker and Duncan, 2020</xref>; <xref ref-type="bibr" rid="bib77">Tarder-Stoll et al., 2020</xref>) and signaled by prediction errors based in episodic memory (<xref ref-type="bibr" rid="bib5">Bein et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib75">Sinclair and Barense, 2018</xref>; <xref ref-type="bibr" rid="bib31">Greve et al., 2017</xref>). Relatedly, unsigned prediction errors, which are a marker of surprise, improve later episodic memory (<xref ref-type="bibr" rid="bib66">Rouhani et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Rouhani and Niv, 2021</xref>; <xref ref-type="bibr" rid="bib1">Antony et al., 2021</xref>; <xref ref-type="bibr" rid="bib6">Ben-Yakov et al., 2022</xref>). Findings have even suggested that states of familiarity and novelty can bias decisions toward the use of single past experiences or not (<xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Duncan and Shohamy, 2016</xref>).</p><p>One alternative hypothesis that emerges from this work is that change-induced uncertainty and novelty could exert similar effects on memory, such that novelty signaled by expectancy violations increases encoding in a protracted manner that dwindles as uncertainty is resolved, or the state of the environment becomes familiar. Our results provide mixed support for this interpretation. While subsequent memory was improved by the presence of uncertainty at encoding, as would be predicted by this work, there was little effect of uncertainty at encoding time on the extent to which decisions were guided by individual memories. It, therefore, seems likely that uncertainty and novelty operate in concert but exert different effects over decision-making, an interpretation supported by recent evidence (<xref ref-type="bibr" rid="bib84">Xu et al., 2021</xref>).</p><p>This work raises further questions about the neurobiological basis of memory-based decisions and the role of neuromodulation in signaling uncertainty and aiding memory. In particular, studies have revealed unique functions for norepinephrine (NE) and acetylcholine (ACh) on uncertainty and learning. These findings suggest that volatility, as defined here, is likely to impact the noradrenergic modulatory system, which has been found to signal unexpected changes throughout learning (<xref ref-type="bibr" rid="bib54">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">Yu and Dayan, 2005</xref>; <xref ref-type="bibr" rid="bib86">Yu and Dayan, 2002</xref>; <xref ref-type="bibr" rid="bib88">Zhao et al., 2019</xref>). Noradrenergic terminals densely innervate the hippocampus (<xref ref-type="bibr" rid="bib72">Schroeter et al., 2000</xref>), and a role for NE in both explicit memory formation (<xref ref-type="bibr" rid="bib30">Grella et al., 2019</xref>) and retrieval (<xref ref-type="bibr" rid="bib52">Murchison et al., 2004</xref>) has been posited. Future studies involving a direct investigation of NE or an indirect investigation using pupillometry (<xref ref-type="bibr" rid="bib54">Nassar et al., 2012</xref>) may help to isolate its contributions to the interaction between incremental learning and episodic memory in decision-making. ACh is also important for learning and memory as memory formation is facilitated by ACh in the hippocampus, which may contribute to its role in separating and storing new experiences (<xref ref-type="bibr" rid="bib33">Hasselmo, 2006</xref>; <xref ref-type="bibr" rid="bib20">Decker and Duncan, 2020</xref>). In addition to this role, ACh is heavily involved in incremental learning and has been widely implicated in signaling expected uncertainty (<xref ref-type="bibr" rid="bib86">Yu and Dayan, 2002</xref>; <xref ref-type="bibr" rid="bib7">Bland and Schaefer, 2012</xref>). ACh may therefore play an important part in managing the tradeoff between incremental learning and episodic memory.</p><p>Indeed, while in this work we investigated the impact of uncertainty on learning using a well-established manipulation of environmental volatility, in general (and even in this task) uncertainty also arises from many other parameters of the environment, such as stochasticity (trial-wise outcome variance) (<xref ref-type="bibr" rid="bib59">Piray and Daw, 2021</xref>). It remains to be seen whether similar results would be observed using other types of manipulations targeting uncertainty. In our task, the outcome variance was held constant, making it difficult to isolate the effects of stochasticity on participantsâ subjective experience of uncertainty. The decision to focus on volatility was based on a rich prior literature demonstrating that volatility manipulations are a reliable means to modulate uncertainty in incremental learning (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Mathys et al., 2011</xref>; <xref ref-type="bibr" rid="bib55">OâReilly, 2013</xref>; <xref ref-type="bibr" rid="bib54">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Piray and Daw, 2020</xref>). Nonetheless, altering outcome variance to capture effects of stochasticity on episodic memory remains a critical avenue for further study. Still other attributes of the learning environment, like valence, have been shown to impact both uncertainty estimation (<xref ref-type="bibr" rid="bib2">Aylward et al., 2019</xref>; <xref ref-type="bibr" rid="bib62">Pulcu and Browning, 2019</xref>) and subsequent memory (<xref ref-type="bibr" rid="bib65">Rosenbaum et al., 2022</xref>; <xref ref-type="bibr" rid="bib40">Kensinger, 2004</xref>). It remains an open question how the valence of outcomes may impact the effects we observed here.</p><p>Further, another interpretation of this work is that, rather than capturing a tradeoff between multiple memory systems, our task could possibly be accomplished by a single system learning about, and dynamically weighting, independent features. Specifically, here we operationalized incremental learning as learning about a feature shared across multiple events (deck color) and episodic memory as learning about a trial-unique feature (an object that could be repeated once). Shifting attention between these independent features whenever one is less reliable could then yield similar behavior to arbitrating between incremental learning and episodic memory as we have posited here. While a scheme like this is possible, much prior work (<xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Lee et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Poldrack et al., 2001</xref>; <xref ref-type="bibr" rid="bib57">Packard and McGaugh, 1996</xref>; <xref ref-type="bibr" rid="bib51">McDonald and White, 1994</xref>; <xref ref-type="bibr" rid="bib82">Wimmer et al., 2014</xref>) indicates that multiple memory systems (differentiated by numerous other behavioral and neural signatures) are involved in the types of repeated vs. one-shot learning measured here. Further, our subsequent memory findings that individual objects and their associated value were better remembered from putatively episodic choices lend further support to the idea that episodic memory is used throughout the task. Nevertheless, more work is needed to distinguish between these alternatives and verify the connection between our task and other signatures of incremental vs. episodic memory.</p><p>For example, while in this study we disadvantaged incremental learning relative to episodic memory, similar predictions about their balance could be made by instead preferentially manipulating episodic memory, for example, through effects such as interference or recency and primacy. Another direction would be to look to the computational literature for additional task circumstances in which there are theoretical benefits to deploying episodic memory, and where incremental learning is generally ill suited, such as in environments that are high dimensional or require planning far into the future (<xref ref-type="bibr" rid="bib28">Gershman and Daw, 2017</xref>). In principle, the brain can use episodic memory to precisely target individual past experiences in these situations depending on the relevance of their features to decisions in the present. Recent advances in computational neuroscience have, for example, demonstrated that artificial agents endowed with episodic memory are able to exploit its rich representation of past experience to make faster, more effective decisions (<xref ref-type="bibr" rid="bib46">Lengyel and Dayan, 2007</xref>; <xref ref-type="bibr" rid="bib8">Blundell, 2016</xref>; <xref ref-type="bibr" rid="bib69">Santoro et al., 2016</xref>). While here we provided episodic memory as an alternative source of value to be used in the presence of uncertainty about incremental estimates, future studies making use of paradigms tailored more directly toward episodic memoryâs assets will help to further elucidate how and when the human brain recruits episodic memory for decisions.</p><p>Finally, it is worth noting that many individuals, in both the main and replication samples, failed to meet our baseline performance criterion of altering the incremental learning rate between the low- and high-volatility environments (see âMaterials and methodsâ). It is unclear whether this insensitivity to volatility was due to the limitations of online data collection, such as inattentiveness, or whether it is a more general feature of human behavior. While the low-volatility environment used here had half as many reversals as the high-volatility environment, it was still much more volatile than some environments used previously to study the effects of volatility on incremental learning (e.g., in entirely stable environments; <xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>). Thus, the relatively subtle difference between environments may also have contributed to some participantsâ volatility insensitivity.</p><p>In conclusion, we have demonstrated that uncertainty induced by volatile environments impacts whether incremental learning or episodic memory is recruited for decisions. Greater uncertainty increased the likelihood that single experiences were retrieved for decision-making. This effect suggests that episodic memory aids decision-making when simpler sources of value are less accurate. By focusing on uncertainty, our results shed light on the exact circumstances under which episodic memory is used for decision-making.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental tasks</title><p>The primary experimental task used here builds upon a paradigm previously developed by our lab (<xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>) to successfully measure the relative contribution of incremental and episodic memory to decisions (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Participants were told that they would be playing a card game where their goal was to win as much money as possible. Each trial consisted of a choice between two decks of cards that differed based on their color (shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> as purple and orange). Participants had 2 s to decide between the decks and, upon making their choice, a green box was displayed around their choice until the full 2 s had passed. The outcome of each decision was then immediately displayed for 1 s. Following each decision, participants were shown a fixation cross during the intertrial interval period that varied in length (mean = 1.5 s, min = 1 s, max = 2 s). Decks were equally likely to appear on either side of the screen (left or right) on each trial and screen side was not predictive of outcomes. Participants completed a total of 320 trials and were given a 30 s break every 80 trials.</p><p>Participants were made aware that there were two ways they could earn bonus money throughout the task, which allowed for the use of incremental and episodic memory, respectively. First, at any point in the experiment one of the two decks was âlucky,â meaning that the expected value (<inline-formula><mml:math id="inf46"><mml:mi>V</mml:mi></mml:math></inline-formula>) of one deck color was higher than the other (<inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 63Â¢, <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 37Â¢). Outcomes ranged from $0 to $1 in increments of 20Â¢. Critically, the mapping from <inline-formula><mml:math id="inf49"><mml:mi>V</mml:mi></mml:math></inline-formula> to deck color underwent an unsignaled reversal periodically throughout the experiment (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), which incentivized participants to utilize each deckâs recent reward history in order to determine the identity of the currently lucky deck. Each participant completed the task over two environments (with 160 trials in each) that differed in their relative volatility: a low-volatility environment with 8 <inline-formula><mml:math id="inf50"><mml:mi>V</mml:mi></mml:math></inline-formula> reversals, occurring every 20 trials on average, and a high-volatility environment with 16 <inline-formula><mml:math id="inf51"><mml:mi>V</mml:mi></mml:math></inline-formula> reversals, occurring every 10 trials on average. Reversal trials in each environment were determined by generating a list of bout lengths (high volatility: 16 bouts between 6 trials minimum and 14 trials maximum; low volatility: 8 bouts between 15 trials minimum and 24 trials maximum) at the beginning of the task and then randomizing this list for each participant. Participants were told that they would be playing in two different casinos and that in one casino deck luckiness changed less frequently while in the other deck luckiness changed more frequently. Participants were also made aware of which casino they were currently in by a border on the screen, with a solid black line indicating the low-volatility casino and a dashed black line indicating the high-volatility casino. The order in which the environments were seen was counterbalanced across participants.</p><p>Second, in order to allow us to assess the use of episodic memory throughout the task, each card within a deck featured an image of a trial-unique object that could reappear once throughout the experiment after initially being chosen. Participants were told that if they encountered a card a second time it would be worth the same amount as when it was first chosen, regardless of whether its deck color was currently lucky or not. On a given trial <inline-formula><mml:math id="inf52"><mml:mi>t</mml:mi></mml:math></inline-formula>, cards chosen once from trials <inline-formula><mml:math id="inf53"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>9</mml:mn></mml:math></inline-formula> through <inline-formula><mml:math id="inf54"><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>30</mml:mn></mml:math></inline-formula> had a 60% chance of reappearing following a sampling procedure designed to prevent each deckâs expected value from becoming skewed by choice, minimize the correlation between the expected value of previously seen cards and deck expected value, and ensure that choosing a previously selected card remained close to 50Â¢. Specifically, outcomes for each deck were drawn from a pseudorandom list of deck values that was generated at the start of the task, sampled without replacement, and repopulated after each reversal. Previously seen cards were then sampled using the following procedure: (i) a list of objects from the past 9â30 trials equal to an outcome left in the current list of potential deck outcomes was generated; (ii) the list was narrowed down to objects whose value was incongruent with the current expected value of their associated deck if such objects were available; and (iii) if the average value of objects shown to a participant was greater than 50Â¢, the object with the lowest value was shown, otherwise an object was randomly sampled without replacement. This sampling procedure is identical to that used previously in <xref ref-type="bibr" rid="bib23">Duncan et al., 2019</xref>.</p><p>Participants also completed a separate decision-making task prior to the combined deck learning and card memory task that was identical in design but lacked trial-unique objects on each card. This task, the deck learning task, was designed to isolate the sole contribution of incremental learning to decisions and to allow participants to gain prior experience with each environmentâs volatility level. In this task, all participants first saw the low-volatility environment followed by the high-volatility environment in order to emphasize the relative increase in the high-volatility environment. Participants completed the combined deck learning and card memory task immediately following completion of the deck learning task and were told that the likelihood of deck luckiness reversals in each environment would be identical for both the deck learning task and the deck learning and card memory task. Instructions were presented immediately prior to each task, and participants completed five practice trials and a comprehension quiz prior to starting each.</p><p>Following completion of the combined deck learning and card memory task, we tested participantsâ memory for the trial-unique objects. Participants completed 80 (up to) three-part memory trials. An object was first displayed on the screen, and participants were asked whether or not they had previously seen the object and were given five response options: Definitely New, Probably New, Donât Know, Probably Old, and Definitely Old. If the participant indicated that they had not seen the object before or did not know, they moved on to the next trial. If, however, they indicated that they had seen the object before, they were then asked if they had chosen the object or not. Lastly, if they responded that they had chosen the object, they were asked what the value of that object was (with options spanning each of the six possible object values between $0 and $1). Of the 80 trials, 48 were previously seen objects and 32 were new objects that had not been seen before. Of the 48 previously seen objects, half were sampled from each environment (24 each) and, of these, an equal number were taken from each possible object value (with 4 from each value in each environment). As with the decision-making tasks, participants were required to pass a comprehension quiz prior to starting the memory task.</p><p>All tasks were programmed using the jsPsych JavaScript library (<xref ref-type="bibr" rid="bib19">de Leeuw, 2015</xref>) and hosted on a Google Cloud server running Apache and the Ubuntu operating system. Object images were selected from publicly available stimulus sets (<xref ref-type="bibr" rid="bib42">Konkle and Oliva, 2012</xref>; <xref ref-type="bibr" rid="bib11">Brady et al., 2008</xref>) for a total of 665 unique objects that could appear in each run of the experiment.</p></sec><sec id="s4-2"><title>Participants</title><p>A total of 418 participants between the ages of 18â35 were recruited for our main sample through Amazon Mechanical Turk using the Cloud Research Approved Participants feature (<xref ref-type="bibr" rid="bib48">Litman et al., 2017</xref>). Recruitment was restricted to the United States, and $9 compensation was provided following completion of the 50 min experiment. Participants were also paid a bonus in proportion to their final combined earnings on both the training task and the combined deck learning and card memory task (total earnings/100). Before starting each task, all participants were required to score 100% on a quiz that tested their comprehension of the instructions and were made to repeat the instructions until this score was achieved. Informed consent was obtained with approval from the Columbia University Institutional Review Board.</p><p>From the initial pool, participants were excluded from analysis on the deck learning and card memory task if they (i) responded to fewer trials than the group average minus 1 standard deviation on the deck learning and card memory task, (ii) responded faster than the group average minus 1 standard deviation on this task, or (iii) did not demonstrate faster learning in the high- compared to the low-volatility environment on the independent deck learning task. Our reasoning for this latter decision was that it is only possible to test for effects of volatility on episodic memory recruitment in participants who were sensitive to the difference in volatility between the environments, and it is well-established that a higher learning rate should be used in more volatile conditions (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>). Further, our independent assessment of deck learning was designed to avoid issues of selection bias in this procedure. We measured the effect of environment on learning by fitting a mixed-effects logistic regression model to predict if subjects chose the lucky deck up to five trials after a reversal event in the deck learning task. For each subject <inline-formula><mml:math id="inf55"><mml:mi>s</mml:mi></mml:math></inline-formula> and trial <inline-formula><mml:math id="inf56"><mml:mi>t</mml:mi></mml:math></inline-formula>, this model predicts the probability that the lucky deck was chosen:<disp-formula id="equ1"><mml:math id="m1"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <italic>Î²</italic>s are fixed effects, <inline-formula><mml:math id="inf57"><mml:mi>b</mml:mi></mml:math></inline-formula> s are random effects, <inline-formula><mml:math id="inf58"><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula> is the trial number coded as distance from a reversal event (1â5), and <inline-formula><mml:math id="inf59"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula> is the environment a choice was made in coded as â0.5 and 0.5 for the low- and high-volatility environments, respectively. Participants with positive values of <inline-formula><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> can be said to have chosen the lucky deck more quickly following a reversal in the high- compared to the low-volatility environment, and we included only these participants in the rest of our analyses. A total of 254 participants survived after applying these criteria, with 120 participants failing to respond to the volatility manipulation (criteria iii) and 44 participants responding to too few trials (criteria i) or too quickly (criteria ii).</p></sec><sec id="s4-3"><title>Deck learning and card memory task behavioral analysis</title><p>For regression models described here as well as those in the following sections, fixed effects are reported in the text as the median of each parameterâs marginal posterior distribution alongside 95% credible intervals, which indicate where 95% of the posterior density falls. Parameter values outside of this range are unlikely given the model, data, and priors. Thus, if the range of likely values does not include zero, we conclude that a meaningful effect was observed.</p><p>We first analyzed the extent to which previously seen (old) objects were used in the combined deck learning and card memory task by fitting the following mixed-effects regression model to predict whether an old object was chosen:<disp-formula id="equ3"><mml:math id="m3"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf61"><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula> is the centered value (between â0.5 and 0.5) of an old object. We additionally controlled for the influence of deck value on this analysis by adding a regressor, <inline-formula><mml:math id="inf62"><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula>, which is the centered true average value of the deck on which each object was shown. Trials not featuring old objects were dropped from this analysis.</p><p>We then similarly assessed the extent to which participants engaged in incremental learning overall by looking at the impact of reversals on incremental accuracy directly. To do this, we grouped trials according to their distance from a reversal, up to four trials prior to (<inline-formula><mml:math id="inf63"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>4</mml:mn><mml:mo>:</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), during (<inline-formula><mml:math id="inf64"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>), and after (<inline-formula><mml:math id="inf65"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula>) a reversal occurred. We then dummy coded them to measure their effects on incremental accuracy separately. We also controlled for the influence of old object value in this analysis by including in this regression the coded value of a previously seen object (ranging from 0.5 if the value was $1 on the lucky deck or $0 on the lucky deck to â0.5 if the value was $0 on the lucky deck and $1 on the unlucky deck), for a total of 18 estimated effects:<disp-formula id="equ4"><mml:math id="m4"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn><mml:mo>:</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn><mml:mo>:</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>:</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>:</mml:mo><mml:mn>18</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>To next focus on whether there was an effect of environment on the extent to which the value of old objects was used for decisions, we restricted all further analyses involving old objects to âincongruentâ trials, which were defined as trials on which either the old object was high valued (&gt;50Â¢) and on the unlucky deck or low valued (&lt;50Â¢) and on the lucky deck. To better capture participantsâ beliefs, deck luckiness was determined by the best-fitting incremental learning model (see next section) rather than using the experimenter-controlled ground truth: whichever deck had the higher model-derived value estimate on a given trial was labeled the lucky deck. Our logic in using only incongruent trials was that choices that stray from choosing whichever deck is more valuable should reflect choices that were based on the episodic value for an object. Lastly, we defined our outcome measure of EBCI to equal 1 on trials where the âcorrectâ episodic response was given (i.e., high-valued objects were chosen and low-valued object were avoided), and 0 on trials where the âcorrectâ incremental response was given (i.e., the opposite was true). A single mixed-effects logistic regression was then used to assess the possible effects of environment <inline-formula><mml:math id="inf66"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula> on EBCI:<disp-formula id="equ5"><mml:math id="m5"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf67"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula> was coded identically to the above analyses. We included a covariate <inline-formula><mml:math id="inf68"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:math></inline-formula> in this analysis to account for the possibility that participants are likely to make noisier incremental value-based decisions in the high-volatility compared to the low-volatility environment, which may contribute to the effects of environment on EBCI. To calculate this index, we fit the following mixed-effects logistic regression model to capture an interaction effect of environment and RB model-estimated deck value (see âDeck learning computational modelsâ section below) on whether the orange deck was chosen:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>O</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>Ï</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We fit this model only to trials without the presence of a previously seen object in order to achieve a measure of noise specific to incremental learning. Each participantâs random effect of the interaction between deck value and environment, <inline-formula><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , was then used as the <inline-formula><mml:math id="inf70"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:math></inline-formula> covariate in the logistic regression testing for an effect of environment on EBCI.</p><p>To assess the effect of episodic-based choices on reaction time (RT), we used the following mixed-effects linear regression model:<disp-formula id="equ7"><mml:math id="m7"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf71"><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula> was coded as â0.5 for incremental-based trials and 0.5 for episodic-based trials. We also included covariates to control for three other possible effects on RT. The first, <inline-formula><mml:math id="inf72"><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:math></inline-formula>, captured possible RT slowing due to exploratory decisions, which in the present task required participants to switch from choosing one deck to the other. This variable was coded as â0.5 if a stay occurred and 0.5 if a switch occurred. The second, <inline-formula><mml:math id="inf73"><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula>, captured any effects due to the value of the option that may have guided choice, and was set to be the value of the previously seen object on episodic-based trials and the running average true value on incremental-based trials. Finally, the third, <inline-formula><mml:math id="inf74"><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:math></inline-formula>, captured effects due to possible slowing when choices occurred under conditions of greater uncertainty as estimated by the reduced Bayesian model (see below).</p></sec><sec id="s4-4"><title>Deck learning computational models</title><p>We next assessed the performance of several computational learning models on our task in order to best capture incremental learning. A detailed description of each model can be found in the âSupplementary methods.â In brief, these included one model that performed (&quot;Rescorla-Wagner style updating [<xref ref-type="bibr" rid="bib64">Rescorla and Wagner, 1972</xref>]â) with both a single (RW1Î±) and a separate (RW2Î±) fixed learning rate for each environment, two reduced Bayesian (RB) models (<xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>) with both a single (RB1<italic>H</italic>) and a separate hazard rate for each environment (RB1<italic>H</italic>), a contextual inference model (CI), and a RescorlaâWagner model that learned only a single-value estimate (RW1Q). Models were fit to the deck learning task (see âPosterior inferenceâ and Appendix 3) and used to generate subject-wise estimates of deck value, and where applicable, uncertainty in the combined deck learning and card memory task.</p></sec><sec id="s4-5"><title>Combined choice models</title><p>After fitting the above models to the deck learning task, parameter estimates for each subject were then used to generate trial-by-trial time series for deck value and uncertainty (where applicable) throughout performance on the combined deck learning and card memory task. Mixed-effects Bayesian logistic regressions for each deck learning model were then used to capture the effects of multiple memory-based sources of value on incongruent trial choices in this task. For each subject <inline-formula><mml:math id="inf75"><mml:mi>s</mml:mi></mml:math></inline-formula> and trial <inline-formula><mml:math id="inf76"><mml:mi>t</mml:mi></mml:math></inline-formula>, these models can be written as<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>O</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>Ï</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mtext>Â </mml:mtext><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where the intercept captures a bias toward choosing either of the decks regardless of outcome, <inline-formula><mml:math id="inf77"><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula> is the deck value estimated from each model, the effect of <inline-formula><mml:math id="inf78"><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:math></inline-formula> captures a bias toward choosing a previously seen card regardless of its value, and <inline-formula><mml:math id="inf79"><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula> is the coded value of a previously seen object (ranging from 0.5 if the value was $1 on the orange deck or $0 on the purple deck to â0.5 if the value was $0 on the orange deck and $1 on the purple deck). To capture variations in sensitivity to old object value due to volatility (represented here by a categorical environment variable, <inline-formula><mml:math id="inf80"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula>, coded as â0.5 for the low- and 0.5 for the high-volatility environment), we also included an interaction term between old object value and environment in each model. An additional seventh regression that also incorporated our hypothesized effect of increased sensitivity to old object value when uncertainty about deck value is higher was also fit. This regression was identical to the others but included an additional interaction effect of uncertainty and old object value: <inline-formula><mml:math id="inf81"><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and used the RB2<italic>H</italic> modelâs <inline-formula><mml:math id="inf82"><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula> estimate alongside its estimate of RU to estimate the effect of <inline-formula><mml:math id="inf83"><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>Ã</mml:mo><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:math></inline-formula>. RU was chosen over CPP because it captures the reducible uncertainty about deck value, which is the quantity we were interested in for this study. Prior to fitting the model, all predictors were z scored in order to report effects in standard units.</p></sec><sec id="s4-6"><title>Relative uncertainty analyses</title><p>We conducted several other analyses that tested effects on or of RU throughout the combined deck learning and card memory task. RU was mean-centered in each of these analyses. First, we assessed separately the effect of RU at retrieval time on EBCI using a mixed-effects logistic regression:<disp-formula id="equ9"><mml:math id="m9"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msubsup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>An additional binomial term was included in this model to allow for the possibility that the effect of RU is nonlinear, although this term was found to have no effect. The effect of RU at encoding time was assessed using an identical model but with RU at encoding included instead of RU at retrieval.</p><p>Next, to ensure that the RB model captured uncertainty related to changes in deck luckiness, we tested for an effect of environment on RU using a mixed-effects linear regression:<disp-formula id="equ10"><mml:math id="m10"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We then also looked at the impact of reversals on RU. To do this, we calculated the difference in RU on reversal trials and up to four trials following a reversal from the average RU on the four trials immediately preceding a reversal. Then, using a dummy coded approach similar to that used for the model testing effects of reversals on incremental accuracy, we fit the following mixed-effects linear regression with five effects:<disp-formula id="equ11"><mml:math id="m11"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:mi>U</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We also assessed the effect of RU on reaction time using another mixed-effects linear regression:<disp-formula id="equ12"><mml:math id="m12"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec id="s4-7"><title>Subsequent memory task behavioral analysis</title><p>Performance on the subsequent memory task was analyzed in several ways across recognition memory and value memory trials. We first assessed participantsâ recognition memory accuracy in general by computing the signal detection metric d prime for each participant adjusted for extreme proportions using a log-linear rule (<xref ref-type="bibr" rid="bib34">Hautus, 1995</xref>). The relationship with d prime and sensitivity to both episodic value and incremental value was then determined using simple linear regressions of the form <inline-formula><mml:math id="inf84"><mml:mi>d</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf85"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:math></inline-formula> was either the random effect of episodic value from the combined choice model for each participant or the random effect of incremental value from the combined choice value for each participant. We additionally assessed the difference in recognition memory performance between environments by computing d prime for each environment separately, with the false alarm rate shared across environments and hit rate differing between environments, using the following mixed-effects linear regression:<disp-formula id="equ13"><mml:math id="m13"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We next determined the extent to which participantsâ memory for previously seen objects was impacted by whether an object was seen initially on either an episodic- or incremental-based choice using the following mixed-effects logistic regression model:<disp-formula id="equ14"><mml:math id="m14"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf86"><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:math></inline-formula> was 0 if an object was incorrectly labeled as new and 1 if it was accurately identified as old. The final recognition memory analysis we performed was focused on assessing the impact of variables (RU, changepoint probability [CPP], and the absolute value of prediction error [APE]) extracted from the RB model at encoding time on future subsequent memory. Because these variables are, by definition, highly correlated with one another (see âSupplementary methodsâ), we fit separate simple mixed-effects logistic regression models predicting recognition memory from each variable separately and then compared the predictive performance of each model (see below) to determine which best accounted for subsequent memory performance. The models additionally controlled for potential recognition memory enhancements due to the absolute magnitude of an objectâs true value by including this quantity as a covariate in each of these models.</p><p>In addition to the analyses of recognition memory, analogous effects were assessed for performance on memory for value. General value memory accuracy and a potential effect of environment on remembered value were assessed using the following mixed-effect linear regression:<disp-formula id="equ15"><mml:math id="m15"><mml:mtable columnalign="right"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>Ã</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>[</mml:mo><mml:mi>t</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf87"><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:math></inline-formula> is the remembered value of an object on each memory trial (between $0 and $1), and <inline-formula><mml:math id="inf88"><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:math></inline-formula> is an objectâs true value. We next assessed whether value memory was similarly impacted by whether an object was seen initially on either ran episodic- or incremental-based choice using a similar model for objects from incongruent trials only with <inline-formula><mml:math id="inf89"><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:math></inline-formula> as a predictor rather than <inline-formula><mml:math id="inf90"><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:math></inline-formula>. Lastly, as with the recognition memory analyses, we determined the extent to which trial-wise variables from the RB model (RU, CPP, and APE) at encoding impacted subsequent value memory by using each of these as a predictor instead in similar models and then comparing the predictive performance of each in an identical manner to the recognition memory models.</p></sec><sec id="s4-8"><title>Posterior inference and model comparison</title><p>Parameters for all incremental learning models were estimated using hierarchical Bayesian inference such that group-level priors were used to regularize subject-level estimates. This approach to fitting reinforcement learning models improves parameter identifiability and predictive accuracy (<xref ref-type="bibr" rid="bib79">van Geen and Gerraty, 2021</xref>). The joint posterior was approximated using No-U-Turn Sampling (<xref ref-type="bibr" rid="bib36">Hoï¬man and Gelman, 2011</xref>) as implemented in Stan (<xref ref-type="bibr" rid="bib78">Team SD, 2020</xref>). Four chains with 2000 samples (1000 discarded as burn-in) were run for a total of 4000 posterior samples per model. Chain convergence was determined by ensuring that the GelmanâRubin statistic <inline-formula><mml:math id="inf91"><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> was close to 1. A full description of the parameterization and choice of priors for each model can be found in Appendix 3. All regression models were fit using No-U-Turn Sampling in Stan with the same number of chains and samples. Default weakly informative priors implemented in the rstanarm package (<xref ref-type="bibr" rid="bib68">Rstanarm, 2022</xref>) were used for each regression model. Model fit for the combined choice models and the models measuring trial-wise effects of encoding on subsequent memory was assessed by separating each dataset into 20-folds and performing a cross-validation procedure by leaving out N/20 subjects per fold, where N is the number of subjects in each sample. The expected log pointwise predictive density (ELPD) was then computed and used as a measure of out-of-sample predictive fit for each model.</p></sec><sec id="s4-9"><title>Replication</title><p>We identically repeated all procedures and analyses applied to the main sample on an independently collected replication sample. A total of 401 participants were again recruited through Amazon Mechanical Turk, and 223 survived exclusion procedures carried out identically to those used for the main sample, with 124 participants failing to respond to the volatility manipulation (criteria iii) and 54 participants responding to too few trials (criteria i) or too quickly (criteria ii).</p></sec><sec id="s4-10"><title>Citation race and gender diversity statement</title><p>The gender balance of papers cited within this work was quantified using databases that store the probability of a first name being carried by a woman. Excluding self-citations to the first and last authors of this article, the gender breakdown of our references is 12.16% woman (first)/woman (last), 6.76% man/woman, 23.44% woman/man, and 57.64% man/man. This method is limited in that (i) names, pronouns, and social media profiles used to construct the databases may not, in every case, be indicative of gender identity and (b) it cannot account for intersex, nonbinary, or transgender people. Second, we obtained predicted racial/ethnic category of the first and last authors of each reference using databases that store the probability of a first and last name being carried by an author of color. By this measure (and excluding self-citations), our references contain 9.55% author of color (first)/author of color(last), 19.97% white author/author of color, 22.7% author of color/white author, and 47.78% white author/white author. This method is limited in that (i) using names and Florida Voter Data to make the predictions may not be indicative of racial/ethnic identity, and (ii) it cannot account for indigenous and mixed-race authors, or those who may face differential biases due to the ambiguous racialization or ethnicization of their names.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Methodology, Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing â review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Informed consent was obtained online with approval from the Columbia University Institutional Review Board (IRB #1488).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-81679-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code, data, and software needed to reproduce the manuscript can be found here: <ext-link ext-link-type="uri" xlink:href="https://codeocean.com/capsule/2024716/tree/v1">https://codeocean.com/capsule/2024716/tree/v1</ext-link>; DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.24433/CO.1266819.v1">https://doi.org/10.24433/CO.1266819.v1</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Nicholas</surname><given-names>J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Uncertainty alters the balance between incremental learning and episodic memory</data-title><source>Code Ocean</source><pub-id pub-id-type="doi">10.24433/CO.1266819.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors thank Sam Gershman, Raphael Gerraty, Camilla van Geen, Mariam Aly, and members of the Shohamy Lab for insightful discussion and conversations. Support was provided by the NSF Graduate Research Fellowship (JN; award # 1644869), the NSF (DS, ND; award # 1822619), the NIMH/NIH (DS, ND; award # MH121093), and the Templeton Foundation (DS grant #60844).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antony</surname><given-names>JW</given-names></name><name><surname>Hartshorne</surname><given-names>TH</given-names></name><name><surname>Pomeroy</surname><given-names>K</given-names></name><name><surname>Gureckis</surname><given-names>TM</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>McDougle</surname><given-names>SD</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Behavioral, physiological, and neural signatures of surprise during naturalistic sports viewing</article-title><source>Neuron</source><volume>109</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.10.029</pub-id><pub-id pub-id-type="pmid">33242421</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aylward</surname><given-names>J</given-names></name><name><surname>Valton</surname><given-names>V</given-names></name><name><surname>Ahn</surname><given-names>W-Y</given-names></name><name><surname>Bond</surname><given-names>RL</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Robinson</surname><given-names>OJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Altered learning under uncertainty in unmedicated mood and anxiety disorders</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>1116</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0628-0</pub-id><pub-id pub-id-type="pmid">31209369</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakkour</surname><given-names>A</given-names></name><name><surname>Palombo</surname><given-names>DJ</given-names></name><name><surname>Zylberberg</surname><given-names>A</given-names></name><name><surname>Kang</surname><given-names>YH</given-names></name><name><surname>Reid</surname><given-names>A</given-names></name><name><surname>Verfaellie</surname><given-names>M</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The hippocampus supports deliberation during value-based decisions</article-title><source>eLife</source><volume>8</volume><elocation-id>e46080</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46080</pub-id><pub-id pub-id-type="pmid">31268419</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bein</surname><given-names>O</given-names></name><name><surname>Duncan</surname><given-names>K</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mnemonic prediction errors bias hippocampal states</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3451</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17287-1</pub-id><pub-id pub-id-type="pmid">32651370</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yakov</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>V</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The limited reach of surprise: evidence against effects of surprise on memory for preceding elements of an event</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>29</volume><fpage>1053</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.3758/s13423-021-01954-5</pub-id><pub-id pub-id-type="pmid">34173187</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bland</surname><given-names>AR</given-names></name><name><surname>Schaefer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Different varieties of uncertainty in human decision-making</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>85</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00085</pub-id><pub-id pub-id-type="pmid">22701401</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Blundell</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Model-Free Episodic Control</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1606.04460</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornstein</surname><given-names>AM</given-names></name><name><surname>Khaw</surname><given-names>MW</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reminders of past choices bias decisions for reward in humans</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15958</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15958</pub-id><pub-id pub-id-type="pmid">28653668</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornstein</surname><given-names>AM</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reinstated episodic context guides sampling-based decisions for reward</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>997</fpage><lpage>1003</lpage><pub-id pub-id-type="doi">10.1038/nn.4573</pub-id><pub-id pub-id-type="pmid">28581478</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual long-term memory has a massive storage capacity for object details</article-title><source>PNAS</source><volume>105</volume><fpage>14325</fpage><lpage>14329</lpage><pub-id pub-id-type="doi">10.1073/pnas.0803390105</pub-id><pub-id pub-id-type="pmid">18787113</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Jocham</surname><given-names>G</given-names></name><name><surname>OâReilly</surname><given-names>JX</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/nn.3961</pub-id><pub-id pub-id-type="pmid">25730669</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction strength modulates responses in human area CA1 to sequence violations</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>1227</fpage><lpage>1238</lpage><pub-id pub-id-type="doi">10.1152/jn.00149.2015</pub-id><pub-id pub-id-type="pmid">26063773</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title><source>The European Journal of Neuroscience</source><volume>35</volume><fpage>1024</fpage><lpage>1035</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07980.x</pub-id><pub-id pub-id-type="pmid">22487033</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The tortoise and the hare: interactions between reinforcement learning and working memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1422</fpage><lpage>1432</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01238</pub-id><pub-id pub-id-type="pmid">29346018</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Within- and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory</article-title><source>PNAS</source><volume>115</volume><fpage>2502</fpage><lpage>2507</lpage><pub-id pub-id-type="doi">10.1073/pnas.1720963115</pub-id><pub-id pub-id-type="pmid">29463751</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1704</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1038/nn1560</pub-id><pub-id pub-id-type="pmid">16286932</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Huys</surname><given-names>QJMS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Serotonin, inhibition, and negative mood</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e4</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0040004</pub-id><pub-id pub-id-type="pmid">18248087</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>JsPsych: a javascript library for creating behavioral experiments in a web browser</article-title><source>Behavior Research Methods</source><volume>47</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3758/s13428-014-0458-y</pub-id><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decker</surname><given-names>AL</given-names></name><name><surname>Duncan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Acetylcholine and the complex interdependence of memory and attention</article-title><source>Current Opinion in Behavioral Sciences</source><volume>32</volume><fpage>21</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.01.013</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>K</given-names></name><name><surname>Sadanand</surname><given-names>A</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Memoryâs penumbra: episodic memory decisions induce lingering mnemonic biases</article-title><source>Science</source><volume>337</volume><fpage>485</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1126/science.1221936</pub-id><pub-id pub-id-type="pmid">22837528</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>KD</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Memory states influence value-based decisions</article-title><source>Journal of Experimental Psychology. General</source><volume>145</volume><fpage>1420</fpage><lpage>1426</lpage><pub-id pub-id-type="doi">10.1037/xge0000231</pub-id><pub-id pub-id-type="pmid">27797556</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>K</given-names></name><name><surname>Semmler</surname><given-names>A</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Modulating the use of multiple memory systems in value-based decisions with contextual novelty</article-title><source>Journal of Cognitive Neuroscience</source><volume>31</volume><fpage>1455</fpage><lpage>1467</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01447</pub-id><pub-id pub-id-type="pmid">31322467</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebbinghaus</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Memory: a contribution to experimental psychology</article-title><source>Annals of Neurosciences</source><volume>20</volume><fpage>155</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.5214/ans.0972.7531.200408</pub-id><pub-id pub-id-type="pmid">25206041</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ersche</surname><given-names>KD</given-names></name><name><surname>Gillan</surname><given-names>CM</given-names></name><name><surname>Jones</surname><given-names>PS</given-names></name><name><surname>Williams</surname><given-names>GB</given-names></name><name><surname>Ward</surname><given-names>LHE</given-names></name><name><surname>Luijten</surname><given-names>M</given-names></name><name><surname>de Wit</surname><given-names>S</given-names></name><name><surname>Sahakian</surname><given-names>BJ</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Carrots and sticks fail to change behavior in cocaine addiction</article-title><source>Science</source><volume>352</volume><fpage>1468</fpage><lpage>1471</lpage><pub-id pub-id-type="doi">10.1126/science.aaf3700</pub-id><pub-id pub-id-type="pmid">27313048</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everitt</surname><given-names>BJ</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural systems of reinforcement for drug addiction: from actions to habits to compulsion</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1481</fpage><lpage>1489</lpage><pub-id pub-id-type="doi">10.1038/nn1579</pub-id><pub-id pub-id-type="pmid">16251991</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hill</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Data Analysis Using Regression and Multilevel/Hierarchical Models</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511790942</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reinforcement learning and episodic memory in humans and animals: an integrative framework</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>101</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033625</pub-id><pub-id pub-id-type="pmid">27618944</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillan</surname><given-names>CM</given-names></name><name><surname>Papmeyer</surname><given-names>M</given-names></name><name><surname>Morein-Zamir</surname><given-names>S</given-names></name><name><surname>Sahakian</surname><given-names>BJ</given-names></name><name><surname>Fineberg</surname><given-names>NA</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>de Wit</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Disruption in the balance between goal-directed behavior and habit learning in obsessive-compulsive disorder</article-title><source>The American Journal of Psychiatry</source><volume>168</volume><fpage>718</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2011.10071062</pub-id><pub-id pub-id-type="pmid">21572165</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grella</surname><given-names>SL</given-names></name><name><surname>Neil</surname><given-names>JM</given-names></name><name><surname>Edison</surname><given-names>HT</given-names></name><name><surname>Strong</surname><given-names>VD</given-names></name><name><surname>Odintsova</surname><given-names>IV</given-names></name><name><surname>Walling</surname><given-names>SG</given-names></name><name><surname>Martin</surname><given-names>GM</given-names></name><name><surname>Marrone</surname><given-names>DF</given-names></name><name><surname>Harley</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Locus coeruleus phasic, but not tonic, activation initiates global remapping in a familiar environment</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>445</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1956-18.2018</pub-id><pub-id pub-id-type="pmid">30478033</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>A</given-names></name><name><surname>Cooper</surname><given-names>E</given-names></name><name><surname>Kaula</surname><given-names>A</given-names></name><name><surname>Anderson</surname><given-names>MC</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Does prediction error drive One-shot declarative learning?</article-title><source>Journal of Memory and Language</source><volume>94</volume><fpage>149</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2016.11.001</pub-id><pub-id pub-id-type="pmid">28579691</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The construction system of the brain</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>364</volume><fpage>1263</fpage><lpage>1271</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0296</pub-id><pub-id pub-id-type="pmid">19528007</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of acetylcholine in learning and memory</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>710</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.09.002</pub-id><pub-id pub-id-type="pmid">17011181</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hautus</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Corrections for extreme proportions and their biasing effects on estimated values ofdâ²</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>27</volume><fpage>46</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.3758/BF03203619</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Houk</surname><given-names>JC</given-names></name><name><surname>Adams</surname><given-names>JL</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>A model of how the basal ganglia generate and use neural signals that predict reinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Houk</surname><given-names>JC</given-names></name></person-group><source>Models of Information Processing in the Basal Ganglia</source><publisher-name>The MIT Press</publisher-name><fpage>249</fpage><lpage>270</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hoï¬man</surname><given-names>MD</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1111.4246</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>LE</given-names></name><name><surname>Meer</surname><given-names>EA</given-names></name><name><surname>Gillan</surname><given-names>CM</given-names></name><name><surname>Hsu</surname><given-names>M</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Increased and biased deliberation in social anxiety</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>146</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01180-y</pub-id><pub-id pub-id-type="pmid">34400815</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>OâNions</surname><given-names>E</given-names></name><name><surname>Sheridan</surname><given-names>L</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bonsai trees in your head: how the Pavlovian system sculpts goal-directed choices by pruning decision trees</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002410</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002410</pub-id><pub-id pub-id-type="pmid">22412360</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kakade</surname><given-names>S</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Acquisition and extinction in autoshaping</article-title><source>Psychological Review</source><volume>109</volume><fpage>533</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.109.3.533</pub-id><pub-id pub-id-type="pmid">12088244</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kensinger</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Remembering emotional experiences: the contribution of valence and arousal</article-title><source>Reviews in the Neurosciences</source><volume>15</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1515/revneuro.2004.15.4.241</pub-id><pub-id pub-id-type="pmid">15526549</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keramati</surname><given-names>M</given-names></name><name><surname>Dezfouli</surname><given-names>A</given-names></name><name><surname>Piray</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Speed/Accuracy trade-off between the habitual and the goal-directed processes</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002055</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002055</pub-id><pub-id pub-id-type="pmid">21637741</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A real-world size organization of object responses in occipitotemporal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>1114</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.036</pub-id><pub-id pub-id-type="pmid">22726840</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Cushman</surname><given-names>FA</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When does model-based control pay off?</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005090</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005090</pub-id><pub-id pub-id-type="pmid">27564094</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SW</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name><name><surname>OâDoherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural computations underlying arbitration between model-based and model-free learning</article-title><source>Neuron</source><volume>81</volume><fpage>687</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.028</pub-id><pub-id pub-id-type="pmid">24507199</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SW</given-names></name><name><surname>OâDoherty</surname><given-names>JP</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural computations mediating One-shot learning in the human brain</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002137</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002137</pub-id><pub-id pub-id-type="pmid">25919291</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><chapter-title>Hippocampal contributions to control: the third way</chapter-title><person-group person-group-type="editor"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Koller,</surname><given-names>D</given-names></name><name><surname>Roweis</surname><given-names>ST</given-names></name><name><surname>Platt</surname><given-names>JC</given-names></name></person-group><source>In Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>889</fpage><lpage>896</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowski</surname><given-names>D</given-names></name><name><surname>Kurowicka</surname><given-names>D</given-names></name><name><surname>Joe</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating random correlation matrices based on vines and extended onion method</article-title><source>Journal of Multivariate Analysis</source><volume>100</volume><fpage>1989</fpage><lpage>2001</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2009.04.008</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litman</surname><given-names>L</given-names></name><name><surname>Robinson</surname><given-names>J</given-names></name><name><surname>Abberbock</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>TurkPrime.com: a versatile crowdsourcing data acquisition platform for the behavioral sciences</article-title><source>Behavior Research Methods</source><volume>49</volume><fpage>433</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.3758/s13428-016-0727-z</pub-id><pub-id pub-id-type="pmid">27071389</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mason</surname><given-names>A</given-names></name><name><surname>Madan</surname><given-names>CR</given-names></name><name><surname>Simonsen</surname><given-names>N</given-names></name><name><surname>Spetch</surname><given-names>ML</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Biased Confabulation in Risky Choice</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/vphgc</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A Bayesian foundation for individual learning under uncertainty</article-title><source>Frontiers in Human Neuroscience</source><volume>5</volume><elocation-id>39</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00039</pub-id><pub-id pub-id-type="pmid">21629826</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname><given-names>RJ</given-names></name><name><surname>White</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Parallel information processing in the water maze: evidence for independent memory systems involving dorsal striatum and hippocampus</article-title><source>Behavioral and Neural Biology</source><volume>61</volume><fpage>260</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/s0163-1047(05)80009-3</pub-id><pub-id pub-id-type="pmid">8067981</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murchison</surname><given-names>CF</given-names></name><name><surname>Zhang</surname><given-names>X-Y</given-names></name><name><surname>Zhang</surname><given-names>W-P</given-names></name><name><surname>Ouyang</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>A</given-names></name><name><surname>Thomas</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A distinct role for norepinephrine in memory retrieval</article-title><source>Cell</source><volume>117</volume><fpage>131</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1016/s0092-8674(04)00259-4</pub-id><pub-id pub-id-type="pmid">15066288</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Rumsey</surname><given-names>KM</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Parikh</surname><given-names>K</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id><pub-id pub-id-type="pmid">22660479</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>OâReilly</surname><given-names>JX</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Making predictions in a changing world-inference, uncertainty, and learning</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>105</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00105</pub-id><pub-id pub-id-type="pmid">23785310</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Markman</surname><given-names>AB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The curse of planning: dissecting multiple reinforcement-learning systems by taxing the central executive</article-title><source>Psychological Science</source><volume>24</volume><fpage>751</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1177/0956797612463080</pub-id><pub-id pub-id-type="pmid">23558545</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packard</surname><given-names>MG</given-names></name><name><surname>McGaugh</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Inactivation of hippocampus or caudate nucleus with lidocaine differentially affects expression of place and response learning</article-title><source>Neurobiology of Learning and Memory</source><volume>65</volume><fpage>65</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1006/nlme.1996.0007</pub-id><pub-id pub-id-type="pmid">8673408</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A simple model for learning in volatile environments</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007963</pub-id><pub-id pub-id-type="pmid">32609755</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A model for learning based on the joint estimation of stochasticity and volatility</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6587</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26731-9</pub-id><pub-id pub-id-type="pmid">34782597</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plonsky</surname><given-names>O</given-names></name><name><surname>Teodorescu</surname><given-names>K</given-names></name><name><surname>Erev</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reliance on small samples, the wavy recency effect, and similarity-based learning</article-title><source>Psychological Review</source><volume>122</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1037/a0039413</pub-id><pub-id pub-id-type="pmid">26075914</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>ParÃ©-Blagoev</surname><given-names>EJ</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Creso Moyano</surname><given-names>J</given-names></name><name><surname>Myers</surname><given-names>C</given-names></name><name><surname>Gluck</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Interactive memory systems in the human brain</article-title><source>Nature</source><volume>414</volume><fpage>546</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.1038/35107080</pub-id><pub-id pub-id-type="pmid">11734855</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulcu</surname><given-names>E</given-names></name><name><surname>Browning</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The misestimation of uncertainty in affective disorders</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>865</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.07.007</pub-id><pub-id pub-id-type="pmid">31431340</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>LR</given-names></name><name><surname>Juang</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>An introduction to hidden markov models</article-title><source>IEEE ASSP Magazine</source><volume>3</volume><fpage>4</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/MASSP.1986.1165342</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>R</given-names></name><name><surname>Wagner</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement in classical conditioning II</chapter-title><person-group person-group-type="editor"><name><surname>Prokasy</surname><given-names>WF</given-names></name><name><surname>Black</surname><given-names>AH</given-names></name></person-group><source>Current Research and Theory</source><publisher-loc>New York</publisher-loc><publisher-name>Appleton- Century-Crofts</publisher-name><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenbaum</surname><given-names>GM</given-names></name><name><surname>Grassie</surname><given-names>HL</given-names></name><name><surname>Hartley</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory</article-title><source>eLife</source><volume>11</volume><elocation-id>e64620</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.64620</pub-id><pub-id pub-id-type="pmid">35072624</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouhani</surname><given-names>N</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dissociable effects of surprising rewards on learning and memory</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>44</volume><fpage>1430</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1037/xlm0000518</pub-id><pub-id pub-id-type="pmid">29553767</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouhani</surname><given-names>N</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Signed and unsigned reward prediction errors dynamically enhance learning and memory</article-title><source>eLife</source><volume>10</volume><elocation-id>e61077</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61077</pub-id><pub-id pub-id-type="pmid">33661094</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Rstanarm</collab></person-group><year iso-8601-date="2022">2022</year><data-title>Bayesian applied regression modeling via stan</data-title><version designator="2.21.1">2.21.1</version><source>R Package</source><ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/rstanarm/">https://mc-stan.org/rstanarm/</ext-link></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Bartunov</surname><given-names>S</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>One-Shot Learning with Memory-Augmented Neural Networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1605.06065</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Guerin</surname><given-names>SA</given-names></name><name><surname>St Jacques</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Memory distortion: an adaptive perspective</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>467</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.08.004</pub-id><pub-id pub-id-type="pmid">21908231</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Addis</surname><given-names>DR</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Martin</surname><given-names>VC</given-names></name><name><surname>Spreng</surname><given-names>RN</given-names></name><name><surname>Szpunar</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The future of memory: remembering, imagining, and the brain</article-title><source>Neuron</source><volume>76</volume><fpage>677</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.11.001</pub-id><pub-id pub-id-type="pmid">23177955</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeter</surname><given-names>S</given-names></name><name><surname>Apparsundaram</surname><given-names>S</given-names></name><name><surname>Wiley</surname><given-names>RG</given-names></name><name><surname>Miner</surname><given-names>LH</given-names></name><name><surname>Sesack</surname><given-names>SR</given-names></name><name><surname>Blakely</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Immunolocalization of the cocaine- and antidepressant-sensitive l-norepinephrine transporter</article-title><source>The Journal of Comparative Neurology</source><volume>420</volume><fpage>211</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(20000501)420:2&lt;211::AID-CNE5&gt;3.0.CO;2-3</pub-id><pub-id pub-id-type="pmid">10753308</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>DA</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Environmental statistics and the trade-off between model-based and TD learning in humans</chapter-title><person-group person-group-type="editor"><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name><name><surname>Zemel</surname><given-names>RS</given-names></name><name><surname>Bartlett</surname><given-names>PL</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><source>In Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates</publisher-name><fpage>127</fpage><lpage>135</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinclair</surname><given-names>AH</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Surprise and destabilize: prediction error influences episodic memory reconsolidation</article-title><source>Learning &amp; Memory</source><volume>25</volume><fpage>369</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1101/lm.046912.117</pub-id><pub-id pub-id-type="pmid">30012882</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Reinforcement learning: an introduction</article-title><source>IEEE Transactions on Neural Networks</source><volume>9</volume><elocation-id>1054</elocation-id><pub-id pub-id-type="doi">10.1109/TNN.1998.712192</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarder-Stoll</surname><given-names>H</given-names></name><name><surname>Jayakumar</surname><given-names>M</given-names></name><name><surname>Dimsdale-Zucker</surname><given-names>HR</given-names></name><name><surname>GÃ¼nseli</surname><given-names>E</given-names></name><name><surname>Aly</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamic internal states shape memory retrieval</article-title><source>Neuropsychologia</source><volume>138</volume><elocation-id>107328</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.107328</pub-id><pub-id pub-id-type="pmid">31887313</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Team SD</collab></person-group><year iso-8601-date="2020">2020</year><data-title>Stan reference manual</data-title><version designator="2.27">2.27</version><source>Stan Development Team</source><ext-link ext-link-type="uri" xlink:href="https://mc-stan.org/docs/2_27/reference-manual/index.html">https://mc-stan.org/docs/2_27/reference-manual/index.html</ext-link></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van Geen</surname><given-names>C</given-names></name><name><surname>Gerraty</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hierarchical Bayesian Models of Reinforcement Learning: Introduction and Comparison to Alternative Methods</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.10.19.345512</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vikbladh</surname><given-names>O</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Episodic contributions to model-based reinforcement learning</article-title><conf-name>Annual Conference on Cognitive Computational Neuroscience</conf-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voon</surname><given-names>V</given-names></name><name><surname>Derbyshire</surname><given-names>K</given-names></name><name><surname>RÃ¼ck</surname><given-names>C</given-names></name><name><surname>Irvine</surname><given-names>MA</given-names></name><name><surname>Worbe</surname><given-names>Y</given-names></name><name><surname>Enander</surname><given-names>J</given-names></name><name><surname>Schreiber</surname><given-names>LRN</given-names></name><name><surname>Gillan</surname><given-names>C</given-names></name><name><surname>Fineberg</surname><given-names>NA</given-names></name><name><surname>Sahakian</surname><given-names>BJ</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Harrison</surname><given-names>NA</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Grant</surname><given-names>JE</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Disorders of compulsivity: a common bias towards learning habits</article-title><source>Molecular Psychiatry</source><volume>20</volume><fpage>345</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1038/mp.2014.44</pub-id><pub-id pub-id-type="pmid">24840709</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Braun</surname><given-names>EK</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Episodic memory encoding interferes with reward learning and decreases striatal prediction errors</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>14901</fpage><lpage>14912</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0204-14.2014</pub-id><pub-id pub-id-type="pmid">25378157</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>BÃ¼chel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reactivation of Pain-Related Patterns in the Hippocampus from Single Past Episodes Relates to Successful Memory-Based Decision Making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.29.123893</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>HA</given-names></name><name><surname>Modirshanechi</surname><given-names>A</given-names></name><name><surname>Lehmann</surname><given-names>MP</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Herzog</surname><given-names>MH</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Novelty is not surprise: human exploratory and adaptive behavior in sequential decision-making</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009070</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009070</pub-id><pub-id pub-id-type="pmid">34081705</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>AH</given-names></name><name><surname>Collins</surname><given-names>AGE</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>How working memory and reinforcement learning are intertwined: a cognitive, neural, and computational perspective</article-title><source>Journal of Cognitive Neuroscience</source><volume>34</volume><fpage>551</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01808</pub-id><pub-id pub-id-type="pmid">34942642</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>A</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Expected and unexpected uncertainty: ACh and NE in the neocortex</article-title><conf-name>NIPSâ02: Proceedings of the 15th International Conference on Neural Information Processing Systems</conf-name><fpage>173</fpage><lpage>180</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>AJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Uncertainty, neuromodulation, and attention</article-title><source>Neuron</source><volume>46</volume><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.04.026</pub-id><pub-id pub-id-type="pmid">15944135</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Furukawa</surname><given-names>S</given-names></name><name><surname>Liao</surname><given-names>HI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pupil-linked phasic arousal evoked by violation but not emergence of regularity within rapid sound sequences</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4030</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12048-1</pub-id><pub-id pub-id-type="pmid">31492881</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Replication results</title><p>Here, we repeat and describe all analyses reported in the main text with replication sample. All results are reported in the same order as in the main text.</p></sec><sec sec-type="appendix" id="s9"><title>Episodic memory is used more under conditions of greater volatility</title><p>Participants in the replication sample were substantially more likely to choose high-valued old objects compared to low-valued old objects (<inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.723</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.624</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.827</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1A</xref>). Participants also altered their behavior in response to reversals in deck value. The higher-valued (lucky) deck was chosen more frequently on trials immediately preceding a reversal <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.095</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.016</mml:mn><mml:mo>,</mml:mo><mml:mn>0.176</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.128</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.047</mml:mn><mml:mo>,</mml:mo><mml:mn>0.213</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.168</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.085</mml:mn><mml:mo>,</mml:mo><mml:mn>0.251</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.161</mml:mn><mml:mo>,</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.075</mml:mn><mml:mo>,</mml:mo><mml:mn>0.25</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1B</xref>). This tendency was then disrupted by trails on which a reversal occurred (<inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.373</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>â</mml:mo><mml:mn>0.464</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>â</mml:mo><mml:mn>0.286</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>), with performance quickly recovering as the newly lucky deck became chosen more frequently on the trials following a reversal (<inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.256</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.337</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.175</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.144</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.22</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.064</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf103"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>: <inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.024</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.102</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.053</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.113</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.055</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.174</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>). Thus, participants in the replication sample were also sensitive to reversals in deck value, thereby indicating that they engaged in incremental learning throughout the task.</p><p>Participants in the replication sample also based more decisions on episodic value in the high-volatility environment compared to the low-volatility environment (<inline-formula><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.146</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.06</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mn>0.228</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1C</xref>). Furthermore, decisions based on episodic value again took longer (<inline-formula><mml:math id="inf107"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>39.445</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>29.660</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>49.328</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1D</xref>).</p></sec><sec sec-type="appendix" id="s10"><title>Uncertainty increases sensitivity to episodic value</title><p>In the replication sample, the reduced Bayesian model with two hazard rates was again the best-fitting model (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1A</xref>). Participants detected higher levels of volatility in the high- compared to the low-volatility environment, as indicated by the generally larger hazard rates recovered from the high- compared to the low-volatility environment (<inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.048</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.038</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.06</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.071</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.058</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.088</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1B</xref>). Compared to an average of the four trials prior to a reversal, RU also increased immediately following a reversal and stabilized over time (<inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.021</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.014</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.056</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.22</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.253</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>â</mml:mo><mml:mn>0.185</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.144</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.178</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>â</mml:mo><mml:mn>0.11</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.098</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.129</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>â</mml:mo><mml:mn>0.064</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>â</mml:mo><mml:mn>0.083</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>â</mml:mo><mml:mn>0.019</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1C</xref>). RU was again also, on average, greater in the high- compared to the low-volatility environment (<inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.007</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.013</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>) and related to reaction time such that choices made under more uncertain conditions took longer (<inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.364</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.407</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>2.338</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>).</p><p>Episodic memory was also used more on incongruent trial decisions made under conditions of high RU (<inline-formula><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.718</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1.096</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>4.436</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1A</xref>). We again fit the combined choice model to the replication sample and found the following. Participants again used both sources of value throughout the task: both deck value as estimated by the model (<inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.431</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.335</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.516</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1B</xref>) and the episodic value from old objects (<inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.191</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.137</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.245</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>) strongly impacted choice. Lastly, episodic value again impacted choices more when RU was high (<inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.043</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.00003</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.088</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>) and in the high- compared to the low-volatility environment (<inline-formula><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.092</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.047</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.136</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>).</p><p>Finally, there was again no relationship between the use of episodic memory on incongruent trial decisions and RU at encoding (<inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.642</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>2.576</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>). Including a sixth parameter to assess increased sensitivity to old object value due to RU at encoding time did not have an effect in the combined choice model (<inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.003</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.046</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.037</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> ; <xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>), which is also reported in the main text. As with the main sample, including this parameter did not provide a better fit to subjectsâ choices than the combined choice model with only increased sensitivity due to RU at retrieval time.</p></sec><sec sec-type="appendix" id="s11"><title>Episodic and incremental value sensitivity predicts subsequent memory performance</title><p>Participants in the replication sample again performed well above chance on the test of recognition memory (<inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.874</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1.772</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>1.977</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>), and objects from episodic choice trials were better remembered than those from incremental choice trials (<inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.157</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.033</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.278</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1A</xref>). Recall for the value of previously seen objects was also well predicted by their true value (<inline-formula><mml:math id="inf131"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.181</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.162</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.120</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>) and value recall was improved for objects from episodic choice trials (<inline-formula><mml:math id="inf132"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>:</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.049</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.030</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.067</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1B</xref>). Participants with better subsequent recognition memory were again more sensitive to episodic value (<inline-formula><mml:math id="inf133"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.334</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.229</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.44</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1C</xref>), and these same participants were again less sensitive to incremental value (<inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.124</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.238</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mo>-</mml:mo><mml:mn>0.009</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1D</xref>).</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s12"><title>Uncertainty during encoding improves subsequent memory in both samples</title><p>The subsequent memory task provided us with the opportunity to test whether participants have better subsequent memory for objects encoded under conditions of greater uncertainty. Supporting the notion that uncertainty improves subsequent memory, recognition memory for objects encoded in the high-volatility environment was better than for those encoded in the low-volatility environment (main: <inline-formula><mml:math id="inf135"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.053</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.009</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.098</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; replication: <inline-formula><mml:math id="inf136"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.078</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>0.031</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.126</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>). This coarse effect was limited to recognition memory, however, as memory for object value was less impacted by the environment in which it was seen (main: <inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.002</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.012</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.009</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>; replication: <inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.008</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi>%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>0.002</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.019</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>).</p><p>We next examined the impact of RU at encoding on subsequent memory. Both recognition memory (main: <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.129</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.022</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.241</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>; replication: <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.179</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.041</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.329</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and value memory (main: <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.012</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.023</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> ; replication: <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>:</mml:mo><mml:mi>R</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.012</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>95</mml:mn><mml:mi mathvariant="normal">%</mml:mi><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mn>0.023</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> ; <xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>) were associated with greater RU at encoding time. Lastly, we assessed how these effects of uncertainty at encoding compared to the effects of surprise, which is thought to also improve subsequent memory and is separately estimated by the RB model (see âSupplementary methodsâ). We found that surprise at encoding (quantified here as both the probability of a reversal in deck value and the absolute value of reward prediction error) led to modest improvement in subsequent memory, but these effects were less consistent across samples and types of memory (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>). Models of subsequent memory performance featuring surprise were also outperformed by those that instead predicted memory from RU. Together, these results indicate that the presence of uncertainty at encoding improves subsequent memory.</p></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s13"><title>Supplementary methods</title><sec sec-type="appendix" id="s13-1"><title>Description of incremental learning models</title><sec sec-type="appendix" id="s13-1-1"><title>RescorlaâWagner (RW)</title><p>The first model we considered was a standard model-free reinforcement learner that assumes a stored value (<inline-formula><mml:math id="inf143"><mml:mi>Q</mml:mi></mml:math></inline-formula>) for each deck is updated over time. <inline-formula><mml:math id="inf144"><mml:mi>Q</mml:mi></mml:math></inline-formula> is then referenced on each decision in order to guide choices. After each outcome <inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the value for the orange deck <inline-formula><mml:math id="inf146"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is updated according to the following rule (<xref ref-type="bibr" rid="bib64">Rescorla and Wagner, 1972</xref>) if the orange deck is chosen:<disp-formula id="equ16"><mml:math id="m16"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>And is not updated if the purple deck is chosen:<disp-formula id="equ17"><mml:math id="m17"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Likewise, the value for the purple deck <inline-formula><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is updated equivalently. Large differences between estimated value and outcomes therefore have a larger impact on updates, but the overall degree of updating is controlled by the learning rate, <inline-formula><mml:math id="inf148"><mml:mi>Î±</mml:mi></mml:math></inline-formula>. Two versions of this model were fit, one with a single learning rate (RW1Î±), and one with two learning rates (RW2Î±), <inline-formula><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , depending on which environment the current trial was completed in. These parameters are constrained to lie between 0 and 1. A separate learning rate was used for each environment in the (RW2Î±) version to capture the well-established idea that a higher learning rate should be used in more volatile conditions (<xref ref-type="bibr" rid="bib4">Behrens et al., 2007</xref>). A third RW model (RW1Q), also with two learning rates, was additionally fit to better match the property of the reduced Bayesian model (described below) in which anticorrelation between each deckâs value is assumed due to learning only a single value. This was accomplished by forcing the model to learn only one <inline-formula><mml:math id="inf151"><mml:mi>Q</mml:mi></mml:math></inline-formula>, where outcomes were coded in terms of the orange deck. For example, this means that an outcome worth $1 on the orange deck is treated the same as an outcome worth $0 on the purple deck by this model.</p></sec><sec sec-type="appendix" id="s13-1-2"><title>Reduced Bayesian (RB)</title><p>The second model we considered was the reduced Bayesian (RB) model developed by Nassar and colleagues (<xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>). This model tracks and updates its belief that the orange deck is lucky based on trial-wise outcomes, <inline-formula><mml:math id="inf152"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , using the following prediction error-based update:<disp-formula id="equ18"><mml:math id="m18"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>This update is identical to that used in the RW model; however, the learning rate <inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is itself updated following each outcome according to the following rule:<disp-formula id="equ19"><mml:math id="m19"><mml:msub><mml:mrow><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf154"><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability that a change in deck luckiness has occurred on the most recent trial (the CPP) and <inline-formula><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the imprecision in the modelâs belief about deck value (the RU). The learning rate therefore increases whenever CPP or RU increases. CPP can be written as<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">Î©</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>Ï</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf156"><mml:mi>H</mml:mi></mml:math></inline-formula> is the hazard rate or probability of a change in deck luckiness. Two versions of this model were fit, one with a single hazard rate (RB1<italic>H</italic>), and one with two hazard rates (RB2<italic>H</italic>), <inline-formula><mml:math id="inf157"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf158"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, depending on the environment the current trial was completed in. In this equation, the numerator represents the probability that an outcome was sampled from a new average deck value, whereas the denominator indicates the combined probability of a change and the probability that the outcome was generated by a Gaussian distribution centered around the most recent belief about deck luckiness and the variance of this distribution, <inline-formula><mml:math id="inf159"><mml:msup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. Because CPP is a probability, it is constrained to lie between 0 and 1. In our implementation, <inline-formula><mml:math id="inf160"><mml:mi>H</mml:mi></mml:math></inline-formula> was a free parameter (see âPosterior inferenceâ section below) and <inline-formula><mml:math id="inf161"><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was initialized to 1.</p><p>RU, which is the uncertainty about deck value relative to the amount of noise in the environment, is quite similar to the Kalman gain used in Kalman filtering:<disp-formula id="equ21"><mml:math id="m21"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula><disp-formula id="equ22"><mml:math id="m22"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf162"><mml:msup><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is the observation noise and was here fixed to the true observation noise (0.33). <inline-formula><mml:math id="inf163"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> consists of three terms: the first is the variance of the deck value distribution conditional on a change point, the second is the variance of the deck value distribution conditional on no change, and the third is the variance due to the difference in means between these two distributions. These terms are then used in the equation for <inline-formula><mml:math id="inf164"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to provide the uncertainty about whether an outcome was due to a change in deck value or the noise in observations that is expected when a change point has not occurred. Because this model does not follow the two-armed bandit assumption of our task (i.e., that outcomes come from two separate decks), all outcomes were coded in terms of the orange deck, as in the RW1Q model described above. While this description represents a brief overview of the critical equations of the reduced Bayesian model, a full explanation can be found in <xref ref-type="bibr" rid="bib53">Nassar et al., 2010</xref>.</p></sec><sec sec-type="appendix" id="s13-1-3"><title>Softmax choice</title><p>All incremental learning models were paired with a softmax choice function in order to predict participantsâ decisions on each trial:<disp-formula id="equ23"><mml:math id="m23"><mml:msub><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf165"><mml:msub><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability that the orange deck was chosen on trial <inline-formula><mml:math id="inf166"><mml:mi>t</mml:mi></mml:math></inline-formula>. This function also consists of two inverse temperature parameters: <inline-formula><mml:math id="inf167"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to model an intercept and <inline-formula><mml:math id="inf168"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to model the slope of the decision function related to deck value. The primary difference for each model was how <inline-formula><mml:math id="inf169"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is computed: RW (<inline-formula><mml:math id="inf170"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>); RB (<inline-formula><mml:math id="inf171"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>); RW1Q (<inline-formula><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). In each of these cases, a positive <inline-formula><mml:math id="inf173"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicates evidence that the orange deck is more valuable while a negative <inline-formula><mml:math id="inf174"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicates evidence that the purple deck is more valuable.</p></sec><sec sec-type="appendix" id="s13-1-4"><title>Posterior inference</title><p>For all incremental learning models, the likelihood function can be written as<disp-formula id="equ24"><mml:math id="m24"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î¸</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf175"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is 1 if subject <inline-formula><mml:math id="inf176"><mml:mi>s</mml:mi></mml:math></inline-formula> chose the orange deck on trial <inline-formula><mml:math id="inf177"><mml:mi>t</mml:mi></mml:math></inline-formula> and 0 if purple was chosen. Following the recommendations of <xref ref-type="bibr" rid="bib27">Gelman and Hill, 2006</xref> and <xref ref-type="bibr" rid="bib79">van Geen and Gerraty, 2021</xref>, <inline-formula><mml:math id="inf178"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is drawn from a multivariate normal distribution with mean vector <inline-formula><mml:math id="inf179"><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and covariance matrix <inline-formula><mml:math id="inf180"><mml:msub><mml:mrow><mml:mi>Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> :<disp-formula id="equ25"><mml:math id="m25"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf181"><mml:msub><mml:mrow><mml:mi>Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is decomposed into a vector of coefficient scales <inline-formula><mml:math id="inf182"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and a correlation matrix <inline-formula><mml:math id="inf183"><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> via<disp-formula id="equ26"><mml:math id="m26"><mml:msub><mml:mrow><mml:mi>Î£</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>Weakly informative hyperpriors were then set on the hyperparameters <inline-formula><mml:math id="inf184"><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf185"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> :<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ28"><mml:math id="m28"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mn>0,2.5</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ29"><mml:math id="m29"><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi><mml:mi>J</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>These hyperpriors were chosen for their respective desirable properties: the half Cauchy is bounded at zero and has a relatively heavy tail that is useful for scale parameters, the LKJ prior with shape = 2 concentrates some mass around the unit matrix, thereby favoring less correlation (<xref ref-type="bibr" rid="bib47">Lewandowski et al., 2009</xref>), and the normal is a standard choice for regression coefficients.</p><p>Because sampling from heavy-tailed distributions like the Cauchy is difficult for Hamiltonian Monte Carlo (<xref ref-type="bibr" rid="bib78">Team SD, 2020</xref>), a reparameterization of the Cauchy distribution was used here. <inline-formula><mml:math id="inf186"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was thereby defined as the transform of a uniformly distributed variable <inline-formula><mml:math id="inf187"><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>_</mml:mo><mml:mi>u</mml:mi></mml:math></inline-formula> using the Cauchy inverse cumulative distribution function such that<disp-formula id="equ30"><mml:math id="m30"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>_</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>_</mml:mo><mml:mi>u</mml:mi><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ31"><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">_</mml:mi><mml:mi>u</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In addition, a multivariate noncentered parameterization specifying the model in terms of the Cholesky factorized correlation matrix was used in order to shift the dataâs correlation with the parameters to the hyperparameters, which increases the efficiency of sampling the parameters of hierarchical models (<xref ref-type="bibr" rid="bib78">Team SD, 2020</xref>). The full correlation matrix <inline-formula><mml:math id="inf188"><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was replaced with a Cholesky factorized parameter <inline-formula><mml:math id="inf189"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> such that<disp-formula id="equ32"><mml:math id="m32"><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></disp-formula><disp-formula id="equ33"><mml:math id="m33"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>)</mml:mo><mml:mo>Ã</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>Ã</mml:mo><mml:mi>z</mml:mi><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula><disp-formula id="equ34"><mml:math id="m34"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Î©</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>L</mml:mi><mml:mi>K</mml:mi><mml:mi>J</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:math></disp-formula><disp-formula id="equ35"><mml:math id="m35"><mml:mrow><mml:mi>z</mml:mi><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where multiplying the Cholesky factor of the correlation matrix by the standard normally distributed additional parameter <inline-formula><mml:math id="inf190"><mml:mi>z</mml:mi></mml:math></inline-formula> and adding the group mean <inline-formula><mml:math id="inf191"><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> creates a <inline-formula><mml:math id="inf192"><mml:msub><mml:mrow><mml:mi>Î²</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> vector distributed identically to the original model.</p><p>While the choice function is identical for each model, the parameters used in generating deck value differ for each. All were fit hierarchically and were modeled with the following priors and hyperpriors:</p><p>RescorlaâWagner with a single learning rate (RW1Î±):<disp-formula id="equ36"><mml:math id="m36"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>Î±</mml:mi><mml:mo>â¼</mml:mo><mml:mi>Î²</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mn>2</mml:mn><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>RescorlaâWagner with two learning rates (RW2Î±) and with one Q-value (RW1Q):<disp-formula id="equ37"><mml:math id="m37"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>Î²</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>Î²</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Reduced Bayes with a single hazard rate (RB1<italic>H</italic>):<disp-formula id="equ38"><mml:math id="m38"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>H</mml:mi><mml:mo>â¼</mml:mo><mml:mi>Î²</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Reduced Bayes with two hazard rates (RB2<italic>H</italic>):<disp-formula id="equ39"><mml:math id="m39"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>Î²</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>Î²</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s13-1-5"><title>Description of contextual inference model</title><p>Because of the structure of our task, one possibility is that participants did not engage in incremental learning, but instead inferred which one of two switching contexts they were in (either that the orange deck was lucky and the purple deck was unlucky or vice versa). To address this, we developed a contextual inference (CI) model based on a standard hidden Markov model (HMM) with two latent states. While HMMs are covered extensively elsewhere (<xref ref-type="bibr" rid="bib63">Rabiner and Juang, 1986</xref>), we provide the following brief overview. The model assumes that each outcome, <inline-formula><mml:math id="inf193"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , was generated by a hidden state, <inline-formula><mml:math id="inf194"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , which may take one of two values on each trial, <inline-formula><mml:math id="inf195"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:mo>[</mml:mo><mml:mn>1,2</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>. The goal of the model is then to infer which of the two states gave rise to each outcome on each trial using the following generative model:<disp-formula id="equ40"><mml:math id="m40"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf196"><mml:mi>Î¼</mml:mi><mml:mo>â</mml:mo><mml:mo>[</mml:mo><mml:mn>1,2</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, and <inline-formula><mml:math id="inf197"><mml:mi>Î¸</mml:mi></mml:math></inline-formula> is a 2 Ã 2 transition matrix. Here, we assume that each outcome is normally distributed with a known scale parameter and unknown location parameters, <inline-formula><mml:math id="inf198"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. The state variable follows a categorical distribution parameterized by <inline-formula><mml:math id="inf199"><mml:mi>Î¸</mml:mi></mml:math></inline-formula>, which determines the likelihood that, on a given trial, each state will transition to either the other state or itself. Here, <inline-formula><mml:math id="inf200"><mml:mi>Î¸</mml:mi></mml:math></inline-formula> was modeled separately for each environment to mirror the difference in volatility between environments. Î¼ and Î¸ were then fit as free parameters for each participant using Hamiltonian Monte Carlo, following recommendations for fitting HMMs in Stan (<xref ref-type="bibr" rid="bib78">Team SD, 2020</xref>). The following priors were used for each parameter:<disp-formula id="equ41"><mml:math id="m41"><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Î¸</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>â¼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf201"><mml:mi>Ï</mml:mi></mml:math></inline-formula> is the true standard deviation of outcomes, and <inline-formula><mml:math id="inf202"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf203"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the true expected values of the lucky and unlucky decks, respectively.</p><p>We then calculated the likelihood of each participantâs sequence of outcomes using the forward algorithm to compute the following marginalization:<disp-formula id="equ42"><mml:math id="m42"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>o</mml:mi><mml:mo>|</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>,</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>,</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>Upon estimating the parameters, the most probable sequence of states to have generated the observed outcomes was computed using the Viterbi algorithm. Assigning a state to each timepoint allowed us to make use of the assigned stateâs Î¼ as the expected state value for the timepoint. This was then treated as the deck value for further analyses, as for the incremental learning models. Lastly, outcomes were coded similarly to the RB and RW1Q models.</p></sec></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81679.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.07.05.498877" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.05.498877"/></front-stub><body><p>This paper posits that higher uncertainty environments should lead to more reliance on episodic memory, finding compelling evidence for this idea across several analysis approaches and across two independent samples. This is an important paper that will be of interest to a broad group of learning, memory, and decision-making researchers.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81679.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.07.05.498877">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.07.05.498877v2">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Uncertainty alters the balance between incremental learning and episodic memory&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>All of the reviewers felt that this was a promising paper with compelling results, but they also raised a number of questions about the methodology and interpretation of the results that should be addressed in a revision, as detailed below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The wide vs. skinny error bars are very difficult to visually differentiate. I recommend a more obvious difference.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. 40-45% of the participants are excluded from the analysis in the main and replication samples. The authors should clarify how many were excluded for each criterion. Was the main culprit whether people responded to the volatility manipulation in the original deck learning task? If so, what does this mean about the generality of the effects of uncertainty in incremental learning? I suspect this may be due to the relative attentiveness of online participants, but the authors should address the caveat in the text. Some aspects of these excluded data may still be relevant. For example, if these participants do not register any differences in uncertainty between the two environments, wouldn't the prediction be that their use of episodic memory also does not differ?</p><p>2. Some aspects of the methods were not clear.</p><p>a. Was the order of the high and low volatility blocks counterbalanced across participants? Was the order the same in the first deck learning and second deck learning + card memory tasks? Were participants told explicitly that the environments would carry over between the two tasks? (The last point would further support using estimates fit to the first task out of the sample in the second.)</p><p>b. How individual objects were re-sampled was described in the overview (lines 435-439), but I imagine the details will matter a lot to people interested in replicating and extending this work. It would help to point the interested reader to where these could be found (eg, is the code provided, or is it based on a previous study described in detail, etc.).</p><p>3. Given that the results precede the methods, there are some aspects of the task that would be helpful to explain at the outset (around Figure 1). I was initially confused because the outcome in Figure 1 is $1 but the value memory was on a scale of 0-100, but this could be cleared up with a sentence about the possible outcomes. It would also be helpful to mention the mean outcome on the lucky versus unlucky deck, how frequently the lucky deck changes, and what participants are told explicitly about the volatility manipulation. This could be done in the text or with a revision to Figure 1. I was also confused about how the two samples were used until the end of the Results section (are they being analyzed together or separately? What am I looking at in Figures 2-5?). Again, a well-placed sentence at the top of the Results section would clear this up.</p><p>4. In Figure 3, I think a slightly different comparison would be useful, in addition to or instead of the two Rescorla-Wagner models. One difference between the reduced Bayesian and RW models is that the learning rate is dynamic in the RB model but not the RW model. But another difference between the way the two models are implemented is that the RB model assumes the value of the two decks are perfectly anti-correlated (ie, it is learning only one value estimate), while the RW model does not (ie, it is learning about the two decks independently). Thus, the RB model assumes a critical aspect of the structure of the task that the RW model does not. I doubt this difference completely accounts for its better performance, but this should be tested. A Î´-rule model with a fixed learning rate that learns a single value estimate (like the RB model) would be the needed comparison. This comparison would also isolate the effect of including the dynamic learning rate (according to RU and CPP) in the model.</p><p>5. The discussion goes into the different effects of novelty, surprise, and uncertainty on subsequent memory (lines 349-364), in the context of the lack of effect of uncertainty (RU from the reduced Bayesian model) at encoding. But have the authors looked at the effect of surprise (changepoint probability in the reduced Bayesian model) at encoding? The previous studies discussed here would predict that surprise at encoding should enhance subsequent memory (and perhaps the use of episodic memory in choice). This point is not central to the manuscript, of course, but the authors have additional data relevant to the distinctions they are raising here.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>As mentioned in the public review, I thought this was a very interesting study and the results were clearly communicated. However, I have a number of questions/recommendations for the authors to strengthen the results and interpretation.</p><p>Regarding the points made in the public review about uncertainty v volatility and context, I'd make the following recommendations:</p><p>(1) Uncertainty vs. volatility (described in public review): it would make sense to reframe results around volatility rather than uncertainty, or strengthen the trial-wise RU analyses to look at trial-wise RU only at trials far away from reversals. At least there should be some discussion of where uncertainty arises besides volatility.</p><p>(2) Context: I think the analyses would be strengthened with an additional model using context inference rather than incremental learning. A natural choice might be Gershman and Niv 2012, although you could possibly get away with something simpler if you assume 2 contexts.</p><p>(3) The focus on incongruent trials seems potentially thorny. It is intuitive why the authors do this: trials in which episodic and incremental values disagree are informative about which normative strategy the subjects are using. However, in the high volatility condition, if subjects are using incremental value, they may be more likely to have an outdated incremental value which would look consistent with the episodic choice. I would propose the authors look at congruent trials as well to confirm that they are indeed less likely to make errors on these trials in the high volatility condition than they are in the low volatility condition.</p><p>(4) Another question relates to the interpretation of competing for episodic v. incremental strategies, as opposed to just learning about independent features. One could argue that the subjects are doing instrumental learning over objects and colors separately, and when the reliability of one feature (color) is decremented, the other feature is relatively up-weighted. This also seems consistent with the fact that episodic and incremental learning tradeoff -- attending more to the object feature would perhaps compete with color.</p><p>(5) The authors show that uncertainty at encoding time does not have a discernible effect on the episodic index. This is evidence that volatility is modulating episodic contribution to decision-making, rather than encoding strength, which is a pretty fundamental part of the results (and in contrast to eg Sun et al. 2021, where unpredictability modulates episodic consolidation). One key thing to look at is if subjects show any difference in their ability to recall familiarity/value of objects from the different conditions. This would also speak to the question of if volatility is affecting encoding rather than just recall during decision-making. It would also make sense to look at the role of other variables at encoding time (eg prediction error) to see if these predict future use. It would also be interesting to see if subjects are storing the object value or the incremental value at the time the object was first shown -- this would be easy to check (when subjects rate the value in the last block, are they more likely to err in the direction of the incremental value at the time of encoding (eg like Figure 2A but with x-axis = incremental value at the time estimated with RW)). This would shed insight into exactly what kind of episodic strategy the subjects are deploying.</p><p>(6) The analysis showing that subjects that were better at recall in block 3 also had higher episodic index was a useful sanity check. It seems it would also be possible to perform this analysis within-subject (eg does episodic choice correlate with accurate value memory) and that would bear more on the question of whether it was uncertainty or simply a subjective preference for one strategy or another.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81679.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The wide vs. skinny error bars are very difficult to visually differentiate. I recommend a more obvious difference.</p></disp-quote><p>Thank you. We agree that it was difficult to differentiate between the 80% and 95% posterior intervals that were plotted around group-level estimates. This is largely because there was little difference between these intervals on the scale that we are plotting in order to visualize individual subject estimates in addition to group-level estimates. In the revision, we have altered figures that previously had both intervals (e.g. Figure 3B and Figure 4B) to have only 95% posterior intervals, as these are more informative and in line with what is reported throughout the rest of the paper. We have additionally changed the visualization of group-level estimates in Figure 4B from lines to bars in order to more explicitly differentiate between how error and estimates are visualized.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. 40-45% of the participants are excluded from the analysis in the main and replication samples. The authors should clarify how many were excluded for each criterion. Was the main culprit whether people responded to the volatility manipulation in the original deck learning task? If so, what does this mean about the generality of the effects of uncertainty in incremental learning? I suspect this may be due to the relative attentiveness of online participants, but the authors should address the caveat in the text. Some aspects of these excluded data may still be relevant. For example, if these participants do not register any differences in uncertainty between the two environments, wouldn't the prediction be that their use of episodic memory also does not differ?</p></disp-quote><p>Thank you for giving us the opportunity to clarify. We have now added how many participants were excluded due to insensitivity to the volatility manipulation or for their general performance during the second task in the text (lines 603-605 and 786-788) and have included in the Discussion a new paragraph focused on the nature of our online sample and some participantsâ insensitivity to the volatility manipulation (lines 477-485). As suggested, we have also verified that the participants excluded due to their insensitivity to the volatility manipulation were indeed less affected by environment when making episodic-based choices. We repeated the same analysis as in the paper of the effect of environment on these participantsâ episodic-based choice index. In both the main (Î²=0.087, 95% <italic>CI</italic> = [â0.112, 0.182]) and replication (Î² = 0.087, 95% <italic>CI</italic> = [0.000, 0.168]) samples, there was a reduced and less reliable effect of environment on choice type in these excluded participants compared to the included participants.</p><disp-quote content-type="editor-comment"><p>2. Some aspects of the methods were not clear.</p></disp-quote><p>Thank you for these suggestions of places where the methods could be made more clear. We have made several changes to address these points, as noted below:</p><disp-quote content-type="editor-comment"><p>a. Was the order of the high and low volatility blocks counterbalanced across participants? Was the order the same in the first deck learning and second deck learning + card memory tasks? Were participants told explicitly that the environments would carry over between the two tasks? (The last point would further support using estimates fit to the first task out of the sample in the second.)</p></disp-quote><p>The order in which participants saw the two environments was counterbalanced across participants for the deck learning and card memory task. We have clarified this in the paper (lines 139 and 523-524). In the deck learning task, all participants first saw the low volatility environment and then saw the high volatility environment. This decision was made in order to emphasize the increased volatility of the high volatility environment relative to the low volatility environment, and this information has been added to the methods (lines 547-549). Lastly, participants were indeed told explicitly that the volatility levels of each environment would carry over from one task to the other, and we have also added this to the methods (lines 550-552). Thank you for catching that this information was not included in the methods.</p><disp-quote content-type="editor-comment"><p>b. How individual objects were re-sampled was described in the overview (lines 435-439), but I imagine the details will matter a lot to people interested in replicating and extending this work. It would help to point the interested reader to where these could be found (eg, is the code provided, or is it based on a previous study described in detail, etc.).</p></disp-quote><p>The sampling procedure has now been described in detail in the methods (lines 533-542).</p><disp-quote content-type="editor-comment"><p>3. Given that the results precede the methods, there are some aspects of the task that would be helpful to explain at the outset (around Figure 1). I was initially confused because the outcome in Figure 1 is $1 but the value memory was on a scale of 0-100, but this could be cleared up with a sentence about the possible outcomes. It would also be helpful to mention the mean outcome on the lucky versus unlucky deck, how frequently the lucky deck changes, and what participants are told explicitly about the volatility manipulation. This could be done in the text or with a revision to Figure 1. I was also confused about how the two samples were used until the end of the Results section (are they being analyzed together or separately? What am I looking at in Figures 2-5?). Again, a well-placed sentence at the top of the Results section would clear this up.</p></disp-quote><p>Thank you. We have clarified this information in multiple places throughout the text prior to the methods. In the first two paragraphs of the Results, we have added information on how often reversals occurred (lines 103-104), what participants were told (lines 105-109) and the expected value of each deck (lines 105-106). We have further added information about the types and range of outcomes to the caption of Figure 1 (lines 132-133) and information about where the main and replication sample results are reported in the introduction (lines 70-72).</p><disp-quote content-type="editor-comment"><p>4. In Figure 3, I think a slightly different comparison would be useful, in addition to or instead of the two Rescorla-Wagner models. One difference between the reduced Bayesian and RW models is that the learning rate is dynamic in the RB model but not the RW model. But another difference between the way the two models are implemented is that the RB model assumes the value of the two decks are perfectly anti-correlated (ie, it is learning only one value estimate), while the RW model does not (ie, it is learning about the two decks independently). Thus, the RB model assumes a critical aspect of the structure of the task that the RW model does not. I doubt this difference completely accounts for its better performance, but this should be tested. A Î´-rule model with a fixed learning rate that learns a single value estimate (like the RB model) would be the needed comparison. This comparison would also isolate the effect of including the dynamic learning rate (according to RU and CPP) in the model.</p></disp-quote><p>Thank you for your idea to include this model, as we agree that it helps to isolate exactly how the RB model improves over the RW models. We added a model that implements a Î´ rule identical to the RW models, but with a single Q-value (labeled as RW1Q in the text). Like the RB model, this model assumes that the value of the two decks are perfectly anti-correlated, and learns over outcomes that have been re-coded in terms of the orange deck (e.g. $1 on orange is treated equivalently to $0 on blue). This model is now described in the Results of the main text (lines 228-230), listed in the Methods (line 675), and explained in detail in Appendix 3. Using a procedure identical to how the other models were fit and compared, we found that this model performed worse than both the RB and RW models we had previously presented in both samples, suggesting that the dynamic learning rate used in the RB model does indeed account for its performance improvements. The results of this comparison are reflected in an updated Figure 3A.</p><disp-quote content-type="editor-comment"><p>5. The discussion goes into the different effects of novelty, surprise, and uncertainty on subsequent memory (lines 349-364), in the context of the lack of effect of uncertainty (RU from the reduced Bayesian model) at encoding. But have the authors looked at the effect of surprise (changepoint probability in the reduced Bayesian model) at encoding? The previous studies discussed here would predict that surprise at encoding should enhance subsequent memory (and perhaps the use of episodic memory in choice). This point is not central to the manuscript, of course, but the authors have additional data relevant to the distinctions they are raising here.</p></disp-quote><p>This is a great suggestion, thank you. We agree that our data can provide additional insights into the effects of novelty, surprise, and uncertainty at the time of encoding on subsequent memory and episodic-memory based choice. While previously we had only investigated effects of relative uncertainty (RU) at encoding on the use of episodic memory for decisions, we additionally looked at the effects of changepoint probability (CPP) and absolute prediction error (the absolute value of prediction error; APE), which are both potential markers of surprise at the time of encoding on episodic choices. Similar to the effects of RU at encoding time reported in the Results of the main text, there was no effect of CPP (Main: Î² = 0.044, 95% <italic>CI</italic> = [â0.004, 0.092]; Replication: Î² = 0.004, 95% <italic>CI</italic> = [â0.04, 0.048]) in either sample. There was an effect of APE at encoding in the main sample (Î² = 0.1, 95% <italic>CI</italic> = [0.039, 0.165]), but this effect did not replicate (Î² = 0.056, 95% <italic>CI</italic> = [â0.013, 0.123]). Based on this, our original conclusion about the effects of these variables at encoding time on episodic-based choice remains unchanged.</p><p>In addition, based on your suggestion here along with Reviewer Threeâs fifth recommendation below, we also looked at the effects of RU, CPP, and APE on participantsâ performance on the subsequent memory test. Because these variables are, by definition, highly correlated with one another (e.g. at encoding RU and CPP are correlated with r=0.827), we fit multiple mixed effects regression models predicting either recognition memory (hits or misses) or value memory (response between $0-$1) for objects from each variable separately. We then performed a 20Fold leave-N-subjects out cross validation procedure to compare these models in order to determine which provided the best prediction of subsequent memory. This information is now provided in the Methods (lines 742-751 and 761-765) and the results are now mentioned in the main text (lines 342-347) and reported in Appendix 2, and in a new supplementary figure (Figure 5âFigure supplement 2). In brief, only RU at encoding time had an effect on both recognition and value memory in both samples. Specifically, higher RU at encoding predicted greater subsequent memory. Further, in both the main and replication samples, both recognition and value memory were best predicted by RU. We have now amended our discussion of the effects of surprise and uncertainty on subsequent memory to incorporate these results (lines 414-419).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>As mentioned in the public review, I thought this was a very interesting study and the results were clearly communicated. However, I have a number of questions/recommendations for the authors to strengthen the results and interpretation.</p><p>Regarding the points made in the public review about uncertainty v volatility and context, I'd make the following recommendations:</p><p>(1) Uncertainty vs. volatility (described in public review): it would make sense to reframe results around volatility rather than uncertainty, or strengthen the trial-wise RU analyses to look at trial-wise RU only at trials far away from reversals. At least there should be some discussion of where uncertainty arises besides volatility.</p></disp-quote><p>Thank you for this suggestionâwe would like to expand on our response in the Public Review to elaborate on the variant we pursued of the specific analysis suggested here. Our understanding is that the main issue here is whether the results really are mediated by uncertainty rather than reflecting some effect of blockwise volatility other than through its dynamic effects on uncertainty. We agree with the reviewer that the trialwise RU analyses, if correctly done, could provide additional supporting evidence on this point, because the timeseries of trial-wise posterior uncertainty reflects many finer details of uncertainty, even within a block, than the cruder high-vs-low blockwise condition variable. But we think the main issue here is distinguishing blockwise effects from trialwise dynamics rather than, within trialwise effects, the dynamic effect on uncertainty of each individual ground-truth reversal event vs. other (i.i.d.) outcome variability. We are not clear what would be revealed by parsing out response to reversal events vs other noisy outcomes, since the inferential issue in this type of design (from the subjectsâ perspective) is precisely that they canât be reliably distinguished. Thus, even far from a reversal, an ideal observer will have higher posterior uncertainty in a highvolatility block due to a higher expected hazard of reversal, so even there these effects are intertwined.</p><p>Accordingly, to address one take on this, we added an additional effect of the interaction between the environment and episodic value to our combined choice model. This allowed us to look at, separately, participantsâ tendency to modulate their reliance on episodic memory in response to volatility (as captured by our categorical environment variable) and in response to trialwise fluctations in posterior uncertainty (as captured by relative uncertainty), in the same model. After doing this, we found that both the environment and relative uncertainty increased sensitivity to episodic value. These changes and results are described in the Methods (lines 686-694) and Results (lines 276-277; Figure 4C).</p><disp-quote content-type="editor-comment"><p>(2) Context: I think the analyses would be strengthened with an additional model using context inference rather than incremental learning. A natural choice might be Gershman and Niv 2012, although you could possibly get away with something simpler if you assume 2 contexts.</p></disp-quote><p>Thank you for this suggestion. We agree that including another model to capture context inference would substantially strengthen the paper. As mentioned in our response to the related question raised in the Public Review, we have addressed this point using a hidden Markov model with two states. While the model used in Gershman and Niv, 2012 solves a similar problem, it uses a Chinese Restaurant Process to infer the total number of hidden contexts. We think it is unlikely that the participants in our task engaged in inference over the total number of contexts as they were explicitly informed that each deck could be either lucky or unlucky at a given time (essentially informing them that there were only two contexts in this task). Further, this model was developed for binary outcomes, whereas the outcomes used in our task range between $0 and $1.</p><disp-quote content-type="editor-comment"><p>(3) The focus on incongruent trials seems potentially thorny. It is intuitive why the authors do this: trials in which episodic and incremental values disagree are informative about which normative strategy the subjects are using. However, in the high volatility condition, if subjects are using incremental value, they may be more likely to have an outdated incremental value which would look consistent with the episodic choice. I would propose the authors look at congruent trials as well to confirm that they are indeed less likely to make errors on these trials in the high volatility condition than they are in the low volatility condition.</p></disp-quote><p>Thank you for raising this issue; we agree that it is important to disambiguate episodic-based choices from noisy choices. This point is related to Reviewer Oneâs first Public Review suggestion, and our solution is described in detail in our response there. In brief, we first assessed the extent to which each subject made noisier choices in the high volatility compared to the low volatility environment and then controlled for this in our analysis of episodic-based choice between environments. The effect of environment was similar to that originally reported in the manuscript following this adjustment. The reported effects (lines 178 and Appendix 1) and methods (lines 643-655) have been updated to reflect these changes.</p><disp-quote content-type="editor-comment"><p>(4) Another question relates to the interpretation of competing for episodic v. incremental strategies, as opposed to just learning about independent features. One could argue that the subjects are doing instrumental learning over objects and colors separately, and when the reliability of one feature (color) is decremented, the other feature is relatively up-weighted. This also seems consistent with the fact that episodic and incremental learning tradeoff -- attending more to the object feature would perhaps compete with color.</p></disp-quote><p>We agree that this is a possible interpretation of our taskâfor the purposes of this study, we operationalized incremental learning as a repeated feature and episodic memory as a trialunique feature, but future work can be done to more directly implicate each of these memory systems in a task that allows for them to trade off. We have added a paragraph to the paper discussing and responding to this point in more detail (lines 447-461).</p><disp-quote content-type="editor-comment"><p>(5) The authors show that uncertainty at encoding time does not have a discernible effect on the episodic index. This is evidence that volatility is modulating episodic contribution to decision-making, rather than encoding strength, which is a pretty fundamental part of the results (and in contrast to eg Sun et al. 2021, where unpredictability modulates episodic consolidation). One key thing to look at is if subjects show any difference in their ability to recall familiarity/value of objects from the different conditions. This would also speak to the question of if volatility is affecting encoding rather than just recall during decision-making. It would also make sense to look at the role of other variables at encoding time (eg prediction error) to see if these predict future use. It would also be interesting to see if subjects are storing the object value or the incremental value at the time the object was first shown -- this would be easy to check (when subjects rate the value in the last block, are they more likely to err in the direction of the incremental value at the time of encoding (eg like Figure 2A but with x-axis = incremental value at the time estimated with RW)). This would shed insight into exactly what kind of episodic strategy the subjects are deploying.</p></disp-quote><p>Thank you for these suggestions, as we agree that there are many other opportunities for analysis of the subsequent memory data. We now expand these analyses, as detailed below as well as in response to Reviewer Two (point five, above). First, we looked at the effects of RU, change-point probability (CPP), and absolute prediction error (APE) at encoding time on both subsequent recognition and value memory (lines 342-347 and Appendix 2).</p><p>In addition, based on your other points here, we also performed several other analyses of the subsequent memory data. We first looked at whether subsequent memory differed depending on whether an object was seen in either the low or high volatility environment. For recognition memory, this analysis consisted of calculating the signal detection metric d-prime for objects seen in each environment and testing for a difference in performance. For value memory, we tested for the presence of an interaction between an objectâs true value and the environment in which it appeared on the value that was remembered by each participant. While environment did not impact value memory, recognition memory performance was better for objects seen in the high compared to the low volatility environment, suggesting that greater volatility at encoding time improved subsequent recall. These analyses are now included in the updated Methods (lines 732-751) and Results (lines 308-318) and Appendix 2. Lastly, based on your final point, we additionally looked at whether participants were more sensitive to episodic or incremental value at the time of encoding when reporting their remembered value for objects and found that object value (Main: Î² = 0.173, 95% <italic>CI</italic> = [0.159, 0.187]; Replication: Î² = 0.183, 95% <italic>CI</italic> = [0.168, 0.197]) was a substantially stronger predictor than incremental value (Main: Î² = 0.012, 95% <italic>CI</italic> = [0.001, 0.024]; Replication: Î² = 0.014, 95% <italic>CI</italic> = [0.002, 0.026]) in both samples, thereby suggesting that episodic value was more likely to drive these memory responses.</p><disp-quote content-type="editor-comment"><p>(6) The analysis showing that subjects that were better at recall in block 3 also had higher episodic index was a useful sanity check. It seems it would also be possible to perform this analysis within-subject (eg does episodic choice correlate with accurate value memory) and that would bear more on the question of whether it was uncertainty or simply a subjective preference for one strategy or another.</p></disp-quote><p>Thank you for this idea, which complements Reviewer Oneâs Public Review suggestion to sort recognition memory trials by whether the object was from episodic- or incremental-choice trials, where we found that participants have greater recognition memory for objects from episodicbased choices. We have additionally performed the within-subject analysis you suggested here by looking at whether participants better remember the value of objects from episodic-based choice trials. To do this, we fit a mixed effects linear regression predicting each participantâs subsequent memory value response from the interaction between choice type and an objectâs true value (lines 752-758). We found that, in both samples, participants better remembered the value of objects from episodic-based choices. This effect is now reported in the Results (lines 308-318) and appears as a new panel in Figure 5 (Figure 5B).</p></body></sub-article></article>