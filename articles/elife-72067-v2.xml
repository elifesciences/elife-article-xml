<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72067</article-id><article-id pub-id-type="doi">10.7554/eLife.72067</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Shallow neural networks trained to detect collisions recover features of visual loom-selective neurons</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-246961"><name><surname>Zhou</surname><given-names>Baohua</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2627-9447</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-246963"><name><surname>Li</surname><given-names>Zifan</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-246962"><name><surname>Kim</surname><given-names>Sunnie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8901-7233</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-246960"><name><surname>Lafferty</surname><given-names>John</given-names></name><email>john.lafferty@yale.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-27474"><name><surname>Clark</surname><given-names>Damon A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8487-700X</contrib-id><email>damon.clark@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Molecular, Cellular and Developmental Biology, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Statistics and Data Science, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Wu Tsai Institute, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Interdepartmental Neuroscience Program, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Physics, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v76x132</institution-id><institution>Department of Neuroscience, Yale University</institution></institution-wrap><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Facebook, Menlo Park, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>Department of Computer Science, Princeton University, Princeton, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>13</day><month>01</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e72067</elocation-id><history><date date-type="received" iso-8601-date="2021-07-08"><day>08</day><month>07</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-01-11"><day>11</day><month>01</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-07-08"><day>08</day><month>07</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.07.07.451307"/></event></pub-history><permissions><copyright-statement>© 2022, Zhou et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Zhou et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72067-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-72067-figures-v2.pdf"/><abstract><p>Animals have evolved sophisticated visual circuits to solve a vital inference problem: detecting whether or not a visual signal corresponds to an object on a collision course. Such events are detected by specific circuits sensitive to visual looming, or objects increasing in size. Various computational models have been developed for these circuits, but how the collision-detection inference problem itself shapes the computational structures of these circuits remains unknown. Here, inspired by the distinctive structures of LPLC2 neurons in the visual system of <italic>Drosophila</italic>, we build anatomically-constrained shallow neural network models and train them to identify visual signals that correspond to impending collisions. Surprisingly, the optimization arrives at two distinct, opposing solutions, only one of which matches the actual dendritic weighting of LPLC2 neurons. Both solutions can solve the inference problem with high accuracy when the population size is large enough. The LPLC2-like solutions reproduces experimentally observed LPLC2 neuron responses for many stimuli, and reproduces canonical tuning of loom sensitive neurons, even though the models are never trained on neural data. Thus, LPLC2 neuron properties and tuning are predicted by optimizing an anatomically-constrained neural network to detect impending collisions. More generally, these results illustrate how optimizing inference tasks that are important for an animal’s perceptual goals can reveal and explain computational properties of specific sensory neurons.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>loom detection</kwd><kwd>visual circuits</kwd><kwd>inference</kwd><kwd>receptive fields</kwd><kwd>task optimization</kwd><kwd>machine learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EY026555</award-id><principal-award-recipient><name><surname>Zhou</surname><given-names>Baohua</given-names></name><name><surname>Clark</surname><given-names>Damon A</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>CCF-1839308</award-id><principal-award-recipient><name><surname>Zhou</surname><given-names>Baohua</given-names></name><name><surname>Lafferty</surname><given-names>John</given-names></name><name><surname>Clark</surname><given-names>Damon A</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DMS-1513594</award-id><principal-award-recipient><name><surname>Lafferty</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001201</institution-id><institution>Kavli Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Lafferty</surname><given-names>John</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When artificial networks modeled on a <italic>Drosophila</italic> neuron are tasked with discriminating objects on collision course from other visual scenes, their optimized solutions reproduce many functional features of the original loom-selective neuron.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>For animals living in dynamic visual environments, it is important to detect the approach of predators or other dangerous objects. Many species, from insects to humans, rely on a range of visual cues to identify approaching, or looming, objects (<xref ref-type="bibr" rid="bib45">Regan and Beverley, 1978</xref>; <xref ref-type="bibr" rid="bib58">Sun and Frost, 1998</xref>; <xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib8">Card and Dickinson, 2008</xref>; <xref ref-type="bibr" rid="bib39">Münch et al., 2009</xref>; <xref ref-type="bibr" rid="bib61">Temizer et al., 2015</xref>). Looming objects create characteristic visual flow fields. When an object is on a straight-line collision course with an animal, its edges will appear to the observer to expand radially outward, gradually occupying a larger and larger portion of the visual field (<xref ref-type="video" rid="video1">Video 1</xref>). An object heading toward the animal, but which will not collide with it, also expands to occupy an increasing portion of the visual field, but its edges do not expand radially outwards with respect to the observer. Instead, they expand with respect to the object’s center so that opposite edges can move in the same direction across the retina (<xref ref-type="video" rid="video2">Video 2</xref>). A collision detector must distinguish between these two cases, while also avoiding predicting collisions in response to a myriad of other visual flow fields, including those created by an object moving away (<xref ref-type="video" rid="video3">Video 3</xref>) or by the animal’s own motion (<xref ref-type="video" rid="video4">Video 4</xref>). Thus, loom detection can be framed as a visual inference problem.</p><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video1.mp4"><label>Video 1.</label><caption><title>Movie for a hit stimulus (single unit).</title><p>Top left panel: 3d rendering as in the top row of <xref ref-type="fig" rid="fig3">Figure 3</xref>; bottom left panel: optical signal as in the second row of <xref ref-type="fig" rid="fig3">Figure 3</xref>; top right panel: flow fields in the horizontal direction as in rows 7 and 8 of <xref ref-type="fig" rid="fig3">Figure 3</xref>; bottom right panel: flow fields in the vertical direction as in rows 5 and 6 of <xref ref-type="fig" rid="fig3">Figure 3</xref>. Since we combined left (down) and right (up) flow fields in one panel, we used blue and red colors to indicate left (down) and right (up) directions, respectively. The movie has been slowed down by a factor of 5. All the movies shown in this paper can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/ClarkLabCode/LoomDetectionANN/tree/main/results/movies_exp">https://github.com/ClarkLabCode/LoomDetectionANN/tree/main/results/movies_exp</ext-link>.</p></caption></media><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video2.mp4"><label>Video 2.</label><caption><title>Movie for a miss stimulus (single unit).</title><p>The same arrangement as <xref ref-type="video" rid="video1">Video 1</xref>.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video3.mp4"><label>Video 3.</label><caption><title>Movie for a retreat stimulus (single unit).</title><p>The same arrangement as <xref ref-type="video" rid="video1">Video 1</xref>.</p></caption></media><media id="video4" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video4.mp4"><label>Video 4.</label><caption><title>Movie for a rotation stimulus (single unit).</title><p>The same arrangement as <xref ref-type="video" rid="video1">Video 1</xref>.</p></caption></media><p>Many sighted animals solve this inference problem with high precision, thanks to robust loom-selective neural circuits evolved over hundreds of millions of years. The neuronal mechanisms for response to looming stimuli have been studied in a wide range of vertebrates, from cats and mice to zebrafish, as well as in humans (<xref ref-type="bibr" rid="bib27">King et al., 1992</xref>; <xref ref-type="bibr" rid="bib25">Hervais-Adelman et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Ball and Tronick, 1971</xref>; <xref ref-type="bibr" rid="bib31">Liu et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Salay et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Liu et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Shang et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Wu et al., 2005</xref>; <xref ref-type="bibr" rid="bib61">Temizer et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="bib6">Bhattacharyya et al., 2017</xref>). In invertebrates, detailed anatomical, neurophysiological, behavioral, and modeling studies have investigated loom detection, especially in locusts and flies (<xref ref-type="bibr" rid="bib40">Oliva and Tomsic, 2014</xref>; <xref ref-type="bibr" rid="bib53">Sato and Yamawaki, 2014</xref>; <xref ref-type="bibr" rid="bib52">Santer et al., 2005</xref>; <xref ref-type="bibr" rid="bib47">Rind and Bramwell, 1996</xref>; <xref ref-type="bibr" rid="bib8">Card and Dickinson, 2008</xref>; <xref ref-type="bibr" rid="bib12">de Vries and Clandinin, 2012</xref>; <xref ref-type="bibr" rid="bib38">Muijres et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>; <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). An influential mathematical model of loom detection was derived by studying the responses of the giant descending neurons of locusts (<xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>). This model established a relationship between the timing of the neurons’ peak responses and an angular size threshold for the looming object. Similar models have been applied to analyze neuronal responses to looming signals in flies, where genetic tools make it possible to precisely dissect neural circuits, revealing various neuron types that are sensitive to looming signals (<xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Morimoto et al., 2020</xref>).</p><p>However, these computational studies did not directly investigate the relationship between the structure of the loom-sensitive neural circuits and the inference problem they appear to solve. On the one hand, the properties of many sensory circuits appear specifically tuned to the tasks that they are executing (<xref ref-type="bibr" rid="bib62">Turner et al., 2019</xref>). In particular, by taking into account relevant behaviors mediated by specific sensory neurons, experiments can provide insight into their tuning properties (<xref ref-type="bibr" rid="bib29">Krapp and Hengstenberg, 1996</xref>; <xref ref-type="bibr" rid="bib49">Sabbah et al., 2017</xref>). On the other hand, computational studies that have trained artificial neural networks to solve specific visual and cognitive tasks, such as object recognition or motion estimation, have revealed response patterns similar to the corresponding biological circuits (<xref ref-type="bibr" rid="bib69">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib70">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib46">Richards et al., 2019</xref>) or even individual neurons (<xref ref-type="bibr" rid="bib33">Mano et al., 2021</xref>). Thus, here we ask whether we can reproduce the properties associated with neural loom detection simply by optimizing shallow neural networks for collision detection.</p><p>The starting point for our computational model of loom detection is the known neuroanatomy of the visual system of the fly. In particular, the loom-sensitive neuron LPLC2 (lobula plate/lobula columnar, type 2) has been studied in detail (<xref ref-type="bibr" rid="bib68">Wu et al., 2016</xref>). These neurons tile visual space, sending their axons to descending neurons called the giant fibers (GFs), which trigger the fly’s jumping and take-off behaviors (<xref ref-type="bibr" rid="bib60">Tanouye and Wyman, 1980</xref>; <xref ref-type="bibr" rid="bib8">Card and Dickinson, 2008</xref>; <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). Each LPLC2 neuron has four dendritic branches that receive inputs at the four layers of the lobula plate (LP) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). The retinotopic LP layers host the axon terminals of motion detection neurons, and each layer uniquely receives motion information in one of the four cardinal directions (<xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>). Moreover, the physical extensions of the LPLC2 dendrites align with the preferred motion directions in the corresponding LP layers (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). These dendrites form an outward radial structure, which matches the moving edges of a looming object that expands from the receptive field center (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Common stimuli such as the wide-field motion generated by movement of the insect only match part of the radial structure, and strong inhibition for inward-directed motion suppresses responses to such stimuli. Thus, the structure of the LPLC2 dendrites favors responses to visual stimuli with edges moving radially outwards, corresponding to objects approaching the receptive field center.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Sketches of the anatomy of LPLC2 neurons (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>).</title><p>(<bold>A</bold>) An LPLC2 neuron has dendrites in lobula and the four layers of the lobula plate (LP): LP1, LP2, LP3, and LP4. (<bold>B</bold>) Schematic of the four branches of the LPLC2 dendrites in the four layers of the LP. The arrows indicate the preferred direction of motion sensing neurons with axons in each LP layer (<xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>). (<bold>C</bold>) The outward dendritic structure of an LPLC2 neuron is selective for the outwardly expanding edges of a looming object (black circle). (<bold>D</bold>) The axons of a population of more than 200 LPLC2 neurons converge to the giant fibers, descending neurons that mediate escape behaviors (<xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig1-v2.tif"/></fig><p>The focus of this paper is to investigate how loom detection in LPLC2 can be seen as the solution to a computational inference problem. Can the structure of the LPLC2 neurons be explained in terms of optimization—carried out during the course of evolution—for the task of predicting which trajectories will result in collisions? How does coordination among the population of more than 200 LPLC2 neurons tiling a fly’s visual system affect this optimization? To answer these questions, we built simple anatomically-constrained neural network models, which receive motion signals in the four cardinal directions. We trained the model using artificial stimuli to detect visual objects on a collision course with the observer. Surprisingly, optimization finds two distinct types of solutions, with one resembling the LPLC2 neurons and the other having a very different configuration. We analyzed how each of these solutions detects looming events and where they show distinct individual and population behaviors. When tested on visual stimuli not in the training data, the optimized solutions with filters that resemble LPLC2 neurons exhibit response curves that are similar to those of LPLC2 neurons measured experimentally (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). Importantly, although it only receives motion signals, the optimized model shows characteristics of an angular size encoder, which is consistent with many biological loom detectors, including LPLC2 (<xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). Our results show that optimizing a neural network to detect looming events can give rise to the properties and tuning of LPLC2 neurons.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A set of artificial visual stimuli is designed for training models</title><p>Our goal is to compare computational models trained to perform loom-detection with the biological computations in LPLC2 neurons. We first created a set of stimuli to act as training data for the inference task (Materials and methods). We considered the following four types of motion stimuli: loom-and-hit (abbreviated as hit), loom-and-miss (miss), retreat, and rotation (<xref ref-type="fig" rid="fig2">Figure 2</xref>). The hit stimuli consist of a sphere that moves in a straight line towards the origin on a collision course (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="video" rid="video1">Video 1</xref>). The miss stimuli consist of a sphere that moves in a straight line toward the origin but misses it (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="video" rid="video2">Video 2</xref>). The retreat stimuli consist of a sphere moving in a straight line away from the origin (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="video" rid="video3">Video 3</xref>). The rotation stimuli consist of objects rotating about an axis that goes through the origin (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="video" rid="video4">Video 4</xref>). All stimuli were designed to be isotropic; the first three stimuli could have any orientation in space, while the rotation could be about any axis through the origin (<xref ref-type="fig" rid="fig2">Figure 2</xref>). All trajectories were simulated in the frame of reference of the fly at the origin, with distances measured with respect to the origin. For simplicity, the fly is assumed to be a point particle with no volume (Red dots in <xref ref-type="fig" rid="fig2">Figure 2</xref> and the apexes of the cones in <xref ref-type="fig" rid="fig3">Figure 3</xref>). For hit, miss, and retreat stimuli, the spherical object has unit radius, and for the case of rotation, there were 100 objects of various radii scattered isotropically around the fly (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Four types of synthetic stimuli (Materials and methods).</title><p>(<bold>A</bold>) Orange lines represent trajectories of the stimuli. The black dots represent the starting points of the trajectories. For hit, miss, and retreat cases, multiple trajectories are shown. For rotation, only one trajectory is shown. (<bold>B</bold>) Distances of the objects to the fly eye as a function of time. Among misses, only the approaching portion of the trajectory was used. The horizontal black lines indicate the distance of 1, below which the object would collide with the origin.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig2-v2.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Snapshots of optical flows and flow fields calculated by a Hassenstein Reichardt correlator (HRC) model (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, Materials and methods) for the four types of stimuli (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</title><p>First row: 3d rendering of the spherical objects and the LPLC2 receptive field (represented by a cone) at a specific time in the trajectory. The orange arrows indicate the motion direction of each object. Second row: 2d projections of the objects (black shading) within the LPLC2 receptive field (the gray circle). Third row: the thin black arrows indicate flow fields generated by the edges of the moving objects. Forth to seventh rows: decomposition of the flow fields in the four cardinal directions with respect to the LPLC2 neuron under consideration: downward, upward, rightward, and leftward, as indicated by the thick black arrows. These act as models of the motion signal fields in each layer of the LP.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Tuning curve of HRC motion estimator and distributions of the estimated flow fields.</title><p>(<bold>A</bold>) Diagram of a simple HRC motion estimator (Materials and methods), where <inline-formula><mml:math id="inf1"><mml:mi>τ</mml:mi></mml:math></inline-formula> indicates temporal delay, the two crosses indicate multiplication, and the minus sign indicates subtraction. (<bold>B</bold>) Tuning curves of the HRC motion estimator for different stimuli. Gray region indicates the velocity range used in simulations (Materials and methods). (<bold>C</bold>) The distributions of the magnitude of all the estimated flow fields in the four cardinal directions for different types of stimuli.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig3-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>An anatomically constrained mathematical model</title><p>We designed and trained simple, anatomically constrained neural networks (<xref ref-type="fig" rid="fig4">Figure 4</xref>) to infer whether or not a moving object will collide with the fly. The features of these networks were designed to mirror anatomical features of the fly’s LPLC2 neurons (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We will consider two types of single units in our models (<xref ref-type="fig" rid="fig4">Figure 4</xref>): a linear receptive field (LRF) unit and a rectified inhibition (RI) unit. Both types of model units receive input from a 60 degree diameter cone of visual space, represented by white cones and grey circles in <xref ref-type="fig" rid="fig3">Figure 3</xref>, approximately the same size as the receptive fields measured in LPLC2 (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). The four stimulus sets were projected into this receptive field for training and evaluating the models. The inputs to the units are local directional signals computed in the four cardinal directions at each point of the visual space: downward, upward, rightward, and leftward (<xref ref-type="fig" rid="fig3">Figure 3</xref>). These represent the combined motion signals from T4 and T5 neurons in the four layers of the lobula plate (<xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>). They are computed as the non-negative components of a Hassenstein-Reichardt correlator model (<xref ref-type="bibr" rid="bib24">Hassenstein and Reichardt, 1956</xref>) in both horizontal and vertical directions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>; Materials and methods). The motion signals are computed with a spacing of 5 degrees, roughly matching the spacing of the ommatidia and processing columns in the fly eye (<xref ref-type="bibr" rid="bib57">Stavenga, 2003</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Schematic of the models (Materials and methods).</title><p>(<bold>A</bold>) Single LRF model unit. There are four linear spatial filters, labeled LP4, LP3, LP2, and LP1, which correspond to the four LP layers (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Each filter has real-valued elements, and if the element is positive (negative), it is excitatory (inhibitory), represented by the color red (blue). The opposing spatial arrangement of the excitatory and inhibitory filters are illustrative, and do not represent constraints on the model. Each filter receives a field of motion signals from the corresponding layer of the model LP (fourth to seventh rows in <xref ref-type="fig" rid="fig3">Figure 3</xref>), indicated by the four black arrows (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The four filtered signals are summed together before a rectifier is applied to produce the output, which is the response of a single unit. (<bold>B</bold>) Single RI model unit. There are two sets of nonnegative filters: excitatory (red) and inhibitory (blue). Each set has four filters, and each filter receives the same motion signals as the corresponding one in the LRF unit. The weighted signals from the excitatory filters and the inhibitory filters (rectified) are pooled together before a rectifier is applied to produce the output, which is the response of a single unit. When the inhibitory filters are not rectified, this model effectively reduces to the LRF model in (<bold>A</bold>) (Materials and methods). (<bold>C</bold>) The outputs from <inline-formula><mml:math id="inf2"><mml:mi>M</mml:mi></mml:math></inline-formula> units are summed and fed into a sigmoid function to estimate the probability of hit. (<bold>D</bold>) The <inline-formula><mml:math id="inf3"><mml:mi>M</mml:mi></mml:math></inline-formula> units have their orientations almost evenly distributed in angular space. Red dots represent the centers of the receptive fields and the grey lines represent the boundaries of the receptive fields on unit sphere. The red lines are drawn from the origin to the center of each receptive field.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Coordinate system for model and stimuli.</title><p>The coordinate system used in stimulus generation and modeling (Materials and methods). The frame of reference <inline-formula><mml:math id="inf4"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> is fixed on the fly head. The frame of references <inline-formula><mml:math id="inf5"><mml:mrow><mml:mpadded width="+5pt"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mpadded><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are associated with each local model unit, the center of which is represented by a red dot. The origin of the <inline-formula><mml:math id="inf6"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> coincides with the origin of <inline-formula><mml:math id="inf7"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula>, but for better illustration, we have translated its origin radially along <italic>z</italic><sub><italic>m</italic></sub> so that all the <inline-formula><mml:math id="inf8"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>’s sit on a sphere. For <inline-formula><mml:math id="inf9"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>, only <italic>x</italic><sub><italic>m</italic></sub> and <italic>z</italic><sub><italic>m</italic></sub> are shown, and <italic>y</italic><sub><italic>m</italic></sub> axis should be chosen such that <inline-formula><mml:math id="inf10"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is right-handed. For the unit coordinate systems, <italic>z</italic><sub><italic>m</italic></sub> is chosen to be normal to the unit sphere while <italic>x</italic><sub><italic>m</italic></sub> points north tangent to the sphere. This system does not impose the left-right mirror symmetry of the fly eyes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig4-figsupp1-v2.tif"/></fig></fig-group><p>For the LRF model unit (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), there is a single set of four real-valued filters, the elements of which can be positive (excitatory, red) or negative (inhibitory, blue). The four filters integrate motion signals from the four cardinal directions, respectively, and their outputs are summed and rectified to generate the output of a single model unit (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). These spatial filters effectively represent excitatory inputs to LPLC2 directly from T4 and T5 in the LP (positive elements), and inhibitory inputs mediated by local interneurons (negative elements) (<xref ref-type="bibr" rid="bib35">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). All filters act on the 60 degree receptive field of a unit. A 90 degree rotational symmetry is imposed on the filters, so that the filters in each layer are identical. Moreover, each filter is symmetric about the axis of motion (Materials and methods). Although these symmetry assumptions are not necessary and may be learned from training (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>), they greatly reduce the number of parameters in the models. No further assumptions were made about the structure of the filters. Note that the opposing spatial patterns of the excitatory and inhibitory components in <xref ref-type="fig" rid="fig4">Figure 4</xref> are only for illustration purposes, and are not imposed on the models. The LRF model unit is equivalent to a linear-nonlinear model and constitutes one of the simplest possible models for the LPLC2 neurons. In this work, we will focus most of our analysis on this simplest form of the model.</p><p>In addition, we will also consider a more complex model unit, which we call the rectified inhibition (RI) model unit (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). In this unit, all the same filter symmetries are enforced, but there are two sets of non-negative filters: a set of excitatory filters and a set of inhibitory filters. The RI unit incorporates a fundamental difference between the excitatory and inhibitory filters: while the integrated signals from each excitatory filter are sent directly to the downstream computations, the integrated signals from each inhibitory filter are rectified before being sent downstream. The outputs of the eight filters are summed and rectified to generate the output of a single model unit in response to a given stimulus (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). If one removes the rectification of the inhibitory filters (as is possible with an appropriate choice of parameters), the RI unit becomes equivalent to the LRF unit (Materials and methods). This difference between the two model units reflects different potential constraints on the inhibitory inputs to an actual LPLC2 neuron. While the excitatory inputs to LPLC2 are direct connections, the inhibitory inputs are mediated by inhibitory interneurons (LPi) between LP layers (<xref ref-type="bibr" rid="bib35">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). The LRF model unit assumes that LPi interneurons are linear and do not strongly rectify signals, while the RI model unit approximates rectified transmission by LPi interneurons.</p><p>In the fly brain, a population of LPLC2 neurons converges onto the GFs (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Accordingly, in our model there are <inline-formula><mml:math id="inf11"><mml:mi>M</mml:mi></mml:math></inline-formula> replicates of model units, with orientations that are spread uniformly over the <inline-formula><mml:math id="inf12"><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula> steradians of the unit sphere (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, Materials and methods). In this way, the receptive fields of the <inline-formula><mml:math id="inf13"><mml:mi>M</mml:mi></mml:math></inline-formula> units roughly tile the whole angular space, with or without overlap, depending on the value of <inline-formula><mml:math id="inf14"><mml:mi>M</mml:mi></mml:math></inline-formula>. The sum of the responses of the <inline-formula><mml:math id="inf15"><mml:mi>M</mml:mi></mml:math></inline-formula> model units is fed into a sigmoid function to generate the predicted probability of collision for a given trajectory (Materials and methods). The loss function then is defined as the cross entropy between the predicted probabilities and the stimulus labels (hits are labeled one and all others are labeled 0).</p></sec><sec id="s2-3"><title>Optimization finds two distinct solutions to the loom-inference problem</title><p>The objective of this study is to investigate how the binary classification task shapes the structure of the filters, and how the number of units <inline-formula><mml:math id="inf16"><mml:mi>M</mml:mi></mml:math></inline-formula> affects the results. We begin with the simplest LRF model, which possesses only a single unit, <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. After training with 200 random initializations of the filters, we find that the converged solutions fall into three broad categories (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Two solutions have spatial structures that are—surprisingly—roughly opposite from one another (magenta and green). Based on the configurations of the positive-valued elements (stronger excitation) of the filters (Materials and methods), we call one solution type <italic>outward solutions</italic> (magenta) and the other type <italic>inward solutions</italic> (green) (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). In this single-unit model, the inward solutions have higher area under the curve (AUC) scores for both receiver operating characteristic (ROC) and precision recall curves, and thus perform better than the outward solutions on the discrimination task (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). A third category of solution has all the elements in the filters very close to zero (<italic>zero solutions</italic> in black squares, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This solution appears roughly 5–15% of the time, and appears to be a local minimum of the optimization, dependent on the random initialization. This uninteresting category of solutions is ignored in subsequent analyses.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Three distinct types of solutions appear from training a single unit on the binary classification task (LRF model).</title><p>(<bold>A</bold>) Clustering of the trained filters/weights shown as a dendrogram (Materials and methods). Different colors indicate different clusters, which are preserved for the rest of the paper: outward, inward, and zero solutions are magenta, green, and black, respectively. (<bold>B</bold>) The trajectories of the loss functions during training. More than one example are shown for each type of solution, but lines fall on top of one another. (<bold>C</bold>) Two distinct types of solutions are represented by two types of filters that have roughly opposing structures: an outward solution (magenta boxes) and an inward solution (green boxes). For each solution type, two examples of the trained filters from different initializations are shown; they are almost identical. The third type of solution (color black in (<bold>A</bold>) and (<bold>B</bold>)) has filter elements all close to zero. We call these zero solutions (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). (<bold>D</bold>) Performance of the three solution types (Materials and methods). TPR: true positive rate; FPR: false positive rate; ROC: receiver operating characteristic; PR: precision recall; AUC: area under the curve. More than one example is shown for each type of solution, but lines and dots with the same color fall on top of one another.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>More examples of the trained filters for the three types of solutions.</title><p>Trained filters: outward solution (magenta), inward solution (green), and zero solution (black). For each solution type, the trained filters from different initializations are almost identical to one another.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>As in the main figure but for the RI model.</title><p>As in the main figure but for the RI model. (<bold>A</bold>) Three types of solutions appear. (<bold>B</bold>) The trajectories of loss functions. (<bold>C</bold>) For each type of solution, there are two filters: one excitatory (red) and one inhibitory (blue). Some outward solutions have inhibitory filters close to zero. (<bold>D</bold>) Performance of the three solution types. More than one example is shown for each type of solution.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Examples of the trained outward and inward filters without imposed symmetries.</title><p>Trained solutions for models without imposing the 90-degree rotational and mirror symmetries. (A) Trained filters for the LRF model. First two rows: outward and inward filters trained with the original data set as used in the main figure. Last two rows: outward and inward filters trained with eight-fold more data. For each row, from left to right: rightward-, leftward-, upward-, downward-sensitive filter, and symmetrized filter (in dotted boxes). The symmetrized filter was computed by averaging the aligned filters for the four directions and then averaging again over the mirror symmetry. (B) Distributions of the cosine similarity between the unsymmetrized trained filter and the corresponding symmetrized filter. Red: models trained with the original data set; Blue: models trained with eight-fold more data.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig5-figsupp3-v2.tif"/></fig></fig-group><p>As the number of units <inline-formula><mml:math id="inf18"><mml:mi>M</mml:mi></mml:math></inline-formula> increases, the population of units covers more angular space, and when <inline-formula><mml:math id="inf19"><mml:mi>M</mml:mi></mml:math></inline-formula> is large enough (<inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≥</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula>), the receptive fields of the units begin to overlap with one another (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). In the fly visual system there are over 200 LPLC2 neurons across both eyes (<xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>), which corresponds to a dense distribution of units. This is illustrated by the third row in <xref ref-type="fig" rid="fig6">Figure 6A</xref>, where <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>. When <inline-formula><mml:math id="inf22"><mml:mi>M</mml:mi></mml:math></inline-formula> is large, objects approaching from any direction are detectable, and such object signals can be detected simultaneously by many neighboring units. The two oppositely structured solutions persist, regardless of the value of <inline-formula><mml:math id="inf23"><mml:mi>M</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>, <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). Strikingly, the inhibitory component of the outward solutions becomes broader as <inline-formula><mml:math id="inf24"><mml:mi>M</mml:mi></mml:math></inline-formula> increases and expands to extend across the entire receptive field (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). This broad inhibition is consistent with the large receptive field of LPi neurons suggested by experiments (<xref ref-type="bibr" rid="bib35">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). The outward, inward, and zero solutions all also appear in trained solutions of the RI models. (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The outward and inward solutions also arise for models with multiple units (LRF models).</title><p>(<bold>A</bold>) Left column: angular distribution of the units, where red dots are centers of the receptive fields, the grey circles are the boundaries of the receptive fields, and the black star indicates the top of the fly head. Middle column: 2d map of the units with the same symbols as in the left column, with one unit highlighted in black. Right column: clustering results shown as dendrogams with color codes as in <xref ref-type="fig" rid="fig5">Figure 5</xref>. (<bold>B</bold>) Examples of the trained filters for outward and inward solutions with different numbers of units.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Performance of the different solutions (LRF models).</title><p>Same as in <xref ref-type="fig" rid="fig5">Figure 5D</xref> but for LRF models with multiple units. The magenta and green lines/points almost completely overlap with each other in the last row.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>More examples of the outward and inward filters (LRF models).</title><p>Outward and inward solutions for the LRF models with different numbers of units. For both outward and inward solutions, five examples are shown for each model. It can be seen that for outward solutions, all the examples within a specific model are almost identical to each other, while for inward solutions, different configures can appear (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Examples of the outward and inward filters for RI models.</title><p>Outward and inward solutions for RI models. For both outward and inward solutions, 10 examples are shown for each model. In many outward solutions, structures on the right side of the inhibitory filters are similar to structures of the corresponding excitatory filters. This indicates a degree of redundancy, or non-identifiability in the model.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig6-figsupp3-v2.tif"/></fig></fig-group><p>Units with outward-oriented filters are activated by motion radiating outwards from the center of the receptive field, such as the hit event illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref>. These excitatory components resemble the dendritic structures of the actual LPLC2 neurons observed in experiments, where for example, the rightward motion-sensitive component (LP2) occupies mainly the right side of the receptive field. In the outward solutions of the LRF models, the rightward motion-sensitive inhibitory components mainly occupy the <italic>left</italic> side of the receptive field (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). This is also consistent with the properties of the lobula plate intrinsic (LPi) interneurons, which project inhibitory signals roughly retinotopically from one LP layer to the adjacent layer with opposite directional tuning (<xref ref-type="bibr" rid="bib35">Mauss et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>).</p><p>The unexpected inward-oriented filters have the opposite structure. In the inward solutions, the rightward sensitive excitatory component occupies the left side of the receptive field, and the inhibitory component occupies the right side. Such weightings make the model selective for motion converging toward the receptive field center, such as the retreat event shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. This is a puzzling structure for a loom detector, and warrants a more detailed exploration of the response properties of the inward and outward solutions.</p></sec><sec id="s2-4"><title>Units with outward and inward filters respond to hits originating in distinct regions</title><p>To understand the differences between the two types of solutions and why the inward ones can predict collisions, we investigated how units respond to hit stimuli originating at different angles <inline-formula><mml:math id="inf25"><mml:mi>θ</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). When there is no signal, the baseline activity of outward units is zero; however, the baseline activity of inward units is above zero (grey dashed lines in <xref ref-type="fig" rid="fig7">Figure 7B and C</xref>). This is because the trained intercepts are negative (positive) in the outward (inward) case and when the input is zero (no signal), the unit activity cannot (can) get through the rectifier (Materials and methods). (The training did not impose any requirements on these intercepts.) The outward units respond strongly to stimuli originating near the center of the receptive field, but do not respond to stimuli originating at angles larger than approximately <inline-formula><mml:math id="inf26"><mml:msup><mml:mn>30</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7B and C</xref>). In contrast, inward units respond below baseline to hit stimuli approaching from the center and above baseline to stimuli approaching from the periphery of the receptive field, with <inline-formula><mml:math id="inf27"><mml:mi>θ</mml:mi></mml:math></inline-formula> between roughly <inline-formula><mml:math id="inf28"><mml:msup><mml:mn>30</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7B and C</xref>). This helps explain why the inward units can act as loom detectors: they are sensitive to hit stimuli coming from the edges of the receptive field rather than from the center. The hit stimuli are isotropic (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), so the number of stimuli with angles between <inline-formula><mml:math id="inf30"><mml:msup><mml:mn>30</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> is much larger than the number of stimuli with angles below <inline-formula><mml:math id="inf32"><mml:msup><mml:mn>30</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7D</xref>). Thus, the inward units are sensitive to more hit cases than the outward ones. One may visualize these responses as heat maps of the mean response of the units in terms of object distance to the fly and the incoming angle (<xref ref-type="fig" rid="fig7">Figure 7E</xref>). For the hit cases, the response patterns are consistent with the intuition about trajectory angles (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Both outward and inward units respond less strongly to miss signals than to hit signals. As expected, while the outward units respond at most weakly to retreating signals, the inward ones respond to these signals with angles near <inline-formula><mml:math id="inf33"><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>, since the motion of edges in such cases is radially inward. The RI model units have similar response patterns to the LRF model units (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>LRF units with outward and inward filters show distinct patterns of responses.</title><p>(<bold>A</bold>) Trajectories of hit stimuli originating at different angles from the receptive field center, denoted by <inline-formula><mml:math id="inf34"><mml:mi>θ</mml:mi></mml:math></inline-formula>. Symbols are the same as in <xref ref-type="fig" rid="fig2">Figure 2</xref> except that the upward red arrow represents the orientation of one unit (z direction, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The numbers with degree units indicate the specific values of the incoming angles of different hit trajectories. (<bold>B</bold>) Response patterns of a single unit with either outward (magenta) or inward (green) filters obtained from optimized solutions with 32 and 256 units, respectively. The horizontal gray dashed lines show the baseline activity of the unit when there is no stimulus. The solid grey concentric circles correspond to the values of the incoming angles in (<bold>A</bold>). The responses have been scaled so that each panel has the same maximum value. (<bold>C</bold>) Temporally averaged responses against the incoming angle <inline-formula><mml:math id="inf35"><mml:mi>θ</mml:mi></mml:math></inline-formula> in (<bold>A</bold>). Symbols and colors are as in (<bold>B</bold>). (<bold>D</bold>) Histogram of the incoming angles for the hit stimuli in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. The gray curve represents a scaled sine function equal to the expected probability for isotropic stimuli. (<bold>E</bold>) Heatmaps of the response of a single unit against the incoming angle <inline-formula><mml:math id="inf36"><mml:mi>θ</mml:mi></mml:math></inline-formula> and the distance to the fly head, for both outward and inward filters obtained from optimized models with 32 and 256 units, respectively. The responses were calculated using the stimuli in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>As in the main figure but for the RI units.</title><p>(<bold>A-E</bold>) As in the main figure but for the RI units.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig7-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Outward solutions have sparse codings and populations of units accurately predict hit probabilities</title><p>Individual units of the two solutions are very different from one another in both their filter structure and their response patterns to different stimuli. In populations of units, the outward and inward solutions also exhibit very different response patterns for a given hit stimulus (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>, <xref ref-type="video" rid="video5">Videos 5</xref> and <xref ref-type="video" rid="video6">6</xref>). In particular, active outward units usually respond more strongly than inward units, but more inward units will be activated by a hit stimulus. This is consistent with the findings above, in which inward filter shapes responded to hits arriving from a wider distribution of angles (<xref ref-type="fig" rid="fig7">Figure 7</xref>). For all the four types of stimuli, the outward solutions generally show relatively sparse activity among units, especially for models with larger numbers of units <inline-formula><mml:math id="inf37"><mml:mi>M</mml:mi></mml:math></inline-formula>, while the inward solutions show broader activity among units (<xref ref-type="fig" rid="fig8">Figure 8A and B</xref>, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>, <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>, <xref ref-type="video" rid="video5">Videos 5</xref>–<xref ref-type="video" rid="video12">12</xref>).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Population coding of stimuli (LRF models).</title><p>(<bold>A</bold>) Top row: snapshots of the unit responses of outward solutions (magenta dots) and inward solutions (green dots) for a hit stimulus. The size of the dots represents the strength of the response. The gray shading represents the looming object in the snapshot. See also <xref ref-type="video" rid="video5">Videos 5</xref> and <xref ref-type="video" rid="video6">6</xref>. Symbols and colors are as in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Middle row: time traces of the responses for the same hit stimulus as in the top row. Time proceeds to the right in each trace. Bottom row: time trace of the probability of hit for the same hit stimulus as in the top row (Materials and methods). Black dots in the middle and bottom rows indicate the time of the snapshot in the top row. The dotted gray line represents the basal model response. (<bold>B</bold>) Fractions of the units that are activated above the baseline by different types of stimuli (hit, miss, retreat, rotation) as a function of the number of units <inline-formula><mml:math id="inf38"><mml:mi>M</mml:mi></mml:math></inline-formula> in the model. The lines represent the mean values averaged across stimuli, and the shaded areas show one standard deviation (Materials and methods). (<bold>C</bold>) Histograms of the probability of hit inferred by models with 32 or 256 units for the four types of synthetic stimuli (Materials and methods). (<bold>D</bold>) The inferred probability of hit as a function of the minimum distance of the object to the fly eye for the miss cases. For comparison, the hit distribution is represented by a box plot (the center line in the box: the median; the upper and lower boundaries of the box: 25% and 75% percentiles; the upper and lower whiskers: the minimum and maximum of non-outlier data points; the circles: outliers).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Geometry of responses as in <xref ref-type="fig" rid="fig8">Figure 8A</xref>, but for miss and retreat stimuli (LRF models).</title><p>(<bold>A</bold>) An example of response patterns to a miss stimulus (<xref ref-type="video" rid="video7">Videos 7</xref> and <xref ref-type="video" rid="video8">8</xref>). (<bold>B</bold>) An example of response patterns to a retreat stimulus (<xref ref-type="video" rid="video9">Videos 9</xref> and <xref ref-type="video" rid="video10">10</xref>). The snapshots of the rotational case are not shown, but can be found in the videos (<xref ref-type="video" rid="video11">Videos 11</xref> and <xref ref-type="video" rid="video12">12</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Sample individual unit response curves (LRF models with <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</title><p>(<bold>A</bold>) Sample response curves of the active units in the outward solution with <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> for different types of stimuli (from left to right: hit, miss, retreat, and rotation) (<bold>B</bold>) As in (<bold>A</bold>), but for an inward solution. Lines in different colors represent responses of different units.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig8-figsupp2-v2.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>As in the main figure but for the RI models.</title><p>(<bold>A-D</bold>) As in the main figure but for the RI models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig8-figsupp3-v2.tif"/></fig></fig-group><media id="video5" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video5.mp4"><label>Video 5.</label><caption><title>Movie of unit responses for a hit stimulus (outward solution of the LRF model with 32 units).</title><p>Top left panel: the same as in the top row of <xref ref-type="fig" rid="fig8">Figure 8A</xref>; bottom left, top right, bottom left panels: the same as in <xref ref-type="video" rid="video1">Video 1</xref> but with more units. The movie has been slowed down by a factor of 10.</p></caption></media><media id="video6" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video6.mp4"><label>Video 6.</label><caption><title>Movie of unit responses for a hit stimulus (inward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video5">Video 5</xref> but for an inward model.</p></caption></media><media id="video7" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video7.mp4"><label>Video 7.</label><caption><title>Movie of unit responses for a miss stimulus (outward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video5">Video 5</xref>.</p></caption></media><media id="video8" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video8.mp4"><label>Video 8.</label><caption><title>Movie of unit responses for a miss stimulus (inward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video6">Video 6</xref>.</p></caption></media><media id="video9" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video9.mp4"><label>Video 9.</label><caption><title>Movie of unit responses for a retreat stimulus (outward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video5">Video 5</xref>.</p></caption></media><media id="video10" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video10.mp4"><label>Video 10.</label><caption><title>Movie of unit responses for a retreat stimulus (inward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video6">Video 6</xref>.</p></caption></media><media id="video11" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video11.mp4"><label>Video 11.</label><caption><title>Movie of unit responses for a rotation stimulus (outward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video5">Video 5</xref>.</p></caption></media><media id="video12" mime-subtype="mp4" mimetype="video" xlink:href="elife-72067-video12.mp4"><label>Video 12.</label><caption><title>Movie of unit responses for a rotation stimulus (inward solution of the LRF model with 32 units).</title><p>The same arrangement as <xref ref-type="video" rid="video6">Video 6</xref>.</p></caption></media><p>When a population of units encodes stimuli, at each time point, the sum of the activities of the units is used to infer the probability of hit. In our trained models, the outward and inward solutions predict similar trajectories of probabilities of hit (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). The outward solutions suppress the miss and retreat signals better, while the inward solutions better suppress rotation signals (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). Both 32 unit and 256 unit models have units covering the entire visual field (<xref ref-type="fig" rid="fig6">Figure 6</xref>), but the models with 256 units can more accurately detect hit stimuli (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). In some cases, misses can appear very similar to hits if the object passes near the origin. Both inward and outward solutions reflect this in their predictions in response to near misses, which have higher hit probabilities than far misses (<xref ref-type="fig" rid="fig8">Figure 8D</xref>).</p><p>The inward and outward solutions of RI models have similar behaviors to the LRF models (<xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref>). This indicates that the linear receptive field intuition captures behavior of the potentially more complicated RI model, as well.</p></sec><sec id="s2-6"><title>Large populations of units improve performance</title><p>Since a larger number of units will cover a larger region of the visual field, a larger population of units can in principle provide more information about the incoming signals. In general, the models perform better as the number of units <inline-formula><mml:math id="inf41"><mml:mi>M</mml:mi></mml:math></inline-formula> increases (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). When <inline-formula><mml:math id="inf42"><mml:mi>M</mml:mi></mml:math></inline-formula> is above 32, both the ROC-AUC and PR-AUC scores are almost 1 (Materials and methods), which indicates that the model is very accurate on the binary classification task presented by the four types of synthetic stimuli. As <inline-formula><mml:math id="inf43"><mml:mi>M</mml:mi></mml:math></inline-formula> increases, the outward solutions become closer to the inward solutions in terms of both AUC score and cross entropy loss (<xref ref-type="fig" rid="fig9">Figure 9A and B</xref>). This is also true for the RI models (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A and B</xref>).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Large populations of units improve performance (LRF models) (Materials and methods).</title><p>(<bold>A</bold>) Both ROC and PR AUC scores increase as the number of units increases. Colored lines and dots: average scores; shading: one standard deviation of the scores over the trained models. Magenta: outward solutions; green: inward solutions. The dotted horizontal gray lines indicate the value of 1. (<bold>B</bold>) As the population of units increases, cross entropy losses of the outward solutions approach the losses of the inward solutions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>As in the main figure but for RI models.</title><p>(<bold>A, B</bold>) As in the main figure but for RI models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig9-figsupp1-v2.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>The ratio of the number of the two types of solutions.</title><p>The black line and dots show the ratio of the numbers of the two types of solutions in the set of randomly initialized, trained models. The gray shading is one standard deviation, assuming that the distribution is binomial (Materials and methods). The dotted horizontal gray lines indicate a ratio of 1. From left to right: LRF models with ReLU activation functions, LRF models with ELU activation functions, RI models ReLU activation functions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig9-figsupp2-v2.tif"/></fig><fig id="fig9s3" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 3.</label><caption><title>As in the main figure but for LRF models trained using stimuli that include self-rotation during hits, misses, and retreats.</title><p>(<bold>A, B</bold>) As in the main figure but for LRF models trained using stimuli that include self-rotation during hits, misses, and retreats. (<bold>C</bold>) Example filters for outward and inward solutions for the noted number of units <inline-formula><mml:math id="inf44"><mml:mi>M</mml:mi></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig9-figsupp3-v2.tif"/></fig></fig-group><p>Beyond performance of the two solution types, we also calculated the ratio of the number of outward to inward solutions in 200 random initializations of the models with <inline-formula><mml:math id="inf45"><mml:mi>M</mml:mi></mml:math></inline-formula> units. For the LRF models, as the number of units increases, the ratio remained relatively constant, fluctuating around 0.5 (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>). For the RI models, on the other hand, as the number of units increases, an increasing proportion of solutions have outward filters (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>). For RI models with 256 units, the chance that an outward filter appears as a solution is almost 90% compared with roughly 50% when <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Because of this qualitative difference between the LRF and RI models, we next asked whether the form of the nonlinearity in the LRF model could influence the solutions found through optimization. When we replaced the rectified linear unit (ReLU) in LRF models with the exponential linear unit (ELU) (4 Methods and Materials), only inward solutions exist for models with <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the outward solutions emerge more often as <inline-formula><mml:math id="inf48"><mml:mi>M</mml:mi></mml:math></inline-formula> increases (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>). Combined, these results with LRF and RI models indicate that the form and position of the nonlinearity in the circuit play a role in selecting between different optimized solutions. This suggests that further studies of the nonlinearities in LPLC2 processing will lead to additional insight into how a population of LPLC2s encodes looming stimuli.</p><p>The current binary prediction task is relatively easy for our loom detection models, as can be seen by the saturated AUC scores when <inline-formula><mml:math id="inf49"><mml:mi>M</mml:mi></mml:math></inline-formula> is large (<xref ref-type="fig" rid="fig9">Figure 9A</xref>, <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A</xref>). Thus, we engineered a new set of stimuli, where for the hit, miss, and retreat cases, we added a rotational background to increase the difficulty of the task. The object of interest, which is moving toward or away from the observer, also rotates with the background. This arrangement mimics self-rotation of the fly while observing a looming or retreating object in a cluttered background. To train an LRF model, we added such rotational distraction to half of the hit, miss, and retreat cases. The outward and inward solutions both persist (<xref ref-type="fig" rid="fig9s3">Figure 9—figure supplement 3</xref>), although in this case, outward solutions outperform inward ones (<xref ref-type="fig" rid="fig9s3">Figure 9—figure supplement 3</xref>). It remains unclear whether this added complexity brings our artificial stimuli closer to actual detection tasks performed by flies, but this result makes clear that identifying the natural statistics of loom will be important to understanding loom inferences.</p></sec><sec id="s2-7"><title>Activation patterns of computational solutions resemble biological responses</title><p>The outward solutions have a receptive field structure that is similar to LPLC2 neurons, based on anatomical and functional studies. However, it is not clear whether these models possess the functional properties of LPLC2 neurons, which have been studied systematically (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). To see how trained units compare to LPLC2 neuron properties, we presented stimuli to the trained outward model solution to compare its responses to those measured in LPLC2 (<xref ref-type="fig" rid="fig10">Figure 10</xref>).</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Units of models trained on binary classification tasks exhibit similar responses to LPLC2 neuron experimental measurements (outward solution of the LRF model with 256 units).</title><p>(<bold>A</bold>) The trained filter. (<bold>B–H</bold>) Comparisons of the responses of the unit with the trained filter in (<bold>A</bold>) and LPLC2 neurons to a variety of stimuli (Materials and methods). Black lines: data (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>); magenta lines: LRF unit. Compared with the original plots (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>), all the stimulus icons here except the ones in (<bold>B</bold>) have been rotated 45 degrees to match the cardinal directions of LP layers as described in this study. All response curves are normalized by the peak value of the left most panel in (<bold>B</bold>). (<bold>I</bold>) Top: temporal trajectories of the angular sizes for different <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios (color labels apply throughout (<bold>I–L</bold>)) (Materials and methods). Middle: response as a function of time for the sum of all 256 units. Bottom: response as a function of time for one of the 256 units. (<bold>J–L</bold>) Top: experimental data (LPLC2/non-LC4 components of GF activity. Data from <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). Middle: sum of all 256 units. Bottom: response of one of the 256 units. Responses as function of angular size (<bold>J</bold>), response as function of angular velocity (<bold>K</bold>), relationship between peak time relative to collision and <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios (<bold>L</bold>). We considered the first peak when there were two peaks in the response, such as in the grey curves in the middle panel of (<bold>I</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig10-v2.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>As in the main figure but for an inward solution of the LRF model obtained from the same training procedure.</title><p>(<bold>A-L</bold>) As in the main figure but for an inward solution of the LRF model obtained from the same training procedure, the filter of which is shown in (<bold>A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig10-figsupp1-v2.tif"/></fig><fig id="fig10s2" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 2.</label><caption><title>As in the main figure but for an outward solution of the RI model with 256 units.</title><p>(<bold>A-L</bold>) As in the main figure but for an outward solution of the RI model with 256 units, the trained filters of which are shown in (<bold>A</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig10-figsupp2-v2.tif"/></fig><fig id="fig10s3" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 3.</label><caption><title>As in the main figure but for a second outward solution of the RI model with 256 units.</title><p>(<bold>A-L</bold>) As in the main figure but for a second outward solution of the RI model with 256 units, the trained filters of which are shown in (<bold>A</bold>). The response curves in (<bold>B–H</bold>) are produced by setting the threshold of the single unit to be zero (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, Materials and methods). Responses are plotted in this way because the dynamic range of responses of this model to these stimuli does not often exceed the trained threshold.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig10-figsupp3-v2.tif"/></fig><fig id="fig10s4" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 4.</label><caption><title>As in the main figure but for a third outward solution of the RI model with 256 units.</title><p>(<bold>A-L</bold>) As in the main figure but for a third outward solution of the RI model with 256 units, the trained filters of which are shown in (<bold>A</bold>). The response curves in (<bold>B–H</bold>) are produced by setting the threshold of the single unit to be zero (<xref ref-type="fig" rid="fig4">Figure 4B</xref> and Materials and methods). Responses are plotted in this way because the dynamic range of responses of this model to these stimuli does not often exceed the trained threshold.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72067-fig10-figsupp4-v2.tif"/></fig></fig-group><p>The outward unit behaves similarly to LPLC2 neurons on many different types of stimuli. Not surprisingly, the unit is selective for loom signals and does not have strong responses to non-looming signals (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Moreover, the unit closely follows the responses of LPLC2 neurons to various expanding bar stimuli, including the inhibitory effects of inward motion (<xref ref-type="fig" rid="fig10">Figure 10C and D</xref>). In addition, in experiments, motion signals that appear at the periphery of the receptive field suppress the activity of the LPLC2 neurons (periphery inhibition) (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>), and this phenomenon is successfully predicted by the outward unit (<xref ref-type="fig" rid="fig10">Figure 10E and F</xref>) due to its broad inhibitory filters (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). The unit also correctly predicts response patterns of the LPLC2 neurons for expanding bars with different orientations (<xref ref-type="fig" rid="fig10">Figure 10G and H</xref>).</p><p>The ratio of object size to approach velocity, or <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>, is an important parameter for looming stimuli, and many studies have investigated how the response patterns of loom-sensitive neurons depend on this ratio (Top panels in <xref ref-type="fig" rid="fig10">Figure 10I, J, K and L</xref>; <xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>; <xref ref-type="bibr" rid="bib12">de Vries and Clandinin, 2012</xref>). Here, we presented the trained model (<xref ref-type="fig" rid="fig10">Figure 10A</xref>) with hit stimuli with different <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios, and compared its response with experimental measurements (<xref ref-type="fig" rid="fig10">Figure 10I–L</xref>). Surprisingly, although our model only has angular velocities as inputs (<xref ref-type="fig" rid="fig3">Figure 3</xref>), it reliably encodes the angular size of the stimulus rather than its angular velocity (<xref ref-type="fig" rid="fig10">Figure 10J</xref>). This is indicated by the collapsed response curves with different <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios (up to different scales) when plotted against the angular sizes (<xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>). When the curves are plotted against angular velocity, they shift for different <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios, which means the response depends on the velocity <inline-formula><mml:math id="inf56"><mml:mi>v</mml:mi></mml:math></inline-formula> of the object, since <inline-formula><mml:math id="inf57"><mml:mi>R</mml:mi></mml:math></inline-formula> is fixed to be 1. The relative shifts in these curves are consistent with properties of LPLC2.</p><p>There are two ways that this angular size tuning likely arises. First, in hit stimuli, the angular size and angular velocity are strongly correlated (<xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>), which means the angular size affects the magnitude of the motion signals. Second, in hit stimuli, the angular size is proportional to the path length of the outward-moving edges. This angular circumference of the hit stimulus determines how many motion detectors are activated, so that integrated motion signal strength is related to the size. Both of these effects influence the response patterns of the model units (and the LPLC2 neurons). Beyond the tuning to stimulus size, the outward model also reproduces a canonical linear relationship between the peak response time relative to the collision and the <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratio (<xref ref-type="fig" rid="fig10">Figure 10L</xref>; <xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>).</p><p>Not surprisingly, the inward solution cannot reproduce the neural data (<xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>). On the other hand, some forms of the the RI model outward solutions can closely reproduce the neural data (<xref ref-type="fig" rid="fig10s2">Figure 10—figure supplement 2</xref>), while other outward solutions fail to do so (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3</xref>, <xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4</xref>). For example, some RI outward solutions predict the patterns in the wide expanding bars differently and out of phase from the biological data (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3H</xref>), do a poor job predicting the response curves of the LPLC2 neurons to looming signals with different <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios (<xref ref-type="fig" rid="fig10s3">Figure 10—figure supplement 3J and K</xref>), respond strongly to the moving gratings (<xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4B</xref>), cannot show the peripheral inhibition (<xref ref-type="fig" rid="fig10s4">Figure 10—figure supplement 4E and F</xref>), and so on. This shows that, for the RI models, even within the family of learned outward solutions, there is variability in the learned response properties. Although solving the inference problem with the RI model obtains many of the neural response properties, additional constraints could be required. These additional constraints may be built into the LRF model, causing its outward solutions to more closely match neural responses.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we have shown that training a simple network to detect collisions gives rise to a computation that closely resembles neurons that are sensitive to looming signals. Specifically, we optimized a neural network model to detect whether an object is on a collision course based on visual motion signals (<xref ref-type="fig" rid="fig3">Figure 3</xref>), and found that one class of optimized solution matched the anatomy of motion inputs to LPLC2 neurons (<xref ref-type="fig" rid="fig1">Figures 1</xref>, <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig6">6</xref>). Importantly, this solution reproduces a wide range of experimental observations of LPLC2 neuron responses (<xref ref-type="fig" rid="fig10">Figure 10</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>; <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>).</p><p>The radially structured dendrites of the LPLC2 neuron in the LP can account for its response to motion radiating outward from the receptive field center (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). Our results show that the logic of this computation can be understood in terms of inferential loom detection by the <italic>population</italic> of units. In particular, for an individual detector unit, an inward structure can make a better loom detector than an outward structure, since it is sensitive to colliding objects originating from a wider array of incoming angles (<xref ref-type="fig" rid="fig7">Figure 7</xref>). As the number of units across visual space increases, the performance of the outward-sensitive receptive field structure comes to match the performance of the inward solutions (<xref ref-type="fig" rid="fig9">Figure 9</xref>, <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>, <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>). As the number of units increases, the inhibitory component of the outward solutions also becomes broader as the population size becomes larger, which is crucial for reproducing key experimental observations, such as peripheral inhibition (<xref ref-type="fig" rid="fig10">Figure 10</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). The optimized solutions depend on the number of detectors, and this is likely related to the increasing overlap in receptive fields as the population grows (<xref ref-type="fig" rid="fig6">Figure 6</xref>). This result is consistent with prior work showing that populations of neurons often exhibit different and improved coding strategies compared to individual neurons (<xref ref-type="bibr" rid="bib43">Pasupathy and Connor, 2002</xref>; <xref ref-type="bibr" rid="bib20">Georgopoulos et al., 1986</xref>; <xref ref-type="bibr" rid="bib64">Vogels, 1990</xref>; <xref ref-type="bibr" rid="bib18">Franke et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Zylberberg et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Cafaro et al., 2020</xref>). Thus, understanding anatomical, physiological, and algorithmic properties of individual neurons can require considering the population response. The solutions we found to the loom inference problem suggest that individual LPLC2 responses should be interpreted in light of the population of LPLC2 responses.</p><p>Our results shed light on discussions of <inline-formula><mml:math id="inf60"><mml:mi>η</mml:mi></mml:math></inline-formula>-like (encoding angular size) and <inline-formula><mml:math id="inf61"><mml:mi>ρ</mml:mi></mml:math></inline-formula>-like (encoding angular velocity) looming sensitive neurons in the literature (<xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib67">Wu et al., 2005</xref>; <xref ref-type="bibr" rid="bib31">Liu et al., 2011</xref>; <xref ref-type="bibr" rid="bib54">Shang et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Temizer et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). In particular, these optimized models clarify an interesting but puzzling fact: LPLC2 neurons transform their inputs of direction-selective motion signals to computations of angular size (<xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). Consistent with this tuning, our model also shows a linear relationship between the peak time relative to collision and the <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratio, which should be followed by loom sensitive neurons that encode angular size (<xref ref-type="bibr" rid="bib44">Peek and Card, 2016</xref>). In both cases, these properties appear to be the simple result of training the constrained model to reliably detect looming stimuli.</p><p>The units of the outward solution exhibit sparsity in their responses to looming stimuli, in contrast to the denser representations in the inward solution (<xref ref-type="fig" rid="fig8">Figure 8</xref>). During a looming event, in an outward solution, most of the units are quiet and only a few adjacent units have very large activities, reminiscent of sparse codes that seem to be favored, for instance, in cortical encoding of visual scenes (<xref ref-type="bibr" rid="bib41">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib42">Olshausen and Field, 1997</xref>). Since the readout of our model is a summation of the activities of the units, sparsity does not directly affect the performance of the model, but is an attribute of the favored solution. For a model with a different loss function or with noise, the degree of sparsity might be crucial. For instance, the sparse code of the outward model might make it easier to localize a hit stimulus (<xref ref-type="bibr" rid="bib37">Morimoto et al., 2020</xref>), or might make the population response more robust to noise (<xref ref-type="bibr" rid="bib16">Field, 1994</xref>).</p><p>Experiments have shown that inhibitory circuits play an important role for the selectivity of LPLC2 neurons. For example, motion signals at the periphery of the receptive field of an LPLC2 neuron inhibit its activity. This peripheral inhibition causes various interesting response patterns of the LPLC2 neurons to different types of stimuli (<xref ref-type="fig" rid="fig10">Figure 10E and F</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). However, the structure of this inhibitory field is not fully understood, and our model provides a tool to investigate how the inhibitory inputs to LPLC2 neurons affect circuit performance on loom detection tasks. The strong inhibition on the periphery of the receptive field arises naturally in the outward solutions after optimization. The extent of the inhibitory components increases as more units are added to models (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The broad inhibition appears in our model to suppress responses to the non-hit stimuli, and as in the data, the inhibition is broader than one might expect if the neuron were simply being inhibited by inward motion. These larger inhibitory fields are also consistent with the larger spatial pooling likely to be supplied by inhibitory LPi inputs (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>).</p><p>The synthetic stimuli used to train models in this study were unnatural in two ways. The first way was in the proportion of hits and non-hits. We trained with 25% of the training data representing hits. The true fraction of hits among all stimuli encountered by a fly is undoubtedly much less, and this affects how the loss function weights different types of errors. It is also clear that a false-positive hit (in which a fly might jump to escape an object not on collision course) is much less penalized during evolution than a false-negative (in which a fly doesn’t jump and an object collides, presumably to the detriment of the fly). It remains unclear how to choose these weights in the training data or in the loss function, but they affect the receptive field weights optimized by the model.</p><p>The second issue with the stimuli is that they were caricatures of stimulus types, but did not incorporate the richness of natural stimuli. This richness could include natural textures and spatial statistics (<xref ref-type="bibr" rid="bib48">Ruderman and Bialek, 1994</xref>), which seem to impact motion detection algorithms (<xref ref-type="bibr" rid="bib17">Fitzgerald and Clark, 2015</xref>; <xref ref-type="bibr" rid="bib30">Leonhardt et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2019</xref>). This richness could also include more natural trajectories for approaching objects. Another way to enrich the stimuli would be to add noise, either in inputs to the model or in the model’s units themselves. We explored this briefly by adding self-rotation-generated background motion; under those conditions, both solutions were present but optimized outward solutions performed better than the inward solutions (<xref ref-type="fig" rid="fig9s3">Figure 9—figure supplement 3</xref>, Materials and methods). This indicates that the statistics of the stimuli may play an important role in selecting solutions for loom detection. However, it remains less clear what the true performance limits of loom detection are, since most experiments use substantially impoverished looming stimuli. Moreover, it is challenging to characterize the properties of natural looming events. An interesting future direction will be to investigate the effects of more complex and naturalistic stimuli on the model’s filters and performance, as well as on LPLC2 neuron responses themselves.</p><p>For simplicity, our models did not impose the hexagonal geometry of the compound eye ommatidia. Instead, we assumed that the visual field is separated into a Cartesian lattice with <inline-formula><mml:math id="inf63"><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> spacing, each representing a local motion detector with two spatially separated inputs (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This simplification alters slightly the geometry of the motion signals compared to the real motion detector receptive fields (<xref ref-type="bibr" rid="bib55">Shinomiya et al., 2019</xref>). This could potentially affect the learned spatial weightings and reproduction of the LPLC2 responses to various stimuli, since the specific shapes of the filters matter (<xref ref-type="fig" rid="fig10">Figure 10</xref>). Thus, the hexagonal ommatidial structure and the full extent of inputs to T4 and T5 might be crucial if one wants to make comparisons with the dynamics and detailed responses of LPLC2 neurons. However, this geometric distinction seems unlikely to affect the main results of how to infer the presence of hit stimuli.</p><p>Our model requires a field of estimates of the local motion. Here, we used the simplest model – the Hassenstein-Reichardt correlator model <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> (Materials and methods, <xref ref-type="bibr" rid="bib24">Hassenstein and Reichardt, 1956</xref>) – but the model could be extended by replacing it with a more sophisticated model for motion estimation. Some biophysically realistic ones might take into account synaptic conductances (<xref ref-type="bibr" rid="bib21">Gruntman et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Gruntman et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Badwan et al., 2019</xref>; <xref ref-type="bibr" rid="bib71">Zavatone-Veth et al., 2020</xref>) and could respond to static features of visual scenes (<xref ref-type="bibr" rid="bib3">Agrochao et al., 2020</xref>). Alternatively, in natural environments, contrasts fluctuate in time and space. Thus, if one includes more naturalistic spatial and temporal patterns, one might consider a motion detection model that could adapt to changing contrasts in time and space (<xref ref-type="bibr" rid="bib14">Drews et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Matulis et al., 2020</xref>).</p><p>Although the outward filter of the unit emerges naturally from our gradient descent training protocol, that does not mean that the structure is learned by LPLC2 neurons in the fly. There may be some experience dependent plasticity in the fly eye (<xref ref-type="bibr" rid="bib26">Kikuchi et al., 2012</xref>), but these visual computations are likely to be primarily genetically determined. Thus, one may think of the computation of the LPLC2 neuron as being shaped through millions of years of evolutionary optimization. Optimization algorithms at play in evolution may be able to avoid getting stuck in local optima (<xref ref-type="bibr" rid="bib56">Stanley et al., 2019</xref>), and thus work well with the sort of shallow neural network found in the fly eye.</p><p>In this study, we focused on the motion signal inputs to LPLC2 neurons, and we neglected other inputs to LPLC2 neurons, such as those coming from the lobula that likely report non-motion visual features. It would be interesting to investigate how this additional non-motion information affects the performance and optimized solutions of the inference units. For instance, another lobula columnar neurons, LC4, is loom sensitive and receives inputs in the lobula (<xref ref-type="bibr" rid="bib66">von Reyn et al., 2017</xref>). The LPLC2 and LC4 neurons are the primary excitatory inputs to the GF, which mediates escape behaviors (<xref ref-type="bibr" rid="bib65">von Reyn et al., 2014</xref>; <xref ref-type="bibr" rid="bib2">Ache et al., 2019</xref>). The inference framework set out here would allow one to incorporate parallel non-motion intensity channels, either by adding them into the inputs to the LPLC2-like units, or by adding in a parallel population of LC4-like units. This would require a reformulation of the probabilistic model in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. Notably, one of the most studied loom detecting neurons, the lobula giant movement detector (LGMD) in locusts, does not appear to receive direction-selective inputs, as LPLC2 does (<xref ref-type="bibr" rid="bib47">Rind and Bramwell, 1996</xref>; <xref ref-type="bibr" rid="bib19">Gabbiani et al., 1999</xref>). Thus, the inference framework set out here could be flexibly modified to investigate loom detection under a wide variety of constraints and inputs, which allow it to be applied to other neurons, beyond LPLC2.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Code availability</title><p>Code to perform all simulations in this paper and to reproduce all figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ClarkLabCode/LoomDetectionANN">https://github.com/ClarkLabCode/LoomDetectionANN</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:8fb251126d26c99072427e7043bfdcc5c6b09789;origin=https://github.com/ClarkLabCode/LoomDetectionANN;visit=swh:1:snp:7bf0646fcc23421b8e4321934badbccf03532e3c;anchor=swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448">swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448</ext-link>; <xref ref-type="bibr" rid="bib72">Zhou, 2021</xref>).</p></sec><sec id="s4-2"><title>Coordinate system and stimuli</title><p>We designed a suite of visual stimuli to simulate looming objects, retreating objects, and rotational visual fields. In this section, we describe the suite of stimuli and the coordinate systems used in our simulations (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>In our simulations and training, the fly is at rest on a horizontal plane, with its head pointing in a specific direction. The fly head is modeled to be a point particle with no volume. A three-dimensional right-handed frame of reference <inline-formula><mml:math id="inf64"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> is set up and attached to the fly head at the origin. The <inline-formula><mml:math id="inf65"><mml:mi>z</mml:mi></mml:math></inline-formula> axis points in the anterior direction from the fly head, perpendicular to the line that connects the two eyes, and in the horizontal plane of the fly; the <inline-formula><mml:math id="inf66"><mml:mi>y</mml:mi></mml:math></inline-formula> axis points toward the right eye, also in the horizontal plane; and the <inline-formula><mml:math id="inf67"><mml:mi>x</mml:mi></mml:math></inline-formula> axis points upward and perpendicular to the horizontal plane. Looming or retreating objects are represented in this space by a sphere with radius <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, and the coordinates of an object’s center at time <inline-formula><mml:math id="inf69"><mml:mi>t</mml:mi></mml:math></inline-formula> are denoted as <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the distance between the object center and the fly head is <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Within this coordinate system, we set up cones to represent individual units. The receptive field of LPLC2 neurons is measured at roughly <inline-formula><mml:math id="inf72"><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> in diameter (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). Thus, we here model each unit as a cone with its vertex at the origin and with half-angle of <inline-formula><mml:math id="inf73"><mml:msup><mml:mn>30</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>. For each unit <inline-formula><mml:math id="inf74"><mml:mi>m</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), we set up a local frame of reference <inline-formula><mml:math id="inf76"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>): the <italic>z</italic><sub><italic>m</italic></sub> axis is the axis of the cone and its positive direction points outward from the origin. The local <inline-formula><mml:math id="inf77"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> can be obtained from <inline-formula><mml:math id="inf78"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> by two rotations: around <inline-formula><mml:math id="inf79"><mml:mi>x</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="inf80"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> and around the new <inline-formula><mml:math id="inf81"><mml:msup><mml:mi>y</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup></mml:math></inline-formula> after the rotation around <inline-formula><mml:math id="inf82"><mml:mi>x</mml:mi></mml:math></inline-formula>. For each unit, its cardinal directions are defined as: upward (positive direction of <italic>x</italic><sub><italic>m</italic></sub>), downward (negative direction of <italic>x</italic><sub><italic>m</italic></sub>), leftward (negative direction of <italic>y</italic><sub><italic>m</italic></sub>). and rightward (positive direction of <italic>y</italic><sub><italic>m</italic></sub>). To get the signals that are received by a specific unit <inline-formula><mml:math id="inf83"><mml:mi>m</mml:mi></mml:math></inline-formula>, the coordinates of the object in <inline-formula><mml:math id="inf84"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> are rotated to the local frame of reference <inline-formula><mml:math id="inf85"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Within this coordinate system, we can set up cones representing the extent of a spherical object moving in the space. The visible outline of a spherical object spans a cone with its point at the origin. The half-angle of this cone is a function of time and can be denoted as <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>arcsin</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>One can calculate how the cone of the object overlaps with the receptive field cones of each unit.</p><p>There are multiple layers of processing in the fly visual system (<xref ref-type="bibr" rid="bib59">Takemura et al., 2017</xref>), but here we focus on two coarse grained stages of processing: (1) the estimation of local motion direction from optical intensities by motion detection neurons T4 and T5 and (2) the integration of the flow fields by LPLC2 neurons. In our simulations, the interior of the <inline-formula><mml:math id="inf87"><mml:mi>m</mml:mi></mml:math></inline-formula>th unit cone is represented by a <italic>N</italic>-by-<italic>N</italic> matrix, so that each element in this matrix (except the ones at the four corners) indicates a specific direction in the angular space within the unit cone. If an element also falls within the object cone, then its value is set to 1; otherwise it is 0. Thus, at each time <inline-formula><mml:math id="inf88"><mml:mi>t</mml:mi></mml:math></inline-formula>, this matrix is an optical intensity signal and can be represented by <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf90"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are the coordinates in <inline-formula><mml:math id="inf91"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>. In general, <inline-formula><mml:math id="inf92"><mml:mi>N</mml:mi></mml:math></inline-formula> should be large enough to provide good angular resolutions. Then, <inline-formula><mml:math id="inf93"><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (<inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) motion detectors are evenly distributed within the unit cone, with each occupying an <inline-formula><mml:math id="inf95"><mml:mi>L</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf96"><mml:mi>L</mml:mi></mml:math></inline-formula> grid in the <inline-formula><mml:math id="inf97"><mml:mi>N</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf98"><mml:mi>N</mml:mi></mml:math></inline-formula> matrix, where <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This <inline-formula><mml:math id="inf100"><mml:mi>L</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf101"><mml:mi>L</mml:mi></mml:math></inline-formula> grid represents a <inline-formula><mml:math id="inf102"><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf103"><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> square in the angular space, consistent with the approximate spacing of the inputs of motion detectors T4 and T5. This arrangement effectively uses high spatial resolution intensity data to compute local intensity before it is discretized into motion signals with a resolution of <inline-formula><mml:math id="inf104"><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>. Since the receptive field of an LPLC2 neuron is roughly <inline-formula><mml:math id="inf105"><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>, the value of <inline-formula><mml:math id="inf106"><mml:mi>K</mml:mi></mml:math></inline-formula> is chosen to be 12. To get sufficient angular resolution for the local motion detectors, <inline-formula><mml:math id="inf107"><mml:mi>L</mml:mi></mml:math></inline-formula> is set to be 4, so that <inline-formula><mml:math id="inf108"><mml:mi>N</mml:mi></mml:math></inline-formula> is set to 48.</p><p>Each motion detector is assumed to be a Hassenstein Reichardt Correlator (HRC) and calculates local flow fields from <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib24">Hassenstein and Reichardt, 1956</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The HRC used here has two inputs, separated by <inline-formula><mml:math id="inf110"><mml:msup><mml:mn>5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> in angular space. Each input applies first a spatial filter on the contrast <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and then temporal filters:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf112"><mml:mrow><mml:mpadded width="+5pt"><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mpadded><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a temporal filter and <inline-formula><mml:math id="inf113"><mml:mi>G</mml:mi></mml:math></inline-formula> is a discrete 2d Gaussian kernel with mean <inline-formula><mml:math id="inf114"><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> and standard deviation of <inline-formula><mml:math id="inf115"><mml:msup><mml:mn>2.5</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> to approximate the acceptance angle of the fly photoreceptors (<xref ref-type="bibr" rid="bib57">Stavenga, 2003</xref>). The temporal filter <italic>f</italic><sub>1</sub> was chosen to be an exponential function <inline-formula><mml:math id="inf116"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf117"><mml:mi>τ</mml:mi></mml:math></inline-formula> set to 0.03 seconds (<xref ref-type="bibr" rid="bib51">Salazar-Gatzimas et al., 2016</xref>), and <italic>f</italic><sub>2</sub> a delta function <inline-formula><mml:math id="inf118"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:msup><mml:mi/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This leads to<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>as the local flow field at time <inline-formula><mml:math id="inf119"><mml:mi>t</mml:mi></mml:math></inline-formula> between two inputs located at <inline-formula><mml:math id="inf120"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf121"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>Four types of T4 and T5 neurons have been found that project to layers 1, 2, 3, and 4 of the LP. Each type is sensitive to one of the cardinal directions: down, up, left, right (<xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>). Thus, in our model, there are four non-negative, local flow fields that serve as the only inputs to the model: <inline-formula><mml:math id="inf122"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (downward, corresponding LP layer 4), <inline-formula><mml:math id="inf123"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (upward, LP layer 3), <inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (leftward, LP layer 1), and <inline-formula><mml:math id="inf125"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (rightward, LP layer 2), each of which is a <inline-formula><mml:math id="inf126"><mml:mi>K</mml:mi></mml:math></inline-formula>-by-<inline-formula><mml:math id="inf127"><mml:mi>K</mml:mi></mml:math></inline-formula> matrix. To calculate these matrices, two sets of motion detectors are needed, one for the vertical directions and one for the horizontal directions. The HRC model in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> is direction sensitive and is opponent, meaning that for motion in the preferred (null) direction, the output of the HRC model is positive (negative). Thus, assuming that upward (rightward) is the preferred vertical (horizontal) direction, we obtain the non-negative elements of the four flow fields as<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf128"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the above expressions, for <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, the vertical motion detector at <inline-formula><mml:math id="inf131"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> has its two inputs located at <inline-formula><mml:math id="inf132"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. Similarly, for for <inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, the horizontal motion detector at <inline-formula><mml:math id="inf136"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> has its two inputs located at <inline-formula><mml:math id="inf137"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Using the opponent HRC output as the motion signals for each layer is reasonable because the motion detectors T4 and T5 are highly direction-selective over a large range of inputs (<xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Creamer et al., 2018</xref>) and synaptic, 3-input models for T4 are approximately equivalent to opponent HRC models (<xref ref-type="bibr" rid="bib71">Zavatone-Veth et al., 2020</xref>).</p><p>We simulated the trajectories <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of the object in the frame of reference <inline-formula><mml:math id="inf140"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> at a time resolution of 0.01 s, which is also be the time step of the training and testing stimuli. For hit, miss, and retreat cases, the trajectories of the object are always straight lines, and the velocities of the object were randomly sampled from a range <inline-formula><mml:math id="inf141"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the trajectories confined to be within a sphere of <inline-formula><mml:math id="inf142"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> centered at the fly head. The radius of the object, <inline-formula><mml:math id="inf143"><mml:mi>R</mml:mi></mml:math></inline-formula>, is always set to be one except in the rotational stimuli. To generate rotational stimuli, we placed 100 objects with various radii selected uniformly from <inline-formula><mml:math id="inf144"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> at random distances ([5, 15]) and positions around the fly, and rotated them all around a randomly chosen axis. The rotational speed was chosen from a Gaussian distribution with mean <inline-formula><mml:math id="inf145"><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf146"><mml:mrow><mml:msup><mml:mn>200</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, a reasonable rotational velocity for walking flies (<xref ref-type="bibr" rid="bib13">DeAngelis et al., 2019</xref>). In one case, training data included both an object moving and global rotation due to self-rotation (<xref ref-type="fig" rid="fig9s3">Figure 9—figure supplement 3</xref>). These stimuli were simply combinations of the rotational stimuli with the other three cases (hit, miss, and retreat), so that the object that moves in the depth dimension also rotates together with the background.</p><p>We reproduced a range of stimuli used in a previous study (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>) and tested them on our trained model (<xref ref-type="fig" rid="fig10">Figure 10B–H</xref>). To match the cardinal directions of LP layers (<xref ref-type="fig" rid="fig1">Figure 1</xref>), we have rotated the stimuli (except in <xref ref-type="fig" rid="fig10">Figure 10B</xref>) 45 degrees compared with the ones displayed in the figures in <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>. The disc (<xref ref-type="fig" rid="fig10">Figure 10B and C</xref>) expands from <inline-formula><mml:math id="inf147"><mml:msup><mml:mn>20</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> to <inline-formula><mml:math id="inf148"><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> with an edge speed of <inline-formula><mml:math id="inf149"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>. All the bar and edge motions have an edge speed of <inline-formula><mml:math id="inf150"><mml:mrow><mml:msup><mml:mn>20</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>. The width of the bars are <inline-formula><mml:math id="inf151"><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> (right panel of <xref ref-type="fig" rid="fig10">Figure 10E and H</xref>), <inline-formula><mml:math id="inf152"><mml:msup><mml:mn>20</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> (middle panel of <xref ref-type="fig" rid="fig10">Figure 10E</xref>), and <inline-formula><mml:math id="inf153"><mml:msup><mml:mn>10</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> (all the rest). All the responses of the models (except in <xref ref-type="fig" rid="fig10">Figure 10B</xref>) have been normalized by the peak of the response to the expanding disc (<xref ref-type="fig" rid="fig10">Figure 10B</xref>).</p><p>We created a range of hit stimuli with various <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios: <inline-formula><mml:math id="inf155"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.01</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf156"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.02</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf157"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.04</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf158"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.08</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf159"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.10</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf160"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.12</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf161"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.14</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf162"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.16</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf163"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.18</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf164"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0.20</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>. The radius <inline-formula><mml:math id="inf165"><mml:mi>R</mml:mi></mml:math></inline-formula> of the spherical object is fixed to be 1, and the velocity is changed accordingly to achieve different <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> ratios.</p></sec><sec id="s4-3"><title>Models</title><p>LPLC2 neurons have four dendritic structures in the four LP layers, and they receive direct excitatory inputs from T4/T5 motion detection neurons (<xref ref-type="bibr" rid="bib32">Maisak et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). It has been proposed that each dendritic structure also receives inhibitory inputs mediated by lobula plate intrinsic interneurons, such as LPi4-3 (<xref ref-type="bibr" rid="bib28">Klapoetke et al., 2017</xref>). We built two model units to approximate this anatomy.</p><p>LRF models A linear receptive field (LRF) model is characterized by a real-valued filter, represented by a 12-by-12 matrix <inline-formula><mml:math id="inf167"><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The elements of the filter combine the effects of the excitatory and inhibitory inputs and can take both positive (stronger excitation) and negative (stronger inhibition) values. We rotate <inline-formula><mml:math id="inf168"><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup></mml:math></inline-formula> counterclockwise by multiples of 90° to obtain the filters that are used to integrate the four motion signals: <inline-formula><mml:math id="inf169"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf171"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf172"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Specifically, we define the corresponding four filters as: <inline-formula><mml:math id="inf173"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>270</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>r</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>rotate</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>r</mml:mtext></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf175"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf176"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In addition, we impose mirror symmetry on the filters, and with the above definitions of the rotated filters, the upper half of <inline-formula><mml:math id="inf177"><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup></mml:math></inline-formula> is a mirror image of the lower half of <inline-formula><mml:math id="inf178"><mml:msup><mml:mi>W</mml:mi><mml:mtext>r</mml:mtext></mml:msup></mml:math></inline-formula>. Thus, there are in total 72 parameters in the filters. In fact, since only the elements within a 60 degree cone contribute to the filter for the units, the corners are excluded, resulting in only 56 trainable parameters.</p><p>In computer simulations, the filters or weights and flow fields are flattened to be one-dimensional column vectors. The response of a single LRF unit <inline-formula><mml:math id="inf179"><mml:mi>m</mml:mi></mml:math></inline-formula> is:<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="260%" minsize="260%">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo maxsize="260%" minsize="260%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf180"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the rectified linear unit (ReLU), and <inline-formula><mml:math id="inf181"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mtext>r</mml:mtext></mml:msup><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> is the intercept (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The ReLU is used as the activation function in all the figures, except in one panel (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>), where an exponential linear unit is used:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mtext>ELU</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em"><mml:mtr><mml:mtd><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the LRF model is very close to a generalized linear model, except that it includes an additional activation function <inline-formula><mml:math id="inf183"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. This activation function changes the convexity of the model to make it a non-convex optimization problem, in general.</p><p>RI models The rectified inhibition (RI) models have two types of nonnegative filters, one excitatory and one inhibitory, represented by <inline-formula><mml:math id="inf184"><mml:msup><mml:mi>W</mml:mi><mml:mtext>e</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf185"><mml:msup><mml:mi>W</mml:mi><mml:mtext>i</mml:mtext></mml:msup></mml:math></inline-formula>, respectively (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Each filter is a 12-by-12 matrix. The same rotational and mirror symmetries are imposed as in the LRF models, which leads to four excitatory filters as: <inline-formula><mml:math id="inf186"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>e</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>270</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf187"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>e</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf188"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>e</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf189"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>e</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and four inhibitory filters as: <inline-formula><mml:math id="inf190"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>i</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>270</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf191"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>i</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>90</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf192"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>i</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>180</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf193"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>rotate</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mtext>i</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, there are in total 112 parameters in the two sets of filters, excluding the elements in the corners.</p><p>The responses of the inhibitory units are:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf194"><mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the ReLU, and <inline-formula><mml:math id="inf195"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mtext>i</mml:mtext></mml:msup><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> is the intercept. In the RI model, the rectification of each inhibitory layer is motivated by the LPi neurons, which mediate the inhibition within each layer, and could themselves rectify their inhibitory input into LPLC2. The response of a single RI unit <inline-formula><mml:math id="inf196"><mml:mi>m</mml:mi></mml:math></inline-formula> is<disp-formula id="equ8"><label>(5)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mtext>e</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf197"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mtext>e</mml:mtext></mml:msup><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> is the intercept (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Interestingly, the RI models become equivalent to the LRF models if we remove the ReLU in the inhibitions and define <inline-formula><mml:math id="inf198"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf199"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf200"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf201"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>r</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>e</mml:mtext></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mtext>i</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf202"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mtext>r</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mtext>e</mml:mtext></mml:msup><mml:mo>-</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mtext>i</mml:mtext></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>For both the LRF and RI models, the inferred probability of hit for a specific trajectory is<disp-formula id="equ9"><label>(6)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>hit</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf203"><mml:mi>T</mml:mi></mml:math></inline-formula> is the total number of time steps in the trajectory and <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the sigmoid function. Since we are adding two intercepts (<inline-formula><mml:math id="inf205"><mml:msup><mml:mi>b</mml:mi><mml:mtext>r</mml:mtext></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf206"><mml:mi>b</mml:mi></mml:math></inline-formula>) to the LRF models, and three intercepts (<inline-formula><mml:math id="inf207"><mml:msup><mml:mi>b</mml:mi><mml:mtext>i</mml:mtext></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf208"><mml:msup><mml:mi>b</mml:mi><mml:mtext>e</mml:mtext></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf209"><mml:mi>b</mml:mi></mml:math></inline-formula>) to the RI models, there are 58 and 115 parameters to train the two models, respectively.</p></sec><sec id="s4-4"><title>Training and testing</title><p>We created a synthetic data set containing four types of motion: <italic>loom-and-hit</italic>, <italic>loom-and-miss</italic>, <italic>retreat</italic>, and <italic>rotation</italic>. The proportions of these types were 0.25, 0.125, 0.125, and 0.5, respectively. In total, there were 5200 trajectories, with 4000 for training and 1200 for testing. Trajectories with motion type <italic>loom-and-hit</italic> are labeled as hit or <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (probability of hit is 1), while trajectories of other motion types are labeled as non-hit or <inline-formula><mml:math id="inf211"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (probability of hit is 0), where <inline-formula><mml:math id="inf212"><mml:mi>n</mml:mi></mml:math></inline-formula> is the index of each specific sample. Models with smaller <inline-formula><mml:math id="inf213"><mml:mi>M</mml:mi></mml:math></inline-formula> have fewer trajectories in the receptive field of any unit. For stability of training, we therefore increased the number of trajectories by factors of eight, four, and two for <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p><p>The loss function to be minimized in our training was the cross entropy between the label <italic>y</italic><sub><italic>n</italic></sub> and the inferred probability of hit <inline-formula><mml:math id="inf215"><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>hit</mml:mtext></mml:msub></mml:math></inline-formula>, and averaged across all samples, together with a regularization term:<disp-formula id="equ10"><label>(7)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mtext>cross entropy loss</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>hit</mml:mtext></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>hit</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>W</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mi>W</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf216"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>hit</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the inferred probability of hit for sample <inline-formula><mml:math id="inf217"><mml:mi>n</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf218"><mml:mi>β</mml:mi></mml:math></inline-formula> is the strength of the <inline-formula><mml:math id="inf219"><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> regularization, and <inline-formula><mml:math id="inf220"><mml:mi>W</mml:mi></mml:math></inline-formula> represents all the effective parameters in the two excitatory and inhibitory filters.</p><p>The strength of the regularization <inline-formula><mml:math id="inf221"><mml:mi>β</mml:mi></mml:math></inline-formula> was set to be <inline-formula><mml:math id="inf222"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, which was obtained by gradually increasing <inline-formula><mml:math id="inf223"><mml:mi>β</mml:mi></mml:math></inline-formula> until the performance of the model on test data started to drop. The regularization sped up convergence of solutions, but the regularization strength did not strongly influence the main results in the paper.</p><p>To speed up training, rather than taking a temporal average as shown in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, a snapshot was sampled randomly from each trajectory, and the probability of hit of this snapshot was used to represent the whole trajectory, that is, <inline-formula><mml:math id="inf224"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>hit</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf225"><mml:mi>t</mml:mi></mml:math></inline-formula> is a random sample from <inline-formula><mml:math id="inf226"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. Mini-batch gradient descent was used in training, and the learning rate was 0.001.</p><p>After training, the models were tested on the entire trajectories with the probability of hit defined in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>. Models trained only on snapshots performed well on the test data. During testing, the performance of the model was evaluated by the area under the curve (AUC) of the receiver operating characteristic (ROC) and precision-recall (PR) curves (<xref ref-type="bibr" rid="bib23">Hanley and McNeil, 1982</xref>; <xref ref-type="bibr" rid="bib11">Davis and Goadrich, 2006</xref>). TensorFlow (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>) was used to train all models.</p></sec><sec id="s4-5"><title>Clustering the solutions</title><p>We used the following procedure to cluster the solutions. For the LRF models, the filter of each solution was simply flattened to form a vector. But, for the RI mdoels, each solution had an excitatory and an inhibitory filter. We flattened these two filters, and concatenated them into a single vector. (The elements at the corners were deleted since they are outside of the receptive field.) Thus, each solution was represented by a vector, from which we calculated the cosine distance for each pair of solutions. The obtained distance matrix was then fed into a hierarchical clustering algorithm (<xref ref-type="bibr" rid="bib63">Virtanen et al., 2020</xref>). After obtaining the hierarchical clustering, the outward and inward filters were identified by their shape. We counted the positive filter elements corresponding to flow fields with components radiating outward and subtracted the number of positive filter elements corresponding to flow fields with components directed inward. If the resulting value was positive, the filters were labeled as outward; otherwise, the filters were labeled as inward. We could also sum the elements rather than count them, but we found that the latter was more robust. If the elements in the concatenated vector were all close to zero, then the corresponding filters were labeled as zero solutions.</p></sec><sec id="s4-6"><title>Statistics</title><p>To calculate the fraction of active units for the model with <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), we examined the response curves of each unit to all trajectories of a specific type of stimuli. If a unit response is above baseline (dotted lines in <xref ref-type="fig" rid="fig7">Figure 7B</xref>), then the unit is counted as active. For each trajectory/stimulus, we obtained the number of active units. We used this number to calculate the mean and standard deviation of active units across all the trajectories within each type of stimulus (hit, miss, retreat, rotation).</p><p>For a model with <inline-formula><mml:math id="inf228"><mml:mi>M</mml:mi></mml:math></inline-formula> units, where <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>64</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>192</mml:mn><mml:mo>,</mml:mo><mml:mn>256</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, 200 random initializations were used to train it. For the LRF models, within these 200 training runs, the number of outward solutions <inline-formula><mml:math id="inf230"><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub></mml:math></inline-formula> were (starting from smaller values of <inline-formula><mml:math id="inf231"><mml:mi>M</mml:mi></mml:math></inline-formula>) 45, 59, 60, 65, 52, 63, 52, 57, 58, 49, and the number of inward solutions <inline-formula><mml:math id="inf232"><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula> were 142, 135, 127, 119, 139, 133, 130, 118, 123, 119. For the RI models, the number of outward solutions <inline-formula><mml:math id="inf233"><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub></mml:math></inline-formula> were 44, 46, 48, 50, 48, 50, 53, 55, 58, 64, and the number of inward solutions <inline-formula><mml:math id="inf234"><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:math></inline-formula> were 39, 40, 39, 46, 53, 51, 35, 38, 12, 10. The average score curves and points in <xref ref-type="fig" rid="fig9">Figure 9A</xref>, <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1A</xref> and <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2A</xref> were obtained by taking the average among each type of solution, with the shading indicating the standard deviations. The curves and point in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1C</xref> are the ratio of the number of outward solutions to the number of inward solutions. To obtain error bars (grey shading), we considered the training results as a binomial distribution, with the probability of obtaining an outward solution being <inline-formula><mml:math id="inf235"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and with the probability of obtaining an inward solution being <inline-formula><mml:math id="inf236"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, the standard deviation of this binomial distribution is <inline-formula><mml:math id="inf237"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. From this, we calculate the error bars as the propagated standard deviation (<xref ref-type="bibr" rid="bib36">Morgan et al., 1990</xref>):<disp-formula id="equ11"><label>(8)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mtext>propagated error</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>σ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>σ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Software</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Methodology, Supervision, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Methodology, Supervision, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-72067-transrepform1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Code to perform all simulations in this paper and to reproduce all figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ClarkLabCode/LoomDetectionANN">https://github.com/ClarkLabCode/LoomDetectionANN</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:8fb251126d26c99072427e7043bfdcc5c6b09789;origin=https://github.com/ClarkLabCode/LoomDetectionANN;visit=swh:1:snp:7bf0646fcc23421b8e4321934badbccf03532e3c;anchor=swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448">swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>Research supported in part by NSF grants DMS-1513594, CCF-1839308, DMS-2015397, NIH R01EY026555, a JP Morgan Faculty Research Award, and the Kavli Foundation. We thank G Card and N Klapoetke for sharing data traces from their paper. We thank members of the Clark lab for discussions and comments.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><conf-name>Tensorflow: A system for large-scale machine learning</conf-name><article-title>In 12th USENIX symposium on operating systems design and implementation</article-title><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ache</surname><given-names>JM</given-names></name><name><surname>Polsky</surname><given-names>J</given-names></name><name><surname>Alghailani</surname><given-names>S</given-names></name><name><surname>Parekh</surname><given-names>R</given-names></name><name><surname>Breads</surname><given-names>P</given-names></name><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>von Reyn</surname><given-names>CR</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural Basis for Looming Size and Velocity Encoding in the <italic>Drosophila</italic> Giant Fiber Escape Pathway</article-title><source>Current Biology</source><volume>29</volume><fpage>1073</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.01.079</pub-id><pub-id pub-id-type="pmid">30827912</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrochao</surname><given-names>M</given-names></name><name><surname>Tanaka</surname><given-names>R</given-names></name><name><surname>Salazar-Gatzimas</surname><given-names>E</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mechanism for analogous illusory motion perception in flies and humans</article-title><source>PNAS</source><volume>117</volume><fpage>23044</fpage><lpage>23053</lpage><pub-id pub-id-type="doi">10.1073/pnas.2002937117</pub-id><pub-id pub-id-type="pmid">32839324</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badwan</surname><given-names>BA</given-names></name><name><surname>Creamer</surname><given-names>MS</given-names></name><name><surname>Zavatone-Veth</surname><given-names>JA</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dynamic nonlinearities enable direction opponency in <italic>Drosophila</italic> elementary motion detectors</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1318</fpage><lpage>1326</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0443-y</pub-id><pub-id pub-id-type="pmid">31346296</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>W</given-names></name><name><surname>Tronick</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Infant responses to impending collision: optical and real</article-title><source>Science</source><volume>171</volume><fpage>818</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1126/science.171.3973.818</pub-id><pub-id pub-id-type="pmid">5541165</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhattacharyya</surname><given-names>K</given-names></name><name><surname>McLean</surname><given-names>DL</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual Threat Assessment and Reticulospinal Encoding of Calibrated Responses in Larval Zebrafish</article-title><source>Current Biology</source><volume>27</volume><fpage>2751</fpage><lpage>2762</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.08.012</pub-id><pub-id pub-id-type="pmid">28889979</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cafaro</surname><given-names>J</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Field</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Global Motion Processing by Populations of Direction-Selective Retinal Ganglion Cells</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>5807</fpage><lpage>5819</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0564-20.2020</pub-id><pub-id pub-id-type="pmid">32561674</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Card</surname><given-names>G</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visually mediated motor planning in the escape response of <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>18</volume><fpage>1300</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.07.094</pub-id><pub-id pub-id-type="pmid">18760606</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Mandel</surname><given-names>HB</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Asymmetric ON-OFF processing of visual motion cancels variability induced by the structure of natural scenes</article-title><source>eLife</source><volume>8</volume><elocation-id>e47579</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47579</pub-id><pub-id pub-id-type="pmid">31613221</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creamer</surname><given-names>MS</given-names></name><name><surname>Mano</surname><given-names>O</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual Control of Walking Speed in <italic>Drosophila</italic></article-title><source>Neuron</source><volume>100</volume><fpage>1460</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.028</pub-id><pub-id pub-id-type="pmid">30415994</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>J</given-names></name><name><surname>Goadrich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><conf-name>The relationship between Precision-Recall and ROC curves</conf-name><article-title>Proceedings of the 23rd international conference on Machine learning</article-title><conf-loc>Pittsburgh Pennsylvania</conf-loc><fpage>233</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1145/1143844.1143874</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Loom-sensitive neurons link computation to action in the <italic>Drosophila</italic> visual system</article-title><source>Current Biology</source><volume>22</volume><fpage>353</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.01.007</pub-id><pub-id pub-id-type="pmid">22305754</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname><given-names>BD</given-names></name><name><surname>Zavatone-Veth</surname><given-names>JA</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The manifold structure of limb coordination in walking <italic>Drosophila</italic></article-title><source>eLife</source><volume>8</volume><elocation-id>e46409</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46409</pub-id><pub-id pub-id-type="pmid">31250807</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drews</surname><given-names>MS</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Pirogova</surname><given-names>N</given-names></name><name><surname>Richter</surname><given-names>FG</given-names></name><name><surname>Schuetzenberger</surname><given-names>A</given-names></name><name><surname>Braun</surname><given-names>L</given-names></name><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamic Signal Compression for Robust Motion Vision in Flies</article-title><source>Current Biology</source><volume>30</volume><fpage>209</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.035</pub-id><pub-id pub-id-type="pmid">31928873</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>TW</given-names></name><name><surname>Gebhardt</surname><given-names>C</given-names></name><name><surname>Naumann</surname><given-names>EA</given-names></name><name><surname>Riegler</surname><given-names>C</given-names></name><name><surname>Ahrens</surname><given-names>MB</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name><name><surname>Del Bene</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural Circuits Underlying Visually Evoked Escapes in Larval Zebrafish</article-title><source>Neuron</source><volume>89</volume><fpage>613</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.021</pub-id><pub-id pub-id-type="pmid">26804997</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>What Is the Goal of Sensory Coding?</article-title><source>Neural Computation</source><volume>6</volume><fpage>559</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1162/neco.1994.6.4.559</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Nonlinear circuits for naturalistic visual motion estimation</article-title><source>eLife</source><volume>4</volume><elocation-id>e09123</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.09123</pub-id><pub-id pub-id-type="pmid">26499494</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>F</given-names></name><name><surname>Fiscella</surname><given-names>M</given-names></name><name><surname>Sevelev</surname><given-names>M</given-names></name><name><surname>Roska</surname><given-names>B</given-names></name><name><surname>Hierlemann</surname><given-names>A</given-names></name><name><surname>da Silveira</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Structures of Neural Correlation and How They Favor Coding</article-title><source>Neuron</source><volume>89</volume><fpage>409</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.037</pub-id><pub-id pub-id-type="pmid">26796692</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabbiani</surname><given-names>F</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Computation of object approach by a wide-field, motion-sensitive neuron</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>1122</fpage><lpage>1141</lpage><pub-id pub-id-type="pmid">9920674</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Kettner</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Neuronal population coding of movement direction</article-title><source>Science</source><volume>233</volume><fpage>1416</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1126/science.3749885</pub-id><pub-id pub-id-type="pmid">3749885</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruntman</surname><given-names>E</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Simple integration of fast excitation and offset, delayed inhibition computes directional selectivity in <italic>Drosophila</italic></article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>250</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0046-4</pub-id><pub-id pub-id-type="pmid">29311742</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruntman</surname><given-names>E</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The computation of directional selectivity in the <italic>Drosophila</italic> OFF motion pathway</article-title><source>eLife</source><volume>8</volume><elocation-id>e50706</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50706</pub-id><pub-id pub-id-type="pmid">31825313</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanley</surname><given-names>JA</given-names></name><name><surname>McNeil</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The meaning and use of the area under a receiver operating characteristic (ROC) curve</article-title><source>Radiology</source><volume>143</volume><fpage>29</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1148/radiology.143.1.7063747</pub-id><pub-id pub-id-type="pmid">7063747</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassenstein</surname><given-names>B</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1956">1956</year><article-title>Systemtheoretische Analyse der Zeit-, Reihenfolgen- und Vorzeichenauswertung bei der Bewegungsperzeption des Rüsselkäfers Chlorophanus</article-title><source>Zeitschrift Für Naturforschung B</source><volume>11</volume><fpage>513</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1515/znb-1956-9-1004</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hervais-Adelman</surname><given-names>A</given-names></name><name><surname>Legrand</surname><given-names>LB</given-names></name><name><surname>Zhan</surname><given-names>M</given-names></name><name><surname>Tamietto</surname><given-names>M</given-names></name><name><surname>de Gelder</surname><given-names>B</given-names></name><name><surname>Pegna</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Looming sensitive cortical regions without V1 input: evidence from a patient with bilateral cortical blindness</article-title><source>Frontiers in Integrative Neuroscience</source><volume>9</volume><elocation-id>51</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2015.00051</pub-id><pub-id pub-id-type="pmid">26557059</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikuchi</surname><given-names>A</given-names></name><name><surname>Ohashi</surname><given-names>S</given-names></name><name><surname>Fuse</surname><given-names>N</given-names></name><name><surname>Ohta</surname><given-names>T</given-names></name><name><surname>Suzuki</surname><given-names>M</given-names></name><name><surname>Suzuki</surname><given-names>Y</given-names></name><name><surname>Fujita</surname><given-names>T</given-names></name><name><surname>Miyamoto</surname><given-names>T</given-names></name><name><surname>Aonishi</surname><given-names>T</given-names></name><name><surname>Miyakawa</surname><given-names>H</given-names></name><name><surname>Morimoto</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Experience-dependent plasticity of the optomotor response in <italic>Drosophila melanogaster</italic></article-title><source>Developmental Neuroscience</source><volume>34</volume><fpage>533</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1159/000346266</pub-id><pub-id pub-id-type="pmid">23406844</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>SM</given-names></name><name><surname>Dykeman</surname><given-names>C</given-names></name><name><surname>Redgrave</surname><given-names>P</given-names></name><name><surname>Dean</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Use of a distracting task to obtain defensive head movements to looming visual stimuli by human adults in a laboratory setting</article-title><source>Perception</source><volume>21</volume><fpage>245</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1068/p210245</pub-id><pub-id pub-id-type="pmid">1513673</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klapoetke</surname><given-names>NC</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Rogers</surname><given-names>EM</given-names></name><name><surname>Breads</surname><given-names>P</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ultra-selective looming detection from radial motion opponency</article-title><source>Nature</source><volume>551</volume><fpage>237</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1038/nature24626</pub-id><pub-id pub-id-type="pmid">29120418</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krapp</surname><given-names>HG</given-names></name><name><surname>Hengstenberg</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Estimation of self-motion by optic flow processing in single visual interneurons</article-title><source>Nature</source><volume>384</volume><fpage>463</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1038/384463a0</pub-id><pub-id pub-id-type="pmid">8945473</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Ammer</surname><given-names>G</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Asymmetry of <italic>Drosophila</italic> ON and OFF motion detectors enhances real-world velocity estimation</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>706</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1038/nn.4262</pub-id><pub-id pub-id-type="pmid">26928063</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>YJ</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neuronal responses to looming objects in the superior colliculus of the cat</article-title><source>Brain, Behavior and Evolution</source><volume>77</volume><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1159/000327045</pub-id><pub-id pub-id-type="pmid">21546772</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maisak</surname><given-names>MS</given-names></name><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Ammer</surname><given-names>G</given-names></name><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Schilling</surname><given-names>T</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name><name><surname>Reiff</surname><given-names>DF</given-names></name><name><surname>Hopp</surname><given-names>E</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A directional tuning map of <italic>Drosophila</italic> elementary motion detectors</article-title><source>Nature</source><volume>500</volume><fpage>212</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1038/nature12320</pub-id><pub-id pub-id-type="pmid">23925246</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mano</surname><given-names>O</given-names></name><name><surname>Creamer</surname><given-names>MS</given-names></name><name><surname>Badwan</surname><given-names>BA</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predicting individual neuron responses with anatomically constrained task optimization</article-title><source>Current Biology</source><volume>31</volume><fpage>4062</fpage><lpage>4075</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.090</pub-id><pub-id pub-id-type="pmid">34324832</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matulis</surname><given-names>CA</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Gonzalez-Suarez</surname><given-names>AD</given-names></name><name><surname>Behnia</surname><given-names>R</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Heterogeneous Temporal Contrast Adaptation in <italic>Drosophila</italic> Direction-Selective Circuits</article-title><source>Current Biology</source><volume>30</volume><fpage>222</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.11.077</pub-id><pub-id pub-id-type="pmid">31928874</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauss</surname><given-names>AS</given-names></name><name><surname>Pankova</surname><given-names>K</given-names></name><name><surname>Arenz</surname><given-names>A</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural Circuit to Integrate Opposing Motions in the Visual Field</article-title><source>Cell</source><volume>162</volume><fpage>351</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.06.035</pub-id><pub-id pub-id-type="pmid">26186189</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>MG</given-names></name><name><surname>Henrion</surname><given-names>M</given-names></name><name><surname>Small</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1990">1990</year><source>Uncertainty: A Guide to Dealing with Uncertainty in Quantitative Risk and Policy Analysis</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511840609</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morimoto</surname><given-names>MM</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>A</given-names></name><name><surname>Rogers</surname><given-names>EM</given-names></name><name><surname>Wong</surname><given-names>AM</given-names></name><name><surname>Isaacson</surname><given-names>MD</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatial readout of visual looming in the central brain of <italic>Drosophila</italic></article-title><source>eLife</source><volume>9</volume><elocation-id>e57685</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57685</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muijres</surname><given-names>FT</given-names></name><name><surname>Elzinga</surname><given-names>MJ</given-names></name><name><surname>Melis</surname><given-names>JM</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flies evade looming targets by executing rapid visually directed banked turns</article-title><source>Science</source><volume>344</volume><fpage>172</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1126/science.1248955</pub-id><pub-id pub-id-type="pmid">24723606</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Münch</surname><given-names>TA</given-names></name><name><surname>da Silveira</surname><given-names>RA</given-names></name><name><surname>Siegert</surname><given-names>S</given-names></name><name><surname>Viney</surname><given-names>TJ</given-names></name><name><surname>Awatramani</surname><given-names>GB</given-names></name><name><surname>Roska</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Approach sensitivity in the retina processed by a multifunctional neural circuit</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1308</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1038/nn.2389</pub-id><pub-id pub-id-type="pmid">19734895</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>D</given-names></name><name><surname>Tomsic</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Computation of object approach by a system of visual motion-sensitive neurons in the crab Neohelice</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1477</fpage><lpage>1490</lpage><pub-id pub-id-type="doi">10.1152/jn.00921.2013</pub-id><pub-id pub-id-type="pmid">24899670</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Population coding of shape in area V4</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1038/nn972</pub-id><pub-id pub-id-type="pmid">12426571</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparative approaches to escape</article-title><source>Current Opinion in Neurobiology</source><volume>41</volume><fpage>167</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.09.012</pub-id><pub-id pub-id-type="pmid">27710794</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regan</surname><given-names>D</given-names></name><name><surname>Beverley</surname><given-names>KI</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Looming detectors in the human visual pathway</article-title><source>Vision Research</source><volume>18</volume><fpage>415</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(78)90051-2</pub-id><pub-id pub-id-type="pmid">664320</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rind</surname><given-names>FC</given-names></name><name><surname>Bramwell</surname><given-names>DI</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural network based on the input organization of an identified neuron signaling impending collision</article-title><source>Journal of Neurophysiology</source><volume>75</volume><fpage>967</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.75.3.967</pub-id><pub-id pub-id-type="pmid">8867110</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruderman</surname><given-names>DL</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistics of natural images: Scaling in the woods</article-title><source>Physical Review Letters</source><volume>73</volume><fpage>814</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.73.814</pub-id><pub-id pub-id-type="pmid">10057546</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabbah</surname><given-names>S</given-names></name><name><surname>Gemmer</surname><given-names>JA</given-names></name><name><surname>Bhatia-Lin</surname><given-names>A</given-names></name><name><surname>Manoff</surname><given-names>G</given-names></name><name><surname>Castro</surname><given-names>G</given-names></name><name><surname>Siegel</surname><given-names>JK</given-names></name><name><surname>Jeffery</surname><given-names>N</given-names></name><name><surname>Berson</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A retinal code for motion along the gravitational and body axes</article-title><source>Nature</source><volume>546</volume><fpage>492</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1038/nature22818</pub-id><pub-id pub-id-type="pmid">28607486</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salay</surname><given-names>LD</given-names></name><name><surname>Ishiko</surname><given-names>N</given-names></name><name><surname>Huberman</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A midline thalamic circuit determines reactions to visual threat</article-title><source>Nature</source><volume>557</volume><fpage>183</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0078-2</pub-id><pub-id pub-id-type="pmid">29720647</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salazar-Gatzimas</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Creamer</surname><given-names>MS</given-names></name><name><surname>Mano</surname><given-names>O</given-names></name><name><surname>Mandel</surname><given-names>HB</given-names></name><name><surname>Matulis</surname><given-names>CA</given-names></name><name><surname>Pottackal</surname><given-names>J</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Direct Measurement of Correlation Responses in <italic>Drosophila</italic> Elementary Motion Detectors Reveals Fast Timescale Tuning</article-title><source>Neuron</source><volume>92</volume><fpage>227</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.017</pub-id><pub-id pub-id-type="pmid">27710784</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santer</surname><given-names>RD</given-names></name><name><surname>Simmons</surname><given-names>PJ</given-names></name><name><surname>Rind</surname><given-names>FC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Gliding behaviour elicited by lateral looming stimuli in flying locusts</article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>191</volume><fpage>61</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1007/s00359-004-0572-x</pub-id><pub-id pub-id-type="pmid">15558287</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>K</given-names></name><name><surname>Yamawaki</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Role of a looming-sensitive neuron in triggering the defense behavior of the praying mantis Tenodera aridifolia</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>671</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1152/jn.00049.2014</pub-id><pub-id pub-id-type="pmid">24848471</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Cao</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>BRAIN CIRCUITS. A parvalbumin-positive excitatory visual pathway to trigger fear responses in mice</article-title><source>Science</source><volume>348</volume><fpage>1472</fpage><lpage>1477</lpage><pub-id pub-id-type="doi">10.1126/science.aaa8694</pub-id><pub-id pub-id-type="pmid">26113723</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinomiya</surname><given-names>K</given-names></name><name><surname>Huang</surname><given-names>G</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Parag</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>CS</given-names></name><name><surname>Aniceto</surname><given-names>R</given-names></name><name><surname>Ansari</surname><given-names>N</given-names></name><name><surname>Cheatham</surname><given-names>N</given-names></name><name><surname>Lauchie</surname><given-names>S</given-names></name><name><surname>Neace</surname><given-names>E</given-names></name><name><surname>Ogundeyi</surname><given-names>O</given-names></name><name><surname>Ordish</surname><given-names>C</given-names></name><name><surname>Peel</surname><given-names>D</given-names></name><name><surname>Shinomiya</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>C</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Talebi</surname><given-names>I</given-names></name><name><surname>Rivlin</surname><given-names>PK</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Comparisons between the ON- and OFF-edge motion pathways in the <italic>Drosophila</italic> brain</article-title><source>eLife</source><volume>8</volume><elocation-id>e40025</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.40025</pub-id><pub-id pub-id-type="pmid">30624205</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname><given-names>KO</given-names></name><name><surname>Clune</surname><given-names>J</given-names></name><name><surname>Lehman</surname><given-names>J</given-names></name><name><surname>Miikkulainen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Designing neural networks through neuroevolution</article-title><source>Nature Machine Intelligence</source><volume>1</volume><fpage>24</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1038/s42256-018-0006-z</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stavenga</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Angular and spectral sensitivity of fly photoreceptors. II. Dependence on facet lens F-number and rhabdomere type in <italic>Drosophila</italic></article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>189</volume><fpage>189</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/s00359-003-0390-6</pub-id><pub-id pub-id-type="pmid">12664095</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>H</given-names></name><name><surname>Frost</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Computation of different optical variables of looming objects in pigeon nucleus rotundus neurons</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>296</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/1110</pub-id><pub-id pub-id-type="pmid">10195163</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takemura</surname><given-names>SY</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The comprehensive connectome of a neural substrate for on motion detection in <italic>Drosophila</italic></article-title><source>eLife</source><volume>6</volume><elocation-id>e24394</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.24394</pub-id><pub-id pub-id-type="pmid">28432786</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanouye</surname><given-names>MA</given-names></name><name><surname>Wyman</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Motor outputs of giant nerve fiber in <italic>Drosophila</italic></article-title><source>Journal of Neurophysiology</source><volume>44</volume><fpage>405</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1152/jn.1980.44.2.405</pub-id><pub-id pub-id-type="pmid">6774064</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temizer</surname><given-names>I</given-names></name><name><surname>Donovan</surname><given-names>JC</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name><name><surname>Semmelhack</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Visual Pathway for Looming-Evoked Escape in Larval Zebrafish</article-title><source>Current Biology</source><volume>25</volume><fpage>1823</fpage><lpage>1834</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.06.002</pub-id><pub-id pub-id-type="pmid">26119746</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Sanchez Giraldo</surname><given-names>LG</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stimulus- and goal-oriented frameworks for understanding natural vision</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0284-0</pub-id><pub-id pub-id-type="pmid">30531846</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Author Correction: SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-0772-5</pub-id><pub-id pub-id-type="pmid">32094914</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Population coding of stimulus orientation by striate cortical cells</article-title><source>Biological Cybernetics</source><volume>64</volume><fpage>25</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1007/BF00203627</pub-id><pub-id pub-id-type="pmid">2285759</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Reyn</surname><given-names>CR</given-names></name><name><surname>Breads</surname><given-names>P</given-names></name><name><surname>Peek</surname><given-names>MY</given-names></name><name><surname>Zheng</surname><given-names>GZ</given-names></name><name><surname>Williamson</surname><given-names>WR</given-names></name><name><surname>Yee</surname><given-names>AL</given-names></name><name><surname>Leonardo</surname><given-names>A</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A spike-timing mechanism for action selection</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>962</fpage><lpage>970</lpage><pub-id pub-id-type="doi">10.1038/nn.3741</pub-id><pub-id pub-id-type="pmid">24908103</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Reyn</surname><given-names>CR</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Williamson</surname><given-names>WR</given-names></name><name><surname>Breads</surname><given-names>P</given-names></name><name><surname>Wu</surname><given-names>M</given-names></name><name><surname>Namiki</surname><given-names>S</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Feature Integration Drives Probabilistic Behavior in the <italic>Drosophila</italic> Escape Response</article-title><source>Neuron</source><volume>94</volume><fpage>1190</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.036</pub-id><pub-id pub-id-type="pmid">28641115</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>LQ</given-names></name><name><surname>Niu</surname><given-names>YQ</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Tectal neurons signal impending collision of looming objects in the pigeon</article-title><source>European Journal of Neuroscience</source><volume>22</volume><fpage>2325</fpage><lpage>2331</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04397.x</pub-id><pub-id pub-id-type="pmid">16262670</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>M</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Williamson</surname><given-names>WR</given-names></name><name><surname>Morimoto</surname><given-names>MM</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name><name><surname>Card</surname><given-names>GM</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual projection neurons in the <italic>Drosophila</italic> lobula link feature detection to distinct behavioral programs</article-title><source>eLife</source><volume>5</volume><elocation-id>e21022</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21022</pub-id><pub-id pub-id-type="pmid">28029094</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zavatone-Veth</surname><given-names>JA</given-names></name><name><surname>Badwan</surname><given-names>BA</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A minimal synaptic model for direction selective neurons in <italic>Drosophila</italic></article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.2.2</pub-id><pub-id pub-id-type="pmid">32040161</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>LoomDetectionANN</data-title><version designator="swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448">swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:8fb251126d26c99072427e7043bfdcc5c6b09789;origin=https://github.com/ClarkLabCode/LoomDetectionANN;visit=swh:1:snp:7bf0646fcc23421b8e4321934badbccf03532e3c;anchor=swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448">https://archive.softwareheritage.org/swh:1:dir:8fb251126d26c99072427e7043bfdcc5c6b09789;origin=https://github.com/ClarkLabCode/LoomDetectionANN;visit=swh:1:snp:7bf0646fcc23421b8e4321934badbccf03532e3c;anchor=swh:1:rev:864fd3d591bc9e3923189320d7197bdd0cd85448</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Cafaro</surname><given-names>J</given-names></name><name><surname>Turner</surname><given-names>MH</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Direction-Selective Circuits Shape Noise to Ensure a Precise Population Code</article-title><source>Neuron</source><volume>89</volume><fpage>369</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.019</pub-id><pub-id pub-id-type="pmid">26796691</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72067.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.07.07.451307" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.07.451307"/></front-stub><body><p>This paper trains a simple neural network model to perform a behaviorally important task: the detection of looming objects. Two solutions emerge, one of which shares several properties with the actual circuit. This is a nice demonstration that training a CNN on a behaviorally-relevant task can reveal how the underlying computations work.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72067.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>von Reyn</surname><given-names>Catherine</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.07.451307">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.07.07.451307v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Shallow neural networks trained to detect collisions recover features of visual loom-selective neurons&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Fred Rieke as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Ronald Calabrese as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Catherine von Reyn (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The followed issues emerged in review – and were agreed upon by all of the reviewers in consultations.</p><p>1. Questions about the model architecture. Several model components (rotation and symmetry) were imposed rather than learned. Was this necessary? Can the model make (testable) predictions about connectomics data?</p><p>2. Types of solutions. The text and results needs to explore all three types of solution (inward, outward and unstructured) in more detail. It is currently difficult to understand why the inward and unstructured solutions are essentially dropped part way through.</p><p>3. More challenging tests of the model. Can you add distracting optic flow to the current stimulus set and/or use more naturalistic stimuli? This could help reduce the number of viable solutions.</p><p>4. Inhibitory component of the model. Inhibition is assumed to have specific properties (e.g. rectification) – and it is not clear if these are essential. Further, it is absent in some solutions. Are the properties of inhibition (when present) consistent with the broad LPi receptive fields?</p><p>5. Comparison of model with neural data. A stronger rationale is needed for why two of the many outward models are selected for comparison with neural data (and why comparisons are not made for the inward or unstructured models). It is also important to quantify the similarity of the models with neural data.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Line 26-27: It would be helpful to make a somewhat more general statement about the power of the approach that you take here.</p><p>Figure 3 is the first figure referred to, so moving it up to Figure 1 would make reading easier.</p><p>Line 79: clarify here you mean object motion, not motion of one of the edges.</p><p>Line 94-95: the relationship between timing and size-to-speed ratio is likely hard for most readers to make sense of here – suggest deleting.</p><p>Lines 150-151: suggest clarifying that excitation and inhibition in the model are not constrained to have opposite spatial dependencies as depicted in the Figure 4.</p><p>Line 170: suggest describing the loss function in a sentence in the Results.</p><p>Lines 174-176: It would be helpful to connect the outward and inward model terminology more clearly to the flow fields in Figure 3 here. I think this is just a matter of highlighting which elements of the grid in Figure 3 are relevant for each model.</p><p>Lines 177-178: describe performance measures here qualitatively.</p><p>Lines 206-209: the reason for the difference in baseline activity is not clear – and it requires a lot of effort to extract that from the methods. Can you give more intuition here in the results?</p><p>Lines 336-340: this is helpful, and some of it could come up earlier in the Results. More generally, it would be helpful to be clearer (especially in results) how much of the encoding of angular size is a property of expansion of the stimulus, and how much of how the computation is implemented.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– The manuscript is a bit difficult to understand. The authors may want to improve their explanations and figures to make them more accessible. For example, in Figure 7B, I can barely see the responses and don't see any grey lines. Perhaps showing only a subset of responses would make the figure clearer -- less is more.</p><p>– The usage of the term &quot;ballistic&quot; in the introduction is confusing. In many contexts, &quot;ballistic&quot; suggests free-falling motion; in this paper, the authors are referring to the distinction between ballistic and diffusive motion. To avoid confusion, I would suggest not using the term ballistic at all; instead, &quot;straight line&quot; or &quot;linear&quot; is just as expressive.</p><p>– The first figure that is cited in the text is Figure 3. I suggest reorganizing either the text or the figures so that the first figure that is cited is Figure 1.</p><p>– Figure 5, panel D: why are there two magenta curves?</p><p>– I would also suggest a careful reading to screen for typos -- I found a dozen or so, from misspelled words to mismatched parentheses.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. Suggestions for improved or additional experiments, data or analyses:</p><p>a. The authors should provide their criteria for selecting a particular solution to compare to neural data.</p><p>b. The authors should evaluate how well their solutions predict neural data.</p><p>c. The authors need to mention that certain outward solutions have no inhibitory component (see Figure 5C, Figure 6 supplement 2). It needs to be discussed in the text and it would be very interesting to see how well these solutions recreate actual data.</p><p>d. It would be helpful for the authors to provide an example of an &quot;unstructured&quot; solution and an evaluation of its performance, even if it is included as a supplemental figure.</p><p>2. Recommendations for improving writing and presentation</p><p>a. Lines 89-90 – this can be better supported by adding the criteria/evaluation mentioned above.</p><p>b. Methods (~ line 483) – How is the HRC model using T5 (off) and T4 (on) motion input?</p><p>c. Lines 492-502 – What was the frame rate (timestep) for both training and testing stimuli?</p><p>d. Figures – Please increase the size when there is white space available. Make sure the pink and green color scheme for the two solution sets are very obvious.</p><p>e. Figure 1 caption – approximately half of the 200 LPLC2 are directly synaptic to the GF.</p><p>f. Figure 5 – is cross entropy loss the same as what is referred to as the loss function (equation 6) in the methods? If so, keep consistent. If not, please explain.</p><p>g. Figure 8D, it is difficult to see the boxplots.</p><p>h. Figure 10 I-L, it is difficult at first glance to realize what is neural data vs model output. Maybe label the rows instead?</p><p>i. Supplemental Figure 1. Add a schematic for the HRC model for readers who may not be familiar with it.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72067.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The followed issues emerged in review – and were agreed upon by all of the reviewers in consultations.</p><p>1. Questions about the model architecture. Several model components (rotation and symmetry) were imposed rather than learned. Was this necessary? Can the model make (testable) predictions about connectomics data?</p></disp-quote><p>Thank you for these two questions. The first question is whether symmetries would arise naturally if not imposed. In our optimization, we imposed rotational and mirror symmetries on the excitatory and inhibitory weights, and also aligned the upward directions when there were multiple units in the models. At the same time, we also chose our stimuli to be isotropic: all stimulus positions were distributed at random across visual angles. This is a reasonable null distribution in the absence of information about the true distribution of looming stimuli. But this isotropy means our basal expectation is that because there are no left-right or up-down asymmetries in the stimuli, we expect the filters to be largely symmetric, especially in the model with only one unit <italic>M</italic> = 1.</p><p>There is one large advantage to training our models with imposed symmetries: it reduces the total number of parameters in the model by almost by eight-fold. In terms of training examples per parameter, one may also think of this as effectively decreasing the data required for training by eight-fold. This computational efficiency justified our use of the symmetries in the paper.</p><p>To directly answer this question, we retrained our <italic>M</italic> = 1 model, this time without imposing the rotation and mirror symmetries (Figure 5—figure supplement 3 in the revised manuscript). As this figure shows, the trained weights possess roughly the rotational and mirror symmetries noted above. To see this more clearly, we quantified the degree of symmetry in these learned filters. To do this, we averaged the filters of the four directions (rotated accordingly to be aligned) and averaged the two halves of each filter. In this way, we created a symmetrized weighting, which we compared to the original unsymmetrized weights by calculating the cosine distance between them. The distribution of the cosine distances (Figure 5—figure supplement 3) shows that most of the trained weights are very close to the symmetrized one. In addition, when we trained the model using a data set eightfold larger than before, the trained weights become more symmetric (Figure 5—figure supplement 3). These results suggest that the isotropic training will naturally lead to the symmetrized weights, and imposing these symmetries in advance will make the training more efficient.</p><p>When we align multiple units on a sphere, with upward filters all pointing north, we break the isotropy of the model. In this case, it is possible that solutions without imposed filter symmetry would have asymmetric filters. However, those asymmetries would depend on the details of how we align and position our units. Those unit positions and their alignment, especially near the poles, is not well constrained by data, so we would be extremely reluctant to interpret any asymmetries that arose from our choices about how to distribute and align units. Thus, we do not believe it would be profitable to interpret any such asymmetries or try to tie them to the connectome.</p><p>A further important difficulty in comparing our results to connectomic data is that the hemibrain dataset from Janelia regrettably cut off most of the lobula plate, so that there is limited ultra-structure information about LPLC2 dendrites in that brain region. In the future, more ultra-structure data on LPLC2 would be useful both in constraining models of the sort we use here and in testing model predictions.</p><disp-quote content-type="editor-comment"><p>2. Types of solutions. The text and results needs to explore all three types of solution (inward, outward and unstructured) in more detail. It is currently difficult to understand why the inward and unstructured solutions are essentially dropped part way through.</p></disp-quote><p>Thank you for this suggestion. Although we never closely analyzed the unstructured data, in our original submission we did analyze both outward and inward solutions until the very last figure in the main text. In that last figure, we only showed the comparison of the outward solutions with the experimental data. In the revised manuscript, we now provide a supplementary figure for the last figure (Figure 10-supplemental figure 1) that shows the comparison of the response curves of the inward model with the experimental data. Thus, we now follow the inward and outward solutions through to the end of the paper.</p><p>In the revised manuscript, we have also addressed the unstructured solution. In the original manuscript, we created some confusion by calling these ’unstructured solutions’, when a more accurate term would have been ’zero solutions’. The spatial weights of the unstructured solutions are exactly zero or very close to zero, and thus, a rather uninteresting solution. In the revised manuscript, we have updated Figure 5 to include the zero solutions labeled in black boxes (Figure 5—figure supplement 1). We also now more clearly describe the unstructured solutions in the first paragraph of the section ’Optimization finds two distinct solutions to the loom-inference problem’. In the new manuscript, we also refer to these solutions as ’zero solutions’ to reduce the confusion brought about by ’unstructured’.</p><disp-quote content-type="editor-comment"><p>3. More challenging tests of the model. Can you add distracting optic flow to the current stimulus set and/or use more naturalistic stimuli? This could help reduce the number of viable solutions.</p></disp-quote><p>This is an interesting suggestion. When we were designing the stimuli used for training, it was not clear to us what naturalistic looming stimuli should look like. It was also not clear what the statistics of the stimuli are and how they should be distributed. Thus, it is not easy to systematically engineer stimuli that are close to the ones that a fly could experience in reality.</p><p>However, following this suggestion, we have engineered a new set of stimuli, where for the hit, miss, and retreat cases, we added a rotational background. In these new stimuli, the object of interest, i.e., the one that is moving in the depth direction, also rotates with the background. This mimics the effect of self rotation of the fly while observing a looming or retreating object. In a new training set, we replaced half of the hit cases, half of the miss cases and half of the retreat cases in the original data set with these rotational ones. When optimizing with this more challenging dataset, the outward and inward solutions both continue to exist, and we did observe an expected decrease of the model performance, with a larger decrease for the inward solutions. We present the optimization to this more challenging training set in Figure9—figure supplement 3 and discuss this in the Results section ’Large populations of units improve performance’.</p><p>There are two important points to make about interpreting these results. First, we are not aware of any experiments measuring LPLC2 responses to loom with additional background flow fields. Thus, it is not clear that LPLC2 can even perform this task, which makes it difficult to connect these results to data. We are also not aware of data on insect performance to looming stimuli of this more difficult type, so it is also difficult to relate to a true task of an insect. The second point for naturalistic stimuli is that, as mentioned in the Discussion section of the manuscript, the input to our model is not the optical signal itself, but rather the flow field that is calculated by a motion estimator. We have used the simple Reichardt correlator to estimate the motion signals. A more thorough future investigation might examine a more realistic motion detection model that can deal with more complicated, naturalistic visual stimuli, including textures and natural scene statistics, which could add new signals to the inputs to the model.</p><disp-quote content-type="editor-comment"><p>4. Inhibitory component of the model. Inhibition is assumed to have specific properties (e.g. rectification) – and it is not clear if these are essential. Further, it is absent in some solutions. Are the properties of inhibition (when present) consistent with the broad LPi receptive fields?</p></disp-quote><p>This comment and question led to our major change to the linear receptive field model. It is true that it was not clear if the rectified inhibition was necessary, and in fact it was not. As we discussed in our revision summary, we have replaced the model that has a rectified inhibitory component, in our original submission, with a simpler model that has a linear receptive field (Figure 4A). This new model performs almost identically to the previous model, in terms of both AUC performance and replication of the neural data (Figure 9 and supplements, Figure 10 and supplements). The primary difference we observed using the linear receptive field model is that the ratio between the numbers of the outward and inward solutions does not increase as the number of units increases (Figure9—figure supplement 2), and there are fewer outward solutions than inward ones. The nonlinearity of the inhibitory components plays an important role in selecting the outward solutions over the inward ones. Interestingly, if we replace the rectified linear unit (ReLU) with an exponential linear unit (ELU), which has a negative slope below the threshold, for a small number of units, all solutions are inward. But when the number of units increases and become larger than 16, the outward solutions emerge more often from the training. The ratio in this case remains below 1. Combined, these results indicate that the form and position of the nonlinearity in the circuit play a role in selecting between different optimized solutions. This suggests that further studies of the nonlinearities may lead to additional insight into how a population of LPLC2s encodes looming stimuli.</p><p>This question also mentions the fact that in some outward solutions, the inhibitory components are zero. In the revised manuscript, for the simpler linear receptive field model, the inhibitory negative component exists in all outward solutions. While it is interesting to examine the family of solutions from the rectified inhibition model, they are no longer the focus of the paper and not central to its claims. In the interest of length, we have not added to our paper by analyzing this specific type of outward solution.</p><p>The second question above is whether the inhibitory fields are consistent with broad LPi receptive fields. Here, the new linear model shows that the negative regions (stronger inhibition) are generally broader than the positive regions (stronger excitation) when the number of units is large. In the rectified inhibition model, where excitatory and inhibitory components are dissociable, the inhibitory weights of outward solutions spanned most of the 60-degree receptive field. For both models, the inhibition in outward solutions extended out far enough that responses were inhibited by motion far from looming centers (Figure 10 panels E, F and Figure 10—figure supplement 2 E, F), as in the data from LPLC2.</p><p>It is a bit difficult to compare these results directly to LPi data. LPis extend over regions that are similar in size to LPLC2 dendrites (Klapoetke et al., Nature, 2017, Figure 5K and Extended Data Figure 9). Moreover, it is not clear how much LPi cells integrate over space, or whether can have more localized input-output signals, as in neurons like CT1. Overall, for models with large number of units, the inhibition is broader than the excitation and seems consistent with broad (averaged) inputs from LPi neurons.</p><disp-quote content-type="editor-comment"><p>5. Comparison of model with neural data. A stronger rationale is needed for why two of the many outward models are selected for comparison with neural data (and why comparisons are not made for the inward or unstructured models). It is also important to quantify the similarity of the models with neural data.</p></disp-quote><p>Thank you for these suggestions. One advantage of the new linear receptive field model is that the variability in the solutions is mostly eliminated. In the revised manuscript, we now show the outward solution comparison with the data in the main figure (Figure 10) and the inward solution comparison in Figure10—figure supplement 1. We now include the rectified inhibition model comparison in a supplementary figure (Figure 10—figure supplement 2).</p><p>In the initial submission, in which there was a distribution of solutions, it might have been useful to quantify the relative fits of the different outward solutions. But with the linear receptive field model, this within-model quantification does not seem warranted because there is no distribution of trained filters.</p><p>Our goal in the comparison between the model and the data is to see how the two compare <italic>qualitatively</italic>, rather than quantitatively. For a quantification of the comparison to be interpretable, one would have to account for calcium indicator dynamics, which we have not included in our model. The important points in comparing the model to the data are: (1) the model responds strongly and selectively to loom signals rather than other non-looming signals; (2) the model qualitatively reproduces LPLC2 responses to various expanding bar stmuli; (3) the model shows periphery inhibition as observed in experiments; and (4) the model shows similar size tuning properties to LPLC2 neurons. We outline these qualitative similarities in the text analyzing the data in Figure 10. In general, we are strong proponents of quantifying similarities and differences, but in this case a quantification of these qualitative results does not seem as though it would provide additional insight.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Line 26-27: It would be helpful to make a somewhat more general statement about the power of the approach that you take here.</p></disp-quote><p>We have added a more general statement here, and expanded later in the introduction on how this approach relates to others.</p><disp-quote content-type="editor-comment"><p>Figure 3 is the first figure referred to, so moving it up to Figure 1 would make reading easier.</p></disp-quote><p>We want to keep the anatomy as the first figure, and so we removed the reference to Figure 3 in the first paragraph of the introduction.</p><disp-quote content-type="editor-comment"><p>Line 79: clarify here you mean object motion, not motion of one of the edges.</p></disp-quote><p>We rewrote the sentence to make it more clear that it is object motion.</p><disp-quote content-type="editor-comment"><p>Line 94-95: the relationship between timing and size-to-speed ratio is likely hard for most readers to make sense of here – suggest deleting.</p></disp-quote><p>Removed.</p><disp-quote content-type="editor-comment"><p>Lines 150-151: suggest clarifying that excitation and inhibition in the model are not constrained to have opposite spatial dependencies as depicted in the Figure 4.</p></disp-quote><p>We have added some sentences in both the main text (model section in the results) and the model figure caption to clarify this.</p><disp-quote content-type="editor-comment"><p>Line 170: suggest describing the loss function in a sentence in the Results.</p></disp-quote><p>Did as suggested in the last paragraph of the Results section ’An anatomically-constrained mathematical model’.</p><disp-quote content-type="editor-comment"><p>Lines 174-176: It would be helpful to connect the outward and inward model terminology more clearly to the flow fields in Figure 3 here. I think this is just a matter of highlighting which elements of the grid in Figure 3 are relevant for each model.</p></disp-quote><p>In the revised manuscript, these connections are made in the last two paragraphs of Results section ’Optimization finds two distinct solutions to the loom-inference problem’.</p><disp-quote content-type="editor-comment"><p>Lines 177-178: describe performance measures here qualitatively.</p></disp-quote><p>We have added this.</p><disp-quote content-type="editor-comment"><p>Lines 206-209: the reason for the difference in baseline activity is not clear – and it requires a lot of effort to extract that from the methods. Can you give more intuition here in the results?</p></disp-quote><p>Thank you for highlighting this. Yes, it does require the details of the model to think through this. The baseline activity of the inward solutions does not have to be positive, but it just happens to be. We have added some comments on this in the section ’Outward and inward filters are selective to signals in different ranges of angles’.</p><disp-quote content-type="editor-comment"><p>Lines 336-340: this is helpful, and some of it could come up earlier in the Results. More generally, it would be helpful to be clearer (especially in results) how much of the encoding of angular size is a property of expansion of the stimulus, and how much of how the computation is implemented.</p></disp-quote><p>These comments have been moved to earlier the Results section ’Activation patterns of computational solutions resemble biological responses’. With these comments, we want to provide an intuitive explanation of why the LPLC2 neurons and our models are angular size encoder, but it is not straightforward to quantify the contributions of the two aspects to the angular size tuning.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– The manuscript is a bit difficult to understand. The authors may want to improve their explanations and figures to make them more accessible. For example, in Figure 7B, I can barely see the responses and don't see any grey lines. Perhaps showing only a subset of responses would make the figure clearer -- less is more.</p></disp-quote><p>We have made the lines thicker and panels larger to make the figures clearer.</p><disp-quote content-type="editor-comment"><p>– The usage of the term &quot;ballistic&quot; in the introduction is confusing. In many contexts, &quot;ballistic&quot; suggests free-falling motion; in this paper, the authors are referring to the distinction between ballistic and diffusive motion. To avoid confusion, I would suggest not using the term ballistic at all; instead, &quot;straight line&quot; or &quot;linear&quot; is just as expressive.</p></disp-quote><p>We agree this was inappropriate. We now use the suggested term ”straight line motion”.</p><disp-quote content-type="editor-comment"><p>– The first figure that is cited in the text is Figure 3. I suggest reorganizing either the text or the figures so that the first figure that is cited is Figure 1.</p></disp-quote><p>We have deleted the reference to the Figure 3 in the first paragraph of the introduction.</p><disp-quote content-type="editor-comment"><p>– Figure 5, panel D: why are there two magenta curves?</p></disp-quote><p>In the initial submission, there were more than one example. In the new linear receptive field model, the curves are on top of each other, so there is only one curve apparent. We now state in figure captions when curves lie on top of one another.</p><disp-quote content-type="editor-comment"><p>– I would also suggest a careful reading to screen for typos -- I found a dozen or so, from misspelled words to mismatched parentheses.</p></disp-quote><p>We have read carefully through the manuscript and attempted to find and correct all typos.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. Suggestions for improved or additional experiments, data or analyses:</p><p>a. The authors should provide their criteria for selecting a particular solution to compare to neural data.</p></disp-quote><p>Please see Essential Revisions 5. The new linear RF model means that we no longer deal with this distribution of solutions for the main model we study, and selection is not required. Moreover, we now show all three different subtypes of outward solutions for the rectified inhibition model in Figure10—figure supplement 2, 3, 4.</p><disp-quote content-type="editor-comment"><p>b. The authors should evaluate how well their solutions predict neural data.</p></disp-quote><p>Please see Essential Revisions 5. We believe that the qualitative evaluation of the model with data is extremely informative, and without a family of solutions, we are not sure of the goal of a more formal, quantitative comparison between model and data.</p><disp-quote content-type="editor-comment"><p>c. The authors need to mention that certain outward solutions have no inhibitory component (see Figure 5C, Figure 6 supplement 2). It needs to be discussed in the text and it would be very interesting to see how well these solutions recreate actual data.</p></disp-quote><p>The inhibition-absent outward solutions only exist in the rectified inhibition models, but not in the new linear receptive field models. The outward solution without inhibitory component will respond strongly to the moving gratings in Figure 10—figure supplement 4B (which is different from experimental observations), and it cannot show the periphery inhibition in Figure 10—figure supplement 4E and F. We now mention this as among the family outward solutions in the rectified inhibition model, and point out its short-comings.</p><disp-quote content-type="editor-comment"><p>d. It would be helpful for the authors to provide an example of an &quot;unstructured&quot; solution and an evaluation of its performance, even if it is included as a supplemental figure.</p></disp-quote><p>This is now provided in the Figure 5 supplemental figure 1, shown as zero solutions. Please see Essential Revisions 2.</p><disp-quote content-type="editor-comment"><p>2. Recommendations for improving writing and presentation</p><p>a. Lines 89-90 – this can be better supported by adding the criteria/evaluation mentioned above.</p></disp-quote><p>Thank you for this suggestion. We have added more detail about the evaluations of the models in the Results section ’Optimization finds two distinct solutions to the loom-inference problem’.</p><disp-quote content-type="editor-comment"><p>b. Methods (~ line 483) – How is the HRC model using T5 (off) and T4 (on) motion input?</p></disp-quote><p>The HRC model we use does not distinguish between light and dark edges. Using it as the input is most similar to having both T4 and T5 input (which is also why HS cell activity can often be well-approximated by an HRC).</p><disp-quote content-type="editor-comment"><p>c. Lines 492-502 – What was the frame rate (timestep) for both training and testing stimuli?</p></disp-quote><p>We have added this information in the methods: the time step for the stimuli is also 0.01 second.</p><disp-quote content-type="editor-comment"><p>d. Figures – Please increase the size when there is white space available. Make sure the pink and green color scheme for the two solution sets are very obvious.</p></disp-quote><p>Increased the sizes of some panels.</p><disp-quote content-type="editor-comment"><p>e. Figure 1 caption – approximately half of the 200 LPLC2 are directly synaptic to the GF.</p></disp-quote><p>We are uncertain where this information comes from. In the Ache et al., paper (Current Biology, 2019), they reported 108 LPLC2 neurons projecting to the GF in the right hemisphere of an adult <italic>Drosophila</italic>. So, in total, there should be about 200 LPLC2 neurons directly projecting to the two GFs. In the hemibrain dataset, there are 68 annotated LPLC2-R neurons and all 68 LPLC2-R neurons are listed a presynaptic to the right giant fiber in a neuprint query. When not restricted to the ’-R’ suffix, one finds a similarly large fraction of LPLC2 neurons presynaptic to the giant fiber. Unless we are mistaken, it appears that most LPLC2 neurons synapse onto the GF. In the Figure 1 caption and introduction, we changed GF to GFs to indicate that these 200 LPLC2 project to two GFs, respectively. If we have missed an important measurement of this connectivity, we would be happy to correct this description if the reviewer could provide the reference.</p><disp-quote content-type="editor-comment"><p>f. Figure 5 – is cross entropy loss the same as what is referred to as the loss function (equation 6) in the methods? If so, keep consistent. If not, please explain.</p></disp-quote><p>Yes, they are the same. We have changed the l.h.s of Equation 6 from loss to cross entropy loss.</p><disp-quote content-type="editor-comment"><p>g. Figure 8D, it is difficult to see the boxplots.</p></disp-quote><p>In the revised manuscript, we have made the boxes larger and hopefully easier to see.</p><disp-quote content-type="editor-comment"><p>h. Figure 10 I-L, it is difficult at first glance to realize what is neural data vs model output. Maybe label the rows instead?</p></disp-quote><p>We have labeled the rows as suggested.</p><disp-quote content-type="editor-comment"><p>i. Supplemental Figure 1. Add a schematic for the HRC model for readers who may not be familiar with it.</p></disp-quote><p>Added as suggested.</p></body></sub-article></article>