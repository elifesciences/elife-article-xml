<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103636</article-id><article-id pub-id-type="doi">10.7554/eLife.103636</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103636.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Elucidating the selection mechanisms in context-dependent computation through low-rank neural network modeling</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Yiteng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-5192-521X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Feng</surname><given-names>Jianfeng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5987-2258</contrib-id><email>jianfeng64@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Min</surname><given-names>Bin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1006-9629</contrib-id><email>minbmath@gmail.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>School of Data Science, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution>Lingang Laboratory</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013q1eq08</institution-id><institution>Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence, Fudan University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>03</day><month>07</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP103636</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-10-03"><day>03</day><month>10</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-09-03"><day>03</day><month>09</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.02.610896"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-11"><day>11</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103636.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-10"><day>10</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103636.2"/></event></pub-history><permissions><copyright-statement>© 2024, Zhang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Zhang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103636-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-103636-figures-v1.pdf"/><abstract><p>Humans and animals exhibit a remarkable ability to selectively filter out irrelevant information based on context. However, the neural mechanisms underlying this context-dependent selection process remain elusive. Recently, the issue of discriminating between two prevalent selection mechanisms—input modulation versus selection vector modulation—with neural activity data has been highlighted as one of the major challenges in the study of individual variability underlying context-dependent decision-making (CDM). Here, we investigated these selection mechanisms through low-rank neural network modeling of the CDM task. We first showed that only input modulation was allowed in rank-one neural networks and additional dimensions of network connectivity were required to endow neural networks with selection vector modulation. Through rigorous information flow analysis, we gained a mechanistic understanding of why additional dimensions are required for selection vector modulation and how additional dimensions specifically contribute to selection vector modulation. This new understanding then led to the identification of novel neural dynamical signatures for selection vector modulation at both single neuron and population levels. Together, our results provide a rigorous theoretical framework linking network connectivity, neural dynamics, and selection mechanisms, paving the way towards elucidating the circuit mechanisms when studying individual variability in context-dependent computation.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>context-dependent computation</kwd><kwd>decision making</kwd><kwd>neural dynamics</kwd><kwd>cognitive control</kwd><kwd>low-rank RNN</kwd><kwd>individual variability</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002855</institution-id><institution>Ministry of Science and Technology (China)</institution></institution-wrap></funding-source><award-id>STI2030-Major Project, 2021ZD0204105</award-id><principal-award-recipient><name><surname>Min</surname><given-names>Bin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32271149</award-id><principal-award-recipient><name><surname>Min</surname><given-names>Bin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Mechanistic modeling with low-rank recurrent networks uncovers the relationship between network connectivity, neural dynamics, and selection modulation mechanisms in context-dependent computation.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Imagine you are playing a card game. Your strategy depends not only on the cards you have but also on what your opponents are doing. As the game progresses, you adjust your moves based on their actions to increase your chances of winning. This example illustrates how much of our decision-making, both in everyday life and in more complex tasks, is influenced by the context in which we are acting (<xref ref-type="bibr" rid="bib22">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib32">Roy et al., 2010</xref>; <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib34">Saez et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Siegel et al., 2015</xref>; <xref ref-type="bibr" rid="bib6">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib41">Takagi et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib3">Barbosa et al., 2023</xref>). However, how the brain performs such context-dependent computation remains elusive (<xref ref-type="bibr" rid="bib13">Fusi et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Cohen, 2017</xref>; <xref ref-type="bibr" rid="bib2">Badre et al., 2021</xref>; <xref ref-type="bibr" rid="bib26">Okazawa and Kiani, 2023</xref>).</p><p>Using a monkey CDM behavioral paradigm, a recent work uncovered a novel mechanism in the brain that helps adjust decisions based on context (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>). This mechanism, called ‘selection vector modulation,’ is distinct from the early sensory input modulation counterpart (<xref ref-type="bibr" rid="bib10">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib25">Noudoost et al., 2010</xref>). More recently, building on this work, new research with rats supported a novel theoretical framework regarding how the brain makes context-dependent decisions and revealed how this process may vary between individuals (<xref ref-type="bibr" rid="bib30">Pagan et al., 2025</xref>). Critically, this theoretical framework pointed out that current neurophysiological data fell short of distinguishing between selection vector modulation and sensory input modulation, calling for rethinking what kind of evidence is required for differentiating different selection mechanisms.</p><p>Here, we aim to address this challenge by using the low-rank recurrent neural network (RNN) modeling approach (<xref ref-type="bibr" rid="bib17">Landau and Sompolinsky, 2018</xref>; <xref ref-type="bibr" rid="bib21">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib15">Kadmon et al., 2020</xref>; <xref ref-type="bibr" rid="bib36">Schuessler et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib5">Beiran et al., 2023</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Valente et al., 2022</xref>; <xref ref-type="bibr" rid="bib27">Ostojic and Fusi, 2024</xref>). This approach allowed us to simulate and better understand the neural processes behind context-dependent computation. More importantly, endowed by the low-rank RNN theory (<xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>), this approach allowed us to develop a set of analyses and derivations uncovering a previously unknown link between connectivity dimensionality, neural dynamics, and selection mechanisms. This link then led to the identification of novel neural dynamical signatures for selection vector modulation at both the single neuron and population levels. Together, our work provides a neural circuit basis for different selection mechanisms, shedding new light on the study of individual variability in neural computation underlying the ubiquitous context-dependent behaviors.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Task paradigm, key concept, and modeling approach</title><p>The task paradigm we focused on is the pulse-based CDM task (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>), a novel rat-version CDM paradigm inspired by the previous monkey CDM work (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>). In this paradigm, rats were presented with sequences of randomly-timed auditory pulses that varied in both location and frequency (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In alternating blocks of trials, rats were cued by an external context signal to determine the prevalent location (in the ‘LOC’ context) or frequency (in the ‘FRQ’ context). Note that compared to the continuous sensory input setting in previous works (e.g. <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>), this pulse-based sensory input setting allowed the experimenters to better characterize both behavioral and neural responses (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). We will also demonstrate the unique advantage of this pulse-based input setting later in the present study (e.g. Figure 7).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Prevalent candidate selection mechanisms in context-dependent decision-making (CDM) cannot be dissociated by classical neural dynamics analysis.</title><p>(<bold>A</bold>) A pulse-based context-dependent decision-making task (adapted from <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). In each trial, rats were first cued by sound to indicate whether the current context was the location (LOC) context or the frequency (FRQ) context. Subsequently, rats were presented with a sequence of randomly timed auditory pulses. Each pulse could come from either the left speaker or right speaker and could be of low frequency (6.5 kHz, light blue) or high frequency (14 kHz, dark blue). In the LOC context, rats were trained to turn right (left) if more pulses are emitted from the right (left) speaker. In the FRQ context, rats were trained to turn right (left) if there are more (fewer) high-frequency pulses compared to low-frequency pulses. (<bold>B</bold>) Two prevalent candidate mechanisms for context-dependent decision-making. <italic>Top</italic>: The input modulation mechanism. In this scenario, while the selection vector remains invariant across contexts, the stimulus input representation is altered in a way such that only the relevant stimulus input representation (i.e. the location input in the LOC context and the frequency input in the FRQ context) is well aligned with the selection vector, thereby fulfilling the requirement of context-dependent computation. <italic>Bottom</italic>: The selection vector modulation mechanism. In this scenario, although the stimulus input representation remains constant across different contexts, the selection vector itself is altered by the context input to align with the relevant sensory input. Red line: line attractor (choice axis). Green arrow: selection vector. Thick gray and blue arrows stand for the projections of the location and frequency input representation directions on the space spanned by the line attractor and selection vector, respectively. The small gray arrows stand for direction of relaxing dynamics. (<bold>C</bold>) Networks with distinct selection mechanisms may lead to similar trial-averaged neural dynamics (adapted from <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). In a model with pure input modulation, the irrelevant sensory input can still be represented by the network in a direction orthogonal to the selection vector. Therefore, using the classical targeted dimensionality reduction method (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>), both the input modulation model (top) and the selection vector modulation model (bottom) would exhibit similar trial-averaged neural dynamics as shown in <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>. (<bold>D</bold>) The setting of low-rank RNN modeling for the CDM task. The network has four input channels. Input 1 and input 2 represent two sensory inputs, while the other two channels indicate the context. The connectivity matrix <inline-formula><alternatives><mml:math id="inf1"><mml:mi>J</mml:mi></mml:math><tex-math id="inft1">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula> is constrained to be low-rank, expressed as <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$\frac{1}{N}\sum _{r=1}^{R}m_{r}n_{r}^{T}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf3"><mml:mi>N</mml:mi></mml:math><tex-math id="inft3">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> is the number of neurons, <inline-formula><alternatives><mml:math id="inf4"><mml:mi>R</mml:mi></mml:math><tex-math id="inft4">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> is the matrix’s rank, and <inline-formula><alternatives><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft5">\begin{document}$m_{r}n_{r}^{T}$\end{document}</tex-math></alternatives></inline-formula> is a rank-1 matrix formed by the outer product of two <italic>N</italic>-dimensional connectivity vectors <inline-formula><alternatives><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft6">\begin{document}$m_{r}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft7">\begin{document}$n_{r}$\end{document}</tex-math></alternatives></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig1-v1.tif"/></fig><p>To solve this task, rats had to select the relevant information for the downstream evidence accumulation process based upon the context. There were at least two different mechanisms capable of performing this selection operation, i.e., selection vector modulation and input modulation (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). To better introduce these mechanisms, we first reviewed the classical linearized dynamical systems analysis and the concept of selection vector (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Sussillo and Barak, 2013</xref>; <xref ref-type="bibr" rid="bib18">Maheswaranathan et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Maheswaranathan and Sussillo, 2020</xref>; <xref ref-type="bibr" rid="bib23">Nair et al., 2023</xref>). In the linearized dynamical systems analysis, the neural dynamics around the choice axis (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, red line) is approximated by a line attractor model. Specifically, the dynamics in the absence of external input can be approximated by the following linear equation <inline-formula><alternatives><mml:math id="inf8"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>r</mml:mi></mml:math><tex-math id="inft8">\begin{document}$\frac{dr}{dt}=Mr$\end{document}</tex-math></alternatives></inline-formula>, where <italic>M</italic> is a matrix with one eigenvalue being equal to 0 and all other eigenvalues having a negative real part. For brevity, let us denote the left eigenvector of the 0 eigenvalue as <inline-formula><alternatives><mml:math id="inf9"><mml:mi>s</mml:mi></mml:math><tex-math id="inft9">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. In this linear dynamical system, the effect of any given perturbation can be decomposed along the directions of different eigenvectors: the projection onto the <inline-formula><alternatives><mml:math id="inf10"><mml:mi>s</mml:mi></mml:math><tex-math id="inft10">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> direction will remain constant while the projections onto all other eigenvectors will exponentially decay to zero. Thus, for any given input <inline-formula><alternatives><mml:math id="inf11"><mml:mi>I</mml:mi></mml:math><tex-math id="inft11">\begin{document}$I$\end{document}</tex-math></alternatives></inline-formula>, only the component projecting onto the <inline-formula><alternatives><mml:math id="inf12"><mml:mi>s</mml:mi></mml:math><tex-math id="inft12">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> direction (i.e. <inline-formula><alternatives><mml:math id="inf13"><mml:mi>I</mml:mi><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi></mml:math><tex-math id="inft13">\begin{document}$I\cdot s$\end{document}</tex-math></alternatives></inline-formula>) can be integrated along the line attractor (see Methods for more details). In other words, <inline-formula><alternatives><mml:math id="inf14"><mml:mi>s</mml:mi></mml:math><tex-math id="inft14">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> serves as a vector selecting the input information, which is known as the ‘selection vector’ in the literature (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>).</p><p>Two distinct selection mechanisms can then be introduced based upon the concept of selection vector (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). Specifically, to perform the CDM task, the stimulus input (LOC input, for example) must have a larger impact on evidence accumulation in the relevant context (LOC context) than in the irrelevant context (FRQ context). That is, <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$\boldsymbol I\cdot \boldsymbol s$\end{document}</tex-math></alternatives></inline-formula> must be larger in the relevant context than in the irrelevant context. The difference between these two can be decomposed into two components:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  \Delta (\boldsymbol I\cdot\boldsymbol s)=\Delta \boldsymbol I\cdot \bar{\boldsymbol s} +\bar{\boldsymbol I} \cdot \Delta \boldsymbol s,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the <inline-formula><alternatives><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft16">\begin{document}$\Delta $\end{document}</tex-math></alternatives></inline-formula> symbol denotes difference across two contexts (relevant – irrelevant) and the bar symbol denotes average across two contexts (see Methods for more details). The first component <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Δ</mml:mi><mml:mi>I</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$\boldsymbol \Delta I\cdot \bar{\boldsymbol s}$\end{document}</tex-math></alternatives></inline-formula> is called input modulation in which the change of input information across different contexts is emphasized (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, top). In contrast, the second component <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$\bar{\boldsymbol I}\cdot \Delta \boldsymbol s$\end{document}</tex-math></alternatives></inline-formula> is called selection vector modulation in which the change of selection vector across contexts is instead highlighted (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, bottom).</p><p>While these two selection mechanisms were clearly defined, recent work showed that it is actually challenging to differentiate them through neural dynamics (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). For example, both input modulation and selection vector modulation can lead to similar trial-averaged neural dynamics through targeted dimensionality reduction (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). Take the input modulation as an example (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, top). One noticeable aspect we can observe is that the input information (e.g. location information) is preserved in both relevant (LOC context) and irrelevant contexts (FRQ context), which seems contradictory to the definition of input modulation. What is the mechanism underlying this counterintuitive result? As Pagan et al. pointed out earlier, input modulation is not the input change (<inline-formula><alternatives><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$\Delta \boldsymbol I$\end{document}</tex-math></alternatives></inline-formula>) per se. Rather, it means the change of input multiplied by selection vector (i.e. <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$\Delta \boldsymbol I\cdot \bar{\boldsymbol s}$\end{document}</tex-math></alternatives></inline-formula>). Therefore, for the input modulation, while input information indeed is modulated by context along the selection vector direction, input information can still be preserved across contexts along other directions orthogonal to the selection vector, which explains the counterintuitive result and highlights the challenge of distinguishing input modulation from selection vector modulation in experiments (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>).</p><p>In this study, we sought to address this challenge using the low-rank RNN modeling approach. In contrast to the ‘black-box’ vanilla RNN approach (e.g. <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>), the low-rank RNN approach features both well-controlled model complexity and mechanistic transparency, potentially providing a fresh view into the mechanisms underlying the intriguing selection process. Specifically, the low-rank RNNs we studied here implemented an input-output task structure similar to the classical RNN modeling work of CDM (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>). More concretely, the hidden state <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$\boldsymbol x$\end{document}</tex-math></alternatives></inline-formula> of a low-rank RNN with <inline-formula><alternatives><mml:math id="inf22"><mml:mi>N</mml:mi></mml:math><tex-math id="inft22">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> neurons evolves over time according to<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  \tau \frac{d\boldsymbol x}{dt}=- x+J\phi \left (\boldsymbol x\right)+\sum _{s=1}^{2}\boldsymbol I_{s}u_{s}\left (t\right)+\sum _{s=1}^{2}I_{s}^{ctx}u_{s}^{ctx}\left (t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$J=\sum _{r=1}^{R}\boldsymbol m_{r}\boldsymbol n_{r}^{T}/N$\end{document}</tex-math></alternatives></inline-formula> is a low-rank matrix with <inline-formula><alternatives><mml:math id="inf24"><mml:mi>R</mml:mi></mml:math><tex-math id="inft24">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> output vectors <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$\boldsymbol m_{r},r=1,\cdots ,R$\end{document}</tex-math></alternatives></inline-formula> and R input-selection vectors <inline-formula><alternatives><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$\boldsymbol n_{r},r=1,\cdots ,R$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf27"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft27">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> is the time constant of single neurons, <inline-formula><alternatives><mml:math id="inf28"><mml:mi>ϕ</mml:mi></mml:math><tex-math id="inft28">\begin{document}$\phi $\end{document}</tex-math></alternatives></inline-formula> is the nonlinear activation function, <inline-formula><alternatives><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft29">\begin{document}$u_{s}\left (t\right),s=1,2$\end{document}</tex-math></alternatives></inline-formula> embedded into the network through <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$\boldsymbol I_{s}$\end{document}</tex-math></alternatives></inline-formula> mimic the location and frequency click inputs, and <inline-formula><alternatives><mml:math id="inf31"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft31">\begin{document}$u_{s}^{ctx}\left (t\right),s=1,2$\end{document}</tex-math></alternatives></inline-formula> embedded through <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$\boldsymbol I_{s}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> indicate whether the current context is location or frequency. The output of the network is a linear projection of neural activity (see Methods for more model details). Under this general architectural setting, on one hand, through controlling the rank <inline-formula><alternatives><mml:math id="inf33"><mml:mi>R</mml:mi></mml:math><tex-math id="inft33">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> of the matrix <inline-formula><alternatives><mml:math id="inf34"><mml:mi>J</mml:mi></mml:math><tex-math id="inft34">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula> during backpropagation training, we can determine the minimal rank required for performing the CDM task and reverse-engineer the underlying mechanism, which will be demonstrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>. On the other hand, recent theoretical progress of low-rank RNNs (<xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>) enabled us to explicitly construct neural network models with mechanistic transparency, complementing the reverse-engineering analysis (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Sussillo and Barak, 2013</xref>), which will be shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>No selection vector modulation in rank-1 neural network models.</title><p>(<bold>A</bold>) Illustration of rank-1 connectivity matrix structure. <italic>Left</italic>: a rank-1 matrix can be represented as the outer product of an output vector <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$\boldsymbol m_{dv}$\end{document}</tex-math></alternatives></inline-formula> and an input-selection vector <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula>, of which the input-selection vector <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> played the role of selecting the input information through its overlap with the input embedding vectors <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$\boldsymbol I_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$\boldsymbol I_{2}$\end{document}</tex-math></alternatives></inline-formula>. The context signals are fed forward to the network with embedding vectors <inline-formula><alternatives><mml:math id="inf40"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft40">\begin{document}$I_{1}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf41"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft41">\begin{document}$I_{2}^{ctx}$\end{document}</tex-math></alternatives></inline-formula>. Since the overlap between the context embedding vectors and input-selection vector <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> are close to 0, for simplicity, we omitted the context embedding vectors here. <italic>Right</italic>: an example of the trained rank-1 connectivity structure characterized by the cosine angle between every pair of connectivity vectors (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and Methods for details). (<bold>B</bold>) The psychometric curve of the trained rank-1 recurrent neural networks (RNNs). In context 1, input 1 strongly affects the choice, while input 2 has little impact on the choice. In context 2, the effect of input 1 and input 2 on the choice is exchanged. The shaded area indicates the standard deviation. Ctx. 1, context 1. Ctx. 2, context 2. (<bold>C</bold>) Characterizing the change of selection vector as well as input representation direction across contexts using cosine angle. The selection vector in each context is computed using linearized dynamical system analysis. The input representation direction is defined as the elementwise multiplication between the single neuron gain vector and the input embedding vector (see Methods for details). ***p&lt;0.001, one-way ANOVA test, n=100. Inp., input. Rep., representation. (<bold>D</bold>) Characterizing the overlap between the input representation direction and the selection vector. ***p&lt;0.001, one-way ANOVA test, n=100. Dir., direction. (<bold>E</bold>) The state space analysis, for example, trained rank-1 RNN. The space is spanned by the line attractor axis (red line) and the selection vector (green arrow). (<bold>F</bold>) Trial-averaged dynamics for example rank-1 RNN. We applied targeted dimensionality reduction (TDR) to identify the choice, input 1, and input 2 axes. The neuron activities were averaged according to input 1 strength, choice and context, and then projected onto the choice and input 1 axes to obtain the trial-averaged population dynamics (see Methods for details).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Connectivity structure for the example rank-1 recurrent neural network (RNN).</title><p>(<bold>A</bold>) Projection of the connectivity space for the example rank-1 RNN. Each dot denotes a neuron. On each panel, the x and y coordinates of the <inline-formula><alternatives><mml:math id="inf43"><mml:mi>i</mml:mi></mml:math><tex-math id="inft43">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th dot represent the <inline-formula><alternatives><mml:math id="inf44"><mml:mi>i</mml:mi></mml:math><tex-math id="inft44">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th entry of the corresponding connectivity vectors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig2-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>A rank-3 neural network model with pure selection vector modulation.</title><p>(<bold>A</bold>) Illustration of the utilized rank-3 connectivity matrix structure. <italic>Left</italic>: the rank-3 matrix can be represented as the summation of three outer products, including the one with the output vector <inline-formula><alternatives><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft45">\begin{document}$m_{dv}$\end{document}</tex-math></alternatives></inline-formula> and the input-selection vector <inline-formula><alternatives><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft46">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula>, the one with the output vector <inline-formula><alternatives><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft47">\begin{document}$m_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and the input-selection vector <inline-formula><alternatives><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft48">\begin{document}$n_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula>, and the one with the output vector <inline-formula><alternatives><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft49">\begin{document}$m_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula> and the input-selection vector <inline-formula><alternatives><mml:math id="inf50"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft50">\begin{document}$n_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula>, of which the input-selection vectors <inline-formula><alternatives><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft51">\begin{document}$n_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft52">\begin{document}$n_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula> played the role of selecting the input information from <inline-formula><alternatives><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft53">\begin{document}$I_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft54">\begin{document}$I_{2}$\end{document}</tex-math></alternatives></inline-formula>, respectively. <italic>Right</italic>: the connectivity structure of the handcrafted RNN model characterized by the cosine angle between every pair of connectivity vectors (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and Methods for more details). (<bold>B</bold>) The psychometric curve of the handcrafted rank-3 recurrent neural network (RNN) model. (<bold>C</bold>) Characterizing the change of selection vector as well as input representation direction across contexts using cosine angle. The selection vector in each context is computed using linearized dynamical system analysis. The input representation direction is defined as the elementwise multiplication between the single neuron gain vector and the input embedding vector (see Methods for details). ***p&lt;0.001, one-way ANOVA test, n=100. (<bold>D</bold>) Characterizing the overlap between the input representation direction and the selection vector. ***p&lt;0.001, one-way ANOVA test, n=100. (<bold>E</bold>) The state space analysis for example rank-3 RNN. The space is spanned by the line attractor axis (red line, invariant across contexts), selection vector in context 1 (green arrow, top panel), and selection vector in context 2 (green arrow, bottom panel). (<bold>F</bold>) Trial-averaged dynamics for example rank-3 RNN.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Connectivity structure for the example rank-3 recurrent neural network (RNN).</title><p>(<bold>A</bold>) Projection of the connectivity space for the example rank-3 RNN. This RNN has 30,000 neurons divided into three populations. Dots of the same color represent neurons within the same population. The inset in the top right corner shows the projection on the two context input axes. For brevity, we did not include the projections onto the context input axis and other connectivity vectors. Within each population, the context input axis is independent of the other connectivity vectors. This independence implies that the context signal only affects the average sensitivity of each neuron population, thereby serving a modulatory function.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig3-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>No selection vector modulation in rank-one models</title><p>In the literature, it was found that rank-one RNN models suffice to solve the CDM task (<xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>). Here, we further asked whether selection vector modulation can occur in rank-one RNN models. To this end, we trained many rank-1 models (see Methods for details) and found that indeed having rank-1 connectivity (e.g. with the overlap structure listed in <xref ref-type="fig" rid="fig2">Figure 2A</xref>; for the detailed connectivity structure, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) is sufficient to perform the CDM task, consistent with the earlier work. As shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, in context 1, the decision was made based on input-1 evidence, ignoring input-2 evidence, indicating that the network can effectively filter out irrelevant information. To answer what kind of selection mechanisms underlie this context-dependent computation, we computed the selection vector in two contexts through linearized dynamical systems analysis (<xref ref-type="bibr" rid="bib40">Sussillo and Barak, 2013</xref>). Cosine angle analysis revealed that selection vectors kept invariant across different contexts (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, left), indicating no selection vector modulation. This result was preserved across different hyperparameter settings (such as different regularization coefficients or activation functions). Note that this result actually can be mathematically proved (<xref ref-type="bibr" rid="bib29">Pagan et al., 2023</xref>; <xref ref-type="bibr" rid="bib30">Pagan et al., 2025</xref>). Therefore, our modeling result reconfirmed the limitation of rank-one models on selection vector modulation.</p><p>While the selection vector was not altered by contexts, the direction of input representations changed significantly across different contexts (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, left; see Methods for the definition of input representation direction). Further analysis revealed that the overlap between the input representation direction and the unchanged selection vector is large in the relevant context and small in the irrelevant context, supporting the input modulation mechanism (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). These results indicate that while a rank-1 network can perform the task, it can only achieve flexible computation through input modulation (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Importantly, when applying a similar targeted dimensionality reduction method to this rank-1 model, we found that the irrelevant sensory input information was indeed well-represented in neural activity state space (<xref ref-type="fig" rid="fig2">Figure 2F</xref>), supporting the conclusion made in the Pagan et al. paper that the presence of irrelevant sensory input in neural state space cannot be used as a reliable indicator for the absence of input modulation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>In summary, we conclude that to study the mechanism of selection vector modulation, instead of limiting to the simplest model of CDM task, it is necessary to explore network models with higher ranks.</p></sec><sec id="s2-3"><title>A low-rank model with pure selection vector modulation</title><p>To study the mechanism of selection vector modulation, we designed a rank-3 neural network model, with one additional rank for each sensory input feature (i.e. <inline-formula><alternatives><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$\boldsymbol m_{iv_{1}}\boldsymbol n_{iv_{1}}^{T}$\end{document}</tex-math></alternatives></inline-formula> for input 1 and <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$\boldsymbol m_{iv_{2}}\boldsymbol n_{iv_{2}}^{T}$\end{document}</tex-math></alternatives></inline-formula> for input 2; <xref ref-type="fig" rid="fig3">Figure 3A</xref>, left). Specifically, we ensured that <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}$\boldsymbol I_{1}(\boldsymbol I_{2})$\end{document}</tex-math></alternatives></inline-formula> has a positive overlap with <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$\boldsymbol n_{iv_{1}}(\boldsymbol n_{iv_{2}})$\end{document}</tex-math></alternatives></inline-formula> and zero overlap with <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula>, while <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$\boldsymbol m_{iv1}(\boldsymbol m_{iv_{2}})$\end{document}</tex-math></alternatives></inline-formula> has a positive overlap with <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> and Methods for more details). Moreover, our rank-3 network relies on a multi-population structure, consistent with the notion that higher-rank networks still require a multi-population structure to perform flexible computations (<xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>). This configuration implies that the stimulus input 1 (2) is first selected by <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$\boldsymbol n_{iv_{1}}(\boldsymbol n_{iv_{2}})$\end{document}</tex-math></alternatives></inline-formula>, represented by <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$\boldsymbol m_{iv_{1}}(\boldsymbol m_{iv_{2}})$\end{document}</tex-math></alternatives></inline-formula>, and subsequently selected by <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> before being integrated by the accumulator. In principle, this sequential selection process enables more sophisticated contextual modulations.</p><p>We confirmed that a model with such a connectivity structure can perform the task (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and then conducted an analysis similar to that performed for rank-1 models. Unlike the rank-1 model, the selection vector for this rank-3 model changes across contexts while the input representation direction remains invariant (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Further analysis revealed that the overlap between the selection vector and the unchanged input representation direction is large in the relevant context and small in the irrelevant context (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), supporting a pure selection vector modulation mechanism (<xref ref-type="fig" rid="fig3">Figure 3E</xref>) distinct from the input modulation counterpart shown in <xref ref-type="fig" rid="fig2">Figure 2D</xref>. When applying a similar targeted dimensionality reduction method to this rank-3 model, as what we expected, we found that both relevant and irrelevant sensory input information was indeed well-represented in neural activity state space (<xref ref-type="fig" rid="fig3">Figure 3F</xref>), which was indistinguishable from the input modulation counterpart (<xref ref-type="fig" rid="fig2">Figure 2F</xref>).</p><p>Together, through investigating these two extreme cases—one with pure input modulation and the other with pure selection vector modulation, we not only reconfirm the challenge of distinguishing input modulation from selection modulation based on neural activity data (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>) but also point out the previously unknown link between selection vector modulation and network connectivity dimensionality.</p></sec><sec id="s2-4"><title>Understanding context-dependent modulation in Figs. 2 and 3 through pathway-based information flow analysis</title><p>What is the machinery underlying this link between selection vector modulation and network connectivity dimensionality? One possible way to address this issue is through linearized dynamical systems analysis: first computing the selection vector and the sensory input representation direction through reverse-engineering (<xref ref-type="bibr" rid="bib40">Sussillo and Barak, 2013</xref>) and then calculating both selection vector modulation and input modulation according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. However, the connection between the network connectivity dimensionality and the selection vector obtained through reverse-engineering is implicit and in general non-trivial (<xref ref-type="bibr" rid="bib29">Pagan et al., 2023</xref>), hindering further investigation of the underlying machinery. Here, by combining recent theoretical progress in low-rank RNNs (<xref ref-type="bibr" rid="bib21">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>) and linearized dynamical systems analysis (<xref ref-type="bibr" rid="bib40">Sussillo and Barak, 2013</xref>), we introduced a novel pathway-based information flow analysis approach, providing an explicit link between network connectivity, neural dynamics, and selection mechanisms.</p><p>To start with, the low-rank RNN dynamics (i.e. <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) can be described by an information flow graph, with each task variable as a node and each effective coupling between task variables as an edge (<xref ref-type="bibr" rid="bib21">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>). Take the rank-1 RNN in <xref ref-type="fig" rid="fig2">Figure 2</xref> as an example. A graph with three nodes, including two input variables <inline-formula><alternatives><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft65">\begin{document}$\kappa _{p_{s}}\left (t\right),s=1,2$\end{document}</tex-math></alternatives></inline-formula> and one decision variable <inline-formula><alternatives><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft66">\begin{document}$\kappa _{dv}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, suffices (<xref ref-type="fig" rid="fig4">Figure 4A</xref>; see Methods for more details). In this graph, the dynamical evolution of the task variable <inline-formula><alternatives><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft67">\begin{document}$\kappa _{dv}$\end{document}</tex-math></alternatives></inline-formula> can be expressed as:<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  \tau \frac{d\kappa _{dv}}{dt}=- \kappa _{dv}+E_{inp_{1}\rightarrow dv}\kappa _{inp_{1}}+E_{inp_{2}\rightarrow dv}\kappa _{inp_{2}}+E_{dv\rightarrow dv}\kappa _{dv}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the effective coupling <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$E_{inp_{s}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> from the input variable <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$\kappa _{inp_{s}}$\end{document}</tex-math></alternatives></inline-formula> to the decision variable <inline-formula><alternatives><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft70">\begin{document}$\kappa _{dv}$\end{document}</tex-math></alternatives></inline-formula> is equal to the overlap between the input representation direction <inline-formula><alternatives><mml:math id="inf71"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft71">\begin{document}$\overset{\sim }{I}_{s}$\end{document}</tex-math></alternatives></inline-formula> (each element is defined by <inline-formula><alternatives><mml:math id="inf72"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft72">\begin{document}$\overset{\sim }{I}_{s,i}=\phi ^{'}\left (x_{i}\right)I_{s,i}$\end{document}</tex-math></alternatives></inline-formula>) and the input-selection vector <inline-formula><alternatives><mml:math id="inf73"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft73">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula>. More precisely, <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$E_{inp_{s}\rightarrow dv}=\left \langle \overset{\sim }{I}_{s},n_{dv}\right \rangle $\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf75"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft75">\begin{document}$\left \langle a,b\right \rangle $\end{document}</tex-math></alternatives></inline-formula> is defined as <inline-formula><alternatives><mml:math id="inf76"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math><tex-math id="inft76">\begin{document}$\frac{1}{N}\sum _{1}^{N}a_{i}b_{i}$\end{document}</tex-math></alternatives></inline-formula> for two length-<inline-formula><alternatives><mml:math id="inf77"><mml:mi>N</mml:mi></mml:math><tex-math id="inft77">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> vectors. Since the input representation direction <inline-formula><alternatives><mml:math id="inf78"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft78">\begin{document}$\overset{\sim }{I}_{s}$\end{document}</tex-math></alternatives></inline-formula> depends on the single neuron gain <inline-formula><alternatives><mml:math id="inf79"><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft79">\begin{document}$\phi ^{'}$\end{document}</tex-math></alternatives></inline-formula> and the context input can modulate this gain, the effective coupling <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$E_{inp_{s}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> is context-dependent. Indeed, as shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>, <inline-formula><alternatives><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft81">\begin{document}$E_{inp_{1}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> exhibited a large value in context 1 but was negligible in context 2, while <inline-formula><alternatives><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft82">\begin{document}$E_{inp_{2}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> exhibited a large value in context 2 but was negligible in context 1. In other words, information from the input variable can arrive at the decision variable only in the relevant context, which exactly is the computation required by the CDM task.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Pathway-based information flow analysis.</title><p>(<bold>A</bold>) The information flow graph of the rank-1 model presented in <xref ref-type="fig" rid="fig2">Figure 2</xref>. In this graph, nodes represented task variables communicating with each other through directed connections (denoted as <inline-formula><alternatives><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft83">\begin{document}$E_{sender\rightarrow receiver}$\end{document}</tex-math></alternatives></inline-formula>) between them. Note that <inline-formula><alternatives><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft84">\begin{document}$E_{sender\rightarrow receiver}$\end{document}</tex-math></alternatives></inline-formula> is the overlap between the representation direction of the sender variable (e.g. the representation directions of input variable and decision variable <inline-formula><alternatives><mml:math id="inf85"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft85">\begin{document}$\overset{\sim }{I}_{inp}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf86"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft86">\begin{document}$\overset{\sim }{m}_{dv}$\end{document}</tex-math></alternatives></inline-formula>) and the input-selection vector of the receiver variable (e.g. the input-selection vector of decision variable <inline-formula><alternatives><mml:math id="inf87"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft87">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula>). As such, <inline-formula><alternatives><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft88">\begin{document}$E_{sender\rightarrow receiver}$\end{document}</tex-math></alternatives></inline-formula> naturally inherits the context dependency from the representation direction of task variable: while <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$E_{inp_{1}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> exhibited a large value and <inline-formula><alternatives><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft90">\begin{document}$E_{inp_{2}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> was negligible in context 1, the values of these two exchanged in context 2. (<bold>B</bold>) Illustration of information flow dynamics in (<bold>A</bold>) through discretized steps. At step 1, sensory information <inline-formula><alternatives><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft91">\begin{document}$A_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft92">\begin{document}$A_{2}$\end{document}</tex-math></alternatives></inline-formula> were placed in <inline-formula><alternatives><mml:math id="inf93"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:math><tex-math id="inft93">\begin{document}$inp1$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf94"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:math><tex-math id="inft94">\begin{document}$inp2$\end{document}</tex-math></alternatives></inline-formula> slots, respectively. Depending on the context, different information contents (i.e. <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$E_{inp_{1}\rightarrow dv}A_{1}$\end{document}</tex-math></alternatives></inline-formula> in context 1 and <inline-formula><alternatives><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft96">\begin{document}$E_{inp_{2}\rightarrow dv}A_{2}$\end{document}</tex-math></alternatives></inline-formula> in context 2) entered into the <inline-formula><alternatives><mml:math id="inf97"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft97">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot at step 2 and were maintained by recurrent connections in the following steps, which is desirable for the context-dependent decision-making task. (<bold>C</bold>) The information flow graph of the rank-3 model presented in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Different from (<bold>A</bold>), here to arrive at the <inline-formula><alternatives><mml:math id="inf98"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft98">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot, the input information has to first go through an intermediate slot (e.g. the <inline-formula><alternatives><mml:math id="inf99"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft99">\begin{document}$p_{1}\rightarrow iv_{1}\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> pathway in context 1 and the <inline-formula><alternatives><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft100">\begin{document}$p_{2}\rightarrow iv_{2}\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> pathway in context 2). (<bold>D</bold>) Illustration of information flow dynamics in (<bold>C</bold>) through discretized steps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Effective coupling between task variables for rank-1 and rank-3 recurrent neural networks (RNNs).</title><p>(<bold>A</bold>) Effective coupling between task variables for 100 trained rank-1 RNNs (<xref ref-type="fig" rid="fig2">Figure 2</xref>) in each context. Effective coupling between two task variables is defined as the overlap between the corresponding representation vector and input-selection vector. For example, the effective coupling from input 1 to decision variable <inline-formula><alternatives><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft101">\begin{document}$E_{inp_{1}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> is the overlap between <inline-formula><alternatives><mml:math id="inf102"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft102">\begin{document}$\overset{\sim }{I}_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft103">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula> (<inline-formula><alternatives><mml:math id="inf104"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><tex-math id="inft104">\begin{document}$\left \langle \overset{\sim }{I}_{1},n_{dv}\right \rangle $\end{document}</tex-math></alternatives></inline-formula>). As can be seen, the effective couplings of recurrent connection (<inline-formula><alternatives><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft105">\begin{document}$E_{dv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula>) are close to 1 in both contexts. The effective coupling from input task variables to decision variable is large in the relevant context (i.e. <inline-formula><alternatives><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft106">\begin{document}$E_{inp_{1}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> in context 1 and <inline-formula><alternatives><mml:math id="inf107"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft107">\begin{document}$E_{inp_{2}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> in context 2) and is negligible in the irrelevant context. (<bold>B</bold>) Effective coupling between task variables for 100 trained rank-3 RNNs (<xref ref-type="fig" rid="fig3">Figure 3</xref>) in each context. The effective coupling for recurrent connectivity (<inline-formula><alternatives><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>e</mml:mi><mml:mo>.</mml:mo></mml:math><tex-math id="inft108">\begin{document}$E_{dv\rightarrow dv},i.e.$\end{document}</tex-math></alternatives></inline-formula> overlap between <inline-formula><alternatives><mml:math id="inf109"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft109">\begin{document}$\overset{\sim }{m}_{dv}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft110">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula>) is close to 1 in both contexts. There is no direction connectivity from the input task variable to the decision variable since <inline-formula><alternatives><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft111">\begin{document}$E_{inp_{s}\rightarrow dv},s=1,2$\end{document}</tex-math></alternatives></inline-formula> are zero in both contexts. The difference between <inline-formula><alternatives><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft112">\begin{document}$E_{iv_{s}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> in the two contexts leads to selection vector modulation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Neural activity and task variable dynamics for single pulse input.</title><p>(<bold>A</bold>) Task setting for single pulse input. We study the neural activity dynamics for low-rank recurrent neural networks (RNN) when they receive pulse input. For simplicity, only the RNNs’ neural activity given a pulse from input 1 in context 1 is considered. (<bold>B</bold>) Illustration of neural activity for rank-1 RNN given pulse input. For rank-1 RNN (<xref ref-type="fig" rid="fig2">Figure 2</xref>), dynamics of <inline-formula><alternatives><mml:math id="inf113"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft113">\begin{document}$x\left (t\right)- I_{1}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> is always constrained in the subspace spanned by <inline-formula><alternatives><mml:math id="inf114"><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math><tex-math id="inft114">\begin{document}$\left \{I_{1},m_{dv}\right \}$\end{document}</tex-math></alternatives></inline-formula> , with the corresponding coefficients being the input task variable <inline-formula><alternatives><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft115">\begin{document}$(k_{inp_{1}})$\end{document}</tex-math></alternatives></inline-formula> and decision variable <inline-formula><alternatives><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft116">\begin{document}$(k_{dv})$\end{document}</tex-math></alternatives></inline-formula>, respectively. Moreover, the neural activity <inline-formula><alternatives><mml:math id="inf117"><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft117">\begin{document}$x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> is always constrained in a line (dashed line) orthogonal to the selection vector. (<bold>C</bold>) Task variable dynamics for rank-1 RNN given pulse input. We run the example RNN for a given input and project the results onto each axis to obtain the dynamics of each task variable (solid line). The analytical expressions for the dynamics of each task variable are provided in the methods section (dotted line). The simulated RNN results closely match the theoretical values. (<bold>D</bold>) Illustration of neural activity for rank-3 RNN given pulse input. For rank-3 RNN (<xref ref-type="fig" rid="fig3">Figure 3</xref>), dynamics of <inline-formula><alternatives><mml:math id="inf118"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft118">\begin{document}$x\left (t\right)- I_{1}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> are always constrained in the subspace spanned by <inline-formula><alternatives><mml:math id="inf119"><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math><tex-math id="inft119">\begin{document}$\left \{I_{1},m_{iv},m_{dv}\right \}$\end{document}</tex-math></alternatives></inline-formula>, with the corresponding coefficients being input task variable (<inline-formula><alternatives><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$k_{inp_{1}}$\end{document}</tex-math></alternatives></inline-formula>), intermediate task variable (<inline-formula><alternatives><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft121">\begin{document}$k_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula>) and decision variable (<inline-formula><alternatives><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft122">\begin{document}$k_{dv}$\end{document}</tex-math></alternatives></inline-formula>), respectively. (<bold>E</bold>) Task variable dynamics for rank-3 RNN given pulse input. Solid lines denote task variable dynamics calculated numerically by RNN simulation and dotted lines denote theoretical results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig4-figsupp2-v1.tif"/></fig></fig-group><p>To get a more intuitive understanding of the underlying information flow process, we discretized the equation and followed the information flow step by step. Specifically, by discretizing <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> using Euler’s method with a time step equal to the time constant <inline-formula><alternatives><mml:math id="inf123"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft123">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> of the system, we get<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle  \kappa _{dv}\left (t+\tau \right)=E_{inp_{1}\rightarrow dv}\kappa _{inp_{1}}\left (t\right)+E_{inp_{2}\rightarrow dv}\kappa _{inp_{2}}\left (t\right)+E_{dv\rightarrow dv}\kappa _{dv}\left (t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Take context 1 as an example (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top panel). Initially, there is no information in the system. In step 1, pulse inputs of size <inline-formula><alternatives><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft124">\begin{document}$A_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft125">\begin{document}$A_{2}$\end{document}</tex-math></alternatives></inline-formula> are placed in the <inline-formula><alternatives><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft126">\begin{document}$p_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft127">\begin{document}$p_{2}$\end{document}</tex-math></alternatives></inline-formula> slots, respectively. The information from these slots, after being multiplied by the corresponding effective coupling, then flows to the <inline-formula><alternatives><mml:math id="inf128"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft128">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot. In context 1, <inline-formula><alternatives><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft129">\begin{document}$E_{inp_{2}\rightarrow dv}\approx 0$\end{document}</tex-math></alternatives></inline-formula>, meaning only the content from the <inline-formula><alternatives><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft130">\begin{document}$p_{1}$\end{document}</tex-math></alternatives></inline-formula> slot can arrive at the <inline-formula><alternatives><mml:math id="inf131"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft131">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot. Consequently, in step 2, the information content in the <inline-formula><alternatives><mml:math id="inf132"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft132">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot would be <inline-formula><alternatives><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$E_{inp_{1}\rightarrow dv}A_{1}$\end{document}</tex-math></alternatives></inline-formula>. The following steps will replicate step 2 due to the recurrent connectivity of the <inline-formula><alternatives><mml:math id="inf134"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft134">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot. The scenario in context 2 is similar to context 1, except that only the content from the <inline-formula><alternatives><mml:math id="inf135"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft135">\begin{document}$p_{2}$\end{document}</tex-math></alternatives></inline-formula> slot arrives at the <inline-formula><alternatives><mml:math id="inf136"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft136">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, bottom panel). The continuous dynamics for each task variable given pulse input is displayed in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2A–C</xref>.</p><p>The same pathway-based information flow analysis can also be applied to the rank-3 model (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). In this model, similar to the rank-1 models, there are input variables (<inline-formula><alternatives><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft137">\begin{document}$k_{inp_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft138">\begin{document}$k_{inp_{2}}$\end{document}</tex-math></alternatives></inline-formula>) and decision variable <inline-formula><alternatives><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft139">\begin{document}$(k_{dv})$\end{document}</tex-math></alternatives></inline-formula>. Additionally, it includes intermediate variables (<inline-formula><alternatives><mml:math id="inf140"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft140">\begin{document}$k_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf141"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft141">\begin{document}$k_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula>) corresponding to the activity along the <inline-formula><alternatives><mml:math id="inf142"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft142">\begin{document}$m_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf143"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft143">\begin{document}$m_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula> axes. In this scenario, instead of flowing directly from the input to the decision variable, the information flows first to the intermediate variables and then to the decision variable. These intermediate variables act as intermediate nodes. Introducing these nodes does more than simply increase the steps from the input variable to the decision variable. In the rank-1 case, the context signals can only modulate the pathway from the input to the decision variable. However, in the rank-3 case, context signals can modulate the system in two ways: from the input to the intermediate variables and from the intermediate variables to the decision variable.</p><p>Take the rank-3 model introduced in <xref ref-type="fig" rid="fig3">Figure 3</xref> as an example. Context signals did not alter the representation of input signals, leading to constant effective couplings (i.e. constant <inline-formula><alternatives><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft144">\begin{document}$E_{inp_{1}\rightarrow iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft145">\begin{document}$E_{inp_{2}\rightarrow iv_{2}}$\end{document}</tex-math></alternatives></inline-formula>) from input to intermediate variables across contexts. Instead, it changed the effective coupling from the intermediate variables to the decision variable (i.e. large <inline-formula><alternatives><mml:math id="inf146"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft146">\begin{document}$E_{iv_{1}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> in context 1 and near zero <inline-formula><alternatives><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft147">\begin{document}$E_{iv_{1}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> in context 2; <xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Consider the context 1 scenario in the discrete case. In step 1, pulse inputs of size <inline-formula><alternatives><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft148">\begin{document}$A_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft149">\begin{document}$A_{2}$\end{document}</tex-math></alternatives></inline-formula> are placed in the <inline-formula><alternatives><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft150">\begin{document}$p_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf151"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft151">\begin{document}$p_{2}$\end{document}</tex-math></alternatives></inline-formula> slots, respectively. In step 2, information flows to the intermediate slots, with <inline-formula><alternatives><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft152">\begin{document}$E_{inp_{1}\rightarrow iv_{1}}A_{1}$\end{document}</tex-math></alternatives></inline-formula> in the <inline-formula><alternatives><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft153">\begin{document}$iv_{1}$\end{document}</tex-math></alternatives></inline-formula> slot and <inline-formula><alternatives><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft154">\begin{document}$E_{inp_{2}\rightarrow iv_{2}}A_{2}$\end{document}</tex-math></alternatives></inline-formula> in the <inline-formula><alternatives><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft155">\begin{document}$iv_{2}$\end{document}</tex-math></alternatives></inline-formula> slot. In step 3, only the information in the <inline-formula><alternatives><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft156">\begin{document}$iv_{1}$\end{document}</tex-math></alternatives></inline-formula> slot flows to the <inline-formula><alternatives><mml:math id="inf157"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft157">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot, with the information content being <inline-formula><alternatives><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft158">\begin{document}$E_{inp_{1}\rightarrow iv_{1}}E_{iv_{1}\rightarrow dv}A_{1}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, top panel). The scenario in context 2 is similar to context 1, except that only the content from the <inline-formula><alternatives><mml:math id="inf159"><mml:msub><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft159">\begin{document}$iv_{2}$\end{document}</tex-math></alternatives></inline-formula> slot reaches the <inline-formula><alternatives><mml:math id="inf160"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft160">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot in the third step, with the content being <inline-formula><alternatives><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft161">\begin{document}$E_{inp_{2}\rightarrow iv_{2}}E_{iv_{2}\rightarrow dv}A_{2}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, bottom panel). The continuous dynamics for each task variable given pulse input is displayed in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2D, E</xref>.</p><p>Together, this pathway-based information flow analysis provides an in-depth understanding of how the input information can be routed to the accumulator depending on the context, laying the foundation for a novel pathway-based information flow definition of selection vector and input contextual modulations.</p></sec><sec id="s2-5"><title>Information flow-based definition of selection vector modulation and selection vector for more general cases</title><p>Based on the understanding gained from the pathway-based information flow analysis, we now provide a novel definition of input modulation and selection vector modulation distinct from the one in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. To begin with, we first considered a model with mixed input and selection vector modulation (<xref ref-type="fig" rid="fig5">Figure 5</xref>, left), instead of studying extreme cases (i.e. one with pure input modulation in <xref ref-type="fig" rid="fig2">Figure 2</xref> and the other with pure selection vector modulation in <xref ref-type="fig" rid="fig3">Figure 3</xref>). In this more general model, input information can either go directly to the decision variable (with effective coupling <inline-formula><alternatives><mml:math id="inf162"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft162">\begin{document}$E_{inp\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula>) or first pass through the intermediate variables before reaching the decision variable (with effective coupling <inline-formula><alternatives><mml:math id="inf163"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft163">\begin{document}$E_{inp\rightarrow iv}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf164"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft164">\begin{document}$E_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> respectively). Applying the same information flow analysis, we see that a pulse input of unit size will ultimately reach the <inline-formula><alternatives><mml:math id="inf165"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft165">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot with a magnitude of <inline-formula><alternatives><mml:math id="inf166"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft166">\begin{document}$E_{inp\rightarrow dv}+E_{inp\rightarrow iv}E_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right; see Methods for more details). In other words, the total effective coupling <inline-formula><alternatives><mml:math id="inf167"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft167">\begin{document}$E_{tol}$\end{document}</tex-math></alternatives></inline-formula> from the input to the decision variable is equal to <inline-formula><alternatives><mml:math id="inf168"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft168">\begin{document}$E_{inp\rightarrow dv}+E_{inp\rightarrow iv}E_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula>. Now, it is straightforward to decompose the context-dependent modulation of <inline-formula><alternatives><mml:math id="inf169"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft169">\begin{document}$E_{tol}$\end{document}</tex-math></alternatives></inline-formula> in terms of input or selection vector change:<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  \Delta E_{tol}=\left (\Delta E_{inp\rightarrow dv}+\Delta E_{inp\rightarrow iv}\bar{E}_{iv\rightarrow dv}\right)+\left (\bar{E}_{inp\rightarrow iv}\Delta E_{iv\rightarrow dv}\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>in which the first component <inline-formula><alternatives><mml:math id="inf170"><mml:mi>Δ</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft170">\begin{document}$\Delta E_{inp\rightarrow dv}+\Delta E_{inp\rightarrow iv}\bar{E}_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> stands for the change of input representation (termed as input modulation) and the second component <inline-formula><alternatives><mml:math id="inf171"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft171">\begin{document}$\bar{E}_{inp\rightarrow iv}\Delta E_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> is the one without changing the stimulus input representation (termed as selection vector modulation).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A novel pathway-based definition of selection vector modulation.</title><p>(<bold>A</bold>) A pathway-based decomposition of contextual modulation in a model with both input and selection vector modulations. This definition is based on an explicit formula of the effective connection from the input variable to the decision variable in the model (i.e. <inline-formula><alternatives><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft172">\begin{document}$E_{inp\rightarrow dv}+E_{inp\rightarrow iv}E_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula>; see Method for details). The input modulation component is then defined as the modulation induced by the change of the input representation direction across contexts. The remaining component is then defined as the selection vector modulation one. (<bold>B</bold>) Illustration of contextual modulation decomposition introduced in <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>. In this definition, the selection vector has to be first reverse-engineered through linearized dynamical systems analysis. The input modulation component is then defined as the modulation induced by the change of input representation direction across contexts, while the selection vector modulation component is defined as the one induced by the change of the selection vector across contexts. (<bold>C</bold>) A family of handcrafted recurrent neural networks (RNNs) with both input and selection vector modulations. <inline-formula><alternatives><mml:math id="inf173"><mml:mi>α</mml:mi><mml:mo>,</mml:mo></mml:math><tex-math id="inft173">\begin{document}$\alpha ,$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf174"><mml:mi>β</mml:mi></mml:math><tex-math id="inft174">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf175"><mml:mi>η</mml:mi></mml:math><tex-math id="inft175">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula> represent the associated effective coupling between task variables. In this model family, the <inline-formula><alternatives><mml:math id="inf176"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft176">\begin{document}$inp\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> pathway, susceptible to the input modulation, is parameterized by <inline-formula><alternatives><mml:math id="inf177"><mml:mi>α</mml:mi></mml:math><tex-math id="inft177">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula> while the <inline-formula><alternatives><mml:math id="inf178"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft178">\begin{document}$inp\rightarrow iv\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> pathway, susceptible to the selection vector modulation, is parameterized by <inline-formula><alternatives><mml:math id="inf179"><mml:mi>β</mml:mi></mml:math><tex-math id="inft179">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf180"><mml:mi>η</mml:mi></mml:math><tex-math id="inft180">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula>. As such, the ratio of the input modulation to the selection vector modulation can be conveniently controlled by adjusting <inline-formula><alternatives><mml:math id="inf181"><mml:mi>α</mml:mi><mml:mo>,</mml:mo></mml:math><tex-math id="inft181">\begin{document}$\alpha ,$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf182"><mml:mi>β</mml:mi></mml:math><tex-math id="inft182">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf183"><mml:mi>η</mml:mi></mml:math><tex-math id="inft183">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula>. (<bold>D</bold>) Comparison of pathway-based definition in (<bold>A</bold>) with the classical definition in (<bold>B</bold>) using the model family introduced in (<bold>C</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig5-v1.tif"/></fig><p>We then asked if this pathway-based definition is equivalent to the one in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> based on linearized dynamical system analysis (<xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). To answer this question, we numerically compared these two definitions using a family of models with both input and selection vector modulations (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; see Methods for model details) and found that these two definitions produced the same proportion of selection vector modulation across a wide range of parameter regimes (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Together with theoretical derivation of equivalence (see Methods), this consistency confirmed the validity of our pathway-based definition of contextual modulation decomposition.</p><p>Having elucidated the pathway-based definitions of input and selection vector modulation, we next provide a novel pathway-based definition of selection vector. For the network depicted in <xref ref-type="fig" rid="fig5">Figure 5A</xref>, the total effective coupling <inline-formula><alternatives><mml:math id="inf184"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft184">\begin{document}$E_{inp\rightarrow dv}+E_{inp\rightarrow iv}E_{iv\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> can be rewritten as <inline-formula><alternatives><mml:math id="inf185"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><tex-math id="inft185">\begin{document}$\left \langle \overset{\sim }{I},n_{tol}\right \rangle $\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf186"><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><tex-math id="inft186">\begin{document}$\overset{\sim }{I}$\end{document}</tex-math></alternatives></inline-formula> is the input representation direction and <inline-formula><alternatives><mml:math id="inf187"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft187">\begin{document}$n_{tol}=n_{dv}+\left \langle \overset{\sim }{m}_{iv},n_{dv}\right \rangle n_{iv}$\end{document}</tex-math></alternatives></inline-formula>. This reformulation aligns with the insight that the amount of input information that can be integrated by the accumulator is determined by the dot product between the input representation direction <inline-formula><alternatives><mml:math id="inf188"><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><tex-math id="inft188">\begin{document}$\overset{\sim }{I}$\end{document}</tex-math></alternatives></inline-formula> and the selection vector. Thus, <inline-formula><alternatives><mml:math id="inf189"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft189">\begin{document}$n_{tol}$\end{document}</tex-math></alternatives></inline-formula> is the selection vector of the circuit in <xref ref-type="fig" rid="fig5">Figure 5A</xref>.</p><p>To better understand why selection vector has such a formula, we visualized the information propagation from input to the choice axis using a low-rank matrix (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Specifically, it comprises two components, each corresponding to a distinct pathway. For the first component, input information is directly selected by <inline-formula><alternatives><mml:math id="inf190"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft190">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula>. For the second component, input information is sent first to the intermediate variable and then to the decision variable. This pathway involves two steps: in the first step, the input representation vector is selected by <inline-formula><alternatives><mml:math id="inf191"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft191">\begin{document}$n_{iv}$\end{document}</tex-math></alternatives></inline-formula>, and in the second step, to arrive at the choice axis, the selected information has to be multiplied by the effective coupling <inline-formula><alternatives><mml:math id="inf192"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><tex-math id="inft192">\begin{document}$E_{iv\rightarrow dv}=\left \langle \overset{\sim }{m}_{iv},n_{dv}\right \rangle $\end{document}</tex-math></alternatives></inline-formula>. By concatenating these two steps, information propagation from the input to the choice axis in this pathway can be effectively viewed as a selection process mediated by the vector <inline-formula><alternatives><mml:math id="inf193"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft193">\begin{document}$\left \langle \overset{\sim }{m}_{iv},n_{dv}\right \rangle n_{iv}$\end{document}</tex-math></alternatives></inline-formula> (termed as the second-order selection vector component). Therefore, <inline-formula><alternatives><mml:math id="inf194"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft194">\begin{document}$n_{tol}$\end{document}</tex-math></alternatives></inline-formula> provides a novel pathway-based definition of selection vector in the network. We further verified the equivalence between this pathway-based definition and the linearized-dynamical-systems-based classical definition (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>) in our simple circuit through theoretical derivation (see Methods) and numerical comparison (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>An explicit pathway-based formula of selection vector.</title><p>(<bold>A</bold>) Illustration of how an explicit pathway-based formula of selection vector is derived. In a model with both the first-order selection pathway (i.e. <inline-formula><alternatives><mml:math id="inf195"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft195">\begin{document}$inp\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula>) and the second-order selection pathway (i.e. <inline-formula><alternatives><mml:math id="inf196"><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft196">\begin{document}$inp\rightarrow iv\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula>), the second-order pathway can be reduced to a pathway with the effective selection vector <inline-formula><alternatives><mml:math id="inf197"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft197">\begin{document}$\left \langle \overset{\sim }{m}_{iv},n_{dv}\right \rangle n_{iv}$\end{document}</tex-math></alternatives></inline-formula> that exhibited the contextual dependency missing in rank-1 models. (<bold>B</bold>) Comparison between this pathway-based selection vector and the classical one (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>) using 1000 recurrent neural networks (RNNs) (see Methods for details). (<bold>C</bold>) The connection between our understanding and the classical understanding in neural state space. Based upon the explicit formula of selection vector in (<bold>A</bold>), the selection vector modulation has to rely on the contextual modulation of additional representation direction (i.e. <inline-formula><alternatives><mml:math id="inf198"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft198">\begin{document}$\overset{\sim }{m}_{iv}$\end{document}</tex-math></alternatives></inline-formula>) orthogonal to both the input representation direction (<inline-formula><alternatives><mml:math id="inf199"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft199">\begin{document}$\overset{\sim }{I}_{inp}$\end{document}</tex-math></alternatives></inline-formula>) and decision variable representation direction (<inline-formula><alternatives><mml:math id="inf200"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft200">\begin{document}$\overset{\sim }{m}_{dv}$\end{document}</tex-math></alternatives></inline-formula>, line attractor). Therefore, it requires at least three dimensions (i.e. <inline-formula><alternatives><mml:math id="inf201"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft201">\begin{document}$\overset{\sim }{I}_{inp}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf202"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft202">\begin{document}$\overset{\sim }{m}_{dv}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf203"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft203">\begin{document}$\overset{\sim }{m}_{iv}$\end{document}</tex-math></alternatives></inline-formula>) to account for the selection vector modulation in neural state space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig6-v1.tif"/></fig><p>To visualize the pathway-based selection vector in neural activity state space, we found that a minimum of three dimensions is required, including the input representation direction, the decision variable representation direction, and the intermediate variable representation direction (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, left). This geometric visualization highlighted the role of extra dimensions beyond the classical two-dimensional neural activity space spanned by the line attractor and selection vector (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, right) in accounting for the selection vector modulation. This is simply because only the second-order selection vector component, which depends on the existence of the intermediate variable, is subject to contextual modulation. In other words, without extra dimensions to support intermediate variable encoding, there will be no selection modulation.</p><p>Together, this set of analyses provided a parsimonious pathway-based understanding for both selection vector and its contextual modulation.</p></sec><sec id="s2-6"><title>Model prediction verification with vanilla RNN models</title><p>The new insights obtained from our new framework enable us to generate testable predictions for better differentiating selection vector modulation from input modulation, a major challenge unresolved in the field (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>).</p><p>First, we predict that it is more likely to have a large proportion of selection vector modulation for a neural network with high-dimensional connectivity. To better explain the underlying rationale, we can simply compare the number of potential connections contributing to the input modulation with those contributing to the selection vector modulation for a given neural network model. For example, in the network presented in <xref ref-type="fig" rid="fig5">Figure 5</xref>, there are three connections (including light blue, dark blue, and green ones) while only one connection (i.e. the green one) supporting selection vector modulation. For a circuit with many higher-order pathways (e.g. <xref ref-type="fig" rid="fig7">Figure 7A</xref>), only those connections with the input as the sender is able to support input modulation. In other words, there exist far more connections potentially eligible to support the selection vector modulation (<xref ref-type="fig" rid="fig7">Figure 7B</xref>), thereby leading to a large selection vector modulation proportion. We then tested this prediction on vanilla RNNs trained through backpropagation (<xref ref-type="fig" rid="fig7">Figure 7C</xref>; <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib45">Yang and Wang, 2020</xref>). Using effective dimension (see Methods for a formal definition; <xref ref-type="bibr" rid="bib33">Rudelson and Vershynin, 2007</xref>; <xref ref-type="bibr" rid="bib35">Sanyal et al., 2020</xref>) to quantify the dimensionality of the connectivity matrix, we found a strong positive correlation between the effective dimension of connectivity matrix and the selection vector modulation (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, left panel and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1A</xref>, see Method for details).</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The correlation between the dimensionality of neural dynamics and the proportion of selection vector modulation is confirmed in vanilla recurrent neural networks (RNNs).</title><p>(<bold>A</bold>) A general neural circuit model of context-dependent decision-making (CDM). In this model, there are multiple pathways capable of propagating the input information to the decision variable slot, of which the blue connections are susceptible to the input modulation while the green connections are susceptible to the selection vector modulation (see Methods for details). (<bold>B</bold>) The explicit formula of both the effective connection from the input variable to the decision variable and the effective selection vector for the model in (<bold>A</bold>). (<bold>C</bold>) The setting of vanilla RNNs trained to perform the CDM task. See Methods for more details. (<bold>D</bold>) Positive correlation between effective connectivity dimension and proportion of selection vector modulation. Given a trained RNN with matrix <italic>J</italic>, the effective connectivity dimension, defined by <inline-formula><alternatives><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft204">\begin{document}$\sum _{i=1}^{n}\sigma _{i}^{2}/\sigma _{1}^{2}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>…</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft205">\begin{document}$\sigma _{1}\geq \sigma _{2}\geq \ldots \geq \sigma _{n}$\end{document}</tex-math></alternatives></inline-formula> are singular values of <italic>J</italic>, is used to quantify the connectivity dimensionality. Spearman’s rank correlation, <italic>r</italic>=0.919, p&lt;1e-3, n=3892. The x-axis is displayed in log scale. (<bold>E</bold>) Single neuron response kernels for two example RNNs. The neuron response kernels were calculated using a regression method (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>; see Methods for details). For simplicity, only response kernels for input 1 are displayed. <italic>Top</italic>: Response kernels for two example neurons in the RNN with low effective dimension (indicated by a star marker in panel D). Two typical response kernels, including the decision variable profile (left) and the sensory input profile (right), are displayed. <italic>Bottom</italic>: Response kernels for three example neurons in the RNN with high effective dimension (indicated by a square marker in panel D). In addition to the decision variable profile (left) and sensory input profile (middle), there are neurons whose response kernels initially increase and then decrease (right). Gray lines, response kernels in context 1 (i.e. rel. ctx.). Blue lines, response kernels in context 2 (i.e. irrel. Ctx.). (<bold>F</bold>) Principal dynamical modes for response kernels in the population level extracted by singular value decomposition. <italic>Left</italic>: Shared dynamical modes, including one persistent choice mode (gray) and three transient modes (blue, orange, green) are identified across both RNNs. <italic>Right</italic>: For the <inline-formula><alternatives><mml:math id="inf206"><mml:mi>i</mml:mi></mml:math><tex-math id="inft206">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th transient mode, the normalized percentage of explained variance (PEV) is given by <inline-formula><alternatives><mml:math id="inf207"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>39</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math><tex-math id="inft207">\begin{document}$\sigma _{i}^{2}/\sum _{j=1}^{39}\sigma _{j}^{2}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf208"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>…</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>39</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft208">\begin{document}$\sigma _{1}\geq \sigma _{2}\geq \ldots \geq \sigma _{39}$\end{document}</tex-math></alternatives></inline-formula> are singular values for each transient mode (see Methods for details). (<bold>G</bold>) Positive correlation between response-kernel-based index and proportion of selection vector modulation. For a given RNN, percentage of explained variance (PEV) of extra dynamical modes is defined as the accumulated normalized PEV of the second and subsequent transient dynamical modes (see Methods for details). Spearman’s rank correlation, <italic>r</italic>=0.902, p&lt;1e-3, n=3892. The x-axis is displayed in log scale.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Training vanilla recurrent neural networks (RNNs) with different regularization coefficients.</title><p>(<bold>A</bold>) The influence of regularization coefficient on effective connectivity dimension of trained RNNs. For each regularization coefficient, we trained 100 full-rank RNNs (<xref ref-type="fig" rid="fig7">Figure 7</xref>, panel D). Larger regularization results in connectivity matrices with lower rank, leading to a smaller effective connectivity dimension. (<bold>B</bold>) The influence of regularization coefficient on selection vector modulation of trained RNNs. Distribution of selection vector modulation for networks trained with different regularization coefficients. Larger regularization leads to networks that favor the input modulation strategy. (<bold>C</bold>) The relationship between the proportion of explained variance (PEV) in extra dimensions and effective connectivity dimension. There is a strong positive correlation between the PEV in extra dimensions and the effective connectivity dimension in both contexts. In each panel, each dot denotes a trained RNN, with different colors denoting different regularization coefficients.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig7-figsupp1-v1.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Verification correlation results using vanilla recurrent neural networks (RNNs) trained with different hyper-parameter settings.</title><p>(<bold>A</bold>) Similar results in trained vanilla RNNs with a softplus activation function. <italic>Left</italic>: Spearman’s rank correlation, <italic>r</italic>=0.945, p&lt;1e-3, n=2564. <italic>Right</italic>: Spearman’s rank correlation, <italic>r</italic>=0.803, p&lt;1e-3, n=2564. The x-axes are displayed in log scale for both panels. (<bold>B</bold>) Similar results in trained vanilla RNNs initialized with a variance of <inline-formula><alternatives><mml:math id="inf209"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft209">\begin{document}$1/N$\end{document}</tex-math></alternatives></inline-formula>. <italic>Left</italic>: Spearman’s rank correlation, <italic>r</italic>=0.973, p&lt;1e-3, n=2630. <italic>Right</italic>: Spearman’s rank correlation, <italic>r</italic>=0.976, p&lt;1e-3, n=2630. The x-axes are displayed in log scale for both panels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig7-figsupp2-v1.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Two recurrent neural networks (RNNs) with distinct modulation strategies produce the same neural activities.</title><p>(<bold>A</bold>) Information flow graph for the two RNNs. The black arrows denote that the effective coupling from the head to the tail is 1. For RNN1, the closure of <inline-formula><alternatives><mml:math id="inf210"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft210">\begin{document}$p_{1}\rightarrow iv_{2}$\end{document}</tex-math></alternatives></inline-formula> on the pathway <inline-formula><alternatives><mml:math id="inf211"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft211">\begin{document}$p_{1}\rightarrow iv_{2}\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> prevents <inline-formula><alternatives><mml:math id="inf212"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft212">\begin{document}$p_{1}$\end{document}</tex-math></alternatives></inline-formula> from reaching <inline-formula><alternatives><mml:math id="inf213"><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft213">\begin{document}$iv_{2}$\end{document}</tex-math></alternatives></inline-formula> and subsequently the decision variable <inline-formula><alternatives><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft214">\begin{document}$(dv)$\end{document}</tex-math></alternatives></inline-formula>, indicating that RNN1 uses solely input modulation strategy for input 1. For RNN2, the closure of <inline-formula><alternatives><mml:math id="inf215"><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft215">\begin{document}$iv_{1}\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> on the pathway <inline-formula><alternatives><mml:math id="inf216"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft216">\begin{document}$p_{1}\rightarrow iv_{1}\rightarrow dv$\end{document}</tex-math></alternatives></inline-formula> means that although <inline-formula><alternatives><mml:math id="inf217"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft217">\begin{document}$p_{1}$\end{document}</tex-math></alternatives></inline-formula> can reach <inline-formula><alternatives><mml:math id="inf218"><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft218">\begin{document}$iv_{1}$\end{document}</tex-math></alternatives></inline-formula>, the subsequent step of <inline-formula><alternatives><mml:math id="inf219"><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft219">\begin{document}$iv_{1}$\end{document}</tex-math></alternatives></inline-formula> reading <inline-formula><alternatives><mml:math id="inf220"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft220">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> is blocked. This indicates RNN2 uses solely the selection vector modulation strategy for input 1. (<bold>B</bold>) Connectivity weight among three example neurons in the two RNNs. Each neuron belongs to one of the three neuron populations (see Method for more details). Notice that the connectivity weights from <inline-formula><alternatives><mml:math id="inf221"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft221">\begin{document}$n_{1}$\end{document}</tex-math></alternatives></inline-formula> (neuron 1) to <inline-formula><alternatives><mml:math id="inf222"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft222">\begin{document}$n_{3}$\end{document}</tex-math></alternatives></inline-formula> (or <inline-formula><alternatives><mml:math id="inf223"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft223">\begin{document}$n_{2}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf224"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft224">\begin{document}$n_{3}$\end{document}</tex-math></alternatives></inline-formula>) are different between the two RNNs. (<bold>C</bold>) Neural activities for the three neurons in three example trials. Orange lines denote activities for RNN1 and blue lines denote activities for RNN2. The neural activity is approximately equal between the two RNNs. (<bold>D</bold>) Histogram of the single neuron activity similarity between the two RNNs. We calculated the similarity between the activity of the <inline-formula><alternatives><mml:math id="inf225"><mml:mi>i</mml:mi></mml:math><tex-math id="inft225">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron in RNN1 and the <inline-formula><alternatives><mml:math id="inf226"><mml:mi>i</mml:mi></mml:math><tex-math id="inft226">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron in RNN2 during trial <inline-formula><alternatives><mml:math id="inf227"><mml:mi>k</mml:mi></mml:math><tex-math id="inft227">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> (<italic>r2_score</italic> function in the <italic>sklearn</italic> package of Python). Averaging over the batches provides the similarity between corresponding neurons (neuron <inline-formula><alternatives><mml:math id="inf228"><mml:mi>i</mml:mi></mml:math><tex-math id="inft228">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> in RNN1 and neuron <inline-formula><alternatives><mml:math id="inf229"><mml:mi>i</mml:mi></mml:math><tex-math id="inft229">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> in RNN2).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig7-figsupp3-v1.tif"/></fig><fig id="fig7s4" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 4.</label><caption><title>Artificially introducing redundant structure can disrupt the percentage of explained variance (PEV) of extra dynamical modes index.</title><p>(<bold>A</bold>) Positive correlation between response-kernel-based index and proportion of selection vector modulation in trained vanilla recurrent neural networks (RNNs). (Spearman’s rank correlation, n=75). (<bold>B</bold>) No significant correlation between these two metrics in RNNs with additional task-irrelevant variance. (Spearman’s rank correlation, n=75). (<bold>C</bold>) PEV of irrelevant activity showed a significant difference between trained vanilla RNNs and RNNs with additional task-irrelevant variance. (one-way ANOVA test, n=75, ***p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-fig7-figsupp4-v1.tif"/></fig></fig-group><p>We then asked if we could generate predictions to quantify the proportion of selection vector modulation purely based on neural activities. To this end, as what has been performed in Pagan et al., we took advantage of the pulse-based sensory input setting to calculate the single neuron response kernel (see Methods for details). For a given neuron, the associated response kernel is defined to characterize the influence of a pulse input on its firing rate in later times. For example, for a neuron encoding the decision variable, the response kernel should exhibit a profile accumulating evidence over time (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, top left). In contrast, for a neuron encoding the sensory input, the response kernel should exhibit an exponential decay profile with time constant <inline-formula><alternatives><mml:math id="inf230"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft230">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, top right). For each RNN trained through backpropagation, we then examined the associated single neuron response kernels. We found that for the model with low effective dimension (denoted by a star marker in <xref ref-type="fig" rid="fig7">Figure 7D</xref>), there were mainly two types of response kernels, including sensory input profile and decision variable profile (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, top). In contrast, for the model with the highest effective dimension (denoted by a square marker in <xref ref-type="fig" rid="fig7">Figure 7D</xref>), aside from the sensory input and decision variable profiles, richer response kernel profiles were exhibited (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, bottom). In particular, there was a set of single neuron response kernels with peak amplitudes occurring between the pulse input onset and the choice onset (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, bottom right). These response kernels cannot be explained by the combination of sensory input and decision variables. Instead, the existence of these response kernels signifies neural dynamics in extra dimensions beyond the subspaces spanned by the input and decision variable, the genuine neural dynamical signature of the existence of selection vector modulation (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p><p>While single neuron response kernels are illustrative in highlighting the model difference, they lack explanatory power at the population level. Therefore, we employed the singular value decomposition method to extract the principal dynamical modes of response kernels at the population level (see Methods for details). We found that similar dynamical modes, including one persistent choice mode (gray) and three transient modes (blue, orange, green), were shared across both the low and high effective dimension models (<xref ref-type="fig" rid="fig7">Figure 7F</xref>, left). The key difference between these two models lies in the percentage of explained variance (PEV) of the second transient mode (orange): while there is near-zero PEV in the low effective dimension model (<xref ref-type="fig" rid="fig7">Figure 7F</xref>, top right), there is substantial PEV in the high effective dimension model (<xref ref-type="fig" rid="fig7">Figure 7F</xref>, bottom right), consistent with the single neuron picture shown in <xref ref-type="fig" rid="fig7">Figure 7E</xref>. This result led us to use the PEV of extra dynamical modes (including the orange and green ones; see Methods for details) as a simple index to quantify the amount of selection vector modulation in these models. As expected, we found that the PEV of extra dynamical modes can serve as a reliable index reflecting the proportion of selection vector modulation in these models (<xref ref-type="fig" rid="fig7">Figure 7G</xref>, right panel and <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1B and C</xref>). Similar results for vanilla RNNs trained with different hyperparameter settings are displayed in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>.</p><p>Together, we identified novel neural dynamical signatures of section vector modulation at both the single neuron and population level, suggesting the potential great utility of these neural dynamical signatures in distinguishing the contribution of selection vector modulation from input modulation in experimental data.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Using low-rank RNNs, we provided a rigorous theoretical framework linking network connectivity, neural dynamics, and selection mechanisms, and gained an in-depth algebraic and geometric understanding of both input and selection vector modulation mechanisms, and accordingly uncovered a previously unknown link between selection vector modulation and extra dimensions in neural state space. This gained understanding enabled us to generate novel predictions linking novel neural dynamic modes with the proportion of selection vector modulation, paving the way towards addressing the intricacy of neural variability across subjects in context-dependent computation.</p><sec id="s3-1"><title>A pathway-based definition of selection vector modulation</title><p>In their seminal work, Mante, Sussillo, and their collaborators developed a numerical approach to compute the selection vector for trained RNN models (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>). Based on this concept of selection vector, recently, Pagan et al. proposed a new theoretical framework to decompose the solution space of context-dependent decision-making, in which input modulation and selection vector modulation were explicitly defined (i.e. <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Here, taking the theoretical advantage of low-rank RNNs (<xref ref-type="bibr" rid="bib21">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>), we went beyond numerical reverse-engineering and provided a complementary pathway-based definition of both selection vector and selection vector modulation (i.e. <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). This new definition gained us a novel geometric understanding of selection vector modulation, revealed a previously unknown link between extra dimensions and selection vector modulation (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), and eventually provided us with experimentally identifiable neural dynamical signature of selection vector modulation at both the single neuron and population levels (<xref ref-type="fig" rid="fig7">Figure 7, E-G</xref>).</p></sec><sec id="s3-2"><title>Individual neural variability in higher cognition</title><p>One hallmark of higher cognition is individual variability, as the same higher cognition problem can be solved equally well with different strategies. Therefore, studying the neural computations underlying individual variability is no doubt of great importance (<xref ref-type="bibr" rid="bib14">Hariri, 2009</xref>; <xref ref-type="bibr" rid="bib31">Parasuraman and Jiang, 2012</xref>; <xref ref-type="bibr" rid="bib16">Keung et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Nelli et al., 2023</xref>). Recent experimental advances enabled researchers to investigate this important issue in a systematic manner using delicate behavioral paradigms and large-scale recordings (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). However, the computation underlying higher cognition is largely internal, requiring discovering novel neural activity patterns as internal indicators to differentiate distinct circuit mechanisms. In the example of context-dependent decision-making studied here, to differentiate selection vector modulation from input modulation, we found the PEV in extra dynamical modes is a reliable index for a wide variety of RNNs (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). However, cautions have to be made here as we can conveniently construct counter-examples deviating from the picture depicted by this index. For instance, manually introducing additional dimensions that do not directly contribute to the computation can disrupt the index (<xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4A and B</xref>). In the extreme scenario, we can construct two models with distinct circuit mechanisms (selection vector modulation and input modulation, respectively) but having the same neural activities (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>), suggesting that any activity-based index alone would fail to make this differentiation. Then, why did the proposed index work for the trained vanilla RNNs shown in <xref ref-type="fig" rid="fig7">Figure 7D</xref>? Our lesion analysis suggests that the underlying reason is that the major variance in neural activity of vanilla RNNs learned through backpropagation is task-relevant (<xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4C</xref>, see Methods for details). However, it is highly likely that task-irrelevant neural activity variance exists in higher brain regions, meaning the proposed index may not perform well in neural recordings. Therefore, our modeling work suggests that, to address the intricacy of individual variability of neural computations underlying higher cognition, integrative efforts incorporating not only large-scale neural activity recordings but also activity perturbations, neuronal connectivity knowledge, and computational modeling may be inevitably required.</p></sec><sec id="s3-3"><title>Beyond context-dependent decision-making</title><p>While we mainly focused on context-dependent decision-making tasks in this study, the issue of whether input or selection vector modulation prevails is not limited to the domain of decision-making. For instance, recent work by <xref ref-type="bibr" rid="bib8">Chen et al., 2024</xref> demonstrated that during sequence working memory control (<xref ref-type="bibr" rid="bib7">Botvinick and Watanabe, 2007</xref>; <xref ref-type="bibr" rid="bib44">Xie et al., 2022</xref>), sensory inputs presented at different ordinal ranks first entered into a common sensory subspace and then were routed to the corresponding rank-specific working memory subspaces in monkey frontal cortex. Here, similar to the decision-making case (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), the same issue arises: where is the input information selected by the context (here, the ordinal rank)? Can the presence of a common sensory subspace (similar to the presence of location information in both relevant and irrelevant contexts in <xref ref-type="fig" rid="fig1">Figure 1C</xref>) preclude the input modulation? The pathway-based understanding of input and selection vector modulation gained from CDM in this study may be transferable to address these similar issues.</p></sec><sec id="s3-4"><title>The role of transient dynamics in extra dimensions in context-dependent computation</title><p>In this study, we linked the selection vector modulation with transient dynamics (<xref ref-type="bibr" rid="bib1">Aoi et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Soldado-Magraner et al., 2023</xref>) in extra dimensions. While the transient dynamics in extra dimensions are not necessary in context-dependent decision-making here (<xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>; <xref ref-type="fig" rid="fig2">Figure 2</xref>), more complex context-dependent computation may require its presence. For example, recent work by <xref ref-type="bibr" rid="bib42">Tian et al., 2024</xref> found that transient dynamics in extra subspaces is required to perform the switch operation (i.e. exchanging information in subspace 1 with information in subspace 2). Understanding how transient dynamics in extra dimensions contribute to complex context-dependent computation warrants further systematic investigation.</p><p>In summary, through low-rank neural network modeling, our work provided a parsimonious mechanistic account for how information can be selected along different pathways, making significant contributions towards understanding the intriguing selection mechanisms in context-dependent computation.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Python</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_00839">SCR_008394</ext-link></td><td align="left" valign="bottom">version: 3.9</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>The general form of RNNs</title><p>We investigated networks of <inline-formula><alternatives><mml:math id="inf231"><mml:mi>N</mml:mi></mml:math><tex-math id="inft231">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> neurons with <inline-formula><alternatives><mml:math id="inf232"><mml:mi>S</mml:mi></mml:math><tex-math id="inft232">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> input channels, described by the following temporal evolution equation<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  \tau \frac{dx_{i}\left (t\right)}{dt}=- x_{i}\left (t\right)+\sum _{j=1}^{N}J_{ij}\phi \left (x_{j}\left (t\right)\right)+\sum _{s=1}^{S}I_{si}u_{s}\left (t\right)+\epsilon _{i}\left (t\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>In this equation, <inline-formula><alternatives><mml:math id="inf233"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft233">\begin{document}$x_{i}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> represents the activation of neuron <inline-formula><alternatives><mml:math id="inf234"><mml:mi>i</mml:mi></mml:math><tex-math id="inft234">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf235"><mml:mi>t</mml:mi></mml:math><tex-math id="inft235">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf236"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft236">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> denotes the characteristic time constant of a single neuron, and <inline-formula><alternatives><mml:math id="inf237"><mml:mi>ϕ</mml:mi></mml:math><tex-math id="inft237">\begin{document}$\phi $\end{document}</tex-math></alternatives></inline-formula> is a nonlinear activation function. Unless otherwise specified, we use the tanh function as the activation function. The coefficient <inline-formula><alternatives><mml:math id="inf238"><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft238">\begin{document}$J_{ij}$\end{document}</tex-math></alternatives></inline-formula> represents the connectivity weight from neuron <inline-formula><alternatives><mml:math id="inf239"><mml:mi>j</mml:mi></mml:math><tex-math id="inft239">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula> to neuron <inline-formula><alternatives><mml:math id="inf240"><mml:mi>i</mml:mi></mml:math><tex-math id="inft240">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>. The input <inline-formula><alternatives><mml:math id="inf241"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft241">\begin{document}$u_{s}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> corresponds to the <inline-formula><alternatives><mml:math id="inf242"><mml:mi>s</mml:mi></mml:math><tex-math id="inft242">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>-th input channel at time <inline-formula><alternatives><mml:math id="inf243"><mml:mi>t</mml:mi></mml:math><tex-math id="inft243">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>, with feedforward weight <inline-formula><alternatives><mml:math id="inf244"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft244">\begin{document}$I_{si}$\end{document}</tex-math></alternatives></inline-formula> to neuron <inline-formula><alternatives><mml:math id="inf245"><mml:mi>i</mml:mi></mml:math><tex-math id="inft245">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf246"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft246">\begin{document}$\epsilon _{i}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> represents white noise at time <inline-formula><alternatives><mml:math id="inf247"><mml:mi>t</mml:mi></mml:math><tex-math id="inft247">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>. The network’s output is obtained from the neuron’s activity <inline-formula><alternatives><mml:math id="inf248"><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft248">\begin{document}$\phi \left (x\right)$\end{document}</tex-math></alternatives></inline-formula> through a linear projection:<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  z\left (t\right)=\frac{1}{N}\sum _{i=1}^{N}w_{i}\phi \left (x_{i}\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>The connectivity matrix <inline-formula><alternatives><mml:math id="inf249"><mml:mi>J</mml:mi></mml:math><tex-math id="inft249">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula>, specified as <inline-formula><alternatives><mml:math id="inf250"><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><tex-math id="inft250">\begin{document}$J=\left \{J_{ij}\right \},i=1,\ldots ,N,j=1,\ldots ,N$\end{document}</tex-math></alternatives></inline-formula>, can be a low-rank or a full-rank matrix. In the low-rank case, <inline-formula><alternatives><mml:math id="inf251"><mml:mi>J</mml:mi></mml:math><tex-math id="inft251">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula> is restricted to a low-rank matrix <inline-formula><alternatives><mml:math id="inf252"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math><tex-math id="inft252">\begin{document}$\frac{1}{N}\sum _{r=1}^{R}m_{r}n_{r}^{T}$\end{document}</tex-math></alternatives></inline-formula>, in which <inline-formula><alternatives><mml:math id="inf253"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft253">\begin{document}$m_{r}$\end{document}</tex-math></alternatives></inline-formula> is the <inline-formula><alternatives><mml:math id="inf254"><mml:mi>r</mml:mi></mml:math><tex-math id="inft254">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>-th <italic>output vector</italic>, and <inline-formula><alternatives><mml:math id="inf255"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft255">\begin{document}$n_{r}$\end{document}</tex-math></alternatives></inline-formula> is the <inline-formula><alternatives><mml:math id="inf256"><mml:mi>r</mml:mi></mml:math><tex-math id="inft256">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>-th <italic>input-selection vector</italic>, with each element of <inline-formula><alternatives><mml:math id="inf257"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft257">\begin{document}$m_{r}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf258"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft258">\begin{document}$n_{r}$\end{document}</tex-math></alternatives></inline-formula> considered an independent parameter (<xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>). In this paper, we set the time constant <inline-formula><alternatives><mml:math id="inf259"><mml:mi>τ</mml:mi></mml:math><tex-math id="inft259">\begin{document}$\tau $\end{document}</tex-math></alternatives></inline-formula> to be 100 ms, and use Euler’s method to discretize the evolution equation with a time step <inline-formula><alternatives><mml:math id="inf260"><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:math><tex-math id="inft260">\begin{document}$\Delta t=20ms$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-2"><title>Task setting</title><p>We modeled the click-version CDM task recently investigated by <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>. The task involves four input channels <inline-formula><alternatives><mml:math id="inf261"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft261">\begin{document}$u_{1}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf262"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft262">\begin{document}$u_{2}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf263"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft263">\begin{document}$u_{1}^{ctx}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf264"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft264">\begin{document}$u_{2}^{ctx}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf265"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft265">\begin{document}$u_{1}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf266"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft266">\begin{document}$u_{2}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> are stimulus inputs and <inline-formula><alternatives><mml:math id="inf267"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft267">\begin{document}$u_{1}^{ctx}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf268"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft268">\begin{document}$u_{2}^{ctx}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> are context inputs. Initially, there is a fixation period lasting for <inline-formula><alternatives><mml:math id="inf269"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:math><tex-math id="inft269">\begin{document}$T_{fix}=200ms$\end{document}</tex-math></alternatives></inline-formula>. This is followed by a stimulus period of <inline-formula><alternatives><mml:math id="inf270"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>800</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:math><tex-math id="inft270">\begin{document}$T_{sti}=800ms$\end{document}</tex-math></alternatives></inline-formula> and then a decision period of <inline-formula><alternatives><mml:math id="inf271"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:math><tex-math id="inft271">\begin{document}$T_{decision}=20ms$\end{document}</tex-math></alternatives></inline-formula>.</p><p>For trial <inline-formula><alternatives><mml:math id="inf272"><mml:mi>k</mml:mi></mml:math><tex-math id="inft272">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, at each time step, the total number of pulse inputs (#pulse) is sampled from a Poisson distribution with a mean value of <inline-formula><alternatives><mml:math id="inf273"><mml:mn>40</mml:mn><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math><tex-math id="inft273">\begin{document}$40\Delta t=0.8$\end{document}</tex-math></alternatives></inline-formula>. Each pulse has two properties: location and frequency. Location can be either left or right, and frequency can be either high or low. We randomly sample a pulse to be right with probability <inline-formula><alternatives><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft274">\begin{document}$p^{k} $\end{document}</tex-math></alternatives></inline-formula> (hence left with probability <inline-formula><alternatives><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft275">\begin{document}$1- p^{k} $\end{document}</tex-math></alternatives></inline-formula>) and to be high with probability <inline-formula><alternatives><mml:math id="inf276"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft276">\begin{document}$p_{high}^{k}$\end{document}</tex-math></alternatives></inline-formula> (hence low with probability <inline-formula><alternatives><mml:math id="inf277"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft277">\begin{document}$1- p_{high}^{k}$\end{document}</tex-math></alternatives></inline-formula>). The values of <inline-formula><alternatives><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft278">\begin{document}$p^{k} $\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf279"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft279">\begin{document}$p_{high}^{k}$\end{document}</tex-math></alternatives></inline-formula> are independently chosen from the set {39/40, 35/40, 25/40, 15/40, 5/40, 1/40}. The stimulus strength for the location input at trial <inline-formula><alternatives><mml:math id="inf280"><mml:mi>k</mml:mi></mml:math><tex-math id="inft280">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> is defined as <inline-formula><alternatives><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft281">\begin{document}$2\times p^{k} - 1$\end{document}</tex-math></alternatives></inline-formula>, and for the frequency input, it is defined as <inline-formula><alternatives><mml:math id="inf282"><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft282">\begin{document}$2\times p_{high}^{k}- 1$\end{document}</tex-math></alternatives></inline-formula>. The input <inline-formula><alternatives><mml:math id="inf283"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft283">\begin{document}$u_{1}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> represents location evidence, calculated as 0.1 times the difference between the number of right pulses and the number of left pulses (#right-#left) at time step <inline-formula><alternatives><mml:math id="inf284"><mml:mi>t</mml:mi></mml:math><tex-math id="inft284">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>. The input <inline-formula><alternatives><mml:math id="inf285"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft285">\begin{document}$u_{2}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> represents frequency evidence, calculated as 0.1 times the difference between the number of high-frequency pulses and the number of low-frequency pulses (#high-#low) at time step <inline-formula><alternatives><mml:math id="inf286"><mml:mi>t</mml:mi></mml:math><tex-math id="inft286">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>.</p><p>The context is randomly chosen to be either the location context or the frequency context. In the location context, <inline-formula><alternatives><mml:math id="inf287"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft287">\begin{document}$u_{1}^{ctx}=1$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf288"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft288">\begin{document}$u_{2}^{ctx}=0$\end{document}</tex-math></alternatives></inline-formula> throughout the entire period. The target output <inline-formula><alternatives><mml:math id="inf289"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft289">\begin{document}$z_{k}$\end{document}</tex-math></alternatives></inline-formula> is defined as the sign of the location stimulus strength (1 if <inline-formula><alternatives><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft290">\begin{document}$p^{k} \gt 0.5$\end{document}</tex-math></alternatives></inline-formula>, otherwise –1). Thus, in this context, the target output <inline-formula><alternatives><mml:math id="inf291"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft291">\begin{document}$z_{k}$\end{document}</tex-math></alternatives></inline-formula> is independent of the frequency stimulus input. Conversely, in the frequency context, <inline-formula><alternatives><mml:math id="inf292"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft292">\begin{document}$u_{1}^{ctx}=0$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf293"><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft293">\begin{document}$u_{2}^{ctx}=1$\end{document}</tex-math></alternatives></inline-formula>. The target output <inline-formula><alternatives><mml:math id="inf294"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft294">\begin{document}$z_{k}$\end{document}</tex-math></alternatives></inline-formula> is defined as the sign of the frequency stimulus strength (1 if <inline-formula><alternatives><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft295">\begin{document}$p_{high}^{k}\gt 0.5$\end{document}</tex-math></alternatives></inline-formula>, otherwise –1). Thus, in this context, the target output is independent of the location stimulus input.</p></sec><sec id="s4-3"><title>Linearized dynamical system analysis (<xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig7">Figure 7</xref>)</title><p>To uncover the computational mechanism enabling each RNN to perform context-dependent evidence accumulation, we utilize linearized dynamical system analysis to ‘open the black box’ (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>). The dynamical evolution of an RNN in context <inline-formula><alternatives><mml:math id="inf296"><mml:mi>c</mml:mi></mml:math><tex-math id="inft296">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> is given by:<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>J</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  \tau \frac{dx}{dt}=- x+Jr+I_{1}u_{1}+I_{2}u_{2}+I_{c}^{ctx},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf297"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft297">\begin{document}$r=\phi \left (x\right)$\end{document}</tex-math></alternatives></inline-formula> is the neuron activity. First, we identify the slow point of each RNN in each context using an optimization method (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>). Let <inline-formula><alternatives><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft298">\begin{document}$x$\end{document}</tex-math></alternatives></inline-formula> be the discovered slow point, i.e., <inline-formula><alternatives><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≈</mml:mo><mml:mi>J</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft299">\begin{document}$x\approx Jr^{*} +I_{c}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft300">\begin{document}$r=\phi (x^{*} )$\end{document}</tex-math></alternatives></inline-formula>. We define a diagonal matrix <inline-formula><alternatives><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft301">\begin{document}$G=\text{diag}\left (\phi ^{'}\left (x^{*}\right)\right)$\end{document}</tex-math></alternatives></inline-formula>, with the <inline-formula><alternatives><mml:math id="inf302"><mml:mi>i</mml:mi></mml:math><tex-math id="inft302">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th diagonal element representing the sensitivity of the <inline-formula><alternatives><mml:math id="inf303"><mml:mi>i</mml:mi></mml:math><tex-math id="inft303">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron at the slow point. Near the slow point, we have <inline-formula><alternatives><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>≈</mml:mo><mml:mi>G</mml:mi><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft304">\begin{document}$\Delta r=r- r^*\approx G\phi '\left (\Delta x\right)$\end{document}</tex-math></alternatives></inline-formula>. Then, we can derive:<disp-formula id="equ9"><label>(9)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:mi>τ</mml:mi><mml:mi>G</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mi>J</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  \tau \frac{d\Delta r}{dt}\approx \tau G\frac{d\Delta x}{dt}=- \Delta r+GJ\Delta r+GI_{1}u_{1}+GI_{2}u_{2}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Thus, the dynamics of neuron activity around the slow point can be approximated by a linear dynamical system:<disp-formula id="equ10"><label>(10)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle  \tau \frac{d\Delta r}{dt}=M\Delta r+\overset{\sim }{I}_{1}u_{1}+\overset{\sim }{I}_{2}u_{2},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ11"><label>(11)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mi>J</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle  M=- E+GJ,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf305"><mml:mi>E</mml:mi></mml:math><tex-math id="inft305">\begin{document}$E$\end{document}</tex-math></alternatives></inline-formula> is the identity matrix, <inline-formula><alternatives><mml:math id="inf306"><mml:mi>M</mml:mi></mml:math><tex-math id="inft306">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> denotes the state transition matrix of the dynamical system, and <inline-formula><alternatives><mml:math id="inf307"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft307">\begin{document}$\overset{\sim }{I}_{r}=GI_{r},r=1,2$\end{document}</tex-math></alternatives></inline-formula> are stimulus <italic>input representation directions</italic>. Similar to previous work (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>), we find that for every network, the linear dynamical system near the slow point in each context is roughly a linear attractor. Specifically, the transition matrix <inline-formula><alternatives><mml:math id="inf308"><mml:mi>M</mml:mi></mml:math><tex-math id="inft308">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> has a single eigenvalue close to zero, while all other eigenvalues have negative real parts. The right eigenvector of <inline-formula><alternatives><mml:math id="inf309"><mml:mi>M</mml:mi></mml:math><tex-math id="inft309">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> associated with the eigenvalue close to zero defines the stable direction of the dynamical system, forming the line attractor <inline-formula><alternatives><mml:math id="inf310"><mml:mi>ρ</mml:mi></mml:math><tex-math id="inft310">\begin{document}$\rho $\end{document}</tex-math></alternatives></inline-formula>(unit norm). The left eigenvector of <inline-formula><alternatives><mml:math id="inf311"><mml:mi>M</mml:mi></mml:math><tex-math id="inft311">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> associated with that eigenvalue defines the direction of the selection vector <inline-formula><alternatives><mml:math id="inf312"><mml:mi>s</mml:mi></mml:math><tex-math id="inft312">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>. The norm of selection vector <inline-formula><alternatives><mml:math id="inf313"><mml:mi>s</mml:mi></mml:math><tex-math id="inft313">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> is chosen such that <inline-formula><alternatives><mml:math id="inf314"><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft314">\begin{document}$s\cdot \rho =1$\end{document}</tex-math></alternatives></inline-formula>. Previous work (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>) has shown that a perturbation <inline-formula><alternatives><mml:math id="inf315"><mml:mi>Δ</mml:mi><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft315">\begin{document}$\Delta r_{0}$\end{document}</tex-math></alternatives></inline-formula> from the line attractor will eventually converge to the line attractor, with the distance from the starting point being <inline-formula><alternatives><mml:math id="inf316"><mml:mi>s</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft316">\begin{document}$s\cdot \Delta r_{0}$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Based on linearized dynamical systems analysis, Pagan et al. recently defined <bold>input modulation</bold> and <bold>selection vector modulation</bold> (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>) as:<disp-formula id="equ12"><label>(12)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>G</mml:mi><mml:mi>I</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle  mod_{inp}=\Delta GI\cdot \bar{s},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ13"><label>(13)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>G</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle  mod_{sel}=\bar{GI}\cdot \Delta s.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Specifically, the input modulation and selection vector modulation for stimulus input 1 are defined as <inline-formula><alternatives><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft317">\begin{document}$(s^{ctx_{1}}+s^{ctx_{2}})/2\cdot \left (\overset{\sim }{I}_{1}^{ctx_{1}}- \overset{\sim }{I}_{1}^{ctx_{2}}\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf318"><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math><tex-math id="inft318">\begin{document}$\left (s^{ctx_{1}}- s^{ctx_{2}}\right)\cdot \left (\overset{\sim }{I}_{1}^{ctx_{1}}+\overset{\sim }{I}_{1}^{ctx_{2}}\right)/2$\end{document}</tex-math></alternatives></inline-formula>, respectively. Similarly, the input modulation and selection vector modulation for stimulus input 2 are defined as <inline-formula><alternatives><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft319">\begin{document}$(s^{ctx_{1}}+s^{ctx_{2}})/2\cdot \left (\overset{\sim }{I}_{2}^{ctx_{2}}- \overset{\sim }{I}_{2}^{ctx_{1}}\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf320"><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math><tex-math id="inft320">\begin{document}$\left (s^{ctx_{2}}- s^{ctx_{1}}\right)\cdot \left (\overset{\sim }{I}_{2}^{ctx_{1}}+\overset{\sim }{I}_{2}^{ctx_{2}}\right)/2$\end{document}</tex-math></alternatives></inline-formula>, respectively. Proportion for selection vector modulation (<xref ref-type="fig" rid="fig7">Figure 7D and G</xref>) is defined as <inline-formula><alternatives><mml:math id="inf321"><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><tex-math id="inft321">\begin{document}$\frac{mod_{sel}}{mod_{inp}+mod_{sel}}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-4"><title>Training of rank-1 RNNs using backpropagation (<xref ref-type="fig" rid="fig2">Figure 2</xref>)</title><p>The rank-1 RNNs in <xref ref-type="fig" rid="fig2">Figure 2</xref> are trained using backpropagation-through-time with the PyTorch framework. These networks are trained to minimize a loss function defined as:<disp-formula id="equ14"><label>(14)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle  L=\sum _{k,t}M_{t} (\hat{Z}_{k, t}-Z_{k} )+L_{reg} .$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf322"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft322">\begin{document}$z_{k}$\end{document}</tex-math></alternatives></inline-formula> is the target output, <inline-formula><alternatives><mml:math id="inf323"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft323">\begin{document}$\overset{\hat }{z}_{k,t}$\end{document}</tex-math></alternatives></inline-formula> is the network output, and the indices <inline-formula><alternatives><mml:math id="inf324"><mml:mi>t</mml:mi></mml:math><tex-math id="inft324">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf325"><mml:mi>k</mml:mi></mml:math><tex-math id="inft325">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> represent time and trial, respectively. <inline-formula><alternatives><mml:math id="inf326"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft326">\begin{document}$M_{t}$\end{document}</tex-math></alternatives></inline-formula> is a temporal mask with value {0, 1}, where <inline-formula><alternatives><mml:math id="inf327"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft327">\begin{document}$M_{t}$\end{document}</tex-math></alternatives></inline-formula> is 1 only during the decision period. <inline-formula><alternatives><mml:math id="inf328"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft328">\begin{document}$L_{reg}$\end{document}</tex-math></alternatives></inline-formula> is the L2 regularization loss. For full-rank RNNs (<xref ref-type="fig" rid="fig7">Figure 7</xref>), <inline-formula><alternatives><mml:math id="inf329"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math><tex-math id="inft329">\begin{document}$L_{reg}=w_{reg}\sum _{ij}J_{ij}^{2}$\end{document}</tex-math></alternatives></inline-formula> and for low-rank RNNs, <inline-formula><alternatives><mml:math id="inf330"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="inft330">\begin{document}$L_{reg}=w_{reg}\sum _{r}\left (\left \| m_{r}\right \| _{2}^{2}+\left \| n_{r}\right \| _{2}^{2}\right)$\end{document}</tex-math></alternatives></inline-formula>. The loss function is minimized by computing gradients with respect to all trainable parameters. We use the Adam optimizer in PyTorch with the decay rates for the first and second moments set to 0.9 and 0.99, respectively, and a learning rate of <inline-formula><alternatives><mml:math id="inf331"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft331">\begin{document}$10^{- 3}$\end{document}</tex-math></alternatives></inline-formula>. Each RNN is trained for 5000 steps with a batch size of 256 trials.</p><p>For <xref ref-type="fig" rid="fig2">Figure 2</xref>, we trained 100 RNNs of <inline-formula><alternatives><mml:math id="inf332"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:math><tex-math id="inft332">\begin{document}$N=512$\end{document}</tex-math></alternatives></inline-formula> neurons with the same training hyperparameters. The rank of the connectivity matrix is constrained to be rank-1, represented as <inline-formula><alternatives><mml:math id="inf333"><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft333">\begin{document}$J=\frac{1}{N}m_{dv}n_{dv}^{T}$\end{document}</tex-math></alternatives></inline-formula>. We trained the elements of the input vectors <inline-formula><alternatives><mml:math id="inf334"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft334">\begin{document}$I_{1},I_{2},I_{1}^{ctx},I_{2}^{ctx}$\end{document}</tex-math></alternatives></inline-formula>, connectivity vector <inline-formula><alternatives><mml:math id="inf335"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft335">\begin{document}$m_{dv},n_{dv}$\end{document}</tex-math></alternatives></inline-formula>, and the readout vector <inline-formula><alternatives><mml:math id="inf336"><mml:mi>w</mml:mi></mml:math><tex-math id="inft336">\begin{document}$w$\end{document}</tex-math></alternatives></inline-formula>. All trainable parameters were initialized with random independent Gaussian weights with a mean of 0 and a variance of <inline-formula><alternatives><mml:math id="inf337"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math><tex-math id="inft337">\begin{document}$1/N^{2}$\end{document}</tex-math></alternatives></inline-formula>. The regularization coefficient <inline-formula><alternatives><mml:math id="inf338"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft338">\begin{document}$w_{reg}$\end{document}</tex-math></alternatives></inline-formula> is set to <inline-formula><alternatives><mml:math id="inf339"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft339">\begin{document}$10^{- 4}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-5"><title>Trial-averaged analyses (<xref ref-type="fig" rid="fig2">Figures 2F</xref> and <xref ref-type="fig" rid="fig3">3F</xref>)</title><p>For the trial-averaged analyses shown in <xref ref-type="fig" rid="fig2">Figures 2f</xref> and <xref ref-type="fig" rid="fig3">3f</xref>, we followed a procedure similar to <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>. Specifically, at each time point <inline-formula><alternatives><mml:math id="inf340"><mml:mi>t</mml:mi></mml:math><tex-math id="inft340">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> and for each neuron <inline-formula><alternatives><mml:math id="inf341"><mml:mi>i</mml:mi></mml:math><tex-math id="inft341">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>, we fit the following linear regression model to characterize how different task variables contribute to that neuron’s activity:<disp-formula id="equ15"><label>(15)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle  r_{i,t}\left (k\right)=\beta _{choice;i,t}choice\left (k\right)+\beta _{inp_{1};i,t}u_{1,k}+\beta _{inp_{2};i,t}u_{2,k}+\beta _{context;i,t}context\left (k\right)+\beta _{0;i,t}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf342"><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft342">\begin{document}$choice\left (k\right)$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf343"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft343">\begin{document}$u_{1,k}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf344"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft344">\begin{document}$u_{2,k}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf345"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft345">\begin{document}$context\left (k\right)$\end{document}</tex-math></alternatives></inline-formula> represent the values of the corresponding variables on trial k. Next, for each regressor (choice, input 1, input 2, context), we pooled the resulting regression coefficients across neurons to get the time-vary regression vectors <inline-formula><alternatives><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft346">\begin{document}$\boldsymbol\beta _{choice;t},\boldsymbol\beta _{inp_{1};t},\boldsymbol\beta _{inp_{2};t,}\boldsymbol\beta _{context;t}$\end{document}</tex-math></alternatives></inline-formula>. For regressor <inline-formula><alternatives><mml:math id="inf347"><mml:mi>v</mml:mi></mml:math><tex-math id="inft347">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula>, we identified the time <inline-formula><alternatives><mml:math id="inf348"><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft348">\begin{document}$t_{v}^{max}$\end{document}</tex-math></alternatives></inline-formula> when the regression vector <inline-formula><alternatives><mml:math id="inf349"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft349">\begin{document}$\beta _{v;t}$\end{document}</tex-math></alternatives></inline-formula> has maximum norm, and got the time-independent regression vectors:<disp-formula id="equ16"><label>(16)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle  t_{v}^{max}=\text{argmax}_{t}\left \| \boldsymbol\beta _{v;t}\right \| _{2},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ17"><label>(17)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle  \beta _{v}=\beta _{v;t_{v}^{max}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Next, we assembled these vectors into a matrix <inline-formula><alternatives><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft350">\begin{document}$B=\left [\boldsymbol\beta _{choice}\boldsymbol\beta _{inp_{1}}\boldsymbol\beta _{inp_{2}}\boldsymbol\beta _{context}\right ]$\end{document}</tex-math></alternatives></inline-formula> and used QR decomposition to get the orthogonalized regression basis <inline-formula><alternatives><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊥</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft351">\begin{document}$\left [\boldsymbol\beta _{choice}^{\bot },\boldsymbol\beta _{inp_{1}}^{\bot },\boldsymbol\beta _{inp_{2}}^{\bot },\boldsymbol\beta _{context}^{\bot }\right ]$\end{document}</tex-math></alternatives></inline-formula>. Finally, we averaged neuronal activity across trials sharing the same condition (choice, context, input 1), and then projected this average activity onto the choice and input 1 axes. This process resulted in the trial-averaged population dynamics illustrated in <xref ref-type="fig" rid="fig2">Figures 2F</xref> and <xref ref-type="fig" rid="fig3">3F</xref>.</p></sec><sec id="s4-6"><title>Proof of no selection vector modulation in rank-1 RNNs (<xref ref-type="fig" rid="fig2">Figure 2</xref>)</title><p>The transition matrix of neuron activity in the rank-1 RNN is given by:<disp-formula id="equ18"><label>(18)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>G</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle  M=- E+\frac{1}{N}G\boldsymbol m_{dv}\boldsymbol n_{dv}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Multiplying <inline-formula><alternatives><mml:math id="inf352"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft352">\begin{document}$M^{T}$\end{document}</tex-math></alternatives></inline-formula> on the right by <inline-formula><alternatives><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft353">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula>, we obtain<disp-formula id="equ19"><label>(19)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle  M^{T}\boldsymbol n_{dv}=- \boldsymbol n_{dv}+\left \langle \overset{\sim }{\boldsymbol m}_{dv},\boldsymbol n_{dv}\right \rangle \boldsymbol n_{dv}\approx 0,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the <inline-formula><alternatives><mml:math id="inf354"><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow></mml:mfenced></mml:math><tex-math id="inft354">\begin{document}$\left \langle \cdot ,\cdot \right \rangle $\end{document}</tex-math></alternatives></inline-formula> symbol is defined as <inline-formula><alternatives><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft355">\begin{document}$\left \langle \boldsymbol a,\boldsymbol b\right \rangle =\frac{1}{N}\sum _{i=1}^{N}a_{i}b_{i}$\end{document}</tex-math></alternatives></inline-formula> for two vectors of length-<inline-formula><alternatives><mml:math id="inf356"><mml:mi>N</mml:mi></mml:math><tex-math id="inft356">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula>. The requirement for the linear attractor approximation, <inline-formula><alternatives><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft357">\begin{document}$\left \langle \overset{\sim }{\boldsymbol m}_{dv},\boldsymbol n_{dv}\right \rangle \approx 1$\end{document}</tex-math></alternatives></inline-formula>, is met by all our trained and handcrafted RNNs (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). This demonstrates that <inline-formula><alternatives><mml:math id="inf358"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft358">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula> is the left eigenvector of the transition matrix, and hence <inline-formula><alternatives><mml:math id="inf359"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft359">\begin{document}$\frac{1}{N}\left \| \overset{\sim }{m}_{dv}\right \| _{2}n_{dv}$\end{document}</tex-math></alternatives></inline-formula> is the selection vector in each context. Therefore, the direction of the selection vector is invariant across different contexts for the rank-1 model, indicating no selection vector modulation and this is consistent with the training results shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></sec><sec id="s4-7"><title>Handcrafting rank-3 RNNs with pure selection vector modulation (<xref ref-type="fig" rid="fig3">Figure 3</xref>)</title><p>First, we provide the implementation details of the rank-3 RNNs used in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The network consists of 30,000 neurons divided into three populations, each with 10,000 neurons. The first population (neurons 1–10,000) receives stimulus input, accounting for both the information flow from the stimulus input to the intermediate variables and the recurrent connection of the decision variable. The second (neurons 10,001–20,000) and third populations (neurons 20,001–30,000) handle the information flow from the intermediate variables to the decision variable and are modulated by the context signal. To achieve this, we generate three Gaussian random matrices <inline-formula><alternatives><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft360">\begin{document}$(M^{\left (1\right)},M^{\left (2\right)},M^{\left (3\right)})$\end{document}</tex-math></alternatives></inline-formula> of shape 10,000×3. Let <inline-formula><alternatives><mml:math id="inf361"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math><tex-math id="inft361">\begin{document}$M_{\colon ,r}^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula> denote the <inline-formula><alternatives><mml:math id="inf362"><mml:mi>r</mml:mi></mml:math><tex-math id="inft362">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>-th column of matrix <inline-formula><alternatives><mml:math id="inf363"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math><tex-math id="inft363">\begin{document}$M^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula>. The stimulus input <inline-formula><alternatives><mml:math id="inf364"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft364">\begin{document}$I_{1}$\end{document}</tex-math></alternatives></inline-formula> is given by the concatenation of three length-10,000 vectors <inline-formula><alternatives><mml:math id="inf365"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft365">\begin{document}$\left [M_{\colon ,1}^{\left (1\right)};0;0\right ]$\end{document}</tex-math></alternatives></inline-formula>, 0 where  denotes a length-10,000 zero vector. The stimulus input <inline-formula><alternatives><mml:math id="inf366"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft366">\begin{document}$I_{2}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf367"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft367">\begin{document}$\left [M_{\colon ,2}^{\left (1\right)};0;0\right ]$\end{document}</tex-math></alternatives></inline-formula>. The context input <inline-formula><alternatives><mml:math id="inf368"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft368">\begin{document}$I_{1}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf369"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft369">\begin{document}$\left [0;M_{\colon ,1}^{\left (2\right)};0\right ]$\end{document}</tex-math></alternatives></inline-formula>. The context input <inline-formula><alternatives><mml:math id="inf370"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft370">\begin{document}$I_{2}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf371"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft371">\begin{document}$\left [0;0;M_{\colon ,1}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>. The connectivity vectors <inline-formula><alternatives><mml:math id="inf372"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft372">\begin{document}$m_{iv1},m_{iv2}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf373"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft373">\begin{document}$m_{dv}$\end{document}</tex-math></alternatives></inline-formula> are given by <inline-formula><alternatives><mml:math id="inf374"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft374">\begin{document}$\left [0;M_{\colon ,2}^{\left (2\right)};M_{\colon ,2}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf375"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft375">\begin{document}$\left [0;M_{\colon ,3}^{\left (2\right)};M_{\colon ,3}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf376"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft376">\begin{document}$\left [M_{\colon ,3}^{\left (1\right)};0;0\right ]$\end{document}</tex-math></alternatives></inline-formula>, respectively. The input-selection vectors <inline-formula><alternatives><mml:math id="inf377"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><tex-math id="inft377">\begin{document}$n_{iv1},n_{iv2},$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf378"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft378">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula> are given by <inline-formula><alternatives><mml:math id="inf379"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>10</mml:mn><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft379">\begin{document}$\left [10M_{\colon ,1}^{\left (1\right)};0;0\right ]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf380"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>10</mml:mn><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft380">\begin{document}$\left [10M_{\colon ,2}^{\left (1\right)};0;0\right ]$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf381"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>3</mml:mn><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft381">\begin{document}$\left [3M_{\colon ,3}^{\left (1\right)};- gM_{\colon ,2}^{\left (2\right)}+M_{\colon ,3}^{\left (2\right)};M_{\colon ,2}^{\left (3\right)}- gM_{\colon ,3}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>, respectively, where <inline-formula><alternatives><mml:math id="inf382"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft382">\begin{document}$g=\int _{- \infty }^{+\infty }\frac{1}{\sqrt{2\pi }}e^{- x^{2}/2}\phi ^{'}\left (x\right)dx$\end{document}</tex-math></alternatives></inline-formula> represents the average gain of the second population in context 1 or the third population in context 2. The readout vector <inline-formula><alternatives><mml:math id="inf383"><mml:mi>w</mml:mi></mml:math><tex-math id="inft383">\begin{document}$w$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf384"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mn>4</mml:mn><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft384">\begin{document}$\left [4M_{\colon ,3}^{\left (1\right)};0;0\right ]$\end{document}</tex-math></alternatives></inline-formula>. We generate 100 RNNs based on this method to ensure that the conclusions in <xref ref-type="fig" rid="fig3">Figure 3</xref> do not depend on the specific realization of random matrices. Linearized dynamical system analysis reveals that all these RNNs perform flexible computation through pure selection vector modulation. Please see the section ‘<bold>The construction of rank-3 RNN models</bold>’ for a mean-field-theory-based understanding.</p></sec><sec id="s4-8"><title>Pathway-based information flow graph analysis of low-rank RNNs (<xref ref-type="fig" rid="fig4">Figure 4</xref>)</title><p>The dynamical evolution equation for low-rank RNN with <inline-formula><alternatives><mml:math id="inf385"><mml:mi>R</mml:mi></mml:math><tex-math id="inft385">\begin{document}$R$\end{document}</tex-math></alternatives></inline-formula> ranks and <inline-formula><alternatives><mml:math id="inf386"><mml:mi>S</mml:mi></mml:math><tex-math id="inft386">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> input channels in context <italic>c</italic> is given by<disp-formula id="equ20"><label>(20)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle  \frac{dx}{dt}=- x+\frac{1}{N}\sum _{r=1}^{R}m_{r}n_{r}^{T}\phi \left (x\right)+\sum _{s=1}^{S}I_{s}u_{s}+I_{c}^{ctx}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Assuming <inline-formula><alternatives><mml:math id="inf387"><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft387">\begin{document}$x\left (0\right)=I_{c}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> at <inline-formula><alternatives><mml:math id="inf388"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft388">\begin{document}$t=0$\end{document}</tex-math></alternatives></inline-formula>, the dynamics of <inline-formula><alternatives><mml:math id="inf389"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math><tex-math id="inft389">\begin{document}$x\left (t\right)- x\left (0\right)$\end{document}</tex-math></alternatives></inline-formula> are always constrained in the subspace spanned by <inline-formula><alternatives><mml:math id="inf390"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft390">\begin{document}$\left \{m_{r},r=1,\ldots ,R\right \}$\end{document}</tex-math></alternatives></inline-formula> and {<inline-formula><alternatives><mml:math id="inf391"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>}</mml:mo></mml:math><tex-math id="inft391">\begin{document}$I_{s},s=1,\ldots ,S\right \}$\end{document}</tex-math></alternatives></inline-formula>. Therefore, <inline-formula><alternatives><mml:math id="inf392"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft392">\begin{document}$x\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> can be expressed as a linear combination of these vectors: <inline-formula><alternatives><mml:math id="inf393"><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math><tex-math id="inft393">\begin{document}$x\left (t\right)=I_{c}^{ctx}+\sum _{r=1}^{R}k_{r}\left (t\right)m_{r}+\sum _{s=1}^{S}k_{p_{s}}\left (t\right)I_{s},$\end{document}</tex-math></alternatives></inline-formula> leading to the following evolving dynamics of task variables:<disp-formula id="equ21"><label>(21)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle  \tau \frac{dk_{p_{s}}\left (t\right)}{dt}=- k_{p_{s}}\left (t\right)+u_{s}\left (t\right),$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ22"><label>(22)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle  \tau \frac{dk_{r}\left (t\right)}{dt}=- k_{r}\left (t\right)+\frac{1}{N}n_{r}^{T}\phi \left (I_{c}^{ctx}+\sum _{j=1}^{R}k_{j}\left (t\right)m_{j}+\sum _{s=1}^{S}k_{p_{s}}\left (t\right)I_{s}\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf394"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft394">\begin{document}$k_{p_{s}}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> denotes activation along the <inline-formula><alternatives><mml:math id="inf395"><mml:mi>s</mml:mi></mml:math><tex-math id="inft395">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>-th input vector, termed the input task variable and <inline-formula><alternatives><mml:math id="inf396"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft396">\begin{document}$k_{r}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> denotes activation along the <inline-formula><alternatives><mml:math id="inf397"><mml:mi>j</mml:mi></mml:math><tex-math id="inft397">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula>-th output vector, termed the internal task variable.</p></sec><sec id="s4-9"><title>The rank-1 RNN case</title><p>Therefore, for rank-1 RNNs, the latent dynamics of decision variable (internal task variable associated with <inline-formula><alternatives><mml:math id="inf398"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft398">\begin{document}$m_{dv}$\end{document}</tex-math></alternatives></inline-formula>) in context <inline-formula><alternatives><mml:math id="inf399"><mml:mi>c</mml:mi></mml:math><tex-math id="inft399">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> is given by<disp-formula id="equ23"><label>(23)</label><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}\left (t\right)}{dt}=- k_{dv}\left (t\right)+\frac{1}{N}n_{dv}^{T}\phi \left (I_{c}^{ctx}+k_{dv}\left (t\right)m_{dv}+\sum _{s=1}^{2}k_{p_{s}}\left (t\right)I_{s}\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>Similar to the linearized dynamical systems analysis introduced earlier, we linearized <xref ref-type="disp-formula" rid="equ15">Equation 15</xref> around <inline-formula><alternatives><mml:math id="inf400"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft400">\begin{document}$I_{c}^{ctx}$\end{document}</tex-math></alternatives></inline-formula>, obtaining the following linearized equation:<disp-formula id="equ24"><label>(24)</label><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mover><mml:mi>m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}\left (t\right)}{dt}=- k_{dv}\left (t\right)+\frac{1}{N}\left (n_{dv}^{T}\phi \left (I_{c}^{ctx}\right)+n_{dv}^{T}\overset{\sim }{m}_{dv}^{ctx_{c}}k_{dv}\left (t\right)+\sum _{s=1}^{2}n_{dv}^{T}\overset{\sim }{I}_{s}^{ctx_{c}}k_{p_{s}}\left (t\right)\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf401"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft401">\begin{document}$\overset{\sim }{m}_{dv}^{ctx_{c}}=G_{c}m_{dv}$\end{document}</tex-math></alternatives></inline-formula> (termed as the decision variable representation direction) and <inline-formula><alternatives><mml:math id="inf402"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft402">\begin{document}$\overset{\sim }{I}_{s}^{ctx_{c}}=G_{c}I_{s}$\end{document}</tex-math></alternatives></inline-formula> (termed as the input representation direction), with <inline-formula><alternatives><mml:math id="inf403"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft403">\begin{document}$G_{c}$\end{document}</tex-math></alternatives></inline-formula> equal to <inline-formula><alternatives><mml:math id="inf404"><mml:mtext>diag</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><tex-math id="inft404">\begin{document}$\text{diag}\left (\phi ^{'}\left (I_{c}^{ctx}\right)\right)$\end{document}</tex-math></alternatives></inline-formula>. By denoting <inline-formula><alternatives><mml:math id="inf405"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math><tex-math id="inft405">\begin{document}$\frac{1}{N}n_{dv}^{T}\overset{\sim }{m}_{dv}^{ctx_{c}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf406"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math><tex-math id="inft406">\begin{document}$\frac{1}{N}n_{dv}^{T}\overset{\sim }{I}_{s}^{ctx_{c}}$\end{document}</tex-math></alternatives></inline-formula> as <inline-formula><alternatives><mml:math id="inf407"><mml:msubsup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math><tex-math id="inft407">\begin{document}$E_{dv\rightarrow dv}^{ctx_{c}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf408"><mml:msubsup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:math><tex-math id="inft408">\begin{document}$E_{p_{s}\rightarrow dv}^{ctx_{c}}$\end{document}</tex-math></alternatives></inline-formula>, respectively, together with the fact that <inline-formula><alternatives><mml:math id="inf409"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><tex-math id="inft409">\begin{document}$\frac{1}{N}n_{dv}^{T}\phi \left (I_{c}\right)$\end{document}</tex-math></alternatives></inline-formula> is close to zero for all trained rank-1 RNNs, we obtain<disp-formula id="equ25"><label>(25)</label><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}\left (t\right)}{dt}=- k_{dv}\left (t\right)+E_{dv\rightarrow dv}\kappa _{dv}\left (t\right)+E_{p_{1}\rightarrow dv}^{ctx_{c}}k_{p_{1}}\left (t\right)+E_{p_{2}\rightarrow dv}^{ctx_{c}}k_{p_{2}}\left (t\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>which is <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> in the main text.</p></sec><sec id="s4-10"><title>The rank-3 RNN case</title><p>Using a similar method, we can uncover the latent dynamics of rank-3 RNNs, as shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Note that the rank-3 RNN in <xref ref-type="fig" rid="fig3">Figure 3</xref> is a special case of this more general form. The latent dynamics for internal task variables in context <inline-formula><alternatives><mml:math id="inf410"><mml:mi>c</mml:mi></mml:math><tex-math id="inft410">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> can be written as:<disp-formula id="equ26"><label>(26)</label><alternatives><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t26">\begin{document}$$\displaystyle \tau \frac{dk_{iv_{s}}}{dt}=- k_{iv_{s}}\left (t\right)+\frac{1}{N}n_{iv_{s}}^{T}\phi \left (I_{c}^{ctx}+k_{dv}m_{dv}+\sum _{s=1}^{2}k_{iv_{s}}m_{iv_{s}}+\sum _{s=1}^{2}k_{inp_{s}}I_{s}\right),$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ27"><label>(27)</label><alternatives><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t27">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}}{dt}=- k_{dv}\left (t\right)+\frac{1}{N}n_{dv}^{T}\phi \left (I_{c}^{ctx}+k_{dv}m_{dv}+\sum _{s=1}^{2}k_{iv_{s}}m_{iv_{s}}+\sum _{s=1}^{2}k_{inp_{s}}I_{s}\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>By applying the same first-order Taylor expansion, we obtain the following equations:<disp-formula id="equ28"><label>(28)</label><alternatives><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi>m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mover><mml:mi>m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t28">\begin{document}$$\displaystyle  \tau \frac{dk_{iv_{s}}}{dt}=- k_{iv_{s}}+\frac{1}{N}n_{iv_{s}}^{T}\left (\overset{\sim }{m}_{dv}^{ctx_{c}}k_{dv}+\sum _{s=1}^{2}\overset{\sim }{m}_{iv_{s}}^{ctx_{c}}k_{iv_{s}}+\sum _{s^{'}=1}^{2}\overset{\sim }{I}_{s^{'}}^{ctx_{c}}k_{inp_{s^{'}}}\right),$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ29"><label>(29)</label><alternatives><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi>m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mover><mml:mi>m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mover><mml:mi>I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t29">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}}{dt}=- k_{dv}+\frac{1}{N}n_{dv}^{T}\left (\overset{\sim }{m}_{dv}^{ctx_{c}}k_{dv}+\sum _{s=1}^{2}\overset{\sim }{m}_{iv_{s}}^{ctx_{c}}k_{iv_{s}}+\sum _{s=1}^{2}\overset{\sim }{I}_{s}^{ctx_{c}}k_{inp_{s}}\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>We consider the case in which the intermediate variables (<inline-formula><alternatives><mml:math id="inf411"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft411">\begin{document}$k_{iv_{s}},s=1,2$\end{document}</tex-math></alternatives></inline-formula>, internal task variables associated with <inline-formula><alternatives><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft412">\begin{document}$\boldsymbol m_{iv_{s}},s=1,2$\end{document}</tex-math></alternatives></inline-formula>, respectively) only receive information from the corresponding stimulus input, and the effective coupling of the recurrent connection is 1 in both contexts. Specifically, we assume:<disp-formula id="equ30"><label>(30)</label><alternatives><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t30">\begin{document}$$\displaystyle  \left \langle \overset{\sim }{\boldsymbol I}_{iv_{s^{'}}}^{ctx_{c}},\boldsymbol n_{iv_{s}}\right \rangle =0,s\neq s^{'},c=1,2,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ31"><label>(31)</label><alternatives><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t31">\begin{document}$$\displaystyle  \left \langle \overset{\sim }{\boldsymbol m}_{iv_{s^{'}}}^{ctx_{c}},\boldsymbol n_{iv_{s}}\right \rangle =0,s,s^{'}\in \left \{1,2\right \},c=1,2,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ32"><label>(32)</label><alternatives><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t32">\begin{document}$$\displaystyle  \left \langle \overset{\sim }{\boldsymbol m}_{dv}^{ctx_{c}},\boldsymbol n_{dv}\right \rangle =1,c=1,2.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Our construction methods for rank-3 RNNs in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig5">5</xref> guarantee these conditions when the network is large enough (for example, <inline-formula><alternatives><mml:math id="inf413"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>30,000</mml:mn></mml:math><tex-math id="inft413">\begin{document}$N=30,000$\end{document}</tex-math></alternatives></inline-formula> in our setting). Under these conditions, <xref ref-type="disp-formula" rid="equ20 equ21">Equations 20 and 21</xref> can be simplified to:<disp-formula id="equ33"><label>(33)</label><alternatives><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t33">\begin{document}$$\displaystyle  \tau \frac{dk_{iv_{s}}}{dt}=- k_{iv_{s}}+E_{inp_{s}\rightarrow iv_{s}}^{ctx_{c}}k_{inp_{s}},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ34"><label>(34)</label><alternatives><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t34">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}}{dt}=E_{inp_{1}\rightarrow dv}^{ctx_{c}}k_{inp_{1}}+E_{inp_{2}\rightarrow dv}^{ctx_{c}}k_{inp_{2}}+E_{iv_{1}\rightarrow dv}^{ctx_{c}}k_{iv_{1}}+E_{iv_{2}\rightarrow dv}^{ctx_{c}}k_{iv_{2}}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Suppose at <inline-formula><alternatives><mml:math id="inf414"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:math><tex-math id="inft414">\begin{document}$t=0,$\end{document}</tex-math></alternatives></inline-formula> the network receives a pulse from input 1 with size <inline-formula><alternatives><mml:math id="inf415"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft415">\begin{document}$A_{1}$\end{document}</tex-math></alternatives></inline-formula> and a pulse from input 2 with size <inline-formula><alternatives><mml:math id="inf416"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft416">\begin{document}$A_{2}$\end{document}</tex-math></alternatives></inline-formula>, which correspond to <inline-formula><alternatives><mml:math id="inf417"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft417">\begin{document}$u_{1}\left (t\right)=A_{1}\tau \delta \left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf418"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft418">\begin{document}$u_{2}\left (t\right)=A_{2}\tau \delta \left (t\right)$\end{document}</tex-math></alternatives></inline-formula>. Under this condition, the expression for <inline-formula><alternatives><mml:math id="inf419"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft419">\begin{document}$k_{dv}$\end{document}</tex-math></alternatives></inline-formula> is given by<disp-formula id="equ35"><label>(35)</label><alternatives><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mi>τ</mml:mi></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t35">\begin{document}$$\displaystyle  k_{dv}\left (t\right)=\sum _{s=1}^{2}A_{s}\left (E_{inp_{s}\rightarrow dv}^{ctx_{c}}\left (1- e^{- t/\tau }\right)+E_{inp_{s}\rightarrow iv_{s}}^{ctx_{c}}E_{iv_{s}\rightarrow dv}^{ctx_{c}}\left (1- e^{- t/\tau }- \frac{t}{\tau }e^{- t/\tau }\right)\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ27">Equation 27</xref>, as <inline-formula><alternatives><mml:math id="inf420"><mml:mi>t</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math><tex-math id="inft420">\begin{document}$t\rightarrow \infty $\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf421"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft421">\begin{document}$k_{dv}$\end{document}</tex-math></alternatives></inline-formula> will converge to <inline-formula><alternatives><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft422">\begin{document}$\sum _{s=1}^{2}A_{s}\left (E_{inp_{s}\rightarrow dv}^{ctx_{c}}+E_{inp_{s}\rightarrow iv_{s}}^{ctx_{c}}E_{iv_{s}\rightarrow dv}^{ctx_{c}}\right)$\end{document}</tex-math></alternatives></inline-formula>, providing a theoretical basis for the pathway-based information flow formula presented in <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>.</p></sec><sec id="s4-11"><title>Building rank-3 RNNs with both input and selection vector modulations (<xref ref-type="fig" rid="fig5">Figure 5</xref>)</title><sec id="s4-11-1"><title>Understanding low-rank RNNs in the mean-field limit</title><p>The dynamics of task variables in low-rank RNNs can be mathematically analyzed under the mean-field limit (<inline-formula><alternatives><mml:math id="inf423"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:math><tex-math id="inft423">\begin{document}$N\rightarrow +\infty $\end{document}</tex-math></alternatives></inline-formula>) when each neuron’s connectivity component is randomly drawn from a multivariate Gaussian mixture model (GMM) (<xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib11">Dubreuil et al., 2022</xref>). Specifically, we assume that, for the <inline-formula><alternatives><mml:math id="inf424"><mml:mi>i</mml:mi></mml:math><tex-math id="inft424">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron, the connectivity component vector <inline-formula><alternatives><mml:math id="inf425"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>}</mml:mo></mml:math><tex-math id="inft425">\begin{document}$\left \{I_{s}^{\left (i\right)},s=1,\ldots ,S,\left (I_{c}^{ctx}\right)^{\left (i\right)},c=1,2,m_{r}^{\left (i\right)},r=1,\ldots ,R,n_{r}^{\left (i\right)},r=1,\ldots ,R\right \}$\end{document}</tex-math></alternatives></inline-formula> is drawn independently from a GMM with <inline-formula><alternatives><mml:math id="inf426"><mml:mi>P</mml:mi></mml:math><tex-math id="inft426">\begin{document}$P$\end{document}</tex-math></alternatives></inline-formula> components. The weight for the <inline-formula><alternatives><mml:math id="inf427"><mml:mi>j</mml:mi></mml:math><tex-math id="inft427">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula>-th component is <inline-formula><alternatives><mml:math id="inf428"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft428">\begin{document}$\alpha _{j}$\end{document}</tex-math></alternatives></inline-formula>, and this component is modeled as a Gaussian distribution with mean zero and covariance matrix <inline-formula><alternatives><mml:math id="inf429"><mml:msup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft429">\begin{document}$\Sigma ^{\left (j\right)}$\end{document}</tex-math></alternatives></inline-formula>. Let <inline-formula><alternatives><mml:math id="inf430"><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ℑ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math><tex-math id="inft430">\begin{document}$\Sigma _{\Im }^{\left (j\right)}$\end{document}</tex-math></alternatives></inline-formula> denote the upper-left <inline-formula><alternatives><mml:math id="inf431"><mml:mfenced separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft431">\begin{document}$\left (S+R+2\right)\times \left (S+R+2\right)$\end{document}</tex-math></alternatives></inline-formula> submatrix of <inline-formula><alternatives><mml:math id="inf432"><mml:msup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft432">\begin{document}$\Sigma ^{\left (j\right)}$\end{document}</tex-math></alternatives></inline-formula>, which represents the covariance matrix of <inline-formula><alternatives><mml:math id="inf433"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>}</mml:mo></mml:math><tex-math id="inft433">\begin{document}$\left \{I_{s}^{\left (i\right)},s=1,\ldots ,S,\left (I_{c}^{ctx}\right)^{\left (i\right)},c=1,2,m_{r}^{\left (i\right)},r=1,\ldots ,R\right \}$\end{document}</tex-math></alternatives></inline-formula> within the <inline-formula><alternatives><mml:math id="inf434"><mml:mi>j</mml:mi></mml:math><tex-math id="inft434">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula>-th component. Let <inline-formula><alternatives><mml:math id="inf435"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft435">\begin{document}$\sigma _{ab}^{\left (p\right)},a,b\in \left \{I_{s},s=1,\ldots ,S,I_{c}^{ctx},c=1,2,m_{r},r=1,\ldots ,R,n_{r},r=1,\ldots ,R\right \}$\end{document}</tex-math></alternatives></inline-formula> denote the covariance of <inline-formula><alternatives><mml:math id="inf436"><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft436">\begin{document}$a^{\left (i\right)}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf437"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft437">\begin{document}$b^{\left (i\right)}$\end{document}</tex-math></alternatives></inline-formula>, where the <inline-formula><alternatives><mml:math id="inf438"><mml:mi>i</mml:mi></mml:math><tex-math id="inft438">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron belongs to the <inline-formula><alternatives><mml:math id="inf439"><mml:mi>p</mml:mi></mml:math><tex-math id="inft439">\begin{document}$p$\end{document}</tex-math></alternatives></inline-formula>-th component.</p><p>Given these assumptions, under the mean-field limit (<inline-formula><alternatives><mml:math id="inf440"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:math><tex-math id="inft440">\begin{document}$N\rightarrow +\infty $\end{document}</tex-math></alternatives></inline-formula>), <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> can be expressed as<disp-formula id="equ36"><label>(36)</label><alternatives><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>∧</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t36">\begin{document}$$\displaystyle  \begin{array}{c}  \tau \frac{dk_{r}\left (t\right)}{dt}\wedge - k_{r}\left (t\right)+\sum _{j=1}^{R}\sum _{p=1}^{P}\alpha _{p}\left \langle \phi ^{'}\right \rangle _{p}\sigma _{m_{j}n_{r}}^{\left (p\right)}k_{j}\left (t\right)\\  +\sum _{s=1}^{S}\sum _{p=1}^{P}\alpha _{p}\left \langle \phi ^{'}\right \rangle _{p}\sigma _{I_{s}n_{r}}^{\left (p\right)}k_{inp_{s}}+\sum _{p=1}^{P}\alpha _{p}\left \langle \phi ^{'}\right \rangle _{p}\sigma _{I_{c}^{ctx}n_{r}}^{\left (p\right)},\\\end{array}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ37"><label>(37)</label><alternatives><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t37">\begin{document}$$\displaystyle  \left \langle \phi ^{'}\right \rangle _{p}=\int _{- \infty }^{\infty }\frac{1}{\sqrt{2\pi }}e^{\frac{- x^{2}}{2}}\phi ^{'}\left (\Delta _{p}x\right)dx,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ38"><label>(38)</label><alternatives><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">ℑ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t38">\begin{document}$$\displaystyle  \Delta _{p}^{2}=\left [k_{inp_{1}},\ldots ,k_{inp_{S}},k_{ctx_{1}},k_{ctx_{2}},k_{1},\ldots ,k_{R}\right ]\Sigma _{\Im }^{\left (p\right)}\left [k_{inp_{1}},\ldots ,k_{inp_{S}},k_{ctx_{1}},k_{ctx_{2}},k_{1},\ldots ,k_{R}\right ]^{T},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf441"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft441">\begin{document}$k_{ctx_{i}}=1$\end{document}</tex-math></alternatives></inline-formula> if <inline-formula><alternatives><mml:math id="inf442"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:math><tex-math id="inft442">\begin{document}$i=c$\end{document}</tex-math></alternatives></inline-formula>, otherwise <inline-formula><alternatives><mml:math id="inf443"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math><tex-math id="inft443">\begin{document}$k_{ctx_{i}}=0.$\end{document}</tex-math></alternatives></inline-formula> Under the condition of small task variables (<inline-formula><alternatives><mml:math id="inf444"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:math><tex-math id="inft444">\begin{document}$k_{p_{s}},s=1,\ldots ,S$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf445"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:math><tex-math id="inft445">\begin{document}$k_{r},r=1,\ldots ,R$\end{document}</tex-math></alternatives></inline-formula>), <inline-formula><alternatives><mml:math id="inf446"><mml:msubsup><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft446">\begin{document}$\Delta _{p}^{2}$\end{document}</tex-math></alternatives></inline-formula> is approximately equal to <inline-formula><alternatives><mml:math id="inf447"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math><tex-math id="inft447">\begin{document}$\sigma _{I_{c}^{ctx},I_{c}^{ctx}}^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula> and the quantities <inline-formula><alternatives><mml:math id="inf448"><mml:msub><mml:mrow><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:math><tex-math id="inft448">\begin{document}$\left \langle \phi ^{'}\right \rangle _{p},p=1,\ldots ,P$\end{document}</tex-math></alternatives></inline-formula> are determined solely by the covariance of the context <inline-formula><alternatives><mml:math id="inf449"><mml:mi>c</mml:mi></mml:math><tex-math id="inft449">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> signal input within each population. Clearly, the effective coupling from the input task variable <inline-formula><alternatives><mml:math id="inf450"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft450">\begin{document}$k_{p_{s}}$\end{document}</tex-math></alternatives></inline-formula> to the internal task variable <inline-formula><alternatives><mml:math id="inf451"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft451">\begin{document}$k_{r}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf452"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math><tex-math id="inft452">\begin{document}$\sum _{p=1}^{P}\alpha _{p}\left \langle \phi ^{'}\right \rangle _{p}\sigma _{I_{s}n_{r}}^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula>, and the effective coupling between the internal task variables <inline-formula><alternatives><mml:math id="inf453"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft453">\begin{document}$k_{j}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf454"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft454">\begin{document}$k_{r}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf455"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math><tex-math id="inft455">\begin{document}$\sum _{p=1}^{P}\alpha _{p}\left \langle \phi ^{'}\right \rangle _{p}\sigma _{m_{j}n_{r}}^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec></sec><sec id="s4-12"><title>Mean-field-theory-based model construction</title><p>Utilizing this theory, we can construct RNNs tailored to any given ratio of input modulation to selection vector modulation by properly setting the connectivity vectors <inline-formula><alternatives><mml:math id="inf456"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft456">\begin{document}$( \boldsymbol I_{1}, \boldsymbol I_{2}, \boldsymbol I_{1}^{ctx}, \boldsymbol I_{2}^{ctx}, \boldsymbol m_{iv1}, \boldsymbol m_{iv2}, \boldsymbol m_{dv}, \boldsymbol n_{iv1}, \boldsymbol n_{iv2}, and \,\boldsymbol n_{dv})$\end{document}</tex-math></alternatives></inline-formula>. The RNN we built consists of 30,000 neurons divided into three populations. The first population (neurons 1–10,000) receives the stimulus input, accounting for information flow from stimulus input to intermediate variables (the connection strength is controlled by <inline-formula><alternatives><mml:math id="inf457"><mml:mi>β</mml:mi></mml:math><tex-math id="inft457">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula>) and the recurrent connection of the decision variable. The second (neurons 10,001–20,000) and third populations (neurons 20,001–30,000) receive the stimulus input, accounting for the information flow from the stimulus input to the decision variable (the connection strength is controlled by <inline-formula><alternatives><mml:math id="inf458"><mml:mi>α</mml:mi></mml:math><tex-math id="inft458">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula>), and the information flow from the intermediate variables to the decision variable (the connection strength is controlled by <inline-formula><alternatives><mml:math id="inf459"><mml:mi>η</mml:mi></mml:math><tex-math id="inft459">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula>), and are modulated by contextual input. To achieve this, we generate three Gaussian random matrices <inline-formula><alternatives><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft460">\begin{document}$(M^{\left (1\right)},M^{\left (2\right)},M^{\left (3\right)})$\end{document}</tex-math></alternatives></inline-formula> of shape 10,000 × 3, 10,000 × 5  and 10,000 × 5, respectively. Let <inline-formula><alternatives><mml:math id="inf461"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math><tex-math id="inft461">\begin{document}$M_{\colon ,r}^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula> denote the <inline-formula><alternatives><mml:math id="inf462"><mml:mi>r</mml:mi></mml:math><tex-math id="inft462">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>-th column of matrix <inline-formula><alternatives><mml:math id="inf463"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math><tex-math id="inft463">\begin{document}$M^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula>. The stimulus input <inline-formula><alternatives><mml:math id="inf464"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft464">\begin{document}$I_{1}$\end{document}</tex-math></alternatives></inline-formula> is given by the concatenation of three length-10,000 vectors <inline-formula><alternatives><mml:math id="inf465"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft465">\begin{document}$\left [M_{\colon ,1}^{\left (1\right)};M_{\colon ,1}^{\left (2\right)};M_{\colon ,1}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>. The stimulus input <inline-formula><alternatives><mml:math id="inf466"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft466">\begin{document}$I_{2}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf467"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft467">\begin{document}$\left [M_{\colon ,2}^{\left (1\right)};M_{\colon ,2}^{\left (2\right)};M_{\colon ,2}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>. The context input <inline-formula><alternatives><mml:math id="inf468"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft468">\begin{document}$I_{1}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf469"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft469">\begin{document}$\left [\boldsymbol 0;M_{\colon ,3}^{\left (2\right)};\boldsymbol 0\right ]$\end{document}</tex-math></alternatives></inline-formula>. The context input <inline-formula><alternatives><mml:math id="inf470"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft470">\begin{document}$I_{2}^{ctx}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf471"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft471">\begin{document}$\left [\boldsymbol 0;\boldsymbol 0;M_{\colon ,3}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>. The connectivity vectors <inline-formula><alternatives><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft472">\begin{document}$\boldsymbol m_{iv1},\boldsymbol m_{iv2},$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft473">\begin{document}$\boldsymbol m_{dv}$\end{document}</tex-math></alternatives></inline-formula> are given by <inline-formula><alternatives><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft474">\begin{document}$\left [\boldsymbol 0;M_{\colon ,4}^{\left (2\right)};M_{\colon ,4}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft475">\begin{document}$\left [\boldsymbol 0;M_{\colon ,5}^{\left (2\right)};M_{\colon ,5}^{\left (3\right)}\right ]$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft476">\begin{document}$\left [M_{\colon ,3}^{\left (1\right)};\boldsymbol 0;\boldsymbol 0\right ]$\end{document}</tex-math></alternatives></inline-formula>, respectively. The input-selection vectors <inline-formula><alternatives><mml:math id="inf477"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft477">\begin{document}$n_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf478"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft478">\begin{document}$n_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula> are given by <inline-formula><alternatives><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mi>β</mml:mi><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft479">\begin{document}$\left [3\beta M_{\colon ,1}^{\left (1\right)};\boldsymbol 0;\boldsymbol 0\right ]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf480"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mi>β</mml:mi><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft480">\begin{document}$\left [3\beta M_{\colon ,2}^{\left (1\right)};\boldsymbol 0;\boldsymbol 0\right ]$\end{document}</tex-math></alternatives></inline-formula>, respectively. <inline-formula><alternatives><mml:math id="inf481"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft481">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">[</mml:mo></mml:mrow><mml:mn>3</mml:mn><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>α</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft482">\begin{document}$\bigg [3M_{\colon ,3}^{\left (1\right)};- \frac{3\alpha g}{1- g^{2}}M_{\colon ,1}^{\left (2\right)}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf483"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft483">\begin{document}$+\frac{3\alpha }{1- g^{2}}M_{\colon ,2}^{\left (2\right)}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf484"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>η</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft484">\begin{document}$- \frac{3\eta g}{1- g^{2}}M_{\colon ,4}^{\left (2\right)}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf485"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft485">\begin{document}$+\frac{3\eta }{1- g^{2}}M_{\colon ,5}^{\left (2\right)};\frac{3\alpha }{1- g^{2}}M_{\colon ,1}^{\left (3\right)}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf486"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>α</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft486">\begin{document}$- \frac{3\alpha g}{1- g^{2}}M_{\colon ,2}^{\left (3\right)}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf487"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft487">\begin{document}$+\frac{3\eta }{1- g^{2}}M_{\colon ,4}^{\left (3\right)}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf488"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mi>η</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft488">\begin{document}$- \frac{3\eta g}{1- g^{2}}M_{\colon ,5}^{\left (3\right)} \bigg ]$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf489"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft489">\begin{document}$g=\int _{- \infty }^{+\infty }\frac{1}{\sqrt{2\pi }}e^{- x^{2}/2}\phi ^{'}\left (x\right)dx$\end{document}</tex-math></alternatives></inline-formula> is the average gain of the second population in context 1 or third population in context 2. The readout vector <inline-formula><alternatives><mml:math id="inf490"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft490">\begin{document}$\boldsymbol w$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf491"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft491">\begin{document}$\left [4M_{\colon ,3}^{\left (1\right)};\boldsymbol 0;\boldsymbol 0\right ]$\end{document}</tex-math></alternatives></inline-formula>.</p><p>For the network, in context 1, <inline-formula><alternatives><mml:math id="inf492"><mml:msub><mml:mrow><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft492">\begin{document}$\left \langle \phi ^{'}\right \rangle _{p}$\end{document}</tex-math></alternatives></inline-formula> is only determined by the covariance of the context 1 signal. That is,<disp-formula id="equ39"><label>(39)</label><alternatives><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:msqrt><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t39">\begin{document}$$\displaystyle  \left \langle \phi ^{'}\right \rangle _{p}=\int _{- \infty }^{\infty }\frac{1}{\sqrt{2\pi }}e^{\frac{- x^{2}}{2}}\phi ^{'}\left (\sqrt{\sigma _{I_{1}^{ctx}I_{1}^{ctx}}^{\left (p\right)}}x\right)dx.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Our construction method guarantees that <inline-formula><alternatives><mml:math id="inf493"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft493">\begin{document}$\sigma _{I_{1}^{ctx}I_{1}^{ctx}}^{\left (p\right)}=0,1,0$\end{document}</tex-math></alternatives></inline-formula> for <inline-formula><alternatives><mml:math id="inf494"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:math><tex-math id="inft494">\begin{document}$p=1,2,3$\end{document}</tex-math></alternatives></inline-formula>, respectively. Hence, <inline-formula><alternatives><mml:math id="inf495"><mml:msub><mml:mrow><mml:mfenced open="⟨" close="⟩" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft495">\begin{document}$\left \langle \phi ^{'}\right \rangle _{p}=1,g,1$\end{document}</tex-math></alternatives></inline-formula> for <inline-formula><alternatives><mml:math id="inf496"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:math><tex-math id="inft496">\begin{document}$p=1,2,3$\end{document}</tex-math></alternatives></inline-formula>, respectively. Then the effective coupling from input variable <inline-formula><alternatives><mml:math id="inf497"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft497">\begin{document}$k_{p_{1}}$\end{document}</tex-math></alternatives></inline-formula> to <inline-formula><alternatives><mml:math id="inf498"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft498">\begin{document}$k_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> in context 1 is given by<disp-formula id="equ40"><label>(40)</label><alternatives><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t40">\begin{document}$$\displaystyle  E_{inp_{1}\rightarrow iv_{1}}^{ctx_{1}}=\sum _{p=1}^{P}\frac{1}{3}\left \langle \phi ^{'}\right \rangle _{p}\sigma _{I_{1}n_{iv_{1}}}^{\left (p\right)}=\frac{1}{3}\times 3\times \beta =\beta .$$\end{document}</tex-math></alternatives></disp-formula></p><p>Similarly, we can get that:<disp-formula id="equ41"><label>(41)</label><alternatives><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t41">\begin{document}$$\displaystyle  E_{inp_{2}\rightarrow iv_{2}}^{ctx_{1}}=\beta ,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ42"><label>(42)</label><alternatives><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t42">\begin{document}$$\displaystyle  E_{inp_{1}\rightarrow dv}^{ctx_{1}}=\alpha ,E_{inp_{2}\rightarrow dv}^{ctx_{1}}=0,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ43"><label>(43)</label><alternatives><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t43">\begin{document}$$\displaystyle  E_{iv_{1}\rightarrow dv}^{ctx_{1}}=\eta ,E_{iv_{2}\rightarrow dv}^{ctx_{1}}=0.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Other unmentioned effective couplings are zero. Similarly, the effective couplings in context 2 are given by<disp-formula id="equ44"><label>(44)</label><alternatives><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t44">\begin{document}$$\displaystyle  E_{inp_{1}\rightarrow iv_{2}}^{ctx_{2}}=E_{inp_{2}\rightarrow iv_{2}}^{ctx_{2}}=\beta ,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ45"><label>(45)</label><alternatives><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t45">\begin{document}$$\displaystyle  E_{inp_{1}\rightarrow dv}^{ctx_{2}}=0,E_{inp_{2}\rightarrow dv}^{ctx_{2}}=\alpha ,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ46"><label>(46)</label><alternatives><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t46">\begin{document}$$\displaystyle  E_{iv_{1}\rightarrow dv}^{ctx_{2}}=0,E_{iv_{2}\rightarrow dv}^{ctx_{2}}=\eta .$$\end{document}</tex-math></alternatives></disp-formula></p><p>Thus, for each input, the input modulation is <inline-formula><alternatives><mml:math id="inf499"><mml:mi>α</mml:mi></mml:math><tex-math id="inft499">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula> and the selection vector modulation is <inline-formula><alternatives><mml:math id="inf500"><mml:mi>β</mml:mi><mml:mo>×</mml:mo><mml:mi>η</mml:mi></mml:math><tex-math id="inft500">\begin{document}$\beta \times \eta $\end{document}</tex-math></alternatives></inline-formula>. Therefore, any given ratio of input modulation to selection vector modulation can be achieved by varying the parameters <inline-formula><alternatives><mml:math id="inf501"><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo></mml:math><tex-math id="inft501">\begin{document}$\alpha ,\beta ,$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf502"><mml:mi>η</mml:mi></mml:math><tex-math id="inft502">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula>. The example in <xref ref-type="fig" rid="fig3">Figure 3</xref> with pure selection vector modulation is a special case with <inline-formula><alternatives><mml:math id="inf503"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><tex-math id="inft503">\begin{document}$\alpha =0,\beta =\frac{10}{3},$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf504"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math><tex-math id="inft504">\begin{document}$\eta =\left (1- g^{2}\right)/3$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-13"><title>Pathway-based definition of selection vector (<xref ref-type="fig" rid="fig6">Figure 6</xref>)</title><p>Next, we will consider the rank-3 RNN with latent dynamics depicted in <xref ref-type="disp-formula" rid="equ18 equ19">Equations 18 and 19</xref>. The input representation produced by a pulse input <inline-formula><alternatives><mml:math id="inf505"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft505">\begin{document}$u_{s}=\tau A_{s}\delta \left (t\right),s=1,2$\end{document}</tex-math></alternatives></inline-formula> in a certain context at <inline-formula><alternatives><mml:math id="inf506"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft506">\begin{document}$t=0$\end{document}</tex-math></alternatives></inline-formula> is given by <inline-formula><alternatives><mml:math id="inf507"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft507">\begin{document}$A_{1}\overset{\sim }{\boldsymbol I}_{1}+A_{2}\overset{\sim }{\boldsymbol I}_{2}$\end{document}</tex-math></alternatives></inline-formula>. We have proven that this pulse input will ultimately reach the <inline-formula><alternatives><mml:math id="inf508"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft508">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot with a magnitude of <inline-formula><alternatives><mml:math id="inf509"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math><tex-math id="inft509">\begin{document}$\sum _{s=1}^{2}A_{s}\left (E_{p_{s}\rightarrow dv}+E_{p_{s}\rightarrow iv_{s}}E_{iv_{s}\rightarrow dv}\right)$\end{document}</tex-math></alternatives></inline-formula>. This equation can be rewritten as<disp-formula id="equ47"><label>(47)</label><alternatives><mml:math id="m47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t47">\begin{document}$$\displaystyle  \frac{1}{N}\left (A_{1}\overset{\sim }{\boldsymbol I}_{1}+A_{2}\overset{\sim }{\boldsymbol I}_{2}\right)\cdot \left (\boldsymbol n_{dv}+E_{iv_{1}\rightarrow dv}\boldsymbol n_{iv_{1}}+E_{iv_{2}\rightarrow dv}\boldsymbol n_{iv_{2}}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Therefore, we define <inline-formula><alternatives><mml:math id="inf510"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft510">\begin{document}$\boldsymbol n_{tol}=\boldsymbol n_{dv}+E_{iv_{1}\rightarrow dv}\boldsymbol n_{iv_{1}}+E_{iv_{2}\rightarrow dv}\boldsymbol n_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula> as the pathway-based definition of the selection vector. Through calculations, we can prove that this pathway-based definition of the selection vector is equivalent to the classical definition based on linearized dynamical systems. In fact, the transition matrix of neuron activity in this rank-3 RNN is given by:<disp-formula id="equ48"><label>(48)</label><alternatives><mml:math id="m48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t48">\begin{document}$$\displaystyle  \begin{array}{c} M= - E+\frac{1}{N}G\left (\boldsymbol m_{iv_{1}}\boldsymbol n_{iv_{1}}^{T}+\boldsymbol m_{iv_{2}}\boldsymbol n_{iv_{2}}^{T}+\boldsymbol m_{dv}\boldsymbol n_{dv}^{T}\right)\\  - E+\frac{1}{N}\left (\overset{\sim }{\boldsymbol m}_{iv_{1}}\boldsymbol n_{iv_{1}}^{T}+\overset{\sim }{\boldsymbol m}_{iv_{2}}\boldsymbol n_{iv_{2}}^{T}+\overset{\sim }{\boldsymbol m}_{dv}\boldsymbol n_{dv}^{T}\right).\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Multiplying <inline-formula><alternatives><mml:math id="inf511"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft511">\begin{document}$M^{T}$\end{document}</tex-math></alternatives></inline-formula> on the right by <inline-formula><alternatives><mml:math id="inf512"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft512">\begin{document}$\boldsymbol n_{tol}$\end{document}</tex-math></alternatives></inline-formula>, we obtain:<disp-formula id="equ49"><label>(49)</label><alternatives><mml:math id="m49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t49">\begin{document}$$\displaystyle  M^{T}\boldsymbol n_{tol}=0.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Moreover, under the condition that the choice axis is invariant across contexts, i.e., <inline-formula><alternatives><mml:math id="inf513"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft513">\begin{document}$\overset{\sim }{\boldsymbol m}_{dv}$\end{document}</tex-math></alternatives></inline-formula> is invariant across contexts (<xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>),<disp-formula id="equ50"><label>(50)</label><alternatives><mml:math id="m50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t50">\begin{document}$$\displaystyle  \frac{\overset{\sim }{\boldsymbol m}_{dv}}{\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}}\cdot \left (\frac{1}{N}\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\boldsymbol n_{tol}\right)=\left \langle \overset{\sim }{\boldsymbol m}_{dv},\boldsymbol n_{tol}\right \rangle =1.$$\end{document}</tex-math></alternatives></disp-formula></p><p>This demonstrates that <inline-formula><alternatives><mml:math id="inf514"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft514">\begin{document}$\frac{1}{N}\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\boldsymbol n_{tol}$\end{document}</tex-math></alternatives></inline-formula> is indeed the left eigenvector of the transition matrix as well as the classical selection vector of the linearized dynamical systems.</p></sec><sec id="s4-14"><title>The equivalence between two definitions of selection vector modulation (<xref ref-type="fig" rid="fig5">Figure 5</xref>)</title><p>Here, we use the rank-3 RNN and input 1 as an example to explain why there is an equivalence between our definition of selection vector modulation and the classical one (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>). In the previous section, we have proven that the input representation direction and selection vector are given by <inline-formula><alternatives><mml:math id="inf515"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft515">\begin{document}$\overset{\sim }{\boldsymbol I}_{1}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf516"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft516">\begin{document}$s=\frac{1}{N}\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\left (\boldsymbol n_{dv}+E_{iv_{1}\rightarrow dv}\boldsymbol n_{iv_{1}}+E_{iv_{2}\rightarrow dv}\boldsymbol n_{iv_{2}}\right)$\end{document}</tex-math></alternatives></inline-formula>, respectively. According to the classical definition (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>), the input modulation and selection vector modulation are given by <inline-formula><alternatives><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft517">\begin{document}$mod_{inp}=\Delta \overset{\sim }{\boldsymbol I}_{1}\cdot \bar{\boldsymbol s}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf518"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft518">\begin{document}$mod_{sel}=\bar{\overset{\sim }{\boldsymbol I}_{\text{1}}}\cdot \Delta \boldsymbol s$\end{document}</tex-math></alternatives></inline-formula>, respectively. Since <inline-formula><alternatives><mml:math id="inf519"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft519">\begin{document}$\overset{\sim }{\boldsymbol m}_{dv},\boldsymbol n_{iv_{1}},\boldsymbol n_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf520"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft520">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> are invariant across different contexts, we have:<disp-formula id="equ51"><label>(51)</label><alternatives><mml:math id="m51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t51">\begin{document}$$\displaystyle  \bar{\boldsymbol s}=\frac{1}{N}\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\left (\boldsymbol n_{dv}+\bar{E}_{iv_{1}\rightarrow dv}\boldsymbol n_{iv_{1}}+\bar{E}_{iv_{2}\rightarrow dv}\boldsymbol n_{iv_{2}}\right),$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ52"><label>(52)</label><alternatives><mml:math id="m52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t52">\begin{document}$$\displaystyle  \Delta \boldsymbol s=\frac{1}{N}\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\left (\Delta E_{iv_{1}\rightarrow dv}\boldsymbol n_{iv_{1}}+\Delta E_{iv_{2}\rightarrow dv}\boldsymbol n_{iv_{2}}\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>Substituting these into <xref ref-type="disp-formula" rid="equ7 equ8">Equations 7 and 8</xref> yields:<disp-formula id="equ53"><label>(53)</label><alternatives><mml:math id="m53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t53">\begin{document}$$\displaystyle  mod_{inp}=\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\left (\Delta E_{inp_{1}\rightarrow dv}+\Delta E_{inp_{1}\rightarrow iv_{1}}\bar{E}_{iv_{1}\rightarrow dv}\right),$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ54"><label>(54)</label><alternatives><mml:math id="m54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>∼</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t54">\begin{document}$$\displaystyle  mod_{sel}=\left \| \overset{\sim }{\boldsymbol m}_{dv}\right \| _{2}\left (\bar{E}_{\boldsymbol inp_{1}\rightarrow iv_{1}}\Delta E_{iv_{1}\rightarrow dv}\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>a result fully consistent with the pathway-based definition in <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> of the main text.</p></sec><sec id="s4-15"><title>Comparison between the pathway-based selection vector and the classical one (<xref ref-type="fig" rid="fig6">Figure 6</xref>)</title><p>We generate 1000 RNNs according to the procedure in <xref ref-type="fig" rid="fig5">Figure 5C</xref> (see Method ‘<italic>Mean-field-theory-based model construction’ for details</italic>), with each RNN defined by parameters <inline-formula><alternatives><mml:math id="inf521"><mml:mi>α</mml:mi></mml:math><tex-math id="inft521">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf522"><mml:mi>β</mml:mi></mml:math><tex-math id="inft522">\begin{document}$\beta $\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf523"><mml:mi>γ</mml:mi></mml:math><tex-math id="inft523">\begin{document}$\gamma $\end{document}</tex-math></alternatives></inline-formula> independently sampled from a Uniform (0, 1) distribution. For each RNN, we computed the selection vector for the RNN in a given context (e.g. context 1 or 2) in two ways:</p><p>via linearized dynamical system analysis following <xref ref-type="bibr" rid="bib20">Mante et al., 2013</xref>, producing the selection vector <inline-formula><alternatives><mml:math id="inf524"><mml:msup><mml:mrow><mml:mtext>sv</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft524">\begin{document}$\text{sv}^{classical}$\end{document}</tex-math></alternatives></inline-formula> (classical in <xref ref-type="fig" rid="fig6">Figure 6B</xref>), using the theoretical derivation <inline-formula><alternatives><mml:math id="inf525"><mml:mtext>sv</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft525">\begin{document}$\text{sv}=n_{dv}+\eta n_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> (‘our’s’ in <xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><p>We repeated this process 1000 times and measured the cosine angle between these two selection vectors and plotted the resulting distribution for context 1 (gray) and context 2 (blue) in <xref ref-type="fig" rid="fig6">Figure 6B</xref>.</p></sec><sec id="s4-16"><title>Pathway-based analysis of higher order low-rank RNNs (<xref ref-type="fig" rid="fig7">Figure 7A and B</xref>)</title><p>In this section, we will consider RNNs with more intermediate variables (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Here, we consider only one stimulus modality for simplicity and use the same notation convention as in the previous sections. The network consists of one input variable <inline-formula><alternatives><mml:math id="inf526"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft526">\begin{document}$k_{inp}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf527"><mml:mi>L</mml:mi></mml:math><tex-math id="inft527">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula> intermediate variables <inline-formula><alternatives><mml:math id="inf528"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:math><tex-math id="inft528">\begin{document}$k_{iv_{i}},i=1,\ldots ,L$\end{document}</tex-math></alternatives></inline-formula>, and one decision variable <inline-formula><alternatives><mml:math id="inf529"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft529">\begin{document}$k_{dv}$\end{document}</tex-math></alternatives></inline-formula>. For simplicity, we let <inline-formula><alternatives><mml:math id="inf530"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft530">\begin{document}$k_{iv_{0}}$\end{document}</tex-math></alternatives></inline-formula> be an alias for the input variable and <inline-formula><alternatives><mml:math id="inf531"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft531">\begin{document}$k_{iv_{L+1}}$\end{document}</tex-math></alternatives></inline-formula> be an alias for the decision variable. We use the shorthand <inline-formula><alternatives><mml:math id="inf532"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>→</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft532">\begin{document}$E_{i\rightarrow j}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf533"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft533">\begin{document}$E_{i\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula> to denote the effective coupling <inline-formula><alternatives><mml:math id="inf534"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft534">\begin{document}$E_{iv_{i}\rightarrow iv_{j}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf535"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft535">\begin{document}$E_{iv_{i}\rightarrow dv}$\end{document}</tex-math></alternatives></inline-formula>, respectively. The dynamics of these task variables are given by<disp-formula id="equ55"><label>(55)</label><alternatives><mml:math id="m55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t55">\begin{document}$$\displaystyle  \tau \frac{dk_{inp}}{dt}=- k_{inp}+u,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ56"><label>(56)</label><alternatives><mml:math id="m56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t56">\begin{document}$$\displaystyle  \tau \frac{dk_{iv_{i}}}{dt}=- k_{iv_{i}}+\sum _{j=0}^{i- 1}E_{j\rightarrow i}k_{iv_{j}},i=1,\ldots ,L,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ57"><label>(57)</label><alternatives><mml:math id="m57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t57">\begin{document}$$\displaystyle  \tau \frac{dk_{dv}}{dt}=\sum _{j=0}^{L}E_{j\rightarrow L+1}k_{iv_{j}}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Consider the case when, at time <inline-formula><alternatives><mml:math id="inf536"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><tex-math id="inft536">\begin{document}$t=0$\end{document}</tex-math></alternatives></inline-formula>, the network receives a pulse input with unit size (i.e. <inline-formula><alternatives><mml:math id="inf537"><mml:mi>u</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft537">\begin{document}$u\left (t\right)=\tau \delta \left (t\right)$\end{document}</tex-math></alternatives></inline-formula>). Then, we have<disp-formula id="equ58"><label>(58)</label><alternatives><mml:math id="m58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t58">\begin{document}$$\displaystyle  k_{inp}=e^{\frac{- t}{\tau }},$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ59"><label>(59)</label><alternatives><mml:math id="m59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>j</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mi>τ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t59">\begin{document}$$\displaystyle  k_{iv_{i}}=\sum _{j=1}^{i}\frac{K_{j}^{i}}{j!}e^{\frac{- t}{\tau }}\left (\frac{t}{\tau }\right)^{j}i=0,\ldots ,n,$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ60"><label>(60)</label><alternatives><mml:math id="m60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t60">\begin{document}$$\displaystyle  k_{dv}=k_{iv_{L+1}}=\sum _{j=1}^{L+1}K_{j}^{L+1}\left (1- \frac{\Gamma \left (j,\frac{t}{\tau }\right)}{\left (j- 1\right)!}\right),$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf538"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>…</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo>…</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft538">\begin{document}$K_{j}^{i}=\sum {0=a_{1}\lt a_{2}\lt \ldots \lt a_{j}\lt i}E_{a_{1}\rightarrow a_{2}}\cdot E_{a_{2}\rightarrow a_{3}}\cdot \ldots \cdot E_{a_{j}\rightarrow i}$\end{document}</tex-math></alternatives></inline-formula>. In <xref ref-type="disp-formula" rid="equ52">Equation 52</xref>, <inline-formula><alternatives><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft539">\begin{document}$\Gamma $\end{document}</tex-math></alternatives></inline-formula> stands for the incomplete Gamma function and <inline-formula><alternatives><mml:math id="inf540"><mml:mi>Γ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft540">\begin{document}$\Gamma \left (j,\frac{t}{\tau }\right)=\int _{t/\tau }^{\infty }x^{j- 1}e^{- x}dx$\end{document}</tex-math></alternatives></inline-formula>. These expressions tell us that, as time goes to infinity, all intermediate task variables <inline-formula><alternatives><mml:math id="inf541"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:math><tex-math id="inft541">\begin{document}$k_{iv_{i}},i=1,\ldots ,L$\end{document}</tex-math></alternatives></inline-formula> will decay to zero and the decision variable will converge to <inline-formula><alternatives><mml:math id="inf542"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math><tex-math id="inft542">\begin{document}$\sum _{j=1}^{L+1}K_{j}^{L+1}$\end{document}</tex-math></alternatives></inline-formula>, which means that a pulse input of unit size will ultimately reach the <inline-formula><alternatives><mml:math id="inf543"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft543">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula> slot with a magnitude of <inline-formula><alternatives><mml:math id="inf544"><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math><tex-math id="inft544">\begin{document}$\sum _{j=1}^{L+1}K_{j}^{L+1}$\end{document}</tex-math></alternatives></inline-formula>. Therefore, we define the total effective coupling from the input variable to the decision variable in this higher-order graph as<disp-formula id="equ61"><label>(61)</label><alternatives><mml:math id="m61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>…</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo>…</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t61">\begin{document}$$\displaystyle  E_{tol}=\sum _{j=1}^{L+1}\sum _{0=a_{1}\lt a_{2}\lt \ldots \lt a_{j}\leq L}E_{a_{1}\rightarrow a_{2}}\cdot E_{a_{2}\rightarrow a_{3}}\cdot \ldots \cdot E_{a_{j}\rightarrow dv}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The difference of the total effective coupling between relevant context and irrelevant context can be decomposed into:<disp-formula id="equ62"><label>(62)</label><alternatives><mml:math id="m62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>…</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>3</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo>…</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>…</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mover><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>1</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo>…</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t62">\begin{document}$$\displaystyle  \begin{array}{c} \Delta E_{tol}=\sum _{j=1}^{L+1}\sum _{0=a_{1}\lt a_{2}\lt \ldots \lt a_{j}\leq L}\Delta \left (E_{a_{1}\rightarrow a_{2}}\right)\cdot \bar {E_{a_{\text{2}}\rightarrow a_{\text{3}}}\cdot \ldots \cdot E_{a_{j}\rightarrow dv}}+ \\  \sum _{j=1}^{L+1}\sum _{0=a_{1}\lt a_{2}\lt \ldots \lt a_{j}\leq L}\bar{E_{a_{\text{1}}\rightarrow a_{\text{2}}}}\cdot \Delta \left (E_{a_{2}\rightarrow a_{3}}\cdot \ldots \cdot E_{a_{j}\rightarrow dv}\right).\end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>The first term, caused by changing a stimulus input representation, is defined as the input modulation. The second term, the one without changing the stimulus input representation, is defined as the selection vector modulation.</p><p>Using a similar method in rank-3 RNNs, the selection vector for RNNs of higher order is given by:<disp-formula id="equ63"><label>(63)</label><alternatives><mml:math id="m63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>…</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo>…</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t63">\begin{document}$$\displaystyle  n_{dv}+\sum _{j=1}^{L}\sum _{0\lt a_{1}\lt \ldots \lt a_{j}\leq L}\left (E_{a_{1}\rightarrow a_{2}}\cdot \ldots \cdot E_{a_{j}\rightarrow dv}\right )n_{iv_{a_{1}}}.$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-17"><title>Training of full-rank vanilla RNNs using backpropagation (<xref ref-type="fig" rid="fig7">Figure 7</xref>)</title><p>For <xref ref-type="fig" rid="fig7">Figure 7</xref>, we trained full-rank RNNs of <inline-formula><alternatives><mml:math id="inf545"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:math><tex-math id="inft545">\begin{document}$N=128$\end{document}</tex-math></alternatives></inline-formula> neurons. We trained the elements of the input vectors, the connectivity matrix, and the readout vector. We tested a large range of regularization coefficients ranging from 0 to 0.1. For each <inline-formula><alternatives><mml:math id="inf546"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft546">\begin{document}$r_{reg}$\end{document}</tex-math></alternatives></inline-formula> chosen from the set {0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1}, we trained 100 full-rank RNNs. A larger <inline-formula><alternatives><mml:math id="inf547"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft547">\begin{document}$w_{reg}$\end{document}</tex-math></alternatives></inline-formula> value results in a trained connectivity matrix <inline-formula><alternatives><mml:math id="inf548"><mml:mi>J</mml:mi></mml:math><tex-math id="inft548">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula> with lower rank, making the network more similar to a rank-1 RNN. All trainable parameters were initialized with random independent Gaussian weights with a mean of 0 and variance of <inline-formula><alternatives><mml:math id="inf549"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math><tex-math id="inft549">\begin{document}$1/N^{2}$\end{document}</tex-math></alternatives></inline-formula>. Only trained RNNs with their largest eigenvalue of the activity transition matrix falling within the –0.05–0.05 range in both contexts were selected for subsequent analysis.</p><p>To ensure that the conclusions drawn from <xref ref-type="fig" rid="fig7">Figure 7</xref> are robust and not dependent on specific hyperparameter settings, we conducted similar experiments under different model hyperparameter settings. First, we trained RNNs with the softplus activation function and regularization coefficients chosen from {0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.008, 0.004, 0.002, 0.001} (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2A</xref>). Unlike tanh, softplus does not saturate on the positive end. Additionally, we tested the initialization of trainable parameters with a variance of <inline-formula><alternatives><mml:math id="inf550"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft550">\begin{document}$1/N$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2B</xref>). Together, these experiments confirmed that the main results do not depend on the specific model hyperparameter settings.</p><sec id="s4-17-1"><title>Estimating matrix dimension and extra dimension (<xref ref-type="fig" rid="fig7">Figure 7</xref>)</title><sec id="s4-17-1-1"><title>Effective dimension of connectivity matrix (<xref ref-type="fig" rid="fig7">Figure 7D</xref>)</title><p>Let <inline-formula><alternatives><mml:math id="inf551"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>…</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft551">\begin{document}$\sigma _{1}\geq \sigma _{2}\geq \ldots \geq \sigma _{n}$\end{document}</tex-math></alternatives></inline-formula> be the singular values of the connectivity matrix. The matrix’s rank is the number of nonzero singular values. However, rank alone can overlook differences in how quickly those singular values decay. To capture this, we define the effective dimension as its stable rank (<xref ref-type="bibr" rid="bib35">Sanyal et al., 2020</xref>):<disp-formula id="equ64"><label>(64)</label><alternatives><mml:math id="m64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>e</mml:mtext><mml:mtext>d</mml:mtext><mml:mtext>i</mml:mtext><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>J</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t64">\begin{document}$$\displaystyle  \text{e}\text{d}\text{i}\text{m}\left (J\right)=\sum _{i=1}^{n}\frac{\sigma _{i}^{2}}{\sigma _{1}^{2}}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Each term lies between 0 and 1, so the effective dimension satisfies <inline-formula><alternatives><mml:math id="inf552"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mtext>edim</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mfenced><mml:mo>≤</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft552">\begin{document}$0\leq \text{edim}\left (J\right)\leq rank\left (J\right)$\end{document}</tex-math></alternatives></inline-formula>. When all nonzero singular values are equal, <inline-formula><alternatives><mml:math id="inf553"><mml:mtext>edim</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft553">\begin{document}$\text{edim}\left (J\right)$\end{document}</tex-math></alternatives></inline-formula> equals the matrix rank. But if some singular values are much smaller than others, effective dimension will be closer to 1.</p></sec><sec id="s4-17-1-2"><title>Single neuron response kernel for pulse input (<xref ref-type="fig" rid="fig7">Figure 7E</xref>)</title><p>We apply pulse-based linear regression (<xref ref-type="bibr" rid="bib28">Pagan et al., 2022</xref>) to assess how pulse input affects neuron activity. The activity of neuron <inline-formula><alternatives><mml:math id="inf554"><mml:mi>i</mml:mi></mml:math><tex-math id="inft554">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> in trial <inline-formula><alternatives><mml:math id="inf555"><mml:mi>k</mml:mi></mml:math><tex-math id="inft555">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> at time step <inline-formula><alternatives><mml:math id="inf556"><mml:mi>t</mml:mi></mml:math><tex-math id="inft556">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is given by:<disp-formula id="equ65"><label>(65)</label><alternatives><mml:math id="m65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t65">\begin{document}$$\displaystyle  r_{i,t}\left (k\right)=\beta _{choice;i,t}choice\left (k\right)+\beta _{context;i,t}context\left (k\right)+\beta _{time;i,t}+\beta _{inp_{1},ctx_{c};i}\ast u_{1,k}+\beta _{inp_{2},ctx_{c};i}\ast u_{2,k},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf557"><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft557">\begin{document}$choice\left (k\right)$\end{document}</tex-math></alternatives></inline-formula> is the RNN’s choice on trial <inline-formula><alternatives><mml:math id="inf558"><mml:mi>k</mml:mi></mml:math><tex-math id="inft558">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> defined as the sign of its output during the decision period, <inline-formula><alternatives><mml:math id="inf559"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft559">\begin{document}$context\left (k\right)$\end{document}</tex-math></alternatives></inline-formula> is the context of trial <inline-formula><alternatives><mml:math id="inf560"><mml:mi>k</mml:mi></mml:math><tex-math id="inft560">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> (1 for context 1 and 0 for context 2), <inline-formula><alternatives><mml:math id="inf561"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft561">\begin{document}$u_{i,k}$\end{document}</tex-math></alternatives></inline-formula> indicates signed input <inline-formula><alternatives><mml:math id="inf562"><mml:mi>i</mml:mi></mml:math><tex-math id="inft562">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> evidence as defined previously in Methods. The first three regression coefficients, <inline-formula><alternatives><mml:math id="inf563"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft563">\begin{document}$\beta _{choice;i,t}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf564"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft564">\begin{document}$\beta _{context;i,t}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf565"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft565">\begin{document}$\beta _{time;i,t}$\end{document}</tex-math></alternatives></inline-formula>, capture the influence of choice, context, and time on the neuron’s activity at each time step, each being a 40-dimension vector. The remaining coefficient, <inline-formula><alternatives><mml:math id="inf566"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft566">\begin{document}$\beta _{inp_{1},ctx_{1};i}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf567"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft567">\begin{document}$\beta _{inp_{1},ctx_{2};i}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf568"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft568">\begin{document}$\beta _{inp_{2},ctx_{1};i}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf569"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft569">\begin{document}$\beta _{inp_{2},ctx_{2};i}$\end{document}</tex-math></alternatives></inline-formula>, reflect the impact of pulse input on the neuron activity in a specific context, each also a 40-dimension vector. The asterisk (*) indicates the convolution operation between the response kernel and input evidence, described by:<disp-formula id="equ66"><label>(66)</label><alternatives><mml:math id="m66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t66">\begin{document}$$\displaystyle  \left (\beta _{inp_{1},ctx_{c};i}\ast u_{1,k}\right)\left (t\right)=\sum _{s=1}^{t}u_{1,k}\left (s\right)\beta _{inp_{1},ctx_{c};i}\left (t- s\right).$$\end{document}</tex-math></alternatives></disp-formula></p><p>Therefore, there are a total of 280 regression coefficients for each neuron. We obtained these coefficients using ridge regression with 1000 trials for each RNN. The coefficient <inline-formula><alternatives><mml:math id="inf570"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft570">\begin{document}$\beta _{inp_{i},ctx_{c};i}\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> is termed the single neuron response kernel for input <inline-formula><alternatives><mml:math id="inf571"><mml:mi>i</mml:mi></mml:math><tex-math id="inft571">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> in context <inline-formula><alternatives><mml:math id="inf572"><mml:mi>c</mml:mi></mml:math><tex-math id="inft572">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7E</xref>).</p></sec><sec id="s4-17-1-3"><title>Response kernel modes and normalized percentage of explained variance (PEV) (<xref ref-type="fig" rid="fig7">Figure 7F</xref>)</title><p>For each RNN, we construct a matrix <inline-formula><alternatives><mml:math id="inf573"><mml:mi>B</mml:mi></mml:math><tex-math id="inft573">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> with shape <inline-formula><alternatives><mml:math id="inf574"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft574">\begin{document}$N_{t}\times \left (2N\right)$\end{document}</tex-math></alternatives></inline-formula> from neuron response kernels for input 1 (or input 2) across both contexts, where <inline-formula><alternatives><mml:math id="inf575"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:math><tex-math id="inft575">\begin{document}$N_{t}=40$\end{document}</tex-math></alternatives></inline-formula> represents time steps and <inline-formula><alternatives><mml:math id="inf576"><mml:mi>N</mml:mi></mml:math><tex-math id="inft576">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> is the number of neurons, with each column as a neuron’s response kernel in a context. Then we apply singular value decomposition (SVD) to the matrix:<disp-formula id="equ67"><label>(67)</label><alternatives><mml:math id="m67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t67">\begin{document}$$\displaystyle  B=USV^{T}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf577"><mml:mi>S</mml:mi></mml:math><tex-math id="inft577">\begin{document}$S$\end{document}</tex-math></alternatives></inline-formula> is a diagonal square matrix of size <inline-formula><alternatives><mml:math id="inf578"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft578">\begin{document}$N_{t}$\end{document}</tex-math></alternatives></inline-formula>, with diagonal element <inline-formula><alternatives><mml:math id="inf579"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>…</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft579">\begin{document}$\sigma _{1}\geq \sigma _{2}\geq \ldots \geq \sigma _{N_{t}}$\end{document}</tex-math></alternatives></inline-formula> being the singular value of the matrix. The first column of U serves as a persistent mode (mode 0), while the second and following columns are transient modes (transient dynamical modes 1, 2, etc.). The normalized PEV of transient dynamical mode <inline-formula><alternatives><mml:math id="inf580"><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft580">\begin{document}$r,r\geq 1$\end{document}</tex-math></alternatives></inline-formula> is defined as:<disp-formula id="equ68"><label>(68)</label><alternatives><mml:math id="m68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t68">\begin{document}$$\displaystyle  PEV_{mode_{r}}=\frac{\sigma _{r+1}^{2}}{\sum _{i=2}^{40}\sigma _{i}^{2}}.$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-17-1-4"><title>PEV of extra dynamical modes</title><p>The PEV of extra dynamical modes for input 1 (or 2) is defined based on the normalized PEV of transient dynamical mode:<disp-formula id="equ69"><label>(69)</label><alternatives><mml:math id="m69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t69">\begin{document}$$\displaystyle  PEV_{ED}\left (inp_{1}\right)=\sum _{r=1}^{N_{t}- 1}PEV_{mode_{r}}.$$\end{document}</tex-math></alternatives></disp-formula></p></sec></sec></sec><sec id="s4-18"><title>The counterintuitive extreme example (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>)</title><p>We can manually construct two models (RNN1 and RNN2) with distinct circuit mechanisms (input modulation versus selection vector modulation) but showing the same neural activities. Specifically, we generated three Gaussian random matrices (<inline-formula><alternatives><mml:math id="inf581"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math><tex-math id="inft581">\begin{document}$M^{\left (1\right)},M^{\left (2\right)},M^{\left (3\right)}$\end{document}</tex-math></alternatives></inline-formula>) of shape 10,000 × 5, 10,000 × 5 and 10,000 × 1, respectively. Let <inline-formula><alternatives><mml:math id="inf582"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math><tex-math id="inft582">\begin{document}$M_{\colon ,r}^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula> denote the <inline-formula><alternatives><mml:math id="inf583"><mml:mi>r</mml:mi></mml:math><tex-math id="inft583">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>-th column of matrix <inline-formula><alternatives><mml:math id="inf584"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math><tex-math id="inft584">\begin{document}$M^{\left (p\right)}$\end{document}</tex-math></alternatives></inline-formula>. The two models have the same input vectors, output vectors, and input-selection vectors for intermediate task variables (<inline-formula><alternatives><mml:math id="inf585"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft585">\begin{document}$\boldsymbol n_{iv_{1}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf586"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft586">\begin{document}$\boldsymbol n_{iv_{2}}$\end{document}</tex-math></alternatives></inline-formula>), given by:<disp-formula id="equ70"><label>(70)</label><alternatives><mml:math id="m70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t70">\begin{document}$$\displaystyle  \boldsymbol I_{1}=\left [M_{\colon ,1}^{\left (1\right)};M_{\colon ,1}^{\left (2\right)};\boldsymbol 0\right ],\boldsymbol I_{2}=\left [M_{\colon ,2}^{\left (1\right)};M_{\colon ,2}^{\left (2\right)};\boldsymbol 0\right ]$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ71"><label>(71)</label><alternatives><mml:math id="m71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t71">\begin{document}$$\displaystyle  \boldsymbol I_{1}^{ctx}=\left [M_{\colon ,5}^{\left (1\right)};\boldsymbol 0;\boldsymbol 0\right ],\boldsymbol I_{2}^{ctx}=\left [\boldsymbol 0;M_{\colon ,5}^{\left (2\right)};\boldsymbol 0\right ]$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ72"><label>(72)</label><alternatives><mml:math id="m72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t72">\begin{document}$$\displaystyle  \boldsymbol m_{iv_{1}}=\left [M_{\colon ,3}^{\left (1\right)};M_{\colon ,3}^{\left (2\right)};\boldsymbol 0\right ],\boldsymbol m_{iv_{2}}=\left [M_{\colon ,4}^{\left (1\right)};M_{\colon ,4}^{\left (2\right)};\boldsymbol 0\right ],\boldsymbol m_{dv}=\left [\boldsymbol 0;\boldsymbol 0;M_{\colon ,1}^{\left (3\right)}\right ]$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ73"><label>(73)</label><alternatives><mml:math id="m73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t73">\begin{document}$$\displaystyle  \boldsymbol n_{iv_{1}}=\left [\frac{3}{1+g}M_{\colon ,1}^{\left (1\right)};\frac{3}{1+g}M_{\colon ,1}^{\left (2\right)};\boldsymbol 0\right ],\boldsymbol n_{iv_{2}}=\left [\frac{- 3g}{1- g^{2}}M_{\colon ,1}^{\left (1\right)};\frac{3}{1- g^{2}}M_{\colon ,1}^{\left (2\right)};\boldsymbol 0\right ],$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf587"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math><tex-math id="inft587">\begin{document}$g=\int _{- \infty }^{+\infty }\frac{1}{\sqrt{2\pi }}e^{- x^{2}/2}\phi ^{'}\left (x\right)dx$\end{document}</tex-math></alternatives></inline-formula>. The difference between these two RNNs lies in their input-selection vector <inline-formula><alternatives><mml:math id="inf588"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft588">\begin{document}$n_{dv}$\end{document}</tex-math></alternatives></inline-formula> for the decision variable. For RNN1, <inline-formula><alternatives><mml:math id="inf589"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft589">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> is given by<disp-formula id="equ74"><label>(74)</label><alternatives><mml:math id="m74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn>3</mml:mn><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t74">\begin{document}$$\displaystyle  \boldsymbol n_{dv}=\left [\frac{3}{1+g}M_{\colon ,4}^{\left (1\right)};\frac{3}{1+g}M_{\colon ,4}^{\left (2\right)};\boldsymbol 0\right ]+\left [\frac{3}{1- g^{2}}M_{\colon ,2}^{\left (1\right)};\frac{- 3g}{1- g^{2}}M_{\colon ,2}^{\left (2\right)};\boldsymbol 0\right ]+\left [\boldsymbol 0;\boldsymbol 0;3M_{\colon ,1}^{\left (3\right)}\right ].$$\end{document}</tex-math></alternatives></disp-formula></p><p>For RNN2, <inline-formula><alternatives><mml:math id="inf590"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft590">\begin{document}$\boldsymbol n_{dv}$\end{document}</tex-math></alternatives></inline-formula> is given by:<disp-formula id="equ75"><label>(75)</label><alternatives><mml:math id="m75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>;</mml:mo><mml:mn>3</mml:mn><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t75">\begin{document}$$\displaystyle  \boldsymbol n_{dv}=\left [\frac{- 3g}{1- g^{2}}M_{\colon ,3}^{\left (1\right)};\frac{3}{1- g^{2}}M_{\colon ,3}^{\left (2\right)};\boldsymbol 0\right ]+\left [\frac{3}{1- g^{2}}M_{\colon ,2}^{\left (1\right)};\frac{- 3g}{1- g^{2}}M_{\colon ,2}^{\left (2\right)};\boldsymbol 0\right ]+\left [\boldsymbol 0;\boldsymbol 0;3M_{\colon ,1}^{\left (3\right)}\right ].$$\end{document}</tex-math></alternatives></disp-formula></p><p>In this construction, the information flow graphs from input 1 to the decision variable (<inline-formula><alternatives><mml:math id="inf591"><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:math><tex-math id="inft591">\begin{document}$dv$\end{document}</tex-math></alternatives></inline-formula>) in each context for the two RNNs are shown in <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3A</xref>.</p><p>Although the connectivity matrices of the two networks are different (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3B</xref>), when provided with the same input, the neuron activity of the <inline-formula><alternatives><mml:math id="inf592"><mml:mi>i</mml:mi></mml:math><tex-math id="inft592">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron in RNN1 is exactly the same as that of the <inline-formula><alternatives><mml:math id="inf593"><mml:mi>i</mml:mi></mml:math><tex-math id="inft593">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>-th neuron in RNN2 at the same time point (<xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3C</xref>). The statistical results of similarity between all neuron pairs are given in <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3D</xref>.</p></sec><sec id="s4-19"><title>Manually adding redundant structure and PEV of irrelevant activity (<xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4</xref>)</title><sec id="s4-19-1"><title>Redundant RNN (<xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4B and C</xref>)</title><p>To see if the currently proposed method can work when there is a significant amount of neural activity variance irrelevant to the task, we manually added irrelevant neural activity into the trained RNNs (termed as redundant RNNs). Specifically, we randomly choice <inline-formula><alternatives><mml:math id="inf594"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>75</mml:mn></mml:math><tex-math id="inft594">\begin{document}$K=75$\end{document}</tex-math></alternatives></inline-formula> trained vanilla RNNs (<xref ref-type="fig" rid="fig7">Figure 7D–G</xref>). To add irrelevant additional dimensions to the network without affecting the original function, for each RNN with N neurons, we build a larger RNN with <inline-formula><alternatives><mml:math id="inf595"><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:math><tex-math id="inft595">\begin{document}$2N$\end{document}</tex-math></alternatives></inline-formula> neurons equally divided into two populations. The input embeddings, readout vector, and the connectivity strengths between neurons within the first population (the first to the <inline-formula><alternatives><mml:math id="inf596"><mml:mi>N</mml:mi></mml:math><tex-math id="inft596">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula>-th neurons) are the same as the original RNN. As for the second population (the <inline-formula><alternatives><mml:math id="inf597"><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft597">\begin{document}$N+1$\end{document}</tex-math></alternatives></inline-formula>-th to the <inline-formula><alternatives><mml:math id="inf598"><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:math><tex-math id="inft598">\begin{document}$2N$\end{document}</tex-math></alternatives></inline-formula>-th neurons), the input embeddings are sampled with Gaussian noise, with the standard deviation equal to that of the original RNN input embeddings and the readout vector is set to 0. The connectivity strengths within the second population are sampled with Gaussian noise, with the standard deviation chosen such that the average sum of square activity of the two populations is nearly equal. Moreover, there is no connectivity between the two populations. The resulting RNN, compared to the original network, introduces adding task-irrelevant neural activity (neural activity in the second population). At the same time, it performs the CDM task in the same way as the original network, with the identical selection vector modulation value.</p></sec><sec id="s4-19-2"><title>PEV of irrelevant activity (<xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4C</xref>)</title><p>We hypothesize that the proportion of explained variance (PEV) in extra dynamical modes performs reliably for trained RNNs (<xref ref-type="fig" rid="fig7">Figure 7G</xref>) because task-irrelevant neural activity is minimal in these networks. To test this possibility, we conducted in-silico lesion experiments for the trained RNNs. The main idea is that if an RNN contains a large portion of task-irrelevant variance, there will exist a subspace (termed as task-irrelevant subspace) that captures this part of variance and removing this task-irrelevant subspace will not affect the network’s behavior.</p><p>Based on this idea, we developed an optimization method to identify such a task-irrelevant subspace for any given RNN. The main idea is to find a lesion subspace such that removing all neural activity within this subspace does not affect the network’s behavior, while capturing as much variance in neural activity caused by pulse inputs as possible. Specifically, for any given RNN, we constructed a new RNN with its connectivity matrix defined as <inline-formula><alternatives><mml:math id="inf599"><mml:msup><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><tex-math id="inft599">\begin{document}$J^{'}=J\left (E- QQ^{T}\right)$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf600"><mml:mi>J</mml:mi></mml:math><tex-math id="inft600">\begin{document}$J$\end{document}</tex-math></alternatives></inline-formula> is the connectivity matrix of the original RNN, <inline-formula><alternatives><mml:math id="inf601"><mml:mi>E</mml:mi></mml:math><tex-math id="inft601">\begin{document}$E$\end{document}</tex-math></alternatives></inline-formula> is the identity matrix, and <inline-formula><alternatives><mml:math id="inf602"><mml:mi>Q</mml:mi></mml:math><tex-math id="inft602">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> is the orthogonal basis of the lesion subspace. Here, <inline-formula><alternatives><mml:math id="inf603"><mml:mi>Q</mml:mi></mml:math><tex-math id="inft603">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> is of shape <inline-formula><alternatives><mml:math id="inf604"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi></mml:math><tex-math id="inft604">\begin{document}$N\times r$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf605"><mml:mi>r</mml:mi></mml:math><tex-math id="inft605">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> is the dimension of the subspace (<inline-formula><alternatives><mml:math id="inf606"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math><tex-math id="inft606">\begin{document}$r=10$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4</xref>). For the new RNN, the matrix <inline-formula><alternatives><mml:math id="inf607"><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft607">\begin{document}$E- QQ^{T}$\end{document}</tex-math></alternatives></inline-formula> removes all components of neural activity within the lesion subspace. We try to find the best <inline-formula><alternatives><mml:math id="inf608"><mml:mi>Q</mml:mi></mml:math><tex-math id="inft608">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> by minimizing:<disp-formula id="equ76"><label>(76)</label><alternatives><mml:math id="m76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>Q</mml:mi><mml:msubsup><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mo stretchy="false">∥</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t76">\begin{document}$$\displaystyle  \ell =\lambda \frac{(Z_{40}-\hat{Z}_{40} )^{2}}{(\hat{Z}_{40})^{2}} +(1-\frac{\parallel \hat{f}Q\parallel ^{2}_{2} }{\parallel \hat{f}\parallel ^{2}_{2}} )$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf609"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft609">\begin{document}$z_{40}$\end{document}</tex-math></alternatives></inline-formula> is the network’s output at the last time step for pulse input at the beginning, <inline-formula><alternatives><mml:math id="inf610"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft610">\begin{document}$\overset{\hat }{z}_{40}$\end{document}</tex-math></alternatives></inline-formula> is the output without lesion, <inline-formula><alternatives><mml:math id="inf611"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><tex-math id="inft611">\begin{document}$\overset{\hat }{f}$\end{document}</tex-math></alternatives></inline-formula> is a <inline-formula><alternatives><mml:math id="inf612"><mml:mn>40</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math><tex-math id="inft612">\begin{document}$40\times N$\end{document}</tex-math></alternatives></inline-formula> matrix, indicating the neuron activities caused by a pulse input for the RNN without lesion, <inline-formula><alternatives><mml:math id="inf613"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math><tex-math id="inft613">\begin{document}$\lambda =10$\end{document}</tex-math></alternatives></inline-formula> is a hyperparameter balancing the two parts. We only consider input 1 under condition one for simplicity. The first part ensures that after removing neural activity in the <inline-formula><alternatives><mml:math id="inf614"><mml:mi>Q</mml:mi></mml:math><tex-math id="inft614">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> subspace, the network’s output in response to unit pulse remains unchanged. The second part aims to maximize the variance of neuron activity captured by lesion subspace. The orthogonal basis <inline-formula><alternatives><mml:math id="inf615"><mml:mi>Q</mml:mi></mml:math><tex-math id="inft615">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> is parameterized using the Cayley transform: <inline-formula><alternatives><mml:math id="inf616"><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft616">\begin{document}$Q=\left (E+A- A^{T}\right)\left (E- A+A^{T}\right)^{- 1}$\end{document}</tex-math></alternatives></inline-formula>. Matrix <inline-formula><alternatives><mml:math id="inf617"><mml:mi>A</mml:mi></mml:math><tex-math id="inft617">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> is optimized using gradient descent with the Adam optimizer with a learning rate of <inline-formula><alternatives><mml:math id="inf618"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math><tex-math id="inft618">\begin{document}$10^{- 5}$\end{document}</tex-math></alternatives></inline-formula>. The training process lasts for 2000 steps and the maximum of <inline-formula><alternatives><mml:math id="inf619"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mi>Q</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><tex-math id="inft619">\begin{document}$\frac{\left \| \overset{\hat }{f}Q\right \| _{2}^{2}}{\left \| \overset{\hat }{f}\right \| _{2}^{2}}$\end{document}</tex-math></alternatives></inline-formula> under the condition of <inline-formula><alternatives><mml:math id="inf620"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Z</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft620">\begin{document}$\frac{(Z_{40}-\hat{Z}_{40} )^{2} }{(\hat{Z}_{40})^{2}} &lt;10^{-3}$\end{document}</tex-math></alternatives></inline-formula> is defined as the network’s PEV of irrelevant activity (<xref ref-type="fig" rid="fig7s4">Figure 7—figure supplement 4C</xref>).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Investigation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103636-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code and models supporting the findings of this study have been deposited in <ext-link ext-link-type="uri" xlink:href="https://github.com/sillybun/selection_vector_modulation_lowrank_rnn">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib46">Zhang, 2025</xref>). Additional materials are available from the corresponding author upon reasonable request.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Gouki Okazawa and Xuexin Wei for valuable discussions and comments on earlier versions of the manuscript. This work was supported by the Ministry of Science and Technology of China (STI2030-Major Project, 2021ZD0204105) and the National Natural Science Foundation of China (32271149).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aoi</surname><given-names>MC</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prefrontal cortex exhibits multidimensional dynamic encoding during decision-making</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1410</fpage><lpage>1420</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0696-5</pub-id><pub-id pub-id-type="pmid">33020653</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Bhandari</surname><given-names>A</given-names></name><name><surname>Keglovits</surname><given-names>H</given-names></name><name><surname>Kikumoto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The dimensionality of neural representations for control</article-title><source>Current Opinion in Behavioral Sciences</source><volume>38</volume><fpage>20</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.07.002</pub-id><pub-id pub-id-type="pmid">32864401</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Proville</surname><given-names>R</given-names></name><name><surname>Rodgers</surname><given-names>CC</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Early selection of task-relevant features through population gating</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>6837</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-42519-5</pub-id><pub-id pub-id-type="pmid">37884507</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shaping dynamics with multiple populations in low-rank recurrent networks</article-title><source>Neural Computation</source><volume>33</volume><fpage>1572</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01381</pub-id><pub-id pub-id-type="pmid">34496384</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Parametric control of flexible timing through low-dimensional neural manifolds</article-title><source>Neuron</source><volume>111</volume><fpage>739</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.12.016</pub-id><pub-id pub-id-type="pmid">36640766</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardi</surname><given-names>S</given-names></name><name><surname>Benna</surname><given-names>MK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Munuera</surname><given-names>J</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The geometry of abstraction in the hippocampus and prefrontal cortex</article-title><source>Cell</source><volume>183</volume><fpage>954</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id><pub-id pub-id-type="pmid">33058757</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Watanabe</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>From numerosity to ordinal rank: a gain-field model of serial order representation in cortical working memory</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>8636</fpage><lpage>8642</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2110-07.2007</pub-id><pub-id pub-id-type="pmid">17687041</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Min</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Flexible control of sequence working memory in the macaque frontal cortex</article-title><source>Neuron</source><volume>112</volume><fpage>3502</fpage><lpage>3514</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2024.07.024</pub-id><pub-id pub-id-type="pmid">39178858</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Cognitive control: core constructs and current considerations</chapter-title><person-group person-group-type="editor"><name><surname>Egner</surname><given-names>T</given-names></name></person-group><source>The Wiley Handbook of Cognitive Control</source><publisher-name>Wiley</publisher-name><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1002/9781118920497</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The role of population structure in computations through neural dynamics</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>783</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01088-4</pub-id><pub-id pub-id-type="pmid">35668174</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>T</given-names></name><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Orthogonal representations for robust context-dependent task performance in brains and neural networks</article-title><source>Neuron</source><volume>110</volume><fpage>1258</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.01.005</pub-id><pub-id pub-id-type="pmid">35085492</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why neurons mix: high dimensionality for higher cognition</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.01.010</pub-id><pub-id pub-id-type="pmid">26851755</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hariri</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The neurobiology of individual differences in complex behavioral traits</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>225</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135335</pub-id><pub-id pub-id-type="pmid">19400720</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kadmon</surname><given-names>J</given-names></name><name><surname>Timcheck</surname><given-names>J</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predictive coding in balanced neural networks with noise, chaos and delays</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2006.14178">https://doi.org/10.48550/arXiv.2006.14178</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keung</surname><given-names>W</given-names></name><name><surname>Hagen</surname><given-names>TA</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A divisive model of evidence accumulation explains uneven weighting of evidence over time</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2160</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15630-0</pub-id><pub-id pub-id-type="pmid">32358501</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>ID</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Coherent chaos in a recurrent neural network with structured connectivity</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006309</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006309</pub-id><pub-id pub-id-type="pmid">30543634</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>15696</fpage><lpage>15705</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How recurrent networks implement contextual processing in sentiment analysis</article-title><conf-name>Proceedings of the 37th International Conference on Machine Learning</conf-name><fpage>6608</fpage><lpage>6619</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Linking connectivity, dynamics, and computations in low-rank recurrent neural networks</article-title><source>Neuron</source><volume>99</volume><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.003</pub-id><pub-id pub-id-type="pmid">30057201</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>A</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>An approximate line attractor in the hypothalamus encodes an aggressive state</article-title><source>Cell</source><volume>186</volume><fpage>178</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2022.11.027</pub-id><pub-id pub-id-type="pmid">36608653</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Braun</surname><given-names>L</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural knowledge assembly in humans and neural networks</article-title><source>Neuron</source><volume>111</volume><fpage>1504</fpage><lpage>1516</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.02.014</pub-id><pub-id pub-id-type="pmid">36898375</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noudoost</surname><given-names>B</given-names></name><name><surname>Chang</surname><given-names>MH</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Top-down control of visual attention</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>183</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.003</pub-id><pub-id pub-id-type="pmid">20303256</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>G</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural mechanisms that make perceptual decisions flexible</article-title><source>Annual Review of Physiology</source><volume>85</volume><fpage>191</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1146/annurev-physiol-031722-024731</pub-id><pub-id pub-id-type="pmid">36343603</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Computational role of structure in neural activity and connectivity</article-title><source>Trends in Cognitive Sciences</source><volume>28</volume><fpage>677</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2024.03.003</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Tang</surname><given-names>VD</given-names></name><name><surname>Aoi</surname><given-names>MC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A new theoretical framework jointly explains behavioral and neural variability across subjects performing flexible decision-making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.11.28.518207</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2309.04030</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagan</surname><given-names>M</given-names></name><name><surname>Tang</surname><given-names>VD</given-names></name><name><surname>Aoi</surname><given-names>MC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Individual variability of neural computations underlying flexible decisions</article-title><source>Nature</source><volume>639</volume><fpage>421</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-08433-6</pub-id><pub-id pub-id-type="pmid">39608399</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parasuraman</surname><given-names>R</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Individual differences in cognition, affect, and performance: behavioral, neuroimaging, and molecular genetic approaches</article-title><source>NeuroImage</source><volume>59</volume><fpage>70</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.04.040</pub-id><pub-id pub-id-type="pmid">21569853</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>JE</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Prefrontal cortex activity during flexible categorization</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>8519</fpage><lpage>8528</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4837-09.2010</pub-id><pub-id pub-id-type="pmid">20573899</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudelson</surname><given-names>M</given-names></name><name><surname>Vershynin</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sampling from large matrices: An approach through geometric functional analysis</article-title><source>Journal of the ACM</source><volume>54</volume><elocation-id>e55449</elocation-id><pub-id pub-id-type="doi">10.1145/1255443.1255449</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saez</surname><given-names>A</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Abstract context representations in primate amygdala and prefrontal cortex</article-title><source>Neuron</source><volume>87</volume><fpage>869</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.024</pub-id><pub-id pub-id-type="pmid">26291167</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sanyal</surname><given-names>A</given-names></name><name><surname>Dokania</surname><given-names>PK</given-names></name><name><surname>Torr</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stable rank normalization for improved generalization in neural networks and GANs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1906.04659">https://doi.org/10.48550/arXiv.1906.04659</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuessler</surname><given-names>F</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dynamics of random recurrent networks with correlated low-rank structure</article-title><source>Physical Review Research</source><volume>2</volume><elocation-id>013111</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevResearch.2.013111</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical information flow during flexible sensorimotor decisions</article-title><source>Science</source><volume>348</volume><fpage>1352</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1126/science.aab0551</pub-id><pub-id pub-id-type="pmid">26089513</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Soldado-Magraner</surname><given-names>J</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Inferring context-dependent computations through linear approximations of prefrontal cortex dynamics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.02.06.527389</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: a simple and flexible framework</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004792</pub-id><pub-id pub-id-type="pmid">26928718</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takagi</surname><given-names>Y</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Klein-Flügge</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Adapting non-invasive human recordings along multiple task-axes shows unfolding of spontaneous and over-trained choice</article-title><source>eLife</source><volume>10</volume><elocation-id>e60988</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.60988</pub-id><pub-id pub-id-type="pmid">33973522</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Min</surname><given-names>B</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Mental programming of spatial sequences in working memory in the macaque frontal cortex</article-title><source>Science</source><volume>385</volume><elocation-id>eadp6091</elocation-id><pub-id pub-id-type="doi">10.1126/science.adp6091</pub-id><pub-id pub-id-type="pmid">39325894</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Probing the relationship between latent linear dynamical systems and low-rank recurrent neural network models</article-title><source>Neural Computation</source><volume>34</volume><fpage>1871</fpage><lpage>1892</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01522</pub-id><pub-id pub-id-type="pmid">35896161</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Min</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title><source>Science</source><volume>375</volume><fpage>632</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1126/science.abm0204</pub-id><pub-id pub-id-type="pmid">35143322</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Artificial neural networks for neuroscientists: a primer</article-title><source>Neuron</source><volume>107</volume><fpage>1048</fpage><lpage>1070</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.005</pub-id><pub-id pub-id-type="pmid">32970997</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Selection_vector_modulation_lowrank_rnn</data-title><version designator="swh:1:rev:3c3ff39df2d084f863a9d411c606a378011a3a3b">swh:1:rev:3c3ff39df2d084f863a9d411c606a378011a3a3b</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:983d0ffaf607e268820f197d1e909cf5bca1ddf1;origin=https://github.com/sillybun/selection_vector_modulation_lowrank_rnn;visit=swh:1:snp:208ff732d64410170c29011896dd3a2c742832ca;anchor=swh:1:rev:3c3ff39df2d084f863a9d411c606a378011a3a3b">https://archive.softwareheritage.org/swh:1:dir:983d0ffaf607e268820f197d1e909cf5bca1ddf1;origin=https://github.com/sillybun/selection_vector_modulation_lowrank_rnn;visit=swh:1:snp:208ff732d64410170c29011896dd3a2c742832ca;anchor=swh:1:rev:3c3ff39df2d084f863a9d411c606a378011a3a3b</ext-link></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103636.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>École Normale Supérieure - PSL</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This study provides an <bold>important</bold> set of analyses and theoretical derivations to understand the mechanisms used by recurrent neural networks (RNNs) to perform context-dependent accumulation of evidence. The results regarding the dimensionality and neural dynamical signatures of RNNs are <bold>convincing</bold> and provide new avenues to study the mechanisms underlying context-dependent computations. This manuscript will be of interest to a broad audience in systems and computational neuroscience.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103636.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper investigates how recurrent neural networks (RNNs) can perform context-dependent decision-making (CDM). The authors use low-rank RNN modeling and focus on a CDM task where subjects are presented with sequences of auditory pulses that vary in location and frequency, and they must determine either the prevalent location or frequency based on an external context signal. In particular, the authors focus on the problem of differentiating between two distinct selection mechanisms: input modulation, which involves altering the stimulus input representation, and selection vector modulation, which involves altering the &quot;selection vector&quot; of the dynamical system.</p><p>First, the authors show that rank-one networks can only implement input modulation, and that higher-rank networks are required for selection vector modulation. Then, the authors use pathway-based information flow analysis to understand how information is routed to the accumulator based on context. This analysis allows the authors to introduce a novel definition of selection vector modulation that explicitly links it to changes in the effective coupling along specific pathways within the network.</p><p>The study further generates testable predictions for differentiating selection vector modulation from input modulation based on neural dynamics. In particular, the authors find that: (1) A larger proportion of selection vector modulation is expected in networks with high-dimensional connectivity. (2) Single-neuron response kernels exhibiting specific profiles (peaking between stimulus onset and choice onset) are indicative of neural dynamics in extra dimensions, supporting the presence of selection vector modulation. (3) The percentage of explained variance (PEV) of extra dynamical modes extracted from response kernels at the population level can serve as an index to quantify the amount of selection vector modulation.</p><p>Strengths:</p><p>The paper is clear and well written, and it draws bridges between two recent important approaches in the study of CDM: circuit-level descriptions of low-rank RNNs, and differentiation across alternative mechanisms in terms of neural dynamics. The most interesting aspect of the study involves establishing a link between selection vector modulation, network dimensionality and dimensionality of neural dynamics. The high correlation between the networks' mechanisms and their dimensionality (Fig. 7d) is surprising since differentiating between selection mechanisms is generally a difficult task, and the strength of this result is further corroborated by its consistency across multiple RNN hyperparameters (Figure 7-figure supplement 1 and Figure 7-figure supplement 2). Interestingly, the correlation between the selection mechanism and the dimensionality of neural dynamics is also high (Fig. 7g), potentially providing a promising future avenue for the study of neural recordings in this task.</p><p>Weaknesses:</p><p>As acknowledged by the authors, the results linking selection vector modulation and dimensionality might not generalize to neural representations where a significant fraction of the variance encodes information unrelated to the task. Therefore, these tools might not be applicable to neural recordings or to artificial neural networks with additional high-dimensional activity unrelated to the task (e.g. RNNs trained to perform many other tasks).</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103636.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This manuscript examines network mechanisms that allow networks of neurons to perform context-dependent decision-making.</p><p>In a recent study, Pagan and colleagues identified two distinct mechanisms by which recurrent neural networks can perform such computations. They termed these two mechanisms input-modulation and selection-vector modulation. Pagan and colleagues demonstrated that recurrent neural networks can be trained to implement combinations of these two mechanisms, and related this range of computational strategies with inter-individual variability in rats performing the same task. What type of structure in the recurrent connectivity favors one or the other mechanism however remained an open question.</p><p>The present manuscript addresses this specific question by using a class of mechanistically interpretable recurrent neural networks, low-rank RNNs.</p><p>The manuscript starts by demonstrating that unit-rank RNNs can only implement the input-modulation mechanism, but not the selection-vector modulation. The authors then build rank three networks which implement selection-vector modulation, and show how the two mechanisms can be combined. Finally, they relate the amount of selection-vector modulation with the effective rank, ie the dimensionality of activity, of a trained full-rank RNN.</p><p>Strength:</p><p>- The manuscript is written in an obvious manner</p><p>- The analytic approach adopted in the manuscript is impressive</p><p>- Very clear identification of the mechanisms leading to the two types of context-dependent modulation</p><p>- Altogether, this manuscript reports remarkable insights on a very timely question</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103636.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Yiteng</given-names></name><role specific-use="author">Author</role><aff><institution>Fudan University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Jianfeng</given-names></name><role specific-use="author">Author</role><aff><institution>Fudan University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Min</surname><given-names>Bin</given-names></name><role specific-use="author">Author</role><aff><institution>Lingang Laboratory</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 1 (Public review):</bold></p><p>The first part of the manuscript is not particularly novel, and it would be beneficial to clearly state which aspects of the analyses and derivations are different from previous literature. For example, the derivation that rank-1 RNNs cannot implement selection vector modulation is already present in the Extended Discussion of Pagan et al., 2022 (Equations 42-43). Similarly, it would be helpful to more clearly explain how the proposed pathway-based information flow analysis differs from the circuit diagram of latent dynamics in Dubreuil et al., 2022.</p></disp-quote><p>We thank the reviewer for the insightful comments and providing us a good opportunity to better clarify the novelty of our work regarding the analyses and derivations. In general, as the reviewer pointed out, the major novelty of our work lies in explicitly linking selection mechanisms (proposed in Mante et al. 2013) with circuit-level descriptions of low-rank RNNs (developed in Dubreuil et al. 2022). This is made possible through a set of analyses and derivation integrating both linearized dynamical systems analysis (Mante et al., 2013) and the circuit diagram of latent dynamics (Dubreuil et al. 2022). Specifically, starting from rank-3 RNN models, we first derived the circuit diagram of latent dynamics (Eqs. 18 and 19) by applying the theory developed in Dubreuil et al. 2022. However, without further analysis, there is no explicit link between this latent dynamics and selection mechanism. In this manuscript, based on the line attractor assumption, we linearized the latent dynamics around the line attractor (Mante et al., 2013), which enabled us to explicitly solve the equation (from eq. 20 to eq. 27) and derive an explicit formula for the effective coupling of information flow (Fig. 5A). This formula of effective coupling strength supported an explicit pathway-based definition of selection vector modulation (Fig. 5) and selection vector (Fig. 6), the core result of this manuscript. Importantly, the same analysis can be extended to higher-order lowrank RNNs (Eqs. 47-55), suggesting the general applicability of our result. We have revised the manuscript to clearly state the novelty of our work. Please see Lines 292-294.</p><p>As such a set of analyses and derivation integrates many results from previous literatures, it naturally shared many similarities with previous results as the reviewer pointed out. Below, we compared our work with previous ones mentioned by the reviewer:</p><disp-quote content-type="editor-comment"><p>(1) For example, the derivation that rank-1 RNNs cannot implement selection vector modulation is already present in the Extended Discussion of Pagan et al., 2022 (Equations 42-43).</p></disp-quote><p>For this point, we totally agree with the reviewer that the derivation of rank-1 RNNs’ limitations in implementing selection vector modulation is not particularly novel. The reason why we started from rank-1 RNNs is because these RNNs are the simplest examples revealing the intriguing link between the connectivity property and the modulation mechanism and thereby serving as the ideal introduction for the subsequent in-depth discussion for general audiences. In the original manuscript, we cited the Pagan et al. 2023 note but may not make it explicit enough. As the reviewer pointed out that the derivation has been added into the latest version of Pagan et al. paper (Pagan et al. 2024), we now cite the Pagan et al. 2024 paper and make it clear that the derivation has been derived in Pagan et al. 2024. Please see Lines 186-188 in the main text.</p><disp-quote content-type="editor-comment"><p>(2) Similarly, it would be helpful to more clearly explain how the proposed pathway-based information flow analysis differs from the circuit diagram of latent dynamics in Dubreuil et al., 2022.</p></disp-quote><p>As we explained earlier, the latent dynamics in Dubreuil et al. alone did not provide an explicit link between circuit diagram and selection mechanisms. Our analysis go beyond the theory developed in Dubreuil et al. 2022 paper by integrating the linearized dynamical systems analysis (Mante et al. 2013), eventually providing a previously-unknown explicit link between circuit diagram and selection mechanisms.</p><disp-quote content-type="editor-comment"><p>With regard to the results linking selection vector modulation and dimensionality, more work is required to understand the generality of these results, and how practical it would be to apply this type of analysis to neural recordings. For example, it is possible to build a network that uses input modulation and to greatly increase the dimensionality of the network simply by adding additional dimensions that do not directly contribute to the computation. Similarly, neural responses might have additional high-dimensional activity unrelated to the task. My understanding is that the currently proposed method would classify such networks incorrectly, and it is reasonable to imagine that the dimensionality of activity in high-order brain regions will be strongly dependent on activity that does not relate to this task.</p></disp-quote><p>We thank the reviewer for this insightful comment. As what the reviewer suggested, we did more work to better understand the generality and applicability of the index proposed in the manuscript.</p><p>Firstly, to see if the currently proposed method can work when there is significant amount of neural activity variance irrelevant to the task, we manually added irrelevant neural activity into the trained RNNs (termed as redundant RNNs, see Methods for details, Lines 1200-1215). As expected, we found that for these redundant RNNs, the correlation between the proposed index and the proportion of selection vector modulation indeed disappeared (Figure 7-figure supplement 4B). In fact, in the original version of our manuscript, we presented an extreme example of this idea in our discussion, where we designed two RNNs with theoretically identical neural activity patterns—one relying purely on input modulation and the other on selection vector modulation (Figure 7-figure supplement 3). Therefore, for this extreme example, any activity-based index alone would fail to differentiate between these two mechanisms, suggesting the challenge of distinguishing different selection mechanisms when taskirrelevant neural activity is added.</p><p>Secondly, we asked why the proposed index works well for the trained RNNs, which is kind of surprising in the first place as the reviewer pointed out. One possibility is that for trained RNNs, the task-irrelevant neural activity is minimal. To test this possibility, we conducted in-silico lesion experiments for the trained RNNs. The main idea is that if an RNN contains a large portion of taskirrelevant variance, there will exist a subspace (termed as task-irrelevant subspace) that captures this part of variance and removing this task-irrelevant subspace will not affect the network’s behavior. Based on this idea, we developed an optimization method to identify such a task-irrelevant subspace for any given RNN (see Methods for details, Lines 1216-1244). The results show that in the originally trained RNNs, the identified task-irrelevant subspace can only explain a small portion of neural activity variance (Figure 7-figure supplement 4, panel C). As a control, when applying the same optimization method to the redundant RNNs, we found that the identified task-irrelevant subspace can explain a significantly larger portion of neural activity variance (Figure 7-figure supplement 4, panel C). Taken together, we concluded that the reason why the index works for trained RNNs is because the major variance of the neural activity of the network learned through backpropagation is task-relevant.</p><p>Therefore, this set of analyses provided an understanding why the proposed index works for trained RNNs and failed for the redundant RNNs. We have added this part of analyses in the Discussion part. See Lines 601-610. As the reviewer pointed out that it is highly likely that there exists taskirrelevant neural activity variance in high brain regions, the proposed index may not work well in neural recordings. With this understanding, we tone down the conclusion related to experimentally testable prediction in the main text (e.g., in Abstract and Introduction). We thank the reviewer again for helping us improve the clarity of our work.</p><disp-quote content-type="editor-comment"><p>Finally, a number of aspects of the analysis are not clear. The most important element to clarify is how the authors quantify the &quot;proportion of selection vector modulation&quot; in vanilla RNNs (Figures 7d and 7g). I could not find information about this in the Methods, yet this is a critical element of the study results. In Mante et al., 2013 and in Pagan et al., 2022 this was done by analyzing the RNN linearized dynamics around fixed points: is this the approach used also in this study? Also, how are the authors producing the trial-averaged analyses shown in Figures 2f and 3f? The methods used to produce this type of plot differ in Mante et al., 2013 and Pagan et al., 2022, and it is necessary for the authors to explain how this was computed in this case.</p></disp-quote><p>We thank the reviewer for the valuable comments. Yes, for proportion of selection vector modulation (Figure 7D and 7G) we employed the method used in Mante et al., 2013. For the trial-averaged analyses shown in Figures 2f and 3f, we followed a procedure used in Mante et al., 2013. In the revised version, we have added the relate information. See Lines 852-853 and 872-889. We thank the reviewer again for improving the clarify of our work.</p><disp-quote content-type="editor-comment"><p>I am also confused by a number of analyses done to verify mathematical derivations, which seem to suggest that the results are close to identical, but not exactly identical. For example, in the histogram in Figure 6b, or the histogram in Figure 7-figure supplement 3d: what is the source of the small variability leading to some of the indices being less than 1?</p></disp-quote><p>In Figure 6B, the two selection vectors are considered theoretically equivalent under the meanfield assumption. However, because the RNNs we use have a finite number of neurons, finite-size effects inevitably cause slight deviations from perfect equivalence.</p><p>To verify this, we generated rank-3 RNNs of different sizes in the experiment for Figure 6b (see the Supplementary section “Building rank-3 RNNs with both input and selection vector modulations”). Specifically, for a fixed number of neurons 𝑁, we independently sampled 𝛼, 𝛽 and 𝛾 from a Uniform(0,1) distribution and built an RNN with 𝑁 neurons based on the procedure as in <xref ref-type="fig" rid="fig5">Figure 5C</xref>. We then computed the selection vector for the RNN in a given context (for example, context 1) in two ways:</p><list list-type="simple" id="list1"><list-item><p>(1) via linearized dynamical system analysis following Mante et al. (2013), producing the selection vector sc<sup>classical</sup></p></list-item><list-item><p>(2) using the theoretical derivation <inline-formula><alternatives><mml:math id="sa3m1"><mml:mi>s</mml:mi><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft621">\begin{document}$s v=n_{d v}+\eta n_{i v_{1}}$\end{document}</tex-math></alternatives></inline-formula></p></list-item></list><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>cos angles for selection vectors computed using two methods in RNN with different size.</title><p>Black bars indicate median values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-sa3-fig1-v1.tif"/></fig><p>We repeated this process 1000 times for each 𝑁 and measured the cosine angle between these two selection vectors. As shown in Author response image 1, as 𝑁 increases, the cosine angles approach 1 more consistently, indicating that the two selection vectors become nearly equivalent in larger RNNs. Conversely, smaller RNNs display more pronounced finite-size effects, which accounts for indices slightly below 1.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 2 (Public review):</bold></p><p>The introduction could have been written in a more accessible manner for any non-expert readers.</p></disp-quote><p>We sincerely thank the reviewer for the constructive feedback on the introduction and have revised it accordingly.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>The level of mastery of the low-rank framework is altogether impressive. I need however to point to a technical detail. The derivations of the information flow assume that the vectors m and vectors I are orthogonal (e.g. in Equation 14). This is not necessarily the case in trained networks, and Figure 2F suggests this is not the case in the trained rank 1 network. In that situation, the overlap between m and I leads to an additional term in the Equation going directly from the input to the output vector (see, e.g., Equation 15 in Beiran et al. Neuron 2023). In general, these kind of overlaps can contribute an additional pathway in higher rank networks too.</p></disp-quote><p>We thank the reviewer for the valuable comments. The derivations presented in Equation 14 do not actually require that the vectors 𝒎 and 𝑰 are orthogonal. Rather, our definition of the task variable differs slightly from the one in Beiran et al. (2023). Consider a rank-1 RNN with a single input channel:<disp-formula id="sa3equ1"><alternatives><mml:math id="sa3m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi mathvariant="bold-italic">m</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>u</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t77">\begin{document}$$\displaystyle  \tau \frac{d \boldsymbol{x}}{d t}=-x+\frac{1}{N} \boldsymbol{m} \boldsymbol{n}^{T} \phi(\boldsymbol{x})+\boldsymbol{I} u .$$\end{document}</tex-math></alternatives></disp-formula></p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><caption><title>Difference of the definition of task variable with previous work.</title><p>(A) Our definition of task variable. (B) Definition of task variable in Beiran et al. 2023.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-sa3-fig2-v1.tif"/></fig><p>As long as 𝒎 and 𝑰 are linearly independent, the state 𝒙(𝑡) can be uniquely written as a linear combination of the two vectors (Author response image 2):<disp-formula id="sa3equ2"><alternatives><mml:math id="sa3m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>inp </mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t78">\begin{document}$$\displaystyle  \boldsymbol{x}(t)=\kappa_{d v}(t) \boldsymbol{m}+\kappa_{\text {inp }}(t) \boldsymbol{I},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="sa3m4"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft622">\begin{document}$k_{d v}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="sa3m5"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft623">\begin{document}$k_{i n p}$\end{document}</tex-math></alternatives></inline-formula> are the task variables associated with 𝒎 and 𝑰, respectively. Substituting this expression into the dynamical equations yields:<disp-formula id="sa3equ3"><alternatives><mml:math id="sa3m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t79">\begin{document}$$\displaystyle   \tau \frac{d \kappa_{d v}}{d t} &amp;=-\kappa_{d v}+\frac{1}{N} \boldsymbol{n}^{T} \phi(\boldsymbol{x}), \\  \tau \frac{d \kappa_{i n p}}{d t} &amp; =-\kappa_{i n p}+u$$\end{document}</tex-math></alternatives></disp-formula></p><p>Hence, there is no additional term directly linking the input to the output vector in our formulation. By contrast, in Beiran et al. (2023), the input vector 𝑰 is decomposed into components parallel (𝐼//) and perpendicular (𝑰-) to 𝒎, and the task variables <inline-formula><alternatives><mml:math id="sa3m7"><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft624">\begin{document}$\kappa_{d v}^{\prime}, \kappa_{i n p}^{\prime}$\end{document}</tex-math></alternatives></inline-formula> are defined as (Figure 4-figure supplement 3B):<disp-formula id="sa3equ4"><alternatives><mml:math id="sa3m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>inp </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mo>⊥</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t80">\begin{document}$$\displaystyle  \boldsymbol{x}(t)=\kappa_{d v}^{\prime}(t) \boldsymbol{m}+\kappa_{\text {inp }}^{\prime}(t) \boldsymbol{I}^{\perp}$$\end{document}</tex-math></alternatives></disp-formula></p><p>This leads to dynamics of the form:<disp-formula id="sa3equ5"><alternatives><mml:math id="sa3m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">m</mml:mi></mml:mrow></mml:mfrac><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>u</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t81">\begin{document}$$\displaystyle   \tau \frac{d \kappa_{d v}^{\prime}}{d t} &amp;=-\kappa_{d v}^{\prime}+\frac{1}{N} \boldsymbol{n}^{T} \phi(\boldsymbol{x})+\frac{\boldsymbol{m}^{T} \boldsymbol{I}}{\boldsymbol{m}^{T} \boldsymbol{m}} u \\  \tau \frac{d \kappa_{i n p}^{\prime}}{d t} &amp;=-\kappa_{i n p}^{\prime}+u $$\end{document}</tex-math></alternatives></disp-formula></p><p>thus creating an additional direct term from the input to the output vector under their definition.</p><disp-quote content-type="editor-comment"><p>The designed rank 3 network relies on a multi-population structure. This is explained clearly in the methods, but it could be stressed more in the main text to dispel the notion that higherrank networks may not need a multi-population structure to perform this task (cf Dubreuil et al 2022).</p></disp-quote><p>Thank you for the valuable comments. In the revised version, we emphasize this point by adding the following sentence: “our rank-3 network relies on a multi-population structure, consistent with the notion that higher-rank networks still require a multi-population structure to perform flexible computations (Dubreuil et al. 2022)”. See Lines 238-240.</p><disp-quote content-type="editor-comment"><p>(3) An important result in Pagan et al and Mante et al is that the line attractor direction is invariant across contexts. I believe this is explicitly enforced in the models studied here, but this could be made more clear. It would be interesting to discuss the importance of this constraint.</p></disp-quote><p>We thank the reviewer for the valuable comments. In our hand-crafted RNN examples (Figures 3– 6), we enforce the choice axis to be identical across the two contexts (Figure R4B). Even in the rank-1 example (Figure 2), where we analyze a trained RNN, the choice axis still shows a substantial overlap between the two contexts (Figure R4A). However, in the trained vanilla RNNs shown in Figure 7, when the regularization term is relatively small, the overlap in the choice axis between contexts is smaller (Figure R4C)—i.e., the line attractor direction shifts between different contexts.</p><fig id="sa3fig3" position="float"><label>Author response image 3.</label><caption><title>Cosine angle between the choice axes in two contexts for different RNNs.</title><p>(A) Rank-1 RNNs in Figure 2. (B) Rank-3 RNNs in Figure 3-6. (C) Vanilla RNNs in Figure 7.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103636-sa3-fig3-v1.tif"/></fig><p>Our theoretical framework can also accommodate situations where the direction of the choice axis changes. For instance, consider the rank-3 RNN in Figure 6, where the choice axis <inline-formula><alternatives><mml:math id="sa3m10"><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft625">\begin{document}$\widetilde{\boldsymbol{m}}_{d v}$\end{document}</tex-math></alternatives></inline-formula> is defined as <inline-formula><alternatives><mml:math id="sa3m11"><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft626">\begin{document}$\tilde{\boldsymbol{m}}_{d v}=G \boldsymbol{m}_{d v}$\end{document}</tex-math></alternatives></inline-formula> with 𝐺 being a diagonal matrix whose elements represent the slopes of each neuron’s activation function. Since these slopes can change across contexts, <inline-formula><alternatives><mml:math id="sa3m12"><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft627">\begin{document}$\widetilde{\boldsymbol{m}}_{d v}$\end{document}</tex-math></alternatives></inline-formula> itself can vary across contexts. Likewise, the input representation direction may be written as <inline-formula><alternatives><mml:math id="sa3m13"><mml:msub><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft628">\begin{document}$\tilde{I}_{1}=G I_{1}$\end{document}</tex-math></alternatives></inline-formula>, allowing both the choice axis and the input axis to adapt to the context. The selection vector is given by:<disp-formula id="sa3equ6"><alternatives><mml:math id="sa3m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t82">\begin{document}$$\displaystyle  s=\frac{1}{N}\left\|\widetilde{\boldsymbol{m}}_{d v}\right\|_{2}\left(n_{d v}+E_{i v_{1} \rightarrow d v} n_{i v_{1}}+E_{i v_{2} \rightarrow d v} n_{i v_{2}}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, we no longer assume that <inline-formula><alternatives><mml:math id="sa3m15"><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft629">\begin{document}$\widetilde{\boldsymbol{m}}_{d v}$\end{document}</tex-math></alternatives></inline-formula> is context-invariant; rather, we only assume its norm <inline-formula><alternatives><mml:math id="sa3m16"><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft630">\begin{document}$\left\|\tilde{\boldsymbol{m}}_{d v}\right\|_{2}$\end{document}</tex-math></alternatives></inline-formula> remains the same across contexts. Under this weaker assumption, we still have<disp-formula id="sa3equ7"><alternatives><mml:math id="sa3m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mover><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t83">\begin{document}$$\displaystyle   N \overline{\boldsymbol{s}} &amp;=\left\|\widetilde{\boldsymbol{m}}_{d v}\right\|_{2}\left(\boldsymbol{n}_{d v}+\bar{E}_{i v_{1} \rightarrow d v} \boldsymbol{n}_{i v_{1}}+\bar{E}_{i v_{2} \rightarrow d v} \boldsymbol{n}_{i v_{2}}\right) \\  N \Delta \boldsymbol{s} &amp;=\left\|\widetilde{\boldsymbol{m}}_{d v}\right\|_{2}\left(\Delta E_{i v_{1} \rightarrow d v} n_{i v_{1}}+\Delta E_{i v_{2} \rightarrow d v} n_{i v_{2}}\right) $$\end{document}</tex-math></alternatives></disp-formula></p><p>Substituting these into the equations yields the following expressions for input modulation and selection vector modulation:<disp-formula id="sa3equ8"><alternatives><mml:math id="sa3m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo lspace="thickmathspace" rspace="thickmathspace">mod</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mo lspace="thickmathspace" rspace="thickmathspace">mod</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t84">\begin{document}$$\displaystyle   \bmod _{i n p} &amp;=\left\|\widetilde{\boldsymbol{m}}_{d v}\right\|_{2}\left(\Delta E_{i n p_{1} \rightarrow d v}+\Delta E_{i n p_{1} \rightarrow i v_{1}} \bar{E}_{i v_{2} \rightarrow d v}\right) \\  \bmod _{s e l} &amp;=\left\|\widetilde{\boldsymbol{m}}_{d v}\right\|_{2}\left(\bar{E}_{i n p_{1} \rightarrow i v_{1}} \Delta E_{i v_{1}}\right) $$\end{document}</tex-math></alternatives></disp-formula></p><disp-quote content-type="editor-comment"><p>Figure 6B: it was not clear to me what exactly is plotted here.</p></disp-quote><p>We thank the reviewer for pointing out the missing explanation. In Figure 6B, we show the distribution of the cosine angles between two ways of computing the selection vector for randomly generated rank-3 RNNs. Specifically, We generate 1000 RNNs according to the procedure in Figure 5C, with each RNN defined by parameters 𝛼 , 𝛽 and 𝛾 independently sampled from a Uniform(0,1) distribution. For each RNN, we computed the selection vector for the RNN in a given context (e.g., context 1 or 2) in two ways:</p><p>(1) via linearized dynamical system analysis following Mante et al. (2013), producing the selection vector sv&lt;supclassical (classical in Figure 6B),</p><p>(2) using the theoretical derivation <inline-formula><alternatives><mml:math id="sa3m19"><mml:mi>s</mml:mi><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="inft631">\begin{document}$s v=n_{d v}+\eta n_{i v_{1}}$\end{document}</tex-math></alternatives></inline-formula> (“our’s” in Figure 6B).</p><p>We repeated this process 1000 times and measured the cosine angle between these two selection vectors and plot the resulting distribution for context 1 (gray) and context 2 (blue) in Figure 6B. The figure shows that the computed selection vectors via the two methods are almost equal, as evidenced by the cosine angles clustering very close to 1.</p><p>We have revised it accordingly. See Lines 1135-1143.</p><disp-quote content-type="editor-comment"><p>In Figure 7, how was the effective dimension of vanilla RNNs controlled or varied? The metric used (effective dimension) is relatively non-standard, it would be useful to give some intuition to the reader about it.</p></disp-quote><p>We thank the reviewer for these valuable comments.</p><p>Controlling the effective dimension</p><p>When train vanilla RNNs, we included a regularization term in the loss function of the form<disp-formula id="sa3equ9"><alternatives><mml:math id="sa3m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t85">\begin{document}$$\displaystyle  L_{r e g}=w_{r e g} \sum_{i j} J_{i j}^{2},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where 𝑤536 is a regularization coefficient. By adjusting 𝑤536, we can influence the distribution of singular values of connectivity of 𝐽. When <italic>wreg</italic> is larger, the learned 𝐽 tends to have fewer large singular values, hence with lower effectivity dimension; when 𝑤536 is small, more singular values remain large, increasing the matrix’s effective dimension.</p><p>Definition and intuition: effective dimension</p><p>Consider a connectivity matrix 𝐽 with singular values <inline-formula><alternatives><mml:math id="sa3m21"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>⋯</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft632">\begin{document}$\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{N}$\end{document}</tex-math></alternatives></inline-formula>. The matrix’s rank is the number of nonzero singular values. However, rank alone can overlook differences in how quickly those singular values decay. To capture this, we define the effective dimension as:<disp-formula id="sa3equ10"><alternatives><mml:math id="sa3m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t86">\begin{document}$$\displaystyle  \mathrm{edim}(J)=\sum_{i=1}^{N} \frac{\sigma_{i}^{2}}{\sigma_{1}^{2}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Each term lies between 0 and 1, so the effective dimension satisfies:<disp-formula id="sa3equ11"><alternatives><mml:math id="sa3m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>edim</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>rank</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t87">\begin{document}$$\displaystyle  0 \leq \operatorname{edim}(J) \leq \operatorname{rank}(J)$$\end{document}</tex-math></alternatives></disp-formula></p><p>When all nonzero singular values are equal, edim(𝐽) equals the matrix rank. But if some singular values are much smaller than others, effective dimension will be closer to 1. For example:</p><p>- 𝐽<sub>1</sub> has nonzero singular values (1, 0.1, 0.01). Its effective dimension is 1.0101, indicating that most of the variance is captured by the largest singular value.</p><p>- 𝐽sub&gt;0 has nonzero singular values (1, 0.8, 0.7). Its effective dimension is 2.13, which reflects that multiple singular values contribute significantly.</p><p>Hence, while both &gt;𝐽<sub>1</sub> and 𝐽sub&gt;0 are rank-3 matrices, their effective dimensions highlight the difference in how each matrix distributes its variance.</p><p>We have added the intuition underlying this concept in Methods (see Lines 1135-1143). We thank the reviewer for improving the clarity of our work.</p><p>Eqs 19&amp;21: n^T_r should be n^T_dv?</p><p>Thank you for point out this mistake. We have fixed it in the revised version.</p></body></sub-article></article>