<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103660</article-id><article-id pub-id-type="doi">10.7554/eLife.103660</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103660.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural dynamics of reversal learning in the prefrontal cortex and recurrent neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kim</surname><given-names>Christopher M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1322-6207</contrib-id><email>chrismkkim@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Chow</surname><given-names>Carson C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1463-9553</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Averbeck</surname><given-names>Bruno B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3976-8565</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00adh9b73</institution-id><institution>Laboratory of Biological Modeling, National Institute of Diabetes and Digestive and Kidney Diseases, National Institutes of Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>Laboratory of Neuropsychology, National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ponte Costa</surname><given-names>Rui</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Department of Mathematics, Howard University, Washington, DC, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>09</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP103660</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-10-04"><day>04</day><month>10</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-09-15"><day>15</day><month>09</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.14.613033"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-06"><day>06</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103660.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-06-10"><day>10</day><month>06</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103660.2"/></event></pub-history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103660-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-103660-figures-v1.pdf"/><abstract><p>In probabilistic reversal learning, the choice option yielding reward with higher probability switches at a random trial. To perform optimally in this task, one has to accumulate evidence across trials to infer the probability that a reversal has occurred. We investigated how this reversal probability is represented in cortical neurons by analyzing the neural activity in the prefrontal cortex of monkeys and recurrent neural networks trained on the task. We found that in a neural subspace encoding reversal probability, its activity represented integration of reward outcomes as in a line attractor model. The reversal probability activity at the start of a trial was stationary, stable, and consistent with the attractor dynamics. However, during the trial, the activity was associated with task-related behavior and became non-stationary, thus deviating from the line attractor. Fitting a predictive model to neural data showed that the stationary state at the trial start serves as an initial condition for launching the non-stationary activity. This suggested an extension of the line attractor model with behavior-induced non-stationary dynamics. The non-stationary trajectories were separable indicating that they can represent distinct probabilistic values. Perturbing the reversal probability activity in the recurrent neural networks biased choice outcomes demonstrating its functional significance. In sum, our results show that cortical networks encode reversal probability in stable stationary state at the start of a trial and utilize it to initiate non-stationary dynamics that accommodates task-related behavior while maintaining the reversal information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reversal learning</kwd><kwd>neural dynamics</kwd><kwd>recurrent neural networks</kwd><kwd>prefrontal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00adh9b73</institution-id><institution>National Institute of Diabetes and Digestive and Kidney Diseases</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kim</surname><given-names>Christopher M</given-names></name><name><surname>Chow</surname><given-names>Carson C</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Averbeck</surname><given-names>Bruno B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural activity during reversal learning encodes decision-related evidence integrated across trials and shows substantial dynamics during each trial, suggesting an extension of the line attractor model that incorporates non-stationary dynamics.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>To survive in a dynamically changing world, animals must interact with the environment and learn from their experience to adjust their behavior. Reversal learning has been used for assessing the ability to adapt one’s behavior in such an environment (<xref ref-type="bibr" rid="bib8">Butter, 1969</xref>; <xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Groman et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>; <xref ref-type="bibr" rid="bib50">Su and Cohen, 2022</xref>; <xref ref-type="bibr" rid="bib20">Hyun et al., 2023</xref>). For instance, in two-armed bandit tasks with probabilistic reward, the subject learns from initial trials that one option has higher reward probability than the other. When the reward probability of the two options is reversed at a random trial, the subject must learn to reverse its preferred choice to maximize reward outcome. In these tasks, there is uncertainty in when to reverse one’s choice, as reward is received stochastically even when the less favorable option is chosen. Therefore, it is essential that reward outcomes are integrated over multiple trials before the initial choice preference is reversed. Although neural mechanisms for accumulating evidence within a trial have been studied extensively (<xref ref-type="bibr" rid="bib55">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib21">Inagaki et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Sutton, 1988</xref>), it remains unclear if a recurrent neural circuit uses a similar neural mechanism for accumulating evidence across multiple trials, while performing task-related intervening behavior during each trial.</p><p>In this study, we merged two classes of computational models, i.e., behavioral and neural, to investigate the neural basis of multi-trial evidence accumulation. Behavior models capture subject’s behavioral strategies for performing the reversal learning task. For instance, model-free reinforcement learning (RL) (<xref ref-type="bibr" rid="bib42">Rescorla, 1972</xref>; <xref ref-type="bibr" rid="bib1">Averbeck, 2017</xref>; <xref ref-type="bibr" rid="bib22">Jang et al., 2015</xref>) assumes that the subject learns only from choices and reward outcomes without specific knowledge about task structure. Model-based Bayesian inference (<xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Wilson et al., 2010</xref>; <xref ref-type="bibr" rid="bib61">Wong and Wang, 2006</xref>), in contrast, assumes that the task structure is known to the subject, and one can infer reversal points statistically, resulting in abrupt switches in choice preference. Model-based and model-free RL models are formal models that do not specify implementation in a network of neurons. On the other hand, neural models implemented with recurrent neural networks (RNNs) can be trained to use recurrent activity to perform the reversal learning task. In particular, attractor dynamics, in which the network state moves towards discrete (<xref ref-type="bibr" rid="bib31">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib45">Seung, 1996</xref>) or along continuous (<xref ref-type="bibr" rid="bib21">Inagaki et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Bollimunta et al., 2012</xref>) attractor states, have been studied extensively as a potential neural mechanism for decision-making and evidence accumulation (<xref ref-type="bibr" rid="bib5">Brody and Hanks, 2016</xref>; <xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>).</p><p>We sought to train continuous time RNNs to mimic the behavioral strategies of monkeys performing the reversal learning task. Previous studies (<xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>) have shown that a Bayesian inference model can capture a key aspect of the monkey’s behavioral strategy, i.e., adhere to the preferred choice until the reversal of reward schedule is detected and then switch abruptly. We trained the RNNs to replicate this behavioral strategy by training them on target behaviors generated from the Bayesian model.</p><p>We found that the activity in the neural subspace representing reversal probability could be explained by integrating reward outcomes across trials. At the start of a trial, when the subject was holding fixation before cues were shown, the reversal probability activity was stationary and stable. This stationary activity mode was compatible with the line attractor model that accumulates evidence along attracting states. However, during the trial, the neural activity representing reversal probability had substantial dynamics and was associated with task-related behavior, such as making decisions or receiving feedback. The non-stationary activity during the trial made the line attractor model, which requires the network state to stay close to attractor states (<xref ref-type="bibr" rid="bib21">Inagaki et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib45">Seung, 1996</xref>; <xref ref-type="bibr" rid="bib4">Bollimunta et al., 2012</xref>), inadequate for explaining the neural activity encoding evidence accumulation in reversal learning.</p><p>To better understand how reversal probability is represented in two different activity modes, we investigated the underlying dynamics that link the two. We found that the non-stationary trajectory can be predicted from the stationary activity at the trial start, suggesting that underlying dynamics, associated with task-related behavior, generates the non-stationary activity. In addition, separable points at the initial state remained separable as they propagated in time, allowing distinct probabilistic values to be represented in the trajectories. These findings suggested an extension of the line attractor model where stationary activity on the line attractor provides an initial state from which non-stationary dynamics that preserves separability is initiated. Finally, perturbing reversal probability activity causally affected choice outcomes, demonstrating its functional significance.</p><p>Our results show that, in a probabilistic reversal learning task, cortical networks encode the reversal probability by adopting, not only stationary states as in a line attractor, but also separable dynamic trajectories that can represent distinct probabilistic values and accommodate non-stationary dynamics associated with task-related behavior.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Trained RNN’s choices are consistent with monkey behavior</title><p>We considered a reversal learning task that monkeys were trained to perform in a previous study (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>). In our study, we trained the RNNs to perform a task similar to the task the monkeys performed. A reversal learning task was completed by executing a block of a fixed number of trials. In each trial, the subject (monkey or RNN) chose one of the two available options, and reward for the choice was delivered stochastically. At the beginning of a block, one option was designated as the high-value option, while the other option was designated as the low-value option. The high-value option was rewarded 70% of the time when chosen, and the low-value option was rewarded 30% of the time when chosen. The reward probability of two options was switched at a random trial near the mid-trial of a block, and the reversed reward schedule was maintained until the end of the block. The reversal of reward schedule occurred only once in our task, differing from other versions of reversal learning where reversal occurs multiple times across trials (<xref ref-type="bibr" rid="bib50">Su and Cohen, 2022</xref>; <xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>). The subject had to learn to switch its preferred choice to maximize reward. Because reward delivery was stochastic, the subject had to infer the reversal of reward schedule by accumulating evidence that a reversal had occurred.</p><p>The sequence of events occurring in a trial was structured similarly in the RNN and the monkey tasks. As described in <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>, in each trial, the monkeys were first shown a signal that required them to fixate for a variable time (400 - 800 ms). Then, two cues were presented to both sides of the fixation dot simultaneously. The monkeys made a saccade to an option to report their choice. After holding for 500 ms, the reward was delivered (see Methods Experiment setup for monkeys for details). In the RNNs, they were first stimulated by a cue signaling them to make a choice. After a short delay, the RNNs made a choice between two options. Then, the RNNs received feedback that signaled the choice it made and whether the outcome was rewarded (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This same trial structure was repeated across all trials in a block (see Methods Recurrent neural network for details).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Comparison of the behavior of trained recurrent neural networks (RNNs) and monkeys.</title><p>(<bold>A</bold>) Schematic of RNN training setup. In a trial, the network makes a choice in response to a cue. Then, a feedback input, determined by the choice and reward outcome, is injected to the network. This procedure is repeated across trials. The panel on the right shows this sequence of events unfolding in time in a trial. (<bold>B</bold>) Left: Example of a trained RNN's choice outcomes. Vertical bars show RNN choices in each trial and the reward outcomes (magenta: choice A, blue: choice B, light: rewarded, dark: not rewarded). Horizontal bars on the top show reward schedules (magenta: choice A receiving reward is 70%, choice B receiving reward is 30%; blue: reward schedule is reversed). Black curve shows the RNN output. Green horizontal bars show the posterior of reversal probability at each trial inferred using Bayesian model. Right: Schematic of RNN training scheme. The scheduled reversal indicates the trial at which the reward probabilities of two options switch (color codes for magenta and cyan are the same as the left panel). The inferred reversal is the scheduled reversal trial inferred from the Bayesian model. The behavioral reversal is determined by adding a few delay trials to the inferred reversal trial. The target output, on which the RNNs outputs are trained, switches at the behavioral reversal trial. (<bold>C</bold>) Probability of choosing the initial best (i.e. high-value) option. Relative trial indicates the trial number relative to the behavioral reversal trial inferred from the Bayesian model. Relative trial number 0 is the trial at which the choice was reversed. Shaded region shows the S.E.M (standard error of mean) over blocks in all the sessions (monkeys) or networks (RNNs). (<bold>D</bold>) Fraction of no-reward blocks as a function of relative trial. Dotted lines show 0.3 and 0.7. Shaded region shows the S.E.M (standard error of mean) over blocks in all the sessions (monkeys) or networks (RNNs). (<bold>E</bold>) Distribution of RNN's and monkey's reversal trial, relative to the experimentally scheduled reversal trial.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig1-v1.tif"/></fig><p>The RNN and the monkeys tasks had differences. For the RNNs, there were 36 trials in a block during testing (24 trials were used for training) and, for the monkeys, there were 80 trials in a block. The number of RNN trials was reduced to avoid GPU memory overflow issues when training with backpropagation-through-time. For the RNNs, the reversal of reward schedule occurred on a trial randomly chosen from the 10 middle trials of a block. For the monkeys, the reversal trial was chosen randomly from 20 middle trials of a block. The fixation period was not required in the RNNs since they performed the task without it; however, we examined the effects of having a fixation period on neural dynamics (see Figure 4D). In total, 40 successfully trained RNNs were analyzed in our study, where each RNN performed 20 blocks of reversal learning tasks. On the other hand, two monkeys performed in total of eight sessions of experiments, where each session consisted of 24 blocks of reversal learning.</p><p>To train RNNs, we set up a reward schedule where the high-value option at the start of a trial was randomly selected from two options. Since the initial high-value option was switched to the other option at a random trial (<xref ref-type="fig" rid="fig1">Figure 1</xref>, scheduled reversal), the RNNs had to learn to reverse their preferred choice to maximize reward. To learn to reverse, the RNNs were trained to mimic the outputs of a Bayesian inference model that was shown to capture the monkey’s reversal behavior in previous studies (<xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>). We first let the RNNs perform the task by simulating the RNN dynamics starting at a random initial state and providing relevant stimuli, such as cue and feedback, at every trial. Once the RNNs completed a block of trials, the choice and reward outcomes of all trials in a block were fed into the Bayesian model to infer the trial at which reward schedule was reversed, referred to as the inferred scheduled reversal trial (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, inferred reversal). Previous studies have shown that monkeys reverse their preferred choice (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, behavioral reversal) a few trials after the scheduled reversal trial (<xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>). Therefore, the target choice outputs (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, target output), on which the RNNs were trained, were set to be the initial high-value option until a few trials after the inferred scheduled reversal trial, followed by an abrupt switch to the other option. The recurrent weights and the output weights of the RNNs were trained via supervised learning to minimize the cross-entropy between the RNN choice outputs and the target choice outputs (See Methods Training scheme for details on the RNN training).</p><p>After training, in a typical block, a trained RNN selected the initial high-value option, despite occasionally not receiving a reward, but abruptly switched its choice when consecutive no-reward trials persisted (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, left). Such abrupt reversal behavior was expected as the RNNs were trained to mimic the target outputs of the Bayesian inference model (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, right). The intrinsic time scale of the RNN (τ=20 ms in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in Methods Recurrent neural network) was substantially faster than the duration of a trial (500 ms), thus the persistent behavior over multiple trials was a result of learning the task. Analyzing the choice outputs of all the trained RNNs showed that, as in the example discussed, they selected the high-value option with high probability before the behavioral reversal, at which time they abruptly switched their choice (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, black). This abrupt reversal behavior was also found in the monkey’s behavior trained on the same task (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, orange). The behavioral reversal was preceded by a gradually increasing number of no-reward trials in the RNNs and the monkeys (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The distribution of behavioral reversal trials (i.e. trial at which preferred choice was reversed) relative to the scheduled reversal trial (i.e. trial at which reward schedule was reversed) was in good agreement with the distribution of monkey’s reversal trials (<xref ref-type="fig" rid="fig1">Figure 1E</xref>).</p></sec><sec id="s2-2"><title>Task-relevant neural activity evolves dynamically</title><p>Next, we analyzed the neural activity of neurons in the dorsolateral prefrontal cortex (PFC) of two monkeys and the activation of population of neurons in the RNNs. The spiking activity of PFC neurons was recorded while they performed the reversal learning task using eight microelectrode arrays. In each session, we recorded simultaneously from 573 to 1023 neurons (n=8 sessions, median population size 706). This neural data was collected and analyzed in a previous manuscript (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>). For each PFC neuron, we counted the spikes it emitted in a 300 ms time window that moved in 20 ms increment to analyze its spiking activity over time. For the RNNs, 40 successfully trained RNNs, whose reward rates were close to 70%, were included in the analysis.</p><p>We examined the temporal dynamics of task-relevant neural activity, in particular activity encoding the choice and reversal probability. To capture task-relevant neural activity, we first identified population vectors that encoded the task variables using a method called targeted dimensionality reduction (<xref ref-type="bibr" rid="bib32">Mante et al., 2013</xref>). It regresses the activity of each neuron at each time bin onto task variables across trials and finds the maximal population vector of each task variable. Then, neural activity representing the task variable is obtained by projecting the population activity onto the identified task vectors (see Methods Targeted dimensionality reduction for details). The spiking activity of PFC neurons was shown to encode the reversal of behavior in a previous study (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>). Following this line of work, we focused on analyzing the trials around the behavioral reversal point in each block. We referenced the position of each trial to the behavioral reversal trial as a relative trial.</p><p>Within each trial, the block-averaged neural activity associated with choices and inferred reversal probability, denoted as <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$x_{choice}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula>, respectively, produced non-stationary dynamics (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left). Their activity level reached a maximum around the time of cue onset (white squares in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, left), when the monkey and RNN were about to make a choice. Such rotational neural dynamics were found both in the PFC of monkeys and the RNNs.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural trajectories encoding choice and reversal probability variables.</title><p>(<bold>A</bold>) Neural trajectories of prefrontal cortex (PFC) (top) and RNN (bottom) obtained by projecting population activity onto task vectors encoding choice and reversal probability. Trial numbers indicate their relative position to the behavioral reversal trial. Neural trajectories in each trial were averaged over eight experiment sessions and 23 blocks for the PFC, and 40 networks and 20 blocks for the recurrent neural networks (RNNs). Black square indicates the time of cue onset. (<bold>B</bold>–<bold>C</bold>) Neural activity encoding reversal probability and choice in PFC (top) and RNN (bottom) at the time of cue onset (black squares in panel A) around the behavioral reversal trial. Shaded region shows the S.E.M over sessions (or networks) and blocks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig2-v1.tif"/></fig><p>Across trials, the orientation of rotational trajectories shifted, indicating systematic changes in the choice and reversal probability activity. When the task-relevant activity at cue onset (or fixation) was analyzed, the points in the two-dimensional phase space (<inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$x_{choice}$\end{document}</tex-math></alternatives></inline-formula>) shifted gradually across trials (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right). We found that reversal probability activity, <inline-formula><alternatives><mml:math id="inf5"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft5">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula>, peaked at the reversal trial in the PFC and the RNN (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Choice activity, <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$x_{choice}$\end{document}</tex-math></alternatives></inline-formula>, on the other hand, decreased gradually over trials reflecting the changes in choice preference (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The inverted-V shape of <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> and the monotonic decrease of <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$x_{choice}$\end{document}</tex-math></alternatives></inline-formula> over trials explained the counter-clockwise shift in the rotational trajectories observed in the two-dimensional phase space (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>A recent study found that, when a neural network was trained via reinforcement learning to perform a reversal learning task, the first two principal components of the network activity shifted gradually following a shape similar to <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$x_{choice}$\end{document}</tex-math></alternatives></inline-formula> (see Figure 1 in <xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>). These results suggest that the gradual shift in network states across trials (<xref ref-type="fig" rid="fig2">Figure 2B,C</xref>) could be a common feature that emerges in networks performing a reversal learning task, regardless of training methods. One main difference is that, by design, these neural networks lacked within-trial dynamics in contrast to our RNNs (<xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>). In the following sections, we characterize the dynamics and properties of the non-stationary neural activity that our RNNs and the PFC of monkeys generated during the trials (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p></sec><sec id="s2-3"><title>Integration of reward outcomes drives reversal probability activity</title><p>We first asked if there were underlying dynamics that systematically changed the reversal probability activity, <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula>. Previous works have shown that accumulation of decision-related evidence can be represented as a line attractor in a stable subspace of network activity (<xref ref-type="bibr" rid="bib32">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Nair et al., 2023</xref>). We hypothesized that the gradual shift of <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2A, B</xref>) could be characterized similarly by a line attractor model, where <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> is explained by integrating reward outcomes across trials.</p><p>To test this idea, we set up a reward integration equation <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$x_{rev}^{k+1}(t) = x_{rev}^{k}(t) + R^{k}_{\pm}(t)$\end{document}</tex-math></alternatives></inline-formula> that predicts the next trial’s reversal probability activity at time <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> based on the current trial’s reversal probability activity at time <inline-formula><alternatives><mml:math id="inf16"><mml:mi>t</mml:mi></mml:math><tex-math id="inft16">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> and the reward outcome, therefore, predicting across-trial reversal probability by integrating reward outcomes. Here, <inline-formula><alternatives><mml:math id="inf17"><mml:mi>t</mml:mi></mml:math><tex-math id="inft17">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is a time point within a trial (e.g. <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$t=t_{cue}$\end{document}</tex-math></alternatives></inline-formula> is the time of cue onset), <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$x_{rev}^{k}(t)$\end{document}</tex-math></alternatives></inline-formula> is the reversal probability activity at <inline-formula><alternatives><mml:math id="inf20"><mml:mi>t</mml:mi></mml:math><tex-math id="inft20">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> on trial <inline-formula><alternatives><mml:math id="inf21"><mml:mi>k</mml:mi></mml:math><tex-math id="inft21">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf22"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft22">\begin{document}$R^{k}_{\pm}(t)$\end{document}</tex-math></alternatives></inline-formula> is an estimate of the shift in reversal probability activity at <inline-formula><alternatives><mml:math id="inf23"><mml:mi>t</mml:mi></mml:math><tex-math id="inft23">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> driven by trial <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>’s reward outcome (+ if rewarded and - if not rewarded. See Methods Reward integration equation for details).</p><p>When the reward integration activity at cue onset (<inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$t_{cue}$\end{document}</tex-math></alternatives></inline-formula>) was analyzed, the predicted reversal probability activity was in good agreement with the actual reversal probability activity of PFC and RNN (example blocks shown in <xref ref-type="fig" rid="fig3">Figure 3A,C</xref>; prediction accuracy of all blocks shown in <xref ref-type="fig" rid="fig3">Figure 3E</xref>). In addition, we found that <inline-formula><alternatives><mml:math id="inf26"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft26">\begin{document}$x_{rev}^{k}(t_{cue})$\end{document}</tex-math></alternatives></inline-formula> responded to reward outcomes consistently with how reversal probability itself would be updated: no reward increased the reversal probability activity in the next trial (<xref ref-type="fig" rid="fig3">Figure 3B,D</xref>; no reward), while a reward decreased it (<xref ref-type="fig" rid="fig3">Figure 3B, D</xref>; reward). At the behavioral reversal trial (<inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$k=0$\end{document}</tex-math></alternatives></inline-formula>), however, the reversal probability activity in the following trial (<inline-formula><alternatives><mml:math id="inf28"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$k=1$\end{document}</tex-math></alternatives></inline-formula>) decreased regardless of the reward outcome at the reversal trial.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Integration of reward outcomes drives reversal probability activity.</title><p>(<bold>A</bold>) The reversal probability activity of prefrontal cortex (PFC) (orange) and prediction by the reward integration equation (blue) at the time of cue onset across trials around the behavioral reversal trial. Three example blocks are shown. Pearson correlation between the actual and predicted PFC activity is shown on each panel. Relative trial number indicate the trial position relative to the behavioral reversal trial. (<bold>B</bold>) <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$R^{k}_{\pm}$\end{document}</tex-math></alternatives></inline-formula> of PFC estimated from the reward integration equation at cue onset. <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$R^{k}_{-}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$R^{k}_{+}$\end{document}</tex-math></alternatives></inline-formula> correspond to no-reward (red) and reward trials (blue), respectively. The shaded region shows the S.E.M over blocks and sessions. (<bold>C</bold>–<bold>D</bold>) Same as in panels (<bold>A</bold>) and (<bold>B</bold>) but for trained recurrent neural networks (RNNs). (<bold>E</bold>) Prediction accuracy, quantified with Pearson correlation, of the reward integration equation of all eight PFC recording sessions and all 40 trained RNNs at cue onset. (<bold>F</bold>) Average prediction accuracy, quantified with Pearson correlation, of the reward integration equation across time. The value at each time point shows the prediction accuracy averaged over all blocks in PFC recording sessions (top) or trained RNNs (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig3-v1.tif"/></fig><p>When the reward integration equation was fitted to other time points in the trial (i.e. estimate <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$R^{k}_{\pm}(t)$\end{document}</tex-math></alternatives></inline-formula> at other <inline-formula><alternatives><mml:math id="inf33"><mml:mi>t</mml:mi></mml:math><tex-math id="inft33">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> in the trial), the prediction accuracy remained stable over the trial duration (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). This suggested that a line attractor model might be applicable throughout the trial. However, the reward integration equation is an algebraic relation and does not capture the dynamics of neural activity, such as the non-stationary activity during the trial (e.g. <xref ref-type="fig" rid="fig2">Figure 2</xref>). This observation led us to characterize the dynamics of reversal probability activity to assess whether it was compatible with the line attractor model.</p></sec><sec id="s2-4"><title>Augmented attractor model of reversal probability activity</title><p>We showed that the reversal probability activity encodes the accumulation of reward outcomes, which resembles a line attractor model (<xref ref-type="fig" rid="fig3">Figure 3</xref>). However, a direct application of the line attractor dynamics would imply that, when no decision-related evidence is presented within a trial, the reversal probability activity should remain stationary (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, Stationary), which was incompatible with the non-stationary reversal probability activity observed during a trial (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Augmented model for reversal probability activity.</title><p>(<bold>A</bold>) Schematic of two activity modes of the reversal probability activity. Left: Stationary mode (line attractor) where <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> remains constant during a trial, and non-stationary mode where <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> is dynamic. Right: Augmentation of stationary and non-stationary activity modes where the stationary mode leads the non-stationary mode in time. The time derivative <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> is shown to demonstrate (non-)stationarity of the activity. (<bold>B</bold>) Left: Block-averaged <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$x_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> of prefrontal cortex (PFC) across trial and time. Dotted red lines indicate the onset time of fixation (-0.5 s), cue (0 s) and reward (0.8 s); same lines shown on the right. Right: <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$x_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> averaged over all trials (white), together with the trajectories of five trials around the reversal trial (colored). (<bold>C</bold>) Left: Contraction factor of <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> of PFC at different time points. Dotted line at 1 indicates the threshold of contraction and expansion. Right: Contraction factor of PFC <inline-formula><alternatives><mml:math id="inf40"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> of individual trials between the time interval -2.5 and -1 s. (<bold>D</bold>) Block-averaged <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> of recurrent neural networks (RNNs) at the pre-reversal (left) and post-reversal (right) trials. Note that the sign of the post-reversal trial trajectories was flipped to match the shape of the pre-reversal trajectories. Dotted red lines indicate the time of fixation, cue off, and reward. (<bold>E</bold>) Contraction factor of <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> of RNN. Similar results for RNN as in panel (<bold>C</bold>). (<bold>F</bold>) Generating PFC non-stationary reversal probability trajectories from the stationary activity using support vector regression (SVR) models. Top: Trajectories generated from SVR compared to the PFC reversal probability trajectories in trials around the reversal trial in an example block. The initial state (green) is the input to the SVR model, which then predicts the rest of the trajectory. The normalized mean-squared-error (MSE) between the SVR trajectory (prediction, red) and the PFC trajectory (data, black) is shown in each trial. Bottom: Trajectories generated from the null SVR compared to the PFC reversal probability trajectories. The initial states of trials in a block were shuffled randomly prior to training the null SVR model. The trajectories predicted from the null SVR model (blue) are compared to the PFC reversal probability trajectories (black). (<bold>G</bold>) The normalized MSE of all trials in the test dataset. (<bold>H</bold>) Difference between the normalized MSE of the SVR and the null models. The difference in normalized MSE between two models was calculated for each trial.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comparison of recurrent neural networks (RNNs) trained with and without fixation.</title><p>(<bold>A</bold>) RNNs trained without fixation. Right: The choice output of the RNNs oscillates. Left, Middle: The derivative of reversal probability activity <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> does not converge to 0 during the early part of a trial (start to cue-off). As the cue is turned on, <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> fluctuates with the cue. The white line shows <inline-formula><alternatives><mml:math id="inf45"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft45">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> averaged over all pre-reversal (left) and post-reversal (middle) trials. (<bold>B</bold>) RNNs trained with the choice output fixed at 0 before making a choice.Specifically, during the time interval between fixation and cue-off lines shown in the left and middle panels, the choice output was trained to be fixed at 0. Right: The choice output of the RNNs is flat when they are not making choices. Left, Middle: The derivative of reversal probability activity <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> converges to 0 during the early part of a trial (fixation to cue-on). As the cue is turned on,As the cue is turned on, <inline-formula><alternatives><mml:math id="inf47"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> shows fluctuation milder than RNNs trained without fixation. The white line shows <inline-formula><alternatives><mml:math id="inf48"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> averaged over all pre-reversal (left) and post-reversal (middle) trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig4-figsupp1-v1.tif"/></fig></fig-group><p>To better characterize the dynamics of reversal probability activity, we analyzed how its derivative <inline-formula><alternatives><mml:math id="inf49"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> changes throughout a trial. In the PFC of monkeys, we found that the reversal probability activity was stationary at the start of a trial: <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> was close to zero before and during the fixation (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, Stationary). However, the non-stationary activity emerged at cue onset and was associated with task-related behavior, such as making a choice and receiving reward. Specifically, <inline-formula><alternatives><mml:math id="inf51"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> increased rapidly at the cue onset, when a decision was made, followed by a sharp fluctuation and slow decay until the reward time (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, Non-stationary).</p><p>We then analyzed the contraction factor of the reversal probability activity to assess whether the activity is contracting or expanding around its mean activity (<xref ref-type="fig" rid="fig4">Figure 4C</xref>; see Methods Contraction factor of reversal probability activity for details). We found that the contraction factor was less than 1 (i.e. contracting) before the fixation period (-2.5 to -1 s in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, left; also see the right panel), became close to 1 (i.e. marginally stable) around fixation (-1 to 0 s), and exceeded 1 (i.e. expanding) at cue onset (0 s). This showed that the reversal probability activity is a point attractor (i.e. stationary and contracting) at the start of a trial but loses its stability as task-related behavior is executed.</p><p>In the RNNs, we found that the dynamics of reversal probability activity were similar to the PFC’s activity when an additional constraint was added to the RNN’s objective function to fix its choice output to zero before making a choice (i.e. fixation period was between fixation and cue-off lines in <xref ref-type="fig" rid="fig4">Figure 4D</xref>; indicated as Stationary period). With the fixation constraint, the reversal probability activity was stationary during the fixation period (i.e. <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> converged and stayed close to zero). This contrasted the RNNs trained without the fixation, which exhibited more dynamics before making a choice, suggesting the role of fixation in promoting stationary dynamics (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The RNN’s non-stationary dynamics after cue-off was consistent with the PFC’s non-stationary activity regardless of the fixation constraint (<xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, Non-stationary).</p><p>The contraction factor of reversal probability activity of the RNNs trained with the fixation constraint showed a trend that was similar to the PFC. It was less than 1 during fixation (fixation to cue-off in <xref ref-type="fig" rid="fig4">Figure 4E</xref>; also see the right panel) and became close to 1 immediately after cue-off when the decision was made (the point next to cue-off in <xref ref-type="fig" rid="fig4">Figure 4E</xref>). As in the PFC, the contraction factor shows that the RNN’s reversal probability activity is a point attractor at the start of a trial and becomes unstable as the RNN executes decision making.</p><p>Thus, analyzing <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$dx_{rev}/dt$\end{document}</tex-math></alternatives></inline-formula> across a trial showed that the reversal probability activity consisted of two dynamic modes: a point attractor at the start of a trial, which was consistent with the line attractor model, followed by non-stationary dynamics during the trial, which deviated from the line attractor.</p><p>Next, we investigated whether the two activity modes are linked by common dynamics. In the RNNs, the cue applied at the end of the stationary period (<xref ref-type="fig" rid="fig4">Figure 4D</xref>, cue-on to cue-off) determined the initial state from which the non-stationary activity was generated by the recurrent dynamics. We wondered if the two activity modes of PFC also obeyed the same dynamic relationship, i.e., the non-stationary activity is generated by underlying dynamics with an initial condition given by the stationary state.</p><p>To test this hypothesis, we took the PFC activity at the start of fixation period (-0.5 s) as the initial state. Then, we trained a support vector regression (SVR) to infer the underlying dynamics, which used the PFC activity at fixation as the input and generated the remaining trajectory until the reward time as the output. The SVR model was trained on neural data from 20 trials around the behavioral reversal in 10 randomly selected blocks from a session and tested on the remaining blocks (approximately 10 blocks) from the same session, thus training separate SVR model for each session. This procedure was repeated 10 times to sample different sets of training blocks (see Methods Generating the PFC reversal probability trajectories from initial states for the details). The prediction error was quantified using the normalized mean-squared-error (MSE) between the SVR prediction and the actual reversal probability trajectory normalized by the amplitude of the trajectory.</p><p><xref ref-type="fig" rid="fig4">Figure 4F</xref> (top) compares the SVR trajectory (red, prediction) generated from an initial state (green, initial state) and the actual PFC reversal probability trajectory (black, data) in trials around the behavioral reversal. We found that the SVR trajectories were able to capture the overall shape of data trajectories that had a bump shape and shifted up and down with the initial state. The normalized MSE of all the trials in the test dataset was 0.13, i.e., the mean error is 13% of the trajectory amplitude (<xref ref-type="fig" rid="fig4">Figure 4G</xref>).</p><p>To verify the role of initial state in generating trajectories, we trained a null SVR model, in which the initial states of trials in a block were randomly shuffled before training the model. In other words, a reversal probability trajectory was generated not from its own initial state, but from the initial state of a randomly chosen trial. The rest of the training procedure was identical as described above. We found that, although the null trajectory (<xref ref-type="fig" rid="fig4">Figure 4F</xref>, Null, prediction) resembled the overall shape of the data trajectory (<xref ref-type="fig" rid="fig4">Figure 4F</xref>, Null, data), i.e., increase towards cue onset and then decrease monotonically, it did not shift together with the initial states (<xref ref-type="fig" rid="fig4">Figure 4F</xref>, Null, initial state), as did the data trajectories. When the normalized MSE of the SVR and the null trajectories were compared, we found that in 86% of the test trials the SVR error was smaller than the null error (<xref ref-type="fig" rid="fig4">Figure 4H</xref>).</p><p>Together, our results show that the reversal probability activity consists of two activity modes linked by underlying dynamics. The stable stationary state (i.e. point attractor) at the start of a trial determines the initial condition from which the non-stationary dynamics, associated with task-related behavior, is generated. These findings suggest an extension of the standard line attractor model, in which points on the line attractor serve as initial states that launch non-stationary activity necessary to perform a task.</p></sec><sec id="s2-5"><title>Dynamic neural trajectories encoding reversal probability are separable</title><p>The previous section showed that the underlying neural dynamics, associated with task-related behavior, generates non-stationary activity during a trial. We next asked how distinct probabilistic values can be encoded in this non-stationary activity. In a stationary state, different positions encode different levels of decision-related evidence. When stationary points evolve through the non-stationary dynamics, as in our augmented model, they must remain separable in order to represent distinct values.</p><p>To address this question, we compared trajectories of adjacent trials to examine if the reward outcome drives the next trial’s trajectory away from the current trial’s trajectory, thus separating them. Analysis of PFC activity showed that not receiving a reward increased the next trial’s trajectory <inline-formula><alternatives><mml:math id="inf54"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$x_{rev}^{k+1}(t)$\end{document}</tex-math></alternatives></inline-formula>, compared to the current trial’s trajectory <inline-formula><alternatives><mml:math id="inf55"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$x_{rev}^{k}(t)$\end{document}</tex-math></alternatives></inline-formula>, over the entire trial duration until the next trial’s reward was revealed (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left; <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$R_{-}$\end{document}</tex-math></alternatives></inline-formula>). This was shown in the difference of adjacent trials’ trajectories being positive values, when not rewarded through most of the trial (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right; <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}$R_{-}$\end{document}</tex-math></alternatives></inline-formula>). Moreover, across trials, the same trend was observed in all the trials except at the behavioral reversal trial, at which the reversal probability activity reached its maximum value and decreased in the following trial (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, top; <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$R_{-}$\end{document}</tex-math></alternatives></inline-formula>). On the other hand, when a reward was received, the next trial’s trajectory was decreased, compared to the current trial’s trajectory over the entire trial duration until the next trial’s reward (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, left; <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$R_{+}$\end{document}</tex-math></alternatives></inline-formula>). This was shown in the difference of adjacent trials’ trajectories being negative values, when rewarded through most of the trial (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, right; <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$R_{+}$\end{document}</tex-math></alternatives></inline-formula>). Across trials, the same trend was observed in all the trials except at the trial preceding the behavioral reversal trial, at which the trajectory increased to the maximum value at the reversal trial (<xref ref-type="fig" rid="fig5">Figure 5</xref>, bottom; <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$R_{+}$\end{document}</tex-math></alternatives></inline-formula>). Additional analysis on <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$R_{-}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$R_{+}$\end{document}</tex-math></alternatives></inline-formula> beyond the next trial’s reward time can be found in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Dynamic neural trajectories encoding reversal probability are separated in response to reward outcomes.</title><p>(<bold>A</bold>) Left: <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> of prefrontal cortex (PFC) at current trial (black) is compared to <inline-formula><alternatives><mml:math id="inf65"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> in the next trial when reward is received (top, red) and not received (bottom, blue). Right: The difference of <inline-formula><alternatives><mml:math id="inf66"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft66">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> between current and next trials shown on the left panels. Shaded region shows the S.E.M. across all trials, blocks, and sessions. (<bold>B</bold>) Difference of <inline-formula><alternatives><mml:math id="inf67"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft67">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> of two adjacent trials when reward is not received (top, <inline-formula><alternatives><mml:math id="inf68"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$R_{-}$\end{document}</tex-math></alternatives></inline-formula>) or received (bottom, <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$R_{+}$\end{document}</tex-math></alternatives></inline-formula>). The approximate time of reward outcome is shown. Relative trial number indicate the trial position relative to the behavioral reversal trial. (<bold>C</bold>) Left: <inline-formula><alternatives><mml:math id="inf70"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft70">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> of PFC of consecutive no reward trials before the behavioral reversal trial (top) and consecutive reward trials after the behavioral reversal (bottom). The initial value was subtracted to compare the ramping rates of <inline-formula><alternatives><mml:math id="inf71"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft71">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula>. Right: Difference in the ramping rates of trajectories of adjacent trials, when reward was received (blue) and not received (red). (<bold>D</bold>–<bold>E</bold>) Same as the right panels in (<bold>A</bold>) and (<bold>C</bold>) but for trained recurrent neural networks (RNNs). (<bold>F</bold>) Left, Middle: External (left) and recurrent (middle) inputs to the RNN reversal probability dynamics, when reward was not received (red, magenta) or was received (blue, cyan). Right: Amplification factor shows the ratio of the total input when no reward (or reward) was received to the total input of reference input. The amplification factors for both the external (red, blue) and recurrent (magenta, cyan) inputs are shown. Red and magenta curves and blue and cyan curves overlap.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Decoding reward outcome and the behavioral reversal trial using neural trajectories encoding reversal probability.</title><p>(<bold>A</bold>) Left: Decoding the reward outcome (i.e. reward or no reward) of every trial at each time point, given the difference of neural trajectories of two adjacent trials. At each time point, 300 ms segment of the trajectories were used for decoding. Right: Decoding accuracy is averaged over all trials shown on the left panel. Red dotted line shows the approximate time of next (<bold>B</bold>) Left: Decoding the behavioral reversal trial using neural trajectories of 20 trials around the reversal trial. Decoding error shows the position of predicted reversal trial relative to the actual reverse trial. At each time point, 300 ms segment of each trajectory was used for decoding. Black shows the decoding error when single trial trajectories were used, and green shows the result when randomly chosen five blocks of trajectories were averaged before decoding. Gray dotted line shows the chance level performance. Right: Distance between trajectories was measured by taking the average of normalized mean-squared-error of adjacent trajectories at all trials. Each dot corresponds to a time point shown on the left panel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig5-figsupp1-v1.tif"/></fig></fig-group><p>We then examined what type of activity the dynamic trajectories exhibited when separating away from the previous trial’s trajectory. Ramping activity is often observed in cortical neurons of animals engaged in decision-making (<xref ref-type="bibr" rid="bib29">Li et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Li et al., 2016</xref>; <xref ref-type="bibr" rid="bib12">Finkelstein et al., 2021</xref>; <xref ref-type="bibr" rid="bib28">Latimer et al., 2015</xref>; <xref ref-type="bibr" rid="bib63">Zoltowski et al., 2019</xref>). We found that when no rewards were received, trajectories were separated from the previous trial’s trajectory by increasing their ramping rates towards the decision time (<inline-formula><alternatives><mml:math id="inf72"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$dR_{-}/dt \gt0$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, right). On the other hand, when rewards were received, trajectories were separated by decreasing their ramping rate (<inline-formula><alternatives><mml:math id="inf73"><mml:mstyle><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft73">\begin{document}$dR_{+}/dt \lt0$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, right). The increase (or decrease) in the ramping rates was observed in consecutive no reward (or reward) trials around the reversal trial (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, left).</p><p>Consistent with the PFC activity, the trained RNN exhibited similar activity responses to reward outcomes: neural trajectories encoding reversal probability increased, when not rewarded, and decreased, when rewarded. The shift in trajectories persisted throughout the trial duration (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) and ramping rates changed in agreement with the PFC findings (<xref ref-type="fig" rid="fig5">Figure 5E</xref>).</p><p>We examined the circuit dynamic motif of the trained RNN that separates neural trajectories. We projected the differential equation governing the network dynamics onto a one-dimensional subspace encoding reversal probability and analyzed the contribution of external and recurrent inputs to reversal probability dynamics: <inline-formula><alternatives><mml:math id="inf74"><mml:mstyle><mml:mrow><mml:mi>τ</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≡</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft74">\begin{document}$\tau \dot{x}_{rev}= x_{rec}+ x_{ext}\equiv x_{total}$\end{document}</tex-math></alternatives></inline-formula> (see Methods Recurrent neural network for details). We found that the effect of the external input <inline-formula><alternatives><mml:math id="inf75"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft75">\begin{document}$x_{ext}$\end{document}</tex-math></alternatives></inline-formula> was positive, reflecting that the external feedback input drives the reversal probability. On the other hand, the recurrent input <inline-formula><alternatives><mml:math id="inf76"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft76">\begin{document}$x_{rec}$\end{document}</tex-math></alternatives></inline-formula> was negative, showing that it curtailed the external input (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, external and recurrent). To analyze the effects of no-reward (or reward), we averaged the reversal probability activity over all trials (Fig.5F, reference) at which the subsequent trial was not (or was) rewarded. When no reward was received, <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$x_{ext}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, external, red) and <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$x_{rec}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, recurrent, magenta) were both amplified, compared to the reference, by approximately the same factor and resulted in increased total input: <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}$x_{total}^{no\ rew}= \gamma^{no\ rew}x_{total}$\end{document}</tex-math></alternatives></inline-formula> with <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$\gamma^{no\ rew}\gt 1$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, amplification, red and magenta). On the other hand, when reward was received, <inline-formula><alternatives><mml:math id="inf81"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft81">\begin{document}$x_{ext}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, external, blue) and <inline-formula><alternatives><mml:math id="inf82"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft82">\begin{document}$x_{rec}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, recurrent, cyan) were both suppressed, resulting in decreased total input: <inline-formula><alternatives><mml:math id="inf83"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft83">\begin{document}$x_{total}^{reward}= \gamma^{reward}x_{total}$\end{document}</tex-math></alternatives></inline-formula> with <inline-formula><alternatives><mml:math id="inf84"><mml:mstyle><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft84">\begin{document}$\gamma^{reward}\lt 1$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, amplification, blue and cyan). This suggests a circuit dynamic motif, where positive external feedback balanced by recurrent inhibition drives the reversal probability dynamics. The total drive is amplified or suppressed, depending on reward outcomes, resulting in a trajectory that separates from the previous trial’s trajectory.</p><p>In sum, our findings show that dynamic neural trajectories encoding reversal probability are separated from the previous trial’s trajectory in response to reward outcomes, allowing them to represent distinct values of reversal probability during a trial.</p></sec><sec id="s2-6"><title>Separability of reversal probability trajectories across trials</title><p>We investigated if reversal probability trajectories across multiple trials maintained separability. To this end, we analyzed the mean behavior of trajectories in each trial (referred to as mean trajectory of a trial) and analyzed their separability across trials around the behavioral reversal. Since a mean trajectory of a trial was obtained by averaging over all reward outcomes of the previous trial, we compared how reward and no-reward contributed to modifying the next trial’s mean trajectory, which were quantified by <inline-formula><alternatives><mml:math id="inf85"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft85">\begin{document}$R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf86"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft86">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, respectively.</p><p>We found that the effect of no-reward was larger than the effect of reward before the behavioral reversal. This is shown as the trace <inline-formula><alternatives><mml:math id="inf87"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft87">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> (no reward) lying above the trace <inline-formula><alternatives><mml:math id="inf88"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft88">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> (reward) during a trial and across all pre-reversal trials (see pre-reversal trials -5 to -1 in <xref ref-type="fig" rid="fig6">Figure 6A</xref>; <inline-formula><alternatives><mml:math id="inf89"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft89">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf90"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft90">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> are positive traces since <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}$R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> is mostly negative). The temporal averages of <inline-formula><alternatives><mml:math id="inf92"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft92">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf93"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft93">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> captured this systematic differences in the pre-reversal trials (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, bottom), and the sum <inline-formula><alternatives><mml:math id="inf94"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft94">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> was positive during a trial in 80% of the pre-reversal trials (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, top). The positivity of <inline-formula><alternatives><mml:math id="inf95"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft95">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> in time and across pre-reversal trials implied that the mean trajectories remained separable by increasing monotonically across the trials (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, bottom). Consistent with this observation, the Spearman rank correlation of pre-reversal trajectories was stable in time (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, pre).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Mean trajectories encoding reversal probability shift monotonically across trials.</title><p>(<bold>A</bold>) Traces of <inline-formula><alternatives><mml:math id="inf96"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft96">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf97"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft97">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> around the behavioral reversal trial. Note the sign flip in <inline-formula><alternatives><mml:math id="inf98"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft98">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula>, which was introduced to compare the magnitudes of <inline-formula><alternatives><mml:math id="inf99"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft99">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf100"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft100">\begin{document}$R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula>. Relative trial number indicate the trial position relative to the behavioral reversal trial. (<bold>B</bold>) Top: <inline-formula><alternatives><mml:math id="inf101"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft101">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> across trial and time. Bottom: Temporal averages of <inline-formula><alternatives><mml:math id="inf102"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft102">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf103"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft103">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> over the trial duration. (<bold>C</bold>) Top: Traces of <inline-formula><alternatives><mml:math id="inf104"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft104">\begin{document}$R_{-}^{k}+R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> of pre-reversal trials (relative trial <inline-formula><alternatives><mml:math id="inf105"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft105">\begin{document}$k=-5$\end{document}</tex-math></alternatives></inline-formula> to -1), and the fraction of trials at each time point that satisfy <inline-formula><alternatives><mml:math id="inf106"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft106">\begin{document}$R_{-}^{k}+R_{+}^{k}\gt 0$\end{document}</tex-math></alternatives></inline-formula>. Bottom: Mean prefrontal cortex (PFC) reversal probability trajectories of pre-reversal trials. (<bold>D</bold>) Same as in panel (<bold>C</bold>), but for post-reversal trials (relative trial <inline-formula><alternatives><mml:math id="inf107"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft107">\begin{document}$k=0$\end{document}</tex-math></alternatives></inline-formula> to 4). (<bold>E</bold>) Spearman rank correlation between trial numbers and the mean PFC reversal probability trajectories across pre-reversal (red) and post-reversal (blue) trials at each time point. For the post-reversal trials, Spearman rank correlation was calculated with the trial numbers in reversed order to capture the descending order. (<bold>F</bold>) <inline-formula><alternatives><mml:math id="inf108"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft108">\begin{document}$R_{-}^{k}+R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> of trained recurrent neural networks (RNNs) across trial and time. (<bold>G</bold>–<bold>I</bold>) Trained RNNs' block-averaged <inline-formula><alternatives><mml:math id="inf109"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft109">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> before and after the reversal trial and their average Spearman correlation at each time point.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Break down of <inline-formula><alternatives><mml:math id="inf110"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft110">\begin{document}$R^{+}, R^{-}$\end{document}</tex-math></alternatives></inline-formula> by the reward outcomes of two consecutive trials.</title><p>(<bold>A</bold>) <inline-formula><alternatives><mml:math id="inf111"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft111">\begin{document}$R^{+}$\end{document}</tex-math></alternatives></inline-formula> <inline-formula><alternatives><mml:math id="inf112"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft112">\begin{document}$R^{+}= R^{++}+ R^{+-}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf113"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft113">\begin{document}$R^{++}$\end{document}</tex-math></alternatives></inline-formula> indicates two consecutive reward trials and <inline-formula><alternatives><mml:math id="inf114"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft114">\begin{document}$R^{+-}$\end{document}</tex-math></alternatives></inline-formula> indicates a reward followed by no reward. Left: <inline-formula><alternatives><mml:math id="inf115"><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo lspace="0em" rspace="0em">+</mml:mo><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">+</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="inft115">\begin{document}$R^{++}$\end{document}</tex-math></alternatives></inline-formula> across trial and time (top). Traces of <inline-formula><alternatives><mml:math id="inf116"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft116">\begin{document}$R^{++}$\end{document}</tex-math></alternatives></inline-formula> at individual trials and the fraction of trials whose traces are negative (bottom). Middle: Same as the left panel but for.<inline-formula><alternatives><mml:math id="inf117"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft117">\begin{document}$R^{+-}$\end{document}</tex-math></alternatives></inline-formula> Right: Same as the other panels but for <inline-formula><alternatives><mml:math id="inf118"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft118">\begin{document}$R^{+}$\end{document}</tex-math></alternatives></inline-formula>. (<bold>B</bold>) <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$R^{-}$\end{document}</tex-math></alternatives></inline-formula> was decomposed into two components <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$R^{-}=R^{-+}+ R^{--}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf121"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft121">\begin{document}$R^{-+}$\end{document}</tex-math></alternatives></inline-formula> indicates no reward followed by a reward and <inline-formula><alternatives><mml:math id="inf122"><mml:mstyle><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft122">\begin{document}$R^{-}$\end{document}</tex-math></alternatives></inline-formula> indicates two consecutive no rewards. Same analysis as in panel (<bold>A</bold>) was performed.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig6-figsupp1-v1.tif"/></fig></fig-group><p>After the behavioral reversal, the effects of no-reward and reward were the opposite of pre-reversal trials. The trace <inline-formula><alternatives><mml:math id="inf123"><mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft123">\begin{document}$-R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> lied above the trace <inline-formula><alternatives><mml:math id="inf124"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft124">\begin{document}$R_{-}^{k}$\end{document}</tex-math></alternatives></inline-formula> (see post-reversal trials 0 to 4 in <xref ref-type="fig" rid="fig6">Figure 6A</xref>), and the temporal average <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$\langle -R_{+}^{k}\rangle_{t}$\end{document}</tex-math></alternatives></inline-formula> was larger than <inline-formula><alternatives><mml:math id="inf126"><mml:mstyle><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft126">\begin{document}$\langle R_{-}^{k}\rangle_{t}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, bottom). This showed that <inline-formula><alternatives><mml:math id="inf127"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft127">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> was mostly negative during post-reversal trials. The fraction of trials, for which <inline-formula><alternatives><mml:math id="inf128"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft128">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> is negative, was close to 80% among the post-reversal trials (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, top). The negativity of <inline-formula><alternatives><mml:math id="inf129"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft129">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> in time and across post-reversal trials implied that the post-reversal trajectories remained separable by decreasing monotonically across the trials (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, bottom). The order of post-reversal trajectories was stable over the trial duration, similarly to the pre-reversal trials but in the reversed order (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, post).</p><p>In the trained RNNs, the effects of reward outcomes on mean trajectories were consistent with the PFC findings: <inline-formula><alternatives><mml:math id="inf130"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft130">\begin{document}$R_{-}^{k}+ R_{+}^{k}$\end{document}</tex-math></alternatives></inline-formula> was positive and negative before and after the behavioral reversal, respectively (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). Consequently, reversal probability trajectories of the RNNs maintained separability by shifting monotonically as in the PFC (<xref ref-type="fig" rid="fig6">Figure 6G,H</xref>). The order of trajectories was also stable over the trial duration (<xref ref-type="fig" rid="fig6">Figure 6I</xref>).</p><p>These findings show that the mean behavior of reversal probability trajectories is to shift monotonically across trials. It suggests that a family of separable neural trajectories around the reversal trial can represent varying estimates of reversal probability stably in time.</p></sec><sec id="s2-7"><title>Perturbing reversal probability activity biases choice outcomes</title><p>Here, we turned to the RNN to see if we could perturb activity within the reversal probability space and consequently perturb the network’s choice preference. We defined the reversal probability activity <inline-formula><alternatives><mml:math id="inf131"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft131">\begin{document}$x_{rev}$\end{document}</tex-math></alternatives></inline-formula> as the activity in a neural subspace correlated to the behavioral reversal probability (Methods Targeted dimensionality reduction). However, it remains to be shown if the activity within this neural subspace can causally affect network’s behavioral outcomes.</p><p>Previous experimental works demonstrated that perturbing neural activity of medial frontal cortex (<xref ref-type="bibr" rid="bib36">Murphy et al., 2022</xref>), specific cell types (<xref ref-type="bibr" rid="bib62">Yi et al., 2024</xref>; <xref ref-type="bibr" rid="bib23">Jeong et al., 2020</xref>), or neuromodulators (<xref ref-type="bibr" rid="bib50">Su and Cohen, 2022</xref>; <xref ref-type="bibr" rid="bib20">Hyun et al., 2023</xref>) affect the performance of reversal learning. In our perturbation experiments in the RNNs, the perturbation was tailored to be within the reversal probability space by applying an external stimulus aligned (<inline-formula><alternatives><mml:math id="inf132"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft132">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula>) or opposite (<inline-formula><alternatives><mml:math id="inf133"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft133">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula>) to the reversal probability vector. An external stimulus in a random direction was also applied as a control (<inline-formula><alternatives><mml:math id="inf134"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft134">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula>). All the stimuli were applied before the time of choice at the reversal trial or at preceding trials (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Perturbing recurrent neural networks (RNN's) neural activity encoding reversal probability biases choice outcomes.</title><p>(<bold>A</bold>) RNN perturbation scheme. Three perturbation stimuli were used; <inline-formula><alternatives><mml:math id="inf135"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft135">\begin{document}$v_+$\end{document}</tex-math></alternatives></inline-formula>, population vector encoding the reversal probability; <inline-formula><alternatives><mml:math id="inf136"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft136">\begin{document}$v_-$\end{document}</tex-math></alternatives></inline-formula>, negative of <inline-formula><alternatives><mml:math id="inf137"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft137">\begin{document}$v_+$\end{document}</tex-math></alternatives></inline-formula>; <inline-formula><alternatives><mml:math id="inf138"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft138">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula>, control stimulus in random direction. Perturbation stimuli were applied at the reversal (0) and two preceding (-2, -1) trials. (B) Deviation of reversal probability activity <inline-formula><alternatives><mml:math id="inf139"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft139">\begin{document}$\Delta x_{rev}$\end{document}</tex-math></alternatives></inline-formula> and choice activity <inline-formula><alternatives><mml:math id="inf140"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft140">\begin{document}$\Delta x_{choice}$\end{document}</tex-math></alternatives></inline-formula> from the unperturbed activity. Perturbation was applied at the reversal trial during a time interval the cue was presented (shaded red). Choice was made after a short delay (shaded gray). Perturbation response along the reversal probability vector <inline-formula><alternatives><mml:math id="inf141"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft141">\begin{document}$v_+$\end{document}</tex-math></alternatives></inline-formula> (solid) and random vector <inline-formula><alternatives><mml:math id="inf142"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft142">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula> (dotted) are shown. (<bold>C</bold>) Perturbation of reversal probability activity (left) and choice activity (right) in response to three types of perturbation stimuli. Each dot shows the response of a perturbed network. Two perturbation strengths (multiplicative factor of 3 and 4 shown in panels D and E) were applied to 40 RNNs. <inline-formula><alternatives><mml:math id="inf143"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft143">\begin{document}$\Delta x_{rev}$\end{document}</tex-math></alternatives></inline-formula> shows the activity averaged over the duration of perturbation, and <inline-formula><alternatives><mml:math id="inf144"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft144">\begin{document}$\Delta x_{choice}$\end{document}</tex-math></alternatives></inline-formula> shows the averaged activity over the duration of choice. <inline-formula><alternatives><mml:math id="inf145"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft145">\begin{document}$\Delta x_{choice}$\end{document}</tex-math></alternatives></inline-formula> of <inline-formula><alternatives><mml:math id="inf146"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft146">\begin{document}$v_+$\end{document}</tex-math></alternatives></inline-formula> is significantly smaller than <inline-formula><alternatives><mml:math id="inf147"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft147">\begin{document}$\Delta x_{choice}$\end{document}</tex-math></alternatives></inline-formula> of (KS-test, p-value = 0.007). (<bold>D</bold>–<bold>E</bold>) Fraction of blocks in all 40 trained RNNs that exhibited delayed or accelerated reversal trials in response to perturbations of the reversal probability activity. Perturbations at trial number -1 by three stimulus types are shown on the left panels, and perturbations at all three trials by the stimulus of interest (<inline-formula><alternatives><mml:math id="inf148"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft148">\begin{document}$v_-$\end{document}</tex-math></alternatives></inline-formula> in <bold>D</bold> and <inline-formula><alternatives><mml:math id="inf149"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft149">\begin{document}$v_+$\end{document}</tex-math></alternatives></inline-formula> in <bold>E</bold>) are shown on the right panels. A multiplicative factor on the perturbation stimuli is shown as stimulus strength. (F) Left: The slope of linear regression model fitted to the residual activity of reversal probability and choice. The residual activity at each trial over the time interval [0, 500]ms was used to fit the linear model. Red dot indicates the slope at trial number -1. Relative trial number indicate the trial position relative to the behavioral reversal trial. Right: Each dot is the residual activity of a block at trial number -1. Red line shows the fitted linear model, and its slope (-0.34) is shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103660-fig7-v1.tif"/></fig><p>We found that the deviation of perturbed reversal probability activity from the unperturbed activity peaked at the end of perturbation duration and decayed gradually (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, red solid). The perturbed choice activity, however, deviated more slowly and peaked during the choice duration (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, black solid). This showed that perturbation of the reversal probability activity had its maximal effect on the choice activity when the choice was made. The strong perturbative effects on the reversal probability and choice activity were not observed in the control (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, dotted).</p><p>The perturbation in the aligned (<inline-formula><alternatives><mml:math id="inf150"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft150">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula>) and opposite (<inline-formula><alternatives><mml:math id="inf151"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft151">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula>) directions shifted the reversal probability activity along the same direction as the perturbation vector, as expected (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, left). On the other hand, the choice activity was increased when the perturbation was in the opposite direction <inline-formula><alternatives><mml:math id="inf152"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft152">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula>. The perturbation in the aligned direction <inline-formula><alternatives><mml:math id="inf153"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft153">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula> did not decreases the choice activity substantially, but its effect was significantly smaller than the increase seen in <inline-formula><alternatives><mml:math id="inf154"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft154">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula> perturbation (<xref ref-type="fig" rid="fig7">Figure 7C</xref>, right; KS-test, p-value = 0.007).</p><p>We further analyzed if perturbing within the reversal probability space could affect the choice outcomes, specifically the behavioral reversal trial. We found that the reversal trial was delayed when <inline-formula><alternatives><mml:math id="inf155"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft155">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula> stimulus was applied to reduce the reversal probability activity (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, left). The effect of <inline-formula><alternatives><mml:math id="inf156"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft156">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula> stimulus increased gradually with the stimulus strength and was significantly stronger than the <inline-formula><alternatives><mml:math id="inf157"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft157">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf158"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft158">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula> stimuli in delaying the reversal trial. Perturbation had the strongest effect when applied to the reversal trial, while perturbations on trials preceding the reversal showed appreciable but reduced effects (<xref ref-type="fig" rid="fig7">Figure 7D</xref>, right). When the <inline-formula><alternatives><mml:math id="inf159"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft159">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula> stimulus was applied to trials preceding the reversal trial, the reversal was accelerated (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, left). The effect of <inline-formula><alternatives><mml:math id="inf160"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft160">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula> stimulus also increased with stimulus strength and was significantly stronger than the <inline-formula><alternatives><mml:math id="inf161"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft161">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula> or <inline-formula><alternatives><mml:math id="inf162"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft162">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula> stimuli in accelerating the reversal trial (<xref ref-type="fig" rid="fig7">Figure 7E</xref>, right).</p><p>We asked if perturbation of neural activity in PFC could exhibit similar responses. In other words, does increase (or decrease) in reversal probability activity lead to decrease (or increase) in choice activity in PFC? Although PFC activity was not perturbed by external inputs, we considered the residual activity of single trials, i.e. deviation of single trial neural activity around the trial-averaged activity, to be ‘natural’ perturbation responses. We fitted a linear model to the residual activity of reversal probability and choice and found that they were strongly negatively correlated (i.e. negative slope in <xref ref-type="fig" rid="fig7">Figure 7F</xref>) at the trial preceding the behavioral reversal trial. This analysis demonstrated the correlation between perturbation responses of reversal probability and choice activity. However, it remains to be investigated, through perturbation experiments, whether reversal probability activity is causally linked to choice activity in PFC and, moreover, to animal’s choice outcomes.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Reversal learning</title><p>Reversal learning has been a behavioral framework for investigating how the brain supports flexible behavior (<xref ref-type="bibr" rid="bib8">Butter, 1969</xref>; <xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Groman et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>; <xref ref-type="bibr" rid="bib50">Su and Cohen, 2022</xref>; <xref ref-type="bibr" rid="bib20">Hyun et al., 2023</xref>) and for elucidating neural mechanisms underlying mental health issues (<xref ref-type="bibr" rid="bib16">Groman et al., 2022</xref>; <xref ref-type="bibr" rid="bib44">Schoenbaum et al., 2006</xref>). It has been shown that multiple brain regions (cortical <xref ref-type="bibr" rid="bib15">Groman et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>; <xref ref-type="bibr" rid="bib58">Wilson et al., 2010</xref>; <xref ref-type="bibr" rid="bib36">Murphy et al., 2022</xref>; <xref ref-type="bibr" rid="bib62">Yi et al., 2024</xref>; <xref ref-type="bibr" rid="bib23">Jeong et al., 2020</xref>; <xref ref-type="bibr" rid="bib59">Wilson et al., 2014</xref> and subcortical <xref ref-type="bibr" rid="bib15">Groman et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Averbeck and O’Doherty, 2022</xref>), neuromodulators ; (<xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Su and Cohen, 2022</xref>; <xref ref-type="bibr" rid="bib20">Hyun et al., 2023</xref>), and different inhibitory neuron types (<xref ref-type="bibr" rid="bib62">Yi et al., 2024</xref>; <xref ref-type="bibr" rid="bib23">Jeong et al., 2020</xref>) are involved in reversal learning.</p></sec><sec id="s3-2"><title>Our results</title><p>Despite these recent advances, the dynamics of neural activity in cortical areas during a reversal learning task have not been well characterized. In this study, we investigated how reversal probability is represented in cortical neurons by analyzing neural activity in the prefrontal cortex of monkeys and recurrent neural networks performing the reversal learning task. We found that the activity in a neural subspace encoding reversal probability represented integration of reward outcomes. This reversal probability activity had two activity modes: stable stationary activity at the start of trial, followed by non-stationary activity during the trial. There was underlying dynamics, associated with task-related behavior, that generated the non-stationary activity with an initial condition given by the stationary state. The existence of two activity modes suggested an extension of the standard line attractor model where non-stationary dynamics driven by task-related behavior link the attractor states. The non-stationary trajectories were separable, allowing distinct probabilistic values to be represented in dynamic trajectories. Perturbation experiments in the RNNs demonstrated a potential causal link between reversal the probability activity and choice outcomes.</p></sec><sec id="s3-3"><title>Attractor dynamics</title><p>RNNs with attractor dynamics have been investigated in various contexts as a neural implementation of normative models of decision-making and evidence integration (<xref ref-type="bibr" rid="bib39">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib38">Palmer et al., 2005</xref>; <xref ref-type="bibr" rid="bib46">Shadlen and Newsome, 2001</xref>; <xref ref-type="bibr" rid="bib34">Mazurek et al., 2003</xref>; <xref ref-type="bibr" rid="bib40">Ratcliff et al., 2003</xref>). One perspective is to consider decision variables as discrete or continuous attractor states of an RNN. Then, the network activity converges to an attracting state as a decision is made. Biologically plausible network models (<xref ref-type="bibr" rid="bib55">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib45">Seung, 1996</xref>) and neural recordings in cortical areas have been shown to exhibit discrete (<xref ref-type="bibr" rid="bib31">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Sutton, 1988</xref>; <xref ref-type="bibr" rid="bib14">Genkin et al., 2023</xref>) and continuous (<xref ref-type="bibr" rid="bib60">Wimmer et al., 2014</xref>) attractor dynamics. Another perspective, more closely related to our study, is to consider evidence integration as a movement of network state along a one-dimensional continuous attractor, as demonstrated in <xref ref-type="bibr" rid="bib21">Inagaki et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Nair et al., 2023</xref>; <xref ref-type="bibr" rid="bib53">Sylwestrak et al., 2022</xref> (see also continuous attractor dynamics in spatial mapping <xref ref-type="bibr" rid="bib4">Bollimunta et al., 2012</xref>; <xref ref-type="bibr" rid="bib13">Gardner et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Sorscher et al., 2023</xref>; <xref ref-type="bibr" rid="bib19">Hulse and Jayaraman, 2020</xref>).</p><p>In most of the studies, decision-related evidence was presented without significant interruption until the decision point (<xref ref-type="bibr" rid="bib31">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Sutton, 1988</xref>; <xref ref-type="bibr" rid="bib32">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Genkin et al., 2023</xref>). However, this was not the case in a reversal learning task with probabilistic rewards, as reward outcomes were revealed intermittently over multiple trials while task-related behavior must be performed within each trial. We showed that such multi-trial evidence integration promoted substantial non-stationary activity in the neural subspace encoding reversal probability. Therefore, the continuous attractor dynamics, in which the network state stays close to the attracting states, did not fully account for the observed neural dynamics. Instead, our findings suggest that separable dynamic trajectories in addition to attractor states could serve as a neural mechanism that represents accumulated evidence and accommodates non-stationary behaviors necessary to perform the task.</p></sec><sec id="s3-4"><title>Limitations</title><p>Our work demonstrated similarities in how reversal probability is represented in the PFC of monkeys and the RNNs. However, our approach, which compares the activity of RNNs trained on the task to the PFC activity, does not directly characterize the dynamic landscape of PFC activity. In particular, our analysis only shows that the PFC activity at the initial state of each trial can be characterized as a point attractor (<xref ref-type="fig" rid="fig4">Figure 4C, E</xref>). Although this result is compatible with a line attractor model, it does not demonstrate whether the initial states across trials collectively form a line attractor. In order to show the existence of a line attractor, it is necessary to identify the dynamics in the region of the phase space occupied by the set of point attractors.</p><p>Alternative approaches that infer the latent dynamics of spiking activity of a neural population (<xref ref-type="bibr" rid="bib26">Kim et al., 2023</xref>; <xref ref-type="bibr" rid="bib25">Kim et al., 2021</xref>; <xref ref-type="bibr" rid="bib9">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Sussillo et al., 2016</xref>) could allow us to address this question by deriving a low-dimensional system of differential equations that generates the PFC activity. If the inferred latent dynamics could generate the two activity modes observed in our data, it would allows us to characterize different aspects of the neural dynamics, such as attractor states, the role of external inputs, and the non-stationary dynamics, within a single dynamical system model.</p></sec><sec id="s3-5"><title>Related work</title><p>Recent studies showed that intervening behaviors, such as introducing an intruder (<xref ref-type="bibr" rid="bib37">Nair et al., 2023</xref>) or accumulating reward across trials (<xref ref-type="bibr" rid="bib53">Sylwestrak et al., 2022</xref>), could produce neural trajectories that deviate from and retract to a line attractor. These studies are consistent with our finding in that their neural dynamics temporarily deviated from attractor states. However, in our study, we did not thoroughly investigate the neural activity from dynamical systems perspective. It remains as future work to characterize the dynamical landscape of how the separable dynamic trajectories observed in our study are augmented to the continuous attractor model and compare it to the previous works (<xref ref-type="bibr" rid="bib37">Nair et al., 2023</xref>; <xref ref-type="bibr" rid="bib53">Sylwestrak et al., 2022</xref>).</p><p>A number of relevant studies have trained RNNs to perform various decision-making tasks (<xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Schaeffer et al., 2020</xref>; <xref ref-type="bibr" rid="bib48">Song et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Molano-Mazón et al., 2023</xref>). In a related work (<xref ref-type="bibr" rid="bib43">Schaeffer et al., 2020</xref>), RNNs were trained to perform a change point detection task designed by the International Brain Laboratory (<xref ref-type="bibr" rid="bib11">Findling, 2023</xref>). They showed that trained RNNs exhibited behavior outputs consistent with an ideal Bayesian observer without explicitly learning from the Bayesian observer. This finding shows that the behavioral strategies of monkeys could emerge by simply learning to do the task, instead of directly mimicking the outputs of Bayesian observer as done in our study.</p><p>The trained RNN in their work (<xref ref-type="bibr" rid="bib43">Schaeffer et al., 2020</xref>) exhibited line attractor dynamics in contrast to our observation of stationary and non-stationary dynamics (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In another study, a line attractor-like dynamics, where the principal components of network activity moved gradually across trials, was observed in artificial agents trained to perform the reversal learning task via reinforcement learning (seeFigure 1 in <xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>). One possible reason for the lack of non-stationary dynamics within a trial in these other studies is that a trial consisted of only one (<xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>) or a few time points (<xref ref-type="bibr" rid="bib43">Schaeffer et al., 2020</xref>), which limits the possible range of temporal dynamics RNNs can exhibit during a trial. This suggests that different setup of a task can lead to significantly different dynamics in the trained RNN. Moreover, it needs to be investigated whether such attractor dynamics are present in the neural recordings from mice performing the change point detection task.</p><p>The RNNs in our study were trained via supervised learning. However, in real life, animals most likely learn a reversal learning task via reinforcement learning (RL), i.e., learn the task from reward outcomes. Neuromodulators play a key role in mediating RL in the brain. In a recent study, dopamine-based RL was used to train artificial RNNs to conduct reversal learning tasks. It was shown that neural activity in RNNs and mice performing the same tasks were in good agreement (<xref ref-type="bibr" rid="bib56">Wang et al., 2018</xref>). In addition, projections of serotonin from dorsal raphe nuclei (<xref ref-type="bibr" rid="bib20">Hyun et al., 2023</xref>; <xref ref-type="bibr" rid="bib33">Matias et al., 2017</xref>) and norepinephrine from the locus coeruleus (<xref ref-type="bibr" rid="bib50">Su and Cohen, 2022</xref>) to the cortical areas were shown to be involved in reversal learning. Further studies with biologically plausible network models, including neuromodulatory effects (<xref ref-type="bibr" rid="bib18">Harkin et al., 2023b</xref>; <xref ref-type="bibr" rid="bib57">Wert-Carvajal et al., 2022</xref>) or formal RL theories incorporating neuromodulators (<xref ref-type="bibr" rid="bib17">Harkin et al., 2023a</xref>) could provide further insights into the role of neuromodulators in reversal learning.</p></sec><sec id="s3-6"><title>Conclusion</title><p>Our findings show that, when performing a reversal learning task, a cortical circuit represent reversal probability, not only in stable stationary states as in a line attractor model, but also in dynamic neural trajectories that can accommodate non-stationary task-related behaviors necessary for the task. Such neural mechanism demonstrates the temporal flexibility of cortical computation and opens the opportunity for extending existing neural model for evidence accumulation by augmenting temporal dynamics.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Recurrent neural network</title><sec id="s4-1-1"><title>Network model</title><p>For the network model, we considered a continuous time recurrent neural network that operates in a dynamic regime relevant to cortical circuits. A strongly recurrent network with balanced excitation and inhibition has been known to capture canonical features of cortical neural activity, such as fluctuating activity and large trial-to-trial variability (<xref ref-type="bibr" rid="bib54">van Vreeswijk and Sompolinsky, 1996</xref>; <xref ref-type="bibr" rid="bib47">Sompolinsky et al., 1988</xref>; <xref ref-type="bibr" rid="bib24">Kadmon and Sompolinsky, 2015</xref>). A standard network model for such balanced state contains both excitatory and inhibitory neurons, and its network dynamics have been investigated extensively (<xref ref-type="bibr" rid="bib54">van Vreeswijk and Sompolinsky, 1996</xref>; <xref ref-type="bibr" rid="bib41">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib6">Brunel, 2000</xref>).</p><p>In our study, we considered a network consisting of recurrently connected inhibitory neurons only. There were no excitatory neurons. Instead, constant excitatory external input was applied to inhibitory neurons to sustain the network activity. A purely inhibitory network can also operate in the balanced regime, where external excitatory input and recurrent inhibitory activity balance each other (<xref ref-type="bibr" rid="bib24">Kadmon and Sompolinsky, 2015</xref>; <xref ref-type="bibr" rid="bib7">Brunel and Hansel, 2006</xref>).</p><p>The main reason for adopting a purely inhibitory network was due to the GPU memory issue when training RNNs to perform the reversal learning task. As detailed in Methods Training scheme below, the RNNs are first simulated over all the trials in a block and then backpropagation-through-time is applied to update the connection weights. Since a block consists of many trials, the long duration of a block limits the size of network that can be trained. We observed that if connection probability is <inline-formula><alternatives><mml:math id="inf163"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft163">\begin{document}$p=0.1$\end{document}</tex-math></alternatives></inline-formula>, and the population size of each neuron type is relatively large, then the excitatory-inhibitory network causes GPU memory overflow. This led us to consider a purely inhibitory network that operates in the balanced regime. Throughout network training, the signs of synaptic weights were preserved, resulting in a trained network that had only inhibitory synaptic connections.</p><p>The network dynamics were governed by the following equation<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle \tau \frac{d{\bf u}}{dt}= -{\bf u}+ W^{rec}\phi({\bf u}) +{\bf I}_{base}+{\bf I}_{cue}+{\bf I}_{feedback}$$\end{document}</tex-math></alternatives></disp-formula></p><p>and the network readout was<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle z ={\bf w}^{out}\cdot \phi({\bf u}).$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, τ is the intrinsic time constant of the RNN. <inline-formula><alternatives><mml:math id="inf164"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft164">\begin{document}${\bf u}\in \mathbb{R}^{N}$\end{document}</tex-math></alternatives></inline-formula> is the neural activity of population of <inline-formula><alternatives><mml:math id="inf165"><mml:mstyle><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft165">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> neurons. <inline-formula><alternatives><mml:math id="inf166"><mml:mstyle><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft166">\begin{document}$W^{rec}$\end{document}</tex-math></alternatives></inline-formula> is an <inline-formula><alternatives><mml:math id="inf167"><mml:mstyle><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft167">\begin{document}$N \times N$\end{document}</tex-math></alternatives></inline-formula> recurrent connectivity matrix with inhibitory synaptic weights: <inline-formula><alternatives><mml:math id="inf168"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft168">\begin{document}$W^{rec}_{ij}\lt 0$\end{document}</tex-math></alternatives></inline-formula> for all connection from neuron <inline-formula><alternatives><mml:math id="inf169"><mml:mi>j</mml:mi></mml:math><tex-math id="inft169">\begin{document}$j$\end{document}</tex-math></alternatives></inline-formula> to neuron <inline-formula><alternatives><mml:math id="inf170"><mml:mi>i</mml:mi></mml:math><tex-math id="inft170">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>. The activation function is sigmoidal, <inline-formula><alternatives><mml:math id="inf171"><mml:mstyle><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft171">\begin{document}$\phi(x) = 1 / (1+\exp[-(ax + b)])$\end{document}</tex-math></alternatives></inline-formula>, and is applied to <inline-formula><alternatives><mml:math id="inf172"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft172">\begin{document}${\bf u}$\end{document}</tex-math></alternatives></inline-formula> elementwise in <inline-formula><alternatives><mml:math id="inf173"><mml:mstyle><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft173">\begin{document}$\phi({\bf u})$\end{document}</tex-math></alternatives></inline-formula>. The baseline input <inline-formula><alternatives><mml:math id="inf174"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft174">\begin{document}${\bf I}_{base}$\end{document}</tex-math></alternatives></inline-formula> is constant in time and same for all neurons, the cue <inline-formula><alternatives><mml:math id="inf175"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft175">\begin{document}${\bf I}_{cue}$\end{document}</tex-math></alternatives></inline-formula> is turned on to signal the RNN to make a choice, and the feedback <inline-formula><alternatives><mml:math id="inf176"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft176">\begin{document}${\bf I}_{feedback}$\end{document}</tex-math></alternatives></inline-formula> provides information about the previous trial’s choice and reward outcome (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Four types of feedback inputs.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Network choice</th><th align="left" valign="bottom">Rewarded choice</th><th align="left" valign="bottom">Feedback</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="2">A</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">A1 (rewarded)</td></tr><tr><td align="left" valign="bottom">B</td><td align="left" valign="bottom">A0 (no reward)</td></tr><tr><td align="left" valign="bottom" rowspan="2">B</td><td align="left" valign="bottom">A</td><td align="left" valign="bottom">B0 (no reward)</td></tr><tr><td align="left" valign="bottom">B</td><td align="left" valign="bottom">B1 (rewarded)</td></tr></tbody></table></table-wrap><p>The duration of a trial was <inline-formula><alternatives><mml:math id="inf177"><mml:mstyle><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft177">\begin{document}$T=500\, \rm ms$\end{document}</tex-math></alternatives></inline-formula>. The intrinsic time constant <inline-formula><alternatives><mml:math id="inf178"><mml:mstyle><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft178">\begin{document}$\tau=20\, \rm ms$\end{document}</tex-math></alternatives></inline-formula> was significantly shorter than the trial duration. The feedback <inline-formula><alternatives><mml:math id="inf179"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft179">\begin{document}${\bf I}_{feedback}$\end{document}</tex-math></alternatives></inline-formula> was applied on the time interval <inline-formula><alternatives><mml:math id="inf180"><mml:mstyle><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft180">\begin{document}$[0,T_{feedback}]$\end{document}</tex-math></alternatives></inline-formula><italic>,</italic> and the cue <inline-formula><alternatives><mml:math id="inf181"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft181">\begin{document}${\bf I}_{cue}$\end{document}</tex-math></alternatives></inline-formula> was applied on the time interval <inline-formula><alternatives><mml:math id="inf182"><mml:mrow><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo separator="true">,</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:math><tex-math id="inft182">\begin{document}$[T_{cue}^{start}, T_{cue}^{end}]$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf183"><mml:mstyle><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>300</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft183">\begin{document}$T_{feedback}= 300\,\rm ms$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf184"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>250</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft184">\begin{document}$T_{cue}^{start}= 250\, \rm ms$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf185"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>300</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft185">\begin{document}$T_{cue}^{end}= 300\, \rm ms$\end{document}</tex-math></alternatives></inline-formula>. The feedback and cue overlapped during the cue period. The network choice was defined using the average of the readout <inline-formula><alternatives><mml:math id="inf186"><mml:mi>z</mml:mi></mml:math><tex-math id="inft186">\begin{document}$z$\end{document}</tex-math></alternatives></inline-formula> on the time interval <inline-formula><alternatives><mml:math id="inf187"><mml:mstyle><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft187">\begin{document}$[T_{choice}^{start}, T_{choice}^{end}]$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf188"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>350</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft188">\begin{document}$T_{choice}^{start}=350\, \rm ms$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf189"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>400</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft189">\begin{document}$T_{choice}^{end}=400\, \rm ms$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The feedback <inline-formula><alternatives><mml:math id="inf190"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft190">\begin{document}${\bf I}_{feedback}$\end{document}</tex-math></alternatives></inline-formula> and cue <inline-formula><alternatives><mml:math id="inf191"><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft191">\begin{document}${\bf I}_{cue}$\end{document}</tex-math></alternatives></inline-formula> were random vectors where each element was sample from Gaussian distribution with mean zero and standard deviation 0.9.</p></sec><sec id="s4-1-2"><title>Reduced model</title><p>One-dimensional reduction of the network dynamics in a subspace defined by a task vector, <inline-formula><alternatives><mml:math id="inf192"><mml:mi mathvariant="bold">v</mml:mi></mml:math><tex-math id="inft192">\begin{document}${\bf v}$\end{document}</tex-math></alternatives></inline-formula>, was derived as follows (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). The projection of network activity onto the task vector was<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle x = \langle \phi({\bf u}),{\bf v}\rangle.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Then, the dynamics of the projected activity is governed by<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle \tau \frac{dx}{dt}= \left \langle \nabla_{{\bf u}}\phi({\bf u}) \cdot \frac{d{\bf u}}{dt},{\bf v}\right \rangle = x_{rec}+ x_{ext}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟨</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle x_{rec}= \Big \langle \nabla_{{\bf u}}\phi({\bf u}) \cdot [-{\bf u}+ W^{rec}\phi ({\bf u})],{\bf v}\Big \rangle$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟨</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">⟩</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle x_{ext}= \Big \langle \nabla_{{\bf u}}\phi({\bf u}) \cdot{\bf I},{\bf v}\Big \rangle.$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf193"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft193">\begin{document}$x_{rec}$\end{document}</tex-math></alternatives></inline-formula> includes both the decay and recurrent terms, and <inline-formula><alternatives><mml:math id="inf194"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft194">\begin{document}$x_{ext}$\end{document}</tex-math></alternatives></inline-formula> accounts for all external inputs <inline-formula><alternatives><mml:math id="inf195"><mml:mstyle><mml:mrow><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft195">\begin{document}${\bf I}={\bf I}_{base}+{\bf I}_{cue}+{\bf I}_{feedback}$\end{document}</tex-math></alternatives></inline-formula>.</p></sec></sec><sec id="s4-2"><title>Reversal learning task</title><sec id="s4-2-1"><title>Experiment setup for monkeys</title><p>The experimental setup for the animals was reported in a previous work (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>). Here, we provide a summary of the behavioral task and neural recordings.</p><p>Two animals performed the reversal learning task in blocks of trials over eight sessions. In each trial, the animals were required to fixate centrally for a variable time (400 - 800 ms), and, subsequently, two cues were presented to the left and right of fixation dot. The animals made a saccade to select a target, hold sight for 500 ms, and reward for the choice was delivered stochastically. In What blocks, one image was designated as the high-value option, while the other image was designated as the low-value option at the beginning of a block, regardless of the location of the images. The high-value option was rewarded 70% of the time when chosen, and the low-value option was rewarded 30% of the time when chosen. In Where blocks, one location (e.g. left) was designated as the high-value option, while the other location (e.g. right) was designated as the low-value option, regardless of the actual images at those locations. Each block consisted of 80 trials. The reward probability of two options was switched at a random trial, within 20 trials centered around the mid-trial of a block. The animals explored available option to identify the block types and best options. The animals’ choice (i.e. object location and identity) and the reward outcome (i.e. rewarded or not rewarded) in all the trials were recorded for further analysis.</p><p>The extracellular activity of neural populations was recorded in the dorso-lateral prefrontal cortex from both hemispheres, using eight multielectrode arrays while the monkeys performed the task. The size of neuronal populations had a range of 573 to 1023 with a median 706. The recorded neurons were evenly distributed across left and right hemispheres. To analyze the spiking activity <inline-formula><alternatives><mml:math id="inf196"><mml:mstyle><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft196">\begin{document}$y_{it}(k)$\end{document}</tex-math></alternatives></inline-formula> of neuron <inline-formula><alternatives><mml:math id="inf197"><mml:mstyle><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft197">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf198"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft198">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> and trial <inline-formula><alternatives><mml:math id="inf199"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft199">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, a 300 ms-wide time window centered at time <inline-formula><alternatives><mml:math id="inf200"><mml:mi>t</mml:mi></mml:math><tex-math id="inft200">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> was slided forward in time with 20 ms increment, as the number of spikes neuron <inline-formula><alternatives><mml:math id="inf201"><mml:mi>i</mml:mi></mml:math><tex-math id="inft201">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> emitted in each time window was counted.</p></sec><sec id="s4-2-2"><title>Training setup for RNNs</title><p><italic>Overview</italic>: To train the network, we used a block consisting of <inline-formula><alternatives><mml:math id="inf202"><mml:mstyle><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft202">\begin{document}$T=24$\end{document}</tex-math></alternatives></inline-formula> trials. For testing the trained RNNs, as described in the main text, we expanded the number of trials in a block to 36 trials (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for an example block). The reversal trial <inline-formula><alternatives><mml:math id="inf203"><mml:mi>r</mml:mi></mml:math><tex-math id="inft203">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> was sampled randomly and uniformly from 10 trials around the midtrial:.<disp-formula id="equ7"><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mspace width="1em"/><mml:mtext>where</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2.</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle r \in Unif[T_{m}-5, T_{m}+5] \quad \text{where}\quad T_{m}= T/2.$$\end{document}</tex-math></alternatives></disp-formula></p><p>The network made a choice in each trial: A or B. To model which choice was rewarded, we generated a ‘rewarded’ choice for each trial. One of the choices was more likely to be rewarded than the other. The network’s choice was compared to the rewarded choice, and the network received a feedback that signaled its choice and reward outcome (e.g., chose A and received a reward). The option that yielded higher reward prior to the reversal trial was switched to the other option at the reversal trial.</p><p>To train the network to reverse its preferred choice, we used the output of an ideal Bayesian observer model as teaching signal. Specifically, we first inferred the scheduled reversal trial (i.e. the trial at which reward probability switched) using the Bayesian model. Then, the network was trained to flip its preferred choice a few trials after the inferred scheduled reversal trial, such that network’s behavioral reversal trial occurred a few trials after the scheduled reversal trial.</p><p>Note that, although we refer to ‘rewarded’ choices, there were no actual rewards in our network model. The ‘rewarded’ choices were set up to define feedback inputs that mimic the reward outcomes monkey received.</p></sec><sec id="s4-2-3"><title>Experiment variables</title><p>The important variables for training the RNN were network choice, rewarded choice and feedback.</p><sec id="s4-2-3-1"><title>Network choice</title><p>To define network choice, we symmetrized the readout <inline-formula><alternatives><mml:math id="inf204"><mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft204">\begin{document}${\bf z}^{sym}= (z,-z)$\end{document}</tex-math></alternatives></inline-formula> and computed its log-softmax <inline-formula><alternatives><mml:math id="inf205"><mml:mstyle><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:mi>s</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:mi>s</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft205">\begin{document}$f({\bf z}^{sym}) = (\frac{e^{z}}{s}, \frac{e^{-z}}{s}) \equiv (f_{0}, f_{1})$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf206"><mml:mstyle><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft206">\begin{document}$s = e^{z}+ e^{-z}$\end{document}</tex-math></alternatives></inline-formula>. The network choice was<disp-formula id="equ8"><label>(7)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd/><mml:mtd><mml:mi>A</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>B</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle c = \arg\max (f_{0},f_{1}) = \begin{cases}&amp;A \quad \text{if}\quad f_{0}&gt; f_{1}\\&amp;B \quad \text{if}\quad f_{0}\lt f_{1}\end{cases}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ9"><label>(8)</label><alternatives><mml:math id="m9"><mml:mtable displaystyle="true" columnalign="left right right" class="tml-jot" style="width:100%;"><mml:mtr><mml:mtd class="tml-right" style="padding:0;width:50%;padding-left:0em;padding-right:0em;"/><mml:mtd class="tml-right" style="padding-left:1em;padding-right:0em;"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo separator="true">,</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>1.</mml:mn></mml:mrow></mml:mtd><mml:mtd class="tml-right" style="padding:0;width:50%;padding-left:0em;padding-right:0em;"><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="t9">\begin{document}$$\displaystyle \begin{align}A=0, B=1.\end{align}$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-2-3-2"><title>Rewarded choice</title><p>To model stochastic rewards, rewarded choices were generated probabilistically for each trial <inline-formula><alternatives><mml:math id="inf207"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft207">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ10"><label>(9)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle \begin{array}{ll}&amp; P_r({\rm rewarded \,choice}=A)=q_k\\ &amp; P_r({\rm rewarded \,choice}=B)=1-q_k. \end{array}$$\end{document}</tex-math></alternatives></disp-formula></p><p>The reversal of reward schedule was implemented by switching the target probability at the scheduled reversal trial of the block, denoted by <inline-formula><alternatives><mml:math id="inf208"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft208">\begin{document}$r_{sch}$\end{document}</tex-math></alternatives></inline-formula>.<disp-formula id="equ11"><label>(10)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mtext>for</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mtext>for</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \begin {aligned} ({\rm before\, reversal})\,q_k &amp;=p\, \text{for} \, k\lt r_{sch}\\ ({\rm after \,reversal})\, q_k &amp;=1-p\, \text{for}\, k\geq r_{sch}.$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-2-3-3"><title>Feedback</title><p>We considered that reward is delivered when the network choice agreed with the rewarded choice, and no reward is delivered when they disagreed. This led to four types of feedback inputs show in <xref ref-type="table" rid="table1">Table 1</xref>.</p></sec></sec></sec><sec id="s4-3"><title>Bayesian inference model</title><p>Here we formulate Bayesian models that infer the scheduled reversal trial or the behavior reversal trial.</p><sec id="s4-3-1"><title>Ideal observer model</title><p>The ideal observer model, developed previously (<xref ref-type="bibr" rid="bib10">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>), inferred the scheduled reversal trial and assumed that (a) the target probability was known (<xref ref-type="disp-formula" rid="equ10">Equation 9</xref>) and (b) it switched at the reversal trial (<xref ref-type="disp-formula" rid="equ11">Equation 10</xref>).</p><p>The data available to the ideal observer were the choice <inline-formula><alternatives><mml:math id="inf209"><mml:mstyle><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft209">\begin{document}$y_{k}\in \{A, B\}$\end{document}</tex-math></alternatives></inline-formula> and the reward outcome <inline-formula><alternatives><mml:math id="inf210"><mml:mstyle><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft210">\begin{document}$z_{k}\in \{0, 1\}$\end{document}</tex-math></alternatives></inline-formula> at all the trials <inline-formula><alternatives><mml:math id="inf211"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft211">\begin{document}$k \in [1,T]$\end{document}</tex-math></alternatives></inline-formula>. We inferred the posterior distribution of scheduled reversal at trials <inline-formula><alternatives><mml:math id="inf212"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft212">\begin{document}$k \in [1,T]$\end{document}</tex-math></alternatives></inline-formula>. By Bayes’ rule<disp-formula id="equ12"><label>(11)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle p(r \vert y,z) = p(y,z \vert r) p(r) / p(y,z).$$\end{document}</tex-math></alternatives></disp-formula></p><p>We evaluated the posterior distribution of <inline-formula><alternatives><mml:math id="inf213"><mml:mi>r</mml:mi></mml:math><tex-math id="inft213">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> when data were available up to any trial <inline-formula><alternatives><mml:math id="inf214"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft214">\begin{document}$t \leq T$\end{document}</tex-math></alternatives></inline-formula>. The likelihood function <inline-formula><alternatives><mml:math id="inf215"><mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft215">\begin{document}$f_{IO}(r) = p(y_{1:t},z_{1:t}\vert r)$\end{document}</tex-math></alternatives></inline-formula> of the ideal observer was defined by<disp-formula id="equ13"><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle f_{IO}(r) = \prod_{k=1}^{t}q_{k}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>For <inline-formula><alternatives><mml:math id="inf216"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math><tex-math id="inft216">\begin{document}$k \lt r$\end{document}</tex-math></alternatives></inline-formula>,<disp-formula id="equ14"><label>(12)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle q_{k}= p \quad \text{if}\quad y_{k}=A, \; z_{k}= 1$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ15"><label>(13)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle = 1 - p \quad \text{if}\quad y_{k}= A, \; z_{k}= 0 $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ16"><label>(14)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle = 1 - p \quad \text{if}\quad y_{k}= B, \; z_{k}= 1 $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ17"><label>(15)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle = p \quad \text{if}\quad y_{k}= B, \; z_{k}= 0.$$\end{document}</tex-math></alternatives></disp-formula></p><p>For <inline-formula><alternatives><mml:math id="inf217"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math><tex-math id="inft217">\begin{document}$k \geq r$\end{document}</tex-math></alternatives></inline-formula>,<disp-formula id="equ18"><label>(16)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle q_{k}= 1-p \quad \text{if}\quad y_{k}=A, \; z_{k}= 1 $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ19"><label>(17)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle = p \quad \text{if}\quad y_{k}= A, \; z_{k}= 0 $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ20"><label>(18)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle = p \quad \text{if}\quad y_{k}= B, \; z_{k}= 1$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ21"><label>(19)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle = 1- p \quad \text{if}\quad y_{k}= B, \; z_{k}= 0.$$\end{document}</tex-math></alternatives></disp-formula></p><p>To obtain the posterior distribution of <inline-formula><alternatives><mml:math id="inf218"><mml:mstyle><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft218">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ12">Equation 11</xref>), the likelihood function <inline-formula><alternatives><mml:math id="inf219"><mml:mstyle><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft219">\begin{document}$f_{IO}(r)$\end{document}</tex-math></alternatives></inline-formula> was evaluated for all <inline-formula><alternatives><mml:math id="inf220"><mml:mstyle><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft220">\begin{document}$r \in [1,t]$\end{document}</tex-math></alternatives></inline-formula><italic>,</italic> assuming flat prior <inline-formula><alternatives><mml:math id="inf221"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft221">\begin{document}$p(r)$\end{document}</tex-math></alternatives></inline-formula> and normalizing by the choice and reward data <inline-formula><alternatives><mml:math id="inf222"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft222">\begin{document}$p(y_{1:t},z_{1:t})$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-3-2"><title>Behavioral model</title><p>To infer the trial at which choice reversed, i.e., behavior reversal, we used a likelihood function that assumed the preferred choice probability switched at the behavior reversal. Here, the reward schedule was not known.</p><p>The data available to the behavioral model were the choice <inline-formula><alternatives><mml:math id="inf223"><mml:mstyle><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft223">\begin{document}$y_{k}\in \{A, B\}$\end{document}</tex-math></alternatives></inline-formula> at all the trials <inline-formula><alternatives><mml:math id="inf224"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft224">\begin{document}$k \in [1,T]$\end{document}</tex-math></alternatives></inline-formula>. We inferred the posterior distribution of behavior reversal at trials <inline-formula><alternatives><mml:math id="inf225"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft225">\begin{document}$k \in [1,T]$\end{document}</tex-math></alternatives></inline-formula>. By Bayes’ rule<disp-formula id="equ22"><label>(20)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle p(r \vert y) = p(y \vert r) p(r) / p(y).$$\end{document}</tex-math></alternatives></disp-formula></p><p>The likelihood function for the behavioral model was<disp-formula id="equ23"><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle f_{BM}(r) = \prod_{k=1}^{t}q_{k}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>For <inline-formula><alternatives><mml:math id="inf226"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft226">\begin{document}$k \lt r$\end{document}</tex-math></alternatives></inline-formula>,<disp-formula id="equ24"><label>(22)</label><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle  q_{k}&amp;= p \quad \text{if}\quad y_{k}=A \\ &amp;= 1 - p \quad \text{if}\quad y_{k}= B.$$\end{document}</tex-math></alternatives></disp-formula></p><p>For <inline-formula><alternatives><mml:math id="inf227"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math><tex-math id="inft227">\begin{document}$k \geq r$\end{document}</tex-math></alternatives></inline-formula>,<disp-formula id="equ25"><label>(24)</label><alternatives><mml:math id="m25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t25">\begin{document}$$\displaystyle  q_{k}&amp;= 1-p \quad \text{if}\quad y_{k}=A \\&amp;= p \quad \text{if}\quad y_{k}= B.$$\end{document}</tex-math></alternatives></disp-formula></p><p>To obtain the posterior distribution of <inline-formula><alternatives><mml:math id="inf228"><mml:mi>r</mml:mi></mml:math><tex-math id="inft228">\begin{document}$r$\end{document}</tex-math></alternatives></inline-formula>, we assumed flat prior <inline-formula><alternatives><mml:math id="inf229"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft229">\begin{document}$p(r)$\end{document}</tex-math></alternatives></inline-formula>, as in the ideal observer, and normalized by the choice data <inline-formula><alternatives><mml:math id="inf230"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft230">\begin{document}$p(y_{1:t})$\end{document}</tex-math></alternatives></inline-formula>.</p></sec></sec><sec id="s4-4"><title>Training scheme</title><sec id="s4-4-1"><title>Overview</title><p>The ideal observer successfully inferred a scheduled reversal trial, which occurred randomly around the mid-trial. To learn to switch its preferred choice, we trained the network to learn from scheduled reversal trials inferred from the ideal observer. In other words, in a block consisting of <inline-formula><alternatives><mml:math id="inf231"><mml:mstyle><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft231">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> trials, the network choices and reward outcomes were fed into the ideal observer model to infer the randomly chosen scheduled reversal trial. Then, the network was trained to switch its preferred choice a few trials after the inferred reversal trial. This delay in the behavior reversal from the scheduled reversal was observed in monkey’s reversal behavior (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>) and a running estimate of the Maximum a Posterior of the reversal probability (see Step 3 below). As the inferred scheduled reversal trial varied across blocks, the network learned to reverse its choice in a block-dependent manner.</p><p>Below, we described the specific steps taken to train the network.</p><p><italic>Step 1</italic>. Simulate the network starting from a random initial state, apply the external inputs, i.e., cue and feedback inputs, at each trial, and store the network choices and reward outcomes at all the trials in a block. The network dynamics is driven by the external inputs applied periodically over the trials.</p><p><italic>Step 2</italic>. Apply the ideal observer model to network’s choice and reward data to infer the scheduled reversal.</p><p><italic>Step 3</italic>. Identify the trial <inline-formula><alternatives><mml:math id="inf232"><mml:mstyle><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft232">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula> at which network choice should be reversed.</p><p>The main observation is that the running estimate of Maximum a Posterior (MAP) of the reversal probability obtained from the ideal observer model converges a few trials past the MAP estimate. In other words, let<disp-formula id="equ26"><alternatives><mml:math id="m26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mtext>MAP estimate</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="1em"/><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>Running estimate</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="1em"/><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t26">\begin{document}$$\displaystyle  \text{MAP estimate}: \quad r^{*}_{T}&amp;= \underset{k \in [1,T]}{\rm argmax} \, p(r =k \vert D_{1:T}) \\ \text{Running estimate}:\quad r^{*}_{t}&amp;=\underset{{k \in [1,t]}}{\rm argmax} \, p(r=k \vert D_{1:t})$$\end{document}</tex-math></alternatives></disp-formula></p><p>then,<disp-formula id="equ27"><alternatives><mml:math id="m27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">→</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="t27">\begin{document}$$\displaystyle r^{*}_{t}\to r^{*}_{T}\quad \text{if}\quad t \gt r^{*}_{T}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where the convergence occurs around<disp-formula id="equ28"><alternatives><mml:math id="m28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mspace width="1em"/><mml:mtext>with</mml:mtext><mml:mspace width="1em"/><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>4.</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t28">\begin{document}$$\displaystyle t^{*}= r^{*}_{T}+ \delta \quad \text{with}\quad \delta = 4.$$\end{document}</tex-math></alternatives></disp-formula></p><p>This observation can be interpreted as follows. If a subject performing the reversal learning task employs the ideal observer model to detect the trial at which the reward schedule is reversed, the subject can infer the reversal of reward schedule a few trials past the actual reversal and then switch its preferred choice. This delay in behavioral reversal, relative to the reversal of reward schedule, is consistent with the monkeys switching their preferred choice a few trials after the reversal of reward schedule (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>).</p><p><italic>Step 4</italic>. Construct the choice sequences the network will learn.</p><p>We used the observation from Step 3 to define target choice outputs that switch abruptly a few trials after the reversal of reward schedule, denoted as <inline-formula><alternatives><mml:math id="inf233"><mml:mstyle><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft233">\begin{document}$t^{*}$\end{document}</tex-math></alternatives></inline-formula> in the following. In each block, the high-value option at the start of a trial was selected randomly between two options. If a block had <inline-formula><alternatives><mml:math id="inf234"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>∗</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft234">\begin{document}$t^{(*)}$\end{document}</tex-math></alternatives></inline-formula> as its initial high-value option, the target choice outputs were<disp-formula id="equ29"><alternatives><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mtext>choice</mml:mtext><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mtext>choice</mml:mtext><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t29">\begin{document}$$\displaystyle  \text{choice}_{IO}(k)&amp;= A \quad \text{if}\quad k \lt t^{*}\\ \text{choice}_{IO}(k)&amp;= B \quad \text{if}\quad k \geq t^{*}$$\end{document}</tex-math></alternatives></disp-formula></p><p>On the other hand, if a block had <inline-formula><alternatives><mml:math id="inf235"><mml:mstyle><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft235">\begin{document}$B$\end{document}</tex-math></alternatives></inline-formula> as its initial high-value option, the target choice outputs were<disp-formula id="equ30"><alternatives><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mtext>choice</mml:mtext><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mtext>choice</mml:mtext><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="1em"/><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t30">\begin{document}$$\displaystyle  \text{choice}_{IO}(k)&amp;= B \quad \text{if}\quad k \lt t^{*}\\ \text{choice}_{IO}(k)&amp;= A \quad \text{if}\quad k \geq t^{*}$$\end{document}</tex-math></alternatives></disp-formula></p><p>An example of target choice outputs with <inline-formula><alternatives><mml:math id="inf236"><mml:mstyle><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft236">\begin{document}$A$\end{document}</tex-math></alternatives></inline-formula> as its initial high-value option is shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>.</p><p><italic>Step 5</italic>. Define the loss function of a block.<disp-formula id="equ31"><alternatives><mml:math id="m31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>C</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>E</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mtext>choice</mml:mtext><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mtext>choice</mml:mtext><mml:mrow><mml:mi>I</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t31">\begin{document}$$\displaystyle  loss = \sum_{k=1}^{T}CrossEnt \Big(\text{choice}_{actual}(k), \text{choice}_{IO}(k) \Big)$$\end{document}</tex-math></alternatives></disp-formula></p><p><italic>Step 6</italic>. Train the recurrent connectivity weights <inline-formula><alternatives><mml:math id="inf237"><mml:mstyle><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft237">\begin{document}$W^{\,rec}$\end{document}</tex-math></alternatives></inline-formula> and the readout weights <inline-formula><alternatives><mml:math id="inf238"><mml:msup><mml:mi mathvariant="bold">w</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft238">\begin{document}${\bf w}^{out}$\end{document}</tex-math></alternatives></inline-formula> with backpropagation using Adam optimizer with learning rate 10<sup>-2</sup>. The learning rate was decayed by a factor 0.9 every 3 epochs. The batch size (i.e. the number of networks trained) was 256. The training was continued until the fraction of rewarded trials was close to reward probability <inline-formula><alternatives><mml:math id="inf239"><mml:mstyle><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft239">\begin{document}$p$\end{document}</tex-math></alternatives></inline-formula> of the preferred option.</p></sec></sec><sec id="s4-5"><title>Targeted dimensionality reduction</title><p>Targeted dimensionality reduction (TDR) identifies population vectors that encode task variables explicitly or implicitly utilized in the experiment the subject or RNN performs (<xref ref-type="bibr" rid="bib32">Mante et al., 2013</xref>). In this study, we were interested in identifying population vectors that encode choice preference and reversal probability. Once those task vectors were identified, we analyzed the neural activity projected to those vectors to investigate neural representation of task variables.</p><p>We describe how TDR was performed in our study (see <xref ref-type="bibr" rid="bib32">Mante et al., 2013</xref> for the original reference). First, we regressed the neural activity of each neuron at each time point onto task variables of interest. Then we used the matrix of regression coefficients (i.e. neuron by time) to identify the task vector. Let <inline-formula><alternatives><mml:math id="inf240"><mml:mstyle><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft240">\begin{document}$y_{it}(k)$\end{document}</tex-math></alternatives></inline-formula> be the spiking rate of neuron <inline-formula><alternatives><mml:math id="inf241"><mml:mstyle><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft241">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> at time <inline-formula><alternatives><mml:math id="inf242"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft242">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> on trial <inline-formula><alternatives><mml:math id="inf243"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft243">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> where we have <inline-formula><alternatives><mml:math id="inf244"><mml:mstyle><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft244">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> neurons and <inline-formula><alternatives><mml:math id="inf245"><mml:mstyle><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft245">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> time points. We regressed the spiking activity on task variables of interest <inline-formula><alternatives><mml:math id="inf246"><mml:mstyle><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft246">\begin{document}$z^{v}(k)$\end{document}</tex-math></alternatives></inline-formula> where the task variables were <inline-formula><alternatives><mml:math id="inf247"><mml:mstyle><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft247">\begin{document}$v \in$\end{document}</tex-math></alternatives></inline-formula> {reversal probability, choice preference, direction, object, block type, reward outcome, trial number}. For each neuron-time pair, <inline-formula><alternatives><mml:math id="inf248"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft248">\begin{document}$(i,t)$\end{document}</tex-math></alternatives></inline-formula>, we performed linear regression over all trials <inline-formula><alternatives><mml:math id="inf249"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft249">\begin{document}$k \in [0,T]$\end{document}</tex-math></alternatives></inline-formula> with a bias:<disp-formula id="equ32"><label>(25)</label><alternatives><mml:math id="m32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t32">\begin{document}$$\displaystyle  y_{it}(k) = \sum_{v}\beta_{it}^{v}z^{v}(k) + bias_{it}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>This regression analysis yielded an <inline-formula><alternatives><mml:math id="inf250"><mml:mstyle><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft250">\begin{document}$N \times M$\end{document}</tex-math></alternatives></inline-formula> coefficient matrix <inline-formula><alternatives><mml:math id="inf251"><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msubsup></mml:math><tex-math id="inft251">\begin{document}$\beta_{it}^{v}$\end{document}</tex-math></alternatives></inline-formula> for each task variable, <inline-formula><alternatives><mml:math id="inf252"><mml:mi>v</mml:mi></mml:math><tex-math id="inft252">\begin{document}$v$\end{document}</tex-math></alternatives></inline-formula>. We considered this coefficient matrix as a population vector evolving in time: <inline-formula><alternatives><mml:math id="inf253"><mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft253">\begin{document}${\boldsymbol \beta}^{v}_{t}= (\beta_{1t}^{v}, ..., \beta_{Nt}^{v})$\end{document}</tex-math></alternatives></inline-formula>. Then, a task vector was defined as the population vector <inline-formula><alternatives><mml:math id="inf254"><mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft254">\begin{document}${\bf w}^{v}\in \mathbb{R}^{N}$\end{document}</tex-math></alternatives></inline-formula> at which the <inline-formula><alternatives><mml:math id="inf255"><mml:mstyle><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft255">\begin{document}$L_{2}$\end{document}</tex-math></alternatives></inline-formula>-norm <inline-formula><alternatives><mml:math id="inf256"><mml:mstyle><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft256">\begin{document}$\Vert{\boldsymbol \beta}^{v}_{t}\Vert_{2}$\end{document}</tex-math></alternatives></inline-formula> achieved its maximum:<disp-formula id="equ33"><label>(26)</label><alternatives><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mi>v</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo form="prefix">max</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mi>t</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t33">\begin{document}$$\displaystyle  \mathbf{w}^v &amp;= \boldsymbol{\beta}^v_{t_{\max}} \\ t_{\max} &amp;= \arg\max\limits_t \left\| \boldsymbol{\beta}^v_t \right\|_2.$$\end{document}</tex-math></alternatives></disp-formula></p><p>We performed QR-decomposition on the matrix of task vectors <inline-formula><alternatives><mml:math id="inf257"><mml:mstyle><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft257">\begin{document}$W = [{\bf w}_{rev},{\bf w}_{choice}, ...]$\end{document}</tex-math></alternatives></inline-formula> to orthogonalize the task vectors. Then, the population activity was projected onto each (orthogonalized) task vector to obtain the neural activity encoding each task variable:<disp-formula id="equ34"><label>(27)</label><alternatives><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t34">\begin{document}$$\displaystyle x^{v}_{t}(k) ={\bf w}^{v}\cdot{\bf y}_{t}(k)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf258"><mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft258">\begin{document}${\bf y}_{t}(k) = (y_{1t}(k), ..., y_{Nt}(k))$\end{document}</tex-math></alternatives></inline-formula> is the population activity at time <inline-formula><alternatives><mml:math id="inf259"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft259">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> on trial <inline-formula><alternatives><mml:math id="inf260"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft260">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-6"><title>Reward integration equation</title><p>To derive the reward integration equation shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, we considered the neural activity in a subspace encoding the reversal probability:<disp-formula id="equ35"><label>(28)</label><alternatives><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t35">\begin{document}$$\displaystyle x^{k}_{rev}(t) ={\bf w}^{rev}\cdot{\bf y}^{k}(t).$$\end{document}</tex-math></alternatives></disp-formula></p><p>We analyzed the neural activity at the time of cue onset <inline-formula><alternatives><mml:math id="inf261"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft261">\begin{document}$t = t_{on}$\end{document}</tex-math></alternatives></inline-formula> and obtained a sequence of reversal probability activity across trials: <inline-formula><alternatives><mml:math id="inf262"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft262">\begin{document}$x_{rev}^{0}(t_{on}), \dots, x_{rev}^{K}(t_{on})$\end{document}</tex-math></alternatives></inline-formula>. To set up the reward integration equation<disp-formula id="equ36"><label>(29)</label><alternatives><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mtext>with</mml:mtext><mml:mspace width="1em"/><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t36">\begin{document}$$\displaystyle x_{pred}^{k+1}(t_{on}) = x_{pred}^{k}(t_{on}) + R^{\pm}_{k}(t_{on}) \quad \text{with}\quad x_{pred}^{0}(t_{on}) = x^{0}_{rev}(t_{on}),$$\end{document}</tex-math></alternatives></disp-formula></p><p>we estimated the update <inline-formula><alternatives><mml:math id="inf263"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft263">\begin{document}$R^{\pm}_{k}(t_{on})$\end{document}</tex-math></alternatives></inline-formula> driven by reward outcomes at each trial <inline-formula><alternatives><mml:math id="inf264"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft264">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>. Specifically, the update term was defined as the block-average of the difference of reversal probability activity at adjacent trials:<disp-formula id="equ37"><label>(30)</label><alternatives><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="1.1em 1.1em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mtext>rev</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mtext>rev</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mtext>rev</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>R</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mtext>rev</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mtext>if rewarded at trial </mml:mtext><mml:mi>k</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>R</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mtext>rev</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mtext>if not rewarded at trial </mml:mtext><mml:mi>k</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t37">\begin{document}$$\displaystyle  \Delta x_{\text{rev}}^{k}(t_{\text{on}}) &amp;= x_{\text{rev}}^{k+1}(t_{\text{on}}) - x_{\text{rev}}^{k}(t_{\text{on}}) \\[8pt] R_k^+(t_{\text{on}}) &amp;= \left\langle \Delta x_{\text{rev}}^{k}(t_{\text{on}}) \right\rangle_{b_k^+} \quad \text{if rewarded at trial } k \\[8pt] R_k^-(t_{\text{on}}) &amp;= \left\langle \Delta x_{\text{rev}}^{k}(t_{\text{on}}) \right\rangle_{b_k^-} \quad \text{if not rewarded at trial } k. $$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf265"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft265">\begin{document}$b^{+}_{k}$\end{document}</tex-math></alternatives></inline-formula> denotes all the blocks across sessions (or networks) in which reward was received at trial <inline-formula><alternatives><mml:math id="inf266"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft266">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>. Similarly, <inline-formula><alternatives><mml:math id="inf267"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft267">\begin{document}$b^{-}_{k}$\end{document}</tex-math></alternatives></inline-formula> denotes all the blocks in which reward was not received at trial <inline-formula><alternatives><mml:math id="inf268"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft268">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>.</p><p>To predict <inline-formula><alternatives><mml:math id="inf269"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft269">\begin{document}$x_{rev}^{k}(t_{on})$\end{document}</tex-math></alternatives></inline-formula>, we set the initial value <inline-formula><alternatives><mml:math id="inf270"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft270">\begin{document}$x_{pred}^{0}(t_{on}) = x^{0}_{rev}(t_{on})$\end{document}</tex-math></alternatives></inline-formula> at trial 0 and sequentially predicted the following trials using <xref ref-type="disp-formula" rid="equ36">Equation 29</xref> with the update term from <xref ref-type="disp-formula" rid="equ37">Equation 30</xref>. The same analysis was performed at different time points <inline-formula><alternatives><mml:math id="inf271"><mml:mi>t</mml:mi></mml:math><tex-math id="inft271">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula>. We derived integration equations for each time and assessed its prediction accuracy as shown in <xref ref-type="fig" rid="fig3">Figure 3F</xref>.</p><p>To evaluate the contribution of reward and no-reward outcomes on the average responses of <inline-formula><alternatives><mml:math id="inf272"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft272">\begin{document}$\Delta x_{rev}^{k}(t)$\end{document}</tex-math></alternatives></inline-formula> over blocks, we computed<disp-formula id="equ38"><label>(31)</label><alternatives><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t38">\begin{document}$$\displaystyle \langle \Delta x_{rev}^{k}(t) \rangle_{b_k}= f^{+}_{k}\langle \Delta x_{rev}^{k}(t) \rangle_{b^+_k}+ f^{-}_{k}\langle \Delta x_{rev}^{k}(t) \rangle_{b^-_k}= f^{+}_{k}R^{+}_{k}(t) + f^{-}_{k}R^{-}_{k}(t)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ39"><label>(32)</label><alternatives><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math><tex-math id="t39">\begin{document}$$\displaystyle f^{+}_{k}= \frac{\vert b^{+}_{k}\vert}{\vert b_{k}\vert}, \quad f^{-}_{k}= \frac{\vert b^{-}_{k}\vert}{\vert b_{k}\vert}$$\end{document}</tex-math></alternatives></disp-formula></p><p>with <inline-formula><alternatives><mml:math id="inf273"><mml:mstyle><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>∪</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft273">\begin{document}$b_{k}= b_{k}^{+}\cup b_{k}^{-}$\end{document}</tex-math></alternatives></inline-formula> denote the fractions of reward and no-reward blocks at trial <inline-formula><alternatives><mml:math id="inf274"><mml:mi>k</mml:mi></mml:math><tex-math id="inft274">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>. In <xref ref-type="fig" rid="fig5">Figure 5D</xref> and <xref ref-type="fig" rid="fig6">Figure 6A</xref>, the weighted responses, i.e., <inline-formula><alternatives><mml:math id="inf275"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft275">\begin{document}$f^{+}_{k}R^{+}_{k}(t)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf276"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft276">\begin{document}$f^{-}_{k}R^{-}_{k}(t)$\end{document}</tex-math></alternatives></inline-formula>, were shown.</p></sec><sec id="s4-7"><title>Contraction factor of reversal probability activity</title><p>We defined a contraction factor to quantify whether the reversal probability activity <inline-formula><alternatives><mml:math id="inf277"><mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft277">\begin{document}$x_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> contracts to or diverges from its mean activity on a short time interval. The contraction factor on the <inline-formula><alternatives><mml:math id="inf278"><mml:mstyle><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft278">\begin{document}$n^{th}$\end{document}</tex-math></alternatives></inline-formula> time interval <inline-formula><alternatives><mml:math id="inf279"><mml:mstyle><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft279">\begin{document}$[nL, (n+1)L]$\end{document}</tex-math></alternatives></inline-formula> of length <inline-formula><alternatives><mml:math id="inf280"><mml:mstyle><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft280">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula> was defined to be the coefficient of a one-dimensional autoregressive equation the mean-centered reversal probability activity <inline-formula><alternatives><mml:math id="inf281"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft281">\begin{document}$z_{rev}^{(n)}(t)$\end{document}</tex-math></alternatives></inline-formula> satisfied.<disp-formula id="equ40"><alternatives><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="1em"/><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="t40">\begin{document}$$\displaystyle z_{rev}^{(n)}(t+1) = az_{rev}^{(n)}(t) \quad \text{for}\quad t=0,...,L-1$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ41"><alternatives><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>m</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t41">\begin{document}$$\displaystyle  z_{rev}^{(n)}(t)&amp;  = x_{rev}(nL + t) - m \\ m&amp;  = \frac{1}{L}\sum_{t=0}^{L}x_{rev}(nL + t).$$\end{document}</tex-math></alternatives></disp-formula></p><p>The contraction factor <inline-formula><alternatives><mml:math id="inf282"><mml:mi>a</mml:mi></mml:math><tex-math id="inft282">\begin{document}$a$\end{document}</tex-math></alternatives></inline-formula> of a time interval was estimated by performing a scalar linear regression without an intercept given the input data <inline-formula><alternatives><mml:math id="inf283"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft283">\begin{document}$(z_{rev}^{(n)}(0),\dots, z_{rev}^{(n)}(L-1))$\end{document}</tex-math></alternatives></inline-formula> and the output data <inline-formula><alternatives><mml:math id="inf284"><mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft284">\begin{document}$(z_{rev}^{(n)}(1),\dots, z_{rev}^{(n)}(L))$\end{document}</tex-math></alternatives></inline-formula>.</p></sec><sec id="s4-8"><title>Generating the PFC reversal probability trajectories from initial states</title><p>We investigated if the PFC reversal probability trajectories <inline-formula><alternatives><mml:math id="inf285"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft285">\begin{document}$x_{rev}^{k}(t)$\end{document}</tex-math></alternatives></inline-formula> of trial <inline-formula><alternatives><mml:math id="inf286"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft286">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> defined on the trial duration, <inline-formula><alternatives><mml:math id="inf287"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft287">\begin{document}$t \in [T_{0}, T_{f}]$\end{document}</tex-math></alternatives></inline-formula><italic>,</italic> can be generated from their initial states (see <xref ref-type="fig" rid="fig4">Figure 4D</xref>). To test this idea, we trained support vector regression (SVR) model on the spiking activity of PFC neurons. The training data consisted of reversal probability trajectories <inline-formula><alternatives><mml:math id="inf288"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft288">\begin{document}$x^{k}_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf289"><mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft289">\begin{document}$t \in [T_{0}, T_{f}]$\end{document}</tex-math></alternatives></inline-formula> of 20 trials around the behavioral reversal trial in each block. About 50% of the blocks in an experiment session (specifically, 10 blocks) was randomly selected for training, and the remaining 50% of the blocks in the same session was used for testing. This procedure was repeated 10 times, training a different SVR model each time, to test models trained on different sets of blocks. For each experiment session, a different SVR model was trained.</p><p>To train an SVR model, reversal probability trajectory at each trial <inline-formula><alternatives><mml:math id="inf290"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft290">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> was divided into initial state and remaining trajectory:<disp-formula id="equ42"><label>(33)</label><alternatives><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="t42">\begin{document}$$\displaystyle {\bf x}^{k}_{init}= x_{rev}^{k}(T_{0}:T_{i}) \in \mathbb{R}^{T_i - T_0 + 1}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ43"><label>(34)</label><alternatives><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="t43">\begin{document}$$\displaystyle {\bf x}^{k}_{traj}= x_{rev}^{k}(T_{i}+1:T_{f}) \in \mathbb{R}^{T_f - T_i}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf291"><mml:mstyle><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>500</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft291">\begin{document}$T_{0}=-500\, \rm ms$\end{document}</tex-math></alternatives></inline-formula> (start of fixation) and <inline-formula><alternatives><mml:math id="inf292"><mml:mstyle><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>300</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft292">\begin{document}$T_{i}=-300\, \rm ms$\end{document}</tex-math></alternatives></inline-formula> (end of initial state). Then, an SVR model <inline-formula><alternatives><mml:math id="inf293"><mml:mstyle><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft293">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula> was constructed that takes the initial state <inline-formula><alternatives><mml:math id="inf294"><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:math><tex-math id="inft294">\begin{document}${\bf x}^{k}_{init}$\end{document}</tex-math></alternatives></inline-formula> and a time point <inline-formula><alternatives><mml:math id="inf295"><mml:mi>s</mml:mi></mml:math><tex-math id="inft295">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula> as its inputs and produces an approximation of <inline-formula><alternatives><mml:math id="inf296"><mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft296">\begin{document}${\bf x}^{k}_{traj}(s)$\end{document}</tex-math></alternatives></inline-formula>:<disp-formula id="equ44"><label>(35)</label><alternatives><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mtext>for</mml:mtext><mml:mspace width="1em"/><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="t44">\begin{document}$$\displaystyle {\bf x}^{k}_{traj}(s) \equiv x^{k}_{rev}(T_{i}+s) \approx f({\bf x}^{k}_{init}, s) \quad \text{for}\quad s \in 1,2,\dots, T_{f}-T_{i}.$$\end{document}</tex-math></alternatives></disp-formula></p><p>As described above, we combined 20 trials around the reversal trial (<inline-formula><alternatives><mml:math id="inf297"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft297">\begin{document}$k=-10,\dots,10$\end{document}</tex-math></alternatives></inline-formula>) from 10 randomly selected blocks in a session (about 50% of blocks in a session) to train the SVR model. The radial basis function was used as a kernel, and the optimal hyperparameters of the SVR model were selected through cross-validation. We used the Python’s sklearn package to implement SVR.</p><sec id="s4-8-1"><title>Null model</title><p>To demonstrate the significance of initial states in generating the reversal probability trajectories, we trained a null SVR model using randomly shuffled initial states. To generate the training and testing data for this null model, we shuffled the initial states <inline-formula><alternatives><mml:math id="inf298"><mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft298">\begin{document}${\bf x}^{k}_{init}$\end{document}</tex-math></alternatives></inline-formula> of 20 trials around the reversal trial in a block, while keeping the remaining trajectories in each trial unchanged. In other words, the null SVR model was trained to generate the reversal probability trajectory of a trial using the initial state from a randomly chosen trial in the same block as the input. As described above, 50% of the blocks in a session was randomly selected for training and the remaining blocks were used for testing. This training and testing procedure was repeated 10 times.</p></sec></sec><sec id="s4-9"><title>Perturbation experiments</title><sec id="s4-9-1"><title>Perturbing RNN</title><p>To perturb the activity of an RNN, a perturbation stimulus was applied to the network together with the cue. The perturbation duration was 50 ms, identical to the cue duration (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The perturbation stimulus was one of <inline-formula><alternatives><mml:math id="inf299"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft299">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf300"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft300">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf301"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft301">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf302"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft302">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula> is the reversal probability population vector derived from targeted dimensionality reduction (<xref ref-type="disp-formula" rid="equ33">Equation 26</xref>), <inline-formula><alternatives><mml:math id="inf303"><mml:mstyle><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft303">\begin{document}$v_{-}$\end{document}</tex-math></alternatives></inline-formula> has the opposite sign of <inline-formula><alternatives><mml:math id="inf304"><mml:msub><mml:mi>v</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math><tex-math id="inft304">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf305"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft305">\begin{document}$v_{rnd}$\end{document}</tex-math></alternatives></inline-formula> is a vector with random elements sampled from a Gaussian distribution with mean 0 and standard deviation identical to that of <inline-formula><alternatives><mml:math id="inf306"><mml:msub><mml:mi>v</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub></mml:math><tex-math id="inft306">\begin{document}$v_{+}$\end{document}</tex-math></alternatives></inline-formula>. The perturbation stimulus was added to the network dynamic equation (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) as one of the external inputs at either trial number -2, -1, or 0. The strength of perturbation was varied by modulating a multiplicative factor on <inline-formula><alternatives><mml:math id="inf307"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo lspace="0em" rspace="0em">+</mml:mo></mml:msub><mml:mo separator="true">,</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mo lspace="0em" rspace="0em">−</mml:mo></mml:msub><mml:mo separator="true">,</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><tex-math id="inft307">\begin{document}$v_{+}, v_{-}, v_{rnd}$\end{document}</tex-math></alternatives></inline-formula> from 0.5 to 4.0.</p></sec><sec id="s4-9-2"><title>Residual PFC activity</title><p>The residual PFC activity of reversal probability <inline-formula><alternatives><mml:math id="inf308"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft308">\begin{document}$x^{k}_{rev}(t)$\end{document}</tex-math></alternatives></inline-formula> and choice <inline-formula><alternatives><mml:math id="inf309"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft309">\begin{document}$x^{k}_{choice}(t)$\end{document}</tex-math></alternatives></inline-formula> was defined as the deviation of their individual trial activity from their block-averaged activity at the same trial: <inline-formula><alternatives><mml:math id="inf310"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft310">\begin{document}$Res^{k}_{rev}(t) = x^{k}_{rev}(t) - \langle x^{k}_{rev}(t) \rangle_{b_k}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf311"><mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft311">\begin{document}$Res^{k}_{choice}(t) = x^{k}_{choice}(t) - \langle x^{k}_{choice}(t) \rangle_{b_k}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf312"><mml:mstyle><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft312">\begin{document}$b_{k}$\end{document}</tex-math></alternatives></inline-formula> denotes trial <inline-formula><alternatives><mml:math id="inf313"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft313">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> in all the blocks across sessions. We analyzed the mean residual activity over the time interval [0 ms, 500 ms], i.e., <inline-formula><alternatives><mml:math id="inf314"><mml:mstyle><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft314">\begin{document}$\overline{Res}^{k}_{rev}= \langle Res^{k}_{rev}(t) \rangle_{t\in[0,500]}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf315"><mml:mstyle><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft315">\begin{document}$\overline{Res}^{k}_{choice}= \langle Res^{k}_{choice}(t) \rangle_{t\in[0,500]}$\end{document}</tex-math></alternatives></inline-formula>. Then, at each trial <inline-formula><alternatives><mml:math id="inf316"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft316">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, we fitted a linear model to characterize the relationship between <inline-formula><alternatives><mml:math id="inf317"><mml:mstyle><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft317">\begin{document}$\overline{Res}^{k}_{rev}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf318"><mml:mstyle><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft318">\begin{document}$\overline{Res}^{k}_{choice}$\end{document}</tex-math></alternatives></inline-formula> by analyzing their values across blocks. <xref ref-type="fig" rid="fig7">Figure 7F</xref> shows the slopes of the linear models at each trial.</p></sec></sec><sec id="s4-10"><title>Decoding monkey’s behavioral reversal trial</title><p>The PFC activity <inline-formula><alternatives><mml:math id="inf319"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft319">\begin{document}$x^{rev}_{t}(k)$\end{document}</tex-math></alternatives></inline-formula> encoding reversal probability was used to decode the behavioral reversal trial at which monkey reversed its preferred choice (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Our analysis is similar to the Linear Discriminant Analysis (LDA) performed in a previous study (<xref ref-type="bibr" rid="bib3">Bartolo and Averbeck, 2020</xref>) at a fixed time point. Here, we applied LDA to time points across a trial.</p><p>For training, 90% of the blocks were randomly selected to train the decoder and remaining 10% of the blocks were used for testing. This was repeated 20 times. Input data to LDA was <inline-formula><alternatives><mml:math id="inf320"><mml:mstyle><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft320">\begin{document}$x^{rev}_{t}(k)$\end{document}</tex-math></alternatives></inline-formula> of <inline-formula><alternatives><mml:math id="inf321"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft321">\begin{document}$\Delta k$\end{document}</tex-math></alternatives></inline-formula> trials around the reverse trial, i.e., <inline-formula><alternatives><mml:math id="inf322"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft322">\begin{document}$k \in [k_{rev}- \Delta k, \dots, k_{rev}+\Delta k]$\end{document}</tex-math></alternatives></inline-formula> with <inline-formula><alternatives><mml:math id="inf323"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft323">\begin{document}$\Delta k = 10$\end{document}</tex-math></alternatives></inline-formula>. At each trial <inline-formula><alternatives><mml:math id="inf324"><mml:mstyle><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft324">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, we took the activity vector <inline-formula><alternatives><mml:math id="inf325"><mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft325">\begin{document}${\bf x}^{rev}_{t_0}(k) = (x^{rev}_{t_0-\Delta t}(k), \dots, x^{rev}_{t_0+\Delta t}(k))$\end{document}</tex-math></alternatives></inline-formula> around time <inline-formula><alternatives><mml:math id="inf326"><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><tex-math id="inft326">\begin{document}$t_{0}$\end{document}</tex-math></alternatives></inline-formula> with <inline-formula><alternatives><mml:math id="inf327"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>160</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft327">\begin{document}$\Delta t = 160\, \rm ms$\end{document}</tex-math></alternatives></inline-formula>. The target output of LDA was a one-hot vector <inline-formula><alternatives><mml:math id="inf328"><mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft328">\begin{document}${\bf y}^{target}$\end{document}</tex-math></alternatives></inline-formula>, whose element was 1 at the reversal trial <inline-formula><alternatives><mml:math id="inf329"><mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft329">\begin{document}$k_{rev}$\end{document}</tex-math></alternatives></inline-formula> and 0 at other trials. The following input-output shows dataset of a block used for training:<disp-formula id="equ45"><label>(36)</label><alternatives><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Input:</mml:mtext><mml:mspace width="1em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="t45">\begin{document}$$\displaystyle \text{Input:}\quad \left[{\bf x}^{rev}_{t_0}(k_{rev}-\Delta k), \dots,{\bf x}^{rev}_{t_0}(k_{rev}), \dots,{\bf x}^{rev}_{t_0}(k_{rev}+\Delta k) \right]$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ46"><label>(37)</label><alternatives><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>Output:</mml:mtext><mml:mspace width="1em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="t46">\begin{document}$$\displaystyle \text{Output:}\quad{\bf y}^{target}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf330"><mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft330">\begin{document}${\bf x}^{rev}_{t_0}(k) \in \mathbb{R}^{T_{dec}}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf331"><mml:mstyle><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft331">\begin{document}$T_{dec}= 2\Delta t / \Delta h + 1$\end{document}</tex-math></alternatives></inline-formula> denotes the number of time points around <inline-formula><alternatives><mml:math id="inf332"><mml:mstyle><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft332">\begin{document}$t_{0}$\end{document}</tex-math></alternatives></inline-formula> with time increment <inline-formula><alternatives><mml:math id="inf333"><mml:mstyle><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft333">\begin{document}$\Delta h = 20\, \rm ms$\end{document}</tex-math></alternatives></inline-formula>, and the one-hot vector <inline-formula><alternatives><mml:math id="inf334"><mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft334">\begin{document}${\bf y}^{target}= [0,\dots,1,\dots,0] \in \mathbb{R}^{K_{dec}}$\end{document}</tex-math></alternatives></inline-formula> where <inline-formula><alternatives><mml:math id="inf335"><mml:mstyle><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft335">\begin{document}$K_{dec}= 2\Delta k + 1$\end{document}</tex-math></alternatives></inline-formula> denotes the number of trials around the reversal trial. As mentioned above, this analysis was repeated for time point <inline-formula><alternatives><mml:math id="inf336"><mml:mstyle><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft336">\begin{document}$t_{0}$\end{document}</tex-math></alternatives></inline-formula> across a trial.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Formal analysis, Supervision, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103660-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The Python code for training RNNs is available in the following Github repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/chrismkkim/LearnToReverse">https://github.com/chrismkkim/LearnToReverse</ext-link> (copy archived at <xref ref-type="bibr" rid="bib27">Kim, 2025</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the Intramural Research Program of the National Institutes of Health: the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and the National Institute of Mental Health (NIMH). This work utilized the computational resources of the NIH HPC Biowulf cluster (<ext-link ext-link-type="uri" xlink:href="https://hpc.nih.gov">https://hpc.nih.gov</ext-link>).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Amygdala and ventral striatum population codes implement multiple learning rates for reinforcement learning</article-title><conf-name>2017 IEEE Symposium Series on Computational Intelligence (SSCI)</conf-name><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/SSCI.2017.8285354</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>B</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reinforcement-learning in fronto-striatal circuits</article-title><source>Neuropsychopharmacology</source><volume>47</volume><fpage>147</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1038/s41386-021-01108-0</pub-id><pub-id pub-id-type="pmid">34354249</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolo</surname><given-names>R</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prefrontal cortex predicts state switches during reversal learning</article-title><source>Neuron</source><volume>106</volume><fpage>1044</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.03.024</pub-id><pub-id pub-id-type="pmid">32315603</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bollimunta</surname><given-names>A</given-names></name><name><surname>Totten</surname><given-names>D</given-names></name><name><surname>Ditterich</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural dynamics of choice: single-trial analysis of decision-related activity in parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12684</fpage><lpage>12701</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5752-11.2012</pub-id><pub-id pub-id-type="pmid">22972993</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural underpinnings of the evidence accumulator</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>149</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.01.003</pub-id><pub-id pub-id-type="pmid">26878969</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title><source>Journal of Computational Neuroscience</source><volume>8</volume><fpage>183</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1023/a:1008925309027</pub-id><pub-id pub-id-type="pmid">10809012</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Hansel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>How noise affects the synchronization properties of recurrent networks of inhibitory neurons</article-title><source>Neural Computation</source><volume>18</volume><fpage>1066</fpage><lpage>1110</lpage><pub-id pub-id-type="doi">10.1162/089976606776241048</pub-id><pub-id pub-id-type="pmid">16595058</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butter</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Perseveration in extinction and in discrimination reversal tasks following selective frontal ablations in <italic>Macaca mulatta</italic></article-title><source>Physiology &amp; Behavior</source><volume>4</volume><fpage>163</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(69)90075-4</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>RTQ</given-names></name><name><surname>Rubanova</surname><given-names>Y</given-names></name><name><surname>Bettencourt</surname><given-names>J</given-names></name><name><surname>Duvenaud</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural ordinary differential equations</article-title><conf-name>NIPS’18: Proceedings of the 32nd International Conference on Neural Information Processing Systems</conf-name><fpage>6572</fpage><lpage>6583</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Tran</surname><given-names>VL</given-names></name><name><surname>Turchi</surname><given-names>J</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reversal learning and dopamine: a bayesian perspective</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>2407</fpage><lpage>2416</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1989-14.2015</pub-id><pub-id pub-id-type="pmid">25673835</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Findling</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Brain-wide representations of prior information in mouse decision-making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.04.547684</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Economo</surname><given-names>MN</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attractor dynamics gate cortical information flow during decision-making</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>843</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00840-6</pub-id><pub-id pub-id-type="pmid">33875892</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>RJ</given-names></name><name><surname>Hermansen</surname><given-names>E</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Baas</surname><given-names>NA</given-names></name><name><surname>Dunn</surname><given-names>BA</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Toroidal topology of population activity in grid cells</article-title><source>Nature</source><volume>602</volume><fpage>123</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04268-7</pub-id><pub-id pub-id-type="pmid">35022611</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Genkin</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Engel</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The dynamics and geometry of choice in premotor cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.22.550183</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groman</surname><given-names>SM</given-names></name><name><surname>Keistler</surname><given-names>C</given-names></name><name><surname>Keip</surname><given-names>AJ</given-names></name><name><surname>Hammarlund</surname><given-names>E</given-names></name><name><surname>DiLeone</surname><given-names>RJ</given-names></name><name><surname>Pittenger</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Orbitofrontal circuits control multiple reinforcement-learning processes</article-title><source>Neuron</source><volume>103</volume><fpage>734</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.042</pub-id><pub-id pub-id-type="pmid">31253468</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groman</surname><given-names>SM</given-names></name><name><surname>Thompson</surname><given-names>SL</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reinforcement learning detuned in addiction: integrative and translational approaches</article-title><source>Trends in Neurosciences</source><volume>45</volume><fpage>96</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2021.11.007</pub-id><pub-id pub-id-type="pmid">34920884</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Harkin</surname><given-names>EF</given-names></name><name><surname>Grossman</surname><given-names>CD</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Béïque</surname><given-names>JC</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023a</year><article-title>Serotonin predictively encodes value</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.09.19.558526</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harkin</surname><given-names>EF</given-names></name><name><surname>Lynn</surname><given-names>MB</given-names></name><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Boucher</surname><given-names>JF</given-names></name><name><surname>Caya-Bissonnette</surname><given-names>L</given-names></name><name><surname>Cyr</surname><given-names>D</given-names></name><name><surname>Stewart</surname><given-names>C</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Béïque</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2023">2023b</year><article-title>Temporal derivative computation in the dorsal raphe network revealed by an experimentally driven augmented integrate-and-fire modeling framework</article-title><source>eLife</source><volume>12</volume><elocation-id>e72951</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.72951</pub-id><pub-id pub-id-type="pmid">36655738</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulse</surname><given-names>BK</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mechanisms underlying the neural computation of head direction</article-title><source>Annual Review of Neuroscience</source><volume>43</volume><fpage>31</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031516</pub-id><pub-id pub-id-type="pmid">31874068</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hyun</surname><given-names>JH</given-names></name><name><surname>Hannan</surname><given-names>P</given-names></name><name><surname>Iwamoto</surname><given-names>H</given-names></name><name><surname>Blakely</surname><given-names>RD</given-names></name><name><surname>Kwon</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Serotonin in the orbitofrontal cortex enhances cognitive flexibility</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.09.531880</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inagaki</surname><given-names>HK</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discrete attractor dynamics underlies persistent activity in the frontal cortex</article-title><source>Nature</source><volume>566</volume><fpage>212</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0919-7</pub-id><pub-id pub-id-type="pmid">30728503</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>AI</given-names></name><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Chudasama</surname><given-names>Y</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The role of frontal cortical and medial-temporal lobe brain areas in learning a bayesian prior belief on reversals</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11751</fpage><lpage>11760</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1594-15.2015</pub-id><pub-id pub-id-type="pmid">26290251</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Paik</surname><given-names>SB</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distinct roles of parvalbumin- and somatostatin-expressing neurons in flexible representation of task variables in the prefrontal cortex</article-title><source>Progress in Neurobiology</source><volume>187</volume><elocation-id>101773</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101773</pub-id><pub-id pub-id-type="pmid">32070716</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kadmon</surname><given-names>J</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transition to chaos in random neuronal networks</article-title><source>Physical Review X</source><volume>5</volume><elocation-id>41030</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.5.041030</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>TD</given-names></name><name><surname>Luo</surname><given-names>TZ</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Inferring latent dynamics underlying neural population activity via neural differential equations</article-title><conf-name>International conference on machine learning</conf-name><fpage>5551</fpage><lpage>5561</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>TD</given-names></name><name><surname>Luo</surname><given-names>TZ</given-names></name><name><surname>Can</surname><given-names>T</given-names></name><name><surname>Krishnamurthy</surname><given-names>K</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Flow-field inference from neural data using deep recurrent networks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.11.14.567136</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>LearnToReverse</data-title><version designator="swh:1:rev:6cd634bf1762f71df0838504f3b5c74b3833a597">swh:1:rev:6cd634bf1762f71df0838504f3b5c74b3833a597</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d9ec4861151f47b3119a725513ba64929c2e0f6f;origin=https://github.com/chrismkkim/LearnToReverse;visit=swh:1:snp:a5706276a6bde4a13c401815f252cb0c78951af8;anchor=swh:1:rev:6cd634bf1762f71df0838504f3b5c74b3833a597">https://archive.softwareheritage.org/swh:1:dir:d9ec4861151f47b3119a725513ba64929c2e0f6f;origin=https://github.com/chrismkkim/LearnToReverse;visit=swh:1:snp:a5706276a6bde4a13c401815f252cb0c78951af8;anchor=swh:1:rev:6cd634bf1762f71df0838504f3b5c74b3833a597</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latimer</surname><given-names>KW</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Meister</surname><given-names>MLR</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal modeling: Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title><source>Science</source><volume>349</volume><fpage>184</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1126/science.aaa4056</pub-id><pub-id pub-id-type="pmid">26160947</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>TW</given-names></name><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A motor cortex circuit for motor planning and movement</article-title><source>Nature</source><volume>519</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature14178</pub-id><pub-id pub-id-type="pmid">25731172</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title><source>Nature</source><volume>532</volume><fpage>459</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1038/nature17643</pub-id><pub-id pub-id-type="pmid">27074502</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>TZ</given-names></name><name><surname>Kim</surname><given-names>TD</given-names></name><name><surname>Gupta</surname><given-names>D</given-names></name><name><surname>Bondy</surname><given-names>AG</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Elliot</surname><given-names>VA</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Transitions in dynamical regime and neural mode underlie perceptual decision-making</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.10.15.562427</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Lottem</surname><given-names>E</given-names></name><name><surname>Dugué</surname><given-names>GP</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Activity patterns of serotonin neurons underlying cognitive flexibility</article-title><source>eLife</source><volume>6</volume><elocation-id>e20552</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.20552</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazurek</surname><given-names>ME</given-names></name><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Ditterich</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A role for neural integrators in perceptual decision making</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1257</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg097</pub-id><pub-id pub-id-type="pmid">14576217</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molano-Mazón</surname><given-names>M</given-names></name><name><surname>Shao</surname><given-names>Y</given-names></name><name><surname>Duque</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Recurrent networks endowed with structural priors explain suboptimal animal behavior</article-title><source>Current Biology</source><volume>33</volume><fpage>622</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.12.044</pub-id><pub-id pub-id-type="pmid">36657448</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>CE</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Ortega</surname><given-names>HK</given-names></name><name><surname>Kwan</surname><given-names>AC</given-names></name><name><surname>Atilgan</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Change point estimation by the mouse medial frontal cortex during probabilistic reward learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.26.493245</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>A</given-names></name><name><surname>Karigo</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Kennedy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>An approximate line attractor in the hypothalamus encodes an aggressive state</article-title><source>Cell</source><volume>186</volume><fpage>178</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2022.11.027</pub-id><pub-id pub-id-type="pmid">36608653</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>J</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The effect of stimulus strength on the speed and accuracy of a perceptual decision</article-title><source>Journal of Vision</source><volume>5</volume><fpage>376</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1167/5.5.1</pub-id><pub-id pub-id-type="pmid">16097871</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Cherian</surname><given-names>A</given-names></name><name><surname>Segraves</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A comparison of macaque behavior and superior colliculus neuronal activity to predictions from models of two-choice decisions</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>1392</fpage><lpage>1407</lpage><pub-id pub-id-type="doi">10.1152/jn.01049.2002</pub-id><pub-id pub-id-type="pmid">12761282</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name><name><surname>Bartho</surname><given-names>P</given-names></name><name><surname>Hollender</surname><given-names>L</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name><name><surname>Reyes</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The asynchronous state in cortical circuits</article-title><source>Science</source><volume>327</volume><fpage>587</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1126/science.1179850</pub-id><pub-id pub-id-type="pmid">20110507</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Rescorla</surname><given-names>RA</given-names></name></person-group><source>Classsical Conditioning II: Current Research and Theory</source><publisher-name>Appleton-Century-Crofts</publisher-name><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schaeffer</surname><given-names>R</given-names></name><name><surname>Khona</surname><given-names>M</given-names></name><name><surname>Meshulam</surname><given-names>L</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice</article-title><conf-name>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</conf-name><fpage>4584</fpage><lpage>4596</lpage><pub-id pub-id-type="doi">10.1101/2020.06.09.142745</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Roesch</surname><given-names>MR</given-names></name><name><surname>Stalnaker</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Orbitofrontal cortex, decision-making and drug addiction</article-title><source>Trends in Neurosciences</source><volume>29</volume><fpage>116</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2005.12.006</pub-id><pub-id pub-id-type="pmid">16406092</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>How the brain keeps the eyes still</article-title><source>PNAS</source><volume>93</volume><fpage>13339</fpage><lpage>13344</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.23.13339</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Crisanti</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Chaos in random neural networks</article-title><source>Physical Review Letters</source><volume>61</volume><fpage>259</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.61.259</pub-id><pub-id pub-id-type="pmid">10039285</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reward-based training of recurrent neural networks for cognitive and value-based tasks</article-title><source>eLife</source><volume>6</volume><elocation-id>e21492</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21492</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorscher</surname><given-names>B</given-names></name><name><surname>Mel</surname><given-names>GC</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A unified theory for the computational and mechanistic origins of grid cells</article-title><source>Neuron</source><volume>111</volume><fpage>121</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.10.003</pub-id><pub-id pub-id-type="pmid">36306779</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Su</surname><given-names>Z</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Two types of locus coeruleus norepinephrine neurons drive reinforcement learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.12.08.519670</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Lfads-latent factor analysis via dynamical systems</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1608.06315</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Learning to predict by the methods of temporal differences</article-title><source>Machine Learning</source><volume>3</volume><fpage>9</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1023/A:1022633531479</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sylwestrak</surname><given-names>EL</given-names></name><name><surname>Jo</surname><given-names>Y</given-names></name><name><surname>Vesuna</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Holcomb</surname><given-names>B</given-names></name><name><surname>Tien</surname><given-names>RH</given-names></name><name><surname>Kim</surname><given-names>DK</given-names></name><name><surname>Fenno</surname><given-names>L</given-names></name><name><surname>Ramakrishnan</surname><given-names>C</given-names></name><name><surname>Allen</surname><given-names>WE</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cell-type-specific population dynamics of diverse reward computations</article-title><source>Cell</source><volume>185</volume><fpage>3568</fpage><lpage>3587</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2022.08.019</pub-id><pub-id pub-id-type="pmid">36113428</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Tirumala</surname><given-names>D</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>860</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0147-8</pub-id><pub-id pub-id-type="pmid">29760527</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wert-Carvajal</surname><given-names>C</given-names></name><name><surname>Reneaux</surname><given-names>M</given-names></name><name><surname>Tchumatchenko</surname><given-names>T</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Dopamine and serotonin interplay for valence-based spatial learning</article-title><source>Cell Reports</source><volume>39</volume><elocation-id>110645</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.110645</pub-id><pub-id pub-id-type="pmid">35417691</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian online learning of the hazard rate in change-point problems</article-title><source>Neural Computation</source><volume>22</volume><fpage>2452</fpage><lpage>2476</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00007</pub-id><pub-id pub-id-type="pmid">20569174</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Takahashi</surname><given-names>YK</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal cortex as a cognitive map of task space</article-title><source>Neuron</source><volume>81</volume><fpage>267</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.005</pub-id><pub-id pub-id-type="pmid">24462094</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>K</given-names></name><name><surname>Nykamp</surname><given-names>DQ</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="doi">10.1038/nn.3645</pub-id><pub-id pub-id-type="pmid">24487232</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>KF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>JH</given-names></name><name><surname>Yoon</surname><given-names>YJ</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Choe</surname><given-names>SY</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Selective engagement of prefrontal VIP neurons in reversal learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.04.03.587891</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoltowski</surname><given-names>DM</given-names></name><name><surname>Latimer</surname><given-names>KW</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discrete stepping and nonlinear ramping dynamics underlie spiking responses of LIP neurons during decision-making</article-title><source>Neuron</source><volume>102</volume><fpage>1249</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.031</pub-id><pub-id pub-id-type="pmid">31130330</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103660.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ponte Costa</surname><given-names>Rui</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Incomplete</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>The findings of this study are <bold>valuable</bold>, offering insights into the neural representation of reversal probability in decision-making tasks, with potential implications for understanding flexible behavior in changing environments. The study contains interesting comparisons between neural data and models, including evidence for partial consistency with line attractor models in this probabilistic reversal learning task. However, it remains <bold>incomplete</bold> due to issues related to how the RNN training and the analysis of its dynamics, which renders the evidence as not complete.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103660.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors aimed to investigate how the probability of a reversal in a decision-making task is computed in cortical neurons. They analyzed neural activity in the prefrontal cortex of monkeys and units in recurrent neural networks (RNNs) trained on a similar task. Their goal was to understand how the dynamical systems that implement computation perform a probabilistic reversal learning task in RNNs and nonhuman primates.</p><p>Major strengths and weaknesses:</p><p>Strengths:</p><p>(1) Integrative Approach: The study exemplifies a modern approach by combining empirical data from monkey experiments with computational modeling using RNNs. This integration allows for a more comprehensive understanding of the dynamical systems that implement computation in both biological and artificial neural networks.</p><p>(2) The focus on using perturbations to identify causal relationships in dynamical systems is a good goal. This approach aims to go beyond correlational observations.</p><p>(3) The revised manuscript provides a more nuanced interpretation of the dynamics, reconciling the observations with aspects of line attractor models.</p><p>Weaknesses:</p><p>(1) The use of targeted dimensionality reduction (TDR) to identify the axis determining reversal probability may not necessarily isolate the dimension along which the RNN computes reversal probability. This should be computed from the RNN update itself rather than through a readout of network variance. Depending on how this is formulated, it could be something like the Jacobian of the state update with respect to inputs at input onset and with respect to the state during relaxation dynamics. This is worth thinking through further. It's important to try to take advantage of access afforded by using RNNs rather than solely relying on analyses available to us in neural data.</p><p>Appraisal of aims and conclusions:</p><p>The authors have substantially revised their interpretation of the results to reconcile their findings with line attractor models. They now acknowledge that their observation of reward integration explaining reversal probability activity (x_rev) is compatible with line attractor models, which addresses one of my main concerns.</p><p>Their expanded analysis now differentiates between two activity modes: (1) substantial non-stationary dynamics during a trial (incompatible with line attractors) and (2) stationary and stable dynamics at trial start (compatible with point attractors and line attractor models). This dual characterization provides a more complete picture of the dynamical system and highlights the composability of dynamical features.</p><p>Likely impact and utility:</p><p>This work makes a stronger contribution to our understanding of how probabilistic information is represented in neural circuits with intervening behaviors. The augmented model that combines elements of attractor dynamics with non-stationary trajectories offers a more comprehensive framework for understanding neural computations in decision-making tasks.</p><p>The data and methods could be useful to the community. While the authors have improved their analysis of network dynamics, additional reverse engineering that takes full advantage of access to the RNN's update equations could further strengthen the work.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103660.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work the authors trained RNN to perform a reversal task also performed by animals while PFC activity is recorded. The authors devised a new method to train RNN on this type of reversal task, which in principle ensures that the behavior of the RNN matches the behavior of the animal. They then performed some analysis of neural activity, both RNN and PFC recording, focusing on the neural representation of the reversal probability and its evolution across trials. Given the analysis presented, it has been difficult for me to asses at which point RNN can reasonably be compared to PFC recordings.</p><p>Strengths:</p><p>Focusing on a reversal task, the authors address a challenge in RNN training, as they do not use a standard supervised learning procedure where the desired output is available for each trial. They propose a new way of doing that.</p><p>They attempt to confront RNN and neural recordings in behaving animals.</p><p>Weaknesses:</p><p>It would be nice to better articulate the analysis results of the two training set-ups (with and without 0 response during fixation). The dynamical system analysis is confusing, the notions of stationary and non-stationary dynamics and its relationship with attractors are puzzling. Is there a line attractor in one case (with inputs orthogonal to the integration direction being called back to the attractor, and reward input aligned with the stable direction)? In the other case, do we have a cylindrical attracting manifold on which activity circles around and is pushed along the axis of the cylinder by reward inputs? Which case is closest to the PFC recordings?</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103660.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Kim et al. present a study of the neural dynamics underlying reversal learning in monkey PFC and neural networks. Their main finding is that neural activity during fixation resembles a line attractor storing the current belief of the reversal state of the task. This is followed by richer dynamics unfolding throughout the remainder of the trial, which eventually converge to a new point on the line attractor by the start of the next trial. The idea of studying neural dynamics throughout the task (including intervening behaviour) is interesting, and the data provides some insights into the neural dynamics driving reversal learning. The modelling seems to support the analyses, but both the modelling and analyses also leave several open questions.</p><p>Strengths:</p><p>The paper addresses an interesting topic of the neural dynamics underlying reversal learning in PFC, using a combination of biological and simulated data. Reversal learning has been studied extensively in neuroscience, but this paper takes a step further by analysing neural dynamics throughout the trials instead of focusing on just the evidence integration epoch.</p><p>The authors show some close parallels between the experimental data and RNN simulations, both in terms of behaviour and neural dynamics. The analyses of how rewarded and unrewarded trials differentially affect dynamics throughout the trials in RNNs and PFC were particularly interesting. This work has the potential to provide new insights into the neural underpinnings of reversal learning.</p><p>Weaknesses:</p><p>Data analyses:</p><p>While the analyses seem mostly sound, one shortcoming is that they are all aligned to the inferred reversal trial rather than the true experimental reversal trial. For example, the analyses showing that 'x_rev' decays strongly after the reversal trial, irrespective of the reward outcome, seem like they are true essentially by design. The choice to align to the inferred reversal trial also makes this trial seem 'special' (e.g. in Fig 2 &amp; Fig 6A), but it is unclear whether this is a real feature of the data or an artifact of effectively conditioning on a change in behaviour. It would be useful to investigate whether any of these analyses differ when aligned to the true reversal trial. It is also unsurprising that x_rev increases before the reversal and decreases after the reversal (it is hard to imagine a system where this is not the case), yet all of Fig 6 and several other analyses are devoted to this point.</p><p>Most of the analyses focus on the dynamics specifically in the x_rev subspace, but a major point of the paper is to say that biological (and artificial) networks may also have to do other things at different times in the trial. If that is the case, it would be interesting to also ask what happens in other subspaces of neural activity, which are not specifically related to evidence integration or choice - are there other subspaces that explain substantial variance? Do they relate to any meaningful features of the experiment?</p><p>This is especially important when considering analyses trying to establish the presence (or absence) of attractor dynamics in the circuit. In particular, activity in the x_rev subspace both affects and depends on other subspaces of neural activity, so it is not as meaningful to analyse the dynamics of this subspace in isolation. It would e.g. have been preferable to analyse the early-trial dynamics in the full state space and then possibly projecting onto x_rev, rather than first projecting activity onto x_rev and then fitting a linear autoregressive model.</p><p>Modelling:</p><p>There are a number of surprising and non-standard modelling choices made in this paper. For example, the choice to only use inhibitory neurons is non-conventional and it is not clear whether and how this impacts the results. The inputs are also provided without any learnable input weights, which makes it harder to interpret the input-driven dynamics during the different phases of a trial.</p><p>It is surprising that the RNN is &quot;trained to flip its preferred choice a few trials after the inferred scheduled reversal trial&quot;, with the reversal trial inferred by an ideal Bayesian observer. A more natural approach would be to directly train the RNN to solve the task (by predicting the optimal choice) and then investigating the emergent behaviour &amp; dynamics. If the authors prefer their imitation learning approach, it is also surprising that the network is trained to predict the reversal trial inferred using Bayesian smoothing instead of Bayesian filtering.</p><p>Finally, it was surprising that the network is trained and tested with different block lengths (24 &amp; 36 trials, respectively), and it is not mentioned whether or how this affects behaviour.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103660.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Christopher M</given-names></name><role specific-use="author">Author</role><aff><institution>National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Chow</surname><given-names>Carson C</given-names></name><role specific-use="author">Author</role><aff><institution>National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Averbeck</surname><given-names>Bruno B</given-names></name><role specific-use="author">Author</role><aff><institution>National Institute of Mental Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><p>Main revision made to the manuscript</p><p>The main revision made to the manuscript is to reconcile our findings with the line attractor model. The revision is based on Reviewer 1’s comment on reinterpreting our results as a superposition of an attractor model with fast timescale dynamics. We expanded our analysis regime to the start of a trial and characterized the overall within-trial dynamics to reinterpret our findings.</p><p>We first acknolwedge that our results are not in contradiction with evidence integration on a line attractor. As pointed out by the reviewers, our finding that the integration of reward outcome explains the reversal probability activity x_rev (Figure 3) is compatible with the line attractor model. However, the reward integration equation is an algebraic relation and does not characterize the dynamics of reversal probability activity. So a closer analysis on the neural dynamics is needed to assess the feasibility of line attractor.</p><p>In the revised manuscript, we show that x_rev exhibits two different activity modes (Figure 4). First, x_rev has substantial non-stationary dynamics during a trial, and this non-stationary activity is incompatible with the line attractor model, as claimed in the original manuscript. Second, we present new results showing that x_rev is stationary (i.e., constant in time) and stable (i.e., contracting) at the start of a trial. These two properties of x_rev support that it is a point attractor at the start of a trial and is compatible with the line attractor model.</p><p>We further analyze how the two activity modes are linked (Figure 4, Support vector regression). We show that the non-stationary activity is predictable from the stationary activity if the underlying dynamics can be inferred. In other words, the non-stationary activity during a trial is generated by an underlying dynamics with the initial condition provided by the stationary state at the start of trial.</p><p>These results suggest an extension of the line attractor model where an attractor state at the start of a trial provides an initial condition from which non-stationary activity is generated during a trial by an underlying dynamics associated with task-related behavior (Figure 4, Augmented model).</p><p>The separability of non-stationary trajectories (Figure 5 and 6) is a property of the non-stationary dynamics that allows separable points in the initial stationary state to remain separable during a trial, thus making it possible to represent distinct probabilistic values in non-stationary activity.</p><p>This revised interpretation of our results (1) retains our original claim that the non-stationary dynamics during a trial is incompatible with the line attractor model and (2) introduces attractor state at the start of a trial which is compatible with the line attractor model. Our anlaysis shows that the two activity modes are linked by an underlying dynamics, and the attractor state serves as initial state to launch the non-stationary activity.</p><p><bold>Responses to the Public Reviews:</bold></p><p><bold>Reviewer # 1:</bold></p><p>(1) To provide better explanation of the reversal learning task and network training method, we added detailed description of RNN and monkey task structure (Result Section 1), included a schematic of target outputs (Figure1B), explained the rationale behind using inhibitory network model (Method Section 1) and explained the supervised RNN training scheme (Result Section 1). This information can also be found in the Methods.</p><p>(2) Our understanding is that the augmented model discussed in the previous page is aligned with the model suggested by Reviewer 1: “a curved line attractor, with faster timescale dynamics superimposed on this structure”. It is likely that the “fast” non-stationary activity observed during the trial is driven by task-related behavior, thus is transient. For instance, we do not observe such non-stationary activity in the inter-trial-interval when the task-related behavior is absent. For this reason, the non-stationary trajectories were not considered to be part of the attractor. Instead, they are transient activity generated by the underlying neural dynamics associated with task-related behavior. We believe such characterization of faster timescale dynamics is consistent with Reviewer 1’s view and wanted to clarify that there are two different activity modes.</p><p>(3) We appreciate the reviewers (Reviewer 1 and Reviewer 2) comment that TDR may be limited in isolating the neural subspace of interest. Our study presents what could be learned from TDR but is by no means the only way to interpret the neural data. It would be of future work to apply other methods for isolating task-related neural activities.</p><p>We would appreciate it if the reviewers could share thoughts on what other alternative methods could better isolate the reversal probability activity.</p><p><bold>Reviewer # 2:</bold></p><p>(1) (i) We respectfully disagree with Reviewer 2’s comment that “no action is required to be performed by neurons in the RNN”. In our network setup, the output of RNN learns to choose a sign (+ or -), as Reviewer 2 pointed out, to make a choice. This is how the RNN takes an action. It is unclear to us what Reviewer 2 has intended by “action” and how reaching a target value (not just taking a sign) would make a significant difference in how the network performs the task.</p><p>(ii) From Reviewer 2’s comment that “no intervening behavior is thus performed by neurons”, we noticed that the term “intervening behavior” has caused confusion. It refers to task-related behavior, such as making choices or receiving reward, that the subject must perform across trials before reversing its preferred choice. These are the behaviors that intervene the reversal of preferred choice. To clarify its meaning, in the revised manuscript, we changed the term to “task-related behavior” and put them in context. For example, in the Introduction we state that “However, during a trial, task-related behavior, such as making decisions or receiving feedback, produced …”</p><p>(iii) As pointed out by Reviewer 2, the lack of fixation period in the RNN could make differences in the neural dynamics of RNN and PFC, especially at the start of a trial. We demonstrate this issue in Result Section 4 where we analyze the stationary activity at the start of a trial. We find that fixating the choice output to zero before making a choice promotes stationary activity and makes the RNN activity more similar to the PFC activity.</p><p><bold>Reviewer #3:</bold></p><p>(1) (i) In the previous study (Figure 1 in [Bartolo and Averbeck ‘20]), it was shown that neural activity can predict the behavioral reversal trial. This is the reason we examined the neural activity in the trials centered at the behavioral reversal trial. We explained in Result Section 2 that we followed this line of analysis in our study.</p><p>(ii) We would like to emphasize that the main point of Figures 4 and 5 is to show the separability of neural trajectories: the entire trajectory shifts without overlapping. It is not obvious that high-dimensional neural population activity from two trials should remain separated when their activities are compressed into a one-dimensional subspace. The onedimensional activities can easily collide since their activities are compressed into a lowdimensional space. We revised the manuscript to bring out these points. We added an opening paragraph that discusses separability of trajectories and revised the main text to bring out the findings on separability.</p><p>(iii) We agree with Reviewer 3 that it would be interesting to look at what happens in other subspace of neural activity that are not related to reversal probability and characterize how different neural subspace interact with each. However, the focus of this paper was the reversal probability activity, and we’d consider these questions out of the scope of current paper. We point out that, using the same dataset, neural activity related to other experimental variables were analyzed in other papers [Bartolo and Averbeck ’20; Tang, Bartolo and Averbeck ‘21]</p><p>(2) (i) In the revised manuscript, we added explanation on the rational behind choosing inhibitory network as a simplified model for the balanced state. In brief, strong inhibitory recurrent connections with strong excitatory external input operates in the balanced state, as in the standard excitatory-inhibitory network. We included references that studied this inhibitory network. We also explained the technical reason (GPU memory) for choosing the inhibitory model.</p><p>(ii) We thank the reviewer for pointing out that the original manuscript did not mention how the feedback and cue were initialized. They were random vectors sample from Gaussian distribution. We added this information in the revised manuscript. In our opinion, it is common to use random external inputs for training RNNs, as it is a priori unclear how to choose them. In fact, it is possible to analyze the effects of random feedback on one-dimensional x_rev dynamics by projecting the random feedback vector to the reversal probability vector. This is shown in Figure 4F.</p><p>(iii) We agree that it would be more natural to train the RNN to solve the task without using the Bayesian model. We point out this issue in the Discussion in the revised manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1:</bold></p><p>(1) My understanding of network training was that a Bayesian ideal observer signaled target output based on previous reward outcomes. However, the authors never mention that networks are trained by supervised learning in the main text until the last paragraph of the discussion. There is no mention that there was an offset in the target based on the behavior of the monkeys in the main text. These are really important things to consider in the context of the network solution after training. I couldn't actually find any figure that presents the target output for the network. Did I miss something key here?</p></disp-quote><p>In Result Section 1, we added a paragraph that describes in detail how the RNN is trained. We explained that the network is first simulated and then the choice outputs and reward outcomes are fed into the Bayesian model to infer the scheduled reversal trial. A few trials are added to the inferred reversal trial to obtain the behavioral reversal trial, as found in a previous study [Bartolo and Averbeck ‘20]. Then the network weights are updated by backpropagation-through-time via supervised learning.</p><p>In the original manuscript, the target output for the network was described in Methods Section 2.5, Step 4. To make this information readily accessible, we added a schematic in Figure 1B that shows the scheduled, inferred and behavioral reversal trials. It also shows how the target choice ouputs are defined. They switch abruptly at the behavioral reversal trial.</p><disp-quote content-type="editor-comment"><p>(2) The role of block structure in the task is an important consideration. What are the statistics of block switches? The authors say on average the reversals are every 36 trials, but also say there are random block switches. The reviewer's notes suggest that both the networks and monkeys may be learning about the typical duration of blocks, which could influence their expectations of reversals. This aspect of the task design should be explained more thoroughly and considered in the context of Figure 1E and 5 results.</p></disp-quote><p>We provided more detailed description of the reversal learning task in Result Section 1. We clarified that (1) a task is completed by executing a block of fixed number of trials and (2) reversal of reward schedule occurrs at a random trial around the mid-trial in a block. The differences in the number of trials in a block that the RNNs (36) and the monkeys (80) perform are also explained. We also pointed out the differences in how the reversal trial is randomly sampled.</p><p>However, it is unclear what Reviewer 1 meant by random block switches. Our reversal learning task is completed when a block of fixed number of trials is executed. Reversal of reward schedule occurs only once on a randomly selected trial in the block, and the reversed reward schedule is maintained until the end of a block. It is different from other versions of reveral learning where the reward schedule switches multiple times across trials. We clarified this point in Result Section 1.</p><disp-quote content-type="editor-comment"><p>(3) The relationship between the supervised learning approach used in the RNNs and reinforcement learning was confused in the discussion. &quot;Although RNNs in our study were trained via supervised learning, animals learn a reversal-learning task from reward feedback, making it into a reinforcement learning (RL) problem.&quot; This is fundamentally not true. In the case of this work, the outcome of the previous trial updates the target output, rather than the trial and error type learning as is typical in reinforcement learning. Networks are not learning by reinforcement learning and this statement is confusing.</p></disp-quote><p>We agree with Reviewer 1’s comment that the statement in the original manuscript is confusing. Our intention was to point out that our study used supervised learning, and this is different from animals learn by reinforcement learning in rea life. We revised the sentence in Discussion as follows:</p><p>“The RNNs in our study were trained via supervised learning. However, in real life, animals learn a reversal learning task via reinforcement learning (RL), i.e., learn the task from reward outcomes.”</p><disp-quote content-type="editor-comment"><p>(4) The distinction between line attractors and the dynamic trajectories described by the authors deserves further investigation. A significant concern arises from the authors' use of targeted dimensionality reduction (TDR), a form of regression, to identify the axis determining reversal probability. While this approach can reveal interesting patterns in the data, it may not necessarily isolate the dimension along which the RNN computes reversal probability. This limitation could lead to misinterpretation of the underlying neural dynamics.</p><p>a) This manuscript cites work described in &quot;Prefrontal cortex as a meta-reinforcement learning system,&quot; which examined a similar task. In that study, the authors identified a v-shaped curve in the principal component space of network states, representing the probability of choosing left or right.</p><p>Importantly, this curve is topologically equivalent to a line and likely represents a line attractor. However, regressing against reversal probability in such a case would show that a single principal component (PC2) directly correlates with reversal probability.</p><p>b) The dynamics observed in the current study bear a striking resemblance to this structure, with the addition of intervening loops in the network state corresponding to within-trial state evolution. Crucially, these observations do not preclude the existence of a line attractor. Instead, they may reflect the network's need to produce fast timescale dynamics within each trial, superimposed on the slower dynamics of the line attractor.</p><p>c) This alternative interpretation suggests that reward signals could function as inputs that shift the network state along the line attractor, with information being maintained across trials. The fast &quot;intervening behaviors&quot; observed by the authors could represent faster timescale dynamics occurring on top of the underlying line attractor dynamics, without erasing the accumulated evidence for reversals.</p><p>d) Given these considerations, the authors' conclusion that their results are better described by separable dynamic trajectories rather than fixed points on a line attractor may be premature. The observed dynamics could potentially be reconciled with a more nuanced understanding of line attractor models, where the attractor itself may be curved and coexist with faster timescale dynamics.</p></disp-quote><p>We appreciate the insightful comments on (1) the similarity of the work by Wang et al ’18 with our findings and (2) an alternative interpretation that augments the line attractor with fast timescale dynamics.</p><p>(1) We added a discussion of the work by Wang et al ’18 in Result Section 2 to point out the similarity of their findings in the principal component space with ours in the x_rev and x_choice space. We commented that such network dynamics could emerge when learning to perform the reversal learning the task, regardless of the training schemes.</p><p>We also mention that the RL approach in Wang et al ’18 does not consider within-trial dynamics, therefore lacks the non-stationary activity observed during the trial in the PFC of monkeys and our trained RNNs.</p><p>(2) We revised our original manuscript substantially to reconcile the line attractor model with the nonstationary activity observed during a trial.</p><p>Here are the highlights of the revised interpretation of the PFC and the RNN network activity</p><p>- The dynamics of x_rev consists of two activity modes, i.e., stationary activity at the start of a trial and non-stationary activity during the trial. Schematic of the augmented model that reconciles two activity modes is shown in Figure 4A. Analysis of the time derivative (dx_reverse / dt) and contractivity of the stationary state are shown in Figure 4B,C to demonstrate two activity modes.</p><p>- We discuss in Result Section 4 main text that the stationary activity is consistent with the line attractor model, but the non-stationary activity deviates from the model.</p><p>- The two activity modes are linked dynamically. There is an underlying dynamics that can map the stationary state to the non-stationary trajectory. This is shown by predicting the nonstationary trajectory with the stationary state using a support vector regression model. The prediction results are shown in Figure 4D,E,F.</p><p>- We discuss in Result Section 4 an extension of the standard line attractor model: points on the line attractor can serve as initial states that launch non-stationary activity associated with taskrelated behavior.</p><p>- The separability of neural trajectories presented in Result Section 5 is framed as a property of the non-stationary dynamics associated with task-related behavior.</p><disp-quote content-type="editor-comment"><p>To strengthen their claims, the authors should:</p><p>(1) Provide a more detailed description of their RNN training paradigm and task structure, including clear illustrations of target outputs.</p><p>(2) Discuss how their findings relate to and potentially extend previous work on similar tasks, particularly addressing the similarities and differences with the v-shaped state organization observed in reinforcement learning contexts. (<ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-018-0147-8">https://www.nature.com/articles/s41593-018-0147-8</ext-link> Figure1).</p><p>(3) Explore whether their results could be consistent with a curved line attractor model, rather than treating line attractors and dynamic trajectories as mutually exclusive alternatives.</p></disp-quote><p>Our response to these three comments is described above.</p><disp-quote content-type="editor-comment"><p>Addressing these points would significantly enhance the impact of the study and provide a more nuanced understanding of how reversal probabilities are represented in neural circuits.</p><p>In conclusion, while this study provides interesting insights into the neural representation of reversal probability, there are several areas where the methodology and interpretations could be refined.</p><p>Additional Minor Concerns:</p><p>(1) Network Training and Reversal Timing: The authors mention that the network was trained to switch after a reversal to match animal behavior, stating &quot;Maximum a Posterior (MAP) of the reversal probability converges a few trials past the MAP estimate.&quot; More explanation of how this training strategy relates to actual animal behavior would enhance the reader's understanding of the meaning of the model's similarity to animal behavior in Figure 1.</p></disp-quote><p>In Method Section 2.5, we described how our observation that the running estimate of MAP converges a few trials after the actual MAP is analogous to the animal’s reversal behavior.</p><p>“This observation can be interpreted as follows. If a subject performing the reversal learning task employs the ideal observer model to detect the trial at which reward schedule is reversed, the subject can infer the reversal of reward schedule a few trials past the actual reversal and then switch its preferred choice. This delay in behavioral reversal, relative to the reversal of reward schedule, is analogous to the monkeys switching their preferred choice a few trials after the reversal of reward schedule.”</p><p>In Step 4, we also mentioned that the target choice outputs are defined based on our observation in Step 3.</p><p>“We used the observation from Step 3 to define target choice outputs that switch abruptly a few trials after the reversal of reward schedule, denoted as <inline-formula><alternatives><mml:math id="sa4m1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>∗</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft337">\begin{document}$t^{(*)}$\end{document}</tex-math></alternatives></inline-formula> in the following. An example of target outputs are shown in Fig.\,\ref{fig_behavior}B.”</p><disp-quote content-type="editor-comment"><p>(2) How is the network simulated in step 1 of training? Is it just randomly initialized? What defines this network structure?</p></disp-quote><p>The initial state at the start of a block was random. We think the initial state is less relevant as the external inputs (i.e., cue and feedback) are strong and drive the network dynamics. We mentioned these setup and observation in Step 1 of training.</p><p>“Step 1. Simulate the network starting from a random initial state, apply the external inputs, i.e., cue and feedback inputs, at each trial and store the network choices and reward outcomes at all the trials in a block. The network dynamics is driven by the external inputs applied periodically over the trials.”</p><disp-quote content-type="editor-comment"><p>(3) Clarification on Learning Approach: More description of the approach in the main text would be beneficial. The statement &quot;Here, we trained RNNs that learned from a Bayesian inference model to mimic the behavioral strategies of monkeys performing the reversal learning task [2, 4]&quot; is somewhat confusing, as the model isn't directly fit to monkey data. A more detailed explanation of how the Bayesian inference model relates to monkey behavior and how it's used in RNN training would improve clarity.</p></disp-quote><p>We described the learning approach in more detail, but also tried to be concise without going into technical details.</p><p>We revised the sentence in Introduction as follows:</p><p>“We sought to train RNNs to mimic the behavioral strategies of monkeys performing the reversal learning task. Previous studies (Costa et al., 2015; Bartolo and Averbeck, 2020) have shown that a Bayesian inference model can capture a key aspect of the monkey's behavioral strategy, i.e., adhere to the preferred choice until the reversal of reward is detected and then switch abruptly. We trained the RNNs to replicate this behavioral strategy by training them on target behaviors generated from the Bayesian model.”</p><p>We also added a paragraph in Result Section 1 that explains in detail how the training approach works.</p><disp-quote content-type="editor-comment"><p>(4) In Figure 1B, it would be helpful to show the target output.</p></disp-quote><p>We added a figure in Fig1B that shows a schematic of how the target output is generated.</p><disp-quote content-type="editor-comment"><p>(5) An important point to consider is that a line attractor can be curved while still being topologically equivalent to a line. This nuance makes Figure 4A somewhat difficult to interpret. It might be helpful to discuss how the observed dynamics relate to potentially curved line attractors, which could provide a more nuanced understanding of the neural representations.</p></disp-quote><p>As discussed above, we interpret the “curved” activity during the trial as non-stationary activity. We do not think this non-stationary activity would be characterized as attractor. Attractor is (1) a minimal set of states that is (2) invariant under the dynamics and (3) attracting when perturbed into its neighborhood [Strogatz, <italic>Nonlinear dynamics and chaos</italic>]. If we consider the autonomous system without the behavior-related external input as the base system, then the non-stationary states could satisfy (2) and (3) but not (1), so they are not part of the attractor. If we include the behavior-related external input to the autonomous dynamics, then it may be possible that the non-stationary trajectories are part of the attractor. We adopted the former interpretation as the behavior-related inputs are external and transient.</p><disp-quote content-type="editor-comment"><p>(6) The results of the perturbation experiments seem to follow necessarily from the way x_rev was defined. It would be valuable to clarify if there's more to these results than what appears to be a direct consequence of the definition, or if there are subtleties in the experimental design or analysis that aren't immediately apparent.</p></disp-quote><p>The neural activity x_rev is correlated to the reversal probability, but it is unclear if the activity in this neural subspace is causally linked to behavioral variables, such as choice output. We added this explanation at the beginning of Results Section 7 to clarify the reason for performing the perturbation experiments.</p><p>“The neural activity $x_{rev}$ is obtained by identifying a neural subspace correlated to reversal probability. However, it remains to be shown if activity within this neural subspace is causally linked to behavioral variables, such as choice output.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>Below is a list of things I have found difficult to understand, and been puzzled/concerned about while reading the manuscript:</p><p>(1) It would be nice to say a bit more about the dataset that has been used for PFC analysis, e.g. number of neurons used and in what conditions is Figure 2A obtained (one has to go to supplementary to get the reference).</p></disp-quote><p>We added information about the PFC dataset in the opening paragraph of Result Section 2 to provide an overview of what type of neural data we’ve analyzed. It includes information about the number of recorded neurons, recording method and spike binning process.</p><disp-quote content-type="editor-comment"><p>(2) It would be nice to give more detail about the monkey task and better explain its trial structure.</p></disp-quote><p>In Result Section 1 we added a description of the overall task structure (and its difference with other versions of revesal learning task), the RNN / monkey trial structure and differences in RNN and monkey tasks.</p><disp-quote content-type="editor-comment"><p>(3) In the introduction it is mentioned that during the hold period, the probability of reversal is represented. Where does this statement come from?</p></disp-quote><p>The fact that neural activity during a hold period, i.e., fixation period before presenting the target images, encodes the probability of reversal was demonstrated in a previous study (Bartolo and Averbeck ’20).</p><p>We realize that our intention was to state that, during the hold period, the reversal probability activity is stationary as in the line attractor model, instead of focusing on that the probability of reversal is represented during this period. We revised the sentence to convey this message. In addition, we revised the entire paragraph to reinterpret our findings: there are two activity modes where the stationary activity is consistent with the line attractor model but the non-stationary activity deviates from it.</p><disp-quote content-type="editor-comment"><p>(4) &quot;Around the behavioral reversal trial, reversal probabilities were represented by a family of rankordered trajectories that shifted monotonically&quot;. This sentence is confusing and hard to understand.</p></disp-quote><p>Thank you for point this out. We rewrote the paragraph to reflect our revised interpretation. This sentence was removed, as it can be considered as part of the result on separable trajectories.</p><disp-quote content-type="editor-comment"><p>(5) For clarity, in the first section, when it is written that &quot;The reversal behavior of trained RNNs was similar to the monkey's behavior on the same task&quot; it would be nice to be more precise, that this is to be expected given the strategy used to train the network.</p></disp-quote><p>We removed this sentence as it makes a blanket statement. Instead, we compared the behavioral outputs of the RNNs and the monkeys one by one.</p><p>We added a sentence in Result Section 1 that the RNN’s abrupt behavioral reversal is expected as they are trained to mimic the target choice outputs of the Bayesian model.</p><p>“Such abrupt reversal behavior was expected as the RNNs were trained to mimic the target outputs of the Bayesian inference model.”</p><disp-quote content-type="editor-comment"><p>(6) What is the value of tau used in eq (1), and how does it compare to trial duration?</p></disp-quote><p>We described the value of time constant tau in Eq (1) and also discussed in Result Section 1 that tau=20ms is much faster than trial duration 500ms, thus the persistent behavior seen in trained RNNs is due to learning.</p><disp-quote content-type="editor-comment"><p>(7) It would be nice to expand around the notion of « temporally flexible representation » to help readers grasp what this means.</p></disp-quote><p>Instead of stating that the separable dynamic trajectories have “temporally flexible representation”, we break down in what sense it is temporally flexible: separable dynamic trajectories can accommodate the effects that task-related behavior have on generating non-stationary neural dynamics.</p><p>“In sum, our results show that, in a probabilistic reversal learning task, recurrent neural networks encode reversal probability by adopting, not only stationary states as in a line attractor, but also separable dynamic trajectories that can represent distinct probabilistic values while accommodating non-stationary dynamics associated with task-related behavior.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>(1) Data:</p><p>It would be useful to describe the experimental task, recording setup, and analyses in much more detail - both in the text and in the methods. What part of PFC are the recordings from? How many neurons were recorded over how many sessions? Which other papers have they been used in? All of these things are important for the reader to know, but are not listed anywhere. There are also some inconsistencies, with the main text e.g. listing the 'typical block length' as 36 trials, and the methods listing the block length as 24 trials (if this is a difference between the biological data and RNN, that should be more explicit and motivated).</p></disp-quote><p>We provided more detailed description of the monkey experimental task and PFC recordings in Result Section 1. We also added a new section in Methods 2.1 to describe the monkey experiment.</p><disp-quote content-type="editor-comment"><p>The experimental analyses should be explained in more detail in the methods. There is e.g. no detailed description of the analysis in Figure 6F.</p></disp-quote><p>We added a new section in Methods 6 to describe how the residual PFC activity is computed. It also describes the RNN perturbation experiments.</p><disp-quote content-type="editor-comment"><p>Finally, it would be useful for more analyses of monkey behaviour and performance, either in the main text or supplementary figures.</p></disp-quote><p>We did not pursue this comment as it is unclear how additional behavioral analyses would improve the manuscript.</p><disp-quote content-type="editor-comment"><p>(2) Model:</p><p>When fitting the network, 'step 1' of training in 2.3 seems superfluous. The posterior update from getting a reward at A is the same as that from not getting a reward at B (and vice versa), and it is therefore completely independent of the network choice. The reversal trial can therefore be inferred without ever simulating the network, simply by generating a sample of which trials have the 'good' option being rewarded and which trials have the 'bad' option being rewarded.</p></disp-quote><p>We respectfully disagree with Reviewer 3’s comment that the reversal trial can be inferred without ever simulating the network. The only way for the network to know about the underlying reward schedule is to perform the task by itself. By simulating the network, it can sample the options and the reward outcomes.</p><p>Our understanding is that Review 3 described a strategy that a human would use to perform this task. Our goal was to train the RNN to perform the task.</p><disp-quote content-type="editor-comment"><p>Do the blocks always start with choice A being optimal? Is everything similar if the network is trained with a variable initial rewarded option? E.g. in Fig 6, would you see the appropriate swap in the effect of the perturbation on choice probability if choice B was initially optimal?</p></disp-quote><p>Thank you for pointing out that the initial high-value option can be random. When setting up the reward schedule, the initial high-value option was chosen randomly from two choice outputs and, at the scheduled reversal, it was switched to the other option. We did not describe this in the original manuscript.</p><p>We added a descrption in Training Scheme Step 4 that the the initial high-value option is selected randomly. This is also explained in Result Section 1 when we give an overview of the RNN training procedure.</p><disp-quote content-type="editor-comment"><p>(3) Content:</p><p>It is rarely explained what the error bars represent (e.g. Figures 3B, 4C, ...) - this should be clear in all figures.</p></disp-quote><p>We added that the error bars represent the standard error of mean.</p><disp-quote content-type="editor-comment"><p>Figure 2A: this colour scheme is not great. There are abrupt colour changes both before and after the 'reversal' trial, and both of the extremes are hard to see.</p></disp-quote><p>We changed the color scheme to contrast pre- and post-reversal trials without the abrupt color change.</p><disp-quote content-type="editor-comment"><p>Figure 3E/F: how is prediction accuracy defined?</p></disp-quote><p>We added that the prediction accuracy is based on Pearson correlation.</p><disp-quote content-type="editor-comment"><p>Figure 4B: why focus on the derivative of the dynamics? The subsequent plots looking at the actual trajectories are much easier to understand. Also - what is 'relative trial' relative to?</p></disp-quote><p>The derivative was analyzed to demonstrate stationarity or non-stationarity of the neural activity. We think it will be clearer in the revised manuscript that the derivative allows us to characterize those two activity modes.</p><p>Relative trial number indicate the trial position relative to the behavioral reversal trial. We added this description to the figures when “relative trial” is used.</p><disp-quote content-type="editor-comment"><p>Figure 4C: what do these analyses look like if you match the trial numbers for the shift in trajectories? As it is now, there will presumably be more rewarded trials early and late in each block, and more unrewarded trials around the reversal point. Does this introduce biases in the analysis? A related question is (i) why the black lines are different in the top and bottom plots, and (ii) why the ends of the black lines are discontinuous with the beginnings of the red/blue lines.</p></disp-quote><p>We could not understand what Reviewer 3 was asking in this comment. It’d help if Review 3 could clarify the following question:</p><p>“Figure 4C: what do these analyses look like if you match the trial numbers for the shift in trajectories?”</p><p>Question (i): We wanted to look at how the trajectory shifts in the subsequent trial if a reward is or is not received in the current trial. The top panel analyzed all the trials in which the subsquent trial did not receive a reward. The bottom panel analyzed all the trials in which the subsequent trial received a reward. So, the trials analyzed in the top and bottom panels are different, and the black lines (x_rev of “current” trial) in the top and bottom panels are different.</p><p>Question (ii): Black line is from the preceding trial of the red/blue lines, so if trials are designed to be continuous with the inter-trial-interval, then black and red/blue should be continuous. However, in the monkey experiment, the inter-trial-intervals were variable, so the end of current trial does not match with the start of next trial. The neural trajectories presented in the manuscript did not include the activity in this inter-trial-interval.</p><disp-quote content-type="editor-comment"><p>Figure 6C: are the individual dots different RNNs? Claiming that there is a decrease in Delta x_choice for a v_+ stimulation is very misleading.</p></disp-quote><p>Yes individual dots are different RNN perturbations. We added explanation about the dots in Figure7C caption.</p><p>We agree with the comment that \Delta x_choice did not decrease. This sentence was removed. Instead, we revised the manuscript to state that x_choice for v_+ stimulation was smaller than the x_choice for v_- stimulation. We performed KS-test to confirm statistical significance.</p><disp-quote content-type="editor-comment"><p>Discussion: &quot;...exhibited behaviour consistent with an ideal Bayesian observer, as found in our study&quot;. The RNN was explicitly trained to reproduce an ideal Bayesian observer, so this can only really be considered an assumption (not a result) in the present study.</p></disp-quote><p>We agree that the statement in the original manuscript is inaccurate. It was revised to reflect that, in the other study, behavior outputs similar to a Bayesian observer emerged by simply learning to do the task, intead of directly mimicking the outputs of Bayesian observer as done in our study.</p><p>“Authors showed that trained RNNs exhibited behavior outputs consistent with an ideal Bayesian observer without explicitly learning from the Bayesian observer. This finding shows that the behavioral strategies of monkeys could emerge by simply learning to do the task, instead of directly mimicking the outputs of Bayesian observer as done in our study.”</p><disp-quote content-type="editor-comment"><p>Methods: Would the results differ if your Bayesian observer model used the true prior (i.e. the reversal happens in the middle 10 trials) rather than a uniform prior? Given the extensive literature on prior effects on animal behaviour, it is reasonable to expect that monkeys incorporate some non-uniform prior over the reversal point.</p></disp-quote><p>Thank you for pointing out the non-uniform prior. We haven’t conducted this analysis, but would guess that the convergence to the posterior distribution would be faster. We’d have to perform further analysis, which is out of the scope of this paper, to investigate whether the posteior distribution would be different from what we obtained from uniform prior.</p><disp-quote content-type="editor-comment"><p>Making the code available would make the work more transparent and useful to the community.</p></disp-quote><p>The code is available in the following Github repository: https://github.com/chrismkkim/LearnToReverse</p></body></sub-article></article>