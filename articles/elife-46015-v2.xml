<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">46015</article-id><article-id pub-id-type="doi">10.7554/eLife.46015</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Human Biology and Medicine</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural ensemble dynamics in dorsal motor cortex during speech in people with paralysis</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-131655"><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5238-0573</contrib-id><email>sergey.stavisky@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund20"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131742"><name><surname>Willett</surname><given-names>Francis R</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-161119"><name><surname>Wilson</surname><given-names>Guy H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0961-1994</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund13"/><xref ref-type="other" rid="fund14"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131743"><name><surname>Murphy</surname><given-names>Brian A</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131744"><name><surname>Rezaii</surname><given-names>Paymon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4803-0853</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131745"><name><surname>Avansino</surname><given-names>Donald T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131746"><name><surname>Memberg</surname><given-names>William D</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131747"><name><surname>Miller</surname><given-names>Jonathan P</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131748"><name><surname>Kirsch</surname><given-names>Robert F</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-30921"><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0261-2273</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund21"/><xref ref-type="other" rid="fund18"/><xref ref-type="other" rid="fund19"/><xref ref-type="other" rid="fund17"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund12"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-131749"><name><surname>Ajiboye</surname><given-names>A Bolu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9402-1165</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-131152"><name><surname>Druckmann</surname><given-names>Shaul</given-names></name><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-131750"><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="aff" rid="aff11">11</xref><xref ref-type="aff" rid="aff12">12</xref><xref ref-type="aff" rid="aff13">13</xref><xref ref-type="aff" rid="aff14">14</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund11"/><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-30922"><name><surname>Henderson</surname><given-names>Jaimie M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3276-2267</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff13">13</xref><xref ref-type="aff" rid="aff14">14</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf4"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Department of Neurosurgery</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Electrical Engineering</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Neurosciences Program</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Biomedical Engineering</institution><institution>Case Western Reserve University</institution><addr-line><named-content content-type="city">Cleveland</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">FES Center, Rehab R&amp;D Service</institution><institution>Louis Stokes Cleveland Department of Veterans Affairs Medical Center</institution><addr-line><named-content content-type="city">Cleveland</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Department of Neurosurgery</institution><institution>University Hospitals Cleveland Medical Center</institution><addr-line><named-content content-type="city">Cleveland</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution content-type="dept">VA RR&amp;D Center for Neurorestoration and Neurotechnology, Rehabilitation R&amp;D Service</institution><institution>Providence VA Medical Center</institution><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff8"><label>8</label><institution content-type="dept">Center for Neurotechnology and Neurorecovery, Department of Neurology</institution><institution>Massachusetts General Hospital, Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff9"><label>9</label><institution content-type="dept">School of Engineering and Robert J. &amp; Nandy D. Carney Institute for Brain Science</institution><institution>Brown University</institution><addr-line><named-content content-type="city">Providence</named-content></addr-line><country>United States</country></aff><aff id="aff10"><label>10</label><institution content-type="dept">Department of Neurobiology</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff11"><label>11</label><institution content-type="dept">Department of Bioengineering</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff12"><label>12</label><institution>Howard Hughes Medical Institute, Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff13"><label>13</label><institution content-type="dept">Wu Tsai Neurosciences Institute</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff14"><label>14</label><institution content-type="dept">Bio-X Program</institution><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>10</day><month>12</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e46015</elocation-id><history><date date-type="received" iso-8601-date="2019-02-14"><day>14</day><month>02</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-11-14"><day>14</day><month>11</month><year>2019</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-46015-v2.pdf"/><abstract><p>Speaking is a sensorimotor behavior whose neural basis is difficult to study with single neuron resolution due to the scarcity of human intracortical measurements. We used electrode arrays to record from the motor cortex ‘hand knob’ in two people with tetraplegia, an area not previously implicated in speech. Neurons modulated during speaking and during non-speaking movements of the tongue, lips, and jaw. This challenges whether the conventional model of a ‘motor homunculus’ division by major body regions extends to the single-neuron scale. Spoken words and syllables could be decoded from single trials, demonstrating the potential of intracortical recordings for brain-computer interfaces to restore speech. Two neural population dynamics features previously reported for arm movements were also present during speaking: a component that was mostly invariant across initiating different words, followed by rotatory dynamics during speaking. This suggests that common neural dynamical motifs may underlie movement of arm and speech articulators.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Speaking involves some of the most precise and coordinated movements humans make. Learning how the brain produces speech could lead to better treatments for speech disorders. But it can be challenging to study. Human speech is unique, limiting what can be learned from animal studies. There also are few opportunities where it would be safe or ethical to take measurements from inside a person’s brain while they talk. Most previous studies have recorded brain activity during speech in patients who have had electrodes placed in the brain for epilepsy or Parkinson’s disease treatment.</p><p>Now, Stavisky et al. show that brain cells that control hand and arm movements are also active during speech. Two patients who had lost the use of their arms and legs but were able to speak participated in the study. The two individuals were already enrolled in a pilot clinical trial of a brain-computer interface to help them control prosthetic devices. As part of this trial, the volunteer participants had two 100-electrode arrays surgically placed in the part of the brain that controls the movement of the arms and hands.</p><p>This study made the unexpected discovery that brain cells multitask controlling not just arm and hand movements, but also carry information about movements of the lips, tongue and mouth necessary for speech. Stavisky et al. also found similarities in the patterns of brain activity during hand and arm movements and speech.</p><p>By analyzing the activity in these brain cells as the two individuals recited words and syllables, Stavisky et al. were also able to train computers to identify which sound the person spoke from the brain activity alone. This is a first step towards developing a technology that could synthesize speech from a person’s brain activity as they try to speak. Much more work is needed to synthesize continuous speech. But the study provides initial evidence that it might be possible to use recordings from inside the brain to one day restore speech to individuals who have lost it.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech</kwd><kwd>motor control</kwd><kwd>intracortical</kwd><kwd>neural dynamics</kwd><kwd>brain-computer interface</kwd><kwd>motor cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000971</institution-id><institution>ALS Association</institution></institution-wrap></funding-source><award-id>Milton Safenowitz Postdoctoral Fellowship 17-PDF-364</award-id><principal-award-recipient><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100002112</institution-id><institution>A.P. Giannini Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Research Fellowship</award-id><principal-award-recipient><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Wu Tsai Neurosciences Institute</institution></institution-wrap></funding-source><award-id>Interdisciplinary Scholar Award</award-id><principal-award-recipient><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name></principal-award-recipient></award-group><award-group id="fund20"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000861</institution-id><institution>Burroughs Wellcome Fund</institution></institution-wrap></funding-source><award-id>Career Award at the Scientific Interface</award-id><principal-award-recipient><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name></principal-award-recipient></award-group><award-group id="fund13"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>Graduate Research Fellowships Program DGE - 1656518</award-id><principal-award-recipient><name><surname>Wilson</surname><given-names>Guy H</given-names></name></principal-award-recipient></award-group><award-group id="fund14"><funding-source><institution-wrap><institution>Regina Casper Stanford Graduate Fellowship</institution></institution-wrap></funding-source><award-id>DGE - 1656518</award-id><principal-award-recipient><name><surname>Wilson</surname><given-names>Guy H</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009633</institution-id><institution>Eunice Kennedy Shriver National Institute of Child Health and Human Development</institution></institution-wrap></funding-source><award-id>R01HD077220</award-id><principal-award-recipient><name><surname>Kirsch</surname><given-names>Robert F</given-names></name></principal-award-recipient></award-group><award-group id="fund17"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000738</institution-id><institution>U.S. Department of Veterans Affairs</institution></institution-wrap></funding-source><award-id>Office of Research and Development, Rehabilitation R&amp;D Service N9228C</award-id><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund19"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000738</institution-id><institution>U.S. Department of Veterans Affairs</institution></institution-wrap></funding-source><award-id>Office of Research and Development, Rehabilitation R&amp;D Service B6453R</award-id><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund18"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000738</institution-id><institution>U.S. Department of Veterans Affairs</institution></institution-wrap></funding-source><award-id>Office of Research and Development, Rehabilitation R&amp;D Service A2295R</award-id><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund21"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000738</institution-id><institution>U.S. Department of Veterans Affairs</institution></institution-wrap></funding-source><award-id>Office of Research and Development, Rehabilitation R&amp;D Service N2864C</award-id><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01DC009899</award-id><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution>Executive Committee on Research of Massachusetts General Hospital</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>5U01NS098968-02</award-id><principal-award-recipient><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Larry and Pamela Garlick</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Henderson</surname><given-names>Jaimie M</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>Samuel and Betsy Reeves</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Henderson</surname><given-names>Jaimie M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01DC014034</award-id><principal-award-recipient><name><surname>Henderson</surname><given-names>Jaimie M</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurons in human dorsal motor cortex, an area involved in controlling arm and hand movements, are also active – and show similar ensemble dynamics – during speaking.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Speaking requires coordinating numerous articulator muscles with exquisite timing and precision. Understanding how the sensorimotor system accomplishes this behavioral feat requires studying its neural underpinnings, which are critical for identifying (<xref ref-type="bibr" rid="bib81">Tankus and Fried, 2018</xref>) and treating the causes of speech disorders and for building brain-computer interfaces (BCIs) to restore lost speech (<xref ref-type="bibr" rid="bib37">Guenther et al., 2009</xref>; <xref ref-type="bibr" rid="bib39">Herff and Schultz, 2016</xref>). Speaking is also a uniquely human behavior, which presents a high barrier to electrophysiological investigations. Previous direct neural recordings during speaking have come from electrocorticography (ECoG) (<xref ref-type="bibr" rid="bib8">Bouchard and Chang, 2014</xref>; <xref ref-type="bibr" rid="bib20">Cheung et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Mugler et al., 2014</xref>) or single-unit (SUA) recordings from penetrating electrodes during the course of clinical treatment for epilepsy (<xref ref-type="bibr" rid="bib17">Chan et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Creutzfeldt et al., 1989</xref>; <xref ref-type="bibr" rid="bib80">Tankus et al., 2012</xref>) or deep brain stimulation for Parkinson’s disease (<xref ref-type="bibr" rid="bib49">Lipski et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Tankus and Fried, 2018</xref>). Such studies have begun to characterize motor cortical population dynamics underlying speech (<xref ref-type="bibr" rid="bib7">Bouchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Pei et al., 2011</xref>), but not at the finer spatial scale (compared to ECoG) or across larger neural ensembles (compared to single electrodes) afforded by the high-density intracortical recordings widely used in animal studies (<xref ref-type="bibr" rid="bib3">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib45">Kiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Smith and Kohn, 2008</xref>), including those examining arm reaching (<xref ref-type="bibr" rid="bib14">Carmena et al., 2003</xref>; <xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Maynard et al., 1999</xref>).</p><p>We studied speech production at this resolution by recording from multielectrode arrays previously placed in human motor cortex as part of the BrainGate2 BCI clinical trial (<xref ref-type="bibr" rid="bib40">Hochberg et al., 2006</xref>). This research context dictated two important elements of the present study’s design. First, both participants had tetraplegia due to spinal-cord injury but were able to speak; this enabled observing motor cortical spiking activity during overt speaking, in contrast to earlier studies of attempted speech by participants unable to speak (<xref ref-type="bibr" rid="bib13">Brumberg et al., 2011</xref>; <xref ref-type="bibr" rid="bib37">Guenther et al., 2009</xref>). However, these participants’ long-term paralysis means that their neurophysiology may differ from that of people who are able-bodied; we will discuss the need for interpretation caution in the Discussion.</p><p>Second, the electrode arrays were in dorsal ‘hand knob’ area of motor cortex, which we previously found to strongly modulate to these participants’ attempted movement of their arm and hand (<xref ref-type="bibr" rid="bib1">Ajiboye et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Brandman et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>). Speech-related activity has not previously been reported in this cortical area, but there are several hints in the literature that dorsal motor cortex may have speech-related activity. Although imaging experiments consistently identify ventral cortical activation during speaking tasks, a meta-analysis of such studies (<xref ref-type="bibr" rid="bib38">Guenther, 2016</xref>) indicates that responses are occasionally seen (though not, to our knowledge, explicitly called out) in dorsal motor cortex. Additionally, behavioral (<xref ref-type="bibr" rid="bib35">Gentilucci and Campione, 2011</xref>; <xref ref-type="bibr" rid="bib84">Vainio et al., 2013</xref>), transcranial magnetic stimulation studies (<xref ref-type="bibr" rid="bib27">Devlin and Watkins, 2007</xref>; <xref ref-type="bibr" rid="bib59">Meister et al., 2003</xref>), and electrical stimulation mapping studies (<xref ref-type="bibr" rid="bib12">Breshears et al., 2018</xref>) have reported interactions (and interference) between motor control of the hand and mouth. This close linkage between hand and speech networks has been hypothesized to be due to a need for hand-mouth coordination and an evolutionary relationship between manual and articulatory gestures (<xref ref-type="bibr" rid="bib36">Gentilucci and Stefani, 2012</xref>; <xref ref-type="bibr" rid="bib71">Rizzolatti and Arbib, 1998</xref>). Here, we explicitly set out to test whether neuronal firing rates in this dorsal motor cortical area modulated when participants produced speech and orofacial movements.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Speech-related activity in dorsal motor cortex</title><p>We recorded neural activity during speaking from participants ‘T5’ and ‘T8’, who previously had two arrays each consisting of 96 electrodes placed in the ‘hand knob’ area of motor cortex (<xref ref-type="fig" rid="fig1">Figure 1A,B</xref>). The participants performed a task in which on each trial they heard one of 10 different syllables or one of 10 short words, and then spoke the prompted sound after hearing a go cue (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> shows audio spectrograms and reaction times for these tasks). We analyzed both sortable SUA that could be attributed to an individual neuron’s action potentials (<xref ref-type="fig" rid="fig1">Figure 1C,D</xref>), and ‘threshold crossing’ spikes (TCs) that might come from one or several neurons (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Firing rates showed robust changes during speaking of syllables (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, <xref ref-type="video" rid="video1">Video 1</xref>) and words (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Significant modulation was found during speaking at least one syllable (p&lt;0.05 compared to during silence) in 73/104 T5 electrodes’ TCs (13/22 SUA) and 47/101 T8 electrodes (12/25 SUA). Active neurons were distributed throughout the area sampled by the arrays, and most modulated to speaking multiple syllables (<xref ref-type="fig" rid="fig1">Figure 1B</xref> and <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>), suggesting a broadly distributed coding scheme. This is consistent with previous single neuron recordings in the temporal lobe (<xref ref-type="bibr" rid="bib25">Creutzfeldt et al., 1989</xref>; <xref ref-type="bibr" rid="bib80">Tankus et al., 2012</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Speech-related neuronal spiking activity in dorsal motor cortex.</title><p>(<bold>A</bold>) Participants heard a syllable or word prompt played from a computer speaker and were instructed to speak it back after hearing a go cue. Motor cortical signals and audio were simultaneously recorded during the task. The timeline shows example audio data recorded during one trial. (<bold>B</bold>) Participants’ MRI-derived brain anatomy. Blue squares mark the locations of the two chronic 96-electrode arrays. Insets show electrode locations, with shading indicating the number of different syllables for which that electrode recorded significantly modulated firing rates (darker shading = more syllables). Non-functioning electrodes are shown as smaller dots. CS is central sulcus. (<bold>C</bold>) Raster plot showing spike times of an example neuron across multiple trials of participant T5 speaking nine different syllables, or silence. Data are aligned to the prompt, the go cue, and acoustic onset (AO). (<bold>D</bold>) Trial-averaged firing rates (mean ± s.e.) for the same neuron and two others. Insets show these neurons’ action potential waveforms (mean ± s.d.). The electrodes where these neurons were recorded are circled in the panel B insets using colors corresponding to these waveforms. (<bold>E</bold>) Time course of overall neural modulation for each syllable after hearing the prompt (left alignment) and when speaking (right alignment). Population neural distances between the spoken and silent conditions were calculated from TCs using an unbiased measurement of firing rate vector differences (see Methods). This metric yields signed values near zero when population firing rates are essentially the same between conditions. Firing rate changes were significantly greater (p &lt; 0.01, sign-rank test) during speech production (comparison epoch shown by the black window after Go) compared to after hearing the prompt (gray window after Prompt). Each syllable’s mean modulation across the comparison epoch is shown with the corresponding color’s horizontal tick to the right of the plot. The vertical scale is the same across participants, revealing the larger speech-related modulation in T5’s recordings.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Prompted speaking tasks behavior.</title><p>(<bold>A</bold>) Acoustic spectrograms for the participants’ spoken syllables. Power was averaged over all analyzed trials. Note that <italic>da</italic> is missing for T5 because he usually misheard this cue as <italic>ga</italic> or <italic>ba</italic>. (<bold>B</bold>) Same as panel A but for the words datasets. (<bold>C</bold>) Reaction time distributions for each dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Example threshold crossing spike rates.</title><p>The left column shows –4.5 × root mean square voltage threshold crossing firing rates during the syllables task, recorded on the electrodes from which the single neuron spikes in <xref ref-type="fig" rid="fig1">Figure 1D</xref> were spike sorted. The right column shows three additional example electrodes’ firing rates. Insets shows the unsorted threshold crossing spike waveforms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Neural activity while speaking short words.</title><p>(<bold>A</bold>) Firing rates during speaking of short words for three example neurons (blue spike waveform insets) and three example electrodes’ −4.5 × RMS threshold crossing spikes (gray waveform insets). Data are presented similarly to <xref ref-type="fig" rid="fig1">Figure 1D</xref> and are from the T5-words and T8-words datasets. (<bold>B</bold>) Firing rate differences compared to the silent condition across the population of threshold crossings, presented as in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. The ensemble modulation was significantly greater when speaking words compared to when hearing the prompts (p&lt;0.01, sign-rank test).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig1-figsupp3-v2.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Neural correlates of spoken syllables are not spatially segregated in dorsal motor cortex.</title><p>(<bold>A</bold>) Electrode array maps similar to <xref ref-type="fig" rid="fig1">Figure 1B</xref> insets are shown for each syllable separately to reveal where modulation was observed during production of that sound. Electrodes where the TCs firing rate changed significantly during speech, as compared to the silent condition, are shown as colored circles. Non-modulating electrodes are shown as larger gray circles, and non-functioning electrodes are shown as smaller dots. Adding up how many different syllables each electrode’s activity modulates to yields the summary insets shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. These plots reveal that electrodes were not segregated into distinct cortical areas based on which syllables they modulated to. (<bold>B</bold>) Histograms showing the distribution of how many different syllables evoke a significant firing rate change for electrode TCs (each participant’s left plot) and sorted single neurons (right plot). The first bar in each plot, which corresponds to electrodes or neurons whose activity only changed when speaking one syllable, is further divided based on which syllable this modulation was specific to (same color scheme as in panel A). This reveals two things. First, single neurons or TCs (which may capture small numbers of nearby neurons) were typically not narrowly tuned to one sound. Second, there was not one specific syllable whose neural correlates were consistently observed on separate electrodes/neurons from the rest of the syllables.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig1-figsupp4-v2.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Neural activity shows phonetic structure.</title><p>(<bold>A</bold>) The T5-phonemes dataset consists of the participant speaking 420 unique words which together sampled 41 American English phonemes. We constructed firing rate vectors for each phoneme using a 150 ms window centered on the phoneme start (one element per electrode), averaged across every instance of that phoneme. This dissimilarity matrix shows the difference between each pair of phonemes’ firing rate vectors, calculated using the same neural distance method as in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. The matrix is symmetrical across the diagonal. Diagonal elements (i.e. within-phoneme distances) were constructed by comparing split halves of each phoneme’s instances. The phonemes are ordered by place of articulation grouping (each group is outlined with a box of different color). (<bold>B</bold>) Violin plots showing all neural distances from panel A divided based on whether the two compared phonemes are in the same place of articulation group (‘Within group’, red) or whether the two phonemes are from different place of articulation groups (‘Between groups’, black). Center circles show each distribution’s median, vertical bars show 25th to 75th percentiles, and horizontal bars shows distribution means. The mean neural distance across all Within group pairs was 30.6 Hz, while the mean across all Between group pairs was 42.8 Hz (difference = 12.2 Hz). (<bold>C</bold>) The difference in between-group versus within-group neural distances from panel B, marked with the blue line, far exceeds the distribution of shuffled distances (brown) in which the same summary statistic was computed 10,000 times after randomly permuting the neural distance matrix rows and columns. These shuffles provide a null control in which the relationship between phoneme pairs’ neural activity differences and these phonemes’ place of articulation groupings are scrambled. (<bold>D</bold>) A hierarchical clustering dendrogram based on phonemes’ neural population distances from panel A. At the bottom level, each phoneme is placed next to the (other) phoneme with the most similar neural population activity. Successive levels combine nearest phoneme clusters. By grouping phonemes based solely on their neural similarities (rather than one specific trait like place of articulation, indicated here with the same colors as in the panel A groupings), this dendrogram provides a complementary view that highlights that many neural nearest neighbors are phonetically similar (e.g. /d/ and /g/ stop-plosives, /θ/ and /v/ fricatives, /ŋ/ and /n/ nasals) and that related phonemes form larger clusters, such as the left-most major branch of mostly vowels or the sibilant cluster /s/, /ʃ/, and /dʒ/. At the same time, there are some phonemes that appear out of place, such as plosive consonant /b/ appearing between vowels /ɑ/ and /ɔ/ (we speculate this could reflect neural correlates of co-articulation from the vowels that frequently followed the brief /b/ sound).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig1-figsupp5-v2.tif"/></fig></fig-group><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-46015-video1.mp4"><label>Video 1.</label><caption><title>Example audio and neural data from eleven contiguous trials of the prompted syllables speaking task.</title><p>The audio track was recorded during the task and digitized alongside the neural data; it starts with the two beeps indicating trial start, after which the syllable prompt was played from computer speakers, followed by the go cue clicks, and finally the participant speaking the syllable. The video shows the concurrent −4.5 × RMS threshold crossing spikes rate on each electrode. Each circle corresponds to one electrode, with their spatial layout corresponding to electrodes’ locations in motor cortex as in the <xref ref-type="fig" rid="fig1">Figure 1B</xref> inset. Each electrode’s moment-by-moment color and size represent its firing rate (soft-normalized with a 10 Hz offset, smoothed with a 50 ms s.d. Gaussian kernel). The color map goes from pink (minimum rate across electrodes) to yellow (maximum rate), while size varies from small (minimum rate) to large (maximum rate). Non-functioning electrodes are shown as small gray dots. To assist the viewer in perceiving the gestalt of the population activity, a larger central disk shows the mean firing rate across all functioning electrodes, without soft-normalization. Data are from the T5-syllables dataset, trial set #23.</p></caption></media><p>Three observations lead us to believe that this neural activity is related to motor cortical control of the speech articulators (<xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Conant et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Mugler et al., 2018</xref>) rather than perception or language. First, modulation was significantly stronger when speaking compared to after hearing the auditory prompts: the neural population firing rate change compared to the silent condition was 4.03 times higher after the go cue compared to after the audio prompt for the T5-syllables dataset, 2.90x for the T8-syllables dataset (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), 6.71x higher for the T5-words dataset, and 2.12x for T8-words (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Modulation following the audio prompt, although small, was significant when compared to a 1 s epoch just prior to the prompt (p&lt;0.01, sign-rank test, all four datasets). In this study, we are unable to disambiguate whether this prompt-related response reflects auditory perception, movement preparation, or small overt movements preceding vocalization. We will primarily focus on the larger, later neural modulation putatively related to speech production.</p><p>Second, analysis of an additional dataset in which participant T5 spoke 41 different phonemes revealed that neural population activity showed phonemic structure (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>): for example, when phonemes were grouped by place of articulation (<xref ref-type="bibr" rid="bib7">Bouchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Lotte et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Moses et al., 2019</xref>), population firing rate vectors were significantly more similar between phonemes within the same group than between phonemes in different groups (p&lt;0.001, shuffle test). Third, in both participants, 99 of 120 electrodes that were active during speaking syllables (24 out of 25 sorted neurons) also were active during production of at least one of seven non-speech orofacial movements (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We also observed weak but significant firing rate correlations with breathing (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Modulation for speaking was stronger than for unattended breathing (~4.7 x) and instructed breathing (~2.6 x), and modulation for attempted arm movements was stronger than for speaking and orofacial movements (~2.8 x, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The same motor cortical population is also active during non-speaking orofacial movements.</title><p>(<bold>A</bold>) Both participants performed an orofacial movement task during the same research session as their syllables speaking task. Examples of single neuron firing rates during seven different orofacial movements are plotted in colors corresponding to the movements in the illustrated legend above. The ‘stay still’ condition is plotted in black. The same three example neurons from <xref ref-type="fig" rid="fig1">Figure 1D</xref> are included here. The other three neurons were chosen to illustrate a variety of observed response patterns. (<bold>B</bold>) Electrode array maps indicating the number of different orofacial movements for which a given electrode’s −4.5 × RMS threshold crossing rates differed significantly from the stay still condition. Data are presented similarly to the <xref ref-type="fig" rid="fig1">Figure 1B</xref> insets. Firing rates on most functioning electrodes modulated for multiple orofacial movements. See <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for individual movements’ electrode response maps. (<bold>C</bold>) Breakdown of how many neurons’ (top) and electrodes’ TCs (bottom) exhibited firing rate modulation during speaking syllables only (red), non-speaking orofacial movements only (blue), both behaviors (purple), or neither behavior (gray). A unit or electrode was deemed to modulate during a behavior if its firing rate differed significantly from silence/staying still for at least one syllable/movement.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Neural correlates of orofacial movements are not spatially segregated in dorsal motor cortex.</title><p>The orofacial movements data were analyzed and are presented like the speaking data in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Dorsal motor cortex correlates of breathing.</title><p>(<bold>A</bold>) We recorded neural data and a breathing proxy (the stretch of a belt wrapped around participant T5’s abdomen) while he performed a BCI cursor task or sat idly (‘unattended’ breathing condition, left) or intentionally breathed in and out based on on-screen instructions (‘instructed’, right). Example continuous breath belt measurements from this T5-breathing dataset are shown. Violet dots mark the identified breath peak times. (<bold>B</bold>) Gray traces show example respiratory belt measurements for 200 unattended (left) and instructed (right) breaths, aligned to the breath peak. The means of all 727 unattended and 275 instructed breaths are shown in black. (<bold>C</bold>) Breath-aligned firing rates for three example electrodes’ TCs (mean ± s.e.m) during unattended breathing. Breath-related modulation depth was calculated as the peak-to-trough firing rate difference. Horizontal dashed lines show the p=0.01 modulation depths for a shuffle control in which breath peak times were chosen at random. Significantly greater breath-related modulation in either the unattended or instructed condition was observed on 71 out of 121 electrodes. (<bold>D</bold>) Breath-aligned firing rates were also calculated for sortable SUA, whose spike waveform snippets are shown as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Firing rates of three example neurons with large waveforms are shown during unattended breathing. Overall, 17 out of 19 sorted units had significant modulation in either the unattended or instructed condition. This argues against the breath-related modulation being an artifact of breath-related electrode array micromotion causing a change in TCs firing rates by bringing additional units in or out of recording range. (<bold>E</bold>) Unattended (top) and instructed (bottom) breath-related modulation depth for each functioning electrode’s TCs, presented as in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. Two extrema values exceeded the 30 Hz color range maximum. (<bold>F</bold>) Histograms of modulation depths during unattended breathing (gray), instructed breathing (blue), and speaking-related modulation depths (red) for all functioning electrodes in the T5-breathing and T5-syllables datasets, respectively. Two outlier datums are grouped in a &gt; 60 Hz bin. Dorsal motor cortical modulation was greater for speaking than breathing (all three distributions significantly differed from one another, p&lt;0.001, rank-sum tests). (<bold>G</bold>) Mean population firing rates (averaged across all functioning electrodes) are shown during unattended (gray) and instructed (blue) breathing, aligned to breath peak, and during speaking (red), aligned to acoustic onset. The vertical offsets of the breathing traces have been bottom-aligned to the speaking trace to facilitate comparison. Note that the mean population rate can obscure counter-acting firing rate increases and decreases across electrodes; panel F provides a complementary description capturing both rate increases and decreases.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Dorsal motor cortex modulates more strongly during attempted arm and hand movements than orofacial movements and speaking.</title><p>Participant T5 performed a variety of instructed movements. We compared neural activity when attempting orofacial and speech production movements (red), versus when attempting arm and hand movements (black). Note that T5, who has tetraplegia, was able to actualize all the orofacial and speech movements, but not all the arm and hand movements. Each point corresponds to the mean TCs firing rate modulation for a single instructed movement (movements are listed in the order corresponding to their points’ vertical coordinates) in the T5-comparisons dataset. Modulation strength is calculated by taking the firing rate vector neural distance (like in <xref ref-type="fig" rid="fig1">Figure 1E</xref>) between when T5 attempted/performed the instructed movement and otherwise similar trials where the instruction was to ‘do nothing’. Bars show the mean modulation across all the movements in each grouping. A rank-sum test was used to compare the distributions of orofacial and speech and arm and hand modulations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig2-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Speech can be decoded from intracortical activity on individual trials</title><p>We next performed a decoding analysis to quantify how much information about the spoken syllable or word was present in the time-varying neural activity. Multi-class support vector machines were used to predict the spoken sound (or silence) from single trial TCs and high-frequency LFP power (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Cross-validated prediction accuracies for syllables were 84.6% for T5 (10 classes, mean chance accuracy was 10.1% across shuffle controls) and 54.7% for T8 (11 classes, chance was 8.6%). Word decoding accuracies were 83.5% for T5 (11 classes, chance was 9.1%) and 61.5% for T8 (11 classes, chance was 9.3%). We also used the same method to decode neural activity from 0 to 500 ms after the speech prompt and found that classification accuracies were only marginally better than chance (overall accuracies between 11.1% and 16.6% across the four datasets, p&lt;0.05 versus shuffle controls in three of the four datasets; <xref ref-type="fig" rid="fig3">Figure 3C</xref> gray bars). The much higher neural discriminability of syllables and words during speaking rather than after hearing the audio prompt is consistent with the previously enumerated evidence that modulation in this cortical area is related to speech production.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Speech can be decoded from intracortical activity.</title><p>(<bold>A</bold>) To quantify the speech-related information in the neural population activity, we constructed a feature vector for each trial consisting of each electrode’s spike count and HLFP power in ten 100 ms bins centered on AO. For visualization, two-dimensional t-SNE projections of this feature vector are shown for all trials of the T5-syllables dataset. Each point corresponds to one trial. Even in this two-dimensional view of the underlying high-dimensional neural data, different syllables’ trials are discriminable and phonetically similar sounds’ clusters are closer together. (<bold>B</bold>) The high-dimensional neural feature vectors were classified using a multiclass SVM. Confusion matrices are shown for each participant’s leave-one-trial-out classification when speaking syllables (top row) and words (bottom row). Each matrix element shows the percentage of trials of the corresponding row’s sound that were classified as the sound of the corresponding column. Diagonal elements show correct classifications. (<bold>C</bold>) Bar heights show overall classification accuracies for decoding neural activity during speech (black bars, summarizing panel B) and decoding neural activity following the audio prompt (gray bars). Each small point corresponds to the accuracy for one class (silence, syllable, or word). Brown boxes show the range of chance performance: each box’s bottom/center/top correspond to minimum/mean/maximum overall classification accuracy for shuffled labels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig3-v2.tif"/></fig><p>During speaking, decoding accuracies for all individual sounds were above chance (p&lt;0.01, shuffle test). Decoding mistakes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) and low-dimensional representations (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) tended to follow phonetic similarities (e.g. <italic>ba</italic> and <italic>ga</italic>, <italic>a</italic> and <italic>ae</italic>). This observation is consistent with previous ECoG studies (<xref ref-type="bibr" rid="bib7">Bouchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Cheung et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Livezey et al., 2019</xref>; <xref ref-type="bibr" rid="bib60">Moses et al., 2019</xref>; <xref ref-type="bibr" rid="bib61">Mugler et al., 2014</xref>), although the larger neural differences we observed between unvoiced <italic>k</italic> and <italic>p</italic> and the beginning of their voiced counterparts at the start of <italic>ga</italic> and <italic>ba</italic> suggests strong laryngeal tuning (<xref ref-type="bibr" rid="bib28">Dichter et al., 2018</xref>). These neural correlate similarities may reflect similarities in the underlying articulator movements (<xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Lotte et al., 2015</xref>; <xref ref-type="bibr" rid="bib62">Mugler et al., 2018</xref>).</p></sec><sec id="s2-3"><title>Neural population dynamics exhibit low-dimensional structure during speech</title><p>These multielectrode recordings enabled us to observe motor cortical dynamics during speech at their fundamental spatiotemporal scale: neuron spiking activity. Specifically, we examined whether two known key dynamical features of motor cortex firing rates during arm reaching were also present during speaking. Importantly, both of these features were revealed when looking not at individual neurons’ firing rates, but rather were seen when examining the time courses of population activity ‘components’ that act as lower dimensional building blocks (or condensed summaries) of the many individual neurons’ activities (<xref ref-type="bibr" rid="bib34">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib67">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Saxena and Cunningham, 2019</xref>). The first prominent neural population dynamics feature (‘dynamical motif’) we tested for is inspired by previous nonhuman primate (NHP) experiments showing that the neural state undergoes a rapid change during movement initiation which is dominated by a condition-invariant signal (CIS) (<xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>). In that study, Kaufman and colleagues provide a comprehensive exposition on why a large neural component that is highly invariant across many different arm reaches is a non-trivial feature of neural population data and could, despite its non-specificity, be important to the overall computations being performed. A similar CIS was recently also reported during NHP grasping (<xref ref-type="bibr" rid="bib41">Intveld et al., 2018</xref>).</p><p>The second dynamical motif we tested for follows studies of NHP arm reaches (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>) and human point-to-point hand movements (<xref ref-type="bibr" rid="bib65">Pandarinath et al., 2015</xref>), which showed that subsequent peri-movement neural ensemble activity is characterized by orderly rotatory dynamics. That is, a substantial portion of moment-by-moment firing rate changes can be explained by a simple rotation of the neural state in a plane that summarizes the correlated activity of groups of neurons. These observations, in concert with neural network modeling (<xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>), have led to a model of motor control in which, prior to movement, inputs specifying the movement goal create attractor dynamics toward an advantageous initial condition (<xref ref-type="bibr" rid="bib74">Shenoy et al., 2013</xref>). During movement initiation, a large transient input ‘kicks’ the network into a different state from which activity evolves according to rotatory dynamics such that muscle activity is constructed from an oscillatory basis set (akin to composing an arbitrary signal from a Fourier basis set) (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib79">Sussillo et al., 2015</xref>).</p><p>We tested whether motor cortical activity during speaking also exhibits these dynamics by applying the analytical methods of <xref ref-type="bibr" rid="bib21">Churchland et al. (2012)</xref> and <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>. These analyses used two different dimensionality reduction techniques (<xref ref-type="bibr" rid="bib26">Cunningham and Yu, 2014</xref>) to reveal latent low-dimensional structure in the trial-averaged firing rates for different conditions (here, speaking different words). Both methods sought to find a modest number of linear weightings of different electrodes’ firing rates (forming the aforementioned neural population activity ‘components’) that capture a large fraction of the overall variance. This is akin to principal components analysis (PCA), but unlike PCA, each method also looks for a specific form of neural population structure: jPCA (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>) seeks components with rotatory dynamics, whereas dPCA (<xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Kobak et al., 2016</xref>) decomposes neural activity into CI and condition-dependent (CD) components. Importantly, these methods do not spuriously find the sought dynamical structure when it is not present in the data (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Elsayed and Cunningham, 2017</xref>; <xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Kobak et al., 2016</xref>; <xref ref-type="bibr" rid="bib65">Pandarinath et al., 2015</xref>).</p><p>We found that these two prominent population dynamics motifs were indeed also present during speaking. Like in <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>, the largest dPCA component summarizing each participants’ neural activity during movement initiation was largely CI: this component was 98.7% CI in participant T5, and 87.3% CI in participant T8 (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). <xref ref-type="fig" rid="fig4">Figure 4B</xref> shows that in T5, this ‘CIS<sub>1</sub>’ component, which rapidly increased after the go cue, was essentially identical regardless of which word was spoken. In T8, the CIS<sub>1</sub> was not as cleanly condition-invariant, but nonetheless showed a similar increase following the go cue for each word. We also found this condition-invariant neural population activity component in all four additional datasets that we examined: T5’s and T8’s syllables task datasets, as well as two additional replication datasets in which participant T5 spoke just five of the words (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). These results were also robust across different choices of how many dPCs to summarize the neural population activity with (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>A condition-invariant signal during speech initiation.</title><p>(<bold>A</bold>) A large component of neural population activity during speech initiation is a condition-invariant (CI) neural state change. Firing rates from 200 ms before to 400 ms after the go cue (participant T5) and 100 ms to 700 ms after the go cue (T8) were decomposed into dPCA components like in <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>. Each bar shows the relative variance captured by each dPCA component, which consists of both CI variance (red) and condition-dependent (CD) variance (blue). These eight dPCs captured 45.1% (T5-words) and 8.4% (T8-words) of the overall neural variance, which includes non-task related variability (‘noise’). (<bold>B</bold>) Neural population activity during speech initiation was projected onto the first dPC dimension; this ‘CIS<sub>1</sub>’ is the first component from panel A. Traces show the trial-averaged CIS<sub>1</sub> activity when speaking different words, denoted by the same colors as in <xref ref-type="fig" rid="fig3">Figure 3B</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Further details of neural population dynamics analyses and additional datasets.</title><p>(<bold>A</bold>) Cumulative trial-averaged firing rate variance explained during each dataset’s speech initiation epoch as a function of the number of PCA dimensions (top set of curves) and dPCA dimensions (bottom set of curves) used to reduce the dimensionality of the data. The dotted lines mark the eight dimensions used for dPCA in the condition-invariant signal analyses for panels C-E and in <xref ref-type="fig" rid="fig4">Figure 4</xref>. In addition to the T5-words and T8-words datasets (blue curves) shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, this figure also shows results from the T5-syllables and T8-syllables datasets (orange curves) as well as the T5-5words-A and T5-5words-B replication datasets (gold and purple curves). (<bold>B</bold>) Cumulative trial-averaged firing rate variance explained during all six datasets’ speech generation epochs. The dotted lines mark the six dimensions used for jPCA in the rotatory dynamics analyses for panels E-H and <xref ref-type="fig" rid="fig5">Figure 5</xref>. PCA and jPCA curves are one and the same because jPCA operates within the neural subspace found using PCA. (<bold>C</bold>) Firing rates during the speech initiation epoch of each dataset were decomposed into dPCA components like in <xref ref-type="fig" rid="fig4">Figure 4A</xref>. The additional inset matrices quantify the relationships between dPC components as in <xref ref-type="bibr" rid="bib47">Kobak et al. (2016)</xref>: each element of the upper triangle shows the dot product between the corresponding row’s and column’s demixed principal <italic>axes</italic> (i.e. the dPCA encoder dimensions), while the bottom triangle shows the correlations between the corresponding row’s and column’s demixed principal <italic>components</italic> (i.e. the neural data projected onto this row’s and column’s demixed principal decoder axes). More red or blue upper triangle elements denote that this pair of neural dimensions are less orthogonal, while more red or blue lower triangle elements denote that this pair of components (summarized population firing rates) have a similar time course. Stars denote pairs of dimensions that are significantly non-orthogonal at p&lt;0.01 (Kendall correlation). (<bold>D</bold>) Neural population activity projected onto the first dPC dimension, shown as in <xref ref-type="fig" rid="fig4">Figure 4B</xref> for all six datasets. (<bold>E</bold>) The subspace angle between the CIS<sub>1</sub> dimension from panels C,D and the top jPC plane from panel F, for each dataset. Across all six datasets, the CIS<sub>1</sub> was not significantly non-orthogonal to any of the six jPC dimensions (p&gt;0.01, Kendall correlation) except for the angle between CIS<sub>1</sub> and jPC<sub>5</sub> in the T5-words dataset (70.5°, p&lt;0.01). (<bold>F</bold>) Neural population activity for each speaking condition was projected into the top jPCA plane as in <xref ref-type="fig" rid="fig5">Figure 5A</xref>, for all six datasets. (<bold>G</bold>) The same statistical significance testing from <xref ref-type="fig" rid="fig5">Figure 5B</xref> was applied to all datasets to measure how well a rotatory dynamical system fit ensemble neural activity. (<bold>H</bold>) Rotatory neural population dynamics were not observed when the same jPCA analysis was applied on each dataset’s neural activity from 100 to 350 ms after the audio prompt. Rotatory dynamics goodness of fits are presented as in the previous panel.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Neural population dynamics when viewed across a range of reduced dimensionalities.</title><p>(<bold>A</bold>) Condition-invariant neural population dynamics during speech initiation are summarized as in <xref ref-type="fig" rid="fig4">Figure 4</xref> for the T5-words (top) and T8-words (bottom) datasets, but now varying the number of dPCA components used for dimensionality reduction. The eight dPCs results shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> are outlined with a dashed gray box. (<bold>B</bold>) Rotatory neural population dynamics during speaking are summarized as in <xref ref-type="fig" rid="fig5">Figure 5</xref> for the T5-words (top) and T8-words (bottom) datasets, but now varying the number of principal components used for the initial dimensionality reduction (after which the jPCA algorithm looks for rotatory planes). The six PCs results shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> are outlined.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig4-figsupp2-v2.tif"/></fig></fig-group><p>We attribute the difference in how condition-invariant the CIS<sub>1</sub> component was between the two participants to the much smaller speech task-related neural modulation recorded in participant T8 compared to in T5, as demonstrated in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref> and the lower classification accuracies of <xref ref-type="fig" rid="fig3">Figure 3</xref>. The practical consequence of T8’s substantially weaker speech-related modulation is that much more of the neural population activity that dimensionality reduction tries to summarize was not task-relevant (i.e. is ‘noise’ for the purpose of these analyses). This lower signal-to-noise ratio can also be appreciated in how the ‘elbow’ of T8’s cumulative neural variance explained by PCA or dPCA components (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A,B</xref>) occurs after fewer components and explains far less overall variance.</p><p>Lastly, we looked for rotatory population dynamics around the time of acoustic onset. <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows ensemble firing rates projected into the top jPCA plane (i.e. the subspace defined by jPC<sub>1</sub> and jPC<sub>2</sub>). In participant T5, all conditions’ neural state trajectories rotated in the same direction (similarly to <xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Pandarinath et al., 2015</xref>), and rotatory dynamics could explain substantial variance in how population activity evolved moment-by-moment during speaking. Application of a recent population dynamics hypothesis testing method (<xref ref-type="bibr" rid="bib29">Elsayed and Cunningham, 2017</xref>) revealed that this rotatory structure was significantly stronger than expected by chance in T5’s speaking data, but not in T8’s speaking data (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) or when this analysis was applied to neural activity following the audio prompt (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1H</xref>). As was the case for the condition-invariant dynamics, these results were also consistent across additional datasets (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E–H</xref>) and across the choice of how many PCA dimensions in which to look for rotatory dynamics (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2B</xref>). We again attribute the observed between-participants difference to T8’s smaller measured neural responses during speech, which likely reflect his older arrays’ lower signal quality. Consistent with this, T8’s BCI computer cursor control performance was also substantially worse than T5’s (<xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>). Other factors that could also have contributed to T8’s reduced speech-related neural activity include his tendency to speak quietly and with less clear enunciation (consistent with <xref ref-type="bibr" rid="bib42">Jiang et al., 2016</xref>), array placement differences, and differences in cortical maps between individuals (<xref ref-type="bibr" rid="bib31">Farrell et al., 2007</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Rotatory neural population dynamics during speech.</title><p>(<bold>A</bold>) The top six PCs of the trial-averaged firing rates from 150 ms before to 100 ms after acoustic onset in the T5-words and T8-words datasets were projected onto the first jPCA plane like in <xref ref-type="bibr" rid="bib21">Churchland et al. (2012)</xref>. This plane captures 38% of T5’s overall population firing rates variance, and rotatory dynamics fit the moment-by-moment neural state change with R<sup>2</sup> = 0.81 in this plane and 0.61 in the top 6 PCs. In T8, this plane captures 15% of neural variance, with a rotatory dynamics R<sup>2</sup> of only 0.32 in this plane and 0.15 in the top six PCs. (<bold>B</bold>) Statistical significance testing of rotatory neural dynamics during speaking. The blue vertical line shows the goodness of fit of explaining the evolution in the top six PC’s neural state from moment to moment using a rotatory dynamical system. The brown histograms show the distributions of this same measurement for 1000 neural population control surrogate datasets generated using the tensor maximum entropy method of <xref ref-type="bibr" rid="bib29">Elsayed and Cunningham (2017)</xref>. These shuffled datasets serve as null hypothesis distributions that have the same primary statistical structure (mean and covariance) as the original data across time, electrodes, and word conditions, but not the same higher order statistical structure (e.g. low-dimensional rotatory dynamics).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-46015-fig5-v2.tif"/></fig><p><xref ref-type="video" rid="video2">Videos 2</xref> and <xref ref-type="video" rid="video3">3</xref> show the temporal relationship between these two dynamical motifs – an initial condition-invariant neural state shift, followed by rotatory dynamics. Neural state rotations occurred after the condition invariant translation; by comparison, in <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref> the neural rotations also lagged the CIS shift, but in the monkey arm reaching data these rotations either partially overlapped with, or more immediately followed, the CIS shift. We note that existing models of how a condition-invariant signal ‘kicks’ dynamics into a different state space region where rotatory dynamics unfold (<xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Sussillo et al., 2015</xref>) do not require that the CIS and rotatory dynamics must be orthogonal, but in these data we did observed that the CIS<sub>1</sub> and jPCA dimensions were largely orthogonal (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>).</p><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-46015-video2.mp4"><label>Video 2.</label><caption><title>The progression of neural population activity during the prompted words task is summarized with dimensionality reduction chosen to highlight the condition-invariant ‘kick’ after the go cue, followed by rotatory population dynamics.</title><p>T5-words dataset neural state space trajectories are shown from 2.5 s before go cue to 2.0 s after go. Each trajectory corresponds to one word condition’s trial-averaged firing rates, aligned to the go cue. The neural states are projected into a three-dimensional space consisting of the CIS<sub>1</sub> dimension (as in <xref ref-type="fig" rid="fig4">Figure 4</xref>) and the first two jPC dimensions (similar to <xref ref-type="fig" rid="fig5">Figure 5</xref>, except that for this visualization we enforced that the jPC plane be orthogonal to the CIS<sub>1</sub>; see Materials and methods). The trajectories change color based on the task epoch: gray is before the audio prompt, blue is after the prompt, and then red-to-green is after the go cue, with conditions ordered as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption></media><media id="video3" mime-subtype="mp4" mimetype="video" xlink:href="elife-46015-video3.mp4"><label>Video 3.</label><caption><title>The same neural trajectories as <xref ref-type="video" rid="video2">Video 2</xref>, but aligned to acoustic on (AO), are shown from 3.5 s before AO to 1.0 s after AO.</title></caption></media></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>There are three main findings from this study. First, these data suggest that ‘hand knob’ motor cortex, an area not previously known to be active during speaking (<xref ref-type="bibr" rid="bib11">Breshears et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Dichter et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Leuthardt et al., 2011</xref>; <xref ref-type="bibr" rid="bib51">Lotte et al., 2015</xref>), may in fact participate, or at least receive correlates of, neural computations underlying speech production. Speech-related single-neuron modulation might have been missed by previous studies due to the coarser resolution of ECoG (<xref ref-type="bibr" rid="bib17">Chan et al., 2014</xref>). If this finding holds true in the wider population, this would underscore that the familiar ‘motor homunculus’ (<xref ref-type="bibr" rid="bib69">Penfield and Boldrey, 1937</xref>) is overly simplistic. It is generally recognized that motor cortex does not rigidly follow a sequential point-to-point somatotopy, and indeed, Penfield and colleagues were aware of this and intended for their diagram to be a simplified summary of results showing partially overlapping motor fields that also varied substantially across individuals (<xref ref-type="bibr" rid="bib15">Catani, 2017</xref>). However, the patchy mosaicism amongst nearby body parts in the current view of precentral gyrus organization still features a dorsal-to-ventral progression and separation of the major body regions (leg, arm, head) (<xref ref-type="bibr" rid="bib31">Farrell et al., 2007</xref>; <xref ref-type="bibr" rid="bib73">Schieber, 2001</xref>).</p><p>The presence of neurons responding to mouth and tongue movements in the dorsal ‘arm and hand’ area of motor cortex indicates that sensorimotor maps for different body parts are even more widespread and overlapping than previously thought. Given our previous finding that activity from these same arrays encodes intended arm and hand movements (<xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>), these observations are consistent with the hypothesis that the systems for speech and manual gestures are interlocked (<xref ref-type="bibr" rid="bib36">Gentilucci and Stefani, 2012</xref>; <xref ref-type="bibr" rid="bib71">Rizzolatti and Arbib, 1998</xref>; <xref ref-type="bibr" rid="bib84">Vainio et al., 2013</xref>). However, emerging work from our group showing that neurons in this area also modulate during attempted movements of the ﻿neck and legs (<xref ref-type="bibr" rid="bib88">Willett et al., 2019</xref>) suggests that much of the body is represented (to varying strengths) in dorsal motor cortex. Thus, the observed neural overlap between hand and speech articulators may be a consequence of distributed whole-body coding, rather than a privileged speech-manual linkage.</p><p>Our data suggest that the observed neural activity reflects movements of the speech articulators (the tongue, lips, jaw, and larynx): modulation was greater during speaking than after hearing the prompt; the same neural population modulated during non-speech orofacial movements; and in T5, the neural correlates of producing different phonemes grouped according to these phonemes’ place of articulation. We also found that firing rates showed modest correlation with T5’s unattended and instructed breathing, which invites the question of how this activity relates to the precise control of breathing necessary for speaking and whether breath-related activity differs depending on behavioral context. A deeper understanding of how motor cortical spiking activity relates to complex speaking behavior will require future work connecting it to continuous articulatory (<xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Conant et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Mugler et al., 2018</xref>) and respiratory kinematics and, ideally, the underlying muscle activations.</p><p>An important unanswered question, however, is to what extent these results were potentially influenced by cortical remapping due to tetraplegia. While we cannot rule this out, we believe that remapping of face representation to the hand knob area is unlikely. Despite these participants’ many years of paralysis, the sites we recorded from still strongly modulate during attempted hand and arm movements (<xref ref-type="bibr" rid="bib1">Ajiboye et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Brandman et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>). We also verified in participant T5 that modulation during attempted arm movements was stronger than during speech production. Our ongoing work also indicates that this area modulates during attempts to move other body parts (e.g. the leg) which, like the arm, are also paralyzed (<xref ref-type="bibr" rid="bib88">Willett et al., 2019</xref>). Taken together, these results are inconsistent with this area being ‘taken over’ by functions related to the participants’ remaining capability to make orofacial movements. Furthermore, motor cortical remapping following arm amputation was recently shown to be smaller than previously thought (<xref ref-type="bibr" rid="bib87">Wesselink et al., 2019</xref>), and in particular much smaller than what would be needed to move lip representations to hand cortex (<xref ref-type="bibr" rid="bib52">Makin et al., 2015</xref>). On the sensory side, emerging evidence suggests that cortical reorganization following injury in adults is more limited than previously thought (<xref ref-type="bibr" rid="bib54">Makin and Bensmaia, 2017</xref>), and a recent microstimulation study in the hand somatosensory cortex of a person with tetraplegia did not find functional reorganization (<xref ref-type="bibr" rid="bib32">Flesher et al., 2016</xref>). While these threads of evidence argue against remapping, definitively resolving this ambiguity would require intracortical recording from this eloquent brain area in able-bodied people.</p><p>Assuming that these results are not due to injury-related remapping, we are left with the question of <italic>why</italic> this speech-related activity is found in dorsal ‘arm and hand’ motor cortex. Speech is spared following lesions in this area (<xref ref-type="bibr" rid="bib19">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bib82">Tei, 1999</xref>), indicating that it is not necessary for speech production. Nonetheless, it is possible that dorsal motor cortex plays some supporting role in speaking, perhaps contributing to more demanding speaking tasks, or that this activity reflects speech efference copy for coordinating orofacial and upper extremity movements. This would be in line with theoretical arguments that high dimensional representations resulting from mixed selectivity – in this case, both within major body regions (a given neuron being tuned for multiple arm movements or for multiple orofacial movements) and across major body regions (neurons being tuned for both arm and face movements) – enable more complex computations (<xref ref-type="bibr" rid="bib33">Fusi et al., 2016</xref>) such as coordinating movements across the body. We anticipate that it will require substantial future work to understand why speech-related activity co-occurs in the same motor cortical area as arm and hand movement activity, but that this line of inquiry may reveal important principles of how sensorimotor control is distributed across the brain (<xref ref-type="bibr" rid="bib63">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Stringer et al., 2019</xref>).</p><p>Our second main finding is that, based on offline decoding results, intracortical recordings show promise as signal sources for BCIs to restore speech to people with some forms of anarthria. Decoding the neural correlates of attempted speech production (<xref ref-type="bibr" rid="bib13">Brumberg et al., 2011</xref>) into audible sounds or text may be more desirable than approaches that decode covert internal speech (<xref ref-type="bibr" rid="bib48">Leuthardt et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Martin et al., 2016</xref>) or more abstract elements of language (<xref ref-type="bibr" rid="bib16">Chan et al., 2011</xref>; <xref ref-type="bibr" rid="bib89">Yang et al., 2017</xref>) because decoding attempted movements leverages existing neural machinery that separates internal monologue and speech preparation from intentional speaking. The present results compare favorably to previously published decoding accuracies using ECoG (<xref ref-type="bibr" rid="bib61">Mugler et al., 2014</xref>; <xref ref-type="bibr" rid="bib70">Ramsey et al., 2018</xref>) despite our dorsal recording locations likely being suboptimal for decoding speech. Multi-electrode arrays placed in ventral motor cortex would be expected to yield even better decoding accuracies. Furthermore, recent order-of-magnitude advances in the number of recording sites on intracortical probes (<xref ref-type="bibr" rid="bib43">Jun et al., 2017</xref>) point to a path that stretches far forward in terms of scaling the number of distinct sources of information (neurons) for speech BCIs.</p><p>That said, these results are only a first step in establishing the feasibility of speech BCIs using intracortical electrode arrays. We decoded amongst a limited set of discrete syllables and words in participants who are able to speak; future studies will be needed to assess how well intracortical signals can be used to discriminate between a wider set of phonemes (<xref ref-type="bibr" rid="bib13">Brumberg et al., 2011</xref>; <xref ref-type="bibr" rid="bib61">Mugler et al., 2014</xref>), in the absence of overt speech (<xref ref-type="bibr" rid="bib13">Brumberg et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Martin et al., 2016</xref>), and to synthesize continuous speech (<xref ref-type="bibr" rid="bib2">Akbari et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Anumanchipalli et al., 2019</xref>; <xref ref-type="bibr" rid="bib53">Makin et al., 2019</xref>). We also observed worse decoding performance in participant T8, highlighting the need for future studies in additional participants to sample the distribution of how much speech-related neural modulation can be expected, and what speech BCI performance these signals can support.</p><p>Our third main finding is that two motor cortical population dynamical motifs present during arm movements were also significant features of speech activity. We observed a large condition-invariant change at movement initiation in both participants, and rotatory dynamics during movement generation in the one of two participants whose arrays recorded substantially more modulation. We speculate that these neural state rotations are well-suited for generating descending muscle commands driving the out-and-back articulator movements that form the kinematic building blocks of speech (<xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Mugler et al., 2018</xref>). The presence of these dynamics during both reaching and speaking could indicate a conserved computational mechanism that is ubiquitously deployed across multiple behaviors to shift the circuit dynamics from withholding movement to generating the appropriate muscle commands from an oscillatory basis set. Testing and refining this hypothesis calls for examining whether these two dynamical motifs are present across an even wider range of behaviors and body parts. For instance, there is emerging evidence that rotatory dynamics may be absent in movements with a greater role of sensory feedback, such as hand grasping (<xref ref-type="bibr" rid="bib78">Suresh et al., 2019</xref>).</p><p>This interpretation should also be tempered by the major unresolved question of whether these dynamics in dorsal motor cortex play a causal role in speaking and/or echo similar dynamics in other areas, such as ventral motor cortex, which are more directly involved in speech (<xref ref-type="bibr" rid="bib7">Bouchard et al., 2013</xref>). An alternative interpretation is that if dorsal motor cortex merely receives an efference copy or ‘coordination’ signal about speech articulator movements, its dynamics may resemble those during arm reaching because this is what the inherent properties of the local circuit are set up to generate – even if in the speech case, this activity is not helping construct muscle activities. Testing these hypotheses will require future research involving recording from the speech articulator muscles (analogous to recording from arm muscles in <xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>), causally stimulating the circuit (<xref ref-type="bibr" rid="bib28">Dichter et al., 2018</xref>), and examining whether these neural ensemble dynamical motifs are present during speech production in ventral (speech) motor cortex.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>The two participants in this study were enrolled in the BrainGate2 Neural Interface System pilot clinical trial (<ext-link ext-link-type="uri" xlink:href="https://clinicaltrials.gov/">ClinicalTrials.gov</ext-link> Identifier: NCT00912041). The overall purpose of the study is to obtain preliminary safety information and demonstrate proof of principle that an intracortical brain-computer interface can enable people with tetraplegia to communicate and control external devices. Permission for the study was granted by the U.S. Food and Drug Administration under an Investigational Device Exemption (﻿Caution: Investigational device. Limited by federal law to investigational use). The study was also approved by the Institutional Review Boards of Stanford University Medical Center (﻿protocol #20804), Brown University (#﻿0809992560), University Hospitals of Cleveland Medical Center (#04-12-17), Partners HealthCare and Massachusetts General Hospital ﻿(#2011P001036), and the Providence VA Medical Center ﻿(#2011–009). Both participants gave informed consent to the study and publications resulting from the research, including consent to publish photographs and audiovisual recordings of them.</p><p>Participant ‘T5’ (male, right-handed, 64 years old at the time of the study) was diagnosed with C4 AIS-C spinal cord injury 10 years prior to these research sessions. ﻿He retained the ability to weakly flex his left elbow and fingers and some slight and inconsistent residual movement of both the upper and lower extremities. T5 was able to speak normally and converse naturally without hearing assistance, but had some trouble hearing from his left ear.</p><p>Participant ‘T8’ (male, right-handed, 56 years old at the time of the study) was diagnosed with ﻿C4 AIS-A spinal cord injury 11 years prior to these sessions. He retained restricted and non-functional voluntary shoulder girdle motion on both sides, and non-functional voluntary finger extension on his left side. He had no sensation below the shoulder. T8 was able to speak normally and converse naturally with the assistance of hearing aids in both his ears.</p></sec><sec id="s4-2"><title>Prompted speaking tasks</title><p>Participants performed a syllables task consisting of discrete trials in which they spoke out loud one of 10 different phonemes or consonant-vowel syllables in response to an auditory prompt. These prompts were <italic>i</italic> (as in ‘beet’); <italic>ae</italic> (as in ‘bat’); <italic>a</italic> (as in ‘bot’); <italic>u</italic> (as in ‘boot’); <italic>ba; da; ga; sh</italic> (as in the start of ‘shot’), and the unvoiced <italic>k</italic> and <italic>p</italic>. All pronunciations were American English. <xref ref-type="video" rid="video1">Video 1</xref> provides a continuous audio recording of one set of each type of syllables task trial.</p><p>Participants sat comfortably in a chair facing a microphone in a quiet room. They were instructed to refrain from attempting movements or speaking during trials except when prompted to speak by a custom experiment control software written in MATLAB (The Mathworks). During trials, they were also asked to fixate on the same object in front of them. Each trial began with two beeps to alert the participant that the trial was starting. Approximately 1 s after the start of the second beep, a pre-recorded syllable prompt was played via computer speakers. Two clicks played ~2 s after the start of the prompt served as the go cue that instructed the participant to speak back the prompted sound. The next trial started 2.8 s after the start of the second click. There was also an eleventh ‘silent’ condition which was identical to the spoken syllables trials, except that instead of playing a syllable prompt, the speakers played a nearly-silent audio file consisting of ambient background noise recorded in the same environment as the syllable prompts. The participants had been previously instructed not to say anything in response to this silent prompt.</p><p>The task was performed in blocks consisting of 10 trial sets. Each set contained 11 trials: one trial of each syllable, plus silence, presented in a randomized order. After the task was explained to each participant, he was given time to practice a few sets of the task until he indicated that he was ready to begin data collection. At the end of each set, we paused the task until the participant indicated that he was ready to continue. These inter-set pauses typically lasted less than 10 s. Participants performed three consecutive blocks of the task during a research session, with longer pauses of several minutes between blocks during which we encouraged the participant to rest, adjust his posture for comfort, and take a drink of water.</p><p>Both the audio prompts played by the experiment control computer, and the participant’s voice, were recorded by the microphone (Shure SM-58). This audio signal was recorded via the analog input port of the electrophysiology data acquisition system and digitized at 30 ksps together with the raw neural data (see Neural Recording section). Each trial’s acoustic onset time (AO) was manually determined by visual and auditory inspection of the recorded audio data. During this review, we also excluded infrequent trials where the participant spoke at the wrong time or when the trial was interrupted (for example, if a caregiver entered the room). Isolated sounds can be difficult to discriminate, and our participants sometimes misheard a syllable prompt as a phonetically similar prompt. In particular, T5 misheard the majority of <italic>da</italic> prompts as <italic>ga</italic> (or occasionally as <italic>ba</italic>). Both participants made a few other substitutions between similar syllables. In this study, we were interested in the neural correlates of preparing and then generating speech, which should reflect the syllable that the participant perceived. We therefore labeled these misheard trials based on the spoken, rather than prompted, syllable for subsequent analyses. This left an insufficient number of T5 <italic>da</italic> trials for subsequent neural analyses; thus, there are 11 conditions shown in T8’s <xref ref-type="fig" rid="fig1">Figure 1</xref> firing rate plots and <xref ref-type="fig" rid="fig3">Figure 3</xref> confusion matrices, but only 10 conditions for T5. The number of trials analyzed for each participant, after excluding trials and re-labeling misheard trials as described above, were: silent (30 trials for T5, 30 trials for T8); <italic>i</italic> (30, 28); <italic>u</italic> (30, 31); <italic>ae</italic> (28, 30); <italic>a</italic> (30, 30); <italic>ba</italic> (31, 29); <italic>ga</italic> (50, 34); <italic>da</italic> (0, 27); <italic>k</italic> (30, 27); p (30, 33); <italic>sh</italic> (30, 30). We refer to these datasets as ‘T5-syllables’ and ‘T8-syllables’.</p><p>Participants also performed a words task which was identical to the syllables task except that they heard and repeated back one of 10 short words, rather than syllables, in response to the auditory prompt. Each participant performed three blocks of ten repetitions of each word during one research session. We refer to these datasets as ‘T5-words’ and ‘T8-words’. Two consecutive trials were excluded from the T8-words dataset because of a large electrical noise artifact across almost all electrodes. The specific words, and the number of trials analyzed for each participant, were: ‘beet’ (30 T5 trials, 29 T8 trials); ‘bat’ (30, 29); ‘bot’ (30, 28); ‘boot’ (30, 30); ‘dot’ (30, 29); ‘got’ (29, 29); ‘shot’ (29, 28); ‘keep’ (30, 30); ‘seal’ (30, 30); ‘more’ (30, 30). As with the syllables task, there was also a silent condition (30 T5 trials, 30 T8 trials). During two additional research sessions (as part of a follow-up study), participant T5 performed the words task with only five of the 10 words. The conditions and trial counts in these two replication datasets, which we refer to as ‘T5-5words-A’ and ‘T5-5words-B’, were: ‘seal’ (33 trials in T5-5-words-A, 34 trials in T5-5words-B); ‘shot’ (34, 34); ‘more’ (33, 34); ‘bat’ (34, 33); beet’ (34, 34); and a silent condition (34, 34).</p><p>Silent condition trials were assigned a ‘faux AO’ so that neural data from comparable epochs of silent and spoken trials could be visualized and analyzed (for example, for generating trial-averaged, AO-aligned firing rates in <xref ref-type="fig" rid="fig1">Figure 1</xref> or for decoding silent trials’ neural activity in <xref ref-type="fig" rid="fig3">Figure 3</xref>). Specifically, each silent trial’s AO was set to equal the mean AO (relative to the go cue) for all the spoken syllables or words during the same block.</p></sec><sec id="s4-3"><title>Orofacial movement task</title><p>Participants also performed an orofacial movement task with a similar trial structure as the syllables and words tasks. Seven different movement conditions were instructed with auditory prompts: ‘mouth open’, ‘lips forward’, ‘lips back’, ‘tongue right’, ‘tongue down’, ‘tongue up’, and ‘tongue left’. An additional ‘stay still’ condition was analogous to the silent condition of the syllables and words tasks. Prior to the first block of the orofacial task, a researcher explained the prompts to the participant, demonstrated the movements, and ran the participant through a few practice sets. Due to clinical trial protocols, we did not collect kinematic tracking data such as ﻿electromagnetic midsagittal articulography (<xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>) or ultrasound recordings (<xref ref-type="bibr" rid="bib24">Conant et al., 2018</xref>). A video recording of the participants’ faces (without markers) did allow the researchers to confirm that the participants were making the instructed movement with acceptable timing precision. Given this limitation, we limited our use of these data to broadly testing for neural responses during orofacial movements, rather than quantifying precise moment-by-moment relationships between neural activity and kinematics.</p><p>Similar to the syllables and words task, an orofacial movement trial began with two ready beeps, after which the computer speaker played a movement prompt (e.g. ‘lips forward’). This was followed by the pair of go clicks; the participants were previously informed that they should begin moving after the second click. Approximately 1.9 s after the go cue click, the experiment control system played the verbal command ‘return’, which instructed the participant to return to a neutral orofacial posture (e.g. close the mouth after ‘mouth open’, move the tongue left after ‘tongue right’). The trial ended ~1.9 s after the start of ‘return’. The purpose of using a return cue was so that there was a known epoch after the movement go cue during which we knew that the participant was not yet returning. The return cue also provided the participant with dedicated time to return to a neutral orofacial position, so that all trials would start from roughly the same posture. For T8, the ‘return’ instruction was immediately followed by a go click. However, we observed that T8 started the return movement upon hearing ‘return’ rather than waiting for the go click. We therefore removed the return go click prior to T5’s research sessions, and instead instructed T5 to start the return movement when he heard ‘return’. In the present study, we did not examine the return portion of the orofacial movement task.</p><p>Each participant’s orofacial movements and syllables datasets were collected on the same day during the same research session; three blocks of the orofacial movement task immediately followed three blocks of the syllables task. We will refer to these orofacial movements task datasets as ‘T5-movements’ and ‘T8-movements’. No trials were excluded from these datasets; thus, there were 30 trials of each condition for each participant.</p></sec><sec id="s4-4"><title>Many words task</title><p>During an additional research session, participant T5 performed a many words task in which he spoke 420 unique words (from <xref ref-type="bibr" rid="bib4">Angrick et al., 2019</xref>) designed to broadly sample American English phonemes. These words were visually prompted, with one word appearing per trial. Each trial started with an instruction period in which a red square appeared in the center of a computer screen facing the participant. White text above the square instructed what word the participant should say once given a go cue (e.g. ‘Prepare: ‘Dog’’). This instruction period lasted 1.2 to 1.8 s (mean 1.4 s, exponential distribution) after which the square turned green, the text changed to ‘Go’, and an audible beep was played. This served as the go cue for T5 to speak out loud the instructed word. A second beep occurred 1.5 s later, which marked the end of the trial. The next trial began 1 s later. The 420 words were divided into four sets, with each set spoken during a continuous block of trials with short breaks between blocks. Each word set was repeated three times during this research session, with a given set’s words appearing in a different random order during each block. We call this the ‘T5-phonemes’ dataset.</p></sec><sec id="s4-5"><title>Breath measurement</title><p>T5’s breath-related abdomen movements were measured with a piezo respiratory belt transducer (model MLT1132, ADIntruments). The stretch sensor was wrapped around his abdomen at the point where it maximally expanded during breathing. Analog voltage signals from the belt were input to the neural signal processor via one of its analog input channels. These data were digitized at 30 ksps along with the neural data. Our goal was to test whether there is breath-correlated neural activity during ‘unattended’ breathing (i.e. natural ‘background’ breathing, when the participant was not consciously attending to his breath) and during consciously attended ‘instructed’ breathing. Both of the unattended and instructed conditions were collected during the same research session, and we refer to this as the ‘T5-breathing’ dataset.</p><p>For the unattended breathing condition, we recorded neural and breath proxy measurements while T5 performed a BCI computer cursor task as part of a different study, and during an interval where he was resting quietly after completing the BCI task. For the instructed breathing task, we recorded neural and breath proxy measurements while T5 performed a cued breathing task that followed a similar structure as the many words task described in the previous section. On each trial, the on-screen instruction text was either ‘Prepare: Breathe in’ or ‘Prepare: Breathe out’. The order of these two trial types was randomized within consecutive two-trial sets, such that breaths in and breaths out were counterbalanced and no more than two out breaths or two in breaths could be prompted in a row. After a random delay of 1.2 to 1.6 s (mean 1.4 s, exponential distribution), the go cue instructed the participant to breathe in or out according to the instruction. After 1.5 s, an audible beep and the on-screen text changing to ‘Return’ instructed the participant to return to a neural lung inflation position. ‘Return’ stayed on screen for 1.5 s, after which the inter-trial interval was 1 s. A block consisted of 12 trials, after which the participant was given a chance to take a break, relax, and breathe naturally before the next block. The participant reported that this task was comfortable and that he was able to match his breaths to the instructions without difficulty.</p></sec><sec id="s4-6"><title>Movement comparisons task</title><p>The purpose of this task, which was performed on a separate day from the other datasets, was to compare the neural modulation when making orofacial movements and speaking, versus when attempting to make arm and hand movements. The task had a similar visually instructed structure to the instructed breathing task. During the instructed delay period, text displayed the upcoming movement, for example, ‘Prepare: Say Ba’, or ‘Prepare: Open Hand’. There was also a ‘Prepare: Do Nothing’ instruction, which otherwise had the same trial structure as the instructed movements. After a random delay period of between 1400 and 1800 ms, the go cue appeared. During this epoch, T5 attempted to make the instructed movement as best as he could. This resulted in complete movements for all the orofacial and speaking movements and ‘shoulder shrug’, partial movements for some of the arm movements (e.g. ‘flex elbow in’), and no overt movement for the other arm movements (e.g. ‘close hand’, ‘thumb up’). We analyzed neural data from 200 ms to 600 ms after the go cue. We note that insofar as there was somatosensory and proprioceptive feedback only during the actualized movements, this would be expected to increase the observed neural modulation to orofacial movements and speaking, and decrease the modulation to attempted arm and hand movements. The go cue stayed on for 1500 ms. This was followed by a return period in which the text changed to ‘Return’; during this epoch, the participant was instructed to return his body to a neutral posture. Thirty-two trials were collected for each movement type. We refer to this as the ‘T5-comparisons’ dataset.</p></sec><sec id="s4-7"><title>Neural recording</title><p>Both participants had two 96-electrode Utah arrays (1.5 mm electrode length, Blackrock Microsystems) neurosurgically placed in dorsal ‘hand knob’ area of the left (motor dominant) hemisphere’s motor cortex. Surgical targeting was stereotactically guided based on prior functional and structural imaging (<xref ref-type="bibr" rid="bib90">Yousry et al., 1997</xref>), and subsequently confirmed by review of intra-operative photographs. T5 and T8 had arrays placed 14 and 34 months, respectively, prior to the present study’s prompted words, syllables, and orofacial movements tasks. The T5-breathing and T5-comparisons datasets were recorded 26 months after array placement, the T5-5words-A and T5-5words-B datasets were recorded 28 months after array placement, and the T5-phonemes dataset was recorded 29 months after array placement. Arrays were placed in areas anticipated to have arm movement-related activity because two goals of the clinical trial are 1) testing the feasibility of intracortical BCI-based communication using point-and-click keyboards and 2) restoration of reach and grasp function via control of a robotic arm or functional electrical stimulation. We note that these implant sites are distinct from the closest known speech area, which is the dorsal laryngeal motor cortex (<xref ref-type="bibr" rid="bib7">Bouchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Dichter et al., 2018</xref>). In this study, we looked for neural correlates of speaking in dorsal motor cortex. To help contextualize the results, here we summarize the other behaviors associated with modulation of the neural activity recorded by these same arrays. Our previous studies have reported that T5 and T8 controlled BCI computer cursors by attempting movements of their arm and hand (<xref ref-type="bibr" rid="bib9">Brandman et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>). T8 was also able to use intended arm movements to command movements of his own paralyzed arm via functional electrical stimulation (<xref ref-type="bibr" rid="bib1">Ajiboye et al., 2017</xref>). We also recorded movement task outcome error signals from T5’s arrays; these signals indicated whether the participant succeeded or failed at acquiring a target using a BCI-controlled cursor (<xref ref-type="bibr" rid="bib30">Even-Chen et al., 2018</xref>).</p><p>Neural signals were recorded from the arrays using the NeuroPort system (Blackrock Microsystems). Voltage was measured between each of the 96 electrodes’ uninsulated tips and that array’s reference wire. Wire bundles ran from each array to cranially-implanted connector pedestals. During research sessions, a ‘patient cable’ with a unity gain pre-amplifier was connected to each array’s corresponding pedestal and carried signals to an isolated unity gain front-end amplifier. These signals were analog filtered from 0.3 Hz to 7.5 kHz, digitized at 30 kHz (250 nV resolution), and sent to the neural signal processor via fiber-optic link. As mentioned earlier, amplified analog voltage data from the microphone were input to the neural signal processor and were digitized time-locked with the neural signals. All these digitized data were sent over a local network to a connected PC where they were recorded to disk for subsequent analysis.</p><p>The naming scheme for neurons or electrodes in figures is &lt;participant&gt;_&lt;array #&gt;.&lt;electrode #&gt;. For example, 'neuron T5_2.4' in <xref ref-type="fig" rid="fig1">Figure 1</xref> refers to a participant T5 neuron identified on the second array (which is the more medial of each participant’s two arrays) on electrode #4 (according to the manufacturer’s electrode numbering scheme).</p><p>For both participants, we did not observe major differences between the two arrays, and we confirmed that the neural population analyses results (ensemble modulation to speech/movements/breathing, phoneme neural correlate similarities, speech decoding, condition-invariant and rotatory population dynamics) were similar when data from each array were analyzed separately. We therefore pool together data from both arrays in all the presented results.</p></sec><sec id="s4-8"><title>Neural signal processing</title><p>Neuronal action potentials (spikes) were detected as follows. We first applied a common average re-referencing to each electrode within an array by subtracting, at each time sample, the mean voltage across all electrodes on that array. These voltage signals were then filtered with a 250 Hz asymmetric FIR high-pass filter designed to extract spike activity from this type of array (<xref ref-type="bibr" rid="bib57">Masse et al., 2014</xref>). To measure single unit activity (SUA), time-varying voltages were manually ‘spike sorted’ by an experienced neurophysiologist using Plexon Offline Spike Sorter v3. This process identified action potentials belonging to putative individual neurons amongst the high amplitude voltage deviation events. Occasionally, the same action potential can be recorded on multiple electrodes (this could happen if a neuron is very large, if an axon passes multiple electrodes, or if there is some electrical cross-talk in the recording hardware). To prevent creating duplicate single neuron units, we excluded ‘cross-talk units’ if their spike time series (using 1 ms binning) had greater than 0.5 correlation with another unit’s. When this happened, we kept the unit with the better spike sorting isolation. Unless otherwise stated, time-varying firing rate plots, also known as peristimulus time histograms (such as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>) were constructed by smoothing spike trains with a 25 ms s.d. Gaussian kernel and averaging continuous-valued firing rates across trials of the same behavioral condition.</p><p>Spike sorting allows us to make statements about the properties of individual motor cortical neurons (for example, how many syllables they modulate to, as in <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4B</xref>). However, a limitation of spike sorting is that action potential event ‘clusters’ with insufficient isolation from other clusters are discarded. For chronic multielectrode array recordings, this can mean that activity recorded from the majority of electrodes is not analyzed, despite these neural signals having a strong relationship with the behavior of interest. This problem is particularly acute in human neuroscience, where replacing arrays, or using newer methods that provide a higher SUA yield (for example high-density probes or optical imaging), is not currently possible. Relaxing the constraint that action potential events must be unambiguously from the same neuron and instead analyzing voltage threshold crossings (TCs) is an effective way to substantially increase the information yield of chronic electrode arrays. In this study, we examined TCs in a number of analyses. Decoding TCs or other non-SUA signals has become standard practice in the intracortical BCI field (e.g. <xref ref-type="bibr" rid="bib1">Ajiboye et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Brandman et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Collinger et al., 2013</xref>; <xref ref-type="bibr" rid="bib30">Even-Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>). This method also provides information about the dynamics of the neural state (i.e. it can be used to make scientific statements about ensemble activity under many conditions) despite combining spikes that may arise from one or more neurons; we provide empirical and theoretical justifications in <xref ref-type="bibr" rid="bib83">Trautmann et al. (2019)</xref>. In the present study, when we refer to an ‘electrode’s’ firing rate, we mean TCs recorded from that electrode. When we refer to a neuron’s firing rate, we mean sorted single unit activity. <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> shows example TCs firing rates, including from the same electrodes that the example neurons in <xref ref-type="fig" rid="fig1">Figure 1</xref> were sorted from.</p><p>A threshold of −4.5 × root mean square (RMS) voltage was used for all analyses and visualizations except for the t-SNE visualization and decoding analyses shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. This threshold choice is somewhat arbitrary but is conservative; it accepts large voltage deviations indicative of action potentials from one or a few neurons near the electrode tip. For the <xref ref-type="fig" rid="fig3">Figure 3</xref> analyses, we used a more relaxed threshold of −3.5 × RMS because we found that this led to slightly better classification performance in a separate pilot dataset (consisting of T5 speaking five words and syllables, collected a month prior to the datasets reported here) which we used for choosing hyperparameters. The better performance of a less restrictive voltage threshold is consistent with collecting more information by accepting spikes from a potentially larger pool of neurons (<xref ref-type="bibr" rid="bib64">Oby et al., 2016</xref>). This trade-off was acceptable because for these engineering-minded decoding analyses, we were less concerned about the possibility of missing tuning selectivity or fast firing rate details due to combining spikes from more neurons.</p><p>Electrodes with TCs firing rates of less than 1 Hz (at a −4.5 × RMS threshold) were considered non-functioning and were excluded from analyses unless there was well-isolated SUA on the electrode. This electrode exclusion applied to both spikes and the local field potential signal described below. Electrodes having TCs time series with greater than 0.5 correlation with another electrode’s were marked for cross-talk de-duplication. To determine which electrode to keep, we chose the one that had the fewest spikes co-occurring (1 ms bins) with the other electrode(s)’ (i.e. we kept the electrode with putatively more unique information).</p><p>For the neural decoding analyses (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we also extracted a high-frequency local field potential (HLFP) feature from each electrode by taking the power of the voltage after filtering from 125 to 5000 Hz (third-order bandpass Butterworth causal filtering forward in time). HLFP is believed to contain substantial power from action potentials (<xref ref-type="bibr" rid="bib86">Waldert et al., 2013</xref>); we view this feature as capturing spiking ‘hash’, that is multiunit activity local to the electrode with contributions from smaller-amplitude and more distant action potentials than TCs. Our previous study found that this signal is highly informative about hand movement intentions and is useful for real-time BCI applications (<xref ref-type="bibr" rid="bib66">Pandarinath et al., 2017</xref>). This feature has some similarities to the ‘high gamma’ activity examined by ECoG studies; the definition of high gamma varies in exact frequency from study to study, but generally has a lower cutoff between 65 and 85 Hz and an upper cutoff between 125 and 250 Hz (<xref ref-type="bibr" rid="bib7">Bouchard et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Chartier et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Cheung et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Dichter et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Martin et al., 2014</xref>; <xref ref-type="bibr" rid="bib61">Mugler et al., 2014</xref>; <xref ref-type="bibr" rid="bib70">Ramsey et al., 2018</xref>). However, the intracortical HLFP in this study should not be viewed as being the exact same as ECoG high gamma activity due to differences in electrode location, electrode geometry, and HLFP’s higher frequency range.</p></sec><sec id="s4-9"><title>Task-related neural modulation</title><p>To quantify which electrodes’ spiking activity changed during speaking (<xref ref-type="fig" rid="fig1">Figure 1B</xref> insets, <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>), we calculated each electrode’s mean firing rate from 0.5 s before to 0.5 s after AO, yielding one datum per electrode, per trial. For each syllable, a rank-sum test was then used to determine whether there was a significant change in the distribution of single-trial firing rates when speaking the syllable compared to the silent condition (p&lt;0.05, Bonferroni corrected for the number of syllables). To identify which electrodes responded to orofacial movements (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) we performed a similar analysis, except that the analysis epoch was from 0.5 s before to 0.5 s after the go cue. This epoch captures strong modulation, as can be seen by the example firing rate plots in <xref ref-type="fig" rid="fig2">Figure 2</xref>. We note that firing rate changes preceding the go cue indicate either substantial movement preparation activity, or that the participants were ‘jumping the gun’ and started moving in anticipation of the go cue; either way, this response indicates modulation related to making orofacial movements. In lieu of a silent condition, the movement conditions’ firing rate distributions were compared to that of the ‘stay still’ condition. The same methods were used to quantify which single neurons’ activities changed during speaking or orofacial movements; for this, we analyzed SUA rather than electrodes’ −4.5 × RMS TCs.</p></sec><sec id="s4-10"><title>Neural population modulation</title><p>To measure the differences in neural modulation across the recorded population following the audio prompt and following the go cue (‘population modulation’ in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref>), at each time point (aligned to either the audio prompt or the go cue) we quantified the differences between the firing rate vector for a given spoken condition <bold>y</bold><sub>speak</sub> (for example, the vector of firing rates across the <italic>ga</italic> syllable trials, where each element of the vector is the firing rate for one electrode) and <bold>y</bold><sub>silent</sub>, the firing rate vector for the silent condition. Importantly, however, we did not simply use ||<bold>y</bold><sub>speak</sub> — <bold>y</bold><sub>silent</sub>||, the Euclidean norm of the vector difference between these two conditions’ trial-averaged firing rates. The problem with that approach is that a vector norm always yields a non-negative value, meaning that if it is used to measure neural activity differences, the metric will be upwardly biased: it will return a positive value instead of 0 even when population firing rates for the two conditions are essentially the same. This is because estimates of firing rates for two sets of trials, even if they are drawn from the same underlying distribution (i.e. from the same behavioral context) will inevitably differ, even just slightly, resulting in a positive vector difference norm. This problem becomes worse when dealing with lower trial counts and low firing rates, and makes it difficult to distinguish weak population modulation from noise.</p><p>To avoid this issue and better estimate neural population activity differences, we used a cross-validated variant of the vector difference norm; we will refer to this metric as the ‘neural distance’. For <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> trials from condition 1 (for example, saying <italic>ga</italic>) and <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> trials from condition 2 (for example, silent trials), we calculate a less biased estimate of the squared vector norm of the difference in the two conditions’ mean firing rates using:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>which leaves one trial out from each condition when calculating the differences in means. Critically, the dot product is taken between firing rates computed from fully non-overlapping sets of trials, and can be negative. To convert this to a signed distance more analogous to a Euclidean vector norm, we define the final neural distance metric as <inline-formula><mml:math id="inf3"><mml:mi mathvariant="normal">d</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo><mml:msqrt><mml:mfenced close="|" open="|" separators="|"><mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mfenced></mml:msqrt></mml:math></inline-formula>.</p><p>This cross-validated neural distance has units of Hz; much like with a standard Euclidean vector norm, having more electrodes, and these electrodes having larger firing rate differences between the two conditions, will both result in larger overall distances. Unlike a Euclidean vector norm, our population neural distance metric can produce negative values. This is required for the metric to be unbiased and should be interpreted as evidence that the true distance between the two distributions’ population firing rates is near zero. A benefit of allowing negative values is that time-averaging across an epoch of essentially no underlying firing rate differences will give a mean distance close to zero. The derivation of this metric is described in detail in <xref ref-type="bibr" rid="bib88">Willett et al. (2019)</xref>, and a software implementation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/fwillett/cvVectorStats">https://github.com/fwillett/cvVectorStats</ext-link>.</p><p>For statistical testing, we compared the time-average of this neural distance across two comparison epochs: a prompt epoch (0 to 1 s after the audio prompt) and a speaking epoch (0 to 1.75 s after the go cue for T5, 0.5 to 1.75 s after go for T8). We chose a later speaking epoch start for T8 to better match this participant’s delayed speech-related modulation, which could reflect less anticipatory preparation prior to the (predictable) go cue time, and/or the reduced speech-related modulation recorded on T8’s arrays. This resulted in one datum for each epoch per speech condition, for example 10 pairs of (prompt, speech) value pairs corresponding to each syllable. We compared the resulting prompt and speech epoch distributions with a Wilcoxon signed-rank test. The same procedure was used to compare the prompt epoch neural population modulation to a ‘baseline’ epoch consisting of the 1 s leading up to the audio prompt.</p><p>When we report the ratio between population modulation during the go epoch and during the prompt epoch, this ratio was computed after taking the mean modulation across all syllables/words for each epoch.</p></sec><sec id="s4-11"><title>Comparing different phonemes’ neural correlates</title><p>To generate <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>, we first manually segmented each word spoken in the T5-phonemes dataset into its constituent phonemes using the Praat software package (<xref ref-type="bibr" rid="bib6">Boersma and Weenink, 2019</xref>). This resulted in 3892 total phonemes. The number of occurrences across the 41 unique phonemes ranged between 14 (/ɔ/) and 239 (/t/), with a median of 80 occurrences. For each unique phoneme, we isolated a 150 ms window of TCs centered around the onset of each instance of that phoneme. This produced an (# instances) × electrodes firing rate matrix for each phoneme. We used these data matrices to calculate the neural population activity difference between all pairs of phonemes using the cross-validated neural distance metric described in the ‘Neural population modulation’ section. This resulted in the matrix of phoneme pair neural distances in <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5A</xref>. Within-phoneme neural distances (the diagonal elements of the distance matrix) were calculated by comparing half of the instances of a given phoneme with the other half; the distances shown are the mean distances across 20 such random splits of each phoneme.</p><p>To relate these neural distances to known differences in the speech articulator movements required to produce the phonemes, we grouped phonemes by their place of articulation as in <xref ref-type="bibr" rid="bib60">Moses et al. (2019)</xref>. We then compared within-group neural distances to between-groups neural distances (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5B</xref>). Every pair of phonemes in the <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5A</xref> neural distance matrix contributes one datum to either the red distribution in that figure’s panel B (if the two phonemes are in the same articulatory grouping) or to the black distribution (if the two phonemes are in different groups). The exception to this is that the three phonemes that are sole members of their own lonely groups were not included in this analysis. The summary statistic of this comparison was the difference between the mean of within-group neural distances and the mean of between-groups neural distances. This statistic was compared against a null distribution built by taking the same summary metric after shuffling neural distance matrix rows and columns, repeated 10,000 times. This null distribution assumes that the phonemes are grouped arbitrarily (but with the same number and sizes of groups), and not according to place of articulation. Comparing the true within-group versus between-groups difference to this null distribution (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5C</xref>) provides a p-value for rejecting the null hypothesis that phoneme neural distances are no more correlated with articulatory grouping than expected by chance.</p><p>The dendrogram shown in <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5D</xref> was generated by applying the widely used ‘unweighted pair group method with arithmetic mean’ (UPGMA) hierarchical clustering algorithm (<xref ref-type="bibr" rid="bib76">Sokal and Michener, 1958</xref>) to the phoneme neural distance matrix.</p></sec><sec id="s4-12"><title>Breath-related neural modulation</title><p>To generate breath-triggered firing rates (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), we first identified breath peak times from the breath belt stretch transducer measurements. The belt signals were pre-processed by removing rare outlier values (&gt;50 μV difference between consecutive samples) and then low-pass filtering (3 Hz pass-band) the signal both forwards and backwards in time to avoid introducing a phase shift. An example of this filtered signal is shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>. Breath peaks were then found using the MATLAB <italic>findpeaks</italic> function, with key parameters of MinPeakDistance = 1 s, and MinPeakProminence = 0.3⋅B, where B is the median of all peak prominences found by first running <italic>findpeaks</italic> using MinPeakDistance = 5 s (in other words, we required a peak to be at least 30% of the prominence of the ‘big’ peaks in the data).</p><p>Breath peak-aligned firing rates were calculated by treating each identified breath peak as one trial, and trial averaging across neural snippets aligned to each breath peak time. Each TCs’ or SUA’s breath-related modulation depth was defined as the maximum – minimum firing rate observed in the interval from 2 s before the breath peak to 1.5 s after the breath peak. To calculate whether a given modulation depth was statistically significant, we used a shuffle control in which we compared the true data’s modulation depth to the distribution of modulation depths observed over 1001 random shuffles in which faux peak breath times were uniformly drawn from the data duration. For comparing breath-related and speaking-related modulation depths (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2F</xref>), we defined a given electrode’s speech modulation depth in the T5-syllables dataset as its maximum – minimum firing rate from 2.5 s before acoustic onset to 1 s after acoustic onset.</p></sec><sec id="s4-13"><title>Arm and hand versus orofacial and speaking movements comparisons</title><p>The neural ensemble modulation comparisons presented in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> were calculated as follows: mean TCs firing rates for each T5-comparisons dataset instructed movement condition were calculated for each electrode from 200 to 600 ms after the go cue. The resulting firing rate vectors were compared to firing rate vectors similarly constructed from the ‘do nothing’ condition. Modulation was calculated by taking the unbiased neural distance between these firing rate vectors as described above in the ‘Neural population modulation’ section.</p></sec><sec id="s4-14"><title>Single-trial low-dimensional neural state projections</title><p>To visualize single-trial high dimensional neural data (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), we used t-distributed stochastic neighbor embedding (tSNE), a dimensionally reduction technique which seeks to represent high-dimensional vectors (such as our time-varying, multielectrode neural data) in a low-dimensional space (such as a 2D plot that can be easily visualized). The tSNE algorithm finds a nonlinear mapping such that similar high-dimensional feature vectors end up close together in the low-dimensional view, while dissimilar vectors end up far apart (<xref ref-type="bibr" rid="bib85">Van Der Maaten and Hinton, 2008</xref>). A neural feature vector was constructed for each trial as follows: for each functioning electrode, spike rates and HLFP power were calculated in ten 100 ms bins that spanned from 0.5 s before to 0.5 s after AO. These features were concatenated into a vector; for example, for the T5-syllables dataset, a single trial’s neural data were represented as a 104 electrodes × 2 features per electrode ×10 time bins = 2080 dimensional vector. All trials’ feature vectors were then projected into a 2D space using the <italic>tsne</italic> function in MATLAB R2017b’s Statistics and Machine Learning Toolbox with NumDimensions = 2; Perplexity = 15 (this is the number of local neighbors examined for each datum); Algorithm = exact (suitable for our relatively small dataset); and Standardize = true (this z-scores the input data, which was desirable due to the variability between different electrodes and the vastly different scales between spike rates and HLFP power). All other algorithm parameters were set to their defaults. <xref ref-type="fig" rid="fig3">Figure 3A</xref> does not have axis labels because t-SNE does not return meaningful axes or units; only the relative distances between points have meaning.</p></sec><sec id="s4-15"><title>Speech decoding</title><p>We evaluated how well the identity of the syllable or word being spoken could be decoded from neural data by classifying single trial neural data. Neural feature vectors were constructed for each trial as described above. These vectors were then associated with a class label, which was the sound being spoken (i.e. word, syllable, or silence). We trained support vector machines (SVMs), a standard classification tool, to predict the class label from a ‘new’ neural feature vector which the classifier had not been trained on. Prediction accuracies were cross-validated using a leave-one-trial-out paradigm in which the classifier was trained on all trials except the trial being classified, and this was repeated for all trials in a dataset. Multiclass classification was achieved using the error-correcting output code (ECOC) technique, which trains multiple binary SVMs between all pairs of labels, that is a one-versus-one coding design. When classifying new input data, the ECOC technique picks the class that minimizes the sum of losses over the set of binary SVM classifiers. Specifically, we used MATLAB R2017b’s implementation: a multiclass model object was fit (<italic>fitcecoc</italic>) using the SVM template (<italic>templateSVM</italic>). Key parameters were to use a linear kernel; OutlierFraction = 0.05 (expecting 5% of data points to be outliers); and Standardize = true (which z-scores the neural features based on the training data). All other parameters were set to their default values. We note that we did not heavily optimize our classification method; rather, our goal here was to use a standard tool to gauge the classification performance that these intracortical neural signals support. More sophisticated machine learning techniques (e.g. <xref ref-type="bibr" rid="bib4">Angrick et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Livezey et al., 2019</xref>) are likely to provide additional improvements.</p><p>To measure chance prediction performance, we used a shuffle test in which we randomly permuted the class labels associated with all trials’ neural data. The same classifier training and leave-one-out prediction process was then repeated on these shuffled data 101 times.</p></sec><sec id="s4-16"><title>Neural population dynamics</title><p>An underlying motivation for the neural population dynamics analyses described in the next several sections is the idea that the activity of many thousands or millions of neurons in a circuit (of which we can only measure on the order of 100 neurons in humans with current technology) can be summarized by the time-varying activity of a handful of latent ‘components’. In this framing, individual neurons’ firing rates reflect various mixtures of these underlying components; in all the analyses we used, this mapping from components to firing rates is assumed to be linear. These components are not meant as discrete physical ‘things’ in the brain, but rather are mathematical abstractions which capture meaningful patterns in the activities of networks of neurons. They are useful insofar as they can help generate hypotheses about the computations neural populations are performing by describing their prominent activity patterns. To this end, not only can latent components succinctly describe the ‘neural state’ (i.e. the firing rate of all neurons at a given moment in time), but furthermore, the time evolution of these components is often more conducive to interpretation and understanding than more complex descriptions of all the individual neurons’ firing rates.</p><p>Here, we built on previous studies showing that these components’ changes over time can be effectively modeled as a lawful time-varying oscillatory dynamical system (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Pandarinath et al., 2015</xref>), and that they reveal a simple population-level pattern in which there is a stereotyped response at the initiation of many different movements (<xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>). This ‘dynamical system’ framework is extensively reviewed in <xref ref-type="bibr" rid="bib74">Shenoy et al. (2013)</xref> as well in the two key studies that inspire the neural population dynamics analyses of the present study (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib44">Kaufman et al., 2016</xref>). We looked for the aforementioned dynamical motifs using two different dimensionality reduction techniques that were specifically designed to reveal the presence (or absence) of these population dynamics features.</p><p>For these analyses, we primarily examined the prompted word speaking task datasets because this was a more naturalistic behavior than the prompted syllables speaking task. Participants reported that it was more difficult to discriminate syllables than words, and that speaking stand-alone syllables felt somewhat awkward, whereas saying words was easy. Consequently, a practical benefit of the words task over the syllables task is that behavior was more stereotyped across trials, which facilitates precise trial-averaging, and there were very few mis-heard or mis-spoken words. Results for the same analyses applied to the syllables task data are shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><p>Both of these neural population state analyses were performed on TCs, which contained more information about the neural population state than the more limited number of recorded SUA. All electrodes with TCs firing rates greater than 1 Hz were included. The Churchland-Cunningham and Kaufman studies analyzed a combination of both SUA from single-electrode recordings and TCs from multielectrode recordings, depending on the dataset, while <xref ref-type="bibr" rid="bib65">Pandarinath et al. (2015)</xref> also analyzed just TCs. To avoid cumbersome switching of terms when describing our methods and comparing them to those of these previous studies, we will use the generic term ‘unit’ to refer to a single channel of neural information, whether it be SUA or TCs.</p></sec><sec id="s4-17"><title>Condition-invariant signal</title><p>The first population dynamics motif we tested for was a specific form of population-level structure at the initiation of movement: a large condition-invariant signal, previously described in <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>. We closely followed Kaufman and colleagues’ analysis methods, adapting them as necessary for these human speaking datasets. As in <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>, spike trains were trial-averaged within a behavioral condition (in our case, speaking one of the 10 different words), smoothed with a 28 ms s.d. Gaussian, and ‘soft normalized’ with a 5 Hz offset. Normalization means that each unit’s firing rate was normalized by its range across all times and conditions. This prevents units with very high firing rates from dominating the estimate of neural population state (<xref ref-type="bibr" rid="bib67">Pandarinath et al., 2018</xref>). The ‘soft’ refers to adding an offset (5 Hz in these analyses) to the denominator to reduce the influence of units with very small modulation. Trial-averaged firing rates were calculated from a speech initiation epoch of 200 ms before go cue to 400 ms after the go cue for T5, and 100 ms to 700 ms after the go cue for T8. T8’s epoch was shifted later relative to T5’s to account for T8’s later neural population activity divergence from the silent condition (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref>). This yields a N × C × T data tensor, where N is the number of units, C is the number of word conditions (10), and T is the number of time samples (600, using 1 ms sliding bins).</p><p>We used demixed principal components analysis (dPCA), a dimensionality-reduction technique developed by <xref ref-type="bibr" rid="bib47">Kobak et al. (2016)</xref>, to look for condition-invariant activity patterns in these high-dimensional neural recordings. This dimensionality reduction method is conceptually similar to PCA, in that it finds a specified number of dPC ‘components’ that can be thought of as ‘building blocks’ from which the responses of individual units can be composed. As with PCA, dPCA attempts to compress the data by identifying dimensions that capture a large fraction of the variance. This takes advantage of the fact that unless the responses of neurons are all independent from one another (which in practice is not the case), then most of the variance of the full population response can be accurately reconstructed as a weighted sum of a smaller number of dPC components. Where dPCA differs from PCA is that it can explicitly attempt to find components that marginalize variance attributable to different parameters of the experiment (such as time or task variables). This is possible because dPCA is a supervised method that trades off finding dimensions that maximize variance in favor of finding dimensions that partition the variance based on labeled properties of the data.</p><p>In our case, this ‘demixing’ was attempted between: 1) condition and condition + time interactions, which together form the condition-dependent (CD) components of the neural population activity; and 2) time only, which forms condition-invariant (CI) components. In other words, dPCA sought a set of components of the population activity for which the time-varying neural responses during producing different words look the same, and also for another set of components which vary across speaking conditions (i.e. are ‘tuned’ for what word is being spoken). Importantly, such variance marginalization (i.e. demixing the parameters) may not be achievable; it depends on the structure of the data itself. Each component that dPCA returns is associated both with how much overall neural variance it captures (the lengths of the bars in <xref ref-type="fig" rid="fig4">Figure 4A</xref>), and how much of this variance is CI or CD (red and blue fraction of each bar, respectively). Thus, the success of this demixing can be examined based on how purely CI or CD each component is. This in turn reveals whether there exists a large and almost completely condition-invariant component of the population neural activity.</p><p>Kaufman and colleagues used an earlier version of the dPCA method and code package, called ‘dPCA-2011’ (<xref ref-type="bibr" rid="bib10">Brendel et al., 2011</xref>). We used the MATLAB implementation of ‘dPCA-2015’ (<xref ref-type="bibr" rid="bib47">Kobak et al., 2016</xref>), downloaded from ﻿<ext-link ext-link-type="uri" xlink:href="https://github.com/machenslab/dPCA">https://github.com/machenslab/dPCA</ext-link>. This is an updated, improved, and widely adopted version of the technique which was not yet available at the time when the <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref> analyses were performed. We specified that dPCA should return eight total components, which was less than then 10 to 12 used in <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>. This reflects the reduced complexity of our datasets, in the sense that they had fewer conditions (10 versus 27–108) and fewer units (96–106 versus 116–213). We also repeated the analyses using 2 to 12 dPCs and observed very similar results. Default <italic>dpca</italic> function parameters were used, with parameters numRep = 10 (repetitions for regularization cross-validation) and simultaneous = true (indicating that the single-trial neural data were simultaneously recorded across electrodes) for the <italic>dpca_optimizeLambda</italic> and <italic>dpca_getNoiseCovariance</italic> functions.</p><p>Unlike the dPCA-2011 used by <xref ref-type="bibr" rid="bib44">Kaufman et al. (2016)</xref>, dPCA-2015 does not enforce that the neural dimensions found for capturing variance attributable to different parameters (here, the CI and CD components) be orthogonal. For example, while the first three (largely CI) components for T5 in <xref ref-type="fig" rid="fig4">Figure 4A</xref> are orthogonal by construction (as are the five largely CD components), these CI and CD components need not be orthogonal. We quantified the angles between the demixed principal axes (the dPCA encoder dimensions), and the (related but distinct degree of correlation between the resulting dPCA components, using the methods described in <xref ref-type="bibr" rid="bib47">Kobak et al. (2016)</xref> and implemented in the dPCA code pack. Unlike <xref ref-type="bibr" rid="bib47">Kobak et al. (2016)</xref>, we used a p-value threshold of 0.01 rather than 0.001 for the Kendall rank correlation coefficient test between each pair of dimensions’ electrode weightings vectors. This means that we were more conservative in the sense that we were more likely to flag neural dimensions as non-orthogonal. For measuring the angle between the CIS<sub>1</sub> dimension and the first jPC plane (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>), we used the <italic>subspacea</italic> package for MATLAB, downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/55-subspacea-m">https://www.mathworks.com/matlabcentral/fileexchange/55-subspacea-m</ext-link> (<xref ref-type="bibr" rid="bib46">Knyazev and Argentati, 2002</xref>). To test whether the CIS<sub>1</sub> was significantly non-orthogonal to each of the jPCA dimensions individually, we used the same Kendall rank correlation test as described above.</p></sec><sec id="s4-18"><title>Rotatory dynamics</title><p>The second form of neural population structure we tested for was rotatory (i.e. oscillatory) low-dimensional dynamics. We applied methods previously developed to identify and quantify rotatory dynamics in motor cortex during NHP arm reaching (<xref ref-type="bibr" rid="bib21">Churchland et al., 2012</xref>). These methods were also recently applied to show rotatory dynamics during hand movements of BrainGate2 study participants (<xref ref-type="bibr" rid="bib65">Pandarinath et al., 2015</xref>). Churchland, Cunningham and colleagues introduced the jPCA dimensionality reduction technique for this purpose; we employed their MATLAB analysis package, downloaded from <ext-link ext-link-type="uri" xlink:href="https://churchland.zuckermaninstitute.columbia.edu/content/code">https://churchland.zuckermaninstitute.columbia.edu/content/code</ext-link>.</p><p>Trial-averaged firing rates for each word speaking condition were generated from 150 ms before to 100 ms after acoustic onset to capture an epoch when speech-producing articulator movements were being produced. Following <xref ref-type="bibr" rid="bib21">Churchland et al. (2012)</xref> and <xref ref-type="bibr" rid="bib65">Pandarinath et al. (2015)</xref>, these firing rates were soft-normalized with a 10 Hz offset and smoothed with a Gaussian kernel; we used a 30 ms s.d. kernel as in <xref ref-type="bibr" rid="bib65">Pandarinath et al. (2015)</xref>. These firing rates were ‘centered’ by subtracting the across-condition mean firing rate of each unit at each time point, and then sampled every 10 ms. The dimensionality of these data was reduced via PCA to six; this ensured that rotatory dynamics would be sought within population activity components that were strongly present in the data. jPCA was then used to find planes with rotatory structure within this six-dimensional subspace. The jPCs are found by fitting the following linear dynamical system:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <bold>x</bold> is the neural state (i.e. the PCA dimensionality-reduced population firing rate) at a given time, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is its time derivative, and <bold>M</bold><sub>skew</sub> is constrained to be a skew-symmetric matrix. The first jPCA plane, which has the strongest rotatory dynamics, is defined by the two complex eigenvectors of <bold>M</bold><sub>skew</sub> with the largest eigenvalues. The choice of real vectors jPC<sub>1</sub> and jPC<sub>2</sub> within this plane is arbitrary and, following convention, were chosen such that conditions’ activities are maximally spread along jPC<sub>1</sub> at the start of the analysis epoch. <xref ref-type="fig" rid="fig5">Figure 5A</xref> plots the trial-averaged population activity during speaking each word (after subtracting the across-conditions mean) in this top jPCA plane. The red/black/green color of each word condition’s neural trajectory corresponds to its projection along jPC<sub>1</sub> at the start of the epoch; this display style is intended to assist in observing that amplitude and phase tend to unfold lawfully from the initial neural state. It is worth emphasizing that each jPC is simply a linear weighting of different units’ firing rates, and that the six jPCs form an orthonormal basis set that spans the same subspace as the top six PCs. The strength of rotatory dynamics was quantified as the goodness of fit for <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> for a 2 × 2 <bold>M</bold><sub>skew</sub> in the first jPCA plane, and for a 6 × 6 <bold>M</bold><sub>skew</sub> in the 6-dimensional subspace defined by the top 6 PCs of the data. <xref ref-type="fig" rid="fig5">Figure 5B</xref> reports this 6D fit quality.</p></sec><sec id="s4-19"><title>Statistical testing of rotatory dynamics</title><p>To calculate the statistical significance of rotatory population dynamics structure in our data, we applied the ‘neural population control’ approach developed by Elsayed and Cunningham (<xref ref-type="bibr" rid="bib29">Elsayed and Cunningham, 2017</xref>). This method was developed to address a potential concern that many specific phenomena that an experimenter could test for (such as fitting low-dimensional rotatory dynamics to neural data) can be found ‘by chance’ in a sufficiently high-dimensional, complex dataset such as the time-varying firing rates of many neurons. To address this, the method tests whether an observed feature of the population activity is ‘novel’ in the sense that it cannot be trivially predicted from known simpler features in the data. This is achieved by constructing surrogate datasets with simple population structure (in the form of means and correlations across time, neurons, and behavioral conditions) matched to the real data. If the neural recordings contain population-level structure that is coordinated above and beyond these first- and second-order features, then the quantification method used to describe this structure should return a stronger read-out when applied to the original dataset than to the surrogate datasets.</p><p>In our case, we used this approach to test whether it is ‘surprising’ to see rotatory dynamics in neural population data, given the particular smoothness across time, units, and word speaking conditions present in these data. A similar approach was used in <xref ref-type="bibr" rid="bib29">Elsayed and Cunningham (2017)</xref> to further validate the original rotatory dynamics finding of <xref ref-type="bibr" rid="bib21">Churchland et al. (2012)</xref>. We used the MATLAB code associated with <xref ref-type="bibr" rid="bib29">Elsayed and Cunningham (2017)</xref> from <ext-link ext-link-type="uri" xlink:href="https://github.com/gamaleldin/TME">https://github.com/gamaleldin/TME</ext-link> to generate 1000 surrogate datasets with time, neuron, and condition means and covariance matched to the real data using the tensor maximum entropy algorithm (‘surrogate-TNC’ flag in <italic>fitMaxEntropy</italic>). We then ran the same jPCA analyses described above on these surrogate datasets and recorded the rotation dynamics goodness of fit for the best <bold>M</bold><sub>skew</sub> matrix found for each surrogate dataset. This distribution of surrogate dataset R<sup>2</sup> values serves as a null distribution for significance testing: we calculated a p-value by counting how many of the surrogate datasets’ R<sup>2</sup> exceeded that of the true original dataset.</p></sec><sec id="s4-20"><title>Neural state trajectory videos</title><p>The goal of <xref ref-type="video" rid="video2">Videos 2</xref> and <xref ref-type="video" rid="video3">3</xref> is to visualize how participant T5’s neural population activity undergoes a condition-invariant ‘kick’ after the go cue (<xref ref-type="fig" rid="fig4">Figure 4</xref>) followed by rotatory dynamics around acoustic onset (<xref ref-type="fig" rid="fig5">Figure 5</xref>). To do so, we projected the ensemble neural activity during speaking short words into a lower dimensional neural state space designed to capture both the prominent condition-invariant component (hence, CIS<sub>1</sub> is one of the three projection dimensions) and rotatory dynamics (hence, the remaining two dimensions are the top jPCA plane). Plotting the word conditions’ neural state trajectories in the same state space required harmonizing the slightly different pre-processing used in the dPCA (<xref ref-type="fig" rid="fig4">Figure 4</xref>) and jPCA (<xref ref-type="fig" rid="fig5">Figure 5</xref>) analyses. Specifically, the trial-averaged neural trajectories in these videos were generated using the 30 ms s.d. Gaussian smoothing and 5 Hz soft-normalization parameters from the dPCA analysis. The CIS<sub>1</sub> dimension was found by applying dPCA to the same time epoch as in <xref ref-type="fig" rid="fig4">Figure 4</xref> (200 ms before go to 400 ms after go), and the jPC<sub>1</sub> and jPC<sub>2</sub> dimensions were found by applying jPCA to the same time epoch as in <xref ref-type="fig" rid="fig5">Figure 5</xref> (150 ms before to 100 ms after acoustic on).</p><p>To facilitate viewing the neural state trajectories in three (orthogonal) dimensions consisting of [CIS<sub>1</sub>, jPC<sub>1</sub>, jPC<sub>2</sub>], for these videos only we enforced that jPC<sub>1</sub> and jPC<sub>2</sub> be orthogonal to CIS<sub>1</sub> (empirically, without this constraint the top jPCA plane was 75° from the CIS<sub>1</sub>, as shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1E</xref>). To do so, prior to running jPCA, the trial-averaged firing rates were projected into the null space of the CIS<sub>1</sub> (the orthogonal complement of the first column of the encoder matrix returned by dPCA). That is, instead of jPCA operating on the <italic>E</italic> = 96 electrodes firing rates, it operated on a 96−1 = 95 dimensional projection of the firing rates. The overall consequence of these decisions is that in these videos, the neural state is projected onto the exact same CIS<sub>1</sub> dimension as in <xref ref-type="fig" rid="fig4">Figure 4</xref>, whereas the jPC<sub>1</sub> and jPC<sub>2</sub> dimensions differ slightly from <xref ref-type="fig" rid="fig5">Figure 5</xref> due to the aforementioned spike train pre-processing differences and CIS<sub>1</sub> orthogonalization.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank participants T5, T8, and their caregivers for their dedicated contributions to this research; Nancy Lam for administrative support; Dr. Sydney Cash and Dr. Laura Ball for helpful discussions; and Dr. Marc Slutzky for helpful discussions and providing the list of many words for sampling many distinct phonemes.</p><p>This work was supported by an ALS Association Milton Safenowitz Postdoctoral Fellowship, A. P. Giannini Foundation Postdoctoral Fellowship, Wu Tsai Neurosciences Institute Interdisciplinary Scholar Award, and Burroughs Wellcome Fund Career Award at the Scientific Interface (SDS); NSF Graduate Research Fellowship DGE – 1656518 and Regina Casper Stanford Graduate Fellowship (GHW); Larry and Pamela Garlick, Samuel and Betsy Reeves (KVS, JMH); NIDCD R01DC014034 (JMH); Office of Research and Development, Rehabilitation R and D Service, Department of Veterans Affairs N9288C, A2295R, B6453R, Executive Committee on Research of Massachusetts General Hospital, NIDCD R01DC009899 (LRH); NICHD R01HD077220 (RFK); NINDS 5U01NS098968-02 (LRH); Howard Hughes Medical Institute (KVS). The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>The MGH Translational Research Center has clinical research support agreements with Paradromics and Synchron Med, for which LRH provides consultative input. LRH is also a consultant for Neuralink</p></fn><fn fn-type="COI-statement" id="conf3"><p>is a consultant for Neuralink Corp and on the scientific advisory boards of CTRL-Labs Inc, MIND-X Inc, Inscopix Inc, and Heal Inc</p></fn><fn fn-type="COI-statement" id="conf4"><p>is a consultant for Neuralink Corp, Proteus Biomedical and Boston Scientific, and serves on the Medical Advisory Boards of Enspire DBS and Circuit Therapeutics</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Visualization</p></fn><fn fn-type="con" id="con4"><p>Investigation</p></fn><fn fn-type="con" id="con5"><p>Investigation</p></fn><fn fn-type="con" id="con6"><p>Investigation</p></fn><fn fn-type="con" id="con7"><p>Investigation, Project administration</p></fn><fn fn-type="con" id="con8"><p>Investigation</p></fn><fn fn-type="con" id="con9"><p>Supervision, Funding acquisition, Project administration</p></fn><fn fn-type="con" id="con10"><p>Supervision, Funding acquisition, Project administration</p></fn><fn fn-type="con" id="con11"><p>Supervision, Funding acquisition</p></fn><fn fn-type="con" id="con12"><p>Formal analysis, Supervision</p></fn><fn fn-type="con" id="con13"><p>Conceptualization, Supervision, Funding acquisition, Project administration</p></fn><fn fn-type="con" id="con14"><p>Conceptualization, Supervision, Funding acquisition, Project administration</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Clinical trial registration NCT00912041.</p></fn><fn fn-type="other" id="fn2"><p>Human subjects: The two participants in this study were enrolled in the BrainGate2 Neural Interface System pilot clinical trial (ClinicalTrials.gov Identifier: NCT00912041). The overall purpose of the study is to obtain preliminary safety information and demonstrate proof of principle that an intracortical brain-computer interface can enable people with tetraplegia to communicate and control external devices. Permission for the study was granted by the U.S. Food and Drug Administration under an Investigational Device Exemption (Caution: Investigational device. Limited by federal law to investigational use). The study was also approved by the Institutional Review Boards of Stanford University Medical Center (protocol #20804), Brown University (#0809992560), University Hospitals of Cleveland Medical Center (#04-12-17), Partners HealthCare and Massachusetts General Hospital (#2011P001036), and the Providence VA Medical Center (#2011-009). Both participants gave informed consent to the study and publications resulting from the research, including consent to publish photographs and audiovisual recordings of them.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Breathing data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data1-v2.zip"/></supplementary-material><supplementary-material id="sdata2"><label>Source data 2.</label><caption><title>Classification data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data2-v2.zip"/></supplementary-material><supplementary-material id="sdata3"><label>Source data 3.</label><caption><title>Dynamics data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data3-v2.zip"/></supplementary-material><supplementary-material id="sdata4"><label>Source data 4.</label><caption><title>PSTHs sorted units data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data4-v2.zip"/></supplementary-material><supplementary-material id="sdata5"><label>Source data 5.</label><caption><title>Syllables PSTHS TCs data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data5-v2.zip"/></supplementary-material><supplementary-material id="sdata6"><label>Source data 6.</label><caption><title>Tuning and behavior data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data6-v2.zip"/></supplementary-material><supplementary-material id="sdata7"><label>Source data 7.</label><caption><title>Video go aligned data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data7-v2.zip"/></supplementary-material><supplementary-material id="sdata8"><label>Source data 8.</label><caption><title>Video and dynamics speak aligned data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data8-v2.zip"/></supplementary-material><supplementary-material id="sdata9"><label>Source data 9.</label><caption><title>Words PSTHs TCs data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-46015-data9-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-46015-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The sharing of the raw human neural data is restricted due to the potential sensitivity of this data. These data are available upon request to the senior authors (KVS or JMH). To respect the participants' expectation of privacy, a legal agreement between the researcher's institution and the BrainGate consortium would need to be set up to facilitate the sharing of these datasets. Processed data is provided as source data, and analysis code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sstavisk/speech_in_dorsal_motor_cortex_eLife_2019">https://github.com/sstavisk/speech_in_dorsal_motor_cortex_eLife_2019</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/speech_in_dorsal_motor_cortex_eLife_2019">https://github.com/elifesciences-publications/speech_in_dorsal_motor_cortex_eLife_2019</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ajiboye</surname> <given-names>AB</given-names></name><name><surname>Willett</surname> <given-names>FR</given-names></name><name><surname>Young</surname> <given-names>DR</given-names></name><name><surname>Memberg</surname> <given-names>WD</given-names></name><name><surname>Murphy</surname> <given-names>BA</given-names></name><name><surname>Miller</surname> <given-names>JP</given-names></name><name><surname>Walter</surname> <given-names>BL</given-names></name><name><surname>Sweet</surname> <given-names>JA</given-names></name><name><surname>Hoyen</surname> <given-names>HA</given-names></name><name><surname>Keith</surname> <given-names>MW</given-names></name><name><surname>Peckham</surname> <given-names>PH</given-names></name><name><surname>Simeral</surname> <given-names>JD</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Kirsch</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Restoration of reaching and grasping movements through brain-controlled muscle stimulation in a person with tetraplegia: a proof-of-concept demonstration</article-title><source>The Lancet</source><volume>389</volume><fpage>1821</fpage><lpage>1830</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(17)30601-3</pub-id><pub-id pub-id-type="pmid">28363483</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akbari</surname> <given-names>H</given-names></name><name><surname>Khalighinejad</surname> <given-names>B</given-names></name><name><surname>Herrero</surname> <given-names>JL</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Towards reconstructing intelligible speech from the human auditory cortex</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>874</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-37359-z</pub-id><pub-id pub-id-type="pmid">30696881</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname> <given-names>WE</given-names></name><name><surname>Chen</surname> <given-names>MZ</given-names></name><name><surname>Pichamoorthy</surname> <given-names>N</given-names></name><name><surname>Tien</surname> <given-names>RH</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Luo</surname> <given-names>L</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Thirst regulates motivated behavior through modulation of brainwide neural population dynamics</article-title><source>Science</source><volume>364</volume><pub-id pub-id-type="doi">10.1126/science.aav3932</pub-id><pub-id pub-id-type="pmid">30948440</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angrick</surname> <given-names>M</given-names></name><name><surname>Herff</surname> <given-names>C</given-names></name><name><surname>Mugler</surname> <given-names>E</given-names></name><name><surname>Tate</surname> <given-names>MC</given-names></name><name><surname>Slutzky</surname> <given-names>MW</given-names></name><name><surname>Krusienski</surname> <given-names>DJ</given-names></name><name><surname>Schultz</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speech synthesis from ECoG using densely connected 3D convolutional neural networks</article-title><source>Journal of Neural Engineering</source><volume>16</volume><elocation-id>036019</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/ab0c59</pub-id><pub-id pub-id-type="pmid">30831567</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anumanchipalli</surname> <given-names>GK</given-names></name><name><surname>Chartier</surname> <given-names>J</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speech synthesis from neural decoding of spoken sentences</article-title><source>Nature</source><volume>568</volume><fpage>493</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1119-1</pub-id><pub-id pub-id-type="pmid">31019317</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Boersma</surname> <given-names>P</given-names></name><name><surname>Weenink</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Praat: doing phonetics by computer</data-title></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchard</surname> <given-names>KE</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Functional organization of human sensorimotor cortex for speech articulation</article-title><source>Nature</source><volume>495</volume><fpage>327</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1038/nature11911</pub-id><pub-id pub-id-type="pmid">23426266</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchard</surname> <given-names>KE</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Control of spoken vowel acoustics and the influence of phonetic context in human speech sensorimotor cortex</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>12662</fpage><lpage>12677</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1219-14.2014</pub-id><pub-id pub-id-type="pmid">25232105</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandman</surname> <given-names>DM</given-names></name><name><surname>Hosman</surname> <given-names>T</given-names></name><name><surname>Saab</surname> <given-names>J</given-names></name><name><surname>Burkhart</surname> <given-names>MC</given-names></name><name><surname>Shanahan</surname> <given-names>BE</given-names></name><name><surname>Ciancibello</surname> <given-names>JG</given-names></name><name><surname>Sarma</surname> <given-names>AA</given-names></name><name><surname>Milstein</surname> <given-names>DJ</given-names></name><name><surname>Vargas-Irwin</surname> <given-names>CE</given-names></name><name><surname>Franco</surname> <given-names>B</given-names></name><name><surname>Kelemen</surname> <given-names>J</given-names></name><name><surname>Blabe</surname> <given-names>C</given-names></name><name><surname>Murphy</surname> <given-names>BA</given-names></name><name><surname>Young</surname> <given-names>DR</given-names></name><name><surname>Willett</surname> <given-names>FR</given-names></name><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Kirsch</surname> <given-names>RF</given-names></name><name><surname>Walter</surname> <given-names>BL</given-names></name><name><surname>Bolu Ajiboye</surname> <given-names>A</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name><name><surname>Eskandar</surname> <given-names>EN</given-names></name><name><surname>Miller</surname> <given-names>JP</given-names></name><name><surname>Sweet</surname> <given-names>JA</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name><name><surname>Jarosiewicz</surname> <given-names>B</given-names></name><name><surname>Harrison</surname> <given-names>MT</given-names></name><name><surname>Simeral</surname> <given-names>JD</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid calibration of an intracortical brain-computer interface for people with tetraplegia</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>026007</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa9ee7</pub-id><pub-id pub-id-type="pmid">29363625</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brendel</surname> <given-names>W</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name></person-group><year iso-8601-date="2011">2011</year><chapter-title>Demixed Principal Component Analysis</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><fpage>2654</fpage><lpage>2662</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breshears</surname> <given-names>JD</given-names></name><name><surname>Molinaro</surname> <given-names>AM</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A probabilistic map of the human ventral sensorimotor cortex using electrical stimulation</article-title><source>Journal of Neurosurgery</source><volume>123</volume><fpage>340</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.3171/2014.11.JNS14889</pub-id><pub-id pub-id-type="pmid">25978714</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breshears</surname> <given-names>JD</given-names></name><name><surname>Southwell</surname> <given-names>DG</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inhibition of Manual Movements at Speech Arrest Sites in the Posterior Inferior Frontal Lobe</article-title><source>Neurosurgery</source><volume>85</volume><fpage>23</fpage><lpage>25</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brumberg</surname> <given-names>JS</given-names></name><name><surname>Wright</surname> <given-names>EJ</given-names></name><name><surname>Andreasen</surname> <given-names>DS</given-names></name><name><surname>Guenther</surname> <given-names>FH</given-names></name><name><surname>Kennedy</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Classification of intended phoneme production from chronic intracortical microelectrode recordings in speech-motor cortex</article-title><source>Front. Neurosci</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmena</surname> <given-names>JM</given-names></name><name><surname>Lebedev</surname> <given-names>MA</given-names></name><name><surname>Crist</surname> <given-names>RE</given-names></name><name><surname>O'Doherty</surname> <given-names>JE</given-names></name><name><surname>Santucci</surname> <given-names>DM</given-names></name><name><surname>Dimitrov</surname> <given-names>DF</given-names></name><name><surname>Patil</surname> <given-names>PG</given-names></name><name><surname>Henriquez</surname> <given-names>CS</given-names></name><name><surname>Nicolelis</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning to control a brain-machine interface for reaching and grasping by primates</article-title><source>PLOS Biology</source><volume>1</volume><elocation-id>e42</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0000042</pub-id><pub-id pub-id-type="pmid">14624244</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A little man of some importance</article-title><source>Brain</source><volume>140</volume><fpage>3055</fpage><lpage>3061</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname> <given-names>AM</given-names></name><name><surname>Baker</surname> <given-names>JM</given-names></name><name><surname>Eskandar</surname> <given-names>E</given-names></name><name><surname>Schomer</surname> <given-names>D</given-names></name><name><surname>Ulbert</surname> <given-names>I</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>First-pass selectivity for semantic categories in human anteroventral temporal lobe</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>18119</fpage><lpage>18129</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3122-11.2011</pub-id><pub-id pub-id-type="pmid">22159123</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname> <given-names>AM</given-names></name><name><surname>Dykstra</surname> <given-names>AR</given-names></name><name><surname>Jayaram</surname> <given-names>V</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Travis</surname> <given-names>KE</given-names></name><name><surname>Gygi</surname> <given-names>B</given-names></name><name><surname>Baker</surname> <given-names>JM</given-names></name><name><surname>Eskandar</surname> <given-names>E</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Speech-specific tuning of neurons in human superior temporal gyrus</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>2679</fpage><lpage>2693</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht127</pub-id><pub-id pub-id-type="pmid">23680841</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chartier</surname> <given-names>J</given-names></name><name><surname>Anumanchipalli</surname> <given-names>GK</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex</article-title><source>Neuron</source><volume>98</volume><fpage>1042</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.031</pub-id><pub-id pub-id-type="pmid">29779940</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>PL</given-names></name><name><surname>Hsu</surname> <given-names>HY</given-names></name><name><surname>Wang</surname> <given-names>PY</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Isolated hand weakness in cortical infarctions</article-title><source>Journal of the Formosan Medical Association</source><volume>105</volume><fpage>861</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1016/S0929-6646(09)60276-X</pub-id><pub-id pub-id-type="pmid">17000462</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Hamiton</surname> <given-names>LS</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The auditory representation of speech sounds in human motor cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e12577</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12577</pub-id><pub-id pub-id-type="pmid">26943778</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Foster</surname> <given-names>JD</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1594</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1038/nn.2439</pub-id><pub-id pub-id-type="pmid">19915566</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collinger</surname> <given-names>JL</given-names></name><name><surname>Wodlinger</surname> <given-names>B</given-names></name><name><surname>Downey</surname> <given-names>JE</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Weber</surname> <given-names>DJ</given-names></name><name><surname>McMorland</surname> <given-names>AJC</given-names></name><name><surname>Velliste</surname> <given-names>M</given-names></name><name><surname>Boninger</surname> <given-names>ML</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-performance neuroprosthetic control by an individual with tetraplegia</article-title><source>The Lancet</source><volume>381</volume><fpage>557</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(12)61816-9</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conant</surname> <given-names>DF</given-names></name><name><surname>Bouchard</surname> <given-names>KE</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human sensorimotor cortex control of directly measured vocal tract movements during vowel production</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2955</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2382-17.2018</pub-id><pub-id pub-id-type="pmid">29439164</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creutzfeldt</surname> <given-names>O</given-names></name><name><surname>Ojemann</surname> <given-names>G</given-names></name><name><surname>Lettich</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Neuronal activity in the human lateral temporal lobe</article-title><source>Experimental Brain Research</source><volume>77</volume><fpage>451</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1007/BF00249600</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1500</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1038/nn.3776</pub-id><pub-id pub-id-type="pmid">25151264</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname> <given-names>JT</given-names></name><name><surname>Watkins</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Stimulating language: insights from TMS</article-title><source>Brain</source><volume>130</volume><fpage>610</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1093/brain/awl331</pub-id><pub-id pub-id-type="pmid">17138570</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dichter</surname> <given-names>BK</given-names></name><name><surname>Breshears</surname> <given-names>JD</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The control of vocal pitch in human laryngeal motor cortex</article-title><source>Cell</source><volume>174</volume><fpage>21</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.05.016</pub-id><pub-id pub-id-type="pmid">29958109</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname> <given-names>GF</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Structure in neural population recordings: an expected byproduct of simpler phenomena?</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1310</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1038/nn.4617</pub-id><pub-id pub-id-type="pmid">28783140</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Even-Chen</surname> <given-names>N</given-names></name><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Blabe</surname> <given-names>CH</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Feasibility of automatic error Detect-and-Undo system in human intracortical Brain–Computer Interfaces</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>65</volume><fpage>1771</fpage><lpage>1784</lpage><pub-id pub-id-type="doi">10.1109/TBME.2017.2776204</pub-id><pub-id pub-id-type="pmid">29989931</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farrell</surname> <given-names>DF</given-names></name><name><surname>Burbank</surname> <given-names>N</given-names></name><name><surname>Lettich</surname> <given-names>E</given-names></name><name><surname>Ojemann</surname> <given-names>GA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Individual variation in human motor-sensory (rolandic) cortex</article-title><source>Journal of Clinical Neurophysiology</source><volume>24</volume><fpage>286</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1097/WNP.0b013e31803bb59a</pub-id><pub-id pub-id-type="pmid">17545834</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesher</surname> <given-names>SN</given-names></name><name><surname>Collinger</surname> <given-names>JL</given-names></name><name><surname>Foldes</surname> <given-names>ST</given-names></name><name><surname>Weiss</surname> <given-names>JM</given-names></name><name><surname>Downey</surname> <given-names>JE</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name><name><surname>Bensmaia</surname> <given-names>SJ</given-names></name><name><surname>Schwartz</surname> <given-names>AB</given-names></name><name><surname>Boninger</surname> <given-names>ML</given-names></name><name><surname>Gaunt</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Intracortical microstimulation of human somatosensory cortex</article-title><source>Science Translational Medicine</source><volume>8</volume><elocation-id>361ra141</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aaf8083</pub-id><pub-id pub-id-type="pmid">27738096</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname> <given-names>S</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Rigotti</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why neurons mix: high dimensionality for higher cognition</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.01.010</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname> <given-names>JA</given-names></name><name><surname>Perich</surname> <given-names>MG</given-names></name><name><surname>Miller</surname> <given-names>LE</given-names></name><name><surname>Solla</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentilucci</surname> <given-names>M</given-names></name><name><surname>Campione</surname> <given-names>GC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Do postures of distal effectors affect the control of actions of other distal effectors? evidence for a system of interactions between hand and mouth</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e19793</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0019793</pub-id><pub-id pub-id-type="pmid">21625428</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentilucci</surname> <given-names>M</given-names></name><name><surname>Stefani</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>From gesture to speech</article-title><source>Biolinguistics</source><volume>6</volume><fpage>338</fpage><lpage>353</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guenther</surname> <given-names>FH</given-names></name><name><surname>Brumberg</surname> <given-names>JS</given-names></name><name><surname>Wright</surname> <given-names>EJ</given-names></name><name><surname>Nieto-Castanon</surname> <given-names>A</given-names></name><name><surname>Tourville</surname> <given-names>JA</given-names></name><name><surname>Panko</surname> <given-names>M</given-names></name><name><surname>Law</surname> <given-names>R</given-names></name><name><surname>Siebert</surname> <given-names>SA</given-names></name><name><surname>Bartels</surname> <given-names>JL</given-names></name><name><surname>Andreasen</surname> <given-names>DS</given-names></name><name><surname>Ehirim</surname> <given-names>P</given-names></name><name><surname>Mao</surname> <given-names>H</given-names></name><name><surname>Kennedy</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A wireless brain-machine interface for real-time speech synthesis</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e8218</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0008218</pub-id><pub-id pub-id-type="pmid">20011034</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Guenther</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Neural Control of Speech Movements</source><publisher-loc>Cambridge MA</publisher-loc><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herff</surname> <given-names>C</given-names></name><name><surname>Schultz</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Automatic speech recognition from neural signals: a focused review</article-title><source>Frontiers in Neuroscience</source><volume>10</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.3389/fnins.2016.00429</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Serruya</surname> <given-names>MD</given-names></name><name><surname>Friehs</surname> <given-names>GM</given-names></name><name><surname>Mukand</surname> <given-names>JA</given-names></name><name><surname>Saleh</surname> <given-names>M</given-names></name><name><surname>Caplan</surname> <given-names>AH</given-names></name><name><surname>Branner</surname> <given-names>A</given-names></name><name><surname>Chen</surname> <given-names>D</given-names></name><name><surname>Penn</surname> <given-names>RD</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neuronal ensemble control of prosthetic devices by a human with tetraplegia</article-title><source>Nature</source><volume>442</volume><fpage>164</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1038/nature04970</pub-id><pub-id pub-id-type="pmid">16838014</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Intveld</surname> <given-names>RW</given-names></name><name><surname>Dann</surname> <given-names>B</given-names></name><name><surname>Michaels</surname> <given-names>JA</given-names></name><name><surname>Scherberger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural coding of intended and executed grasp force in macaque Areas AIP, F5, and M1</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>17985</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-35488-z</pub-id><pub-id pub-id-type="pmid">30573765</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>W</given-names></name><name><surname>Pailla</surname> <given-names>T</given-names></name><name><surname>Dichter</surname> <given-names>B</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name><name><surname>Gilja</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title> Decoding speech using the timing of neural signal modulation </article-title><conf-name>2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</conf-name><fpage>1532</fpage><lpage>1535</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2016.7591002</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname> <given-names>JJ</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>Denman</surname> <given-names>DJ</given-names></name><name><surname>Bauza</surname> <given-names>M</given-names></name><name><surname>Barbarits</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Andrei</surname> <given-names>A</given-names></name><name><surname>Aydın</surname> <given-names>Ç</given-names></name><name><surname>Barbic</surname> <given-names>M</given-names></name><name><surname>Blanche</surname> <given-names>TJ</given-names></name><name><surname>Bonin</surname> <given-names>V</given-names></name><name><surname>Couto</surname> <given-names>J</given-names></name><name><surname>Dutta</surname> <given-names>B</given-names></name><name><surname>Gratiy</surname> <given-names>SL</given-names></name><name><surname>Gutnisky</surname> <given-names>DA</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name><name><surname>Karsh</surname> <given-names>B</given-names></name><name><surname>Ledochowitsch</surname> <given-names>P</given-names></name><name><surname>Lopez</surname> <given-names>CM</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Musa</surname> <given-names>S</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Putzeys</surname> <given-names>J</given-names></name><name><surname>Rich</surname> <given-names>PD</given-names></name><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>WL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Seely</surname> <given-names>JS</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The largest response component in the motor cortex reflects movement timing but not movement type</article-title><source>Eneuro</source><volume>3</volume><fpage>1171</fpage><lpage>1197</lpage><pub-id pub-id-type="doi">10.1523/ENEURO.0085-16.2016</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Cueva</surname> <given-names>CJ</given-names></name><name><surname>Reppas</surname> <given-names>JB</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamics of neural population responses in prefrontal cortex indicate changes of mind on single trials</article-title><source>Current Biology</source><volume>24</volume><fpage>1542</fpage><lpage>1547</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.049</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knyazev</surname> <given-names>AV</given-names></name><name><surname>Argentati</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Principal Angles between Subspaces in an <italic>A</italic> -Based Scalar Product: Algorithms and Perturbation Estimates</article-title><source>SIAM Journal on Scientific Computing</source><volume>23</volume><fpage>2008</fpage><lpage>2040</lpage><pub-id pub-id-type="doi">10.1137/S1064827500377332</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname> <given-names>D</given-names></name><name><surname>Brendel</surname> <given-names>W</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Feierstein</surname> <given-names>CE</given-names></name><name><surname>Kepecs</surname> <given-names>A</given-names></name><name><surname>Mainen</surname> <given-names>ZF</given-names></name><name><surname>Qi</surname> <given-names>XL</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Uchida</surname> <given-names>N</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Demixed principal component analysis of neural population data</article-title><source>eLife</source><volume>5</volume><elocation-id>e10989</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10989</pub-id><pub-id pub-id-type="pmid">27067378</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leuthardt</surname> <given-names>EC</given-names></name><name><surname>Gaona</surname> <given-names>C</given-names></name><name><surname>Sharma</surname> <given-names>M</given-names></name><name><surname>Szrama</surname> <given-names>N</given-names></name><name><surname>Roland</surname> <given-names>J</given-names></name><name><surname>Freudenberg</surname> <given-names>Z</given-names></name><name><surname>Solis</surname> <given-names>J</given-names></name><name><surname>Breshears</surname> <given-names>J</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Using the electrocorticographic speech network to control a brain-computer interface in humans</article-title><source>Journal of Neural Engineering</source><volume>8</volume><elocation-id>036004</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/8/3/036004</pub-id><pub-id pub-id-type="pmid">21471638</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipski</surname> <given-names>WJ</given-names></name><name><surname>Alhourani</surname> <given-names>A</given-names></name><name><surname>Pirnia</surname> <given-names>T</given-names></name><name><surname>Jones</surname> <given-names>PW</given-names></name><name><surname>Dastolfo-Hromack</surname> <given-names>C</given-names></name><name><surname>Helou</surname> <given-names>LB</given-names></name><name><surname>Crammond</surname> <given-names>DJ</given-names></name><name><surname>Shaiman</surname> <given-names>S</given-names></name><name><surname>Dickey</surname> <given-names>MW</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name><name><surname>Turner</surname> <given-names>RS</given-names></name><name><surname>Fiez</surname> <given-names>JA</given-names></name><name><surname>Richardson</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Subthalamic nucleus neurons differentially encode early and late aspects of speech production</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5620</fpage><lpage>5631</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3480-17.2018</pub-id><pub-id pub-id-type="pmid">29789378</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livezey</surname> <given-names>JA</given-names></name><name><surname>Bouchard</surname> <given-names>KE</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning as a tool for neural data analysis: speech classification and cross-frequency coupling in human sensorimotor cortex</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007091</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007091</pub-id><pub-id pub-id-type="pmid">31525179</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lotte</surname> <given-names>F</given-names></name><name><surname>Brumberg</surname> <given-names>JS</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Gunduz</surname> <given-names>A</given-names></name><name><surname>Ritaccio</surname> <given-names>AL</given-names></name><name><surname>Guan</surname> <given-names>C</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Electrocorticographic representations of segmental features in continuous speech</article-title><source>Frontiers in Human Neuroscience</source><volume>09</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2015.00097</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname> <given-names>TR</given-names></name><name><surname>Scholz</surname> <given-names>J</given-names></name><name><surname>Henderson Slater</surname> <given-names>D</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Tracey</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reassessing cortical reorganization in the primary sensorimotor cortex following arm amputation</article-title><source>Brain</source><volume>138</volume><fpage>2140</fpage><lpage>2146</lpage><pub-id pub-id-type="doi">10.1093/brain/awv161</pub-id><pub-id pub-id-type="pmid">26072517</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Makin</surname> <given-names>JG</given-names></name><name><surname>Moses</surname> <given-names>DA</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine translation of cortical activity to text with an encoder-decoder framework</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/708206</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname> <given-names>TR</given-names></name><name><surname>Bensmaia</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stability of sensory topographies in adult cortex</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>195</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.01.002</pub-id><pub-id pub-id-type="pmid">28214130</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>S</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Holdgraf</surname> <given-names>C</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Rieger</surname> <given-names>J</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Pasley</surname> <given-names>BN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding spectrotemporal features of overt and covert speech from the human cortex</article-title><source>Frontiers in Neuroengineering</source><volume>7</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.3389/fneng.2014.00014</pub-id><pub-id pub-id-type="pmid">24904404</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>S</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Iturrate</surname> <given-names>I</given-names></name><name><surname>Millán</surname> <given-names>JdelR</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Pasley</surname> <given-names>BN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Word pair classification during imagined speech using direct brain recordings</article-title><source>Scientific Reports</source><volume>6</volume><pub-id pub-id-type="doi">10.1038/srep25803</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masse</surname> <given-names>NY</given-names></name><name><surname>Jarosiewicz</surname> <given-names>B</given-names></name><name><surname>Simeral</surname> <given-names>JD</given-names></name><name><surname>Bacher</surname> <given-names>D</given-names></name><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Cash</surname> <given-names>SS</given-names></name><name><surname>Oakley</surname> <given-names>EM</given-names></name><name><surname>Berhanu</surname> <given-names>E</given-names></name><name><surname>Eskandar</surname> <given-names>E</given-names></name><name><surname>Friehs</surname> <given-names>G</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Non-causal spike filtering improves decoding of movement intention for intracortical BCIs</article-title><source>Journal of Neuroscience Methods</source><volume>236</volume><fpage>58</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.08.004</pub-id><pub-id pub-id-type="pmid">25128256</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maynard</surname> <given-names>EM</given-names></name><name><surname>Hatsopoulos</surname> <given-names>NG</given-names></name><name><surname>Ojakangas</surname> <given-names>CL</given-names></name><name><surname>Acuna</surname> <given-names>BD</given-names></name><name><surname>Sanes</surname> <given-names>JN</given-names></name><name><surname>Normann</surname> <given-names>RA</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neuronal interactions improve cortical population coding of movement direction</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>8083</fpage><lpage>8093</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-18-08083.1999</pub-id><pub-id pub-id-type="pmid">10479708</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname> <given-names>IG</given-names></name><name><surname>Boroojerdi</surname> <given-names>B</given-names></name><name><surname>Foltys</surname> <given-names>H</given-names></name><name><surname>Sparing</surname> <given-names>R</given-names></name><name><surname>Huber</surname> <given-names>W</given-names></name><name><surname>Töpper</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Motor cortex hand area and speech: implications for the development of language</article-title><source>Neuropsychologia</source><volume>41</volume><fpage>401</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(02)00179-3</pub-id><pub-id pub-id-type="pmid">12559157</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moses</surname> <given-names>DA</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Makin</surname> <given-names>JG</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Real-time decoding of question-and-answer speech dialogue using human cortical activity</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3096</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10994-4</pub-id><pub-id pub-id-type="pmid">31363096</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mugler</surname> <given-names>EM</given-names></name><name><surname>Patton</surname> <given-names>JL</given-names></name><name><surname>Flint</surname> <given-names>RD</given-names></name><name><surname>Wright</surname> <given-names>ZA</given-names></name><name><surname>Schuele</surname> <given-names>SU</given-names></name><name><surname>Rosenow</surname> <given-names>J</given-names></name><name><surname>Shih</surname> <given-names>JJ</given-names></name><name><surname>Krusienski</surname> <given-names>DJ</given-names></name><name><surname>Slutzky</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Direct classification of all american english phonemes using signals from functional speech motor cortex</article-title><source>Journal of Neural Engineering</source><volume>11</volume><elocation-id>035015</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/11/3/035015</pub-id><pub-id pub-id-type="pmid">24836588</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mugler</surname> <given-names>EM</given-names></name><name><surname>Tate</surname> <given-names>MC</given-names></name><name><surname>Livescu</surname> <given-names>K</given-names></name><name><surname>Templer</surname> <given-names>JW</given-names></name><name><surname>Goldrick</surname> <given-names>MA</given-names></name><name><surname>Slutzky</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Differential representation of articulatory gestures and phonemes in precentral and inferior frontal gyri</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9803</fpage><lpage>9813</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1206-18.2018</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname> <given-names>S</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Juavinett</surname> <given-names>AL</given-names></name><name><surname>Gluf</surname> <given-names>S</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oby</surname> <given-names>ER</given-names></name><name><surname>Perel</surname> <given-names>S</given-names></name><name><surname>Sadtler</surname> <given-names>PT</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Mischel</surname> <given-names>JL</given-names></name><name><surname>Montez</surname> <given-names>DF</given-names></name><name><surname>Cohen</surname> <given-names>MR</given-names></name><name><surname>Batista</surname> <given-names>AP</given-names></name><name><surname>Chase</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Extracellular voltage threshold settings can be tuned for optimal encoding of movement and stimulus parameters</article-title><source>Journal of Neural Engineering</source><volume>13</volume><elocation-id>036009</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/13/3/036009</pub-id><pub-id pub-id-type="pmid">27097901</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>Gilja</surname> <given-names>V</given-names></name><name><surname>Blabe</surname> <given-names>CH</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Sarma</surname> <given-names>AA</given-names></name><name><surname>Sorice</surname> <given-names>BL</given-names></name><name><surname>Eskandar</surname> <given-names>EN</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural population dynamics in human motor cortex during movements in people with ALS</article-title><source>eLife</source><volume>4</volume><elocation-id>e07426</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.07436</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>Nuyujukian</surname> <given-names>P</given-names></name><name><surname>Blabe</surname> <given-names>CH</given-names></name><name><surname>Sorice</surname> <given-names>BL</given-names></name><name><surname>Saab</surname> <given-names>J</given-names></name><name><surname>Willett</surname> <given-names>FR</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High performance communication by people with paralysis using an intracortical brain-computer interface</article-title><source>eLife</source><volume>6</volume><elocation-id>e18554</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18554</pub-id><pub-id pub-id-type="pmid">28220753</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>Ames</surname> <given-names>KC</given-names></name><name><surname>Russo</surname> <given-names>AA</given-names></name><name><surname>Farshchian</surname> <given-names>A</given-names></name><name><surname>Miller</surname> <given-names>LE</given-names></name><name><surname>Dyer</surname> <given-names>EL</given-names></name><name><surname>Kao</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Latent factors and dynamics in motor cortex and their application to Brain-Machine interfaces</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9390</fpage><lpage>9401</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1669-18.2018</pub-id><pub-id pub-id-type="pmid">30381431</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pei</surname> <given-names>X</given-names></name><name><surname>Leuthardt</surname> <given-names>EC</given-names></name><name><surname>Gaona</surname> <given-names>CM</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Wolpaw</surname> <given-names>JR</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spatiotemporal dynamics of electrocorticographic high gamma activity during overt and covert word repetition</article-title><source>NeuroImage</source><volume>54</volume><fpage>2960</fpage><lpage>2972</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.10.029</pub-id><pub-id pub-id-type="pmid">21029784</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penfield</surname> <given-names>W</given-names></name><name><surname>Boldrey</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>Somatic motor and sensory representation in the cerebral cortex of man as studied by electrical stimulation</article-title><source>Brain</source><volume>60</volume><fpage>389</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1093/brain/60.4.389</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramsey</surname> <given-names>NF</given-names></name><name><surname>Salari</surname> <given-names>E</given-names></name><name><surname>Aarnoutse</surname> <given-names>EJ</given-names></name><name><surname>Vansteensel</surname> <given-names>MJ</given-names></name><name><surname>Bleichner</surname> <given-names>MG</given-names></name><name><surname>Freudenburg</surname> <given-names>ZV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decoding spoken phonemes from sensorimotor cortex with high-density ECoG grids</article-title><source>NeuroImage</source><volume>180</volume><fpage>301</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.011</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname> <given-names>G</given-names></name><name><surname>Arbib</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Language within our grasp</article-title><source>Trends Neurosci</source><volume>21</volume><fpage>188</fpage><lpage>194</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname> <given-names>S</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Towards the neural population doctrine</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>103</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.02.002</pub-id><pub-id pub-id-type="pmid">30877963</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schieber</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Constraints on somatotopic organization in the primary motor cortex</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>2125</fpage><lpage>2143</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.5.2125</pub-id><pub-id pub-id-type="pmid">11698506</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical control of arm movements: a dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150509</pub-id><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>MA</given-names></name><name><surname>Kohn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatial and temporal scales of neuronal correlation in primary visual cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>12591</fpage><lpage>12603</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2929-08.2008</pub-id><pub-id pub-id-type="pmid">19036953</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokal</surname> <given-names>RR</given-names></name><name><surname>Michener</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>A statistical method for evaluating systematic relationships</article-title><source>University of Kansas Science Bulletin</source><volume>38</volume><fpage>1409</fpage><lpage>1438</lpage></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname> <given-names>C</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>N</given-names></name><name><surname>Reddy</surname> <given-names>CB</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Suresh</surname> <given-names>AK</given-names></name><name><surname>Goodman</surname> <given-names>JM</given-names></name><name><surname>Okorokova</surname> <given-names>EV</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Hatsopoulos</surname> <given-names>NG</given-names></name><name><surname>Bensmaia</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population dynamics in motor cortex are different for reach and grasp</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/667196</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Churchland</surname> <given-names>MM</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tankus</surname> <given-names>A</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name><name><surname>Shoham</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Structured neuronal encoding and decoding of human speech features</article-title><source>Nature Communications</source><volume>3</volume><elocation-id>1015</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms1995</pub-id><pub-id pub-id-type="pmid">22910361</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tankus</surname> <given-names>A</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Degradation of neuronal encoding of speech in the subthalamic nucleus in Parkinson’s Disease</article-title><source>Neurosurgery</source><volume>84</volume><pub-id pub-id-type="doi">10.1093/neuros/nyy027</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tei</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Monoparesis of the right hand following a localised infarct in the left &quot;precentral knob&quot;</article-title><source>Neuroradiology</source><volume>41</volume><fpage>269</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1007/s002340050745</pub-id><pub-id pub-id-type="pmid">10344512</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trautmann</surname> <given-names>EM</given-names></name><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Lahiri</surname> <given-names>S</given-names></name><name><surname>Ames</surname> <given-names>KC</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>O'Shea</surname> <given-names>DJ</given-names></name><name><surname>Vyas</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>X</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Accurate estimation of neural population dynamics without spike sorting</article-title><source>Neuron</source><volume>103</volume><fpage>292</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.003</pub-id><pub-id pub-id-type="pmid">31171448</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vainio</surname> <given-names>L</given-names></name><name><surname>Schulman</surname> <given-names>M</given-names></name><name><surname>Tiippana</surname> <given-names>K</given-names></name><name><surname>Vainio</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Effect of syllable articulation on precision and power grip performance</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e53061</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0053061</pub-id><pub-id pub-id-type="pmid">23326381</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Maaten</surname> <given-names>LJP</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing high-dimensional data using t-sne</article-title><source>Journal of Machine Learning Research : JMLR</source><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waldert</surname> <given-names>S</given-names></name><name><surname>Lemon</surname> <given-names>RN</given-names></name><name><surname>Kraskov</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Influence of spiking activity on cortical local field potentials</article-title><source>The Journal of Physiology</source><volume>591</volume><fpage>5291</fpage><lpage>5303</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2013.258228</pub-id><pub-id pub-id-type="pmid">23981719</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wesselink</surname> <given-names>DB</given-names></name><name><surname>van den Heiligenberg</surname> <given-names>FMZ</given-names></name><name><surname>Ejaz</surname> <given-names>N</given-names></name><name><surname>Dempsey-Jones</surname> <given-names>H</given-names></name><name><surname>Cardinali</surname> <given-names>L</given-names></name><name><surname>Tarall-Jozwiak</surname> <given-names>A</given-names></name><name><surname>Diedrichsen</surname> <given-names>J</given-names></name><name><surname>Makin</surname> <given-names>TR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Obtaining and maintaining cortical hand representation as evidenced from acquired and congenital handlessness</article-title><source>eLife</source><volume>8</volume><elocation-id>e37227</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37227</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Willett</surname> <given-names>FR</given-names></name><name><surname>Deo</surname> <given-names>DR</given-names></name><name><surname>Avansino</surname> <given-names>DT</given-names></name><name><surname>Rezaii</surname> <given-names>P</given-names></name><name><surname>Hochberg</surname> <given-names>L</given-names></name><name><surname>Henderson</surname> <given-names>J</given-names></name><name><surname>Shenoy</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hand knob area of motor cortex in people with tetraplegia represents the whole body in a modular way</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/659839</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Dickey</surname> <given-names>MW</given-names></name><name><surname>Fiez</surname> <given-names>J</given-names></name><name><surname>Murphy</surname> <given-names>B</given-names></name><name><surname>Mitchell</surname> <given-names>T</given-names></name><name><surname>Collinger</surname> <given-names>J</given-names></name><name><surname>Tyler-Kabara</surname> <given-names>E</given-names></name><name><surname>Boninger</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sensorimotor experience and verb-category mapping in human sensory, motor and parietal neurons</article-title><source>Cortex</source><volume>92</volume><fpage>304</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2017.04.021</pub-id><pub-id pub-id-type="pmid">28575757</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yousry</surname> <given-names>TA</given-names></name><name><surname>Schmid</surname> <given-names>UD</given-names></name><name><surname>Alkadhi</surname> <given-names>H</given-names></name><name><surname>Schmidt</surname> <given-names>D</given-names></name><name><surname>Peraud</surname> <given-names>A</given-names></name><name><surname>Buettner</surname> <given-names>A</given-names></name><name><surname>Winkler</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Localization of the motor hand area to a knob on the Precentral Gyrus. A new landmark</article-title><source>Brain</source><volume>120</volume><fpage>141</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1093/brain/120.1.141</pub-id><pub-id pub-id-type="pmid">9055804</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46015.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Reviewer</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Gallego</surname><given-names>Juan Álvaro</given-names> </name><role>Reviewer</role><aff><institution/></aff></contrib><contrib contrib-type="reviewer"><name><surname>Scott</surname><given-names>Sophie K</given-names></name><role>Reviewer</role><aff><institution>UCL</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The paper by Stavisky et al. provides important innovation by providing a first account of the population dynamics relating to motor control of a body part within a 'canonical' motor area of another body part. The authors utilise electrode arrays implanted in participants with paralysis to identify mouth and speech-related motor processing in the hand/arm area of motor cortex. They analyse single neurons and neural population activity during speech, and demonstrate selective responses for spoken words, syllables and orofacial movements, resulting in high classification accuracy across words/syllables. The authors further interrogate the population dynamics, previously established for hand and arm movements in this cortical area, in search for a common neural dynamics underlying motor control from this area. The paper stood out in its quality and rigour of data analysis and clarity of conceptualisation. As such, the paper offers potential innovation on multiple fronts, from basic principles of brain organisation for motor control to assistive technologies via brain-machine interfaces, and is expected to appeal to a broad audience across multiple sub-fields.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neural ensemble dynamics in dorsal motor cortex during speech in people with paralysis&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Tamar R Makin as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Juan Álvaro Gallego (Reviewer #2), and Sophie K Scott (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The paper by Stavisky et al. utilises electrode arrays implanted in participants with paralysis to identify mouth and speech-related motor processing in the hand/arm area of motor cortex. The authors analyse single neurons and neural population activity during speech, and demonstrate selective responses for spoken words, syllables and orofacial movements, resulting in high classification accuracy across words/syllables. The authors further interrogate the population dynamics, previously established for hand and arm movements in this cortical area, in search for a common neural dynamics underlying motor control from this area. While the observation that body-part assignment along the motor 'homunculus' is more broadly distributed than commonly regarded is not in itself new (as highlighted by Penfield in his seminal work), the current study provides important innovation by providing a first account of the population dynamics relating to a 'misplaced' body part. Overall, the paper is well-written, the key analyses are sound, and the results highly interesting. However, the reviewers agreed that the characterisation of the neural results in the specific context of speech production could be improved by taking into consideration the wide range different patterns of motor control, from breath control, laryngeal engagement and control of the articulators. Conceptually, the reviewers felt that further discussion is required in order to place the findings in context, with respect to face-hand overlap, as elaborated below.</p><p>Essential revisions:</p><p>1) There seems to a be a missed opportunity to consider the varying motor demands for the larynx/tongue/mouth/lips across stimuli, or even laryngeal and speech breathing mechanisms (note that metabolic breathing, which the authors have accounted for, is entirely different). In the current analysis, each of the syllables/words is studies in isolation from the others, but in term of motor control there should be some clear similarities and distinctions across these stimuli, which could also be further linked with the motor demands of the orofacial movements. For example, decoding accuracy might vary depending on similarities for motor control of these various motor mechanisms involved in speech production. This will go a long way showing that the findings observed here relate to the motor processing relating to articulation, rather than other forms of information content. More generally, these important considerations relating to the mechanisms of speech productions need to be more thoughtfully integrated in the manuscript, the authors might like to consult with an expert for this purpose.</p><p>2) Conceptually, there is a need to better consider why facial information exists in the hand area. Is that because of a unique association between the mouth and hand for language? Here the authors might like to consider commonality of gestures, and consider whether this a semantic or timing-based gestural relationship, or both? Another interesting link to consider is between speaking and reading/writing? Or topographic proximity? Alternatively, could there be nothing special between the hand and the face – there could also exist information in the hand area for feet movements? Related to that, for participant T8 – the electrode arrays are too dorsomedial to be considered as the hand area. So it seems that the results suggest that orofacial/speed-related information is present throughout motor cortex. This brings us back to the question whether the SCI, and expected E/I balance changes in the deafferented cortex might play a role in the present findings. The reviewers agreed that the conceptual framework of the study could benefit from further justification/interpretation.</p><p>3) The results are often reported in descriptive terms but are not statistically tested, making it difficult to accept some of the characteristics offered by the authors. The reviewers would like to see more quantifications in the paper, including: percentage variance explained as function of the number of components (for PCA and dPCA), pairwise angles between CI and CD dPCs together with their significance threshold (Kobak et al. proposed a method on their paper), etc. Moreover, couldn't the authors apply these methods to the syllables datasets even if they had less trials, they were sorter, and the neural activity was less consistent (they can compensate for this with the speech I think)?</p><p>4) While the classification accuracy is impressive, it’s important to dissociate between the motor control component to others relating to perception and intention. The authors mention that responses during the audio prompt were small and thus they couldn't disambiguate whether they reflect perception, movement preparation, etc (subsection “Speech-related activity in dorsal motor cortex”, first paragraph). Based on Video 1, it seems to one reviewer that there's some modulation during the prompts. Is it possible to classify rapid responses in a small window centered around the auditory cue? If decoding accuracy is significantly greater during articulation, it might be provide support for the overall interpretation of the findings.</p><p>5) Similarly, can the authors explore whether there are any rotation motifs around the prompt? This would help answer the question whether this is an inherent network property of the area, or whether it is specific for movement planning.</p><p>6) The neural population analyses look quite different for the two patients: 1) for T8 there's only one CI dPC, and it explains roughly the same amount of variance as the leading CD dPC, whereas for T5 there are two CI dPCs that explain several times more variance than any CD dPC; 2) the rotational structure identified with jPC is not above the chance level for T8, only for T5. We understand that these differences may very well be motivated by the worse quality of T8's arrays, but the authors should be more cautious in some parts of the paper given these differences and their n=2, e.g., in the Abstract. Moreover, this difference should be addressed to a greater extent in the Discussion.</p><p>7) The authors suggest that the hand area might play a role in speech production. Here they seem to conflate correlation with causation – their findings do not provide any support that this decoding information available in the hand area is actually utilised during speech motor control.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.46015.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>[…] Overall, the paper is well-written, the key analyses are sound, and the results highly interesting. However, the reviewers agreed that the characterisation of the neural results in the specific context of speech production could be improved by taking into consideration the wide range different patterns of motor control, from breath control, laryngeal engagement and control of the articulators. Conceptually, the reviewers felt that further discussion is required in order to place the findings in context, with respect to face-hand overlap, as elaborated below.</p></disp-quote><p>In response to the reviewers’ feedback, we have added additional data and analyses to better relate this activity to the motoric demands of speech and volitional breath control, and we have expanded the Discussion to better place these findings in context. We have also added additional discussion of interpretation limitations based on the reviewers’ feedback. These changes, as well as many others, are described in more detail in responses to specific comments below.</p><p>With regards to the novelty of a broadly distributed homunculus, we appreciate the feedback that this is not entirely new. We were surprised by our results because we are unaware of previous studies showing this extent of distribution (i.e., face activity in this hand knob area), even after taking into account recent work describing fractured somatotopy within major body regions and the partially overlapping distributions of the original Penfield work (which was both due to within-subject and across-subjects effects). Thus, on this topic we hope (and believe) that our report of mixed tuning at the level of single neurons is an interesting and novel contribution that provides additional data of relevance to an emerging view of broad motor maps.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) There seems to a be a missed opportunity to consider the varying motor demands for the larynx/tongue/mouth/lips across stimuli, or even laryngeal and speech breathing mechanisms (note that metabolic breathing, which the authors have accounted for, is entirely different). In the current analysis, each of the syllables/words is studies in isolation from the others, but in term of motor control there should be some clear similarities and distinctions across these stimuli, which could also be further linked with the motor demands of the orofacial movements. For example, decoding accuracy might vary depending on similarities for motor control of these various motor mechanisms involved in speech production. This will go a long way showing that the findings observed here relate to the motor processing relating to articulation, rather than other forms of information content. More generally, these important considerations relating to the mechanisms of speech productions need to be more thoughtfully integrated in the manuscript, the authors might like to consult with an expert for this purpose.</p></disp-quote><p>Thank you for your feedback that the manuscript could, and should, do more to relate the observed speech-related neural activity with the underlying motor actions (the movements of the larynx/tongue/mouth/lips, and breathing) required to produce these sounds. We absolutely agree. We describe below (1) new measurements and analyses that we plan to do (but consider to be future research) now that these new cortical responses are known about and initially characterized, and (2) the considerable amount of new analysis work, <italic>including newly collected data</italic>, that we have done and include in the revised manuscript to directly address these points.</p><p>Regarding (1) above, we start out below by first describing – in some detail – for the reviewers and editors our future research roadmap which will invest considerably in pursuing exactly these questions. These are major research endeavors in and of themselves which will span considerable periods of time (likely a year or more, involving new FDA approval and new research infrastructure), and will lead to subsequent full reports. Regarding (2) above, we then turn to a considerable amount of new data analysis that we have done, including the use of newly collected data, that we have integrated into the revised manuscript (including a new figure), to directly address the questions raised. We believe that these analyses and results add considerably to the manuscript and, again, we are grateful for the reviewers’ and editors’ suggestion to more deeply pursue this topic.</p><p>(1) We think the best way forward toward answering in full detail the question regarding neural mechanisms of speech production is to collect new data and bring in new technical capabilities so that we can either measure speech articulator movements directly, or infer them from recorded audio (for example using new audio-articulator inversion ‘AAI’ methods like in Chartier et al., 2018, which unfortunately are not yet publicly available). Such data would overcome our current limitations: although we know what syllable/word the participants spoke and what prompted orofacial movements they made, we don’t have moment-by-moment kinematic measurements. Also, our original data sampled relatively few orofacial and speech movements. Without the underlying kinematic measurements and without comprehensive sampling of different combinations of kinematics, it becomes very difficult to attribute measured neural activity to specific articulatory degrees-of-freedom, since even short syllables or prompted movements are coordinated high-dimensional movements of multiple articulators.</p><p>We are investing considerably to be able to collect data that overcomes this limitation in the future, with: (A) a major purchase of Electromagnetic Midsagittal Articulography equipment, (B) planning for making a request for FDA regulatory approval to use these (uncomfortable but safe) techniques with our clinical trial participants, if they agree to do so, and (C) a newly established collaboration. However, doing all of this well this is a major undertaking – that will lead to one or more major additional full reports – and that realistically will extend out for over a year. Thus we view this as future work, and outside the scope of the present manuscript. We have added a Discussion paragraph (see below) laying out that we think this is a promising future direction in which to build on the present work. Our hope is that the present work, which provides a first description of this speech/orofacial single neuron activity in this cortical area, will lay the groundwork for this subsequent work.</p><p>This is a key way in which we believe we are addressing, as requested, the reviewer’s comment that “these important considerations relating to the mechanisms of speech productions need to be more thoughtfully integrated in the manuscript” in the longer term. Now we turn to (2), which addresses this request more directly by describing new analyses with new data that we have performed and added to the revised manuscript.</p><p>(2) We recognize that the manuscript would benefit from additional examination of how the neural correlates of different spoken sounds relate to their motor demands, and we are grateful to the reviewers and editors for suggesting this. To this end, we have performed a key new analysis on newly collected data. We asked participant T5 to speak 420 different words (3 times each) chosen to broadly sample 41 American English phonemes (we unfortunately could not repeat this new data collection in participant T8, who is no longer in the clinical trial). We then hand-segmented each of these words’ audio data into individual phonemes, and compared the neural ensemble correlates of these phonemes. This new result reveals that phonemes’ neural correlates clustered based on phonetic groupings and place of articulation, consistent with this activity being related to the underlying motor demands for producing the phonemes. See Figure 1—figure supplement 5.</p><p>We view this new result as consistent with the hypothesis that this neural activity is related to speech articulator movements. We believe that this publication helps motivate future work, by our group and potentially by other groups too, to further pursue the relationship between neural activity and articulatory kinematics.</p><p>This new dataset and its task are described in the ‘Many words task’ Materials and methods section; further details of the analysis are described in the ‘Comparing different phonemes’ neural correlates’ Materials and methods section. The added Results section text reads:</p><p>“Second, analysis of an additional dataset in which participant T5 spoke 41 different phonemes revealed that neural population activity showed phonemic structure (Figure 1—figure supplement 5): for example, when phonemes were grouped by place of articulation (Bouchard et al., 2013; Lotte et al., 2015; Moses et al., 2019), population firing rate vectors were significantly more similar between phonemes within the same group than between phonemes in different groups (p&lt;0.001, shuffle test).”</p><p>Finally, regarding this figure, we would be happy to elevate it to a figure in the main text (instead of a supplementary figure) if the reviewers and/or editors recommend this. While we indeed view this as an important analysis and figure, and are grateful that the reviewers and editors suggested that we pursue this, we also want to be mindful of not making the manuscript too long or over-emphasizing a single participant result. Thus, we pose this question here.</p><p>We share your prediction that breathing may contribute a component of our observed speech-related neural activity, and that it will be useful to study the neural correlates of breathing in the context of speech production, which is distinct from the metabolic breathing we studied here. As discussed above, we think that the best way forward on this question is to include breathing amongst many continuous speech articulatory kinematics in a future study, so that each of these movements’ distinct partial correlations with neural activity can be disentangled.</p><p>With that said, we are grateful to the reviewers and editors for suggesting that we pursue this a bit further, as we believe that we were able to take an additional step towards understanding breath-related activity. We did so by analyzing data from an additional ‘instructed breathing task’ in which breathing was under the participant’s conscious control. This new behavioral context is now analyzed alongside the previously presented unattended breathing data. The updated Figure 2—figure supplement 2, shows that volitional breathing also modulates hand knob cortex.</p><p>Please note that compared to the initial submission’s Figure 2—figure supplement 2, the shuffle distributions (panel C, D horizontal dashed lines) have shifted; this is because a) we caught and fixed a bug in the shuffle ordering code, and b) we changed the significance threshold to 0.01 (from 0.001) to maintain sensitivity after this fix and accommodate the reduced trial count of the new instructed breathing condition. This change does not affect our conclusions: besides moving the panel C, D lines, the net effect is that we now report that 17 (rather than 18) of the single neurons were significantly correlated with breathing. We apologize for this mistake and have carefully checked our code throughout the manuscript.</p><p>Below we have copied the updated Discussion paragraph that summarizes our evidence supporting that the observed neural activity is related to motor control, and suggests future work looking at speech kinematics:</p><p>“Our data suggest that the observed neural activity reflects movements of the speech articulators (the tongue, lips, jaw, and larynx): modulation was greater during speaking than after hearing the prompt; the same neural population modulated during non-speech orofacial movements; and in T5, the neural correlates of producing different phonemes grouped according to these phonemes’ place of articulation. […] A deeper understanding of how motor cortical spiking activity relates to complex speaking behavior will require future work connecting it to continuous articulatory (Chartier et al., 2018; Conant et al., 2018; Mugler et al., 2018) and respiratory kinematics and, ideally, the underlying muscle activations.”</p><disp-quote content-type="editor-comment"><p>2) Conceptually, there is a need to better consider why facial information exists in the hand area. Is that because of a unique association between the mouth and hand for language? Here the authors might like to consider commonality of gestures, and consider whether this a semantic or timing-based gestural relationship, or both? Another interesting link to consider is between speaking and reading/writing? Or topographic proximity? Alternatively, could there be nothing special between the hand and the face – there could also exist information in the hand area for feet movements? Related to that, for participant T8 – the electrode arrays are too dorsomedial to be considered as the hand area. So it seems that the results suggest that orofacial/speed-related information is present throughout motor cortex. This brings us back to the question whether the SCI, and expected E/I balance changes in the deafferented cortex might play a role in the present findings. The reviewers agreed that the conceptual framework of the study could benefit from further justification/interpretation.</p></disp-quote><p>Thank you, and we agree that the manuscript would benefit from more discussion of why there might be face information in “hand” area of motor cortex. While we originally speculated that this was due to the kinds of hand-mouth linkages enumerated by the reviewers and editors (based on previous studies such as those referenced in our Introduction), new results from our group (currently in the pre-print stage) have made us re-evaluate this interpretation. Inspired by finding face activity in hand knob area, we then tested whether there was modulation during actual and attempted movements of other body parts, including the neck, ipsilateral arm, and legs, exactly as you proposed. We found that indeed there is representation of every body part tested (Willett, et al., bioRxiv 2019). In light of this, our interpretation of these results is that finding speech-related activity in this cortical area is a consequence of motor representations being much more distributed at the single-neuron level than we previously imagined, rather than a “special” hand-face relationship (though we can’t rule that out, and it would be interesting to explicitly examine coordinated hand-face movements in future work). We have updated our Discussion accordingly, and we have also added a new paragraph that explicitly calls out that we are far from resolving what the “purpose” of this speech-related activity in hand knob area is (if any) and that we feel this is an important, though difficult, question for future research:</p><p>“There are three main findings from this study. […] Thus, the observed neural overlap between hand and speech articulators may be a consequence of distributed whole-body coding, rather than a privileged speech-manual linkage.”</p><p>“Assuming that these results are not due to injury-related remapping, we are left with the question of <italic>why</italic> this speech-related activity is found in dorsal “arm and hand” motor cortex. […] We anticipate that it will require substantial future work to understand why speech-related activity co-occurs in the same motor cortical area as arm and hand movement activity, but that this line of inquiry may reveal important principles of how sensorimotor control is distributed across the brain (Musall et al., 2019; Stringer et al., 2019).”</p><p>It is our hope that an impact of this manuscript will be to help motivate further work to understand this (fascinating, to us) phenomenon and better appreciate the complexity of human motor representations.</p><p>We have expanded the Discussion paragraph about why we think the presence of speech activity in hand knob cortex is not due to cortical remapping following SCI to incorporate this new whole-body tuning evidence. We have also added new references. This paragraph is reproduced below for convenience:</p><p>“An important unanswered question, however, is to what extent these results were potentially influenced by cortical remapping due to tetraplegia. […] While these threads of evidence argue against remapping, definitively resolving this ambiguity would require intracortical recording from this eloquent brain area in able-bodied people.”</p><p>Regarding the placement of T8’s arrays: placement was guided anatomically by definitively identifying Yousry’s “hand knob” area, which has distinctive contours on volumetrically obtained MRI images and can be identified with a high degree of certainty (&gt;97%) (described in Yousry, 1997). That said, we recognize that the extent of across-individual anatomy differences could raise questions about the accuracy and utility of generalized terms like “hand knob”, despite its adoption by neurosurgeons as a distinct anatomical structure. As further evidence for correct array placement, the functional properties of this area (strong hand and arm-related tuning) are also consistent with these arrays being in the same hand area as T5’s. We have added additional details to the Materials and methods to explain how the arrays were targeted:</p><p>“Both participants had two 96-electrode Utah arrays (1.5 mm electrode length, Blackrock Microsystems, USA) neurosurgically placed in dorsal ‘hand knob’ area of the left (motor dominant) hemisphere’s motor cortex. Surgical targeting was stereotactically guided based on prior functional and structural imaging (Yousry, 1997), and subsequently confirmed by review of intra-operative photographs.”</p><disp-quote content-type="editor-comment"><p>3) The results are often reported in descriptive terms but are not statistically tested, making it difficult to accept some of the characteristics offered by the authors. The reviewers would like to see more quantifications in the paper, including: percentage variance explained as function of the number of components (for PCA and dPCA), pairwise angles between CI and CD dPCs together with their significance threshold (Kobak et al. proposed a method on their paper), etc. Moreover, couldn't the authors apply these methods to the syllables datasets even if they had less trials, they were sorter, and the neural activity was less consistent (they can compensate for this with the speech I think)?</p></disp-quote><p>Thank you for pointing out that our neural population dynamics results and claims will be more strongly supported with additional quantifications and the inclusion of the syllables datasets. We have generated a new Figure 4—figure supplement 1 that provides these additional details, including cumulative variance explained for dPCA and jPCA and pairwise angles between dPCs (including the significance testing from Kobak et al., 2016). These quantifications are also now described in the ‘Condition-invariant signal’ Materials and methods section.</p><p>As per your suggestion, this supplementary figure also includes each participant’s syllables datasets, as well as two new T5 replication datasets. These additional ‘T5-5words-A’ and ‘T5-5words-B’ datasets were collected as part of a follow-up study, but we are happy to pull them in to this work and process them using the exact same analysis parameters used for the original datasets to build more confidence in the robustness of our findings. We believe that the manuscript is substantially strengthened by showing the consistency of these two neural population dynamics motifs across more datasets.</p><p>The additional datasets are discussed in the following updated Results passages:</p><p>“We found that these two prominent population dynamics motifs were indeed also present during speaking. […] These results were also robust across different choices of how many dPCs to summarize the neural population activity with (Figure 4—figure supplement 2).”</p><p>“Lastly, we looked for rotatory population dynamics around the time of acoustic onset. Figure 5A shows ensemble firing rates projected into the top jPCA plane. […] As was the case for the condition-invariant dynamics, these results were also consistent across additional datasets (Figure 4—figure supplement 1E-H) and across the choice of how many PCA dimensions in which to look for rotatory dynamics (Figure 4—figure supplement 2B).”</p><p>Please note that the Figure 4—figure supplement 1C dPC pairwise angles insets are a superset of the information provided in the original Figure 4 CIS<sub>1</sub> vs. CD<sub>1,2</sub> insets, so we have removed those. We now use a similar visual format in the new Figure 4—figure supplement 1E to compare the CIS<sub>1</sub> versus the top jPCA plane, which we think is an interesting comparison to document. Perhaps unsurprisingly, the CIS<sub>1</sub> is nearly orthogonal to the jPC dimensions, though we are careful to note that this need not be the case and that the model of a CIS that shifts dynamics into a different regime for movement generation does not require this to be the case (one could even imagine a CIS that shifts the neural state to a very different position in the exact same neural subspace also acting as a “trigger” for rotatory dynamics). The end of the Results now reads:</p><p>“We note that existing models of how a condition-invariant signal “kicks” dynamics into a different state space region where rotatory dynamics unfold (Kaufman et al., 2016; Sussillo et al., 2015) do not require that the CIS and rotatory dynamics must be orthogonal, but in these data we did observed that the CIS<sub>1</sub> and jPCA dimensions were largely orthogonal (Figure 4—figure supplement 1E).”</p><disp-quote content-type="editor-comment"><p>4) While the classification accuracy is impressive, it’s important to dissociate between the motor control component to others relating to perception and intention. The authors mention that responses during the audio prompt were small and thus they couldn't disambiguate whether they reflect perception, movement preparation, etc (subsection “Speech-related activity in dorsal motor cortex”, first paragraph). Based on Video 1, it seems to one reviewer that there's some modulation during the prompts. Is it possible to classify rapid responses in a small window centered around the auditory cue? If decoding accuracy is significantly greater during articulation, it might be provide support for the overall interpretation of the findings.</p></disp-quote><p>There is indeed (small) modulation after the prompt, which by the way we now quantify in a better way. Thank you for suggesting that we further quantify how much word/syllable-specific information is present in this prompt activity using a similar decoding approach as when decoding the speaking epoch activity. We have added this analysis, which shows very poor prompt-epoch classification performance, to the manuscript (see Figure 3C). As you said, this further supports that this activity is related to speech production.</p><p>The updated Results passage is:</p><p>“We next performed a decoding analysis to quantify how much information about the spoken syllable or word was present in the time-varying neural activity. […] The much higher neural discriminability of syllables and words during speaking rather than after hearing the audio prompt is consistent with the previously enumerated evidence that modulation in this cortical area is related to speech production.”</p><disp-quote content-type="editor-comment"><p>5) Similarly, can the authors explore whether there are any rotation motifs around the prompt? This would help answer the question whether this is an inherent network property of the area, or whether it is specific for movement planning.</p></disp-quote><p>Thank you for this valuable suggestion. We have now performed the same jPCA rotatory dynamics analysis on an epoch (of the same length as the main Figure 5 analyses) shortly after the prompt. These results are shown in the newly added Figure 4—figure supplement 1H, and reveal that there were not rotatory dynamics after the prompt. In the interest of space, and since these prompt rotations were not significant, for this analysis we only show the variance explained summary statistic (right below the significant speech-epoch statistics, for contrast) and not the neural trajectories in the top jPCA plane. In addition to being relevant to the wider question of how ubiquitous (across behaviors) and specific (across time epochs) neural rotations are, this new analysis also provides an empirical control that jPCA doesn’t just trivially find significant rotations in any neural data.</p><p>These new results are described in the Results:</p><p>“Lastly, we looked for rotatory population dynamics around the time of acoustic onset. Figure 5A shows ensemble firing rates projected into the top jPCA plane. […] As was the case for the condition-invariant dynamics, these results were also consistent across additional datasets (Figure 4—figure supplement 1E-H) and across the choice of how many PCA dimensions in which to look for rotatory dynamics (Figure 4—figure supplement 2B).”</p><disp-quote content-type="editor-comment"><p>6) The neural population analyses look quite different for the two patients: 1) for T8 there's only one CI dPC, and it explains roughly the same amount of variance as the leading CD dPC, whereas for T5 there are two CI dPCs that explain several times more variance than any CD dPC; 2) the rotational structure identified with jPC is not above the chance level for T8, only for T5. We understand that these differences may very well be motivated by the worse quality of T8's arrays, but the authors should be more cautious in some parts of the paper given these differences and their n=2, e.g., in the Abstract. Moreover, this difference should be addressed to a greater extent in the Discussion.</p></disp-quote><p>Thank you for your feedback that there was not enough discussion of the neural population analyses differences between the two participants, and that these differences warrant caution when interpreting the results. We have made a number of manuscript changes which we believe address this:</p><p>First of all, after improving our analysis methods for quantifying population-wide task-related modulation, we realized that our speech initiation analysis epoch of 200 ms before the go cue to 400 ms after go, which we had originally selected when initially analyzing T5’s data, was a poor choice for participant T8 because his recorded neural modulation occurs later than T5’s. This choice of a premature (minimally modulating) epoch exacerbated the differences between participants (in addition to the worse array quality, as mentioned by the reviewer). We have now changed T8’s CIS analysis epoch to 100 ms to 700 ms after go, which re-focuses this analysis on a post-go “modulation ramp up” epoch that is more similar to T5’s. This yields CIS results that look much more similar between the two participants (see Figure 4).</p><p>We also described the reasoning for the different epochs in the Materials and methods:</p><p>“Trial-averaged firing rates were calculated from a speech initiation epoch of 200 ms before go cue to 400 ms after the go cue for T5, and 100 ms to 700 ms after the go cue for T8. T8’s epoch was shifted later relative to T5’s to account for T8’s later neural population activity divergence from the silent condition (Figure 1—figure supplement 4B).”</p><p>Second, we have changed the Results section presenting these results to address the differences between the two participants’ CIS results:</p><p>“We found that these two prominent population dynamics motifs were indeed also present during speaking.[…] This lower signal-to-noise ratio can also be appreciated in how the “elbow” of T8’s cumulative neural variance explained by PCA or dPCA components (Figure 4—figure supplement 1A, B) occurs after fewer components and explains far less overall variance.”</p><p>Third, we now also revisit the non-significant T8 rotatory dynamics result in the updated Discussion section, which now reads:</p><p>“Our third main finding is that two motor cortical population dynamical motifs present during arm movements were also significant features of speech activity. We observed a large condition-invariant change at movement initiation in both participants, and rotatory dynamics during movement generation in the one of two participants whose arrays recorded substantially more modulation.”</p><p>Fourth, we have added the n=2 to the Abstract:</p><p>“Speaking is a sensorimotor behavior whose neural basis is difficult to study with single neuron resolution due to the scarcity of human intracortical measurements. We used electrode arrays to record from the motor cortex ‘hand knob’ in two people with tetraplegia, an area not previously implicated in speech.”</p><p>Relatedly, we also now address the differences in the two participant’s decoding performance in the Discussion:</p><p>“That said, these results are only a first step in establishing the feasibility of speech BCIs using intracortical electrode arrays. […] We also observed worse decoding performance in participant T8, highlighting the need for future studies in additional participants to sample the distribution of how much speech-related neural modulation can be expected, and what speech BCI performance these signals can support.”</p><p>Also, please note that T5’s CIS dPCA Figure 4 plots have changed very slightly from the original submission because when revisiting these analyses, we noticed that we had been insufficiently regularizing the dimensionality reduction/variance partition process such that the dPCs didn’t generalize as well to held out data. We have now used the dPCA code’s built-in cross-validated regularization parameter optimization and verified that the resulting dimensionality reduction generalizes well if we do dPCA on only half the data and then compare the resulting dimensions and variance partitions when projecting the other (held-out) half of the data into these dPCs. We have added this detail to the ‘Condition-invariant signal’ Materials and methods section:</p><p>“Default <italic>dpca</italic> function parameters were used, with parameters numRep = 10 (repetitions for regularization cross-validation) and simultaneous = true (indicating that the single-trial neural data were simultaneously recorded across electrodes) for the <italic>dpca_optimizeLambda</italic> and <italic>dpca_getNoiseCovariance</italic> functions.”</p><disp-quote content-type="editor-comment"><p>7) The authors suggest that the hand area might play a role in speech production. Here they seem to conflate correlation with causation – their findings do not provide any support that this decoding information available in the hand area is actually utilised during speech motor control.</p></disp-quote><p>Thank you for pointing out that as originally written, our Discussion came across as suggesting that these data indicate a causal role of hand area in speech production. We apologize for this, as we absolutely agree that we have only observed correlation with speaking, and no evidence for a causal role. We have updated several sections of the Discussion, reproduced below, to be more cautious in speculating, what, if any, role this activity might have in speech or coordinating speech and hand movements:</p><p>“There are three main findings from this study. First, these data suggest that ‘hand knob’ motor cortex, an area not previously known to be active during speaking (Breshears et al., 2015; Dichter et al., 2018; Leuthardt et al., 2011; Lotte et al., 2015), may in fact participate, or at least receive correlates of, neural computations underlying speech production.”</p><p>“Assuming that these results are not due to injury-related remapping, we are left with the question of <italic>why</italic> this speech-related activity is found in dorsal “arm and hand” motor cortex. […] We anticipate that it will take substantial future work to understand why speech-related activity co-occurs in the same motor cortical area as arm and hand movement activity, but that this line of inquiry may reveal important principles of how sensorimotor control is distributed across the brain (Musall et al., 2019; Stringer et al., 2019).”</p></body></sub-article></article>