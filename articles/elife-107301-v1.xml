<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">107301</article-id><article-id pub-id-type="doi">10.7554/eLife.107301</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107301.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Center-surround inhibition in expectation and its underlying computational and artificial neural network models</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Huang</surname><given-names>Ling</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0006-9712-2726</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">â€ </xref><xref ref-type="fn" rid="pa1">â€¡</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Shen</surname><given-names>Shiqi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9082-9178</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">â€ </xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Sun</surname><given-names>Yueling</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">â€ </xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ou</surname><given-names>Shipei</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0654-715X</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6730-1452</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0449-934X</contrib-id><email>xlzhang@m.scnu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>Key Laboratory of Brain, Cognition and Education Sciences, Ministry of Education, South China Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>School of Psychology, Center for Studies of Psychological Application, and Guangdong Provincial Key Laboratory of Mental Health and Cognitive Science, South China Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Brain Health Institute, National Center for Mental Disorders, Shanghai Mental Health Center, Shanghai Jiao Tong University School of Medicine and School of Psychology</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Donders Institute for Brain, Cognition and Behaviour, Radboud University</institution></institution-wrap><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sui</surname><given-names>Jing</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>â€ </label><p>These authors contributed equally to this work</p></fn><fn fn-type="present-address" id="pa1"><label>â€¡</label><p>Department of Psychology, The Ohio State University, Columbus, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>11</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP107301</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-05-10"><day>10</day><month>05</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-08-27"><day>27</day><month>08</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.08.26.609781"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-07-28"><day>28</day><month>07</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107301.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-10-10"><day>10</day><month>10</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107301.2"/></event></pub-history><permissions><copyright-statement>Â© 2025, Huang, Shen, Sun et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Huang, Shen, Sun et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-107301-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-107301-figures-v1.pdf"/><abstract><p>Expectation is beneficial for adaptive behavior through quickly deducing plausible interpretations of information. The profile and underlying neural computations of this process, however, remain unclear. When participants expected a grating with a specific orientation, we found a center-surround inhibition profile in orientation space, which was independent from attentional modulations by task relevance. Using computational modeling, we showed that this center-surround inhibition could be reproduced by either a sharpening of tuning curves of expected orientation or a shift of tuning curves of unexpected orientations. Intriguingly, these two computations were further supported by orientation-adjustment and orientation-discrimination experiments. Finally, the ablation studies in convolutional neural networks revealed that predictive coding feedback played a critical role in the center-surround inhibition in expectation. Altogether, our study reveals for the first time that expectation results in both enhancement and suppression, optimizing plausible interpretations during perception by enhancing expected and attenuating similar but irrelevant and potentially interfering representations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>expectation</kwd><kwd>center-surround inhibition</kwd><kwd>tuning sharpening</kwd><kwd>tuning shift</kwd><kwd>deep predictive coding neural network</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32271099</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xilin</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>Research Center for Brain Cognition and Human Development of Guangdong Province</institution></institution-wrap></funding-source><award-id>2024B0303390003</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Xilin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kq0pv72</institution-id><institution>Striving for the First-Class, Improving Weak Links and Highlighting Features (SIH) Key Discipline for Psychology in South China Normal University</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zhang</surname><given-names>Xilin</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01h0zpd94</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32441102</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05tewj457</institution-id><institution>Shanghai Municipal Education Commission</institution></institution-wrap></funding-source><award-id>2024AIZD014</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Expectation optimizes perception through center-surround inhibition, enhancing expected representations while suppressing similar, irrelevant ones.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Human behavior is surprisingly efficient and adaptive. Although the everyday environment is brimming with noisy and ambiguous information, our cognitive system can quickly and adeptly deduce plausible interpretations of this information by combining it with prior expectations, ultimately facilitating flexible behavioral arises (<xref ref-type="bibr" rid="bib6">Bar, 2004</xref>; <xref ref-type="bibr" rid="bib7">Bar, 2009</xref>). However, the structured manner (the profile, in other words) regarding how expectation demarcates the anticipated target from various distractors and underlying neural computations remains largely unclear. This issue is particularly important since such a profile is thought to closely reflect neural circuitry (<xref ref-type="bibr" rid="bib102">Teufel and Fletcher, 2020</xref>; <xref ref-type="bibr" rid="bib118">Watabe-Uchida et al., 2017</xref>), and therefore, offers us a unique opportunity to give insight into neural circuit level computations of expectation, thereby not only furthering our understanding of how it facilitates perception and behavior to adapt to changing environment, but also addressing a long-standing debate about its underlying neural mechanisms (<xref ref-type="bibr" rid="bib26">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib71">Press et al., 2020</xref>).</p><p>One of the central questions to this debate is about the processing of unexpected stimuli that are sufficiently novel or surprising. The sharpening models (also referred to as Bayesian theories) propose that expectations preferentially suppress neurons tuned toward the unexpected stimuli, resulting in a sharper and more selective population responses (<xref ref-type="bibr" rid="bib26">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib99">Summerfield and de Lange, 2014</xref>). This sharpening account of expectation is similar to the notion of neuronal resonance (<xref ref-type="bibr" rid="bib53">Lee and Mumford, 2003</xref>) and has been supported by neurophysiological (<xref ref-type="bibr" rid="bib11">Bell et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Kaposvari et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib85">Schwiedrzik and Freiwald, 2017</xref>), electro-/magneto-encephalogram (<xref ref-type="bibr" rid="bib2">Aitken et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Todorovic et al., 2011</xref>; <xref ref-type="bibr" rid="bib86">Sedley et al., 2016</xref>; <xref ref-type="bibr" rid="bib114">Wacongne et al., 2011</xref>), and functional magnetic resonance imaging (fMRI; <xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib4">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib97">Summerfield et al., 2008</xref>; <xref ref-type="bibr" rid="bib124">Yon et al., 2018</xref>) studies. Conversely, the cancelation models (also referred to as dampening theories) propose a dampening of neural responses reduces redundancy in the sensory system, through suppressing neurons tuned toward the expected stimulus. By canceling the expected information, the brain could highlight the processing and cognitive resources of unexpected information (<xref ref-type="bibr" rid="bib71">Press et al., 2020</xref>; <xref ref-type="bibr" rid="bib77">Richter et al., 2022</xref>). This theory has also drawn wide support from neurophysiological (<xref ref-type="bibr" rid="bib61">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib85">Schwiedrzik and Freiwald, 2017</xref>; <xref ref-type="bibr" rid="bib51">Kumar et al., 2017</xref>) and brain imaging (<xref ref-type="bibr" rid="bib12">Blakemore et al., 1998</xref>; <xref ref-type="bibr" rid="bib13">Blank and Davis, 2016</xref>; <xref ref-type="bibr" rid="bib40">Han et al., 2019</xref>; <xref ref-type="bibr" rid="bib76">Richter et al., 2018</xref>) studies. Intriguingly, although these two models explaining how expectations render perception either veridical or informative are seemingly conflicting, both could be incorporated in the framework of predictive coding models (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Lee and Mumford, 2003</xref>; <xref ref-type="bibr" rid="bib32">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib36">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib75">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib96">Summerfield and Koechlin, 2008</xref>; <xref ref-type="bibr" rid="bib126">Yuille and Kersten, 2006</xref>), which posits that the brain contains distinct neurons/units representing the best guess about the outside world (prediction units) and the discrepancy between these guesses and incoming sensory evidence (prediction error units). Several studies have proposed that the sharpening and cancelation accounts may occur in prediction and error neurons, respectively (<xref ref-type="bibr" rid="bib71">Press et al., 2020</xref>; <xref ref-type="bibr" rid="bib77">Richter et al., 2022</xref>; <xref ref-type="bibr" rid="bib36">Friston, 2005</xref>). Within this framework, anticipating what is possible or probable in the forthcoming sensory environment can be cast as a process of hierarchical Bayesian inference, in which the prediction units are more strongly weighted towards the expected rather than unexpected stimuli, while at the same time the prediction error units are selectively biased to surprising inputs. Increased gain on these surprising inputs would lead to high-fidelity representations of unexpected stimuli across prediction units. Although, so far, there has been no direct evidence for the existence of these two neuron types, and it is unclear how these two mechanisms are reconciled from different neural populations, the predictive coding framework may provide the underlying computational basis for various potential profiles of expectation.</p><p>Here, given these two mechanisms making opposite predictions about how expectation changes the neural responses of unexpected stimuli, thereby displaying different profiles of expectation, we speculated that if expectation operates by the sharpening model with suppressing unexpected information, we should observe an inhibitory zone surrounding the focus of expectation, and its profile then should display as a center-surround inhibition (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, left). If, however, expectation operates as suggested by the cancelation model with highlighting unexpected information, the inhibitory zone surrounding the focus of expectation should be eliminated, and the profile should instead display a monotonic gradient (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, right). To adjudicate between these theoretical possibilities, we manipulated the distance between the expected and unexpected stimuli in feature space to measure the profile of expectation in two psychophysical experiments (orientation was task-relevant or task-irrelevant on the orientation and spatial frequency discrimination tasks, respectively, <xref ref-type="fig" rid="fig1">Figure 1b</xref>), both of which supported the sharpening account by showing a classical center-surround. inhibition profile in orientation space, with enhanced neural responses to the expected orientation and suppressed neural responses to orientations similar to the expected orientation relative to orientations more distinct from it (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Second, using computational modeling, we showed that the behaviorally observed center-surround inhibition in expectation could be reproduced by either a sharpening of tuning curves of expected orientation (Tuning sharpening account) or a shift of tuning curves of unexpected orientations (Tuning shift account; <xref ref-type="fig" rid="fig3">Figure 3a</xref>). Third, these neural computations, consisting of both the tuning sharpening and tuning shift accounts, were further confirmed by orientation-adjustment (Figure 6) and orientation-discrimination (Figure 7) experiments. Finally, we found that a deep predictive coding neural network (DPCNN) exhibited a similar center-surround inhibition by expectation profile, both when it was trained to perform an orientation or a spatial frequency task. Most importantly, when we ablated predictive feedback, these center-surround inhibitions were eliminated in the DPCNN (Figure 8), strongly supporting the framework of predictive coding models in expectation (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Lee and Mumford, 2003</xref>; <xref ref-type="bibr" rid="bib32">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib36">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib75">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib96">Summerfield and Koechlin, 2008</xref>; <xref ref-type="bibr" rid="bib126">Yuille and Kersten, 2006</xref>). Altogether, our study reveals for the first time that expectation generates an orientation-specific enhancement and suppression profile that optimizes plausible interpretations during visual perception by boosting expected and attenuating interfering sensory representations.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli and protocols of the profile experiment.</title><p>(<bold>a</bold>) Left: the auditory cue, comprising either a low- or high-frequency tone, predicted the orientation of the first grating with equal validity in the baseline experiment. B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°. Right: in the main experiment, the low- or high-frequency tone predicted 20Â° or 70Â° (expected) orientation of the first grating with 75% validity. In the remaining 25% of trials, this orientation was chosen randomly and equally from four non-predicted orientations (30Â°, 40Â°, 50Â°, and 60Â°). There were two types of expected conditions: Expect 20Â° (E20Â°) and Expect 70Â° (E70Â°), and for both conditions, there were five possible distances in orientation space between the expected and test gratings, ranging from Î”0Â° through Î”40Â° with a step size of 10Â°. (<bold>b</bold>) In both baseline and main experiments, each trial began with an auditory cue, followed by an 1800 ms fixation interval. Then, two consecutive gratings were each presented for 150 ms and separated by a 300 ms blank interval. Participants were first asked to make a 2AFC judgment of either the orientation (clockwise or anticlockwise) or the spatial frequency (lower or higher) of the second grating relative to the first on orientation discrimination (OD, purple) and spatial frequency discrimination (SFD, blue) tasks, respectively. Then, participants were asked to make another 2AFC judgment on the tone of auditory cue, either low or high. CW: clockwise; CCW: counterclockwise; HF: higher frequency; LF: lower frequency; HT: high tone; LT: low tone. (<bold>c</bold>) Left: expectation operates by the sharpening model with suppressing unexpected information, under this configuration, the profile of expectation could display as a center-surround inhibition, with an inhibitory zone surrounding the focus of expectation. Right: expectation operates by the cancellation model with highlighting unexpected information. Under this configuration, the profile of expectation could display as a monotonic gradient, without the inhibitory zone.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig1-v1.tif"/></fig><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Results of the profile experiment.</title><p>The discrimination thresholds of OD (top) and SFD (bottom) tasks during baseline (<bold>a</bold>) and main (<bold>b</bold>) experiments. In the baseline experiment, discrimination thresholds did not differ across orientations in either OD or SFD tasks, as confirmed by non-significant one-way repeated-measures ANOVAs (all p&gt;0.18). B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â°. (<bold>c</bold>) The averaged discrimination sensitivity (<italic>DS</italic>) of each distance on OD (top) and SFD (bottom) tasks, and the best fitting Gaussian and Mexican-hat functions to these DSs across distances. In both tasks, DS varied significantly across distances (OD: F(4,92) = 3.739, p=0.010, ğœ‚<sub>p</sub><sup>2</sup>=0.140; SFD: F(4,92) = 2.822, p=0.042, ğœ‚<sub>p</sub><sup>2</sup>=0.109), and <italic>Post hoc</italic> paired <italic>t</italic> tests revealed that, for both tasks, the <italic>DSs</italic> of Î”20Â° were significantly lower than those of both Î”0Â° and Î”40Â°, consistent with the classical center-surround inhibition profile. G, Gaussian model; M, Mexican-hat model. (<bold>d</bold>) <italic>R<sup>2</sup></italic> of the best fitting Gaussian and Mexican-hat functions for individual participants in OD (top) and SFD (bottom) tasks. For both tasks, most dots located in the upper-left zone demonstrated that the Mexican-hat model was favored over the Gaussian model. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. Error bars indicate 1 SEM calculated across participants (N = 24).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2â€”figure supplement 1.</label><caption><title>Accuracies of auditory tone reports in the profile experiment.</title><p>Mean accuracies of auditory tone reports in OD (<bold>a</bold>) and SFD (<bold>b</bold>) tasks during the baseline (left) and main (right) experiments, plotted across different distances in orientation space (Î”0Â° - Î”40Â°). In both the baseline and main experiments, a one-way repeated-measures ANOVA with the distance (Î”0Â°-Î”40Â°) as within-participants factor showed that the main effect of distance was not significant for either OD (baseline: F(4,92) = 1.281, p=0.283, <italic>Î·</italic><sub>p</sub>2 = 0.053; main: F(4,92) = 2.412, p=0.082, <italic>Î·</italic><sub>p</sub>2 = 0.095) or SFD (baseline: F(4,92) = 0.631, p=0.594, <italic>Î·</italic><sub>p</sub>2 = 0.027; main: F(4,92) = 0.963, p=0.414, <italic>Î·</italic><sub>p</sub>2 = 0.040) tasks. Error bars indicate 1 SEM calculated across participants (N = 24).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig2-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Results of computational modeling.</title><p>(<bold>a</bold>) Illustration of the Tuning sharpening model (left) and the Tuning shift model (right). The Tuning sharpening model postulates that expectation sharpens the tuning of individual neurons (thick curves) towards the expected orientation, which results in a center-surround population response profile (black curve) centered at the expected orientation. The Tuning shift model postulates that expectation attracts the tuning of individual neurons (thick curves) from unexpected orientation towards the expected orientation, which also results in a center-surround population response profile. (<bold>b</bold>) The fitted discrimination thresholds on OD (left) and SFD (right) tasks in the baseline (top) and main (bottom) experiments. (<bold>c</bold>) The averaged DSs using Tuning sharpening model on OD (left) and SFD (right) tasks. (<bold>d</bold>) R<sup>2</sup> of the best fitting Gaussian and Mexican-hat functions for individual participants based on the fitted DSs using Tuning sharpening model on OD (left) and SFD (right) tasks. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. (<bold>eâ€“g</bold>) The results from the Tuning shift model, see caption for (<bold>bâ€“d</bold>) for a description of each type of graph. The amplitude <italic>A</italic> (vertical stripes) and width <italic>Ïƒ</italic> (diagonal stripes) differences between the baseline and main experiments using Tuning sharpening model in Î”0Â° (<bold>h</bold>) and Î”10Â°-Î”40Â° (<bold>i</bold>) conditions, on OD (left) and SFD (right) tasks. The location <italic>x0</italic> differences between the baseline and main experiments using Tuning shift model in Î”0Â° (<bold>j</bold>) and Î”10Â°-Î”40Â° (<bold>k</bold>) conditions, on OD (left) and SFD (right) tasks. Statistical comparisons were performed using t-tests against zero. Open symbols represent the data from each participant, and error bars indicate 1 SEM calculated across participants (N = 24). B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â° (*p&lt;0.05; **p&lt;0.005; ***p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3â€”figure supplement 1.</label><caption><title>The calculated parameters of the population responses on OD and SFD tasks.</title><p>(<bold>a</bold>) Amplitude <italic>A</italic> (vertical stripes) and Width <italic>Ïƒ</italic> (diagonal stripes) of the Tuning sharpening model for the baseline (left) and main (right) experiments in the Î”0Â° (top) and Î”10Â°-Î”40Â° (bottom) conditions. (<bold>b</bold>) Location <italic>x0</italic> of the Tuning shift model for the baseline (left) and main (right) experiments in the Î”0Â° (top) and Î”10Â°-Î”40Â° (bottom) conditions. Note that the data were presented as the bias between <italic>x0</italic> and their hypothesized channel location, i.e., 20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°. Error bars indicate 1 SEM calculated across participants (N = 24). (<bold>c</bold> and <bold>d</bold>) Computational modeling for the SFD task, see caption for (<bold>a</bold> and <bold>b</bold>) for a description of each type of graph.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3â€”figure supplement 2.</label><caption><title>Results of combined computational modeling.</title><p>(<bold>a</bold>) Illustration of the Combined model. The Combined model incorporates sharpening of the expected orientation channel (center channel) together with shifting of the unexpected orientation channels (surround channels) from unexpected toward the expected orientation, resulting in a centerâ€“surround population response profile (black curve). (<bold>b</bold>) The fitted discrimination thresholds on OD (left) and SFD (right) tasks in the baseline (top) and main (bottom) experiments. (<bold>c</bold>) The averaged DSs using Combined model on OD (left) and SFD (right) tasks. (<bold>d</bold>) R<sup>2</sup> of the best fitting Gaussian and Mexican-hat functions for individual participants based on the fitted DSs on OD (left) and SFD (right) tasks. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. The amplitude <italic>A</italic> (vertical stripes) and width <italic>Ïƒ</italic> (diagonal stripes) differences between the baseline and main experiments in Î”0Â° (<bold>e</bold>) and Î”10Â°-Î”40Â° (<bold>f</bold>) conditions, on OD (left) and SFD (right) tasks. The location <italic>x0</italic> differences between the baseline and main experiments Î”10Â°-Î”40Â° (<bold>g</bold>) conditions, on OD (left) and SFD (right) tasks. For the expected orientation (Î”0Â°), results showed that the amplitude change was significantly higher than zero on both OD (t(23) = 2.582, p=0.017, Cohenâ€™s d=0.527) and SFD (t(23) = 2.078, p=0.049, Cohenâ€™s d=0.424) tasks (<bold>e</bold>, vertical stripes); the width change was significantly lower than zero on both OD (t(23) = â€“2.438, p=0.023, Cohenâ€™s d=0.498) and SFD (t(23) = â€“2.578, p=0.017, Cohenâ€™s d=0.526) tasks (<bold>e</bold>, diagonal stripes). For unexpected orientations (Î”10Â°-Î”40Â°), however, the amplitude and width changes were not significant with zero on either OD (amplitude change: t(23) = 0.443, p=0.662, Cohenâ€™s d=0.091; width change: t(23) = â€“1.819, p=0.082, Cohenâ€™s d=0.371) or SFD (amplitude change: t(23) = 1.130, p=0.270, Cohenâ€™s d=0.231; width change: t(23) = â€“1.710, p=0.101, Cohenâ€™s d=0.349) tasks (<bold>f</bold>). In the meantime, the location shift was significantly different than zero for unexpected orientations (Î”10Â°-Î”40Â°), OD task: t(23) = 3.611, p=0.001, Cohenâ€™s d=0.737; SFD task: t(23) = 2.418, p=0.024, Cohenâ€™s d=0.493 (<bold>g</bold>). Open symbols represent the data from each participant and error bars indicate 1 SEM calculated across participants (N = 24). B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â° (*p&lt;0.05; **p&lt;0.005; ***p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig3-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Profile experiment</title><p>The profile experiment consisted of a baseline and main experiment, with the baseline experiment always preceding the main experiment. The two experiments were the same, except for the probability relationship between the auditory cue and the orientation (20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°) of the first grating. For the baseline experiment, the auditory cue, comprising either a low- (240 Hz) or high- (540 Hz) frequency tone, predicted the orientation of the first grating with equal validity (16.67%, <xref ref-type="fig" rid="fig1">Figure 1a</xref>, left). In the main experiment, this low- or high-frequency tone auditory cue predicted the orientation (20Â° or 70Â°) of the first grating with 75% validity. In the remaining 25% of trials, this orientation was chosen randomly and equally from four non-predicted orientations (30Â°, 40Â°, 50Â°, and 60Â°, <xref ref-type="fig" rid="fig1">Figure 1a</xref>, right). Thus, for each participant, there were two types of expected conditions: Expect 20Â° and Expect 70Â°, and for both conditions, there were five possible distances in orientation space between the expected and test gratings, ranging from Î”0Â° through Î”40Â° with a step size of 10Â°. Note that the matches between the tone (low- or high-frequency) of auditory cue and the expected orientation (20Â° or 70Â°) of the first grating were flipped across participants, and the order was also counterbalanced across participants. Moreover, for each participant, although the tone of auditory cue could not predict 20Â° or 70Â° orientation in the baseline experiment, the trials in the baseline experiment with the same tone that was matched with 20Â° or 70Â° orientation in the main experiment were defined as Baseline 20Â° (i.e. the baseline of Expect 20Â°) and Baseline 70Â° (i.e. the baseline of Expect 70Â°) conditions, respectively.</p><p>Both the baseline and main experiments consisted of two tasks: the orientation discrimination (OD) task and spatial frequency discrimination (SFD) task. With the two tasks occurring on different days, the order of the two tasks was counterbalanced across participants. Differently, the baseline experiment consisted of four blocks (two for OD task and the other two for SFD task), and each block had two QUEST staircases (<xref ref-type="bibr" rid="bib119">Watson and Pelli, 1983</xref>) for each of six orientations (20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°). The main experiment consisted of 2 blocks (one for OD task and the other one for SFD task), and each block had 24 QUEST staircases for the expected orientations (20Â° and 70Â°) and 2 QUEST staircases for each of unexpected orientations (30Â°, 40Â°, 50Â°, and 60Â°). Each QUEST staircase comprised 40 trials and each trial began with an auditory cue, followed by a fixation interval. Then, two gratings were presented sequentially, and participants were asked to make a two-alternative forced-choice (2AFC) judgment of either the orientation (clockwise or anticlockwise, where orientation was task-relevant) or the spatial frequency (lower or higher, where orientation was task-irrelevant) of the second grating relative to the first, on the OD and SFD tasks, respectively (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The second grating differed trial by trial from the first in either orientation (<italic>Î”Î¸Â°</italic>) or spatial frequency (<italic>Î”Î»</italic> cycles/Â°) on the OD and SFD tasks, respectively. The QUEST staircase was used to control the varied <italic>Î”Î¸Â°</italic> or <italic>Î”Î»</italic> cycles/Â° adaptively for estimating participantsâ€™ discrimination thresholds (75% correct). At the end of each trial, participants needed to report the tone (either low or high) of the auditory cue. For either OD or SFD tasks, there was no significant difference in accuracy of this reporting across different conditions in either baseline or main experiments (<xref ref-type="fig" rid="fig2s1">Figure 2â€”figure supplement 1</xref>).</p><p>In the baseline experiment, participantsâ€™ mean discrimination thresholds in Baseline 20<italic>Â°</italic> and Baseline 70<italic>Â°</italic> conditions were submitted to a one-way repeated-measures ANOVA with orientation (20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°) as a within-participants factor. Results showed that the main effect of orientation was not significant in either OD (Baseline 20Â°: F(5,115) = 0.955, p=0.431, <italic>Î·</italic><sub>p</sub>2 = 0.040; Baseline 70Â°: F(5,115) = 1.314, p=0.274, <italic>Î·</italic><sub>p</sub>2 = 0.054) or SFD (Baseline 20Â°: F(5,115) = 1.163, p=0.331, <italic>Î·</italic><sub>p</sub>2 = 0.048; Baseline 70Â°: F(5,115) = 1.593, p=0.184, <italic>Î·</italic><sub>p</sub>2 = 0.065) tasks (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), indicating that there was no significant difference in participant performance among six orientations. In other words, the tone of auditory cue in the baseline experiment was uninformative about the orientation of gratings. On both OD and SFD tasks and both two expected conditions (Expect 20Â° and Expect 70Â°), for each distance (Î”0Â°-Î”40Â°), we computed a discrimination sensitivity (<italic>DS</italic>) to quantify how much the discrimination threshold (<italic>DT</italic>) changed between baseline (<italic>DT <sub>baseline</sub></italic>) and main (<italic>DT <sub>main</sub></italic>) experiments: <italic>DS = DT <sub>baseline</sub> - DT <sub>main</sub></italic>. Because the <italic>DS</italic> from Expect 20Â° and Expect 70Â° showed a similar pattern, they were pooled together for further analysis (unless otherwise stated, we present average data from two expected conditions). The averaged <italic>DSs</italic> were submitted to a one-way repeated-measures ANOVA with the distance (Î”0Â°-Î”40Â°) as a within-participants factor. Results showed that the main effect of distance was significant in both OD (F(4,92) = 3.739, p=0.010, <italic>Î·</italic><sub>p</sub>2 = 0.140, <xref ref-type="fig" rid="fig2">Figure 2c</xref>, top) and SFD (F(4,92) = 2.822, p=0.042, <italic>Î·</italic><sub>p</sub>2 = 0.109, <xref ref-type="fig" rid="fig2">Figure 2c</xref>, bottom) tasks. To directly address the potential inhibitory zone surrounding the focus of expectation, we compared the <italic>DS</italic>s between Î”20Â° and Î”0Â°, and between Î”20Â° and Î”40Â° on each task. Post hoc paired <italic>t</italic> tests revealed that, for both tasks, the <italic>DSs</italic> of Î”20Â° were significantly lower than those of both Î”0Â° (OD task: t(23) = â€“4.263, p&lt;0.001, Cohenâ€™s <italic>d</italic>=0.870; SFD task: t(23) = â€“4.679, p&lt;0.001, Cohenâ€™s <italic>d</italic>=0.955) and Î”40Â° (OD task: t(23) = â€“2.214, p=0.037, Cohenâ€™s <italic>d</italic>=0.452; SFD task: t(23) = â€“2.694, p=0.013, Cohenâ€™s <italic>d</italic>=0.550), indicating a classical center-surround inhibition in expectation with the enhanced discriminability to the expected orientation (Î”0Â°) and decreased discriminability to orientations (Î”20Â°) similar to the expected orientation relative to orientations (Î”40Â°) more distinct from it. Intriguingly, this center-surround inhibition profile of expectation was independent of attentional modulations by task relevance of the orientation.</p><p>Subsequently, to further assess the shape of this expectation pattern, we fitted a monotonic model and a non-monotonic model to the average <italic>DS</italic>s across distances (Î”0Â°-Î”40Â°) on both OD and SFD tasks. The monotonic and nonmonotonic models were implemented as the Gaussian and Mexican-hat functions, respectively (<xref ref-type="bibr" rid="bib87">Shen et al., 2024</xref>; <xref ref-type="bibr" rid="bib117">Wang et al., 2021</xref>). To compare these two models to our data, we first computed the Akaike information criterion (AIC; <xref ref-type="bibr" rid="bib3">Akaike, 1973</xref>) and Bayesian information criterion (BIC; <xref ref-type="bibr" rid="bib84">Schwarz, 1978</xref>) with the assumption of a normal error distribution. Then, we calculated the Likelihood ratio (LR) and Bayes factor (BF) of the Mexican-hat model over the Gaussian model based on AIC (<xref ref-type="bibr" rid="bib18">Burnham and Anderson, 2002</xref>) and BIC (<xref ref-type="bibr" rid="bib115">Wagenmakers, 2007</xref>) approximation, respectively. Results showed that, in both tasks, the LR/BFs were larger than 1 (OD task: LR/BF = 2.088 Ã— 10<sup>5</sup>; SFD task: LR/BF = 1.288) and therefore strongly favored the Mexican-hat model over the Gaussian model (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Notably, we also conducted similar model comparisons for each participantâ€™s data and found that the Mexican-hat model was favored over the Gaussian model in 23 and 17 of 24 participants, for OD and SFD tasks, respectively (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). Together, these results constituted strong evidence for the center-surround inhibition profile of expectation and further indicated its independence of attentional modulations by task relevance of the orientation.</p><sec id="s2-1-1"><title>Computational models of the center-surround inhibition in expectation</title><p>Our results demonstrated the classical center-surround inhibition profile in expectation, yet it remains unclear what type of neural computations could account for this profile. We proposed that this profile could be explained by either Tuning sharpening (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, left) or Tuning shift (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, right) models. The Tuning sharpening model postulates that expectation sharpens the tuning of individual neurons (thick curves) of the expected orientation, which results in a center-surround population response profile (black curve) centered at the expected orientation. The Tuning shift model postulates that expectation attracts the tuning of individual neurons (thick curves) from unexpected orientations towards the expected orientation, which also results in a center-surround population response profile. Note that, in our study, the shift towards 20Â° was (arbitrarily) considered to be the negative value, whereas the shift towards 70Â° was thus the positive value, and unless otherwise stated, we present the average shift, that is <italic>mean shift = (shift towards 70Â° - shift towards 20Â°)/2</italic>, across conditions hereafter. For both OD and SFD tasks, to compare these two models, we fitted both the Tuning sharpening and Tuning shift models (sum of idealized channel tuning functions) to the population response profiles (the smooth negative values of discrimination thresholds) during baseline and main experiments, and measured their root mean squared deviation (RMSD) metric (<xref ref-type="bibr" rid="bib70">Pitt et al., 2002</xref>). RMSD takes the number of model parameters into account, and a smaller RMSD indicates better model fitness. Our results showed that both models exhibited robust fits to our data (<xref ref-type="fig" rid="fig3">Figure 3b and e</xref>), as indicated by high <italic>R<sup>2</sup></italic> values and comparably low RMSDs in both OD (<xref ref-type="fig" rid="fig4">Figure 4a</xref>) and SFD (<xref ref-type="fig" rid="fig4">Figure 4b</xref>) tasks. Similarly, we computed a discrimination sensitivity (<italic>DS</italic>) to quantify how much the fitted discrimination threshold (<italic>FDT</italic>) changed between baseline (<italic>FDT<sub>baseline</sub></italic>) and main (<italic>FDT<sub>main</sub></italic>) experiments: DS = <italic>FDT<sub>baseline</sub> - FDT<sub>main</sub></italic>. For both models, a similar center-surround inhibition profile of the <italic>DS</italic> was found on both OD (<xref ref-type="fig" rid="fig3">Figure 3c</xref>) and SFD (<xref ref-type="fig" rid="fig3">Figure 3f</xref>) tasks. Further model comparisons for each participantâ€™s data confirmed that the Mexican-hat model was favored over the Gaussian model on both OD (22 and 19 of 24 participants for Tuning sharpening and Tuning shift models, respectively, <xref ref-type="fig" rid="fig3">Figure 3d</xref>) and SFD (14 and 17 of 24 participants for Tuning sharpening and Tuning shift models, respectively, <xref ref-type="fig" rid="fig3">Figure 3g</xref>) tasks. These results imply that the center-surround inhibition in expectation could be reproduced by either Tuning sharpening or Tuning shift models.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>RMSDs of Tuning sharpening and Tuning shift models.</title><p>RMSDs of Tuning sharpening and Tuning shift models during the baseline (top) and main (bottom) experiments, on OD (<bold>a</bold>) and SFD (<bold>b</bold>) tasks. B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â°. Open symbols represent the data from each participant and filled colored dots represented the mean across participants. Error bars indicate 1 SEM calculated across participants (N = 24).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig4-v1.tif"/></fig><p>For each model and each task, to directly compare the tuning curve changes of both the expected (Î”0Â°) and unexpected orientations (Î”10Â°-Î”40Â°) with and without expectation, we calculated the parameters changes of tuning functions (amplitude <italic>A</italic>, location <italic>x0</italic>, and width <italic>Æ¡</italic>) for hypothesized channels between baseline and main experiments (<xref ref-type="fig" rid="fig3s1">Figure 3â€”figure supplement 1</xref>). For the Tuning sharpening model, the tuning width of each channelâ€™s tuning function is parameterized by <italic>Æ¡</italic>, while all tuning functions are evenly distributed with 10Â° spacing on the <italic>x</italic>-axis and the areas under the curves (response energy) are identical. Conversely, for the Tuning shift model, the location of each channelâ€™s tuning function is parameterized by <italic>x0</italic>, while they all share the same tuning amplitude and width. For both models, parameters were varied to obtain the minimal sum of squared errors between the population response profile and the model prediction, which is the sum of all channelsâ€™ tuning responses. For the expected orientation (Î”0Â°) of Tuning sharpening model, results showed that the amplitude change was significantly higher than zero on both OD (t(23) = 4.198, p&lt;0.001, Cohenâ€™s d=0.857) and SFD (t(23) = 3.247, p=0.004, Cohenâ€™s d=0.663) tasks (<xref ref-type="fig" rid="fig3">Figure 3h</xref>, vertical stripes); the width change was significantly lower than zero on both OD (t(23) = â€“2.235, p=0.035, Cohenâ€™s d=0.456) and SFD (t(23) = â€“3.313, p=0.003, Cohenâ€™s d=0.676) tasks (<xref ref-type="fig" rid="fig3">Figure 3h</xref>, diagonal stripes). For unexpected orientations (Î”10Â°-Î”40Â°), however, the amplitude and width changes were not significant with zero on either OD (amplitude change: t(23) = 1.948, p=0.064, Cohenâ€™s d=0.397; width change: t(23) = â€“0.412, p=0.684, Cohenâ€™s d=0.084) or SFD (amplitude change: t(23) = 1.708, p=0.101, Cohenâ€™s d=0.349; width change: t(23) = 1.273, p=0.216, Cohenâ€™s d=0.260) tasks (<xref ref-type="fig" rid="fig3">Figure 3i</xref>). For the Tuning shift model, results showed that the location shift was significantly different than zero for unexpected orientations (Î”10Â°-Î”40Â°), OD task: t(23) = 2.547, p=0.018, Cohenâ€™s d=0.520; SFD task: t(23) = 4.099, p&lt;0.001, Cohenâ€™s d=0.837 (<xref ref-type="fig" rid="fig3">Figure 3k</xref>), but not for the expected orientation Î”0Â°, OD task: t(23) = 0.993, p=0.331, Cohenâ€™s d=0.203; SFD task: t(23) = 1.750, p=0.093, Cohenâ€™s d=0.357 (<xref ref-type="fig" rid="fig3">Figure 3j</xref>). These results further confirm the Tuning sharpening and Tuning shift computations for the center-surround inhibition in expectation.</p><p>In addition, across participants, we further used the non-parametric Wilcoxon signed-rank test to compare both the R<sup>2</sup> and RMSD between two models for Baseline 20Â°, Baseline 70Â°, Expect 20Â°, and Expect 70Â° conditions during each task. Results showed that there was no significant difference between two models in Baseline 20Â° (OD task: R<sup>2</sup>: z=1.372, p=0.170, effect size: <italic>r</italic>=0.280; RMSD: z=1.200, p=0.230, effect size: <italic>r</italic>=0.245; SFD task: R<sup>2</sup>: z=0.857, p=0.391, effect size: <italic>r</italic>=0.175; RMSD: z=0.829, p=0.407, effect size: <italic>r</italic>=0.169), Baseline 70Â° (OD task: R<sup>2</sup>: z=0.371, p=0.710, effect size: <italic>r</italic>=0.076; RMSD: z=0.029, p=0.977, effect size: <italic>r</italic>=0.006; SFD task: R<sup>2</sup>: z=1.657, p=0.097, effect size: <italic>r</italic>=0.338; RMSD: z=0.686, p=0.493, effect size: <italic>r</italic>=0.140), Expect 20Â° (OD task: R<sup>2</sup>: z=0.686, p=0.493, effect size: <italic>r</italic>=0.140; RMSD: z=1.600, p=0.110, effect size: <italic>r</italic>=0.327; SFD task: R<sup>2</sup>: z=1.257, p=0.209, effect size: <italic>r</italic>=0.257; RMSD: z=1.600, p=0.110, effect size: <italic>r</italic>=0.327), or Expect 70Â° (OD task: R<sup>2</sup>: z=1.486, p=0.137, effect size: <italic>r</italic>=0.303; RMSD: z=1.686, p=0.092, effect size: <italic>r</italic>=0.344; SFD task: R<sup>2</sup>: z=0.514, p=0.607, effect size: <italic>r</italic>=0.105; RMSD: z=0.143, p=0.886, effect size: <italic>r</italic>=0.029) conditions (<xref ref-type="fig" rid="fig4">Figure 4</xref>). These results further imply that Tuning sharpening and Tuning shift models may jointly contribute to center-surround inhibition in expectation.</p><p>To further examine whether both mechanisms jointly explain the observed centerâ€“surround inhibition under expectation, we also tested a combined model that incorporates tuning sharpening for the expected orientations (Î”0Â°) and tuning shift for the unexpected orientations (Î”10Â°-Î”40Â°) with and without expectation. This model successfully captured the sharpening of the expected-orientation channel and the shift of the unexpected-orientation channels (<xref ref-type="fig" rid="fig3s2">Figure 3â€”figure supplement 2</xref>), providing further evidence that tuning sharpening and tuning shift jointly contribute to centerâ€“surround inhibition in expectation.</p></sec><sec id="s2-1-2"><title>Orientation adjustment experiment</title><p>Experimentally, to further explore the co-existence of both Tuning sharpening and Tuning shift computations in center-surround inhibition profile of expectation, participants were asked to perform a classic orientation adjustment experiment. Unlike the profile experiment (discrimination tasks), the adjustment experiment provides a direct, trial-by-trial measure of participantsâ€™ perceived orientation, capturing the full distribution of responses. This enables the construction of orientation-specific tuning curves, allowing us to detect both tuning sharpening and tuning shifts, thereby offering a more nuanced understanding of the computational mechanisms underlying expectation. The protocol of orientation adjustment experiment was similar to that of the profile experiment, except for two aspects. First, there were four possible (20Â°, 40Â°, 50Â°, and 70Â°) orientations for the first grating: 20Â°/70Â° (Î”0Â° deviated from the expected orientation) and 40Â°/50Â° (Î”20Â°/Î”30Â° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was set as a random orientation within the range of 0Â° to 90Â°, and participants were required to rotate the orientation of the second grating to match the first (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Similar to the profile experiment, no significant difference was found in tone report accuracies across distances (<xref ref-type="fig" rid="fig5s1">Figure 5â€”figure supplement 1</xref>). For both expected (Î”0Â°) and unexpected (Î”20Â°/Î”30Â°) orientations, we calculated the adjusted orientation difference between the baseline and main experiments. Results showed the adjusted difference was significantly higher than zero for unexpected orientations (0.735Â±0.308: t(19) = 2.387, p=0.028, Cohenâ€™s d=0.534, <xref ref-type="fig" rid="fig6">Figure 6a</xref>, right), but not for the expected orientation (0.143Â±0.523: t(19) = 0.274, p=0.787, Cohenâ€™s d=0.061, <xref ref-type="fig" rid="fig6">Figure 6b</xref>, left), suggesting a significant bias in the unexpected orientation representation towards the expected orientation.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Protocol and error distributions of the orientation adjustment experiment.</title><p>(<bold>a</bold>) The protocol of orientation adjustment experiment was similar to that of the profile experiment, except for two aspects. First, there were four possible (20Â°, 40Â°, 50Â°, and 70Â°) orientations for the first grating: 20Â°/70Â° (Î”0Â° deviated from the expected orientation) and 40Â°/50Â° (Î”20Â°/Î”30Â° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was set as a random orientation within the range of 0Â° to 90Â°, and participants were required to rotate the orientation of the second grating to match the first. HT: high tone; LT: low tone. (<bold>b</bold>) Three-component mixture model to the adjusted errors from baseline (left) and main (middle) experiments. In the current study, the shift towards 20Â° was (arbitrarily) considered to be the negative value (â€˜-â€™), whereas the shift towards 70Â° was thus the positive value (â€˜+â€™). The mean shift was calculated as: <italic>mean shift = (shift towards 70Â° - shift towards 20Â°)/2</italic>. The shaded error bars indicate 1 SEM calculated across participants (N = 20). B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â°.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5â€”figure supplement 1.</label><caption><title>Accuracies of auditory tone report in orientation adjustment experiments.</title><p>Mean accuracies of auditory tone report during the baseline (left) and main (right) experiment. In both the baseline and main experiments, a repeated-measures ANOVA with the expected condition (B/E20Â° vs. B/E70Â°) and orientation distance (Î”0Â° vs. Î”20/30Â°) as within-participants factors showed that the interaction between these two factors was not significant for either the baseline (F(1,19) = 3.330, p=0.084, <italic>Î·</italic><sub>p</sub>2 = 0.149) or main (F(1,19) = 1.509, p=0.234, <italic>Î·</italic><sub>p</sub>2 = 0.074) experiments. Error bars indicate 1 SEM calculated across participants (N = 20).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig5-figsupp1-v1.tif"/></fig></fig-group><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Results of the orientation adjustment experiment.</title><p>(<bold>a</bold>) The adjusted orientation difference between the baseline and main experiments in both expected (20Â°/70Â°, i.e. Î”0Â°, middle) and unexpected (40Â°/50Â°, i.e. Î”20Â°/Î”30Â°, right) conditions. B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â°. (<bold>bâ€“d</bold>) The parameter estimates difference between the baseline and main experiments in both expected (Î”0Â°, middle) and unexpected (Î”20Â°/Î”30Â°, right) orientations. The parameter estimates were obtained by fitting a three-component mixture model to adjusted errors in different conditions. (<bold>b</bold>) <italic>mu</italic> reflects the response distribution shift away from the presented grating orientation. (<bold>c</bold>) <italic>s.d</italic>. reflects precision of responses (with higher values indicating worse precision). (<bold>d</bold>) <italic>g</italic> estimates the probability that the participant produced a random response (i.e. the guess). Statistical comparisons were performed using t-tests against zero. Open symbols represent the data from each participant and error bars indicate 1 SEM calculated across participants (N = 20; *p&lt;0.05; **p&lt;0.005; ***p&lt;0.001).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6â€”figure supplement 1.</label><caption><title>Adjusted errors of orientation adjustment experiments and their parameter estimates with the three-component mixture model.</title><p>The adjusted errors in both baseline (left) and main (right) experiments for expected (<bold>a</bold>) 20Â°/70Â°, i.e. Î”0Â° and unexpected (b) 40Â°/50Â°, i.e. Î”20Â°/Î”30Â° conditions. The estimated parameter <italic>mu</italic> in both the baseline (left) and main (right) experiments for expected (<bold>c</bold>) 20Â°/70Â°, i.e. Î”0Â° and unexpected (<bold>d</bold>) 40Â°/50Â°, i.e. Î”20Â°/Î”30Â° conditions. The estimated parameter <italic>s.d</italic>. in both the baseline (left) and main (right) experiments for expected (<bold>e</bold>) 20Â°/70Â°, i.e. Î”0Â° and unexpected (<bold>f</bold>) 40Â°/50Â°, i.e. Î”20Â°/Î”30Â° conditions. The estimated parameter <italic>g</italic> in both the baseline (left) and main (right) experiments for expected (<bold>g</bold>) 20Â°/70Â°, i.e. Î”0Â° and unexpected (<bold>h</bold>) 40Â°/50Â°, i.e. Î”20Â°/Î”30Â° conditions. Error bars indicate 1 SEM calculated across participants (N = 20).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig6-figsupp1-v1.tif"/></fig></fig-group><p>Subsequently, we employed a three-component mixture model (<xref ref-type="bibr" rid="bib95">Suchow et al., 2013</xref>; <xref ref-type="bibr" rid="bib127">Zhang and Luck, 2008</xref>) to the adjusted errors from both baseline and main experiments (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This allowed us to estimate representation precision, including the mean shift (<italic>mu</italic>) and standard deviation (<italic>s.d</italic>.) of the von Mises distribution (positive values indicating rightward shift and higher values indicating lower precision, respectively), along with assessing the probability of stimulus guessing (<italic>g</italic>). Using the difference between the main and baseline experiments (<xref ref-type="fig" rid="fig6s1">Figure 6â€”figure supplement 1</xref>), we also found that the orientation representation significantly shifted for unexpected orientations (0.752Â±0.303: t(19) = 2.481, p=0.023, Cohenâ€™s d=0.555, <xref ref-type="fig" rid="fig6">Figure 6b</xref>, right), but not for the expected orientation (0.214Â±0.493: t(19) = 0.434, p=0.669, Cohenâ€™s d=0.097, <xref ref-type="fig" rid="fig6">Figure 6b</xref>, left). Conversely, participants exhibited higher orientation representation precision than baseline in the expected orientation (â€“0.973Â±0.271: t(19) = â€“3.597, p=0.002, Cohenâ€™s d=0.804, <xref ref-type="fig" rid="fig6">Figure 6c</xref>, left), but not in unexpected orientations (â€“0.390Â±0.212: t(19) = â€“1.837, p=0.082, Cohenâ€™s d=0.411, <xref ref-type="fig" rid="fig6">Figure 6c</xref>, right). Finally, we found no significant difference with zero in the rate of guessing in either expected (â€“0.0047Â±0.0554: t(19) = â€“0.849, p=0.406, Cohenâ€™s d=0.190, <xref ref-type="fig" rid="fig6">Figure 6d</xref>, left) or unexpected (â€“0.0023Â±0.0148: t(19) = â€“1.551, p=0.138, Cohenâ€™s d=0.347, <xref ref-type="fig" rid="fig6">Figure 6d</xref>, right) orientations. These results provide converging evidence supporting our hypothesis that both Tuning sharpening and Tuning shift contribute to the center-surround inhibition profile of expectation.</p></sec><sec id="s2-1-3"><title>Orientation discrimination experiment</title><p>Note that behavioral benefits in our orientation adjustment task could be due to improvements in either perceptual or decisional processes, as the expectation cue held information about both the most likely stimulus and the most likely correct response (<xref ref-type="bibr" rid="bib26">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib2">Aitken et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Kok et al., 2017</xref>). To remove this link between stimulus and response expectations and thereby avoid potential response biases induced by the cue, we designed an additional orientation discrimination experiment. The protocol of this orientation discrimination experiment was very similar to that of the orientation adjustment experiment, except for two aspects (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). First, there were three possible (20Â°, 45Â°, and 70Â°) orientations for the first grating: 20Â°/70Â° (Î”0Â° deviated from the expected orientation) and 45Â° (Î”25Â° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was 1Â°, 3Â°, 5Â°, 7Â°, and 9Â° deviated from the first grating, either clockwise or counterclockwise. Participants were asked to make a 2AFC judgment of the orientation of the second grating relative to the first, either clockwise or anticlockwise. <xref ref-type="fig" rid="fig7">Figure 7b and c</xref> show the psychometric functions for each condition. We plotted the percentage of trials in which participants indicated the orientation of the second grating that was anticlockwise or clockwise to the first for 20Â° (Baseline 20Â° and Expect 20Â°) and 70Â° (Baseline 70Â° and Expect 70Â°) conditions, respectively, as a function of the actual orientation difference between the two gratings. For each participant and each condition, the psychometric values at 10 orientation differences were fitted to a cumulative Gaussian function, and we interpolated the data to find the slope (orientation uncertainty) and PSE (point of subjective equality, which is the shift here) as an index for Tuning sharpening and Tuning shift models, respectively.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Protocol and results of the orientation discrimination experiment.</title><p>(<bold>a</bold>) The protocol of orientation discrimination experiment was similar to that of the orientation adjustment experiment, except for two aspects. First, there were three possible (20Â°, 45Â°, and 70Â°) orientations for the first grating: 20Â°/70Â° (Î”0Â° deviated from the expected orientation) and 45Â° (Î”25Â° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was 1Â°, 3Â°, 5Â°, 7Â°, and 9Â° deviated from the first grating, either clockwise (CW) or counterclockwise (CCW). Participants were asked to make a 2AFC judgment of the orientation of the second grating relative to the first, either clockwise or anticlockwise. HT: high tone; LT: low tone. Psychometric functions showing orientation judgements in each condition for Î”0Â° (<bold>b</bold>) and Î”25Â° (<bold>c</bold>). Data points averaged across participants were fit using a cumulative normal function. The abscissa refers to 10 orientation differences between the first and second gratings. The ordinate refers to the percentage of trials in which participants indicated the orientation of the second grating that was anticlockwise or clockwise to the first for expected 20Â° (left) and 70Â° (right) conditions, respectively. The slope (an index for the Tuning sharpening model), (<bold>d</bold>) and shift (an index for the Tuning shift model), (<bold>e</bold>) differences between the baseline and main experiments for expected 20Â° and 70Â° conditions. Statistical comparisons were performed using t-tests against zero. Negative: shift to the left; Positive: shift to the right. Open symbols represent the data from each participant and error bars indicate 1 SEM calculated across participants (N=18). B20Â°: Baseline 20Â°; B70Â°: Baseline 70Â°; E20Â°: Expect 20Â°; E70Â°: Expect 70Â° (*p&lt;0.05; **p&lt;0.005).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig7-v1.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7â€”figure supplement 1.</label><caption><title>Accuracies of auditory tone reports in orientation discrimination experiments.</title><p>Mean accuracies of auditory tone report during the baseline (left) and main (right) experiment. In both the baseline and main experiments, a repeated-measures ANOVA with the expected condition (B/E20Â° vs. B/E70Â°) and orientation distance (Î”0Â° vs. Î”25Â°) as within-participants factors showed that the interaction between these two factors was not significant for either the baseline (F(1,17) = 0.481, p=0.497, <italic>Î·</italic><sub>p</sub>2 = 0.028) or main (F(1,17) = 0.288, p=0.599, <italic>Î·</italic><sub>p</sub>2 = 0.017) experiments. Error bars indicate 1 SEM calculated across participants (N = 18).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig7-figsupp1-v1.tif"/></fig></fig-group><p>Similar to the orientation adjustment experiment, no significant difference was found in tone report accuracies across distances (<xref ref-type="fig" rid="fig7s1">Figure 7â€”figure supplement 1</xref>). For both expected (Î”0Â°) and unexpected (Î”25Â°) orientations, we calculated the slope and shift difference between the baseline and main experiments. Results showed the slope difference was significantly higher than zero for expected orientations (0.0250Â±0.0075: t(17) = 3.324, p=0.004, Cohenâ€™s d=0.627, <xref ref-type="fig" rid="fig7">Figure 7d</xref>, top), but not for the unexpected orientation (â€“0.0204Â±0.0113: t(17) = â€“1.812, p=0.088, Cohenâ€™s d=0.402, <xref ref-type="fig" rid="fig7">Figure 7d</xref>, bottom). Conversely, the shift difference was significantly lower than zero for the unexpected orientation (â€“0.696Â±0.287: t(17) = â€“2.423, p=0.027, Cohenâ€™s d=0.507, <xref ref-type="fig" rid="fig7">Figure 7e</xref>, bottom), but not for expected orientations (0.449Â±0.509: t(17) = 0.881, p=0.391, Cohenâ€™s d=0.452, <xref ref-type="fig" rid="fig7">Figure 7e</xref>, top). These results indicated that the expectation not only sharpened the tuning curves of neurons for the expected orientation but also attracted the tuning curves of neurons for unexpected orientations, further confirming Tuning sharpening and Tuning shift models, respectively, in the center-surround inhibition of expectation.</p></sec><sec id="s2-1-4"><title>Artificial neural networks for the center-surround inhibition in expectation</title><p>Finally, we trained a deep predictive coding neural network (DPCNN), modified from Predify (<xref ref-type="bibr" rid="bib21">Choksi and Mozafari, 2021</xref>; <xref ref-type="bibr" rid="bib69">Pang et al., 2021</xref>) to perform both the OD and SFD tasks. For both tasks, the DPCNN consisted of six feedforward encoding layers (<italic>e1-e6</italic>), five generative feedback decoding layers (<italic>d1-d5</italic>), and three fully connected (fc) layers (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). The reconstruction error (<italic>E1-E5</italic>) is computed and used for the proposed predictive coding updates (<xref ref-type="bibr" rid="bib75">Rao and Ballard, 1999</xref>), denoted by <italic>P.C</italic>. loops. Note that the updating is only applied to <italic>e1-e5,</italic> and for the last layer <italic>e6</italic>, there is no feedback. Before the layer <italic>e0,</italic> we obtained the pixel difference between the target and reference images, which was then superimposed on the channels of the reference image. This superimposed image was set as the input of the network and remained constant over timesteps. Besides, we used a feedforward encoding layer (i.e. <italic>e0</italic>) to match the number of the channels between superimposed feature maps and the pre-trained DPCNN. During the training, the last layer of the network was trained to capture the difference between the target and reference and finally obtain the classification by softmax, to model decision making in our 2AFC paradigm (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), in which participants were asked to make a 2AFC judgment of the orientation (either clockwise or anticlockwise) or the spatial frequency (either lower or higher) of the second grating (target) relative to the first (reference) in OD and SFD tasks, respectively. For both tasks, the DPCNN was independently and randomly trained 12 times, and for each distance (Î”0Â°-Î”40Â°), the training effect was defined as the accuracy difference (<italic>ACC<sub>difference</sub></italic>) between the pre- (ACC<sub>baseline</sub>) and post- (<italic>ACC<sub>trained</sub></italic>) training. Similar to our psychophysical results, on both tasks, the LR/BFs were much larger than 1 (OD task: LR/BF = 2.045 Ã— 10<sup>5</sup>, <xref ref-type="fig" rid="fig8">Figure 8d</xref>, left; SFD task: LR/BF = 5.5929, <xref ref-type="fig" rid="fig8">Figure 8h</xref>, left) and therefore strongly favored the Mexican-hat model over the Gaussian model. The model comparison based on fitting individual data advocated that the Mexican-hat model was favored over the Gaussian model in 10 and 11 of 12 training data on OD (<xref ref-type="fig" rid="fig8">Figure 8e</xref>, left) and SFD (<xref ref-type="fig" rid="fig8">Figure 8i</xref>, left) tasks, respectively. Besides, across individual data, a non-parametric Wilcoxon signed-rank test was conducted to compare the <italic>R<sup>2</sup></italic> of two models, and results significantly advocated the Mexican-hat model over the Gaussian model on both OD (z=2.197, p=0.028, effect size: <italic>r</italic>=0.634) and SFD (z=2.981, p=0.003, effect size: <italic>r</italic>=0.861) tasks. These results suggest that our DPCNN can emerge the similar center-surround inhibition by expectation on both the orientation and spatial frequency trainings.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Results of artificial neural networks.</title><p>(<bold>a</bold>) Model structure and stimulus examples for deep predictive coding neural network (DPCNN) and standard feedforward CNN, on both OD (purple) and SFD (blue) tasks. DPCNN consisted of six feedforward encoding layers (e1â€“e6), five generative feedback decoding layers (d1â€“d5), and three fully connected (fc) layers. The reconstruction error (E1â€“E5) is computed and used for the proposed predictive coding updates, denoted by <italic>P.C</italic>. loops. The CNN is the same as DPCNN but removes feedback predictive coding iterations. The accuracy of each distance during the pre- (<bold>b</bold>) and post- (<bold>c</bold>) training for DPCNN (left) and CNN (right), on the OD task. (<bold>d</bold>) The training effect (i.e. the ACC difference between pre- and post-training) of each distance in DPCNN (left) and CNN (right), and the best fitting Mexican-hat and Gaussian functions to these training effects across distances, on the OD task. M, Mexican-hat model; G, Gaussian model. (<bold>e</bold>) <italic>R<sup>2</sup></italic> of the best fitting Mexican-hat and Gaussian functions from individual data in DPCNN (left) and CNN (right) on the OD task. Open symbols represent individual data and filled colored dots represent the mean across data. Error bars indicate 1 SEM calculated across data (N = 12). (<bold>fâ€“i</bold>) The results from the SFD task, see caption for (<bold>bâ€“e</bold>) for a description of each type of graph.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107301-fig8-v1.tif"/></fig><p>Additionally, to further determine the contribution of predictive (reconstructive) feedback to center-surround inhibition in expectation, we performed ablation studies, in which we trained the same network but removed feedback predictive coding iterations (a standard feedforward CNN, that is a modified network of AlexNet <xref ref-type="bibr" rid="bib50">Krizhevsky et al., 2012</xref>). As expected, on both tasks, removing feedback leads to the disappearance of center-surround inhibition in expectation. Across individual data, there was no significant difference in the <italic>R<sup>2</sup></italic> between Mexican-hat and Gaussian models on either OD (non-parametric Wilcoxon signed-rank test: z=â€“0.314, p=0.754, effect size: <italic>r</italic>=0.091, <xref ref-type="fig" rid="fig8">Figure 8e</xref>, right) or SFD (non-parametric Wilcoxon signed-rank test: z=â€“0.356, p=0.722, effect size: <italic>r</italic>=0.103, <xref ref-type="fig" rid="fig8">Figure 8i</xref>, right) tasks. These results further confirm that the predictive coding feedback plays a critical role in producing the center-surround inhibition in expectation.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present results provide support for an attentional modulation-independent center-surround inhibition profile of expectation and further reveal its underlying neural computations. Specifically, on both OD and SFD tasks, the finest-grained discrimination performance, indexed by the lowest thresholds, of the expected orientation confirmed the previous notion that expectation had a facilitatory effect on various perceptions (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Cheadle et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Esterman and Yantis, 2010</xref>; <xref ref-type="bibr" rid="bib58">Mareschal et al., 2013</xref>; <xref ref-type="bibr" rid="bib60">McAuley and Kidd, 1998</xref>; <xref ref-type="bibr" rid="bib92">Stein and Peelen, 2015</xref>; <xref ref-type="bibr" rid="bib93">Stocker and Simoncelli, 2006</xref>). Whereas the coarser-grained discrimination performance, indexed by the higher thresholds, of orientations very similar to the expected orientation relative to orientations more distinct from the expected orientation demonstrated a classical inhibitory zone surrounding the focus of expectation (i.e. the center-surround inhibition profile, <xref ref-type="fig" rid="fig2">Figure 2</xref>). One could argue that this profile was derived from top-down attention rather than expectation. Compared to unexpected gratings with much lower validity, the expected grating with very high validity in our study, presumably, had more degree of top-down attention that has been proven to display the similar center-surround inhibition profile in orientation space by previous studies (<xref ref-type="bibr" rid="bib31">Fang and Liu, 2019</xref>; <xref ref-type="bibr" rid="bib54">Liu et al., 2023</xref>; <xref ref-type="bibr" rid="bib105">Tombu and Tsotsos, 2008</xref>; <xref ref-type="bibr" rid="bib125">Yoo et al., 2018</xref>) and computational models (<xref ref-type="bibr" rid="bib107">Tsotsos et al., 2001</xref>; <xref ref-type="bibr" rid="bib106">Tsotsos et al., 1995</xref>; <xref ref-type="bibr" rid="bib108">Tsotsos et al., 2008</xref>). In other words, our study may not examine a center-surround inhibition profile of expectation, but instead of top-down attention. It is important to note that, in our study, for each grating, participants performed the same discrimination task at threshold, measured by the QUEST staircase procedure (75% correct; <xref ref-type="bibr" rid="bib119">Watson and Pelli, 1983</xref>), which could maximally (although not completely) control the difference in top-down attention among distances. More importantly, our observed center-surround inhibition profile of expectation in orientation space was independent of attentional modulations by the task relevance of orientation (i.e. OD and SFD tasks, <xref ref-type="fig" rid="fig2">Figure 2</xref>), consistent with previous findings (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib99">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib79">Rungratsameetaweemana and Serences, 2019</xref>; <xref ref-type="bibr" rid="bib98">Summerfield and Egner, 2009</xref>; <xref ref-type="bibr" rid="bib39">Gordon et al., 2019</xref>; <xref ref-type="bibr" rid="bib100">Tal-Perry and Yuval-Greenberg, 2022</xref>; <xref ref-type="bibr" rid="bib121">Wilsch et al., 2020</xref>; <xref ref-type="bibr" rid="bib129">Zuanazzi and Noppeney, 2019</xref>), showing an independency between attention and expectation. If the center-surround inhibition profile was derived from attention rather than expectation, then we should not have observed it on the SFD task, in which the orientation was never task-relevant. Participants did not need to direct attention to this task-irrelevant feature, and therefore yielded none of the profiles in orientation space.</p><p>The center-surround inhibition profile of expectation evident in our study is consistent with what has been observed for spatial attention (<xref ref-type="bibr" rid="bib80">Schall and Hanes, 1993</xref>; <xref ref-type="bibr" rid="bib42">Hopf et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib81">Schall et al., 2004</xref>; <xref ref-type="bibr" rid="bib65">Mounts, 2000</xref>; <xref ref-type="bibr" rid="bib66">MÃ¼ller and Kleinschmidt, 2004</xref>; <xref ref-type="bibr" rid="bib67">MÃ¼ller et al., 2005</xref>; <xref ref-type="bibr" rid="bib14">Boehler et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Boehler et al., 2011</xref>), feature-based attention (<xref ref-type="bibr" rid="bib31">Fang and Liu, 2019</xref>; <xref ref-type="bibr" rid="bib54">Liu et al., 2023</xref>; <xref ref-type="bibr" rid="bib105">Tombu and Tsotsos, 2008</xref>; <xref ref-type="bibr" rid="bib125">Yoo et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Fang et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Bartsch et al., 2017</xref>; <xref ref-type="bibr" rid="bib94">StÃ¶rmer and Alvarez, 2014</xref>; <xref ref-type="bibr" rid="bib55">Loach et al., 2008</xref>), working memory (<xref ref-type="bibr" rid="bib45">Kiyonaga and Egner, 2016</xref>; <xref ref-type="bibr" rid="bib89">Shi et al., 2021</xref>; <xref ref-type="bibr" rid="bib90">Shi et al., 2022</xref>), and visual perceptual learning (<xref ref-type="bibr" rid="bib87">Shen et al., 2024</xref>), in various feature spaces. This suggests that center-surround inhibition could be a unifying principle underlying a diversity of visual representations, as previously proposed by the selective tuning model (<xref ref-type="bibr" rid="bib107">Tsotsos et al., 2001</xref>; <xref ref-type="bibr" rid="bib106">Tsotsos et al., 1995</xref>; <xref ref-type="bibr" rid="bib108">Tsotsos et al., 2008</xref>); however, the extent of the inhibitory zone varied largely across these domains and features. For example, within the orientation space, the inhibitory zone was about 20Â°, 45Â°, and 54Â° for expectation evident here, feature-based attention (<xref ref-type="bibr" rid="bib97">Summerfield et al., 2008</xref>), and visual perceptual learning (<xref ref-type="bibr" rid="bib87">Shen et al., 2024</xref>), respectively; within the feature-based attention, it was about 30Â° and 45Â° in color (<xref ref-type="bibr" rid="bib94">StÃ¶rmer and Alvarez, 2014</xref>) and motion direction (<xref ref-type="bibr" rid="bib31">Fang and Liu, 2019</xref>) spaces, respectively. These variations hint at the exciting possibility that the width of the inhibitory surround may flexibly adapt to stimulus context and task demands, ultimately facilitating our perception and behavior in a changing environment. This principle is consistent with the hybrid model of feature-based attention (<xref ref-type="bibr" rid="bib31">Fang and Liu, 2019</xref>; <xref ref-type="bibr" rid="bib54">Liu et al., 2023</xref>; <xref ref-type="bibr" rid="bib30">Fang et al., 2019</xref>), where attention is deployed adaptively to prioritize task-relevant information through feature-similarity gain which filters out the most distinctive distractors, and surround suppression which inhibits similar and confusable ones, thereby jointly shaping the attentional tuning profile. Mechanistically, the center-surround inhibition profile can be optimal to locally resolve competition between inputs that overlap in their neural representations, specifically attenuating the interference from nearby irrelevant and confusable representations that would be presumably within the same cortical map, and therefore at the largest risk to confuse the current processing. Given the presence of a well-defined map-based organization of the cerebral cortex (<xref ref-type="bibr" rid="bib28">Eickhoff et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Glasser et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">Mountcastle, 1997</xref>; <xref ref-type="bibr" rid="bib101">Tanaka, 2003</xref>; <xref ref-type="bibr" rid="bib111">Van Essen and Glasser, 2018</xref>; <xref ref-type="bibr" rid="bib116">Wandell et al., 2007</xref>), the center-surround inhibition would be beneficial across all features and therefore serves as a canonical neural computation that sharpens various cognitive processing across different domains.</p><p>Strikingly, we found that the center-surround inhibition profile of expectation observed behaviorally can be accounted for by sharpening of tuning curves of neurons of the expected orientation, as revealed by the computational model (<xref ref-type="fig" rid="fig3">Figure 3</xref>), orientation adjustment (<xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>), and orientation discrimination (<xref ref-type="fig" rid="fig7">Figure 7</xref>) experiments. These changes â€“ sharpening of tuning curves â€“ are not only in line with the sharpening hypothesis of expectation developed by previous neurophysiological (<xref ref-type="bibr" rid="bib11">Bell et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Kaposvari et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib85">Schwiedrzik and Freiwald, 2017</xref>), electro-/magneto-encephalogram (<xref ref-type="bibr" rid="bib2">Aitken et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="bib103">Todorovic et al., 2011</xref>; <xref ref-type="bibr" rid="bib86">Sedley et al., 2016</xref>; <xref ref-type="bibr" rid="bib114">Wacongne et al., 2011</xref>), and fMRI (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib4">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib97">Summerfield et al., 2008</xref>; <xref ref-type="bibr" rid="bib124">Yon et al., 2018</xref>) studies that have invoked the tuning sharpening as the neural basis of expectation-related effects (e.g. the sharpening of tuning curves facilitates fine orientation discrimination by increasing the activity difference between similar orientations), but also extend this hypothesis by identifying the same neural computation in its center-surround inhibition profile. More importantly, we further found that this profile of expectation can be accounted for by the tuning shift computation that neurons of unexpected orientations shift their spectral tuning toward the expected orientation (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig6">6</xref>, and <xref ref-type="fig" rid="fig7">Figure 7</xref>). We note that our implementation of sharpening and shift at the neuronal level serves as a conceptual model simplification, as population-level tuning, voxel-level selectivity, and behavioral adaptive outcomes may reflect different underlying neuronal mechanisms and do not necessarily align in a one-to-one fashion. Here, we stress that other potential mechanisms beyond sharpening, such as tuning shift, may also contribute to visual expectation. In accordance with expectation, several other cognitive processing tasks have also been shown to shift neuronal tuning curves or receptive fields toward the target, such as spatial (<xref ref-type="bibr" rid="bib23">Connor et al., 1997</xref>; <xref ref-type="bibr" rid="bib35">Fox et al., 2023</xref>; <xref ref-type="bibr" rid="bib46">Klein et al., 2014</xref>; <xref ref-type="bibr" rid="bib88">Sheremata and Silver, 2015</xref>; <xref ref-type="bibr" rid="bib104">Tolias et al., 2001</xref>; <xref ref-type="bibr" rid="bib112">Vo et al., 2017</xref>; <xref ref-type="bibr" rid="bib122">Womelsdorf et al., 2006</xref>) and feature-based (<xref ref-type="bibr" rid="bib24">Ã‡ukur et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Motter, 1994</xref>; <xref ref-type="bibr" rid="bib110">van Es et al., 2018</xref>) attention, as well as visual search (<xref ref-type="bibr" rid="bib106">Tsotsos et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Carrasco et al., 2004</xref>; <xref ref-type="bibr" rid="bib22">Compte and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib52">Lee et al., 1999</xref>; <xref ref-type="bibr" rid="bib68">Olshausen et al., 1993</xref>; <xref ref-type="bibr" rid="bib74">Rao and Ballard, 1997</xref>) and perceptual learning (<xref ref-type="bibr" rid="bib41">Hanson, 1959</xref>; <xref ref-type="bibr" rid="bib82">Schumacher et al., 2022</xref>; <xref ref-type="bibr" rid="bib91">Spence, 1937</xref>). Interestingly, several brain imaging studies have reported that expectation alters the baseline (<xref ref-type="bibr" rid="bib49">Kok et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Lucci et al., 2016</xref>; <xref ref-type="bibr" rid="bib109">van Ede et al., 2010</xref>) or gain (<xref ref-type="bibr" rid="bib96">Summerfield and Koechlin, 2008</xref>; <xref ref-type="bibr" rid="bib34">Foley et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Kok et al., 2016</xref>; <xref ref-type="bibr" rid="bib113">Voss et al., 2008</xref>) of neurons in visual areas, consistent with a classical hypothesis, that is the labeled-line theories of visual information processing (<xref ref-type="bibr" rid="bib1">Adrian and Matthews, 1927</xref>; <xref ref-type="bibr" rid="bib8">Barlow, 1972</xref>; <xref ref-type="bibr" rid="bib25">David et al., 2008</xref>; <xref ref-type="bibr" rid="bib27">Doetsch, 2000</xref>; <xref ref-type="bibr" rid="bib59">Marr, 1982</xref>), which posits that neurons in sensory cortex act as labeled lines with fixed tuning properties that encode input features consistently, regardless of task demands. However, this theory does not account for either tuning curve sharpening or tuning curve shifts of sensory neurons induced by expectation in our study. These changes we observed in the spectral tuning profiles of sensory neurons, conversely, are not only strongly supported by the matched filter hypothesis that neurons could act as matched filters and reshape or shift their tuning to match the target exactly (<xref ref-type="bibr" rid="bib25">David et al., 2008</xref>), but also compatible with both proposals from computational models (<xref ref-type="bibr" rid="bib106">Tsotsos et al., 1995</xref>; <xref ref-type="bibr" rid="bib22">Compte and Wang, 2006</xref>) and Kalman filtering schemes for the signal detection (<xref ref-type="bibr" rid="bib74">Rao and Ballard, 1997</xref>).</p><p>Although our study succeeded in linking the center-surround inhibition profile of expectation directly with the response of sensory neurons whose tuning properties make them optimal for demarcating the expected information from various unexpected information, we cannot deny a potential contribution from other cognitive processes, such as decision making. Indeed, previous studies have indicated that expectations primarily influence decisions by modulating post-perceptual stages of information processing (<xref ref-type="bibr" rid="bib99">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib5">Bang and Rahnev, 2017</xref>; <xref ref-type="bibr" rid="bib38">Gold and Stocker, 2017</xref>; <xref ref-type="bibr" rid="bib78">Rungratsameetaweemana et al., 2018</xref>) or modulate interactions between lower sensory and higher decision areas (<xref ref-type="bibr" rid="bib34">Foley et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Rahnev et al., 2011</xref>). In addition, these changes in the spectral tuning profiles of sensory neurons evident here derive mainly from psychophysics and computational models. To fully understand how changes in sensory responses contribute to both expectation and its center-surround inhibition profile, further work is needed using neurophysiological techniques or ultra-high field fMRI to explore the locus of events responsible for expectation-induced changes, the identity of neurons that undergo these changes, their patterns of connections, their interactions with higher decision processing, and underlying synaptic bases, especially for our observed shifts in unexpected orientation tunings.</p><p>In addition, the emerged center-surround inhibition of expectation in the pretrained DPCNN is not only in line with previous studies and theories that interpret expectation within the predictive coding framework (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Lee and Mumford, 2003</xref>; <xref ref-type="bibr" rid="bib32">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="bib36">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib75">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib96">Summerfield and Koechlin, 2008</xref>; <xref ref-type="bibr" rid="bib126">Yuille and Kersten, 2006</xref>), but also adds strong evidence supporting artificial neural networksâ€™ potential to perform various human-like representations, such as visual perceptual learning (<xref ref-type="bibr" rid="bib87">Shen et al., 2024</xref>; <xref ref-type="bibr" rid="bib57">Manenti et al., 2023</xref>; <xref ref-type="bibr" rid="bib120">Wenliang and Seitz, 2018</xref>) and hierarchical coding (<xref ref-type="bibr" rid="bib10">Bashivan et al., 2019</xref>), face processing (<xref ref-type="bibr" rid="bib128">Zhou et al., 2022</xref>), contour integration (<xref ref-type="bibr" rid="bib16">Boutin et al., 2021</xref>), and the perception of illusory contours (<xref ref-type="bibr" rid="bib69">Pang et al., 2021</xref>). More importantly, our ablation studies further confirm a critical role of the predictive coding feedback in producing the center-surround inhibition in expectation (<xref ref-type="fig" rid="fig8">Figure 8</xref>). Although our similarities between artificial neural networks and humans were mostly qualitative, the artificial neural network can provide new ways of studying expectation from behavior to physiology, serving as a test bed for various theories and assisting in generating predictions for physiological studies.</p><p>In sum, our study provides, to the best of our knowledge, the first evidence for a center-surround inhibition profile of expectation and how it is supported by not only changes in the tuning curves of sensory neurons but also the predictive coding framework, leading the way towards diversifying models or theories and taking a significant step in unraveling the neuronal computations underlying expectation, or, more generally, top-down processing.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>A total of 24 healthy human adults (16 females, 19â€“26 years old) were involved in the study. All of them participated in the profile experiments, 20 and 18 of them participated in the orientation adjustment and orientation discrimination experiments, respectively. The sample size was determined based on previous studies investigating visual expectation (<xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Kok et al., 2017</xref>). All participants had normal or corrected-to-normal vision, were right-handed, and were naive to the purpose of the experiments. They all provided written informed consent for participation and publication. The procedures and protocols were approved by the Human Participants Review Committee of the School of Psychology at South China Normal University and were conducted in accordance with the Declaration of Helsinki.</p></sec><sec id="s4-2"><title>Apparatus</title><p>The experiments were conducted in a dark, acoustically shielded room. Visual stimuli were displayed on an IIYAMA color graphic monitor (model: HM204DT; refresh rate: 60 Hz; resolution: 1280Ã—1024; size: 22 inches) at a viewing distance of 57 cm. Participantsâ€™ head position was stabilized using a chin rest.</p></sec><sec id="s4-3"><title>Experimental stimuli</title><p>Visual stimuli were two consecutive sinusoidal grating stimuli (1.0 contrast, random phase, radius 10Â°), which were generated using MATLAB (MathWorks, Natick, MA) in conjunction with the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib17">Brainard and Vision, 1997</xref>), and displayed centrally on the gray background (11.196 cd/m<sup>2</sup>). A white fixation point (radius 0.278Â°) was always presented at the center of the screen throughout the experiment. The auditory cue consisted of two pure tones (240 Hz and 540 Hz), presented over earphones.</p></sec><sec id="s4-4"><title>Experimental design and statistical analysis</title><sec id="s4-4-1"><title>Profile experiment</title><sec id="s4-4-1-1"><title>Experimental design</title><p>The profile experiment consisted of baseline and main experiments, and the baseline experiment always preceded the main experiment. The two experiments were the same, except for the predicting probability relationship between the auditory cue and the orientation (20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°) of the first grating. For the baseline experiment, the auditory cue, comprising either a low- (240 Hz) or high- (540 Hz) frequency tone, predicted the orientation of the first grating with equal validity (16.67%, <xref ref-type="fig" rid="fig1">Figure 1a</xref>, left). In the main experiment, this low- or high-frequency tone auditory cue predicted the orientation (20Â° or 70Â°) of the first grating with 75% validity. In the remaining 25% of trials, this orientation was chosen randomly and equally from four non-predicted orientations (30Â°, 40Â°, 50Â°, and 60Â°, <xref ref-type="fig" rid="fig1">Figure 1a</xref>, right). Thus, for each participant, there were two types of expected conditions: Expect 20Â° and Expect 70Â°, and for both conditions, there were five possible distances in orientation space between the expected and test gratings, ranging from Î”0Â° through Î”40Â° with a step size of 10Â°. Note that the matches between the tone (low- or high-frequency) of auditory cue and the expected orientation (20Â° or 70Â°) of the first grating were flipped across participants, and the order was also counterbalanced across participants. For each participant, although the tone of auditory cue could not predict 20Â° or 70Â° orientation in the baseline experiment, whose trials with the same tone that was matched with 20Â° or 70Â° orientation in the main experiment, were defined as Baseline 20Â° (i.e. the baseline of Expect 20Â°) and Baseline 70Â° (i.e. the baseline of Expect 70Â°) conditions, respectively.</p><p>Both the baseline and main experiments consisted of two tasks: the orientation discrimination (OD) task and spatial frequency discrimination (SFD) task, with the two tasks occurring on different days; the order of the two tasks was counterbalanced across participants. Differently, the baseline experiment consisted of four blocks (two for OD task and the other two for SFD task), and each block had two QUEST staircases (<xref ref-type="bibr" rid="bib119">Watson and Pelli, 1983</xref>) for each of six orientations (20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°). The main experiment consisted of two blocks (one for OD task and the other one for SFD task), and each block had 24 QUEST staircases for the expected orientations (20Â° and 70Â°) and two QUEST staircases for each of unexpected orientations (30Â°, 40Â°, 50Â°, and 60Â°). Each QUEST staircase comprised 40 trials, and on each trial, a low- (240 Hz) or a high- (540 Hz) frequency tone (i.e. the auditory cue) was randomly and equally presented for 200 ms, followed by an 1800ms fixation interval. Then, two consecutive gratings were each presented for 150 ms and separated by a 300-ms blank interval (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Participants were first asked to make a two-alternative forced-choice (2AFC) judgment of either the orientation (clockwise or anticlockwise, where orientation was task-relevant) or the spatial frequency (lower or higher, where orientation was task-irrelevant) of the second grating relative to the first, on the OD and SFD tasks, respectively. Then, participants were required to make another 2AFC judgment on tone of the auditory cue, either low or high. In the baseline experiment, for both OD and SFD tasks, the orientation of the first grating was chosen randomly and equally from 20Â°, 30Â°, 40Â°, 50Â°, 60Â°, and 70Â°, while its spatial frequency was fixed at 0.9 cycles/Â°. The second grating differed slightly from the first in terms of both orientation and spatial frequency. Differently, for the OD task, its orientation difference (<italic>Î”Î¸Â°</italic>, where orientation was task-relevant) varied trial by trial and was controlled by the QUEST staircase to estimate participantsâ€™ OD thresholds (75% correct), while its spatial frequency difference was set at 0.06 cycle/Â°; for the SFD task, its spatial frequency difference (<italic>Î”Î»</italic> cycles/Â°, where orientation was task-irrelevant) varied trial by trial and was controlled by the QUEST staircase to estimate participantsâ€™ SFD thresholds (75% correct), while its orientation difference was set at 4.8Â° based on pretest data. Similarly, for each participant, the discrimination threshold obtained during the baseline experiment was used to set the undiscriminated feature difference (i.e. the spatial frequency and orientation for OD and SFD tasks, respectively) during the main experiment, to make the stimuli as similar as possible in both contexts.</p></sec><sec id="s4-4-1-2"><title>Model fitting and comparison</title><p>In both OD and SFD tasks, for two expected conditions and each distance (i.e. Î”0Â° - Î”40Â°), we computed a discrimination sensitivity (<italic>DS</italic>) to quantify how much the discrimination threshold (<italic>DT</italic>) changed between baseline (<italic>DT<sub>baseline</sub></italic>) and main (<italic>DT<sub>main</sub></italic>) experiments: DS = <italic>DT<sub>baseline</sub> - DT<sub>main</sub></italic>. Because the <italic>DS</italic> from two expected conditions (Expect 20Â° and Expect 70Â°) showed a similar pattern, they were pooled together for further analysis (unless otherwise stated, we present average data from two expected conditions). During both tasks, for each participant, a monotonic model and a non-monotonic model to the averaged <italic>DS</italic> were fitted. The monotonic and non-monotonic models were implemented as the Gaussian and Mexican-hat (i.e. a negative second derivative of a Gaussian function) functions (<xref ref-type="bibr" rid="bib87">Shen et al., 2024</xref>; <xref ref-type="bibr" rid="bib117">Wang et al., 2021</xref>), respectively, as follows:</p><p>Gaussian function: <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:msqrt><mml:mn>2</mml:mn><mml:mi>Ï€</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mi>w</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$y=y0+\frac{2A}{W\sqrt{2\pi }}e^{- 2\left (\frac{x}{w}\right)^{2}}$\end{document}</tex-math></alternatives></inline-formula></p><p>Mexican-hat function: <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mn>3</mml:mn><mml:mi>m</mml:mi></mml:msqrt><mml:msup><mml:mi>Ï€</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$y=\frac{2H}{\sqrt{3m}\pi ^{\frac{1}{4}}}e^{\frac{- x^{2}}{2m^{2}}}\left (1- \frac{x^{2}}{m^{2}}\right)+y1$\end{document}</tex-math></alternatives></inline-formula></p><p>where <italic>y</italic> is the measured <italic>DS</italic>, <italic>x</italic> is the distance; <italic>w</italic>, <italic>A</italic>, and <italic>y0</italic> are the three parameters controlling the shape of the Gaussian function; <italic>m</italic>, <italic>H</italic>, and <italic>y1</italic> are three parameters controlling the shape of the Mexican-hat function. To compare these two models to our data, we first computed the Akaike information criterion (<italic>AIC</italic>) (<xref ref-type="bibr" rid="bib3">Akaike, 1973</xref>) and Bayesian information criterion (<italic>BIC</italic>) (<xref ref-type="bibr" rid="bib84">Schwarz, 1978</xref>), with the assumption of a normal error distribution as follows:<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>K</mml:mi><mml:mo>âˆ’</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle AIC=Nln\left (\frac{RSS}{N}\right)+2K+\frac{2K\left (K+1\right)}{N- K- 1}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ2"><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>K</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle BIC=Nln\left (\frac{RSS}{N}\right)+Kln\left (N\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>N</italic> is the number of observations, <italic>K</italic> is the number of free parameters, and <italic>RSS</italic> is residual sum of squares (<xref ref-type="bibr" rid="bib72">Raftery, 1999</xref>). Then, we further calculated the likelihood ratio (<italic>LR</italic>) and Bayes factor (<italic>BF</italic>) of the non-monotonic models (Mexican-hat) over monotonic model (Gaussian) based on <italic>AIC</italic> (<xref ref-type="bibr" rid="bib18">Burnham and Anderson, 2002</xref>) and <italic>BIC</italic> (<xref ref-type="bibr" rid="bib115">Wagenmakers, 2007</xref>) approximation, respectively, as follows;<disp-formula id="equ3"><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle LR=e^{\left (\frac{AIC_{G}- AIC_{M}}{2}\right)}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ4"><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>âˆ’</mml:mo><mml:mi>B</mml:mi><mml:mi>I</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle BF=e^{\left (\frac{BIC_{G}- BIC_{M}}{2}\right)}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>AIC<sub>G</sub></italic> and <italic>BIC<sub>G</sub></italic> are for the Gaussian model and <italic>AIC<sub>M</sub></italic> and <italic>BIC<sub>M</sub></italic> are for Mexican-hat models.</p></sec></sec></sec><sec id="s4-5"><title>Computational models of the center-surround inhibition in expectation</title><p>Prior to initiating model fitting, for both OD and SFD tasks, we first transformed the negative values of thresholds during baseline and main experiments into smooth population response profiles using linear interpolation, respectively. Subsequently, we fitted two candidate models, namely Tuning sharpening model and Tuning shift model (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), to these population response profiles for each participant. In both models, the idealized tuning function for each channel was defined by the Gaussian functions:<disp-formula id="equ5"><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>âˆ—</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msup><mml:mi>Ïƒ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle f\left (x\right)=A\ast e^{\frac{- \left (x- x0\right)^{2}}{\sigma ^{2}}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>x</italic> is the grating orientation, <italic>A</italic> is the amplitude of tuning function, <italic>x0</italic> is the location, and <italic>Æ¡</italic> is the width. Six and five tuning channels were hypothesized for data in baseline and main experiments, respectively. For the Tuning sharpening model, the tuning width of each channelâ€™s tuning function is parameterized by <italic>Æ¡</italic>, while all tuning functions are evenly distributed with 10Â° spacing on the x-axis and the areas under the curves (response energy) are identical. Conversely, for the Tuning shift model, the location of each channelâ€™s tuning function is parameterized by <italic>x0</italic>, while they all share the same tuning amplitude and width. The parameter <italic>x0</italic> was constrained within Â± 5Â° of the grating orientation limits, ranging from 15Â° to 75Â° during the baseline experiment, 15Â° to 65Â° and 25Â° to 75Â° for expected 20Â° and expected 70Â° conditions, respectively, during the main experiment. The parameter <italic>Ïƒ</italic> was set within the range of 0.01â€“200 to ensure the comparable goodness of fit. For both models, parameters were varied to obtain the minimal sum of squared errors between the population response profile and the model prediction, which is the sum of all channelsâ€™ tuning responses. To statistically compare the two models, for both orientation and SF discrimination tasks, we computed the root mean squared deviation (RMSD; <xref ref-type="bibr" rid="bib70">Pitt et al., 2002</xref>) of the two fitted models for each participant during baseline and main experiments:<disp-formula id="equ6"><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:msqrt></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle {\rm RMSD}=\sqrt{\frac{SSE}{\left (N- K\right)}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>SSE</italic> is the sum of squared errors. <italic>N</italic> is the number of data points (i.e. 51 and 61), and <italic>K</italic> is the number of model parameters.</p></sec><sec id="s4-6"><title>Orientation adjustment experiment</title><sec id="s4-6-1"><title>Experimental design</title><p>The protocol of orientation adjustment experiment was similar to that of the profile experiment, except for two aspects. First, there were four possible (20Â°, 40Â°, 50Â°, and 70Â°) orientations for the first grating: 20Â°/70Â° (Î”0Â° deviated from the expected orientation) and 40Â°/50Â° (Î”20Â°/Î”30Â° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was set as a random orientation within the range of 0Â° to 90Â°, and participants were required to rotate the orientation of the second grating to match the first (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Each participant completed 8 blocks of 48 trials in the baseline experiment and 16 blocks of 48 trials in the main experiment.</p></sec><sec id="s4-6-2"><title>Modeling response error</title><p>Response error was measured as the angular difference between the orientation of the first grating and the adjusted orientation of the second grating, such that errors ranged from 0Â° (a perfect response) to Â± 90Â° (a maximally imprecise response). To evaluate performance, we categorized the response errors for each participant according to different conditions and modeled their distributions as a three-component mixture model (<xref ref-type="bibr" rid="bib95">Suchow et al., 2013</xref>). This model comprised a von Mises distribution (<inline-formula><alternatives><mml:math id="inf3"><mml:mi>âˆ…</mml:mi></mml:math><tex-math id="inft3">\begin{document}$\varnothing $\end{document}</tex-math></alternatives></inline-formula>) corresponding to trials in which the grating orientation was encoded and a uniform distribution (<inline-formula><alternatives><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft4">\begin{document}$p_{g}$\end{document}</tex-math></alternatives></inline-formula>) accounting for the probability of random guessing without encoding (<xref ref-type="bibr" rid="bib127">Zhang and Luck, 2008</xref>):<disp-formula id="equ7"><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Î¸</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>âˆ’</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>âˆ…</mml:mi><mml:mrow><mml:mi>Î¼</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï€</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle p\left (\theta \right)=\left (1- p_{g}\right)\varnothing _{\mu ,k}+p_{g}\left (\frac{1}{2\pi }\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf5"><mml:mi>Î¸</mml:mi></mml:math><tex-math id="inft5">\begin{document}$\theta $\end{document}</tex-math></alternatives></inline-formula> is the adjusted orientation value, <inline-formula><alternatives><mml:math id="inf6"><mml:mi>âˆ…</mml:mi></mml:math><tex-math id="inft6">\begin{document}$\varnothing $\end{document}</tex-math></alternatives></inline-formula> denotes the Von Mises distribution with mean <italic>Î¼</italic> and shape parameter <inline-formula><alternatives><mml:math id="inf7"><mml:mi>k</mml:mi></mml:math><tex-math id="inft7">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft8">\begin{document}$p_{g}$\end{document}</tex-math></alternatives></inline-formula> represents a uniform distribution. Specifically, the von Mises probability density function for the angle <inline-formula><alternatives><mml:math id="inf9"><mml:mi>x</mml:mi></mml:math><tex-math id="inft9">\begin{document}$x$\end{document}</tex-math></alternatives></inline-formula> is given by:<disp-formula id="equ8"><alternatives><mml:math id="m8"><mml:mi>âˆ…</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>âˆ¨</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>âˆ’</mml:mo><mml:mi>Î¼</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>Ï€</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="t8">\begin{document}$$\displaystyle \varnothing \left (x\vee \mu ,k\right)=\frac{e^{kcos\left (x- \mu \right)}}{2\pi I_{0}\left (k\right)}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft10">\begin{document}$I_{0}\left (k\right)$\end{document}</tex-math></alternatives></inline-formula> is the modified Bessel function of the first kind of order 0, with this scaling constant chosen so that the distribution sums to unity:</p><p><inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>âˆ«</mml:mo><mml:mrow><mml:mo>âˆ’</mml:mo><mml:mi>Ï€</mml:mi></mml:mrow><mml:mrow><mml:mi>Ï€</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>Ï€</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$\int _{- \pi }^{\pi }e^{kcosx}\,dx=2\pi I_{0}\left (k\right)$\end{document}</tex-math></alternatives></inline-formula></p><p>Here, we obtained maximum likelihood estimates for 3 parameters: (1) the systematic shift of von Mises distribution (<italic>mu</italic>), which reflects distribution shift away from the target grating orientation; (2) the dispersion of the von Mises distribution (<italic>s.d.=</italic><inline-formula><alternatives><mml:math id="inf12"><mml:msqrt><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msqrt></mml:math><tex-math id="inft12">\begin{document}$\sqrt{1/k}$\end{document}</tex-math></alternatives></inline-formula>), which reflects response precision or resolution of representation; and (3) the height of the uniform distribution (<italic>g</italic>), which reflects the probability of guessing.</p></sec></sec><sec id="s4-7"><title>Orientation discrimination experiment</title><sec id="s4-7-1"><title>Experimental design</title><p>The protocol of orientation discrimination experiment was very similar to that of the orientation adjustment experiment, except for two aspects (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). First, there were three possible (20Â°, 45Â°, and 70Â°) orientations for the first grating: 20Â°/70Â° (Î”0Â° deviated from the expected orientation) and 45Â° (Î”25Â° deviated from the expected orientation). Second, in both baseline and main experiments, the second grating was 1Â°, 3Â°, 5Â°, 7Â°, and 9Â° deviated from the first grating, either clockwise or counterclockwise. Participants were asked to make a 2AFC judgment of the orientation of the second grating relative to the first, either clockwise or anticlockwise. Each participant completed 10 blocks of 120 trials in the baseline experiment, and 20 blocks of 160 trials in the main experiment.</p></sec></sec><sec id="s4-8"><title>Data fitting and analysis</title><p>We first constructed a psychometric function for each condition shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. We plotted the percentage of trials in which participants indicated the orientation of the second grating that was anticlockwise or clockwise to the first for 20Â° (Baseline 20Â° and Expect 20Â°) and 70Â° (Baseline 70Â° and Expect 70Â°) conditions, respectively, as a function of the real orientation difference between two gratings. For each participant and each condition, the psychometric values at ten orientation differences were fitted to a cumulative Gaussian using Bayesian inference, implemented in the <italic>Psignifit toolbox</italic> for Matlab (Version 4; <xref ref-type="bibr" rid="bib83">SchÃ¼tt et al., 2016</xref>), and we interpolated the data to find the slope (orientation uncertainty) and PSE (point of subjective equality, which is the shift here) as an index for Tuning sharpening and Tuning shift models, respectively.</p></sec><sec id="s4-9"><title>Artificial neural networks for the center-surround inhibition in expectation</title><p>We trained a deep predictive coding neural network (DPCNN), modified from Predify (<xref ref-type="bibr" rid="bib21">Choksi and Mozafari, 2021</xref>; <xref ref-type="bibr" rid="bib69">Pang et al., 2021</xref>) to perform both the OD and SFD tasks. Relative to the reference, on the OD task, DPCNN was trained to classify whether the target was tilted clockwise or counterclockwise; whereas on the SFD task, it was trained to classify whether the target had lower or higher spatial frequency. For both tasks, the DPCNN consisted of six feedforward encoding layers (<italic>e1-e6</italic>), five generative feedback decoding layers (<italic>d1-d5</italic>), and three fully connected (fc) layers (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). The reconstruction error (<italic>E1-E5</italic>) is computed and used for the proposed predictive coding updates (<xref ref-type="bibr" rid="bib75">Rao and Ballard, 1999</xref>), denoted by <italic>P.C</italic>. loops. Note that the updating is only applied to <italic>e1-e5,</italic> and for the last layer <italic>e6</italic>, there is no feedback. Before the layer <italic>e0,</italic> we obtained the pixel difference between the target and reference images, which was then superimposed on the channels of the reference image. This superimposed image was set as the input of the network and remained constant over timesteps. Besides, we used a feedforward encoding layer (i.e. <italic>e0</italic>) to match the number of the channels between superimposed feature maps and the pre-trained DPCNN. Additionally, to further determine the contribution of predictive coding framework to center-surround inhibition in expectation, we also trained the same network but removed feedback predictive coding iterations (a standard feedforward CNN, i.e. a modified network of AlexNet <xref ref-type="bibr" rid="bib50">Krizhevsky et al., 2012</xref>). Note that all these architects were built to mimic our hypothesis of the visual pathway involved in expectation (<xref ref-type="bibr" rid="bib26">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib71">Press et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Kok et al., 2012</xref>; <xref ref-type="bibr" rid="bib99">Summerfield and de Lange, 2014</xref>; <xref ref-type="bibr" rid="bib98">Summerfield and Egner, 2009</xref>) and could learn a lower-dimensional latent representation of a high-dimensional input space (<xref ref-type="bibr" rid="bib44">Kingma and Welling, 2013</xref>), similar to prior-based low-light image enhancement (<xref ref-type="bibr" rid="bib123">Wu et al., 2025</xref>). During the training, the last layer was trained to capture the difference between the target and reference and finally obtain the classification by softmax, to model decision making in our 2AFC paradigm (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), in which participants were asked to make a 2AFC judgment of the orientation (either clockwise or anticlockwise) or the spatial frequency (either lower or higher) of the second grating (target) relative to the first (reference) in OD and SFD tasks, respectively. Moreover, for the OD task, the orientation difference between the target and reference in the network was set to 5Â°; for the SFD task, the spatial wavelength difference between them was set to 0.5.</p><p>For each task, both the DPCNN and CNN were independently and randomly trained 12 times. For each time, the trained orientation was chosen randomly from 0Â° to 180Â°; the 9 test gratings were 0Â°, Â±10Â°, Â±20Â°, Â±30Â°, and Â± 40Â° deviated (clockwise and counterclockwise) from the trained orientation. All grating stimuli (phase: random) were centered on 227Ã—227-pixel images with gray background. To improve the robustness of our model, we trained the network on all combinations of several parameters: contrast (0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, and 0.8), SD of the Gaussian additive noise (5, 25, and 45), and spatial wavelength (5, 10, 15, 20, 25, 30, 40, 50, 60, and 80 pixels) for the OD task; contrast ranging from 0.1 through 0.8 with a step size of 0.05 and SD of the Gaussian additive noise ranging from 3 through 60 with a step size of 3 for the SFD task. For each training, there were thus a total of 840 images; 600 images were the training set and the other 240 images were the test set. For both OD and SFD tasks, during the training set, there were 480 images for the expected orientation (Î”0Â°) and 30 images for each of unexpected orientations (Î”10Â°, Î”20Â°, Î”30Â°, and Î”40Â°); during the test set, there were 48 images for each of distances (Î”0Â°- Î”40Â°). For each distance, the training effect was defined as the accuracy difference (<italic>ACC<sub>difference</sub></italic>) between the pre- (ACC<sub>baseline</sub>) and post- (<italic>ACC<sub>trained</sub></italic>) training.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Investigation, Methodology, Writing â€“ review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Investigation, Methodology, Writing â€“ review and editing</p></fn><fn fn-type="con" id="con4"><p>Investigation</p></fn><fn fn-type="con" id="con5"><p>Funding acquisition, Methodology</p></fn><fn fn-type="con" id="con6"><p>Methodology, Writing â€“ review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Funding acquisition, Writing â€“ original draft, Project administration, Writing â€“ review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>A total of 24 healthy human adults (16 females, 19-26 years old) were involved in the study. All of them participated in the profile experiments, 20 and 18 of them participated in the orientation adjustment and orientation discrimination experiments, respectively. The sample size was determined based on previous studies investigating visual expectation. All participants had normal or corrected-to-normal vision, were right-handed, and were naÃ¯ve to the purpose of the experiments. They all provided written informed consent for participation and publication. The procedures and protocols were approved by the Human Participants Review Committee of the School of Psychology at South China Normal University and were conducted in accordance with the Declaration of Helsinki.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Supplementary materials containing all additional data, analyses, and supporting results for the study.</title></caption><media xlink:href="elife-107301-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-107301-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The datasets and codes for this study are available at Open Science Framework <ext-link ext-link-type="uri" xlink:href="https://osf.io/5tj8c/">https://osf.io/5tj8c/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Center-surround inhibition in expectation and its underlying computational and artificial neural net</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/5tj8c">5tj8c</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We acknowledge the participants for their contribution to this study. XZ was supported by the National Natural Science Foundation of China (32271099), the Research Center for Brain Cognition and Human Development of Guangdong Province (2024B0303390003), and the Striving for the First-Class, Improving Weak Links and Highlighting Features (SIH) Key Discipline for Psychology in South China Normal University. R-YZ was supported by the National Natural Science Foundation of China (32441102) and the Shanghai Municipal Education Commission (2024AIZD014).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adrian</surname><given-names>ED</given-names></name><name><surname>Matthews</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1927">1927</year><article-title>The action of light on the eye: Part I. The discharge of impulses in the optic nerve and its relation to the electric changes in the retina</article-title><source>The Journal of Physiology</source><volume>63</volume><fpage>378</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1927.sp002410</pub-id><pub-id pub-id-type="pmid">16993896</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>F</given-names></name><name><surname>Turner</surname><given-names>G</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prior expectations of motion direction modulate early sensory processing</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>6389</fpage><lpage>6397</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0537-20.2020</pub-id><pub-id pub-id-type="pmid">32641404</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaike</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Maximum likelihood identification of Gaussian autoregressive moving average models</article-title><source>Biometrika</source><volume>60</volume><fpage>255</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1093/biomet/60.2.255</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name><name><surname>Kohler</surname><given-names>A</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus predictability reduces responses in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>2960</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3730-10.2010</pub-id><pub-id pub-id-type="pmid">20181593</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>JW</given-names></name><name><surname>Rahnev</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stimulus expectation alters decision criterion but not sensory signal in perceptual decision making</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>17072</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-16885-2</pub-id><pub-id pub-id-type="pmid">29213117</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual objects in context</article-title><source>Nature Reviews. Neuroscience</source><volume>5</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1038/nrn1476</pub-id><pub-id pub-id-type="pmid">15263892</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The proactive brain: memory for predictions</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>364</volume><fpage>1235</fpage><lpage>1243</lpage><pub-id pub-id-type="doi">10.1098/rstb.2008.0310</pub-id><pub-id pub-id-type="pmid">19528004</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>Single units and sensation: A neuron doctrine for perceptual psychology?</article-title><source>Perception</source><volume>1</volume><fpage>371</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1068/p010371</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartsch</surname><given-names>MV</given-names></name><name><surname>Loewe</surname><given-names>K</given-names></name><name><surname>Merkel</surname><given-names>C</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention to color sharpens neural population tuning via feedback processing in the human visual cortex hierarchy</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>10346</fpage><lpage>10357</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0666-17.2017</pub-id><pub-id pub-id-type="pmid">28947573</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><volume>364</volume><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AH</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Morin</surname><given-names>EL</given-names></name><name><surname>Malecek</surname><given-names>NJ</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Encoding of stimulus probability in macaque inferior temporal cortex</article-title><source>Current Biology</source><volume>26</volume><fpage>2280</fpage><lpage>2290</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.07.007</pub-id><pub-id pub-id-type="pmid">27524483</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blakemore</surname><given-names>SJ</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Central cancellation of self-produced tickle sensation</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>635</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1038/2870</pub-id><pub-id pub-id-type="pmid">10196573</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname><given-names>H</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction errors but not sharpened signals simulate multivoxel fMRI patterns during speech perception</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002577</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002577</pub-id><pub-id pub-id-type="pmid">27846209</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The center-surround profile of the focus of attention arises from recurrent processing in visual cortex</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>982</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn139</pub-id><pub-id pub-id-type="pmid">18755778</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Hopf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural mechanisms of surround attenuation and distractor competition in visual search</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>5213</fpage><lpage>5224</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6406-10.2011</pub-id><pub-id pub-id-type="pmid">21471356</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boutin</surname><given-names>V</given-names></name><name><surname>Franciosini</surname><given-names>A</given-names></name><name><surname>Chavane</surname><given-names>F</given-names></name><name><surname>Ruffier</surname><given-names>F</given-names></name><name><surname>Perrinet</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sparse deep predictive coding captures contour integration capabilities of the early visual system</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008629</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008629</pub-id><pub-id pub-id-type="pmid">33497381</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Vision</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burnham</surname><given-names>KP</given-names></name><name><surname>Anderson</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>A practical information-theoretic approach</chapter-title><person-group person-group-type="editor"><name><surname>Burnham</surname><given-names>KP</given-names></name><name><surname>Anderson</surname><given-names>DR</given-names></name></person-group><source>Model Selection and Multimodel Inference</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/b97636</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrasco</surname><given-names>M</given-names></name><name><surname>Ling</surname><given-names>S</given-names></name><name><surname>Read</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Attention alters appearance</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>308</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1038/nn1194</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheadle</surname><given-names>S</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Feature expectation heightens visual sensitivity during fine orientation discrimination</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.14.14</pub-id><pub-id pub-id-type="pmid">26505967</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Choksi</surname><given-names>B</given-names></name><name><surname>Mozafari</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2106.02749</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Tuning curve shift by attention modulation in cortical neurons: a computational study of its mechanisms</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>761</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj021</pub-id><pub-id pub-id-type="pmid">16135783</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Preddie</surname><given-names>DC</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Spatial attention effects in macaque area V4</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>3201</fpage><lpage>3214</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-09-03201.1997</pub-id><pub-id pub-id-type="pmid">9096154</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ã‡ukur</surname><given-names>T</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention during natural vision warps semantic representation across the human brain</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>763</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/nn.3381</pub-id><pub-id pub-id-type="pmid">23603707</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Mazer</surname><given-names>JA</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title><source>Neuron</source><volume>59</volume><fpage>509</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.07.001</pub-id><pub-id pub-id-type="pmid">18701075</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doetsch</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Patterns in the brain: Neuronal population coding in the somatosensory system</article-title><source>Physiology &amp; Behavior</source><volume>69</volume><fpage>187</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/S0031-9384(00)00201-8</pub-id><pub-id pub-id-type="pmid">10854929</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name><name><surname>Genon</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging-based parcellations of the human brain</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>672</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0071-7</pub-id><pub-id pub-id-type="pmid">30305712</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esterman</surname><given-names>M</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perceptual expectation evokes category-selective cortical activity</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>1245</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp188</pub-id><pub-id pub-id-type="pmid">19759124</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>MWH</given-names></name><name><surname>Becker</surname><given-names>MW</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Attention to colors induces surround suppression at category boundaries</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>1443</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-37610-7</pub-id><pub-id pub-id-type="pmid">30723272</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>MWH</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The profile of attentional modulation to visual features</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/19.13.13</pub-id><pub-id pub-id-type="pmid">31747691</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>H</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention, uncertainty, and free-energy</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>215</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id><pub-id pub-id-type="pmid">21160551</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>A</given-names></name><name><surname>Mahringer</surname><given-names>D</given-names></name><name><surname>Oyibo</surname><given-names>HK</given-names></name><name><surname>Petersen</surname><given-names>AV</given-names></name><name><surname>Leinweber</surname><given-names>M</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1658</fpage><lpage>1664</lpage><pub-id pub-id-type="doi">10.1038/nn.4385</pub-id><pub-id pub-id-type="pmid">27618309</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foley</surname><given-names>NC</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>Mhatre</surname><given-names>H</given-names></name><name><surname>Lopes</surname><given-names>M</given-names></name><name><surname>Gottlieb</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parietal neurons encode expected gains in instrumental information</article-title><source>PNAS</source><volume>114</volume><fpage>E3315</fpage><lpage>E3323</lpage><pub-id pub-id-type="doi">10.1073/pnas.1613844114</pub-id><pub-id pub-id-type="pmid">28373569</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>KJ</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Gardner</surname><given-names>JLG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model</article-title><source>eLife</source><volume>12</volume><elocation-id>e78392</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.78392</pub-id><pub-id pub-id-type="pmid">37184221</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Hacker</surname><given-names>CD</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual decision-making in an uncertain and dynamic world</article-title><source>Annual Review of Vision Science</source><volume>3</volume><fpage>227</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-111815-114511</pub-id><pub-id pub-id-type="pmid">28715956</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>N</given-names></name><name><surname>Tsuchiya</surname><given-names>N</given-names></name><name><surname>Koenig-Robert</surname><given-names>R</given-names></name><name><surname>Hohwy</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Expectation and attention increase the integration of top-down and bottom-up signals in perception through different pathways</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000233</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000233</pub-id><pub-id pub-id-type="pmid">31039146</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>B</given-names></name><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predictable tones elicit stimulus-specific suppression of evoked activity in auditory cortex</article-title><source>NeuroImage</source><volume>200</volume><fpage>242</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.033</pub-id><pub-id pub-id-type="pmid">31229656</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanson</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Effects of discrimination training on stimulus generalization</article-title><source>Journal of Experimental Psychology</source><volume>58</volume><fpage>321</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1037/h0042606</pub-id><pub-id pub-id-type="pmid">13851902</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>JM</given-names></name><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Direct neurophysiological evidence for spatial suppression surrounding the focus of attention in vision</article-title><source>PNAS</source><volume>103</volume><fpage>1053</fpage><lpage>1058</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507746103</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaposvari</surname><given-names>P</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning signals in macaque inferior temporal cortex</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>250</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw374</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auto-Encoding Variational Bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1312.6114">https://doi.org/10.48550/arXiv.1312.6114</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiyonaga</surname><given-names>A</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Center-surround inhibition in working memory</article-title><source>Current Biology</source><volume>26</volume><fpage>64</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.11.013</pub-id><pub-id pub-id-type="pmid">26711496</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>BP</given-names></name><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attraction of position preference by spatial attention throughout human visual cortex</article-title><source>Neuron</source><volume>84</volume><fpage>227</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.047</pub-id><pub-id pub-id-type="pmid">25242220</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>van Lieshout</surname><given-names>LLF</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Local expectation violations result in global activity gain in primary visual cortex</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>37706</elocation-id><pub-id pub-id-type="doi">10.1038/srep37706</pub-id><pub-id pub-id-type="pmid">27874098</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prior expectations induce prestimulus sensory templates</article-title><source>PNAS</source><volume>114</volume><fpage>10473</fpage><lpage>10478</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705652114</pub-id><pub-id pub-id-type="pmid">28900010</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Advances in Neural Information Processing Systems</source><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Kaposvari</surname><given-names>P</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Encoding of predictable and unpredictable stimuli by inferior temporal cortical neurons</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>1445</fpage><lpage>1454</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01135</pub-id><pub-id pub-id-type="pmid">28387590</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DK</given-names></name><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Braun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attention activates winner-take-all competition among visual filters</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>375</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1038/7286</pub-id><pub-id pub-id-type="pmid">10204546</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>TS</given-names></name><name><surname>Mumford</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Hierarchical Bayesian inference in the visual cortex</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>20</volume><fpage>1434</fpage><lpage>1448</lpage><pub-id pub-id-type="doi">10.1364/josaa.20.001434</pub-id><pub-id pub-id-type="pmid">12868647</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Fang</surname><given-names>MWH</given-names></name><name><surname>Saba-Sadiya</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Adaptive visual selection in feature space</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>30</volume><fpage>994</fpage><lpage>1003</lpage><pub-id pub-id-type="doi">10.3758/s13423-022-02221-x</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loach</surname><given-names>D</given-names></name><name><surname>Frischen</surname><given-names>A</given-names></name><name><surname>Bruce</surname><given-names>N</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>An attentional mechanism for selecting appropriate actions afforded by graspable objects</article-title><source>Psychological Science</source><volume>19</volume><fpage>1253</fpage><lpage>1257</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02234.x</pub-id><pub-id pub-id-type="pmid">19121133</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucci</surname><given-names>G</given-names></name><name><surname>Berchicci</surname><given-names>M</given-names></name><name><surname>Perri</surname><given-names>RL</given-names></name><name><surname>Spinelli</surname><given-names>D</given-names></name><name><surname>Di Russo</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Effect of target probability on pre-stimulus brain activity</article-title><source>Neuroscience</source><volume>322</volume><fpage>121</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2016.02.029</pub-id><pub-id pub-id-type="pmid">26912279</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manenti</surname><given-names>GL</given-names></name><name><surname>Dizaji</surname><given-names>AS</given-names></name><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Variability in training unlocks generalization in visual perceptual learning through invariant representations</article-title><source>Current Biology</source><volume>33</volume><fpage>817</fpage><lpage>826</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.01.011</pub-id><pub-id pub-id-type="pmid">36724782</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mareschal</surname><given-names>I</given-names></name><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Clifford</surname><given-names>CWG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Humans have an expectation that gaze is directed toward them</article-title><source>Current Biology</source><volume>23</volume><fpage>717</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.03.030</pub-id><pub-id pub-id-type="pmid">23562265</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision</source><publisher-name>W. H. Freeman Co</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAuley</surname><given-names>JD</given-names></name><name><surname>Kidd</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Effect of deviations from temporal expectations on tempo discrimination of isochronous tone sequences</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>24</volume><fpage>1786</fpage><lpage>1800</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.24.6.1786</pub-id><pub-id pub-id-type="pmid">9861723</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Statistical learning of visual transitions in monkey inferotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>19401</fpage><lpage>19406</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112895108</pub-id><pub-id pub-id-type="pmid">22084090</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Selective attention gates visual processing in the extrastriate cortex</article-title><source>Science</source><volume>229</volume><fpage>782</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1126/science.4023713</pub-id><pub-id pub-id-type="pmid">4023713</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Neural correlates of feature selective memory and pop-out in extrastriate area V4</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>2190</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-04-02190.1994</pub-id><pub-id pub-id-type="pmid">8158265</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The columnar organization of the neocortex</article-title><source>Brain</source><volume>120 (Pt 4)</volume><fpage>701</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1093/brain/120.4.701</pub-id><pub-id pub-id-type="pmid">9153131</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mounts</surname><given-names>JRW</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Evidence for suppressive mechanisms in attentional selection: Feature singletons produce inhibitory surrounds</article-title><source>Perception &amp; Psychophysics</source><volume>62</volume><fpage>969</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.3758/BF03212082</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MÃ¼ller</surname><given-names>NG</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The attentional â€œspotlightâ€™sâ€ penumbra: center-surround modulation in striate cortex</article-title><source>Neuroreport</source><volume>15</volume><fpage>977</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1097/00001756-200404290-00009</pub-id><pub-id pub-id-type="pmid">15076718</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MÃ¼ller</surname><given-names>NG</given-names></name><name><surname>Mollenhauer</surname><given-names>M</given-names></name><name><surname>RÃ¶sler</surname><given-names>A</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The attentional field has a Mexican hat distribution</article-title><source>Vision Research</source><volume>45</volume><fpage>1129</fpage><lpage>1137</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.11.003</pub-id><pub-id pub-id-type="pmid">15707921</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>4700</fpage><lpage>4719</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-11-04700.1993</pub-id><pub-id pub-id-type="pmid">8229193</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>Z</given-names></name><name><surname>Oâ€™May</surname><given-names>CB</given-names></name><name><surname>Choksi</surname><given-names>B</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predictive coding feedback results in perceived illusory contours in a recurrent neural network</article-title><source>Neural Networks</source><volume>144</volume><fpage>164</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2021.08.024</pub-id><pub-id pub-id-type="pmid">34500255</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitt</surname><given-names>MA</given-names></name><name><surname>Myung</surname><given-names>IJ</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Toward a method of selecting among computational models of cognition</article-title><source>Psychological Review</source><volume>109</volume><fpage>472</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.109.3.472</pub-id><pub-id pub-id-type="pmid">12088241</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Press</surname><given-names>C</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Yon</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The perceptual prediction paradox</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>13</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.11.003</pub-id><pub-id pub-id-type="pmid">31787500</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Bayes factors and BIC: Comment on â€œA critique of the Bayesian information criterion for model selection</article-title><source>Sociological Methods &amp; Research</source><volume>27</volume><fpage>411</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1177/0049124199027003005</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnev</surname><given-names>D</given-names></name><name><surname>Lau</surname><given-names>H</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prior expectation modulates the interaction between sensory and prefrontal regions in the human brain</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>10741</fpage><lpage>10748</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1478-11.2011</pub-id><pub-id pub-id-type="pmid">21775617</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Dynamic model of visual recognition predicts neural response properties in the visual cortex</article-title><source>Neural Computation</source><volume>9</volume><fpage>721</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.4.721</pub-id><pub-id pub-id-type="pmid">9161021</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>D</given-names></name><name><surname>Ekman</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Suppressed sensory response to predictable object stimuli throughout the ventral visual stream</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7452</fpage><lpage>7461</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3421-17.2018</pub-id><pub-id pub-id-type="pmid">30030402</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>D</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Dampened sensory representations for expected input across the ventral visual stream</article-title><source>Oxford Open Neuroscience</source><volume>1</volume><elocation-id>kvac013</elocation-id><pub-id pub-id-type="doi">10.1093/oons/kvac013</pub-id><pub-id pub-id-type="pmid">38596702</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rungratsameetaweemana</surname><given-names>N</given-names></name><name><surname>Itthipuripat</surname><given-names>S</given-names></name><name><surname>Salazar</surname><given-names>A</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Expectations do not alter early sensory processing during perceptual decision-making</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5632</fpage><lpage>5648</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3638-17.2018</pub-id><pub-id pub-id-type="pmid">29773755</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rungratsameetaweemana</surname><given-names>N</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissociating the impact of attention and expectation on early sensory processing</article-title><source>Current Opinion in Psychology</source><volume>29</volume><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.copsyc.2019.03.014</pub-id><pub-id pub-id-type="pmid">31022561</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schall</surname><given-names>JD</given-names></name><name><surname>Hanes</surname><given-names>DP</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Neural basis of saccade target selection in frontal eye field during visual search</article-title><source>Nature</source><volume>366</volume><fpage>467</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1038/366467a0</pub-id><pub-id pub-id-type="pmid">8247155</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schall</surname><given-names>JD</given-names></name><name><surname>Sato</surname><given-names>TR</given-names></name><name><surname>Thompson</surname><given-names>KG</given-names></name><name><surname>Vaughn</surname><given-names>AA</given-names></name><name><surname>Juan</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Effects of search efficiency on surround suppression during visual selection in frontal eye field</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>2765</fpage><lpage>2769</lpage><pub-id pub-id-type="doi">10.1152/jn.00780.2003</pub-id><pub-id pub-id-type="pmid">14749315</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schumacher</surname><given-names>JW</given-names></name><name><surname>McCann</surname><given-names>MK</given-names></name><name><surname>Maximov</surname><given-names>KJ</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Selective enhancement of neural coding in V1 underlies fine-discrimination learning in tree shrew</article-title><source>Current Biology</source><volume>32</volume><fpage>3245</fpage><lpage>3260</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.06.009</pub-id><pub-id pub-id-type="pmid">35767997</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>SchÃ¼tt</surname><given-names>HH</given-names></name><name><surname>Harmeling</surname><given-names>S</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Painfree and accurate Bayesian estimation of psychometric functions for (potentially) overdispersed data</article-title><source>Vision Research</source><volume>122</volume><fpage>105</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.02.002</pub-id><pub-id pub-id-type="pmid">27013261</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating the dimension of a model</article-title><source>The Annals of Statistics</source><volume>6</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-level prediction signals in a low-level area of the macaque face-processing hierarchy</article-title><source>Neuron</source><volume>96</volume><fpage>89</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.007</pub-id><pub-id pub-id-type="pmid">28957679</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Gander</surname><given-names>PE</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Kovach</surname><given-names>CK</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Kawasaki</surname><given-names>H</given-names></name><name><surname>Howard</surname><given-names>MA</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural signatures of perceptual inference</article-title><source>eLife</source><volume>5</volume><elocation-id>e11476</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11476</pub-id><pub-id pub-id-type="pmid">26949254</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Mo</surname><given-names>C</given-names></name><name><surname>Fang</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Profiles of visual perceptual learning in feature space</article-title><source>iScience</source><volume>27</volume><elocation-id>109128</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2024.109128</pub-id><pub-id pub-id-type="pmid">38384835</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheremata</surname><given-names>SL</given-names></name><name><surname>Silver</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hemisphere-dependent attentional modulation of human parietal visual field representations</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>508</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2378-14.2015</pub-id><pub-id pub-id-type="pmid">25589746</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>R</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The extent of center-surround inhibition for colored items in working memory</article-title><source>Memory &amp; Cognition</source><volume>49</volume><fpage>733</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.3758/s13421-020-01116-3</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>R</given-names></name><name><surname>Qi</surname><given-names>M</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The ERP correlates of color-based center-surround inhibition in working memory</article-title><source>International Journal of Psychophysiology</source><volume>181</volume><fpage>160</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2022.09.005</pub-id><pub-id pub-id-type="pmid">36165962</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>KW</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>The differential response in animals to stimuli varying within a single dimension</article-title><source>Psychological Review</source><volume>44</volume><fpage>430</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1037/h0062885</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>T</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Content-specific expectations enhance stimulus detectability by increasing perceptual sensitivity</article-title><source>Journal of Experimental Psychology. General</source><volume>144</volume><fpage>1089</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1037/xge0000109</pub-id><pub-id pub-id-type="pmid">26460783</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Noise characteristics and prior expectations in human visual speed perception</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>578</fpage><lpage>585</lpage><pub-id pub-id-type="doi">10.1038/nn1669</pub-id><pub-id pub-id-type="pmid">16547513</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>StÃ¶rmer</surname><given-names>VS</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Feature-based attention elicits surround suppression in feature space</article-title><source>Current Biology</source><volume>24</volume><fpage>1985</fpage><lpage>1988</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.07.030</pub-id><pub-id pub-id-type="pmid">25155510</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suchow</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Fougnie</surname><given-names>D</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Modeling visual working memory with the MemToolbox</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.1167/13.10.9</pub-id><pub-id pub-id-type="pmid">23962734</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A neural representation of prior information during perceptual inference</article-title><source>Neuron</source><volume>59</volume><fpage>336</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.05.021</pub-id><pub-id pub-id-type="pmid">18667160</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Trittschuh</surname><given-names>EH</given-names></name><name><surname>Monti</surname><given-names>JM</given-names></name><name><surname>Mesulam</surname><given-names>MM</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural repetition suppression reflects fulfilled perceptual expectations</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1004</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1038/nn.2163</pub-id><pub-id pub-id-type="pmid">19160497</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Expectation (and attention) in visual cognition</article-title><source>Trends in Cognitive Sciences</source><volume>13</volume><fpage>403</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.06.003</pub-id><pub-id pub-id-type="pmid">19716752</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Expectation in perceptual decision making: neural and computational mechanisms</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>745</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1038/nrn3838</pub-id><pub-id pub-id-type="pmid">25315388</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tal-Perry</surname><given-names>N</given-names></name><name><surname>Yuval-Greenberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The spatiotemporal link of temporal expectations: Contextual temporal expectation is independent of spatial attention</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>2516</fpage><lpage>2523</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1555-21.2022</pub-id><pub-id pub-id-type="pmid">35091506</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Columns for complex visual object features in the inferotemporal cortex: clustering of cells with similar but slightly different stimulus selectivities</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>90</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.1.90</pub-id><pub-id pub-id-type="pmid">12466220</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teufel</surname><given-names>C</given-names></name><name><surname>Fletcher</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Forms of prediction in the nervous system</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>231</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0275-5</pub-id><pub-id pub-id-type="pmid">32157237</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorovic</surname><given-names>A</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prior expectation mediates neural adaptation to repeated sounds in the auditory cortex: an MEG study</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>9118</fpage><lpage>9123</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1425-11.2011</pub-id><pub-id pub-id-type="pmid">21697363</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name><name><surname>Tehovnik</surname><given-names>EJ</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name><name><surname>Schiller</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Eye movements modulate visual receptive fields of V4 neurons</article-title><source>Neuron</source><volume>29</volume><fpage>757</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00250-1</pub-id><pub-id pub-id-type="pmid">11301034</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tombu</surname><given-names>M</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attending to orientation results in an inhibitory surround in orientation space</article-title><source>Perception &amp; Psychophysics</source><volume>70</volume><fpage>30</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.3758/PP.70.1.30</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Culhane</surname><given-names>SM</given-names></name><name><surname>Kei Wai</surname><given-names>WY</given-names></name><name><surname>Lai</surname><given-names>Y</given-names></name><name><surname>Davis</surname><given-names>N</given-names></name><name><surname>Nuflo</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Modeling visual attention via selective tuning</article-title><source>Artificial Intelligence</source><volume>78</volume><fpage>507</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(95)00025-9</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Culhane</surname><given-names>S</given-names></name><name><surname>Cutzu</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title>From theoretical foundations to a hierarchical circuit for selective attention</chapter-title><person-group person-group-type="editor"><name><surname>Braun</surname><given-names>J</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><source>Visual Attention and Cortical Circuits</source><publisher-name>MIT press</publisher-name><fpage>285</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.7551/mitpress/7125.003.0016</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>RodrÃ­guez-SÃ¡nchez</surname><given-names>AJ</given-names></name><name><surname>Rothenstein</surname><given-names>AL</given-names></name><name><surname>Simine</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The different stages of visual recognition need different attentional binding strategies</article-title><source>Brain Research</source><volume>1225</volume><fpage>119</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2008.05.038</pub-id><pub-id pub-id-type="pmid">18585692</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Tactile expectation modulates pre-stimulus beta-band oscillations in human sensorimotor cortex</article-title><source>NeuroImage</source><volume>51</volume><fpage>867</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.02.053</pub-id><pub-id pub-id-type="pmid">20188186</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Es</surname><given-names>DM</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial sampling in human visual cortex is modulated by both spatial and feature-based attention</article-title><source>eLife</source><volume>7</volume><elocation-id>e36928</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36928</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Parcellating cerebral cortex: How invasive animal studies inform noninvasive mapmaking in humans</article-title><source>Neuron</source><volume>99</volume><fpage>640</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.002</pub-id><pub-id pub-id-type="pmid">30138588</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vo</surname><given-names>VA</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial tuning shifts increase the discriminability and fidelity of population codes in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3386</fpage><lpage>3401</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3484-16.2017</pub-id><pub-id pub-id-type="pmid">28242794</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname><given-names>M</given-names></name><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mere expectation to move causes attenuation of sensory signals</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e2866</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0002866</pub-id><pub-id pub-id-type="pmid">18682736</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Labyt</surname><given-names>E</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Bekinschtein</surname><given-names>T</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Evidence for a hierarchy of predictions and prediction errors in human cortex</article-title><source>PNAS</source><volume>108</volume><fpage>20754</fpage><lpage>20759</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117807108</pub-id><pub-id pub-id-type="pmid">22147913</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A practical solution to the pervasive problems of p values</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>14</volume><fpage>779</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.3758/bf03194105</pub-id><pub-id pub-id-type="pmid">18087943</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Brewer</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.012</pub-id><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Awareness-dependent normalization framework of visual bottom-up attention</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>9593</fpage><lpage>9607</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1110-21.2021</pub-id><pub-id pub-id-type="pmid">34611027</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural circuitry of reward prediction error</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>373</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031109</pub-id><pub-id pub-id-type="pmid">28441114</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>AB</given-names></name><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>QUEST: a Bayesian adaptive psychometric method</article-title><source>Perception &amp; Psychophysics</source><volume>33</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.3758/bf03202828</pub-id><pub-id pub-id-type="pmid">6844102</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wenliang</surname><given-names>LK</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep neural networks for modeling visual perceptual learning</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>6028</fpage><lpage>6044</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1620-17.2018</pub-id><pub-id pub-id-type="pmid">29793979</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilsch</surname><given-names>A</given-names></name><name><surname>Mercier</surname><given-names>MR</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Haegens</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatial attention and temporal expectation exert differential effects on visual and auditory discrimination</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>1562</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01567</pub-id><pub-id pub-id-type="pmid">32319865</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Anton-Erxleben</surname><given-names>K</given-names></name><name><surname>Pieper</surname><given-names>F</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamic shifts of visual receptive fields in cortical area MT by spatial attention</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1156</fpage><lpage>1160</lpage><pub-id pub-id-type="doi">10.1038/nn1748</pub-id><pub-id pub-id-type="pmid">16906153</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Lai</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Pedrycz</surname><given-names>W</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>A codebook-driven approach for low-light image enhancement</article-title><source>Engineering Applications of Artificial Intelligence</source><volume>156</volume><elocation-id>111115</elocation-id><pub-id pub-id-type="doi">10.1016/j.engappai.2025.111115</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yon</surname><given-names>D</given-names></name><name><surname>Gilbert</surname><given-names>SJ</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Press</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Action sharpens sensory representations of expected outcomes</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4288</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06752-7</pub-id><pub-id pub-id-type="pmid">30327503</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>SA</given-names></name><name><surname>Tsotsos</surname><given-names>JK</given-names></name><name><surname>Fallah</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The attentional suppressive surround: Eccentricity, location-based and feature-based effects and interactions</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>710</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00710</pub-id><pub-id pub-id-type="pmid">30349452</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuille</surname><given-names>A</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Vision as Bayesian inference: analysis by synthesis?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>301</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.05.002</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Discrete fixed-resolution representations in visual working memory</article-title><source>Nature</source><volume>453</volume><fpage>233</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1038/nature06860</pub-id><pub-id pub-id-type="pmid">18385672</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>A</given-names></name><name><surname>Meng</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Emerged human-like facial expression representation in a deep convolutional neural network</article-title><source>Science Advances</source><volume>8</volume><elocation-id>eabj4383</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abj4383</pub-id><pub-id pub-id-type="pmid">35319988</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuanazzi</surname><given-names>A</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distinct neural mechanisms of spatial attention and expectation guide perceptual inference in a multisensory world</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>2301</fpage><lpage>2312</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2873-18.2019</pub-id><pub-id pub-id-type="pmid">30659086</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107301.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sui</surname><given-names>Jing</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Beijing Normal University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This is a methodologically rich manuscript that is <bold>important</bold> for revealing the center-surround inhibition profile of expectation in orientation space. The analyses are <bold>compelling</bold> in validating the critical role of predictive coding feedback. The findings provide novel insights into how expectation optimizes perception via enhancement and suppression.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107301.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors tested two competing mechanisms of expectation (1) a sharpening model that suppresses unexpected information via center-surround inhibition; (2) a cancellation model that predicts a monotonic gradient response profile. Using two psychophysical experiments manipulating feature space distance between expected and unexpected stimuli, the results consistently supported the sharpening model. Computational modeling further showed that expectation effects were explained by either sharpened tuning curves or tuning shifts. Finally, convolutional neural network simulations revealed that feedback connections critically mediate the observed center-surround inhibition.</p><p>Strengths:</p><p>The manuscript provides compelling and convergent evidence from both psychophysical experiments and computational modeling to robustly support the sharpening model of expectation, demonstrating clear center-surround inhibition of unexpected information.</p><p>Comments on revisions:</p><p>I appreciate the authors' thoughtful revisions. I have no further comments.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107301.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This is a compelling and methodologically rich manuscript. The authors used a variety of methods, including psychophysics, computational modeling, and artificial neural networks, to reveal a non-monotonic, center-surround &quot;Mexican-hat&quot; profile of expectation in orientation space. Their data convincingly extend analogous findings in attention and working memory, and the modeling nicely teases apart sharpening vs. shift mechanisms.</p><p>Strengths:</p><p>The findings are novel and important in elucidating the potential neural mechanisms by which expectation shapes perception. The authors conducted a series of well-designed psychophysical experiments to careful examination of the profile of expectation's modulation. Computational modeling also provides further insights, linking the neural mechanisms of expectation to behavioral results.</p><p>Comments on revisions:</p><p>I think the authors did a great job in addressing my previous comments. I have no further comments.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107301.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Ling</given-names></name><role specific-use="author">Author</role><aff><institution>The Ohio State University</institution><addr-line><named-content content-type="city">Columbus</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Shiqi</given-names></name><role specific-use="author">Author</role><aff><institution>South China Normal University</institution><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Yueling</given-names></name><role specific-use="author">Author</role><aff><institution>South China Normal University</institution><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Ou</surname><given-names>Shipei</given-names></name><role specific-use="author">Author</role><aff><institution>South China Normal University</institution><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Ru-Yuan</given-names></name><role specific-use="author">Author</role><aff><institution>Shanghai Jiao Tong University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role specific-use="author">Author</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xilin</given-names></name><role specific-use="author">Author</role><aff><institution>South China Normal University</institution><addr-line><named-content content-type="city">Guangzhou</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ€™ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>(1) The sharpening model of expectation can predict surround suppression. The authors could further clarify how the cancellation model predicts a monotonic profile of expectation (Figure 1C) with the highest response at the expected orientation, while the cancellation model suggests a suppression of neurons tuned toward the expected stimulus.</p></disp-quote><p>We thank the reviewer for the comment. We would like to emphasize that as the expected signal is suppressed, the relative weight or salience of unexpected inputs increases. We have clarified this interpretation in the manuscript as follows:</p><p>â€œHere, given these two mechanisms making opposite predictions about how expectation changes the neural responses of unexpected stimuli, thereby displaying different profiles of expectation, we speculated that if expectation operates by the sharpening model with suppressing unexpected information, we should observe an inhibitory zone surrounding the focus of expectation, and its profile then should display as a center-surround inhibition (Fig. 1c, left). If, however, expectation operates as suggested by the cancelation model with highlighting unexpected information, the inhibitory zone surrounding the focus of expectation should be eliminated, and the profile should instead display a monotonic gradient (Fig. 1c, right).â€</p><disp-quote content-type="editor-comment"><p>(2) I'm a bit concerned about whether the profile solely arises from modulation of expectation. The two auditory cues are each associated with a fixed orientation, which may be confounded by other cognitive processes like visual working memory or attention (which I think the authors also discussed). Although the authors tried to use SFD task to render orientation task-irrelevant, luminance edges (i.e., orientation) and spatial frequency in gratings are highly intertwined and orientation of the gratings may help recall the first grating's SF (fixed at 0.9 c/{degree sign}), especially given the first and second grating's orientations are not very different (4.8{degree sign}).</p></disp-quote><p>We agree that dissociating expectation from attention and other top-down processes remains a key challenge in visual expectation research (see Summerfield &amp; Egner, 2009; Summerfield &amp; de Lange, 2014; de Lange et al., 2018). As is generally acknowledged, expectation reflects the probability of a sensory event, while selective attention relates to its behavioral relevance. To minimize attentional influences, our task design ensured that grating orientation was not taskrelevant: on each trial, participants discriminated either orientation or spatial frequency difference, such that orientation itself did not require attentional allocation, a point already discussed in the manuscript.</p><p>Regarding visual working memory, we argue that even if participants recalled the first gratingâ€™s spatial frequency in the SFD task, they were not required to retain its precise spatial frequency (or orientation), as their task was simply to judge whether the second grating appeared denser or sparser. In other words, orientation (or spatial frequency) itself was not task-relevant. Moreover, although not included in the manuscript, we conducted a post-experiment debriefing in which participants were asked whether they noticed any association between the auditory tone and the grating orientation. None of the participants reported this relationship correctly, suggesting that the tone-orientation mapping remained implicit and was unlikely to be driven by strategic attention or memory.</p><p>However, we acknowledge that certain confounding processes such as statistical learning or implicit mapping acquisition cannot be fully ruled out given the current paradigm. Future studies using methods with higher temporal resolution (e.g., EEG/MEG) may help to dissociate these mechanisms more precisely.</p><disp-quote content-type="editor-comment"><p>(3) For each of the expected orientations (20{degree sign} or 70{degree sign}), the unexpected ones are linearly separable (i.e., all unexpected ones lie on one side of the expected angle). This might further encourage people to shift their attended or expected orientation, according to the optimal tuning hypothesis. Would this provide an alternative explanation to the tuning shift that the authors found?</p></disp-quote><p>We thank the reviewer for pointing out the relevance of the optimal tuning hypothesis. We acknowledge that the optimal tuning theory (Navalpakkam &amp; Itti, 2007) is an important framework, particularly in visual search paradigms, where attentional templates may shift away from non-target features to enhance discriminability.</p><p>In our task, this hypothesis would predict a shift of expectation toward &lt;20Â° in E20Â° trials and &gt;70Â° in E70Â° trials, given that all unexpected orientations lie on one side of the expected angle. Importantly, the optimal tuning hypothesis predicts such shifts not only in Î”20Â°, Î”25Â°, and Î”30Â° trials but also in the Î”0Â° trials. In this regard, the observed shift in Î”20Â° and Î”30Â° (Experiment 2) and Î”25Â° (Experiment 3) trials is broadly consistent with the predictions of the optimal tuning account. However, we did not observe a corresponding shift away from nontarget features in the Î”0Â° condition, suggesting limited behavioral evidence for optimal tuning effects under our current task settings.</p><p>It is important to note that most previous studies supporting optimal tuning (e.g., Navalpakkam &amp; Itti, 2007; Scolari &amp; Serences, 2009; Geng, DiQuattro, &amp; Helm, 2017; Yu &amp; Geng, 2019) have used visual search paradigms that differ from our design in several critical ways, including the number of stimuli presented, their spatial arrangement (eccentricity), task demands, and so on. Therefore, it is difficult to determine whether the optimal tuning hypothesis could serve as an alternative explanation within the context of our current study. We agree that future studies could further examine how such task parameters influence the presence or absence of optimal tuning.</p><disp-quote content-type="editor-comment"><p>(4) It is great that the authors conducted computational modeling to elucidate the potential neuronal mechanisms of expectation. But I think the sharpening hypothesis (e.g., reviewed in de Lange, Heilbron &amp; Kok, 2018) focuses on the neural population level, i.e., narrowing of population tuning profile, while the authors conducted the sharpening at the neuronal tuning level. However, the sharpening of population does not necessarily rely on the sharpening of individual neuronal tuning. For example, neuronal gain modulation can also account for such population sharpening. I think similar logic applies to the orientation adjustment experiment. The behavioral level shift does not necessarily suggest a similar shift at the neuronal level. I would recommend that the authors comment on this.</p></disp-quote><p>We thank the reviewer for this to-the-point comment. As de Lange et al. (2018) noted, â€œthere is not always a direct correspondence between neural-level and voxel-level selectivity patterns.â€ That is, neuronal tuning, population-level tuning, voxel-level selectivity, and behavioral adaptive outcomes may reflect different underlying mechanisms and do not necessarily align in a one-toone fashion. We fully acknowledge that population-level tuning effects may also result from various neuronal mechanisms such as gain modulation (for review, see Salinas &amp; Thier, 2000), shifts in preferred orientation (Ringach, et al., 1997; Jeyabalaratnam et al., 2013), asymmetric broadening of tuning curves (Schumacher et al., 2022), or tuning curve sharpening (Ringach, et al., 1997; Schoups et al., 2001).</p><p>In our modeling, we implemented sharpening and shifts of neuronal tuning curves as a conceptual model simplification, intended to explore potential mechanisms underlying expectation-related center-surround suppression effects. While sharpening-based accounts (e.g., Kok et al. 2012) have often been emphasized, we stress that other mechanisms, such as gain modulation or tuning shifts, may also contribute. Our goal is not to provide a definitive account, but to highlight such plausible mechanisms and encourage future investigation. We have revised the Discussion to emphasize that multiple mechanisms may underlie the observed effects.</p><p>â€œWe note that our implementation of sharpening and shifts at the neuronal level serves as a conceptual model simplification, as population-level tuning, voxel-level selectivity, and behavioral adaptive outcomes may reflect different underlying neuronal mechanisms and do not necessarily align in a one-to-one fashion. Here, we stress that other potential mechanisms beyond sharpening, such as tuning shifts, may also contribute to visual expectation.â€</p><disp-quote content-type="editor-comment"><p>(5) If the orientation adjustment experiment suggests that both sharpening and shifting are present at the same time, have the authors tried combining both in their computational model?</p></disp-quote><p>We agree with the reviewer that it is necessary to consider the combined model. Accordingly, we implemented a computational model incorporating sharpening of the expected orientation channel together with shifting of the unexpected orientation channels. This model</p><p>successfully captured the sharpening of the expected-orientation channel and the shift of the unexpectedorientation channels (Supplementary Fig. 3). For the expected orientation (Î”0Â°) , results showed that the amplitude change was significantly higher than zero on both OD (t(23) = 2.582, p = 0.017, Cohenâ€™s d = 0.527) and SFD (t(23) = 2.078, p = 0.049, Cohenâ€™s d = 0.424) tasks (Supplementary Fig. 3e, vertical stripes); the width change was significantly lower than zero on both OD (t(23) = -2.438, p = 0.023, Cohenâ€™s d = 0.498) and SFD (t(23) = -2.578, p = 0.017, Cohenâ€™s d = 0.526) tasks (Supplementary Fig. 3e, diagonal stripes). For unexpected orientations (Î”10Â°-Î”40Â°), however, the amplitude and width changes were not significant with zero on either OD (amplitude change: t(23) = 0.443, p = 0.662, Cohenâ€™s d = 0.091; width change: t(23) = -1.819, p = 0.082, Cohenâ€™s d = 0.371) or SFD (amplitude change: t(23) = 1.130, p = 0.270, Cohenâ€™s d = 0.231; width change: t(23) = -1.710, p = 0.101, Cohenâ€™s d = 0.349) tasks (Supplementary Fig. 3f). In the meantime, the location shift was significantly different than zero for unexpected orientations (Î”10Â°-Î”40Â°), OD task: t(23) = 3.611, p = 0.001, Cohenâ€™s d = 0.737; SFD task: t(23) = 2.418, p = 0.024, Cohenâ€™s d = 0.493 (Supplementary Fig. 3g). These results provided further evidence that tuning sharpening and tuning shift jointly contribute to centerâ€“ surround inhibition in expectation.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer#1 (Recommendation for the Author):</bold></p><p>(1) A direct comparison between tasks (baseline vs. expectation conditions) would have strengthened the findings. Specifically, contrasting performance in the orientation discrimination task with the spatial frequency discrimination task could have provided clearer evidence that participants actually used the auditory cues to attend to the expected orientation. This comparison would be particularly important for validating cue manipulation in the orientation discrimination task.</p></disp-quote><p>We agree that a direct comparison between the orientation discrimination (OD) and spatial frequency discrimination (SFD) tasks could further clarify how expectation (auditory cues) differentially modulates orientation relevance. However, the primary goal of the current study was to examine expectation effects within each task separately and to demonstrate that such effects are independent of attentional modulation driven by the task-relevance of orientation.</p><p>In addition, the OD and SFD tasks differ not only in the relevant task features (orientation vs. spatial frequency discrimination), but also in stimulus properties and difficulty, for example, the arbitrary use of 20â€“70Â° as the orientation range and ~0.9 cycles/Â° as the spatial frequency setting, a direct comparison could introduce confounding factors unrelated to expectation.</p><p>Importantly, Previous studies (e.g., Kok et al., 2012, 2017; Aitken et al., 2020) and our current results show that participants performed significantly better when the auditory cue matched the expected orientation, supporting the validity of our expectation manipulation.</p><disp-quote content-type="editor-comment"><p>(2) An interesting consideration is why the center-surround inhibition profile of expectation was independent of the task-relevance of orientation. Previous studies (e.g., Kok et al., 2012) have found that orientation discrimination patterns differ depending on whether orientation is taskrelevant or irrelevant. This could be useful to discuss the possible discrepancies.</p></disp-quote><p>We thank the reviewer for this inspiring comment. Kok et al. (2012) showed that both orientation and contrast tasks elicited similar fMRI decoding results, regardless of task relevance, suggesting neural mechanisms of expectation operate independently of whether orientation is task relevant. Behaviorally, they reported better performance for expected versus unexpected trials in the orientation task (3.4Â° vs. 3.8Â°, t(17) = 2.8, p = 0.013), and a marginal trend (although not significant) in the contrast task (4.3% vs. 5.0%, t(17) = 1.9, p = 0.075). If any differences between the two tasks exist, they may lie in the correlation between behavioral and fMRI effects, a question that goes beyond the scope of the current study. Therefore, it is hard to strongly conclude that orientation discrimination patterns differ depending on whether orientation is taskrelevant or irrelevant in their paper.</p><p>Our study differs from theirs in at least two important ways, which may account for the clearer expectation facilitatory effect we observed in the expectation (Î”0Â°) condition. First, in our study, the orientation-irrelevant task involved spatial frequency discrimination (SFD) rather than contrast discrimination. Compared to contrast, spatial frequency has been shown to exhibit a clear cueing effect, as reported in Fang &amp; Liu (2019). Second, our design included a baseline condition, which was absent in their study. We computed discrimination sensitivity (DS) to quantify how much the discrimination threshold (DT) changed relative to baseline. By using this baseline-referenced approach, we observed a significant facilitatory expectation effect in the Î”0Â° condition, an effect that shifted from marginal significance in their orientation-irrelevant task to clear significance in our study.</p><disp-quote content-type="editor-comment"><p>(3) The authors might consider briefly explaining how the orientation adjustment paradigm used in this study is particularly effective for examining the potential co-existence of tuning sharpening and tuning shift computations, and how this approach complements traditional orientation discrimination tasks in characterizing expectation-related mechanisms.</p></disp-quote><p>We thank the reviewer for this valuable suggestion. We agree that further clarification is needed to better connect the two experiments. To explain this, we have elaborated further in the manuscript.</p><p>â€œTo further explore the co-existence of both Tuning sharpening and Tuning shift computations in center-surround inhibition profile of expectation, participants were asked to perform a classic orientation adjustment experiment. Unlike profile experiment (discrimination tasks), the adjustment experiment provides a direct, trial-by-trial measure of participantsâ€™ perceived orientation, capturing the full distribution of responses. This enables the construction of orientation-specific tuning curves, allowing us to detect both tuning sharpening and tuning shifts, thereby offering a more nuanced understanding of the computational mechanisms underlying expectation.â€</p><disp-quote content-type="editor-comment"><p>(4) These interesting findings raise important questions about their relationship to existing hybrid models of attentional modulation. Could the authors discuss how their results might align with or extend previous work demonstrating combined feature-similarity gain and surround suppression effects for orientation (e.g., Fang &amp; Liu, 2019)? Could a hybrid model potentially provide a better account of these data than the pure surround suppression model?</p></disp-quote><p>We thank the reviewer for this valuable comment. We agree that hybrid model should be mentioned in the manuscript and we have elaborated further in the Discussion.</p><p>â€œFor example, within the orientation space, the inhibitory zone was about 20Â°, 45Â°, and 54Â° for expectation evident here, feature-based attention[21], and visual perceptual learning[35], respectively; within the feature-based attention, it was about 30Â° and 45Â° in color [77] and motion direction [53] spaces, respectively These variations hint at the exciting possibility that the width of the inhibitory surround may flexibly adapt to stimulus context and task demands, ultimately facilitating our perception and behavior in a changing environment. This principle is consistent with the hybrid model of feature-based attention [53,54,75], where attention is deployed adaptively to prioritize task-relevant information through feature-similarity gain which filters out the most distinctive distractors, and surround suppression which inhibits similar and confusable ones, thereby jointly shaping the attentional tuning profile.â€</p><disp-quote content-type="editor-comment"><p>(5) On page 19, there appears to be a missing symbol in the description of the Tuning Sharpening model. The text states: 'the tuning width of each channel's tuning function is parameterized by ??', where the question marks seem to indicate a missing parameter symbol.</p></disp-quote><p>We appreciate the reviewerâ€™s careful attention. Yes, the &quot;Æ¡&quot; is missing, which was likely caused by a formatting issue. We have corrected it.</p></body></sub-article></article>