<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106464</article-id><article-id pub-id-type="doi">10.7554/eLife.106464</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106464.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A geometric shape regularity effect in the human brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Sablé-Meyer</surname><given-names>Mathias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0844-0775</contrib-id><email>mathias.sable-meyer@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Benjamin</surname><given-names>Lucas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9578-6039</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Potier Watkins</surname><given-names>Cassandra</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Chenxi</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Pajot</surname><given-names>Maxence</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Morfoisse</surname><given-names>Théo</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Al Roumi</surname><given-names>Fosca</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9590-080X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04ex24z53</institution-id><institution>Collège de France, Université Paris-Sciences-Lettres (PSL)</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xjwb503</institution-id><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Groen</surname><given-names>Iris IA</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dkp9463</institution-id><institution>University of Amsterdam</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>01</month><year>2026</year></pub-date><volume>14</volume><elocation-id>RP106464</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-02-28"><day>28</day><month>02</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-02-06"><day>06</day><month>02</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.03.13.584141"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-05-30"><day>30</day><month>05</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106464.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-11-26"><day>26</day><month>11</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106464.2"/></event></pub-history><permissions><copyright-statement>© 2025, Sablé-Meyer et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Sablé-Meyer et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106464-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-106464-figures-v1.pdf"/><abstract><p>The perception and production of regular geometric shapes, a characteristic trait of human cultures since prehistory, has unknown neural mechanisms. Behavioral studies suggest that humans are attuned to discrete regularities such as symmetries and parallelism and rely on their combinations to encode regular geometric shapes in a compressed form. To identify the brain systems underlying this ability, as well as their dynamics, we collected functional MRI in both adults and 6-year-olds, and magnetoencephalography data in adults, during the perception of simple shapes such as hexagons, triangles, and quadrilaterals. The results revealed that geometric shapes, relative to other visual categories, induce a hypoactivation of ventral visual areas and an overactivation of the intraparietal and inferior temporal regions also involved in mathematical processing, whose activation is modulated by geometric regularity. While convolutional neural networks captured the early visual activity evoked by geometric shapes, they failed to account for subsequent dorsal parietal and prefrontal signals, which could only be captured by discrete geometric features or by bigger deep-learning models of vision. We propose that the perception of abstract geometric regularities engages an additional symbolic mode of visual perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>geometry</kwd><kwd>fMRI</kwd><kwd>MEG</kwd><kwd>shape perception</kwd><kwd>behavior</kwd><kwd>oddball</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ebnp485</institution-id><institution>Fondation Fyssen</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Sablé-Meyer</surname><given-names>Mathias</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>MathBrain</award-id><principal-award-recipient><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>Institut National de la Santé et de la Recherche Médicale</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00jjx8s55</institution-id><institution>Commissariat à l'Énergie Atomique et aux Énergies Alternatives</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04ex24z53</institution-id><institution>Collège de France</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>fMRI and MEG results in adults and children show encoding of abstract geometric regularities in dorsal-parietal, temporal, and frontal regions, pointing to a system for symbolic geometric representation in humans.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Long before the invention of writing, the very first detectable graphic productions of prehistoric humans were highly regular non-pictorial geometric signs such as parallel lines, zig-zags, triangular, or checkered patterns (<xref ref-type="bibr" rid="bib65">Henshilwood et al., 2018</xref>; <xref ref-type="bibr" rid="bib137">Waerden, 2012</xref>). Human cultures throughout the world compose complex figures using simple geometrical regularities such as parallelism and symmetry in their drawings, decorative arts, tools, buildings, graphics, and maps (<xref ref-type="bibr" rid="bib134">Tversky, 2011</xref>). Cognitive anthropological studies suggest that, even in the absence of formal western education, humans possess intuitions of foundational geometric concepts such as points and lines and how they combine to form regular shapes (<xref ref-type="bibr" rid="bib37">Dehaene et al., 2006</xref>; <xref ref-type="bibr" rid="bib69">Izard et al., 2011</xref>). The scarce data available to date suggests that other primates, including chimpanzees, may not share the same ability to perceive and produce regular geometric shapes (<xref ref-type="bibr" rid="bib32">Close and Call, 2015</xref>; <xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>; <xref ref-type="bibr" rid="bib121">Saito et al., 2014</xref>; <xref ref-type="bibr" rid="bib128">Tanaka, 2007</xref>), though unintentional-but-regular mark-marking behavior has been reported in macaques (<xref ref-type="bibr" rid="bib127">Sueur, 2025</xref>). Thus, studying the brain mechanisms that support the perception of geometric regularities may shed light on the origins of human compositionality and, ultimately, the mental language of mathematics. Here, we provide a first approach through the recording of functional MRI and magneto-encephalography signals evoked by simple geometric shapes such as triangles or squares. Our goal is to probe whether, over and above the pathways for processing the shapes of images such as faces, places, or objects, the regularities of geometric shapes evoke additional activity.</p><p>The present brain-imaging research capitalizes on a series of studies of how humans perceive quadrilaterals (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). In that study, we created 11 tightly matched stimuli that were all simple, non-figurative, textureless four-sided shapes, yet varied in their geometric regularity. The most regular was the square, with four parallel sides of equal length and four identical right angles. By progressively removing some of these features (parallelism, right angles, equality of length, and equality of angles), we created a hierarchy of quadrilaterals ranging from highly regular to completely irregular (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In a variety of tasks, geometric regularity had a large effect on human behavior. For instance, for equal objective amounts of deviation, human adults and children detected a deviant shape more easily among shapes of high regularity, such as squares or rectangles (&lt;5% errors), than among irregular quadrilaterals (&gt;40% errors). The effect appeared as a human universal, present in preschoolers, first-graders, and adults without access to formal western math education (the Himba from Namibia), and thus seemingly independent of education and of the existence of linguistic labels for regular shapes. Strikingly, when baboons were trained to perform the same task, they showed no such geometric regularity effect.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Measuring and modeling the perceptual similarity of geometric shapes.</title><p>(<bold>A</bold>) The 11 quadrilaterals used throughout the experiments (colors are consistently used in all other figures). (<bold>B</bold>) Sample displays for the behavioral visual search task used to estimate the 11 × 11 shape similarity matrix. Participants had to locate the deviant shape. The right insert shows two trials from the behavioral visual search task, used to estimate the 11 × 11 shape similarity matrix. Participants had to find the intruder within nine shapes. (<bold>C</bold>) Multidimensional scaling of human dissimilarity judgments; the gray arrow indicates the projection on the Multi-Dimensional Scaling (MDS) space of the number of geometric primitives in a shape. (<bold>D</bold>) The behavioral dissimilarity matrix (left) was better captured by a geometric feature coding model (middle) than by a convolutional neural network (right). The graph at right (<bold>E</bold>) shows the general linear model (GLM) coefficients for each participant. An accompanying explainer video is provided in <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106464-fig1-video1.mp4" id="fig1video1"><label>Figure 1—video 1.</label><caption><title>Explainer video for <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title><p>Explainer videos are not peer reviewed.</p></caption></media><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Additional convolutional neural network (CNN) encoding models of human behavior.</title><p>(<bold>A</bold>) Correlation matrix between all pairs of representational dissimilarity matrices (RDMs) generated by each CNN and layer considered. (<bold>B</bold>) Replication of <xref ref-type="fig" rid="fig1">Figure 1D</xref> with different CNNs. Stars indicate a significant difference between the geometric feature model and the respective CNN encoding model (p &lt; 0.001). <italic>t</italic> and p values also indicate whether the CNN encoding model is a significant predictor of participant behavior (the geometric feature model is always highly significant). (<bold>C</bold>) Replication of B using different layers of CORnet, organized from early to late layers, from left to right. Note that the late layers are much more significant predictors of human behavior than the early ones – although still far inferior to the geometric feature model. (<bold>D</bold>) Replication of our general linear model (GLM) analysis including only shapes for which there is no obvious name in English – though we gave them names in this manuscript to refer to them: ‘kite’, ‘rightKite’, ‘hinge’, ‘rustedHinge’, and ‘random’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig1-figsupp1-v1.tif"/></fig></fig-group><p>Baboon behavior was accounted for by convolutional neural network (CNN) models of object recognition, but human behavior could only be explained by appealing to a representation of discrete geometric properties of parallelism, right angle, and symmetry, in this and other tasks. We sometimes refer to this model as ‘symbolic’ because it relies on discrete, exact, rule-based features rather than continuous representations (<xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>). In this representational format, geometric shapes are postulated to be represented by symbolic expressions in a ‘language-of-thought’, for example ‘a square is a four-sided figure with four equal sides and four right angles’ or equivalently by a computer-like program from drawing them in a Logo-like language (<xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>).</p><p>We therefore formulated the hypothesis that, in the domain of geometry, humans deploy an additional cognitive process specifically attuned to geometric regularities. On top of the circuits for object recognition, which are largely homologous in human and non-human primates (<xref ref-type="bibr" rid="bib17">Bao et al., 2020</xref>; <xref ref-type="bibr" rid="bib87">Kriegeskorte et al., 2008b</xref>; <xref ref-type="bibr" rid="bib132">Tsao et al., 2008</xref>), the human code for geometric shapes would involve a distinct ‘language of thought’, an encoding of discrete mathematical regularities and their combinations (<xref ref-type="bibr" rid="bib28">Cavanagh, 2021</xref>; <xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib53">Fodor, 1975</xref>; <xref ref-type="bibr" rid="bib93">Leeuwenberg, 1971</xref>; <xref ref-type="bibr" rid="bib115">Quilty-Dunn et al., 2022</xref>; <xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>; <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>).</p><p>This hypothesis predicts that the most elementary geometric shapes, such as a square, are not solely processed within the ventral and dorsal visual pathways, but may also evoke a later stage of geometrical feature encoding in brain areas that were previously shown to encode arithmetic, geometric, and other mathematical properties, that is the bilateral intraparietal, inferotemporal, and dorsal prefrontal areas (<xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref>; <xref ref-type="bibr" rid="bib8">Amalric and Dehaene, 2019</xref>). We hypothesized that (1) such cognitive processes encode shapes according to their discrete geometric properties including parallelism, right angles, equal lengths, and equal angles; (2) the brain compresses this information when those properties are more regularly organized, and thus exhibit activity proportional to minimal description length (<xref ref-type="bibr" rid="bib29">Chater and Vitányi, 2003</xref>; <xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib50">Feldman, 2003</xref>); and (3) these computations occur downstream of other visual processes, since they rely on the initial output of visual processing pathways.</p><p>Here, we assessed these spatiotemporal predictions using two complementary neuroimaging techniques (functional MRI and magnetoencephalography [MEG]). We presented the same 11 quadrilaterals as in our previous research and used representational similarity analysis (<xref ref-type="bibr" rid="bib86">Kriegeskorte et al., 2008a</xref>) to contrast two models for their cerebral encoding, based either on classical CNN models or on exact geometric features. In the fMRI experiment, we also collected simpler images contrasting the category of geometric shapes to other classical categories such as faces, places, or tools. Furthermore, to evaluate how early the brain networks for geometric shape perception arise, we collected those fMRI data in two age groups: adults and children in first grade (6 years old, this year was selected as it marks the first year French students receive formal instruction in mathematics). If geometric shape perception involves elementary intuitions of geometric regularity common to all humans, then the corresponding brain networks should be detectable early on.</p></sec><sec id="s2"><title>Experiment 1: estimating the representational similarity of quadrilaterals with online behavior</title><p>Our research focuses on the 11 quadrilaterals previously used in our behavioral research (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>), which are tightly matched in average pairwise distance between vertices and length of the bottom side, yet differ broadly in geometric regularity (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The goal of our first experiment was to obtain an 11 × 11 matrix of behavioral dissimilarities between the 11 geometric shapes shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>, in order to compare it with predictions from classical visual models, embodied by CNNs, as well as geometric feature models of shape perception. To evaluate perceptual similarity in an objective manner, in experiment 1, we assessed the difficulty of visual search for one shape among rotated and scaled versions of the other (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib2">Agrawal et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Agrawal et al., 2020</xref>). Within a grid of nine shapes, eight are similar and one is different, and participants have to click on it. Intuitively, if two shapes are very dissimilar, we expect both the response time and the error rate of finding one exemplar of one shape among exemplars of the other shape to be low. Conversely, we expect both to be high if shapes are similar. This gives us an empirical measure of shape dissimilarity, which we can compare to the distance predicted by different models.</p><sec id="s2-1"><title>Results</title><p>The 11 × 11 dissimilarity matrix, estimated by aggregating response time and errors from <italic>n</italic> = 330 online participants, is shown in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. The distance was estimated as the average success rate divided by the average response time; method section for details. To better understand its similarity structure, we performed two-dimensional ordinal Multi-Dimensional Scaling (MDS) projection (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="bibr" rid="bib41">De Leeuw and Mair, 2009</xref>). The projection of the 11 shapes on the first dimension showed a strong geometric regularity, with the square and the rectangle landing at the far right, rhombus and parallelogram in the middle, and less regular shapes at the far left. Thus, human perceptual similarity seemed primarily driven by geometric properties. To quantify this resemblance using that 2D MDS projection, we examined the vector which corresponded to simply counting the number of geometric regularity (number of right angles, pairs of parallel lines, pairs of equal angles, and pairs of sides of equal length). This vector (shown in gray in <xref ref-type="fig" rid="fig1">Figure 1C</xref>) had a projection that was significantly different from 0 for both principal axes (both p &lt; 0.01).</p><p>Most diagnostically, we compared the full human dissimilarity matrix to those generated by two competing models of shape processing (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). The geometric feature model proposes that each shape is encoded by a feature vector of its discrete geometric regularities and predicts dissimilarity by counting the number of features not in common: this makes squares and rectangles very similar, but squares and irregular quadrilaterals very dissimilar. On the other hand, we operationalize our visual model by propagating shapes through a feedforward CNN model of the ventral visual pathway (we use Cornet-S, but see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> in Appendix for other CNNs and other layers of Cornet-S, with unchanged conclusions). Shape similarity was estimated as the cross-nobis distance between activation vectors in late layers (<xref ref-type="bibr" rid="bib138">Walther et al., 2016</xref>). Note that these two models are not significantly correlated (<italic>r</italic><sup>2</sup> = 0.04, p &gt; 0.05).</p><p>Multiple regression of the human dissimilarity matrix with the predictions of those two visual and geometric feature models (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) showed that both contributed to explaining perceived similarity (ps &lt; 0.001), but that the weight of the geometric feature model was 3.6 times larger than that of the visual model, a significant difference (p &lt; 0.001). This finding supports the prior proposal that the two strategies contribute to human behavior, but that the geometric feature-based one dominates, especially in educated adults (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Additional models and comparisons are presented in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. In particular, we have included two distance measures based on skeletal representations (<xref ref-type="bibr" rid="bib13">Ayzenberg and Lourenco, 2019</xref>; <xref ref-type="bibr" rid="bib104">Morfoisse and Izard, 2021</xref>), both of which performed better than chance but significantly less than the geometric feature model. Finally, to separate the effect of name availability and geometric features on behavior, we replicated our analysis after removing the square, rectangle, trapezoids, rhombus, and parallelogram from our data (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>). This left us with five shapes and a representational dissimilarity matrix (RDM) with 10 entries. When regressing it in a general linear model (GLM) with our two models, we find that both models are still significant predictors (p &lt; 0.001). The effect size of the geometric feature model is greatly reduced, yet remains significantly higher than that of the neural network model (p &lt; 0.001).</p></sec></sec><sec id="s3"><title>Experiment 2: fMRI geometric shape localizer</title><p>To understand the neural underpinning of the cognition of geometric shape using fMRI, we started with the simplest foray into geometric shape perception by including geometric shapes as an additional visual category in a standard visual localizer used in the lab (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Across short miniblocks, this fMRI run probed whole-brain responses to geometric shapes (triangles, squares, hexagons, etc.) and to a variety of matched visual categories (faces, objects, houses, arithmetic, words, and Chinese characters). In 20 adults in functional MRI (9 females; 19–37 years old, mean age 24.6), we collected three localizer runs using a fast miniblock design. To maintain attention, participants were asked to detect a rare target, which could appear in any miniblock.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Localizing the brain systems involved in geometric shape perception.</title><p>(<bold>A</bold>) Visual categories used in the localizer. (<bold>B</bold>) Task: Passive presentation by miniblocks of consistent visual categories. In some miniblock, among a series of six pictures from a given category, participants had to detect a rare target star. (<bold>C</bold>) Statistical map associated with the contrast ‘single geometric shape &gt; faces, houses, and tools’, projected on an inflated brain (top: adults; bottom: children; clusters significant at cluster-corrected p &lt; 0.05 with non-parametric two-tailed bootstrap test as reported in the text). (<bold>D</bold>) BOLD response amplitude (regression weights, arbitrary units) within each significant cluster with subject-specific localization. Geometric shapes activate the intraparietal sulcus (IPS) and posterior inferior temporal gyrus (pITG), while causing reduced activation in broad bilateral ventral areas compared to other stimuli; see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for analysis of subject-specific ventral subregions. An accompanying explainer video is provided in <xref ref-type="video" rid="fig2video1">Figure 2—video 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Overview of the stimuli used for the category localizer.</title><p>(<bold>A</bold>) Average pixel value (left) and average standard deviation across pixels (right) for stimuli within each category (<italic>y</italic> axis). An ANOVA indicated no significant effect of the stimulus category on either the average or the standard deviation across pixels. (<bold>B</bold>) Average (top) and max (bottom) pixel value at each location across the eight possible visual categories used in the localizer.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Details of the fMRI results in children.</title><p>(<bold>A</bold>) Statistical map associated with the contrast ‘single geometric shape &gt; faces, houses, and tools’, projected on an inflated brain (top: adults; bottom: children; for illustration purposes, we display the uncorrected statistical map at the p &lt; 0.01 level). Notice how similar the activations are in both age groups. (<bold>B</bold>) Same as A, but for the contrast ‘single geometric shape &gt; all single-object visual categories (face, house, tools, Chinese characters)’. The activation maps are very similar to the previous contrast and very similar across age groups. (<bold>C</bold>) Whole-brain correlation of the BOLD signal with geometric regularity in children, as measured by the error rate in a previous online intruder detection task (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Positive correlations are shown in red and negative ones in blue. Voxel threshold p &lt; 0.001, no correction for multiple comparisons, but the p-value indicates the only cluster that was significant at the cluster-level corrected p &lt; 0.05 threshold. (<bold>D</bold>) Results of RSA analysis in children. No cluster was significant at the p &lt; 0.05 level for the geometric feature models; one right-lateralized occipital cluster reached significance for the convolutional neural network (CNN) encoding model (cluster-level corrected p = 0.019), and its symmetrical counterpart was close to the significance threshold (cluster-level corrected p = 0.062).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>fMRI response of subject-specific voxels in the ventral visual pathway to geometric shapes and other visual stimuli.</title><p>The brain slices show the group-level clusters associated with various contrasts known to elicit a selective response in the ventral visual pathway, in both age groups: VWFA (words &gt; others; green), FFA (faces &gt; others; purple), tool-selective regions of interest (ROIs) (tools &gt; others; red), and PPA (houses &gt; others; light blue). Within each ROI, plots show the mean beta coefficients for the BOLD effect within a subject-specific selection of the 10% best voxels, using independent runs for selection and plotting to avoid circularity and ‘double-dipping’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Overlap with math-responsive network and comparison with previous findings.</title><p>(<bold>A</bold>) Overlap in red between our geometry contrast in green (shape versus other single objects) and our number contrast in orange (numbers &gt; words), in three slices: two that intersect with the bilateral IPS areas (<italic>z</italic> = 60 and <italic>z</italic> = 52) and the rITG (<italic>z</italic> = 2) in both populations. To help visualize areas that coincide between populations but did not reach significance in one or the other, the maps here are uncorrected, p &lt; 0.01. (<bold>B</bold>) Statistical map from <xref ref-type="bibr" rid="bib112">Pinel et al., 2001</xref> showing areas where activation was correlated with numerical distance in a number comparison task, slice at <italic>z</italic> = 48 (p &lt; 0.001, uncorrected). (<bold>C</bold>) Statistical map from <xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref> showing the overlap between three math-related tasks, including high-level mathematical judgments in mathematicians; slice at <italic>z</italic> = 52. (<bold>D</bold>) Statistical tests for the ‘shape &gt; other categories’ contrast from the regions of interest (ROIs) identified in independent work (<xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref>) in both populations, with all ROIs having a significant test at the p &lt; 0.05 level except the LpITG.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig2-figsupp4-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106464-fig2-video1.mp4" id="fig2video1"><label>Figure 2—video 1.</label><caption><title>Explainer video for <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>Explainer videos are not peer reviewed.</p></caption></media></fig-group><sec id="s3-1"><title>Results</title><p><italic>Reduced activity to geometric shapes in ventral visual cortex.</italic> Classical ventral visual category-specific responses, for instance to faces or words, were easily replicated (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). However, when contrasting geometric shapes to faces, houses, or tools, we observed a massive under-activation of bilateral ventral occipito-temporal areas (unless otherwise stated, all statistics are at voxelwise p &lt; 0.001, clusterwise p &lt; 0.05 permutation-test corrected for multiple comparisons across the whole brain). All of the regions specialized for words, faces, tools, or houses showed this activity reduction when presented with geometric shapes (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>).</p><p>This group analysis was further supported by subject-specific analyses of regions of interest (ROIs) specialized for various categories of images (see Appendix and <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). First, unsurprisingly, the fusiform face area, which is known for its strong category-selectivity, showed the lowest responses to geometric shapes in the fusiform face area. Second, in a subject-specific ROI analysis of the visual word form area (VWFA), identified by its stronger response to alphabetical stimuli than to face, tools, and houses, no activity was evoked by single shapes or strings of three shapes above the level of other image categories such as objects or faces. This finding eliminates the possibility that geometric shapes might have been processed similarly to letters. Similarly, one could have thought that geometric shapes would be processed together with other complex man-made objects, which are often designed to be regular and symmetrical. However, geometric shapes again yielded a low activation in individual ventral visual voxels selective to tools, thus refuting this possibility. Finally, geometric shapes could have been encoded in the parahippocampal place area (PPA), which is known to encode the geometry of scenes, including abstract ones presented as Lego blocks (<xref ref-type="bibr" rid="bib47">Epstein et al., 1999</xref>). However, again, geometric shapes actually induced minimal activity in individually defined PPA voxels (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for a summary).</p><p><italic>Increased activity to geometric shapes in intraparietal and inferior temporal cortices</italic>. While the activity of the ventral visual cortex thus seemed to be globally reduced during geometric shape perception, we observed, conversely, a superior activation to geometric shapes than to face, tools, and houses in only two significant positive clusters, in the right anterior intraparietal sulcus (aIPS) and posterior inferior temporal gyrus (pITG) bordering on the lateral occipital sulcus. At a lower threshold (voxel p &lt; 0.001 uncorrected), the symmetrical aIPS in the left hemisphere was also activated (also see Appendix for additional results concerning the ‘3-shapes’ condition and with both shape conditions together).</p><p>Those areas are similar to those active during number perception, arithmetic, geometric sequences, and the processing of high-level math concepts (<xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref>; <xref ref-type="bibr" rid="bib8">Amalric and Dehaene, 2019</xref>; <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). To test this idea formally, we used the localizer to identify ROIs activated by numbers more than words. This contrast identified a left IPS cluster (p &lt; 0.05), while the symmetrically identified cluster in right IPS did not reach significance at the whole-brain level (p = 0.18). In both cases, however, the ROI was also significantly more activated by geometric shapes than other visual categories.</p><p>The observed overlap with number-related areas of the IPS is compatible with our hypothesis that geometric shapes are encoded as mental expressions that combine number, length, angle, and other geometric features. However, the association between geometric shapes and other arithmetic or mathematical properties could be acquired during schooling. To test whether the brain activity observed in adults reflects a basic intuition of geometry which is also present early on in child development, we replicated our fMRI study in 22 6-year-old first graders. When comparing the single shape condition and faces, houses, and tools, we observed the same reduction in ventral visual activity. We also observed greater aIPS activity, now significant in both hemispheres, though the identified right pITG cluster did not reach significance. Still, the right pITG voxels extracted from adults reached significance in children for the shape versus face, houses, and tools. Conversely, the left aIPS voxels extracted in children reached significance in adults. Indeed, the activation profiles were quite similar in both age groups (<xref ref-type="fig" rid="fig2">Figure 2D</xref>; see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for uncorrected statistical maps of adults and children, which are quite similar). In particular, aIPS activity was strongest to geometric shapes in children, while in adults strong responses to both geometric shapes and numbers were seen, indicating an overlap with previously observed areas involved in arithmetic (<xref ref-type="bibr" rid="bib8">Amalric and Dehaene, 2019</xref>; <xref ref-type="bibr" rid="bib113">Pinheiro-Chagas et al., 2018</xref>) (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>To better establish the correspondence between our findings and previous work, we evaluated the ‘geometric shape &gt; other visual categories’ contrast in six ROIs previously identified as part of the math-responsive network (<xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref>): bilateral IPS, bilateral MFG, and bilateral pITG. In all ROIs, in both adults and children, the average geometric shape contrast was significant at the p &lt; 0.05 level, except for the left posterior ITG in children (p = 0.09). Nevertheless, cautiousness is required here because activation overlap could arise without indicating that the same exact circuits and processes are involved, especially in smoothed group-level images. To partially mitigate this problem and test for subject-level overlap between the activations to geometric shapes and to numbers, we turned to within-subject analyses. We assessed whether the patterns of activation evoked by numbers and geometric shapes were more similar to each other than those evoked by numbers and other categories. For each subject, we computed the cross-nobis distance between the activations evoked by each of the experimental conditions. We then compared the distances for numbers versus geometric shapes, and for numbers versus the average of the other categories. We found that, in adults, in four of our five ROIs, the activations to geometric shapes were indeed more similar to numbers than to other categories (p &lt; 0.05 in lIPS, rITG, and bilateral ventral ROIs, p = 0.17 in rIPS). In children, the pattern of similarity was too noisy to be conclusive for bilateral IPS or the ITG, but the finding remained significant in bilateral ventral areas (both p &lt; 0.05).</p><p>In sum, geometric shapes led to reduced activity in ventral visual cortex, where other categories of visual images show strong category-specific activity. Instead, geometric shapes activated areas independently found to be activated by math- and number-related tasks, in particular the right aIPS.</p></sec></sec><sec id="s4"><title>Experiment 3: fMRI of the geometric intruder task</title><p>In a second fMRI experiment, we measured the detailed fMRI patterns evoked by our quadrilaterals, with the goal to submit them to a representational similarity analysis and test our hypothesis of a double dissociation between regions encoding visual (CNN) versus geometric codes. Adults and children performed an intruder task similar to our previous behavioral study (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>), with miniblocks allowing us to evaluate the activity pattern evoked by each quadrilateral shape. To render the task doable by 6-year-olds, we tested only 6 quadrilaterals, displayed as two half-circles of three items, and merely asked participants whether the intruder was on the left or the right of the screen (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dissociating two neural pathways for the perception of geometric shape.</title><p>(<bold>A</bold>) fMRI intruder task. Participants indicated the location of a deviant shape via button clicks (left or right). Deviants were generated by moving a corner by a fixed amount in four different directions. (<bold>B</bold>) Performance inside the fMRI: both populations tested displayed an increasing error rate with geometric shape complexity, which significantly correlates with previous data collected online. (<bold>C</bold>) Whole-brain correlation of the BOLD signal with geometric regularity in adults, as measured by the error rate in a previous online intruder detection task (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Positive correlations are shown in red and negative ones in blue. Voxel threshold p &lt; 0.001, cluster-corrected by permutation at p &lt; 0.05. Side panels show the activation in two significant regions of interest (ROIs) whose coordinates were identified in adults and where the correlation was also found in children (one-tailed test, corrected for the number of ROIs tested this way). (<bold>D</bold>) Whole-brain searchlight-based RSA analysis in adults (same statistical thresholds). Colors indicate the model which elicited the cluster: purple for convolutional neural network (CNN) encoding, orange for the geometric feature model, green for their overlap. An accompanying explainer video is provided in <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Additional models: fMRI.</title><p><italic>t</italic>-values inside significant clusters at the p &lt; 0.05 level for four models: geometric features (top left), convolutional neural network (CNN) encoding (top right), DINOv2 last layer (bottom left), and skeletal representations from <xref ref-type="bibr" rid="bib104">Morfoisse and Izard, 2021</xref> (bottom right). Skeletal representations from <xref ref-type="bibr" rid="bib13">Ayzenberg and Lourenco, 2019</xref> did not yield any significant clusters in adults. In children (bottom), only the DINOv2 elicited any significant cluster.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig3-figsupp1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106464-fig3-video1.mp4" id="fig3video1"><label>Figure 3—video 1.</label><caption><title>Explainer video for <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Explainer videos are not peer reviewed.</p></caption></media></fig-group><sec id="s4-1"><title>Results</title><p>Behavioral performance inside the scanner replicated the geometric regularity effect in adults and children: performance was best for squares and rectangles and decreased linearly for figures with fewer geometric regularities (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Behavior in the fMRI scanner was significantly correlated with an empirical measure of geometric regularity measured with an online intruder detection task (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>), which is used in all following analyses whenever we refer to geometric regularity. The neural bases of this effect were studied at the whole-brain level by searching for areas whose activation varied monotonically with geometrical regularity. In adults, this contrast identified a broad bilateral dorsal network including occipito-parietal, middle frontal, anterior insula, and anterior cingulate cortices, with accompanying deactivation in posterior cingulate (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This broad network, however, probably encompassed both regions involved in geometric shape encoding and those involved in the decision about the intruder task, whose difficulty increased when geometric regularity decreased. To isolate the regions specifically involved in shape coding, we performed searchlight RSA analyses, which focused on the pattern rather than the level of activation. Within spheres spanning the entire cortex, we asked in which regions the similarity matrix between the activation patterns evoked by the six shapes was predicted by the CNN encoding model, the geometric feature model, or both (<xref ref-type="fig" rid="fig3">Figure 3D</xref>; <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>). In adults, the CNN encoding model predicted neural similarity in bilateral lateral occipital clusters, while the geometric feature model yielded a large set of clusters in occipito-parietal, superior prefrontal, and right dorsolateral prefrontal cortex. The calcarine cortex was also engaged, possibly due to top–down feedback (<xref ref-type="bibr" rid="bib139">Williams et al., 2008</xref>). Both CNN encoding and geometric feature models had overlapping voxels in anterior cingulate and right premotor cortex, possibly reflecting the pooling of both visual codes for decision-making.</p><p>In children, possibly because the task difficulty was high, few results were obtained. The correlation of brain activity with regularity at the whole-brain level yielded a single significant cluster (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> for an uncorrected statistical map (voxelwise p &lt; 0.001), with the single significant whole-brain, cluster corrected significant cluster (p = 0.012) in the ventromedial prefrontal cortex). When testing the ROIs from adults in children, after correcting for multiple comparisons across the 14 ROIs, only the posterior cingulate was significant (p = 0.049), though two clusters were close to significance, both with p = 0.06: one positive in the left precentral gyrus (shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>), and one negative in the dorsolateral prefrontal cortex. Testing the ROIs identified in the visual localizer showed that they all exhibited a positive geometric difficulty effect in adults (all p &lt; 0.05), but did not reach significance in children, possibly due to excessive noise (as indicated by the much higher error rate in the task inside the scanner). In the searchlight analysis, no cluster associated with the geometric feature model reached significance at the whole-brain level (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). However, a right lateral occipital cluster was significantly captured by the CNN encoding model in children (p = 0.019) and its symmetrical counterpart was close to the significance threshold (p = 0.062) (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). This result might indicate that geometric features are not well differentiated prior to schooling. It could also reflect that children weight the geometric feature strategy less, as was found in previous work (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>); combined with difficulty of obtaining precise subject-specific activation patterns in young children, this could make the geometric feature strategy harder to localize.</p></sec></sec><sec id="s5"><title>Experiment 4: oddball paradigm of geometric shapes in MEG</title><p>The temporal resolution of fMRI does not allow tracking the dynamic of mental representations over time. Furthermore, the previous fMRI experiment suffered from several limitations. First, we studied six quadrilaterals only, compared to 11 in our previous behavioral work (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Second, we used an explicit intruder detection, which implies that the geometric regularity effect was correlated with task difficulty, and we cannot exclude that this factor alone explains some of the activations in <xref ref-type="fig" rid="fig3">Figure 3C</xref> (although it is much less clear how task difficulty alone would explain the RSA results in <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Third, the long display duration, which was necessary for good task performance especially in children, afforded the possibility of eye movements, which were not monitored inside the 3T scanner and again could have affected the activations in <xref ref-type="fig" rid="fig3">Figure 3C</xref>.</p><p>To overcome those issues, we replicated the experiment in adult MEG with three important changes: (1) all 11 quadrilaterals were studied; (2) participants were simply asked to fixate and attend to every shape, without performing any explicit task; (3) shapes were presented serially, one at a time, at the center of screen, with small random changes in rotation and scaling parameters; and (4) in miniblocks of 30 s each, a fixed quadrilateral shape appeared repeatedly, interspersed with rare intruders whose bottom right corner was shifted by a fixed amount (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). This design allowed us to study the neural mechanisms of the geometric regularity effect without confounding effects of task, task difficulty, or eye movements. Would the shapes be automatically encoded according to their geometric regularities even in such a passive context? And would brain responses indicate that the intruders continued to be detected more easily among geometric regular shapes than among irregular ones, as previously found behaviorally in an active task (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>)?</p><sec id="s5-1"><title>Results</title><p>In spite of the passive design, MEG signals revealed an automatic detection of intruders, driven by geometric regularity. We trained logistic regression decoders to classify the MEG signals at each timepoint following a shape as arising from a reference shape or from an intruder (see Method and <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Overall, the decoder performed above chance level, reaching a peak at 428 ms, indicating the presence of brain responses specific to intruders. Crucially, although trained on all shapes together, the decoder performed better with geometrically regular shapes than with irregular shapes (and better than chance for each), indicating that oddball shapes were more easily detected within blocks of regular shapes, as previously found behaviorally (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Indeed, a regression of decoding performance on geometrical regularity yielded a significant spatiotemporal cluster of activity, which first became significant around ~160 ms and peaked at 432 ms (here and after, temporal clusters are purposefully reported with approximate bounds following <xref ref-type="bibr" rid="bib122">Sassenhagen and Draschkow, 2019</xref>). A geometrical regularity effect was also seen in the latency of the outlier response (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>; correlation between geometric regularity and the latency when average decoding performance first exceeded 57% correct; one-tailed <italic>t</italic>-test of each participant’s regression slope against 0: <italic>t</italic> = −1.83, p = 0.041) indicating that oddballs yielded both a larger and a faster response when the shapes were geometrically more regular. The same effect was found when training separate outlier decoders for each shape, thus refuting an alternative hypothesis according to which all outliers evoke identical amounts of surprise, but in different directions of neural space (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Overall, the results fully replicate prior behavioral observations of the geometric regularity effect (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>) and suggest that the computation of a geometric code and its deviations occurs under passive instructions and starts with a latency of about ~150 ms.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Using magnetoencephalography (MEG) to time the two successive neural codes for geometric shapes.</title><p>(<bold>A</bold>) Task structure: participants passively watch a constant stream of geometric shapes, one per second (presentation time 800 ms). The stimuli are presented in blocks of 30 identical shapes up to scaling and rotation, with four occasional deviant shapes. Participants do not have a task to perform besides fixating. (<bold>B, C</bold>) Performance of a classifier using MEG signals to predict whether the stimulus is a regular shape or an oddball. Left: performance for each shape; middle: correlation with geometrical regularity (same <italic>x</italic> axis as in <xref ref-type="fig" rid="fig3">Figure 3C</xref>); right: visualization of the average decoding performance over the cluster. In B, training of the classifier was performed on MEG signals from all 11 shapes; In C, 11 different classifiers were trained separately, one for each shape. (<bold>D</bold>) Sensor-level temporal RSA analysis. At each timepoint, the 11 × 11 dissimilarity matrix of MEG signals was modeled by the two model representational dissimilarity matrices (RDMs) in <xref ref-type="fig" rid="fig1">Figure 1D</xref>, and the graph shows the time course of the corresponding whitened correlation coefficients. Below the time courses, we display the average empirical dissimilarity matrix across participants at two notable timepoints: when the correlation with the convolutional neural network (CNN) and geometric feature models are maximal (CNN: <italic>t</italic> = 84 ms; geometric features: 232 ms) (<bold>E</bold>) Source-level temporal-spatial searchlight RSA. Same analysis as in C, but now after reconstruction of cortical source activity. An accompanying explainer video is provided in <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Additional models: behavior and magnetoencephalography (MEG).</title><p>(<bold>A</bold>) Correlation between several models and the average representational dissimilarity matrix (RDM) across participants. In particular, we have added the last two layers of DINOv2 (<xref ref-type="bibr" rid="bib107">Oquab et al., 2023</xref>) as well as two different implementations of distances in skeletal spaces (<xref ref-type="bibr" rid="bib13">Ayzenberg and Lourenco, 2019</xref>; <xref ref-type="bibr" rid="bib104">Morfoisse and Izard, 2021</xref>). (<bold>B</bold>) <xref ref-type="fig" rid="fig1">Figure 1D</xref> with the symbolic model replaced with the empirical RDM obtained from the last layer of DINOv2 using Euclidean distance. (<bold>C</bold>) <xref ref-type="fig" rid="fig4">Figure 4C</xref> with the same replacement of the symbolic model with the last layer of DINOv2. (<bold>D</bold>) <xref ref-type="fig" rid="fig4">Figure 4D</xref> with the same replacement of the symbolic model with the last layer of DINOv2. (E) Time course of the similarity between empirical RDMs and two additional neural networks: a vision transformer network (ViT, top <xref ref-type="bibr" rid="bib45">Dosovitskiy et al., 2020</xref>), and a large convolutional neural network (CNN) (ConvNeXT, bottom, <xref ref-type="bibr" rid="bib96">Liu et al., 2022</xref>), both with many parameters (respectively ~1 billion and ~800 million) and trained on 2 billion images <xref ref-type="bibr" rid="bib30">Cherti et al., 2023</xref>; <xref ref-type="bibr" rid="bib126">Schuhmann et al., 2022</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-fig4-figsupp1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106464-fig4-video1.mp4" id="fig4video1"><label>Figure 4—video 1.</label><caption><title>Explainer video for <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>Explainer videos are not peer reviewed.</p></caption></media></fig-group><p>We next used temporal RSA to further probe the dynamics of the perception of the reference, non-intruder shapes. For each timepoint, we estimated the 11 × 11 neural dissimilarity matrix across all pairs of reference shapes using sensor data with cross-nobis distances (<xref ref-type="bibr" rid="bib138">Walther et al., 2016</xref>), and entered them in a multiple regression with those predicted by CNN encoding and geometric feature models. The coefficients associated with each predictor are shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. An early cluster (observed cluster extent at approximately 60–320 ms, peak at 84 ms; p &lt; 0.001) showed a significant correlation of the CNN encoding model on brain similarity. It was followed by two significant clusters associated with the geometric feature model separated by two timepoints that did not pass the cluster-formation threshold (~128–184 ms then ~196–400 ms, overall peak at 232 ms; p &lt; 0.001). Those results are compatible with our hypothesis of two distinct stages in geometric shape perception and suggest that a geometric feature-based encoding is present, especially around ~200–250 ms. This analysis yielded similar clusters when performed on a subset of shapes that do not have an obvious name in English, as was the case for the behavior analysis (CNN encoding: 64.0–172.0 ms; then 192.0–296.0 ms; both p &lt; 0.001: geometric features: 312.0–364.0 ms with p = 0.008, the later timing could indicate that geometric features for less regular shapes take longer to estimate, replicating the latency effect found in the oddball decoding analysis).</p><p>To understand which brain areas generated these distinct patterns of activations and probe whether they fit with our previous fMRI results, we performed a source reconstruction of our data. We projected the sensor activity onto each participant’s cortical surfaces estimated from T1 images. The projection was performed using eLORETA (<xref ref-type="bibr" rid="bib74">Jatoi et al., 2014</xref>) and empty room recordings acquired on the same day to estimate noise covariance, with the default parameters of mne-bids-pipeline. Sources were spaced using a recursively subdivided octahedron (oct5). Group statistics were performed after alignment to fsaverage. We then replicated the RSA analysis with searchlights sweeping across cortical patches of 2 cm geodesic radius (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). The CNN encoding model captured brain activity in a bilateral occipital and posterior parietal cluster, while the geometric model accounted for subsequent activity starting at ~200 ms and spanning over broader dorsal occipito-parietal and intraparietal, prefrontal, anterior, and posterior cingulate cortices. This double dissociation closely paralleled fMRI results (compare <xref ref-type="fig" rid="fig3">Figures 3D</xref> and <xref ref-type="fig" rid="fig4">4D</xref>; see also <xref ref-type="video" rid="video1">Video 1</xref> for a movie of the significant sources associated to either model across time), with greater spatial spread due to the unavoidable imprecision of MEG source reconstruction.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106464-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Spatiotemporal dynamics of the magnetoencephalography (MEG) data compared to the models.</title><p>Searchlight-based timepoint-per-timepoint RSA analysis across shapes of the MEG data. Significant (p &lt; 0.05) sources associated with the convolutional neural network (CNN) model are shown in purple, significant sources associated with the geometric feature model in orange; overlap is shown in green. Cluster-corrected clusters reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption></media></sec></sec><sec id="s6"><title>Modeling of the results with alternative models of visual perception</title><p>In the above analyses, we contrasted two models for our data: a CNN versus a list of geometric properties. However, there are many other models of vision. In this section, we model our data with two additional classes of models, based on the shape skeleton and principal axis theory, or vision transformers neural networks.</p><sec id="s6-1"><title>Skeleton models</title><p>Skeletal representations (<xref ref-type="bibr" rid="bib23">Blum, 1973</xref>) offer biologically plausible and computationally tractable models of object representation in human cognition. Several methods to derive skeletal representation from object contour have been put forward, including Bayesian estimation that trades off accuracy for conciseness (<xref ref-type="bibr" rid="bib51">Feldman and Singh, 2006</xref>). More recently, distance estimators between shapes based on their skeletal structure have been used to model human behavior (<xref ref-type="bibr" rid="bib15">Ayzenberg et al., 2022</xref>; <xref ref-type="bibr" rid="bib13">Ayzenberg and Lourenco, 2019</xref>; <xref ref-type="bibr" rid="bib42">Denisova et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Lowet et al., 2018</xref>; <xref ref-type="bibr" rid="bib104">Morfoisse and Izard, 2021</xref>), and hierarchical object decomposition has been rigorously described and tested (<xref ref-type="bibr" rid="bib56">Froyen et al., 2015</xref>). Remarkably, even when simply asked to tap a geometric shape on a touchscreen anywhere they want, human taps cluster along the shape skeleton (<xref ref-type="bibr" rid="bib52">Firestone and Scholl, 2014</xref>).</p><p>To model our data, we needed a distance metric between two shape skeletons. We explored two metrics: one that measures the shortest distance between two skeletons after optimal alignment and rotation (<xref ref-type="bibr" rid="bib13">Ayzenberg and Lourenco, 2019</xref>; <xref ref-type="bibr" rid="bib71">Jacob et al., 2021</xref>), and one that measures the difference in angle and length of matched sub-parts of the shapes’ skeleton (<xref ref-type="bibr" rid="bib104">Morfoisse and Izard, 2021</xref>). Our results are summarized in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>; the first observation is that over our 11 quadrilaterals, these two skeletal metrics were not significantly correlated, suggesting that they capture very different properties when applied to minimal geometric shapes and are possibly not best suited to characterize them. Additionally, neither of these metrics predicted the behavioral data better than the CNN models.</p><p>When tested on fMRI data, the first method based on optimal alignment and rotation did not elicit any significant cluster in either population. In adults, but not in children, the second method based on the structure of the skeletons significantly correlated with bilateral clusters in the visual cortex, as well as a significant cluster in the right premotor cortex (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). These areas constitute a subset of the areas identified with the CNN encoding model.</p><p>When tested on MEG data, the first method similarly did not elicit any significant temporal cluster at the p &lt; 0.05 level. The second method yielded two significant temporal clusters, the first from ~75 to ~260 ms and the second from ~285 to ~370 ms. When looking at the reconstructed sources, the second temporal cluster did not yield any significant spatial cluster, and the first cluster elicited a bilateral cluster encompassing both early occipital area and the early dorsal visual stream, with no significant localization in the frontal cortex.</p><p>Overall, for our dataset, these models appear partially similar to the CNN model insofar as they capture subsets of the timings and localizations of the neural data captured by the CNN model, but they do not strongly predict the behavior data as the geometric feature model does, nor capture the broad cortical networks associated with the geometric feature model.</p></sec><sec id="s6-2"><title>Vision transformers and larger neural networks</title><p>While small CNN models have well-known limits in capturing several aspects of human perception (<xref ref-type="bibr" rid="bib24">Bowers et al., 2022</xref>; <xref ref-type="bibr" rid="bib71">Jacob et al., 2021</xref>), those limits are constantly being pushed. First, connectionist models based on the more advanced transformer architecture may provide a closer match to human data, especially when it comes to symbolic or mathematical regularities (<xref ref-type="bibr" rid="bib26">Campbell et al., 2024</xref>). Second, even within the CNN-like architecture, much larger neural networks, trained on vastly richer datasets, may achieve superior similarity to humans than CORnet. We briefly consider both possibilities.</p><p>First, using the same method as with CNNs, we modeled our empirical RDMs with distance measured between pairs of shapes in the late layers of a visual transformer, DINOv2 (<xref ref-type="bibr" rid="bib107">Oquab et al., 2023</xref>). In <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>, we report the empirical RDMs, the DINO RDM, as well as the CORnet CNN RDM, together with a plot of each subject’s correlation with the CNN and DINO matrices. The DINO predictor yielded a dissimilarity matrix quite similar to the human behavioral one; indeed, the distribution of coefficients for the DINO predictor was significantly greater than 0 (95% confidence interval [0.73, 0.75]; p &lt; 0.001) and significantly different from the CNN predictor (p &lt; 0.001). Note that in this analysis, the distribution of CNN predictors is significantly lower than zero (p &lt; 0.001), which was not the case when contrasted with the symbolic model. This suggests that the DINO predictor may involve a mixture of symbolic-like features and perceptual features. Indeed, its correlation with the symbolic model is 0.78 and the correlation with the cornet is 0.56. This may explain why humans are best predicted with a mixture of these two models, but with different weights: to achieve the best fit, a model with both DINO and CNN puts a high weight on DINO and then negatively corrects the excess perceptual features by putting a negative weight on the CNN model.</p><p>This analysis was confirmed by a fit of the fMRI data (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) and MEG data (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>): in both cases, the RSA analysis with DINO last layer yielded results that were very similar to a mixture of the CORnet CNN and the symbolic model. In fMRI in adults, a distributed set of cortical areas was significantly correlated with the DINO model, and these areas overlapped with the union of the areas predicted by both of our original models. In children, DINOv2 elicited a significant cluster in the right visual cortex, in areas overlapping with the areas activated by the CNN model in children.</p><p>In MEG, a large and significant cluster was predicted by the DINO model. Its timing (significant cluster from ~64 to ~400 ms) overlapped with the spatiotemporal clusters found with both the CNN model and the symbolic model. Source analysis indicated that a correlation with DINO originated from widespread sources that, again, overlapped with both those found with the CNN model (early occipital areas) and the symbolic model (widespread regions included dorsal, anterior temporal, and frontal sources).</p><p>Taken together, these results suggest that the last layer of DINO is driven by both early visual and higher-level geometric features, and that both are needed to fit human data. While these findings may suggest that network architecture may be crucial to mimic human geometric perception, continuing work in our lab challenges this conclusion because similar results were obtained by performing the same analysis, not only with another vision transformer network, ViT, but crucially using a much larger convolutional neural network, ConvNeXT, which comprises ~800 million parameters and has been trained on two billion images, likely including many geometric shapes and human drawings. For the sake of completeness, RSA analysis in sensor space of the MEG data with these two models is provided in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. A systematic investigation of the impact of network architecture, dataset size, and dataset content is beyond the scope of the present paper, but the present data could serve as a reference dataset with which to understand which of these factors ultimately cause a neural network to display symbolic properties close to those found in the human brain.</p></sec></sec><sec id="s7" sec-type="discussion"><title>Discussion</title><p>Our previous behavioral work suggested that the human perception of geometric shapes cannot be solely explained by simple CNN encoding models – rather, humans also encode them using nested combinations of discrete geometric regularities (<xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Here, we provide direct human brain-imaging evidence for the existence of two distinct cortical networks with distinct representational schemes, timing, and localization: an early occipitotemporal network well modeled by a simple CNN, and a dorso-frontal network sensitive to geometric regularity.</p><sec id="s7-1"><title>Behavioral signatures of geometric regularity</title><p>At the behavioral level, the new large-scale data that we report here (<italic>n</italic> = 330 participants) fully replicated our prior finding that human perception of geometric shape dissimilarity, as evaluated by a visual search task, is best predicted by a model that relies on exact geometric features, rather than by a CNN. These exact geometric feature representations can be thought of as a more abstract and compressed representations of shapes: they replace continuous variations along certain dimensions (such as angles or directions) with categorical features (right angle or not, parallel or not). Such a representation, which remains invariant over changes in size and planar rotation, turns a rich visual stimulus into a compressed internal representation.</p><p>Even in educated human adults, a small proportion of the variance in behavioral dissimilarity remained predicted by the simple CNN model, so that both CNN and geometry predictors were significant in a multiple regression of human judgments. Previously, we only found such a significant mixture of predictors in humans without formal western education (whether French preschoolers or adults from the Himba community, mitigating the possible impact of explicit western education, linguistic labels, and statistics of the environment on geometric shape representation) (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). It is likely that the greater power afforded by the present experiment yielded a greater sensitivity. This finding comforts the hypothesis that two strategies for geometric shape perception are available to humans: one based on hierarchical visual processing (captured by the CNN), and the other based on an analysis of discrete geometric features – the latter dominating strongly in educated adults.</p></sec><sec id="s7-2"><title>fMRI results</title><p>Using fMRI, we found that the mere perception of geometric shapes, compared to various other visual categories, yields a reduced bilateral activation of the entire ventral visual pathway, together with a localized increased activation in the anterior IPS and in the posterior ITG. Furthermore, geometric regularity modulated fMRI activation in most of the bilateral occipito-parietal pathway, middle frontal gyrus, and anterior insula. Finally, the most diagnostic result we found is that the representational similarity matrix in these regions was predicted by geometric similarity rather than by the simple CNN (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p><p>The IPS areas activated by geometric shapes overlap with those active during the comprehension of elementary as well as advanced mathematical concepts (<xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>; <xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref>; <xref ref-type="bibr" rid="bib8">Amalric and Dehaene, 2019</xref>). Although this finding must be interpreted with great caution, as activation overlap need not imply shared cognitive processes, it was confirmed at the single-subject level and agrees with the proposed involvement of these regions in an abstract, modality-independent symbolic encoding of shapes. Further support for this idea comes from the fact that these regions can be equally activated in sighted and in blind individuals when they perform mathematics (<xref ref-type="bibr" rid="bib7">Amalric et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Kanjlia et al., 2016</xref>) or when they evaluate the shapes of manmade objects (<xref ref-type="bibr" rid="bib141">Xu et al., 2023</xref>). Thus, the representation of shapes computed in these regions is more abstract and amodal than the hierarchy of visual filters that is thought to capture ventral visual image recognition.</p><p>One referee noted that we contrasted geometric shapes with other categories of images that may, themselves, possess some geometric features. For instance, faces possess a plane of quasi-symmetry, and so do many other man-made tools and houses. Thus, our subtraction isolated the geometrical features that are present in simple regular geometric shapes (e.g. parallels, right angles, and equality of length) over and above those that might already exist, in a less pure form, in other categories.</p></sec><sec id="s7-3"><title>MEG results</title><p>Using MEG, we found that even during the passive perception of simple quadrilateral shapes, in the absence of an explicit task, participants were sensitive to their geometric regularity: occasional oddball shapes elicited greater violation-of-expectation signals within blocks of regular shapes than within blocks of irregular shapes. Additionally, using RSA and source reconstruction, we observed a spatial and temporal dissociation in the processing of quadrilateral shapes: an early occipital response, well predicted by CNN models of object recognition, was followed by broadly distributed cortical activity that correlated with a model of exact geometric features. Despite the limited accuracy afforded by source reconstruction in MEG, there was considerable overlap between our RSA analysis in fMRI and in MEG.</p><p>This early response of occipital areas, followed by a later activation of a broad dorso-frontal network, may seem at odds with some results showing that, during visual image processing, the dorsal pathway may respond faster than the ventral pathway (<xref ref-type="bibr" rid="bib16">Ayzenberg et al., 2023</xref>; <xref ref-type="bibr" rid="bib33">Collins et al., 2019</xref>). However, in this work, we specifically probed the processing of geometric shapes that, if our hypothesis is correct, are represented as mental expressions that combine geometrical and arithmetic features of an abstract categorical nature, for instance representing ‘four equal sides’ or ‘four right angles’. It seems logical that such expressions, combining number, angle, and length information, take more time to be computed than the first wave of feedforward processing within the occipito-temporal visual pathway, and therefore only activate thereafter.</p></sec><sec id="s7-4"><title>Reduced activation of the ventral visual pathway</title><p>Strikingly, our fMRI data also evidenced a hypo-activation of the ventral visual pathways to geometric shapes relative to other pictures. A large bilateral cluster of reduced activity was found when comparing shapes versus other visual categories, and a similar reduced activity was found in each of four visual areas specialized for various visual categories (faces, words, tools, and places). This reduction is compatible with our hypothesis that geometric shapes are processed differently from other pictures, and with empirical finding that a simple feedforward CNN, whose architecture usually provides a relatively good fit to inferotemporal cortex activity (<xref ref-type="bibr" rid="bib34">Conwell et al., 2021</xref>; <xref ref-type="bibr" rid="bib89">Kubilius et al., 2019</xref>; <xref ref-type="bibr" rid="bib125">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="bib142">Yamins et al., 2014</xref>), did not provide a good model of geometric shape perception. Because our shapes consisted of a few straight lines, it is likely that they solicited the computations performed by the ventral pathway only minimally, giving rise to a lower activation when compared to more complex visual stimuli such as pictures of faces or houses.</p></sec><sec id="s7-5"><title>Two visual pathways</title><p>Brain-imaging studies of the ventral visual pathway indicate that it is subdivided into patches responding to different visual categories such as faces, tools, or houses, with a partial correspondence between human and non-human primates (<xref ref-type="bibr" rid="bib11">Arcaro et al., 2017</xref>; <xref ref-type="bibr" rid="bib101">Margalit et al., 2020</xref>; <xref ref-type="bibr" rid="bib116">Rauschecker et al., 2012</xref>; <xref ref-type="bibr" rid="bib131">Tsao and Livingstone, 2008</xref>; <xref ref-type="bibr" rid="bib82">Khosla et al., 2022</xref>; <xref ref-type="bibr" rid="bib4">Allen et al., 2021</xref>; <xref ref-type="bibr" rid="bib87">Kriegeskorte et al., 2008b</xref>). Converging evidence from behavior (<xref ref-type="bibr" rid="bib21">Biederman and Ju, 1988</xref>) and neuroimaging (<xref ref-type="bibr" rid="bib15">Ayzenberg et al., 2022</xref>; <xref ref-type="bibr" rid="bib94">Lescroart and Biederman, 2013</xref>; <xref ref-type="bibr" rid="bib108">Papale et al., 2019</xref>; <xref ref-type="bibr" rid="bib109">Papale et al., 2020</xref>) has underlined the central role of the shape of objects, contour, and texture in object recognition. A basic complementarity has been identified between the ventral visual pathway, crucial for visual identification, and the dorsal occipito-parietal route, extracting visuo-spatial information about orientation and motor affordances. However, this simplified view needs to be nuanced by evidence of shape processing in the dorsal pathway as well, especially in posterior areas (<xref ref-type="bibr" rid="bib55">Freud et al., 2016</xref>; <xref ref-type="bibr" rid="bib140">Xu, 2018</xref>), in both human and non-human primates. More specifically, recent work challenges the idea that the shape of objects is computed solely in the ventral pathway, and instead suggests that the dorsal pathway, and in particular the IPS, may first compute global shape information and then propagate it to the ventral pathway for further processing (<xref ref-type="bibr" rid="bib14">Ayzenberg and Behrmann, 2022</xref>). Our results are compatible with this new look at dorsal/ventral interactions: in our fMRI localizer, geometric shapes elicited reduced ventral and increased dorsal activation when compared to other visual categories. Geometric shapes could be considered as conveying very pure information about global shape, entirely driven by contour geometry and fully devoid of other information such as texture, shading, or curvature. More research is needed to understand the dynamic collaboration between the ventral and dorsal streams during shape recognition, but the present results, as well as the arguments put forward in <xref ref-type="bibr" rid="bib14">Ayzenberg and Behrmann, 2022</xref>, concur to suggest that shape identification does not solely rely on the ventral visual pathway.</p><p>Interestingly, recent work shows that within the visual cortex, the strongest relative difference in cortical surface expansion between human and non-human primates is localized in parietal areas (<xref ref-type="bibr" rid="bib103">Meyer et al., 2025</xref>). If this expansion reflected the acquisition of new processing abilities in these regions, it might explain the observed differences in geometric abilities between human and non-human primates (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>).</p></sec><sec id="s7-6"><title>Lateral occipital cortex</title><p>Of special relevance to this work is the role of the lateral occipital cortex (LOC) in shape perception (<xref ref-type="bibr" rid="bib62">Grill-Spector et al., 2001</xref>). The posterior ITG activation that we observed lies just anterior to the LOC and possibly overlapped with it. The LOC has been repeatedly associated with shape processing (<xref ref-type="bibr" rid="bib61">Grill-Spector et al., 1998</xref>; <xref ref-type="bibr" rid="bib77">Kanwisher et al., 1996</xref>; <xref ref-type="bibr" rid="bib84">Kourtzi and Kanwisher, 2000</xref>; <xref ref-type="bibr" rid="bib100">Malach et al., 1995</xref>), and our work converges to suggest that this region and the cortex just anterior to it play a key role in the perception of simple shapes in a way that is invariant to 2D and 3D rotations (<xref ref-type="bibr" rid="bib85">Kourtzi et al., 2003</xref>). However, unlike ours, previous work focused primarily on irregular potato-like shapes or objects and their contours.</p><p>Object selectivity in the LOC is already present in early infancy, with a sensitivity to shape and not texture already observed at 6 months of age (<xref ref-type="bibr" rid="bib46">Emberson et al., 2017</xref>). At age 5–10, size invariance, but not viewpoint invariance, has been established (<xref ref-type="bibr" rid="bib105">Nishimura et al., 2015</xref>). However, to our knowledge, no study has targeted geometrically regular shapes, and in particular how changes in regularity impact LOC activity. While some of our quadrilateral stimuli can be constructed as 3D projections of one another (either by perspective or by orthogonal projection), this should make them more similar to each other within the LOC given its object viewpoint invariance. Further work should use within-subject LOC localizers such as objects minus scrambled-objects to establish the exact cortical relation between the present geometric regularity effect and the classical LOC region. We speculate that the more anterior part of lateral occipito-temporal cortex may extract more abstract geometric descriptors of shapes than the classical LOC, as also suggested by its sensitivity to mathematical training (<xref ref-type="bibr" rid="bib6">Amalric and Dehaene, 2016</xref>).</p></sec><sec id="s7-7"><title>Development of geometry representations</title><p>Interestingly, the IPS and posterior ITG activations to geometric shapes were consistently observed at very similar locations in adult and 6-year-olds. Furthermore, the overlap with a math-responsive network was present in both age groups, with ROIs that respond to number more than words also responding to geometric shapes more than other visual categories. This finding fits with previous evidence of an early responsivity of the IPS to numbers and arithmetic in young children prior to formal schooling (<xref ref-type="bibr" rid="bib10">Ansari, 2008</xref>; <xref ref-type="bibr" rid="bib27">Cantlon and Li, 2013</xref>; <xref ref-type="bibr" rid="bib67">Izard et al., 2008</xref>). In agreement with the existence of a behavioral geometric regularity effect in preschoolers (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>), we hypothesize that an intuitive encoding of number and geometric features precedes schooling and possibly guides the subsequent acquisition of formal mathematics (<xref ref-type="bibr" rid="bib70">Izard et al., 2022</xref>).</p><p>On the other hand, the fMRI results from the intruder task were much less stable in children, and neither the geometric regularity effect nor the geometry-based RSA analysis yielded strong results at the whole-brain level (only one ventromedial cluster was associated with the regularity effect, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). In previous work, we have shown that besides an overall decrease in performance between adults and first graders, the profile of behavior across different shapes is partially distinct in preschoolers: in them, both the CNN and the symbolic model are on par in predicting behavior, suggesting that unlike adults, children frequently mix the two strategies for identifying geometric shapes. Such a mixture of strategies could explain why the correlation with our empirical estimation of complexity, based on adult data, yielded a weaker and more noisy correlation in children. The RSA analysis is compatible with this: in the group analysis, while the CNN predictor was significant in the visual cortex of children, the symbolic geometry model did not reach significance anywhere. Future research with a larger number of participants could attempt to sort out children as a function of whether their behavior is dominated by geometric features or by the CNN model, and then examine how their brain activity profiles differ.</p></sec><sec id="s7-8"><title>Artificial neural networks: limits and promising directions</title><p>While simple CNNs are predictive of early ventral visual activity, the present work adds to a growing list of their limits as full models of visual perception (<xref ref-type="bibr" rid="bib24">Bowers et al., 2022</xref>; <xref ref-type="bibr" rid="bib71">Jacob et al., 2021</xref>). Simple CNNs have been shown to fail to model many visual illusions (<xref ref-type="bibr" rid="bib24">Bowers et al., 2022</xref>; <xref ref-type="bibr" rid="bib71">Jacob et al., 2021</xref>), geometrically impossible objects (<xref ref-type="bibr" rid="bib64">Heinke et al., 2021</xref>), global shape, part-whole relations, and other Gestalt properties (<xref ref-type="bibr" rid="bib24">Bowers et al., 2022</xref>). While larger CNNs may overcome some of these issues, and indeed here we found that the ConvNeXT CNN-like architecture could mimic our MEG data, their huge training sets likely include human geometric shapes and drawings, without which they may fail in out-of-domain generalization (<xref ref-type="bibr" rid="bib102">Mayilvahanan et al., 2025</xref>). Human children, by contrast, recognize and produce geometric drawings early on and with little or no explicit training (<xref ref-type="bibr" rid="bib57">Goodenough, 1928</xref>; <xref ref-type="bibr" rid="bib97">Long et al., 2024</xref>; <xref ref-type="bibr" rid="bib121">Saito et al., 2014</xref>). Thus, these observations suggest that, to capture the human sense of geometry, current artificial-intelligence models may have to be supplemented, not only by increasing their size (<xref ref-type="bibr" rid="bib34">Conwell et al., 2021</xref>) and training sets, but also by changing their architecture in ways that better incorporate findings from psychology and neuroscience (<xref ref-type="bibr" rid="bib130">Thompson et al., 2023</xref>; <xref ref-type="bibr" rid="bib20">Biederman, 1987</xref>). Indeed, connectionist models based on the transformer architecture seem to be superior in capturing human geometric perception (<xref ref-type="bibr" rid="bib26">Campbell et al., 2024</xref>), and indeed we found that our results could also be captured by the late layers of visual transformer DINOv2 (<xref ref-type="bibr" rid="bib107">Oquab et al., 2023</xref>), see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. This finding may offer an exciting avenue to analyze a mechanistic implementation of symbol-like representations in a connectionist architecture. However, it is also possible that much more sophisticated mechanisms of Bayesian program inference are required to truly capture the remarkable efficiency with which humans grasp abstract shapes and geometric drawings (<xref ref-type="bibr" rid="bib90">Lake et al., 2015</xref>; <xref ref-type="bibr" rid="bib91">Lake et al., 2017</xref>; <xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>).</p></sec><sec id="s7-9"><title>Shape skeleton and principal axis</title><p>Skeletal representations have been proposed as human-like representations of visual shapes, particularly appropriate for biological shapes such as animals or plants (<xref ref-type="bibr" rid="bib23">Blum, 1973</xref>). Even for simple geometric shapes, skeletal representation may be automatically and unconsciously computed (<xref ref-type="bibr" rid="bib52">Firestone and Scholl, 2014</xref>). However, in the present work, the two skeletal shape representations we tested did not model human data well (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Within our quadrilaterals, which are visually similar to one another, all but the square possess the same skeleton topological graph, with only variations in the length and angles of the skeletal segments. Thus, a skeletal representation is probably not the source of the large geometric regularity effect that we observed here and in past work (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). Still, as previously argued (<xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>), geometric and medial axis theories need not be seen as incompatible. Rather, we conceive of them as complementary codes, best suited for distinct shape domains. Even after extraction of a figure’s skeleton, further compression could be achieved by encoding its geometric regularity, and this may be what humans do when they draw a snake, for instance, as a geometrically regular zigzag.</p></sec><sec id="s7-10"><title>Neurophysiological implementation of geometry</title><p>Symbolic models are often criticized for lack of a plausible neurophysiological implementation. It is therefore important to discuss whether and how the postulated symbolic geometric code could be realized in neural circuits. There are several distinct challenges. First, some neurons should encode individual features such as lines and curves, or features formed by their relationships (e.g. parallelism, right angle). Second, these codes should be discrete and categorical, forming sharp boundaries between, say, parallel versus non-parallel lines. Third, they should enter into compositional expressions describing how individual features combine into the whole shape (e.g. ‘a shape with four equal sides and four equal right-angles’).</p><p>The first point has been studied in both humans and monkeys in the context of research on the non-accidental properties (NAPs) of objects. NAPs are qualitative features of object shapes, such as straight versus curved, which remain invariant when an object is rotated. Metric properties (MPs), on the other hand, are quantitative properties such as amount of curvature that do not exhibit such invariance. Behavioral research has demonstrated that, for equivalent amounts of pixel change in the image, changes in NAPs are more discriminable than changes in MPs, in human adults with and without formal western education, toddlers, and even in infants (<xref ref-type="bibr" rid="bib9">Amir et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Biederman et al., 2009</xref>; <xref ref-type="bibr" rid="bib81">Kayaert and Wagemans, 2010</xref>). Furthermore, the firing of neurons in monkey infero-temporal cortex is more sensitive to changes in NAPs than in MPs, whether they are conveyed by 3D shapes (<xref ref-type="bibr" rid="bib78">Kayaert et al., 2003</xref>) or 2D shapes (<xref ref-type="bibr" rid="bib79">Kayaert et al., 2005a</xref>; <xref ref-type="bibr" rid="bib80">Kayaert et al., 2005b</xref>). These findings occurred even in the absence of a task other than passive fixation and without particular training for the stimuli. Further work with 2D shapes, including triangles and quadrilaterals partially overlapping with the present stimuli, showed that IT cortex neurons could also be sensitive to more global shape properties such as axis curvature or ‘taper’ (the difference between a rectangle seen upfront or at a slanted axis) (<xref ref-type="bibr" rid="bib79">Kayaert et al., 2005a</xref>). Multidimensional scaling of macaque neural population responses organized the tested shapes in a systematic ‘shape space’ where, for instance, the rectangle occupies an extreme corner, thus making it distinct from its curved or tapered variants (<xref ref-type="bibr" rid="bib79">Kayaert et al., 2005a</xref>).</p><p>While these previous non-human primate findings may provide a neurophysiological basis for the fMRI and MEG responses observed here, the neural implementation of our third requirement of having neural codes enter compositional expressions remains elusive. What is more, there are still notable differences between humans and macaques: humans do not need to be extensively trained to understand the differences between geometric shapes (<xref ref-type="bibr" rid="bib70">Izard et al., 2022</xref>; <xref ref-type="bibr" rid="bib68">Izard and Spelke, 2009</xref>), and spontaneously impose geometric categories such as ‘parallel’ or ‘right angle’ to a continuum of angles between lines (<xref ref-type="bibr" rid="bib44">Dillon et al., 2019</xref>). Several behavioral findings suggest that a distinct neural code for geometry may exist in humans. Non-human primates perform poorly on a broad variety of perceptual and production tasks with geometric shapes: baboons do not exhibit a human-like geometric complexity effect (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>; <xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>) chimpanzees do not transfer learning of visual categories from concrete pictures to geometric line drawings (<xref ref-type="bibr" rid="bib32">Close and Call, 2015</xref>) and chimpanzees behave very differently from children in free-drawing experiments where they have to complete partial line-drawings of faces (<xref ref-type="bibr" rid="bib121">Saito et al., 2014</xref>). Recent work has argued that crows recognize quadrilaterals in a way that is similar to humans (<xref ref-type="bibr" rid="bib124">Schmidbauer et al., 2025</xref>), though see <xref ref-type="bibr" rid="bib120">Sablé-Meyer and Dehaene, 2025</xref> for a discussion about this result. These findings suggest that an understanding of the mechanisms that underlie the human coding of geometric shapes may ultimately shed light on the cognitive and neural singularity of the human brain (<xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>).</p><p>In the future, replicating the present experiments with monkey fMRI and electrophysiology is therefore an important goal. The methodology could also be extended to the perception of a broader set of geometric patterns (circles, spirals, crosses, zigzags, plaids, etc.) which recur since prehistory and in the drawings of children and adults of various cultures, thus testing whether they too originate in a minimal and universal ‘language of thought’ (<xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>) and whether such a language is unique to the human species (<xref ref-type="bibr" rid="bib39">Dehaene, 2026</xref>; <xref ref-type="bibr" rid="bib38">Dehaene et al., 2022</xref>). Finally, this research should ultimately be extended to the representation of three-dimensional geometric shapes, for which similar symbolic generative models have been proposed (<xref ref-type="bibr" rid="bib20">Biederman, 1987</xref>; <xref ref-type="bibr" rid="bib95">Leyton, 2003</xref>).</p></sec></sec><sec id="s8" sec-type="methods"><title>Methods</title><sec id="s8-1"><title>Experiment 1</title><sec id="s8-1-1"><title>Participants</title><p>330 participants (142 females, 177 males, 11 others; mean age 51.1 years, SD = 16.8) were recruited via a link provided on a New York Times article (available at <ext-link ext-link-type="uri" xlink:href="https://www.nytimes.com/2022/03/22/science/geometry-math-brain-primates.html">here</ext-link>) which reported previous research from the lab and featured a link to our new online experiment. When participants clicked the link, they landed on a page with our usual procedure for online experiments, including informed consent and demographic questions. No personal identity information was collected.</p></sec><sec id="s8-1-2"><title>Task</title><p>On each trial, participants were shown a 3 × 3 square grid of shapes, eight of which were copies of the same shape up to rotation and scaling, and one of which was a different shape. Participants were asked to detect the intruder shape by clicking on it. Auditory feedback was provided in the form of tones of ascending or descending pitch, as well as coloring of the shapes (the intruder was always colored in green indicating what the right answer was, and in case of an erroneous choice, the chosen shape was colored in red indicating a wrong answer).</p></sec><sec id="s8-1-3"><title>Stimuli</title><p>Shapes design followed previous work exactly (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>): 11 quadrilaterals with a varying number of geometric features, matched for average pairwise distance of all vertices and length of the bottom side. Shapes were presented in pure screen white on pure screen black. Each shape was differently scaled and rotated by sampling nine values without replacement in the following scaling factors [0.85, 0.88, 0.92, 0.95, 0.98, 1.02, 1.05, 1.08, 1.12, 1.15] and rotation angles [–25°, –19.4°, –13.8°, –8.3°, –2.7°, 2.7°, 8.3°, 13.8°, 19.4°, 25°]. Note that the rotation angles were centered on 0° but excluded this value so that the sides of the shapes were never strictly vertical or horizontal. Participants performed 110 trials (11 × 10), one for each pair of different reference and intruder shapes. The order of trials was randomized, subject to the constraint that no two identical reference shapes were used on consecutive trials, and that the outlier of a trial was always different from the reference shape of the previous trial. Two examples of trials are shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>.</p></sec><sec id="s8-1-4"><title>Procedure</title><p>The experimental procedure started with instructions, followed by a series of questions: device used (mouse or touchscreen), country of origin, gender, age, highest degree obtained. Participants then provided subjective self-evaluation assessments, with answers on a Likert scale from 1 to 10, for the following items: current skills in mathematics; current skills in first language. Finally, participants performed the task. The instructions text was the following: ‘The game is very simple: you will see sets of shapes on your screen. Apart from small rotation and scaling differences, they will be identical, except for one intruder. Your task is to respond as fast and accurately as you can about the location of the intruder by clicking on it. The difficulty will vary, but you always have to answer’.</p></sec><sec id="s8-1-5"><title>Estimation of empirical representational dissimilarity</title><p>To estimate the representational dissimilarity across shapes, first we estimated the dissimilarity between two shapes as the average success rate divided by the average response time. Indeed, if two shapes are very dissimilar, we expect participants to make few mistakes (high success rate) and find the intruder fast (low response time), yielding a high value, and vice versa. Because we did not have predictions using the asymmetry in visual search (e.g. finding a square within rectangles versus a rectangle versus squares), we then averaged over these paired conditions, thereby turning the square dissimilarity matrix into a triangular dissimilarity matrix. Finally, we <italic>z</italic>-scored these dissimilarity estimates.</p><p>As participants performed a single trial per pair of shapes, the estimation of the dissimilarity is noisy at the single participant level (either 0 or 1/RT depending on whether they answered correctly). We kept this estimate for analyses at the single participant level or mixed-effect analysis; however, for analyses that required a single RDM estimate across participants, we pooled the data from participants to estimate the average success rates and response times, hence before estimating the empirical RDMS, rather than estimating one RDM per participant and then averaging.</p></sec><sec id="s8-1-6"><title>Comparison with model RDMs</title><p>To obtain theoretical RDMs with which to compare the present behavioral data, as well as subsequent brain-imaging data, we proceeded as follows. For the CNN encoding model, we downloaded from GitHub the weights for several neural networks [CORnet (<xref ref-type="bibr" rid="bib88">Kubilius et al., 2018</xref>), ResNet (<xref ref-type="bibr" rid="bib63">He et al., 2016</xref>), and DenseNet (<xref ref-type="bibr" rid="bib66">Huang et al., 2018</xref>); all high scoring on brain-score (<xref ref-type="bibr" rid="bib125">Schrimpf et al., 2018</xref>)], all pre-trained with ImageNet and not specifically trained for our task. We extracted the activation vectors in each hidden layer associated with each shape with the 6 × 6 = 36 different orientations and scaling used in the experiment. For each shape, we separated their 36 exemplars randomly into two groups to have independent estimation of the representation vectors from the network and used the cross-validated Mahalanobis distance between these two splits to estimate the distance between each pair of shapes. This provides us with an RDM that captures how dissimilar shapes are according to their internal representations in a CNN of object recognition. Unless specified otherwise (see appendix), we report correlation with CORnet layer IT following <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>.</p><p>For the geometric feature model, we estimate a feature vector of geometric features for each shape. This feature vector includes information about (1) right angles (one for each angle, four features); (2) angle equality (one for each pair of angles, six features); (3) side length equality (one for each pair of sides, six features); and (4) side parallelism (one for each pair of sides, six features); all of this was done up to a tolerance level (e.g. an angle slightly off a right angle still counts as a right angle), using the tolerance value of 12.5% which was fitted previously to independent behavioral data (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). The dissimilarity between each pair of shapes was the difference between the number of features that each shape possesses: because both the square and the rectangle share many features, they are similar. Two very irregular shapes also end up similar as well. Conversely, the square and an irregular shape end up very dissimilar.</p></sec></sec><sec id="s8-2"><title>Experiment 2</title><sec id="s8-2-1"><title>Participants</title><p>Twenty healthy French adults (9 females; 19–37 years old, mean age = 24.6 years old, SD: 5.2 years old) and 25 French first graders (13 females; all 6 years old) participated in the study. Three children quit the experiment before any task began because they did not like the MRI noise or lying in the confined space for the scanner. One child completed the localizer task but not the other tasks. One child was missing a single run from the intruder task. All participants had normal hearing, normal or corrected-to-normal vision, and no known neurological deficit. All adults and guardians of children provided informed consent, and adult participants were compensated for their participation.</p></sec><sec id="s8-2-2"><title>Task</title><p>In three localizer runs, children and adult participants were exposed to eight different image categories, such that single geometric shapes could be compared to matched displays of faces, houses, and tools, and rows of three geometric shapes could be compared to matched displays of numbers, French words, and Chinese characters. To maintain attention, participants were asked to keep fixating on a green central fixation dot (radius = 8pixels, RGB color = 26, 167, 19, always shown on the screen), and to press a button with their right hand whenever a star symbol was presented. The star spanned roughly the same visual angle as the stimuli from the eight categories and appeared randomly once in one of the two blocks per category (8 target stars total), between the 3rd to the 6th stimuli within that block. As feedback, a 300 ms 650 Hz beep sound was provided after each button press.</p></sec><sec id="s8-2-3"><title>Stimuli</title><p>In each miniblock, participants saw a series of six grayscale images, one per second, belonging to one of eight different categories: faces, houses, tools, numbers, French words, Chinese characters, single geometric shapes, and rows of three geometric shapes. Each category comprised 20 exemplars. All faces, 16 houses, and 18 tools had been used in previous localizer experiments (<xref ref-type="bibr" rid="bib143">Zhan et al., 2018</xref>). For face stimuli, front-view neutral faces (20 identities, 10 males) were selected from the Karolinska Directed Emotional Faces database (<xref ref-type="bibr" rid="bib99">Lundqvist et al., 1998</xref>). The stimuli were aligned by the eyes and the iris distances. A circular mask was applied to exclude the hair and clothing below the neck. House and tool stimuli were royalty-free images obtained from the internet. House stimuli were photos of 2- to 3-story residence houses. Tool stimuli were photos of daily hand-held tools: half of the images were horizontally flipped, so that there were 10 images in a position graspable for the left and right hand, respectively. For French word stimuli, three-letter French words were selected which were known to first graders and had high occurrence frequencies (range = 7–2146 occurrences per million, mean = 302, SD = 505, based on Lexique, <ext-link ext-link-type="uri" xlink:href="http://www.lexique.org/">http://www.lexique.org/</ext-link>). Chinese characters were selected from the school textbook of Chinese first graders. Chinese word frequency (range = 11–1945 occurrences per million, mean = 326, SD = 451; <xref ref-type="bibr" rid="bib25">Cai and Brysbaert, 2010</xref>) was not significantly different from French words used here (<italic>t</italic>(38) = 0.2, p = 0.87). Single-digit formula stimuli were three-character simple operations in the form of ‘x + y’ or ‘x − y’ with x greater than y, x ranging from 2 to 5, and y from 1 to 4. Single shapes consisted of a single, centered outline of a geometrical shape (diamond, hexagon, rhombus, parallelogram, rectangle, square, trapezoid, isosceles triangle, equilateral triangle, and right triangle), and were matched in luminance, contrast, and visual angle to the faces/houses/tools/words/Chinese characters which also displayed single objects. A row of shapes consisted of three different shapes side by side, whose total width, size, and line width were matched with three-letter French words and three-character single-digit operations. To match the appearance of the monospaced font in previous work (<xref ref-type="bibr" rid="bib136">Vinckier et al., 2007</xref>), the monospaced font Consolas was used for the French words and numbers, with identical font weight 900. The font for Chinese characters was Heiti, which looks similar to Consolas. Random font size (uniform in 35–55 px font size) was repeatedly sampled until text stimuli achieved similar variability as with the other categories.</p><p>The stimuli were embedded in a gray circle (RGB color = 157, 157, 157, radius = 155 pixels), on the screen with a black background. Within the gray circle, the mean luminance and contrast of the 8 stimuli categories did not differ significantly (luminance: <italic>F</italic>(7,152) = 0.6, p = 0.749; contrast: <italic>F</italic>(7,152) = 1.2, p = 0.317), see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></sec><sec id="s8-2-4"><title>Procedure</title><p>The eight categories were presented in distinct blocks of 6 s each, fully randomized for block presentation order, with the restriction that there were no consecutive blocks from the same category. Each miniblock comprised six stimuli, in random order. Each stimulus was presented for 1 s, with no interval in between (<xref ref-type="bibr" rid="bib40">Dehaene-Lambertz et al., 2018</xref>). The inter-block interval duration was jittered (4, 6, or 8 s; mean = 6 s). Each of the eight-block types appeared twice within each run. A 6-s fixation period was included at both the beginning and the end of the run. Each run lasted for 3 min 24 s, and participants performed three such runs during the fMRI session.</p></sec></sec><sec id="s8-3"><title>Experiment 3</title><sec id="s8-3-1"><title>Participants</title><p>Identical to experiment 2.</p></sec><sec id="s8-3-2"><title>Procedure and stimuli</title><sec id="s8-3-2-1"><title>Task</title><p>We adapted the geometric intruder detection task (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>) to the fMRI scanner. On each trial, participants (children and adults) saw an array of six shapes around fixation (three on the right, and three on the left; see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). Five shapes were identical except for a small amount of random rotation and scaling, while one was a deviant shape. Because the pointing task used in <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref> was not possible in the limited space of the fMRI scanner, participants were merely asked to click a button with their left or right hand, thereby indicating on which side they thought the deviant was. Participants responded on every trial, the side of the correct response was counterbalanced within each shape, and we verified that the average motor response side was unconfounded with geometric shape or complexity. After each answer, auditory feedback was provided with a tone of high, increasing pitch when the answer was correct, and a low-pitch tone otherwise.</p></sec><sec id="s8-3-2-2"><title>Stimuli</title><p>Geometric shapes were generated following the procedure described in previous work (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>): to fit the experiment within the time constraints of children fMRI, a subset of shapes was used, comprising square, rectangle, isosceles trapezoid, rhombus, right hinge, and irregular shapes to span the range of complexity found in <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>. Following previous work (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>), deviants were generated by displacing the bottom right corner by a constant distance in four possible positions (see <xref ref-type="fig" rid="fig3">Figure 3A</xref>). That distance was a fraction of the average distance between all pairs of points, which was standardized across shapes (45% change). On each trial, six gray-on-black shapes were shown (shape color rgb values: 127, 127, 127). Shapes were displayed along two semicircles: the positions were determined by positioning the three leftmost (resp. rightmost) shapes on the left side (resp. right side) of a circle of radius 120 px, at angles 0, pi/2, and pi, and then shifting them 100 px to the left (resp. right). The rotation and scaling of each shape were randomized so that no two shapes had the same scaling or rotation factor, and values were sampled in [0.875, 0.925, 0.975, 1.025, 1.075, 1.125] for scaling and [–25°, –15°, –5°, 5°, 15°, 25°] for rotations, avoiding 0° to prevent alignment of specific shapes with screen borders. One of the shapes was an outlier, whose position was sampled uniformly in all six possible positions such that no two consecutive trials featured outliers in the same position. Outliers were sampled uniformly from the four possible types of outliers, so that all outlier types occurred as often, but no two consecutive trials featured identical outlier types.</p></sec><sec id="s8-3-2-3"><title>Procedure</title><p>The six shapes were presented in miniblocks, in randomized order, with no two consecutive blocks with the same type of shape. Each block comprised five consecutive trials with an identical base shape, each with 2 s of stimulus presentation and 2 s of fixation. There was a 4-, 6-, or 8-s delay between blocks. A central green fixation cross was always on display, and it turned bold 600 ms before a block would start. Each run of the outlier detection task lasted 3m40s.</p></sec></sec></sec><sec id="s8-4"><title>fMRI methods common to experiments 2 and 3</title><sec id="s8-4-1"><title>MRI acquisition parameters</title><p>MRI acquisition was performed on a 3T scanner (Siemens, Tim Trio), equipped with a 64-channel head coil. Exactly 113 functional scans covering the whole brain were acquired for each localizer run, as well as on 179 functional scans covering the whole brain for each run of the geometry task. All functional scans were using a T2*-weighted gradient echo-planar imaging sequence (69 interleaved slices, TR = 1.81 s, TE = 30.4 ms, voxel size = 2 × 2 × 2 mm, multiband factor = 3, flip angle = 71°, phase encoding direction: posterior to anterior). To reconstruct accurate anatomical details, a 3D T1-weighted structural image was also acquired (TR = 2.30 s, TE = 2.98 ms, voxel size = 1 × 1 × 1 mm, flip angle = 9°). To estimate distortions, two spin-echo field maps with opposite phase encoding directions were acquired: one volume in the anterior-to-posterior direction (AP) and one volume in the other direction (PA). Each fMRI session lasted for around 50 min for children including (in order) three runs of a task not discussed here, three Category localizer runs, T1 collection, and two Geometry runs. For adults, the session lasted for around 1 h 20 min because they took the same runs as for children as well as an additional harder version of the geometry task. This version, which involved smaller deviant distances and a stimulus presentation duration of only 200 ms, turned out to be too difficult. While the overall performance still shows a correlation with complexity (<italic>r</italic>² = 0.63, p &lt; 0.02), it was entirely driven by two shapes, the square and the rectangle: other shapes were equally hard, and although they were better than chance, they did not correlate with complexity (<italic>r</italic><sup>2</sup> = 0.35, p = 0.22) while the correlations remained for the simpler condition.</p></sec><sec id="s8-4-2"><title>Data analysis</title><p>Preprocessing was performed with the standard pipeline fMRIPrep. Results included in this manuscript come from preprocessing performed using fMRIPrep 20.0.5 (<xref ref-type="bibr" rid="bib48">Esteban et al., 2019</xref>), which is based on Nipype 1.4.2 (<xref ref-type="bibr" rid="bib58">Gorgolewski et al., 2011</xref>; <xref ref-type="bibr" rid="bib59">Gorgolewski et al., 2018</xref>), and generated the following detailed method description.</p></sec><sec id="s8-4-3"><title>Anatomical data</title><p>The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="bib133">Tustison et al., 2010</xref>), distributed with ANTs 2.2.0 (<xref ref-type="bibr" rid="bib12">Avants et al., 2008</xref>), and used as T1w-reference throughout the workflow. The T1w reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white matter (WM), and gray matter (GM) was performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="bib144">Zhang et al., 2001</xref>). Brain surfaces were reconstructed using recon-all (<xref ref-type="bibr" rid="bib36">Dale et al., 1999</xref>), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs- and FreeSurfer-derived segmentations of the cortical GM of Mindboggle (<xref ref-type="bibr" rid="bib83">Klein et al., 2017</xref>). Volume-based spatial normalization to two standard spaces (MNI152NLin6Asym, MNI152Nlin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.2.0), using brain-extracted versions of both T1w reference and the T1w template. The following templates were selected for spatial normalization: FSL’s MNI ICBM 152 non-linear 6th Generation Asymmetric Average Brain Stereotaxic Registration Model (<xref ref-type="bibr" rid="bib49">Evans et al., 2012</xref>) and ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="bib54">Fonov et al., 2009</xref>).</p></sec><sec id="s8-4-4"><title>Functional data</title><p>For each of the 10 BOLD EPI runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Susceptibility distortion correction was omitted. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (<xref ref-type="bibr" rid="bib60">Greve and Fischl, 2009</xref>). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (<xref ref-type="bibr" rid="bib75">Jenkinson et al., 2002</xref>). BOLD runs were slice-time corrected using 3dTshift from AFNI 20160207 (<xref ref-type="bibr" rid="bib35">Cox and Hyde, 1997</xref>). The BOLD time series (including slice-timing correction when applied) were resampled onto their original, native space by applying the transforms to correct for head motion. These resampled BOLD time series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time series were resampled into several standard spaces, correspondingly generating the following spatially normalized, preprocessed BOLD runs: MNI152Nlin6Asym and MNI152Nlin2009cAsym. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Several confounding time series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS, and three region-wise global signals. FD and DVARS are calculated for each functional run, both using their implementations in Nipype (following the definitions by <xref ref-type="bibr" rid="bib114">Power et al., 2014</xref>). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors was extracted to allow for component-based noise correction (<xref ref-type="bibr" rid="bib18">Behzadi et al., 2007</xref>). Principal components are estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128 s cut-off) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 5% variable voxels within a mask covering the subcortical regions. This subcortical mask was obtained by heavily eroding the brain mask, which ensures it does not include cortical GM regions. For aCompCor, components are calculated within the intersection of the aforementioned mask and the union of CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run (using the inverse BOLD-to-T1w transformation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components’ time series are sufficient to explain 50% of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (<xref ref-type="bibr" rid="bib123">Satterthwaite et al., 2013</xref>). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardized DVARS were annotated as motion outliers. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (<xref ref-type="bibr" rid="bib92">Lanczos, 1964</xref>). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer).</p><p>Many internal operations of fMRIPrep use Nilearn 0.6.2 (<xref ref-type="bibr" rid="bib1">Abraham et al., 2014</xref>), mostly within the functional processing workflow.</p></sec><sec id="s8-4-5"><title>fMRI GLM models</title><p>fMRI first-level models (GLM) were computed by convolving the experimental design matrix (specific to each task, see below) with SPM’s HRF model as implemented in Nilearn, with the following parameters: spatial smoothing using a full width at half maximum window (fwhm) of 4 mm; a second-order autoregressive noise model; and signal standardized to percent signal change relative to whole-brain mean. The following confound regressors were added: polynomial drift models from constant to fifth order (six regressors); estimated head translation and rotation on three axes (six regressors) as well as the following confound regressors given by fmriprep: average cerebro-spinal fluid signal (one regressor), average WM signal (one regressor), and the first five high-variance confounds estimates (<xref ref-type="bibr" rid="bib18">Behzadi et al., 2007</xref>) (five regressors).</p><p>In the visual category localizer, the design matrix contained distinct events for each visual stimulus, grouped by category, each with a duration of 1 s, including a specific one for the target star, leading to nine regressors (Chinese, face, house, tools, numbers, words, single shape, three shapes, and star). In the geometry task runs, each reference shape was associated with a regressor with trial duration set to 2 s, thus leading to 6 regressors (square, rectangle, rhombus, iso-trapezoid, hinge, and random). In both cases, button presses were not modeled as they were fully correlated with predictors of interest (either the star for the localizer, or every single trial for the geometry task).</p><p>Second-level models were estimated after additional smoothing with a full width at half maximum window of 8 mm. Statistical significance of clusters was estimated with a bootstrap procedure as follows: given an uncorrected p-value of 0.001, clusters were identified by contiguity of voxels that had p-values below this threshold. Then, the p-value of a cluster was derived by comparing its statistical mass (the sum of its t-values) to the distribution of the maximum statistical mass obtained by performing the same contrast after randomly swapping the sign of each participant’s statistical map. We performed this swapping 10,000 times for each contrast we estimated and computed the corrected p-value accordingly; for instance, a cluster whose mass was only outperformed by 3 random swaps out of the 10,000 was assigned a p-value of 0.0003.</p></sec><sec id="s8-4-6"><title>Searchlight RSA analyses</title><p>First, we estimated the RDM within spheres centered on each voxel. For this, we performed a searchlight sweep across the whole brain. We extracted the GLM coefficients of the geometric shapes from each voxel and all the neighboring voxels in a 3-voxel radius (=6 mm), for a total of 93 voxels per sphere. We discarded voxels where more than 50% of this sphere fell outside the participant’s brain. We extracted the betas of each shape and used a cross-validated Mahalanobis distance across runs (crossnobis, implemented in rsatoolbox) to estimate the dissimilarity between each pair of shapes. We attributed this distance to the center of the searchlight, thereby estimating an empirical RDM at each location.</p><p>Then we compared this empirical RDM with the two RDMs derived from our two models separately, using a whitened correlation metric. Both choices of metrics (cross-nobis and whitened correlation) follow the recommendations from a previous methodological publication (<xref ref-type="bibr" rid="bib43">Diedrichsen et al., 2021</xref>).</p><p>Finally, we computed group-level statistics by smoothing the resulting correlation map (fwhm = 8 mm) and performing statistical maps, cluster identification, and statistical inference at the cluster level as we did for the second-order level analysis, but with a one-tailed comparison only as we did not consider negative correlations.</p><p>The bar plot for ROIs in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> reflects a subject-specific voxel localization within ROIs. Within each ROI identified, we find, for each subject, the 10% most responsive subject-specific voxels in the same contrast used to identify the cluster. To avoid double-dipping, we selected these voxels using the contrast from one run, then collected the fMRI responses (beta coefficients) from the other runs; we perform this procedure across all runs and average the responses. Error bars indicate the standard error of the mean across participants.</p></sec></sec><sec id="s8-5"><title>Experiment 4</title><sec id="s8-5-1"><title>Participants</title><p>Twenty healthy French adults (13 females; 21–42 years old, mean: 24.9 years old, SD: 8.1 years old) participated in the MEG study. All participants had normal hearing, normal or corrected-to-normal vision, and no neurological deficit. All adults provided informed consent and were compensated for their participation. For all but one participant, we had access to anatomical recordings in 3T MRI, either from prior, unrelated experiments in the lab, or because the MEG session was immediately followed by a recording. Because of one participant missing an anatomical recording, analyses that required source reconstruction were performed on 19 subjects.</p></sec><sec id="s8-5-2"><title>Task</title><p>During MEG, adult participants were merely exposed to shapes while maintaining fixation and attention. As in previous work (e.g. <xref ref-type="bibr" rid="bib5">Al Roumi et al., 2023</xref>; <xref ref-type="bibr" rid="bib19">Benjamin et al., 2024</xref>), the goal was to examine the spontaneous encoding of stimuli and the presence or absence of a novelty response to occasional deviants.</p></sec><sec id="s8-5-3"><title>Stimuli</title><p>All 11 geometric shapes in <xref ref-type="fig" rid="fig1">Figure 1A</xref> were presented in miniblocks of 30 shapes. Geometric shapes were presented centered on the screen, one shape every second, with shapes remaining onscreen for 800 ms and a centered fixation cross present between shapes for 200 ms. To make the shapes more attractive (and since the same shape was also used in an infant experiment, not reported here), during their 800 ms presentation, the shapes slowly increased in size: in total, a scale factor of 1.2 was applied over the course of 800 ms, with linear interpolation of the shape size during the duration of the presentation. Shapes were presented in miniblocks following an oddball paradigm: within a miniblock, all shapes were identical up to scaling (randomly sampled in [0.875, 0.925, 0.975, 1.025, 1.075, 1.125]) and rotation (sampled in [–25°, –15°, –5°, 5°, 15°, 25°]), except for occasional oddballs which were deviant versions of the reference shape. Each miniblock comprised 30 shapes, 4 of which were oddballs that could replace any shape after the first six. Two oddballs never appeared in a row. There was no specific interval between miniblocks beyond the usual duration between shapes. A run was made of 11 miniblocks, one per shape in random order, and participants attended 8 runs except two participants who are missing a single run due to experimenters’ mistakes when setting up the MEG acquisition.</p></sec><sec id="s8-5-4"><title>Procedure</title><p>After inclusion by the lab’s recruiting team, participants were prepared for the MEG with electrocardiogram (ECG) and electrooculogram (EOG) sensors, as well as four Head Position Indicator coils, which were digitalized to track the head position throughout the experiment. Then we explained to participants that the task was a replication of an experiment with infants, and therefore was purely passive: they would be shown shapes and were instructed to pay attention to each shape, while trying to avoid blinks as well as body and eye movements. They sat in the MEG and we checked the head position, ECG/EOG, and MEG signal. From that point onward, we never opened the MEG door again to avoid resetting MEG signals and allow for optimal cross-run decoding and generalization. Participants took typically eight runs consecutively, with small breaks with no stimuli between runs to rest their eyes. At the end of the experiment, participants took the intruder test from previous work (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>) on a laptop computer outside of the MEG, and finally, we spent some time debriefing with participants the goal of the experiment.</p><p>To ensure high accuracy of the timing in the MEG, each trial’s first frame contained a white square on the bottom of the screen, which was hidden from participants but recorded with a photodiode. The same area was black during the rest of the experiment. The ramping up of the photodiode was therefore synchronized with the screen update and the appearance of the stimulus, ensuring robust timing for analyses. Then each ‘screen update’ event was linked to a recorded list of presented shapes.</p></sec><sec id="s8-5-5"><title>MEG acquisition parameters</title><p>Participants were instructed to look at a screen while sitting inside an electromagnetically shielded room. The magnetic component of their brain activity was recorded with a 306-channel, whole-head MEG by Elekta Neuromag (Helsinki, Finland). The MEG helmet is composed of 102 triplets of sensors, each comprising one magnetometer and two orthogonal planar gradiometers. The brain signals were acquired at a sampling rate of 1000 Hz with a hardware high-pass filter at 0.03 Hz.</p><p>Eye movements and heartbeats were monitored with vertical and horizontal EOGs and ECGs. Head shape was digitized using various points on the scalp as well as the nasion, left and right pre-auricular points (FASTTRACK, Polhemus). Subjects’ head position inside the helmet was measured at the beginning of each run with an isotrack Polhemus Inc system from the location of four coils placed over frontal and mastoid skull areas.</p></sec><sec id="s8-5-6"><title>Preprocessing of MEG signals</title><p>The preprocessing of the data was performed using MNE-BIDS-Pipeline, a streamlined implementation of the core ideas presented in the literature (<xref ref-type="bibr" rid="bib73">Jas et al., 2018</xref>) and leveraging BIDS specifications (<xref ref-type="bibr" rid="bib106">Niso et al., 2018</xref>; <xref ref-type="bibr" rid="bib111">Pernet et al., 2019</xref>). The pipeline performed automatic bad channel detection (both noisy and flat), then applied Maxwell filtering and Signal Space Separation on the raw data (<xref ref-type="bibr" rid="bib129">Taulu and Kajola, 2005</xref>). The data was then filtered between 0.1 and 40 Hz and resampled to 250 Hz. Extraction of epochs was performed for each shape, starting 150 ms before stimulus onset and stopping 1150 ms after, and the relevant metadata (event type, run, trial index, etc.) for each epoch was recovered from the stimulation procedure at this step. Artifacts in the data (e.g. blinks and heartbeats) were repaired with signal-space projection (<xref ref-type="bibr" rid="bib135">Uusitalo and Ilmoniemi, 1997</xref>), and thresholds derived with ‘autoreject global’ (<xref ref-type="bibr" rid="bib72">Jas et al., 2017</xref>). For source reconstruction, some preprocessing steps were performed by fmriprep (see below). Then, sources were positioned using the ‘oct5’ spacing with 1026 sources per hemisphere, and we used the e(xact)LORETA method (following recommendations from the literature <xref ref-type="bibr" rid="bib74">Jatoi et al., 2014</xref>; <xref ref-type="bibr" rid="bib110">Pascual-Marqui et al., 2018</xref>) using empty-room recordings performed right before or right after the experiment to estimate the noise covariance matrix. Additionally, for source reconstruction, anatomical MRI was preprocessed with fmriprep. T1-weighted (T1w) images were corrected for INU with N4BiasFieldCorrection (<xref ref-type="bibr" rid="bib133">Tustison et al., 2010</xref>), distributed with ANTs 2.3.3 (<xref ref-type="bibr" rid="bib12">Avants et al., 2008</xref>). The T1w reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow. Brain tissue segmentation of CSF, WM, and GM was performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="bib144">Zhang et al., 2001</xref>). Brain surfaces were reconstructed using recon-all (<xref ref-type="bibr" rid="bib36">Dale et al., 1999</xref>) and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs- and FreeSurfer-derived segmentations of the cortical GM of Mindboggle (<xref ref-type="bibr" rid="bib83">Klein et al., 2017</xref>).</p></sec><sec id="s8-5-7"><title>Decoding</title><p>After epoching the data, for each timepoint within an epoch and each participant, we trained a logistic regression decoder to classify epochs as reference or oddball using samples from all shapes. For this analysis, we discarded the six first trials at the beginning of each block, since (1) those could not be oddballs ever and (2) there was no warning of transitions between blocks and so the first trials were also ‘oddballs’ with respect to the previous block’s shape. Each epoch was normalized before training the classifier. To avoid decoders being biased by the overall signal’s autocorrelation across timescales, we used sixfolds of cross-validation over runs with the following folds: even runs versus odd runs; first half versus second half; and runs 1, 2, 5, 6 versus 3, 4, 7, and 8. Folds were used in both directions, for example even for training and odd for testing; as well as odd for training and even for testing. The decoders were trained on data from all of the shapes conjointly. When testing its accuracy, we tested it separately on data from each shape (e.g. detecting oddballs within squares only, within rectangles only, etc.) – using runs independent from the training data. In order to estimate accuracy without being biased by the imbalanced number of epochs in the different classes (there are 4 oddballs for every 20 references), we report the ROC area under the curve of the receiver operating characteristic for each shape in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, left subfigure. Then at each timepoint, we correlated the decoding performance with the number of geometric features present in each shape: the r correlation coefficient at each timepoint is plotted in the central subfigure, together with shading for a significant cluster identified with permutation tests across participants (implemented by mne, one-tailed, 2<sup>13</sup> permutations, cluster forming threshold &lt;0.05). Finally, we average the decoding performance in the identified cluster and plot each shape’s average decoding performance against online behavioral data: this is effectively visualizing the same data as the central column’s figure, and therefore no statistical test is reported. The analyses were performed both without any smoothing and with a uniform averaging over a sliding window of 100 ms; the results are identical, but we chose the latter since plots using the smoothed version make the separation of the different shapes easier to see. The same holds true for the next analysis.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4C</xref>, we display a similar sequence of plots, but now instead of training a single classifier to identify epochs as reference or oddball conjointly on all shapes, we train 11 separate such classifiers, one for each shape. This produces very similar results from the previous analysis.</p></sec><sec id="s8-5-8"><title>RSA analysis</title><p>For RSA analyses, data from the oddballs was discarded and we used data from the magnetometers only. The goal of this analysis was to pinpoint when and where the mental representation of shapes followed distances that matched either a neural network model or a geometric feature model. We used the same model RDMs as the one we used to analyze behavior and fMRI data and provide below the details of how we derived the empirical RDMs for our analyses.</p></sec><sec id="s8-5-9"><title>In sensor space</title><p>We estimated, at each timepoint, the dissimilarity between each pair of shape across sensors: we relied on rsatoolbox to compute the Mahalanobis distance, cross-validated with a leave-one-out scheme over runs. This provided us with one empirical RDM for each timepoint, with no spatial information as this was performed across all sensors. Then we compared this RDM with our two models: since our model RDMs are effectively orthogonal, we performed the comparisons with the empirical RDM separately. We used a whitened correlation metric to compare RDMs. This gave us one timeseries for each participant and each model, and we then performed permutation testing in the [0, 800]ms window to identify significant temporal clusters for each model separately. As shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, this yields one significant cluster associated with the CNN encoding model and two significant clusters associated with the geometric feature model.</p></sec><sec id="s8-5-10"><title>In source space</title><p>In order to understand not only the temporal dynamic of the mental representations, but also get an estimate of the localization of the various representations, we turned to RSA analysis in source space. We performed source reconstruction of each reference shape trial. Then we averaged the data over the two identified temporal clusters from the sensor-space RSA analysis (we merged the two clusters associated with the exact geometric feature model): [60, 320] and [128, 400] ms. We then performed RSA analysis at each source’s location using its neighboring sources (geodesic distance of 2 cm on the reconstructed cortical surface). Finally, we compared the resulting RDMs to either the CNN encoding model or the geometric feature model. These steps were performed independently for each subject. Next, we projected the whitened correlation distance between empirical RDMs and model RDM onto a common cortical space, fsaverage. Finally, we performed a permutation spatial cluster test across participants, using adjacency matrices between sources (sources are adjacent if they were immediate neighbors on the reconstructed surface’s mesh). This resulted in two significant clusters associated with the CNN encoding model during the [60, 320] ms time window, located in bilateral occipital areas. Additionally, this resulted in two significant clusters associated with the geometric feature model during the [128, 400] ms, located in very broad bilateral networks encompassing dorsal and frontal areas.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s9"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation</p></fn><fn fn-type="con" id="con5"><p>Formal analysis, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Formal analysis, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Supervision, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Visualization, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All studies were conducted in accordance with the Declaration of Helsinki and French bioethics laws. On-line collection of behavioral data was approved by the Paris-Saclay University Committee for Ethical Research (reference: CER CER-Paris-Saclay-2019-063). Behavioral and brain-imaging studies in the lab in adults and 6-year-old children (MEG and fMRI) were approved by the CEA ethics boards and by a nationally approved ethics committee (reference MEG: CPP 100049; fMRI in children: CPP 100027 and CPP 100053; fMRI in adults: CPP 100050).</p></fn></fn-group></sec><sec sec-type="data-availability" id="s10"><title>Data availability</title><p>Scripts for all of the analyses are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mathias-sm/AGeometricShapeRegularityEffectHumanBrain">https://github.com/mathias-sm/AGeometricShapeRegularityEffectHumanBrain</ext-link> (copy archived at <xref ref-type="bibr" rid="bib119">Sablé-Meyer, 2025</xref>); behavioral data and scripts to generate the models are also available at this url. Raw fMRI data is provided at <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds006010/versions/1.0.1">https://openneuro.org/datasets/ds006010/versions/1.0.1</ext-link> and raw MEG data at <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds006012/versions/1.0.1">https://openneuro.org/datasets/ds006012/versions/1.0.1</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Watkins</surname><given-names>CP</given-names></name><name><surname>He</surname><given-names>C</given-names></name><name><surname>Pajot</surname><given-names>M</given-names></name><name><surname>Morfoisse</surname><given-names>T</given-names></name><name><surname>Roumi</surname><given-names>FA</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>A geometric shape regularity effect in the human brain: fMRI dataset</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds006010.v1.0.1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Watkins</surname><given-names>CP</given-names></name><name><surname>He</surname><given-names>C</given-names></name><name><surname>Pajot</surname><given-names>M</given-names></name><name><surname>Morfoisse</surname><given-names>T</given-names></name><name><surname>Roumi</surname><given-names>FA</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>A geometric shape regularity effect in the human brain: MEG dataset</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds006012.v1.0.1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to Ghislaine Dehaene-Lambertz, Leila Azizi, and the NeuroSpin support teams for help in data acquisition, and Lorenzo Ciccione, Christophe Pallier, Minye Zhan, Alexandre Gramfort, and the MNE team for support in data processing.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>KVS</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reading increases the compositionality of visual word representations</article-title><source>Psychological Science</source><volume>30</volume><fpage>1707</fpage><lpage>1723</lpage><pub-id pub-id-type="doi">10.1177/0956797619881134</pub-id><pub-id pub-id-type="pmid">31697615</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title><source>eLife</source><volume>9</volume><elocation-id>e54846</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54846</pub-id><pub-id pub-id-type="pmid">32369017</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><volume>2021</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1101/2021.02.22.432340</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Compression of Binary Sound Sequences in Human Memory</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.10.15.512361</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Origins of the brain networks for advanced mathematics in expert mathematicians</article-title><source>PNAS</source><volume>113</volume><fpage>4909</fpage><lpage>4917</lpage><pub-id pub-id-type="doi">10.1073/pnas.1603205113</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Denghien</surname><given-names>I</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the role of visual experience in mathematical development: Evidence from blind mathematicians</article-title><source>Developmental Cognitive Neuroscience</source><volume>30</volume><fpage>314</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/j.dcn.2017.09.007</pub-id><pub-id pub-id-type="pmid">29033221</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A distinct cortical network for mathematical knowledge in the human brain</article-title><source>NeuroImage</source><volume>189</volume><fpage>19</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.01.001</pub-id><pub-id pub-id-type="pmid">30611876</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amir</surname><given-names>O</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Hayworth</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Sensitivity to nonaccidental properties across various shape dimensions</article-title><source>Vision Research</source><volume>62</volume><fpage>35</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.03.020</pub-id><pub-id pub-id-type="pmid">22491056</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ansari</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effects of development and enculturation on number representation in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>278</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1038/nrn2334</pub-id><pub-id pub-id-type="pmid">18334999</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Ponce</surname><given-names>CR</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing faces is necessary for face-domain formation</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1404</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1038/nn.4635</pub-id><pub-id pub-id-type="pmid">28869581</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Skeletal descriptions of shape provide unique perceptual information for object recognition</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>9359</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-45268-y</pub-id><pub-id pub-id-type="pmid">31249321</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Does the brain’s ventral visual pathway compute object shape?</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>1119</fpage><lpage>1132</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.09.019</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Kamps</surname><given-names>FS</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Skeletal representations of shape in the human visual cortex</article-title><source>Neuropsychologia</source><volume>164</volume><elocation-id>108092</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2021.108092</pub-id><pub-id pub-id-type="pmid">34801519</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Simmons</surname><given-names>C</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Temporal asymmetries and interactions between dorsal and ventral visual pathways during object recognition</article-title><source>Cerebral Cortex Communications</source><volume>4</volume><elocation-id>tgad003</elocation-id><pub-id pub-id-type="doi">10.1093/texcom/tgad003</pub-id><pub-id pub-id-type="pmid">36726794</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Al Roumi</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Long-horizon associative learning explains human sensitivity to statistical and network structures in auditory sequences</article-title><source>The Journal of Neuroscience</source><volume>44</volume><elocation-id>e1369232024</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1369-23.2024</pub-id><pub-id pub-id-type="pmid">38408873</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological Review</source><volume>94</volume><fpage>115</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id><pub-id pub-id-type="pmid">3575582</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Ju</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Surface versus edge-based determinants of visual recognition</article-title><source>Cognitive Psychology</source><volume>20</volume><fpage>38</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(88)90024-2</pub-id><pub-id pub-id-type="pmid">3338267</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Davidoff</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of shape in individuals from a culture with minimal exposure to regular, simple artifacts: sensitivity to nonaccidental versus metric properties</article-title><source>Psychological Science</source><volume>20</volume><fpage>1437</fpage><lpage>1442</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02465.x</pub-id><pub-id pub-id-type="pmid">19883490</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Biological shape and visual science. I</article-title><source>Journal of Theoretical Biology</source><volume>38</volume><fpage>205</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1016/0022-5193(73)90175-6</pub-id><pub-id pub-id-type="pmid">4689997</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowers</surname><given-names>JS</given-names></name><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>Dujmović</surname><given-names>M</given-names></name><name><surname>Llera Montero</surname><given-names>M</given-names></name><name><surname>Tsvetkov</surname><given-names>C</given-names></name><name><surname>Biscione</surname><given-names>V</given-names></name><name><surname>Puebla</surname><given-names>G</given-names></name><name><surname>Adolfi</surname><given-names>F</given-names></name><name><surname>Hummel</surname><given-names>JE</given-names></name><name><surname>Heaton</surname><given-names>RF</given-names></name><name><surname>Evans</surname><given-names>BD</given-names></name><name><surname>Mitchell</surname><given-names>J</given-names></name><name><surname>Blything</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep problems with neural network models of human vision</article-title><source>The Behavioral and Brain Sciences</source><volume>46</volume><elocation-id>e385</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X22002813</pub-id><pub-id pub-id-type="pmid">36453586</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>Q</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>SUBTLEX-CH: Chinese word and character frequencies based on film subtitles</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e10729</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0010729</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>DI</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Giallanza</surname><given-names>T</given-names></name><name><surname>Cohen</surname><given-names>J</given-names></name><name><surname>Griffiths</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Human-like Geometric Abstraction in Large Pre-trained Neural Networks</article-title><conf-name>Proceedings of the Annual Meeting of the Cognitive Science Society</conf-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cantlon</surname><given-names>JF</given-names></name><name><surname>Li</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural activity during natural viewing of Sesame Street statistically predicts test scores in early childhood</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001462</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001462</pub-id><pub-id pub-id-type="pmid">23300385</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Language of Vision</article-title><source>Perception</source><volume>50</volume><fpage>195</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1177/0301006621991491</pub-id><pub-id pub-id-type="pmid">33583254</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Vitányi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Simplicity: a unifying principle in cognitive science?</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>19</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)00005-0</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cherti</surname><given-names>M</given-names></name><name><surname>Beaumont</surname><given-names>R</given-names></name><name><surname>Wightman</surname><given-names>R</given-names></name><name><surname>Wortsman</surname><given-names>M</given-names></name><name><surname>Ilharco</surname><given-names>G</given-names></name><name><surname>Gordon</surname><given-names>C</given-names></name><name><surname>Schuhmann</surname><given-names>C</given-names></name><name><surname>Schmidt</surname><given-names>L</given-names></name><name><surname>Jitsev</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Reproducible Scaling Laws for Contrastive Language-Image Learning</article-title><conf-name>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00276</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A M/EEG-fMRI fusion primer: resolving human brain responses in space and time</article-title><source>Neuron</source><volume>107</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id><pub-id pub-id-type="pmid">32721379</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Close</surname><given-names>J</given-names></name><name><surname>Call</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>From colour photographs to black-and-white line drawings: an assessment of chimpanzees’ (<italic>Pan troglodytes</italic>’) transfer behaviour</article-title><source>Animal Cognition</source><volume>18</volume><fpage>437</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1007/s10071-014-0813-5</pub-id><pub-id pub-id-type="pmid">25326248</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>E</given-names></name><name><surname>Freud</surname><given-names>E</given-names></name><name><surname>Kainerstorfer</surname><given-names>JM</given-names></name><name><surname>Cao</surname><given-names>J</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Temporal dynamics of shape processing differentiate contributions of dorsal and ventral visual pathways</article-title><source>Journal of Cognitive Neuroscience</source><volume>31</volume><fpage>821</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01391</pub-id><pub-id pub-id-type="pmid">30883289</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Conwell</surname><given-names>C</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><source>What can 5.17 billion regression fits tell us about artificial models of the human visual system</source><publisher-name>Workshop@ NeurIPS</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR in Biomedicine</source><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(sici)1099-1492(199706/08)10:4/5&lt;171::aid-nbm453&gt;3.0.co;2-l</pub-id><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Izard</surname><given-names>V</given-names></name><name><surname>Pica</surname><given-names>P</given-names></name><name><surname>Spelke</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Core knowledge of geometry in an Amazonian indigene group</article-title><source>Science</source><volume>311</volume><fpage>381</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1126/science.1121739</pub-id><pub-id pub-id-type="pmid">16424341</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Symbols and mental programs: a hypothesis about human singularity</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>751</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.06.010</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2026">2026</year><source>The Lascaux Rectangle: How Homo Sapiens Invented Geometry</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Monzalvo</surname><given-names>K</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The emergence of the visual word form: Longitudinal evolution of category-specific ventral visual areas during reading acquisition</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004103</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004103</pub-id><pub-id pub-id-type="pmid">29509766</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Leeuw</surname><given-names>J</given-names></name><name><surname>Mair</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Multidimensional scaling using majorization: SMACOF in <italic>R</italic></article-title><source>Journal of Statistical Software</source><volume>31</volume><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.18637/jss.v031.i03</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denisova</surname><given-names>K</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>X</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Investigating shape representation using sensitivity to part- and axis-based transformations</article-title><source>Vision Research</source><volume>126</volume><fpage>347</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2015.07.004</pub-id><pub-id pub-id-type="pmid">26325393</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Berlot</surname><given-names>E</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Schütt</surname><given-names>HH</given-names></name><name><surname>Shahbazi</surname><given-names>M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Comparing representational geometries using whitened unbiased-distance-matrix similarity</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.02789">http://arxiv.org/abs/2007.02789</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dillon</surname><given-names>MR</given-names></name><name><surname>Duyck</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Izard</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Geometric categories in cognition</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>45</volume><fpage>1236</fpage><lpage>1247</lpage><pub-id pub-id-type="doi">10.1037/xhp0000663</pub-id><pub-id pub-id-type="pmid">31219284</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Weissenborn</surname><given-names>D</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Dehghani</surname><given-names>M</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Heigold</surname><given-names>G</given-names></name><name><surname>Gelly</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An image is worth 16x16 words: transformers for image recognition at scale</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</ext-link></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emberson</surname><given-names>LL</given-names></name><name><surname>Crosswhite</surname><given-names>SL</given-names></name><name><surname>Richards</surname><given-names>JE</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The lateral occipital cortex is selective for object shape, not texture/color, at six months</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3698</fpage><lpage>3703</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3300-16.2017</pub-id><pub-id pub-id-type="pmid">28264984</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Harris</surname><given-names>A</given-names></name><name><surname>Stanley</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The parahippocampal place area: recognition, navigation, or encoding?</article-title><source>Neuron</source><volume>23</volume><fpage>115</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80758-8</pub-id><pub-id pub-id-type="pmid">10402198</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>Janke</surname><given-names>AL</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain templates and atlases</article-title><source>NeuroImage</source><volume>62</volume><fpage>911</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.024</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The simplicity principle in human concept learning</article-title><source>Current Directions in Psychological Science</source><volume>12</volume><fpage>227</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1046/j.0963-7214.2003.01267.x</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian estimation of the shape skeleton</article-title><source>PNAS</source><volume>103</volume><fpage>18014</fpage><lpage>18019</lpage><pub-id pub-id-type="doi">10.1073/pnas.0608811103</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Firestone</surname><given-names>C</given-names></name><name><surname>Scholl</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>“Please tap the shape, anywhere you like”: Shape skeletons in human vision revealed by an exceedingly simple measure</article-title><source>Psychological Science</source><volume>25</volume><fpage>377</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1177/0956797613507584</pub-id><pub-id pub-id-type="pmid">24406395</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fodor</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1975">1975</year><source>The Language of Thought</source><publisher-name>Harvard University Press</publisher-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>V</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name><name><surname>McKinstry</surname><given-names>R</given-names></name><name><surname>Almli</surname><given-names>C</given-names></name><name><surname>Collins</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freud</surname><given-names>E</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>‘What’ is happening in the dorsal visual pathway</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>773</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.08.003</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froyen</surname><given-names>V</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bayesian hierarchical grouping: Perceptual grouping as mixture estimation</article-title><source>Psychological Review</source><volume>122</volume><fpage>575</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1037/a0039540</pub-id><pub-id pub-id-type="pmid">26322548</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodenough</surname><given-names>FL</given-names></name></person-group><year iso-8601-date="1928">1928</year><article-title>Studies in the psychology of children’s drawings</article-title><source>Psychological Bulletin</source><volume>25</volume><fpage>272</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1037/h0071049</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Ziegler</surname><given-names>E</given-names></name><name><surname>Ellis</surname><given-names>DG</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Jarecka</surname><given-names>D</given-names></name><name><surname>Johnson</surname><given-names>H</given-names></name><name><surname>Burns</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Nipype</data-title><version designator="1.9.1">1.9.1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.596855">https://doi.org/10.5281/zenodo.596855</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name><name><surname>Itzchak</surname><given-names>Y</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A sequence of object-processing stages revealed by fMRI in the human occipital lobe</article-title><source>Human Brain Mapping</source><volume>6</volume><fpage>316</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1998)6:4&lt;316::AID-HBM9&gt;3.0.CO;2-6</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinke</surname><given-names>D</given-names></name><name><surname>Wachman</surname><given-names>P</given-names></name><name><surname>van Zoest</surname><given-names>W</given-names></name><name><surname>Leek</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A failure to learn object shape geometry: Implications for convolutional neural networks as plausible models of biological vision</article-title><source>Vision Research</source><volume>189</volume><fpage>81</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2021.09.004</pub-id><pub-id pub-id-type="pmid">34634753</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henshilwood</surname><given-names>CS</given-names></name><name><surname>d’Errico</surname><given-names>F</given-names></name><name><surname>van Niekerk</surname><given-names>KL</given-names></name><name><surname>Dayet</surname><given-names>L</given-names></name><name><surname>Queffelec</surname><given-names>A</given-names></name><name><surname>Pollarolo</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An abstract drawing from the 73,000-year-old levels at Blombos Cave, South Africa</article-title><source>Nature</source><volume>562</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0514-3</pub-id><pub-id pub-id-type="pmid">30209394</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Maaten</surname><given-names>L</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Densely connected convolutional networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.06993">http://arxiv.org/abs/1608.06993</ext-link></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izard</surname><given-names>V</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Distinct cerebral pathways for object identity and number in human infants</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e11</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060011</pub-id><pub-id pub-id-type="pmid">18254657</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izard</surname><given-names>V</given-names></name><name><surname>Spelke</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Development of sensitivity to geometry in visual forms</article-title><source>Human Evolution</source><volume>23</volume><fpage>213</fpage><lpage>248</lpage><pub-id pub-id-type="pmid">21359132</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izard</surname><given-names>V</given-names></name><name><surname>Pica</surname><given-names>P</given-names></name><name><surname>Spelke</surname><given-names>ES</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Flexible intuitions of Euclidean geometry in an Amazonian indigene group</article-title><source>PNAS</source><volume>108</volume><fpage>9782</fpage><lpage>9787</lpage><pub-id pub-id-type="doi">10.1073/pnas.1016686108</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izard</surname><given-names>V</given-names></name><name><surname>Pica</surname><given-names>P</given-names></name><name><surname>Spelke</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visual foundations of Euclidean geometry</article-title><source>Cognitive Psychology</source><volume>136</volume><elocation-id>101494</elocation-id><pub-id pub-id-type="doi">10.1016/j.cogpsych.2022.101494</pub-id><pub-id pub-id-type="pmid">35751917</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>G</given-names></name><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Katti</surname><given-names>H</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Qualitative similarities and differences in visual object representations between brains and deep networks</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1872</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22078-3</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Bekhti</surname><given-names>Y</given-names></name><name><surname>Raimondo</surname><given-names>F</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Autoreject: Automated artifact rejection for MEG and EEG data</article-title><source>NeuroImage</source><volume>159</volume><fpage>417</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.030</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Leppäkangas</surname><given-names>J</given-names></name><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Reproducible MEG/EEG Group Study With the MNE software: recommendations, quality assessments, and good practices</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>530</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00530</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jatoi</surname><given-names>MA</given-names></name><name><surname>Kamel</surname><given-names>N</given-names></name><name><surname>Malik</surname><given-names>AS</given-names></name><name><surname>Faye</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>EEG based brain source localization comparison of sLORETA and eLORETA</article-title><source>Australasian Physical &amp; Engineering Sciences in Medicine</source><volume>37</volume><fpage>713</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1007/s13246-014-0308-3</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanjlia</surname><given-names>S</given-names></name><name><surname>Lane</surname><given-names>C</given-names></name><name><surname>Feigenson</surname><given-names>L</given-names></name><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Absence of visual experience modifies the neural basis of numerical thinking</article-title><source>PNAS</source><volume>113</volume><fpage>11172</fpage><lpage>11177</lpage><pub-id pub-id-type="doi">10.1073/pnas.1524982113</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Ledden</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Functional imaging of human visual recognition</article-title><source>Brain Research. Cognitive Brain Research</source><volume>5</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(96)00041-9</pub-id><pub-id pub-id-type="pmid">9049071</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayaert</surname><given-names>G</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Shape tuning in macaque inferior temporal cortex</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>3016</fpage><lpage>3027</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-07-03016.2003</pub-id><pub-id pub-id-type="pmid">12684489</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayaert</surname><given-names>G</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005a</year><article-title>Tuning for shape dimensions in macaque inferior temporal cortex</article-title><source>European Journal of Neuroscience</source><volume>22</volume><fpage>212</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04202.x</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayaert</surname><given-names>G</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005b</year><article-title>Representation of regular and irregular shapes in macaque inferotemporal cortex</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1308</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi014</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayaert</surname><given-names>G</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Infants and toddlers show enlarged visual sensitivity to nonaccidental compared with metric shape changes</article-title><source>I-Perception</source><volume>1</volume><fpage>149</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1068/i0397</pub-id><pub-id pub-id-type="pmid">23145220</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khosla</surname><given-names>M</given-names></name><name><surname>Murty</surname><given-names>NAR</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Data-driven component modeling reveals the functional organization of high-level visual cortex</article-title><source>Journal of Vision</source><volume>22</volume><elocation-id>4184</elocation-id><pub-id pub-id-type="doi">10.1167/jov.22.14.4184</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Bao</surname><given-names>FS</given-names></name><name><surname>Giard</surname><given-names>J</given-names></name><name><surname>Häme</surname><given-names>Y</given-names></name><name><surname>Stavsky</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>N</given-names></name><name><surname>Rossa</surname><given-names>B</given-names></name><name><surname>Reuter</surname><given-names>M</given-names></name><name><surname>Chaibub Neto</surname><given-names>E</given-names></name><name><surname>Keshavan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mindboggling morphometry of human brains</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005350</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id><pub-id pub-id-type="pmid">28231282</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cortical regions involved in perceiving object shape</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>3310</fpage><lpage>3318</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-09-03310.2000</pub-id><pub-id pub-id-type="pmid">10777794</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Representation of the perceived 3-D object shape in the human lateral occipital complex</article-title><source>Cerebral Cortex (New York, N.Y</source><volume>13</volume><fpage>911</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1093/cercor/13.9.911</pub-id><pub-id pub-id-type="pmid">12902390</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cornet: modeling the neural mechanisms of core object recognition</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/408385</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>N</given-names></name><name><surname>Issa</surname><given-names>E</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain-like object recognition with high-performing shallow recurrent anns</article-title><conf-name>33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</conf-name><conf-loc>Vancouver, Canada</conf-loc></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level concept learning through probabilistic program induction</article-title><source>Science</source><volume>350</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1126/science.aab3050</pub-id><pub-id pub-id-type="pmid">26659050</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Ullman</surname><given-names>TD</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Building machines that learn and think like people</article-title><source>The Behavioral and Brain Sciences</source><volume>40</volume><fpage>1</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1017/S0140525X16001837</pub-id><pub-id pub-id-type="pmid">27881212</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanczos</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Evaluation of Noisy Data</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><volume>1</volume><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1137/0701007</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leeuwenberg</surname><given-names>ELJ</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>A perceptual coding language for visual and auditory patterns</article-title><source>The American Journal of Psychology</source><volume>84</volume><elocation-id>307</elocation-id><pub-id pub-id-type="doi">10.2307/1420464</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical representation of medial axis structure</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>629</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs046</pub-id><pub-id pub-id-type="pmid">22387761</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Leyton</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>A Generative Theory of Shape</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib96"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Mao</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>CY</given-names></name><name><surname>Feichtenhofer</surname><given-names>C</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A ConvNet for the 2020s</article-title><conf-name>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01167</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Fan</surname><given-names>JE</given-names></name><name><surname>Huey</surname><given-names>H</given-names></name><name><surname>Chai</surname><given-names>Z</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Parallel developmental changes in children’s production and recognition of line drawings of visual concepts</article-title><source>Nature Communications</source><volume>15</volume><elocation-id>1191</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-44529-9</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowet</surname><given-names>AS</given-names></name><name><surname>Firestone</surname><given-names>C</given-names></name><name><surname>Scholl</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Seeing structure: Shape skeletons modulate perceived similarity</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>80</volume><fpage>1278</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.3758/s13414-017-1457-8</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lundqvist</surname><given-names>D</given-names></name><name><surname>Flykt</surname><given-names>A</given-names></name><name><surname>Öhman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Karolinska Directed Emotional Faces</source><publisher-name>Cognition and Emotion</publisher-name></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Benson</surname><given-names>RR</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Kennedy</surname><given-names>WA</given-names></name><name><surname>Ledden</surname><given-names>PJ</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>PNAS</source><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.18.8135</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margalit</surname><given-names>E</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Vizioli</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>RY</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ultra-high-resolution fMRI of Human Ventral Temporal Cortex Reveals Differential Representation of Categories and Domains</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>3008</fpage><lpage>3024</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2106-19.2020</pub-id><pub-id pub-id-type="pmid">32094202</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mayilvahanan</surname><given-names>P</given-names></name><name><surname>Zimmermann</surname><given-names>RS</given-names></name><name><surname>Wiedemer</surname><given-names>T</given-names></name><name><surname>Rusak</surname><given-names>E</given-names></name><name><surname>Juhos</surname><given-names>A</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Search of Forgotten Domain Generalization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2410.08258">https://arxiv.org/abs/2410.08258</ext-link></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>EE</given-names></name><name><surname>Martynek</surname><given-names>M</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Expansion of a conserved architecture drives the evolution of the primate visual cortex</article-title><source>PNAS</source><volume>122</volume><elocation-id>e2421585122</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2421585122</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morfoisse</surname><given-names>T</given-names></name><name><surname>Izard</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A unifying parametric language and a toolbox for future investigations of the role of the medial axis parameters in human vision</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>2621</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.9.2621</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishimura</surname><given-names>M</given-names></name><name><surname>Scherf</surname><given-names>KS</given-names></name><name><surname>Zachariou</surname><given-names>V</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Size precedes view: developmental emergence of invariant object representations in lateral occipital complex</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>474</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00720</pub-id><pub-id pub-id-type="pmid">25244115</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niso</surname><given-names>G</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Bock</surname><given-names>E</given-names></name><name><surname>Brooks</surname><given-names>TL</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>T. Moreau</surname><given-names>J</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Wexler</surname><given-names>J</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>MEG-BIDS, the brain imaging data structure extended to magnetoencephalography</article-title><source>Scientific Data</source><volume>5</volume><elocation-id>180110</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2018.110</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oquab</surname><given-names>M</given-names></name><name><surname>Darcet</surname><given-names>T</given-names></name><name><surname>Moutakanni</surname><given-names>T</given-names></name><name><surname>Vo</surname><given-names>H</given-names></name><name><surname>Szafraniec</surname><given-names>M</given-names></name><name><surname>Khalidov</surname><given-names>V</given-names></name><name><surname>Fernandez</surname><given-names>P</given-names></name><name><surname>Haziza</surname><given-names>D</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>El-Nouby</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dinov2: Learning Robust Visual Features without Supervision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2304.07193">https://arxiv.org/abs/2304.07193</ext-link></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papale</surname><given-names>P</given-names></name><name><surname>Betta</surname><given-names>M</given-names></name><name><surname>Handjaras</surname><given-names>G</given-names></name><name><surname>Malfatti</surname><given-names>G</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Rampinini</surname><given-names>A</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Turella</surname><given-names>L</given-names></name><name><surname>Leo</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Common spatiotemporal processing of visual features shapes object representation</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>7601</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-43956-3</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papale</surname><given-names>P</given-names></name><name><surname>Leo</surname><given-names>A</given-names></name><name><surname>Handjaras</surname><given-names>G</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Shape coding in occipito-temporal cortex relies on object silhouette, curvature, and medial axis</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>1560</fpage><lpage>1570</lpage><pub-id pub-id-type="doi">10.1152/jn.00212.2020</pub-id><pub-id pub-id-type="pmid">33052726</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pascual-Marqui</surname><given-names>RD</given-names></name><name><surname>Faber</surname><given-names>P</given-names></name><name><surname>Kinoshita</surname><given-names>T</given-names></name><name><surname>Kochi</surname><given-names>K</given-names></name><name><surname>Milz</surname><given-names>P</given-names></name><name><surname>Nishida</surname><given-names>K</given-names></name><name><surname>Yoshimura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Comparing EEG/MEG Neuroimaging Methods Based on Localization Error, False Positive Activity, and False Positive Connectivity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/269753</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernet</surname><given-names>CR</given-names></name><name><surname>Appelhoff</surname><given-names>S</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>EEG-BIDS, an extension to the brain imaging data structure for electroencephalography</article-title><source>Scientific Data</source><volume>6</volume><elocation-id>103</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-019-0104-8</pub-id><pub-id pub-id-type="pmid">31239435</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinel</surname><given-names>P</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Rivière</surname><given-names>D</given-names></name><name><surname>LeBihan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Modulation of parietal activation by semantic distance in a number comparison task</article-title><source>NeuroImage</source><volume>14</volume><fpage>1013</fpage><lpage>1026</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0913</pub-id><pub-id pub-id-type="pmid">11697933</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinheiro-Chagas</surname><given-names>P</given-names></name><name><surname>Daitch</surname><given-names>A</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain mechanisms of arithmetic: A crucial role for ventral temporal cortex</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1757</fpage><lpage>1772</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01319</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Mitra</surname><given-names>A</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>NeuroImage</source><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quilty-Dunn</surname><given-names>J</given-names></name><name><surname>Porot</surname><given-names>N</given-names></name><name><surname>Mandelbaum</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The best game in town: The reemergence of the language-of-thought hypothesis across the cognitive sciences</article-title><source>The Behavioral and Brain Sciences</source><volume>46</volume><elocation-id>e261</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X22002849</pub-id><pub-id pub-id-type="pmid">36471543</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>AM</given-names></name><name><surname>Bowen</surname><given-names>RF</given-names></name><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Position sensitivity in the visual word form area</article-title><source>PNAS</source><volume>109</volume><elocation-id>24</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.1121304109</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>Mathias</given-names></name><name><surname>Fagot</surname><given-names>J</given-names></name><name><surname>Caparos</surname><given-names>S</given-names></name><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Amalric</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensitivity to geometric shape regularity in humans and baboons: A putative signature of human singularity</article-title><source>PNAS</source><volume>118</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2023123118</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Ellis</surname><given-names>K</given-names></name><name><surname>Tenenbaum</surname><given-names>J</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A language of thought for the mental representation of geometric shapes</article-title><source>Cognitive Psychology</source><volume>139</volume><elocation-id>101527</elocation-id><pub-id pub-id-type="doi">10.1016/j.cogpsych.2022.101527</pub-id><pub-id pub-id-type="pmid">36403385</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>A geometric shape regularity effect human brain</data-title><version designator="swh:1:rev:133669b577b5ae341cd0842dfb0e7f0af85c0020">swh:1:rev:133669b577b5ae341cd0842dfb0e7f0af85c0020</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0c8d9e1649719fd2a10bc2f8071f586c8954e977;origin=https://github.com/mathias-sm/AGeometricShapeRegularityEffectHumanBrain;visit=swh:1:snp:65377248dcbd07156724ab52a33a12308eb08799;anchor=swh:1:rev:133669b577b5ae341cd0842dfb0e7f0af85c0020">https://archive.softwareheritage.org/swh:1:dir:0c8d9e1649719fd2a10bc2f8071f586c8954e977;origin=https://github.com/mathias-sm/AGeometricShapeRegularityEffectHumanBrain;visit=swh:1:snp:65377248dcbd07156724ab52a33a12308eb08799;anchor=swh:1:rev:133669b577b5ae341cd0842dfb0e7f0af85c0020</ext-link></element-citation></ref><ref id="bib120"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Response to “Crows Recognize Geometric Regularity”</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/zkxh4_v1</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saito</surname><given-names>A</given-names></name><name><surname>Hayashi</surname><given-names>M</given-names></name><name><surname>Takeshita</surname><given-names>H</given-names></name><name><surname>Matsuzawa</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The origin of representational drawing: A comparison of human children and chimpanzees</article-title><source>Child Development</source><volume>85</volume><fpage>2232</fpage><lpage>2246</lpage><pub-id pub-id-type="doi">10.1111/cdev.12319</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sassenhagen</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cluster‐based permutation tests of MEG/EEG data do not establish significance of effect latency or location</article-title><source>Psychophysiology</source><volume>56</volume><elocation-id>e13335</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13335</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Satterthwaite</surname><given-names>TD</given-names></name><name><surname>Elliott</surname><given-names>MA</given-names></name><name><surname>Gerraty</surname><given-names>RT</given-names></name><name><surname>Ruparel</surname><given-names>K</given-names></name><name><surname>Loughead</surname><given-names>J</given-names></name><name><surname>Calkins</surname><given-names>ME</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Hakonarson</surname><given-names>H</given-names></name><name><surname>Gur</surname><given-names>RC</given-names></name><name><surname>Gur</surname><given-names>RE</given-names></name><name><surname>Wolf</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data</article-title><source>NeuroImage</source><volume>64</volume><fpage>240</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.052</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidbauer</surname><given-names>P</given-names></name><name><surname>Hahn</surname><given-names>M</given-names></name><name><surname>Nieder</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Crows recognize geometric regularity</article-title><source>Science Advances</source><volume>11</volume><elocation-id>eadt3718</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.adt3718</pub-id><pub-id pub-id-type="pmid">40215319</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-Score: Which artificial neural network for object recognition is most brain-like?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuhmann</surname><given-names>C</given-names></name><name><surname>Beaumont</surname><given-names>R</given-names></name><name><surname>Vencu</surname><given-names>R</given-names></name><name><surname>Gordon</surname><given-names>C</given-names></name><name><surname>Wightman</surname><given-names>R</given-names></name><name><surname>Cherti</surname><given-names>M</given-names></name><name><surname>Coombes</surname><given-names>T</given-names></name><name><surname>Katta</surname><given-names>A</given-names></name><name><surname>Mullis</surname><given-names>C</given-names></name><name><surname>Wortsman</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Laion-5b: An open large-scale dataset for training next generation image-text models</article-title><source>Advances in Neural Information Processing Systems</source><volume>35</volume><fpage>25278</fpage><lpage>25294</lpage></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sueur</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>From stones to sketches: investigating tracing behaviours in Japanese macaques</article-title><source>Primates</source><volume>66</volume><fpage>183</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1007/s10329-024-01176-y</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Recognition of pictorial representations by chimpanzees (Pan troglodytes)</article-title><source>Animal Cognition</source><volume>10</volume><fpage>169</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1007/s10071-006-0056-1</pub-id><pub-id pub-id-type="pmid">17171361</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Kajola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Presentation of electromagnetic multichannel data: The signal space separation method</article-title><source>Journal of Applied Physics</source><volume>97</volume><pub-id pub-id-type="doi">10.1063/1.1935742</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>JA</given-names></name><name><surname>Sheahan</surname><given-names>H</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><chapter-title>Learning to count visual objects by combining “what” and “where” in recurrent memory</chapter-title><person-group person-group-type="editor"><name><surname>Lourentzou</surname><given-names>In</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Kashyap</surname><given-names>S</given-names></name><name><surname>Karargyris</surname><given-names>A</given-names></name><name><surname>Celi</surname><given-names>LA</given-names></name><name><surname>Kawas</surname><given-names>B</given-names></name><name><surname>Talathi</surname><given-names>S</given-names></name></person-group><source>Proceedings of The 1st Gaze Meets ML Workshop</source><publisher-name>PMLR</publisher-name><fpage>199</fpage><lpage>218</lpage></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of face perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>411</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094238</pub-id><pub-id pub-id-type="pmid">18558862</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Comparing face patch systems in macaques and humans</article-title><source>PNAS</source><volume>105</volume><fpage>19514</fpage><lpage>19519</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809662105</pub-id><pub-id pub-id-type="pmid">19033466</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: Improved N3 Bias Correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tversky</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visualizing thought</article-title><source>Topics in Cognitive Science</source><volume>3</volume><fpage>499</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1111/j.1756-8765.2010.01113.x</pub-id><pub-id pub-id-type="pmid">25164401</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uusitalo</surname><given-names>MA</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Signal-space projection method for separating MEG or EEG into components</article-title><source>Medical &amp; Biological Engineering &amp; Computing</source><volume>35</volume><fpage>135</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1007/BF02534144</pub-id><pub-id pub-id-type="pmid">9136207</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinckier</surname><given-names>F</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Jobert</surname><given-names>A</given-names></name><name><surname>Dubus</surname><given-names>JP</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hierarchical coding of letter strings in the ventral stream: dissecting the inner organization of the visual word-form system</article-title><source>Neuron</source><volume>55</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.05.031</pub-id><pub-id pub-id-type="pmid">17610823</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Waerden</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Geometry and Algebra in Ancient Civilizations</source><publisher-name>Springer Science &amp; Business Media</publisher-name></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Mok Shim</surname><given-names>W</given-names></name><name><surname>Dang</surname><given-names>S</given-names></name><name><surname>Triantafyllou</surname><given-names>C</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Feedback of visual object information to foveal retinotopic cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1439</fpage><lpage>1445</lpage><pub-id pub-id-type="doi">10.1038/nn.2218</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A tale of two visual systems: invariant and adaptive visual information representations in the primate brain</article-title><source>Annual Review of Vision Science</source><volume>4</volume><fpage>311</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091517-033954</pub-id><pub-id pub-id-type="pmid">29949722</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Vignali</surname><given-names>L</given-names></name><name><surname>Sigismondi</surname><given-names>F</given-names></name><name><surname>Crepaldi</surname><given-names>D</given-names></name><name><surname>Bottini</surname><given-names>R</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Similar object shape representation encoded in the inferolateral occipitotemporal cortex of sighted and early blind people</article-title><source>PLOS Biology</source><volume>21</volume><elocation-id>e3001930</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001930</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>M</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>de Gelder</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ventral and dorsal pathways relate differently to visual awareness of body postures under continuous flash suppression</article-title><source>Eneuro</source><volume>5</volume><elocation-id>2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0285-17.2017</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s11"><title>Additional results in the behavior task: comparison with other CNNs</title><p>In <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, we report additional results from correlating human behavior with various models (top; DenseNet and ResNet) and various layers of a single model (bottom; many layers of CORnet). In all cases, the CNN encoding model is a much worse predictor of the human behavior than the geometric feature model (all p &lt; 0.001). The late layers of CORnet, DenseNet, and ResNet all capture some of the variance of participants' behaviors to comparable; while ResNet is the highest scoring in this analysis, we can see in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> that they are all highly correlated, and in order to stay close to previous work, we used CORnet’s layer IT throughout the article.</p><p>Additionally, the fit of the successive layers of CORnet (V1, V2, V4: small effect sizes, only significant for V1; IT, flatten: much higher effect sizes, increasing between IT and flatten) indicates that the feed-forward processing of the visual input by the CNN encoding yields internal representations that are increasingly closer to humans’. This suggests that even the visual model goes beyond local properties that V1 would capture – but in all cases, the fit is much worse than the geometric feature model.</p></sec><sec sec-type="appendix" id="s12"><title>Intruder task during fMRI</title><p>Overall, both age groups performed better than chance (all p &lt; 0.001) at detecting the intruder, with an average response time of 1.03 s in adults, 1.36 s in children, for the easy condition, and 0.79 s in adults in the hard condition. Both error rates and response times are modulated by the reference shape (all p &lt; 0.001), and both are correlated with error rate data from outside the scanner (all p &lt; 0.05).</p></sec><sec sec-type="appendix" id="s13"><title>Intruder task in MEG participants</title><p>Participants recruited for the MEG study performed no behavioral task inside the MEG, as we relied on a passive presentation oddball paradigm. After the scanner session, participants took one run of the intruder detection task previously used online and described in <xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref> – we presented them with the task after the scanning session to avoid biasing participants toward actively looking for intruders in the oddball paradigm. At the group level, the data fully replicated the geometric regularity effect (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>) (correlation of error rate with that of previous participants, regression over 11 shapes: <italic>r</italic><sup>2</sup> = 0.90, p &lt; 0.001). A mixed-effect model correlating the error rates of the two groups, with both the slope and the intercept as separate random effects, yielded an intercept not significantly different from 0 (p = 0.42), and a slope significantly different from 0 (p &lt; 1e−10) and not significantly different from 1 (<italic>P</italic>=.19) suggesting that the data was similar in the two groups. We also correlated each participant’s average error rate per shape to the group data from our previous dataset (<xref ref-type="bibr" rid="bib117">Sablé-Meyer et al., 2021</xref>). A one-tailed test for a positive slope indicated that 19 out of 20 participants displayed a significant geometric regularity effect. We still included the data from the participant that did not display a significant effect, as we had not decided on such a rejection criterion beforehand.</p></sec><sec sec-type="appendix" id="s14"><title>fMRI contrasts for geometric shape in the visual localizer</title><p>In order to isolate the brain responses to geometric shapes, we focused on the simplest possible contrast, that is greater activation to the presentation of a single geometric shape versus all of its single-image controls (face, house, and tool). This contrast is presented in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. Note that we excluded Chinese characters from this comparison because they often include geometric features (e.g. parallel lines), but including them gave virtually identical results (compare <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, identical list of cluster-level corrected clusters at the p &lt; 0.05 level in both age groups). We also included in the design rows of three distinct geometric shapes (e.g. square, triangle, and circle). Our logic here was that this condition, although somewhat artificial from the geometric viewpoint, could be very tightly matched with two other conditions, namely a string of three letters (‘words’ condition, e.g. BON) or a small three-symbol mathematical operation (‘numbers’ condition, e.g. 3 + 1). However, the corresponding contrast (three geometric shapes &gt; words and numbers did not give any positive activation: no positive cluster was significant at the whole-brain cluster-level corrected p &lt; 0.05 level in either age group; additionally, none of the regions of interest (ROIs) identified in the single shape contrast was significant for this contrast: aIPS left, p = 0.975 for adults and p = 0.389 for children; aIPS right, p = 0.09 in both adults and children; pITG right, p = 0.13 in adults and p = 0.362 in children). We reasoned, however, that numbers should be excluded from this contrast because, by our very hypothesis, geometric shapes should activate a number-based program-like representations (e.g. square = repeat(4){line, turn}) (<xref ref-type="bibr" rid="bib118">Sablé-Meyer et al., 2022</xref>). When restricting the contrast to ‘three geometric shapes &gt; words’, at the whole-brain level, no cluster reached significance at the p &lt; 0.05 level (cluster-level correction). However, in this case, testing the ROIs identified in the single shape condition revealed that while the left aIPS still did not reach significance at the p &lt; 0.05 level (p = 0.71 in adults, p = 0.30 in children), both the right aIPS and the right pITG reached significance (aIPS right: p &lt; 0.001 in adults and p = 0.014 in children; pITG, p = 0.008 in adults and p = 0.046 in children).</p></sec><sec sec-type="appendix" id="s15"><title>Additional ROI analysis of the ventral pathway in fMRI (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>)</title><p>We used individually defined ROIs to probe the fMRI response to geometric shapes in four cortical regions defined by their preferential responses to four other visual categories known to be selectively processed in the ventral visual pathway: faces, houses, tools, and words. To this aim, we first identified, at the group level, clusters of voxels associated with each visual category. Such clusters were identified using a non-parametric permutation test across participants at the whole-brain level using a contrast for the target category against the mean of the other three (voxel p &lt; 0.001 then clusters p &lt; 0.05 except for fusiform face area [FFA] in children, p = 0.09, and the absence of a well-identified visual word form area [VWFA] in children). Significant clusters that intersect the MNI coordinate <italic>z</italic> = −14 are shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>; in the case of the FFA and the VWFA, we restricted ourselves to clusters at MNI coordinates typically found in the literature, respectively (−45, –57, –14) and (40, −55, −14).</p><p>Then, within each such ROI identified in adults, we identified for each subject, including children, the 10% most responsive subject-specific voxels in the same contrast used to identify the cluster. To avoid double-dipping, we selected the voxels using the contrast from a single run, then collected the fMRI responses (beta coefficients) to all categories from the other runs, and then replicated this procedure across all runs while averaging the responses to a given category. The average coefficients within each such individually defined cortical ROI are shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, separately for children and adults. Several observations are in order, and detailed below for each visual category.</p><p>In the left-hemispheric VWFA, we can see that voxels are indeed responsive to written words in the participants’ language (French), more than to an unknown language (Chinese), in both adults and children (paired <italic>t</italic>-tests, p &lt; 0.001 in adults, p = 0.003 in children). VWFA voxels also responded to the symbolic display of numbers entering into small computations (e.g. 3 + 1) in adults, but this response did not appear to be developed yet in children. VWFA voxels also showed a response to tools, particularly in young children, as has already been reported (<xref ref-type="bibr" rid="bib40">Dehaene-Lambertz et al., 2018</xref>). In the opposite direction, they were particularly under-activated by houses in adults. Finally, and most crucially, geometric shapes, whether presented alone or in a string of 3, did not elicit a strong response, indeed no stronger than non-preferred categories such as Chinese characters or faces (p = 0.37 in adults, p = 0.057 in children on a one-tailed paired <italic>t</italic>-test).</p><p>In the right-hemispheric FFA, we only saw a purely selective response to faces in both adults and children. All the other visual categories yielded an equally low level of activity in this area. In particular, the responses to geometric shapes were not significantly different from those to other visual categories.</p><p>In the bilateral ROIs responsive to tools, a very similar result was found: apart from their strong response to tools and their slightly increased activation to houses and reduced activations to written words and numbers in the right hemisphere, these voxels elicit activations that were essentially equal across all non-preferred visual categories, with geometric shapes being no exception.</p><p>Finally, bilateral house-responsive ROIs corresponding to the parahippocampal place area (PPA) were also mildly responsive to tools in both populations and both hemispheres. However, geometric shapes did not activate these clusters more than other visual categories such as faces (both hemispheres, both age groups have p &gt; 0.05 in one-tailed paired <italic>t</italic>-tests).</p></sec><sec sec-type="appendix" id="s16"><title>Additional analysis: evoked response potentials</title><p>Different participants can, in principle, end up with very different decoder weights, since the decoders are trained independently for each participant. Several of our analyses are based on the decoders’ performance only, and therefore the decoder’s weights associated with each sensor are not considered. To stay closer to the MEG data, we replicated the previous analysis directly from evoked potential data in the gradiometers. Using spatiotemporal permutation testing, we identified a set of sensors and timepoints across participants where the reference and the oddball epochs were significantly different. We identified three significant clusters: [268, 844], [440, 896], and [492, 896] ms.</p><p>Then we computed the average difference between the reference and the oddball trials across these sensors separately for each shape. Finally, we correlated the differences with the number of geometric features in the reference shape. The first cluster, from 268 to 844 ms, did not elicit any significant correlation with the geometric regularity; however, the two others yielded significant clusters at the p &lt; 0.05 level, although with later timing than the decoding analysis, at around ~600 ms.</p></sec><sec sec-type="appendix" id="s17"><title>MEG: joint MEG-fMRI RSA</title><p>Representational similarity analysis also offers a way to directly compare similarity matrices measured in MEG and fMRI, thus allowing for fusion of those two modalities and tentatively assigning a ‘time stamp’ to distinct MRI clusters (<xref ref-type="bibr" rid="bib31">Cichy and Oliva, 2020</xref>). However, we did not attempt such an analysis here for several reasons. First, distinct tasks and block structures were used in MEG and fMRI. Second, a smaller list of shapes was used in fMRI, as imposed by the slower modality of acquisition. Third, our study was designed as an attempt to sort out between two models of geometric shape recognition. We therefore focused all analyses on this goal, which could not have been achieved by direct MEG–fMRI fusion, but required correlation with independently obtained model predictions.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Coordinates and characteristics of significant fMRI clusters responding to geometric shapes in localizer runs.</title><p>For each age group, each line gives the peak coordinates, volume, and statistics of a cluster with p &lt; 0.05 (whole brain, permutation test) for the contrast ‘single shape &gt; other single visual categories’. The sign of the peak <italic>t</italic>-value and the shading indicate whether the contrast was positive (white background) or negative (gray background). Coordinates are given in MNI space.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Age group</th><th align="left" valign="bottom"><italic>X</italic></th><th align="left" valign="bottom"><italic>Y</italic></th><th align="left" valign="bottom"><italic>Z</italic></th><th align="left" valign="bottom">Peak <italic>t</italic>-value</th><th align="left" valign="bottom">Volume in cm<sup>2</sup></th><th align="left" valign="bottom">Cluster corrected p-value</th></tr></thead><tbody><tr><td align="left" valign="bottom">Adults</td><td align="left" valign="bottom">51.5</td><td align="char" char="." valign="bottom">–54.5</td><td align="left" valign="bottom">–8.5</td><td align="left" valign="bottom">5.12</td><td align="left" valign="bottom">4.4</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">35.5</td><td align="char" char="." valign="bottom">–48.5</td><td align="left" valign="bottom">55.5</td><td align="left" valign="bottom">4.93</td><td align="left" valign="bottom">7.2</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">39.5</td><td align="char" char="." valign="bottom">–76.5</td><td align="left" valign="bottom">–12.5</td><td align="left" valign="bottom">–6.33</td><td align="left" valign="bottom">51.4</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">–34.5</td><td align="char" char="." valign="bottom">–70.5</td><td align="left" valign="bottom">–10.5</td><td align="left" valign="bottom">–5.92</td><td align="left" valign="bottom">42</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom">Children</td><td align="left" valign="bottom">–12.5</td><td align="char" char="." valign="bottom">–76.5</td><td align="left" valign="bottom">–48.5</td><td align="left" valign="bottom">5.51</td><td align="left" valign="bottom">3.9</td><td align="left" valign="bottom">0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">55.5</td><td align="char" char="." valign="bottom">–24.5</td><td align="left" valign="bottom">45.5</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">8.3</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">21.5</td><td align="char" char="." valign="bottom">–80.5</td><td align="left" valign="bottom">–46.5</td><td align="left" valign="bottom">4.9</td><td align="left" valign="bottom">4.4</td><td align="left" valign="bottom">0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">–42.5</td><td align="char" char="." valign="bottom">–38.5</td><td align="left" valign="bottom">35.5</td><td align="left" valign="bottom">4.9</td><td align="left" valign="bottom">6.4</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">21.5</td><td align="char" char="." valign="bottom">–94.5</td><td align="left" valign="bottom">–8.5</td><td align="left" valign="bottom">–6.34</td><td align="left" valign="bottom">40.1</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">–24.5</td><td align="char" char="." valign="bottom">–98.5</td><td align="left" valign="bottom">–8.5</td><td align="left" valign="bottom">–6.17</td><td align="left" valign="bottom">55.3</td><td align="left" valign="bottom">&lt;0.01</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Coordinates and characteristics of significant fMRI clusters in the RSA analysis.</title><p>Coordinates and characteristics of significant fMRI clusters in the RSA analysis. Same organization as <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> for the RSA analysis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Age group</th><th align="left" valign="bottom">Model</th><th align="left" valign="bottom"><italic>X</italic></th><th align="left" valign="bottom"><italic>Y</italic></th><th align="left" valign="bottom"><italic>Z</italic></th><th align="left" valign="bottom">Peak <italic>t</italic>-value</th><th align="left" valign="bottom">Volume in cm<sup>2</sup></th><th align="left" valign="bottom">Cluster corrected p-value</th></tr></thead><tbody><tr><td align="left" valign="bottom">Adults</td><td align="left" valign="bottom">Geometric features</td><td align="left" valign="bottom">–0.5</td><td align="left" valign="bottom">13.5</td><td align="left" valign="bottom">49.5</td><td align="left" valign="bottom">5.4</td><td align="left" valign="bottom">64.4</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–22.5</td><td align="left" valign="bottom">–54.5</td><td align="left" valign="bottom">51.5</td><td align="left" valign="bottom">5.38</td><td align="left" valign="bottom">13.6</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–14.5</td><td align="left" valign="bottom">–66.5</td><td align="left" valign="bottom">5.5</td><td align="left" valign="bottom">5.28</td><td align="left" valign="bottom">6.1</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">31.5</td><td align="left" valign="bottom">31.5</td><td align="left" valign="bottom">–8.5</td><td align="left" valign="bottom">4.86</td><td align="left" valign="bottom">2.2</td><td align="left" valign="bottom">0.03</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–26.5</td><td align="left" valign="bottom">–2.5</td><td align="left" valign="bottom">49.5</td><td align="left" valign="bottom">4.79</td><td align="left" valign="bottom">5.3</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–8.5</td><td align="left" valign="bottom">–72.5</td><td align="left" valign="bottom">–38.5</td><td align="left" valign="bottom">4.62</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–34.5</td><td align="left" valign="bottom">23.5</td><td align="left" valign="bottom">5.5</td><td align="left" valign="bottom">4.15</td><td align="left" valign="bottom">3.2</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–10.5</td><td align="left" valign="bottom">71.5</td><td align="left" valign="bottom">3.5</td><td align="left" valign="bottom">4.03</td><td align="left" valign="bottom">1.9</td><td align="left" valign="bottom">0.03</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–46.5</td><td align="left" valign="bottom">–0.5</td><td align="left" valign="bottom">33.5</td><td align="left" valign="bottom">3.88</td><td align="left" valign="bottom">1.6</td><td align="left" valign="bottom">0.04</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">21.5</td><td align="left" valign="bottom">–98.5</td><td align="left" valign="bottom">–6.5</td><td align="left" valign="bottom">3.72</td><td align="left" valign="bottom">2.3</td><td align="left" valign="bottom">0.02</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom">CNN encoding</td><td align="left" valign="bottom">23.5</td><td align="left" valign="bottom">–14.5</td><td align="left" valign="bottom">57.5</td><td align="left" valign="bottom">5.4</td><td align="left" valign="bottom">2.4</td><td align="left" valign="bottom">0.03</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–48.5</td><td align="left" valign="bottom">–82.5</td><td align="left" valign="bottom">–0.5</td><td align="left" valign="bottom">4.96</td><td align="left" valign="bottom">4.4</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">–30.5</td><td align="left" valign="bottom">–80.5</td><td align="left" valign="bottom">25.5</td><td align="left" valign="bottom">4.55</td><td align="left" valign="bottom">2.9</td><td align="left" valign="bottom">0.02</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">1.5</td><td align="left" valign="bottom">21.5</td><td align="left" valign="bottom">45.5</td><td align="left" valign="bottom">4.54</td><td align="left" valign="bottom">3.8</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">27.5</td><td align="left" valign="bottom">33.5</td><td align="left" valign="bottom">5.5</td><td align="left" valign="bottom">4.51</td><td align="left" valign="bottom">3.7</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">45.5</td><td align="left" valign="bottom">–80.5</td><td align="left" valign="bottom">–2.5</td><td align="left" valign="bottom">4.38</td><td align="left" valign="bottom">2.2</td><td align="left" valign="bottom">0.03</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">53.5</td><td align="left" valign="bottom">13.5</td><td align="left" valign="bottom">31.5</td><td align="left" valign="bottom">4.05</td><td align="left" valign="bottom">3.9</td><td align="left" valign="bottom">&lt;0.01</td></tr><tr><td align="left" valign="bottom">Children</td><td align="left" valign="bottom">CNN encoding</td><td align="left" valign="bottom">–22.5</td><td align="left" valign="bottom">–84.5</td><td align="left" valign="bottom">11.5</td><td align="left" valign="bottom">4.59</td><td align="left" valign="bottom">1.4</td><td align="left" valign="bottom">0.06</td></tr><tr><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom">43.5</td><td align="left" valign="bottom">–82.5</td><td align="left" valign="bottom">11.5</td><td align="left" valign="bottom">4.29</td><td align="left" valign="bottom">2.2</td><td align="left" valign="bottom">0.02</td></tr></tbody></table></table-wrap></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106464.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Groen</surname><given-names>Iris IA</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> series of studies provides converging results from complementary neuroimaging and behavioral experiments to identify human brain regions involved in representing regular geometric shapes and their core features. Geometric shape concepts are present across diverse human cultures and possibly involved in human capabilities such as numerical cognition and mathematical reasoning. Identifying the brain networks involved in geometric shape representation is of broad interest to researchers studying human visual perception, reasoning, and cognition. The evidence supporting the presence of representation of geometric shape regularity in dorsal parietal and prefrontal cortex is <bold>solid</bold>, but does not directly demonstrate that these circuits overlap with those involved in mathematical reasoning. Furthermore, the links to defining features of geometric objects and with mathematical and symbolic reasoning would benefit from stronger evidence from more fine-tuned experimental tasks varying the stimuli and experience.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106464.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This paper examines how geometric regularities in abstract shapes (e.g., parallelograms, kites) are perceived and processed in the human brain. The manuscript contains multimodal data (behavior, fMRI, MEG) from adults and additional fMRI data from 6-year-old children. The key findings show that (1) processing geometric shapes lead to reduced activity in ventral areas in comparison to complex stimuli and increased activity in intraparietal and inferior temporal regions, (2) the degree of geometric regularity modulates activity in intraparietal and inferior temporal regions, (3) similarity in neural representation of geometric shapes can be captured early by using CNN models and later by models of geometric regularity. In addition to these novel findings, the paper also includes a replication of behavioral data, showing that the perceptual similarity structure amongst the geometric stimuli used can be explained by a combination of visual similarities (as indexed by feedforward CNN model of ventral visual pathway) and geometric features. The paper comes with openly accessible code in a well-documented GitHub repository and the data will be published with the paper on OpenNeuro.</p><p>In the revised version of this manuscript, the authors clarified certain aspects of the task design, added critical detail to the description of the methods, and updated the figures to show unsmoothed data and variability across participants. Importantly, the authors thoroughly discussed potential task effects (for the fMRI data only) and added additional analyses that indicate that the effects are unlikely to be driven by linguistic labels/name availability of the stimuli.</p><p>Comments on the revision:</p><p>Thank you for carefully addressing all my concerns and especially for clarifying the task design.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106464.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>The current study seeks to understand the neural mechanisms underlying geometric reasoning. Using fMRI with both children and adults, the authors found that contrasting simple geometric shapes with naturalistic images (faces, tools, houses) led to responses in the dorsal visual stream, rather than ventral regions that are generally thought to represent shape properties. The author's followed up on this result using computational modeling and MEG to show that geometric properties explain distinct variance in the neural response than what is captured by a CNN.</p><p>Strengths</p><p>These findings contribute much-needed neural and developmental data to the ongoing debate regarding shape processing in the brain and offer additional insights into why CNNs may have difficulty with shape processing. The motivation and discussion for the study is appropriately measured, and I appreciate the authors' use of multiple populations, neuroimaging modalities, and computational models in explore this question.</p><p>Weaknesses</p><p>The presence of activation in aIPS led the authors to interpret their results to mean that geometric reasoning draws on the same processes as mathematical thinking. However, there is only weak and indirect evidence in the current study that geometric reasoning, as its tested here, draws on the same circuits as math.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106464.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors report converging evidence from behavioral studies as well as several brain-imaging techniques that geometric figures, notably quadrilaterals, are processed differently in visual (lower activation) and spatial (greater) areas of the human brain than representative figures. Comparison of mathematical models to fit activity for geometric figures shows the best fit for abstract geometric features like parallelism and symmetry. The brain areas active for geometric figures are also active in processing mathematical concepts even in blind mathematicians, linking geometric shapes to abstract math concepts. The effects are stronger in adults than in 6-year-old Western children. Similar phenomena do not appear in great apes, suggesting that this is uniquely human and developmental.</p><p>Strengths:</p><p>Multiple converging techniques of brain imaging and testing of mathematical models showing special status of perception of abstract forms. Careful reasoning at every step of research and presentation of research, anticipating and addressing possible reservations. Connecting these findings to other findings, brain, behavior, and historical/anthropological to suggest broad and important fundamental connections between abstract visual-spatial forms and mathematical reasoning.</p><p>Weaknesses:</p><p>I have reservations of the authors' use of &quot;symbolic.&quot; They seem to interpret &quot;symbolic&quot; as relying on &quot;discrete, exact, rule-based features.&quot; Words are generally considered to symbolic (that is their major function), yet words do not meet those criteria. Depictions of objects can be regarded as symbolic because they represent real objects, they are not the same as the object (as Magritte observed). If so then perhaps depictions of quadrilaterals are also symbolic but then they do not differ from depictions of objects on that quality. Relatedly, calling abstract or generalized representations of forms a distinct &quot;language of thought&quot; doesn't seem supportable by the current findings. Minimally, a language has elements that are combined more or less according to rules. The authors present evidence for geometric forms as elements but nowhere is there evidence for combining them into meaningful strings.</p><p>Further thoughts</p><p>Incidentally, there have been many attempts at constructing visual languages from visual elements combined by rules, that is, mapping meaning to depictions. Many written languages like Egyptian hieroglyphics or Mayan or Chinese, began that way; there are current attempts using emoji. Apparently, mapping sound to discrete letters, alphabets, is more efficient and was invented once but spread. That said, for restricted domains like maps, circuit diagrams, networks, chemical interactions, mathematics, and more, visual &quot;languages&quot; work quite well.</p><p>The findings are striking and as such invite speculation about their meaning and limitations. The images of real objects seem to be interpreted as representations of 3D objects as they activate the same visual areas as real objects. By contrast, the images of 2D geometric forms are not interpreted as representations of real objects but rather seemingly as 2D abstractions. It would be instructive to investigate stimuli that are on a continuum from representational to geometric, e. g., real objects that have simple geometric forms like table tops or boxes under various projections or balls or buildings that are rectangular or triangular. Objects differ from geometric forms in many ways: 3D rather than 2D, more complicated shapes; internal features as well as outlines. The geometric figures used are flat, 2-D, but much geometry is 3-D (e. g. cubes) with similar abstract features. The feature space of geometry is more than parallelism and symmetry; angles are important for example. Listing and testing features would be fascinating.</p><p>Can we say that mathematical thinking began with the regularities of shapes or with counting, or both? External representations of counting go far back into prehistory; tallies are frequent and wide-spread. Infants are sensitive to number across domains as are other primates (and perhaps other species). Finding overlapping brain areas for geometric forms and number is intriguing but doesn't show how they are related.</p><p>Categories are established in part by contrast categories; are quadrilaterals and triangles and circles different categories? As for quadrilaterals, the authors say some are &quot;completely irregular.&quot; Not really; they are still quadrilaterals, if atypical. See Eleanor Rosch's insightful work on (visual) categories. One wonders about distinguishing squashed quadrilaterals from squashed triangles.</p><p>What in human experience but not the experience of close primates would drive the abstraction of these geometric properties? It's easy to make a case for elaborate brain processes for recognizing and distinguishing things in the world, shared by many species, but the case for brain areas sensitive to abstracting geometric figures is harder. The fact that these areas are active in blind mathematicians and that they are parietal areas suggest that what is important is spatial far more than visual. Could these geometric figures and their abstract properties be connected in some way to behavior, perhaps with fabrication, construction or use of objects? Or with other interactions with complex objects and environments where symmetry and parallelism (and angles and curvature--and weight and size) would be important? Manual dexterity and fabrication also distinguish humans from great apes (quantitatively not qualitatively) and action drives both visual and spatial representations of objects and spaces in the brain. I certainly wouldn't expect the authors to add research to this already packed paper, but raising some of the conceptual issues would contribute to the significance of the paper.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106464.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sablé-Meyer</surname><given-names>Mathias</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Benjamin</surname><given-names>Lucas</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xjwb503</institution-id><institution>Université Paris-Saclay</institution></institution-wrap><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Potier Watkins</surname><given-names>Cassandra</given-names></name><role specific-use="author">Author</role><aff><institution>Collège de France, Université Paris-Sciences-Lettres (PSL), 11 Place Marcelin Berthelot, 75005 Paris, France</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Chenxi</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Pajot</surname><given-names>Maxence</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Morfoisse</surname><given-names>Théo</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Al Roumi</surname><given-names>Fosca</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution><addr-line><named-content content-type="city">Gif-sur-Yvette</named-content></addr-line><country>France</country></aff></contrib><contrib contrib-type="author"><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name><role specific-use="author">Author</role><aff><institution>Cognitive Neuroimaging Unit, CEA, INSERM, Université Paris-Saclay, NeuroSpin Center</institution><addr-line><named-content content-type="city">Gif/Yvette</named-content></addr-line><country>France</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Weakness:</p><p>I wonder how task difficulty and linguistic labels interact with the current findings. Based on the behavioral data, shapes with more geometric regularities are easier to detect when surrounded by other shapes. Do shape labels that are readily available (e.g., &quot;square&quot;) help in making accurate and speedy decisions? Can the sensitivity to geometric regularity in intraparietal and inferior temporal regions be attributed to differences in task difficulty? Similarly, are the MEG oddball detection effects that are modulated by geometric regularity also affected by task difficulty?</p></disp-quote><p>We see two aspects to the reviewer’s remarks.</p><p>(1) Names for shapes.</p><p>On the one hand, is the question of the impact of whether certain shapes have names and others do not in our task. The work presented here is not designed to specifically test the effect of formal western education; however, in previous work (Sablé-Meyer et al., 2021), we noted that the geometric regularity effect remains present even for shapes that do not have specific names, and even in participants who do not have names for them. Thus, we replicated our main effects with both preschoolers and adults that did not attend formal western education and found that our geometric feature model remained predictive of their behavior; we refer the reader to this previous paper for an extensive discussion of the possible role of linguistic labels, and the impact of the statistics of the environment on task performance.</p><p>What is more, in our behavior experiments we can discard data from any shape that is has a name in English and run our model comparison again. Doing so diminished the effect size of the geometric feature model, but it remained predictive of human behavior: indeed, if we removed all shapes but kite, rightKite, rustedHinge, hinge and random (i.e., more than half of our data, and shapes for which we came up with names but there are no established names), we nevertheless find that both models significantly correlate with human behavior—see plot in Author response image 1, equivalent of our Fig. 1E with the remaining shapes.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106464-sa4-fig1-v1.tif"/></fig><p>An identical analysis on the MEG leads to two noisy but significant clusters (CNN: 64.0ms to 172.0ms; then 192.0ms to 296.0ms; both p&lt;.001: Geometric Features: 312.0ms to 364.0ms with p=.008). We have improved our manuscript thanks to the reviewer’s observation by adding a figure with the new behavior analysis to the supplementary figures and in the result section of the behavior task. We now refer to these analysis where appropriate:</p><p>(intro) “The effect appeared as a human universal, present in preschoolers, first-graders, and adults without access to formal western math education (the Himba from Namibia), and thus seemingly independent of education and of the existence of linguistic labels for regular shapes.”</p><p>(behavior results) “Finally, to separate the effect of name availability and geometric features on behavior, we replicated our analysis after removing the square, rectangle, trapezoids, rhombus and parallelogram from our data (Fig. S5D). This left us with five shapes, and an RDM with 10 entries, When regressing it in a GLM with our two models, we find that both models are still significant predictors (p&lt;.001). The effect size of the geometric feature model is greatly reduced, yet remained significantly higher than that of the neural network model (p&lt;.001).”</p><p>(meg results) “This analysis yielded similar clusters when performed on a subset of shapes that do not have an obvious name in English, as was the case for the behavior analysis (CNN Encoding: 64.0ms to 172.0ms; then 192.0ms to 296.0ms; both p&lt;.001: Geometric Features: 312.0ms to 364.0ms with p=.008).”</p><p>(discussion, end of behavior section) “Previously, we only found such a significant mixture of predictors in uneducated humans (whether French preschoolers or adults from the Himba community, mitigating the possible impact of explicit western education, linguistic labels, and statistics of the environment on geometric shape representation) (Sablé-Meyer et al., 2021).”</p><p>Perhaps the referee’s point can also be reversed: we provide a normative theory of geometric shape complexity which has the potential to explain why certain shapes have names: instead of seeing shape names as the cause of their simpler mental representation, we suggest that the converse could occur, i.e. the simpler shapes are the ones that are given names.</p><p>(2) Task difficulty</p><p>On the other hand is the question of whether our effect is driven by task difficulty. First, we would like to point out that this point could apply to the fMRI task, which asks for an explicit detection of deviants, but does not apply to the MEG experiment. In MEG, participants passively looked at sequences of shapes which, for a given block, comprising many instances of a fixed standard shape and rare deviants–even if they notice deviants, they have no task related to them. Yet two independent findings validated the geometric features model: there was a large effect of geometric regularity on the MEG response to deviants, and the MEG dissimilarity matrix between standard shapes correlated with a model based on geometric features, better than with a model based on CNNs. While the response to rare deviants might perhaps be attributed to “difficulty” (assuming that, in spite of the absence of an explicit task, participants try to spot the deviants and find this self-imposed task more difficult in runs with less regular shapes), it seems very hard to explain the representational similarity analysis (RSA) findings based on difficulty. Indeed, what motivated us to use RSA analysis in both fMRI and MEG was to stop relying on the response to deviants, and use solely the data from standard or “reference” shapes, and model their neural response with theory-derived regressors.</p><p>We have updated the manuscript in several places to make our view on these points clearer:</p><p>(experiment 4) “This design allowed us to study the neural mechanisms of the geometric regularity effect without confounding effects of task, task difficulty, or eye movements.”</p><p>(figure 4, legend) “(A) Task structure: participants passively watch a constant stream of geometric shapes, one per second (presentation time 800ms). The stimuli are presented in blocks of 30 identical shapes up to scaling and rotation, with 4 occasional deviant shape. Participants do not have a task to perform beside fixating.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Weakness:</p><p>Given that the primary take away from this study is that geometric shape information is found in the dorsal stream, rather than the ventral stream there is very little there is very little discussion of prior work in this area (for reviews, see Freud et al., 2016; Orban, 2011; Xu, 2018). Indeed, there is extensive evidence of shape processing in the dorsal pathway in human adults (Freud, Culham, et al., 2017; Konen &amp; Kastner, 2008; Romei et al., 2011), children (Freud et al., 2019), patients (Freud, Ganel, et al., 2017), and monkeys (Janssen et al., 2008; Sereno &amp; Maunsell, 1998; Van Dromme et al., 2016), as well as the similarity between models and dorsal shape representations (Ayzenberg &amp; Behrmann, 2022; Han &amp; Sereno, 2022).</p></disp-quote><p>We thank the reviewer for this opportunity to clarify our writing. We want to use this opportunity to highlight that our primary finding is not about whether the shapes of objects or animals (in general) are processed in the ventral versus or the dorsal pathway, but rather about the much more restricted domain of geometric shapes such as squares and triangles. We propose that simple geometric shapes afford additional levels of mental representation that rely on their geometric features – on top of the typical visual processing. To the best of our knowledge, this point has not been made in the above papers.</p><p>Still, we agree that it is useful to better link our proposal to previous ones. We have updated the discussion section titled “Two Visual Pathways” to include more specific references to the literature that have reported visual object representations in the dorsal pathway. Following another reviewer’s observation, we have also updated our analysis to better demonstrate the overlap in activation evoked by math and by geometry in the IPS, as well as include a novel comparison with independently published results.</p><p>Overall, to address this point, we (i) show the overlap between our “geometry” contrast (shape &gt; word+tools+houses) and our “math” contrast (number &gt; words); (ii) we display these ROIs side by side with ROIs found in previous work (Amalric and Dehaene, 2016), and (iii) in each math-related ROIs reported in that article, we test our “geometry” (shape &gt; word+tools+houses) contrast and find almost all of them to be significant in both population; see Fig. S5.</p><p>Finally, within the ROIs identified with our geometry localizer, we also performed similarity analyses: for each region we extracted the betas of every voxel for every visual category, and estimated the distance (cross-validated mahalanobis) between different visual categories. In both ventral ROIs, in both populations, numbers were closer to shapes than to the other visual categories including text and Chinese characters (all p&lt;.001). In adults, this result also holds for the right ITG (p=.021) and the left IPS (p=.014) but not the right IPS (p=.17). In children, this result did not hold in the areas.</p><p>Naturally, overlap in brain activation does not suffice to conclude that the same computational processes are involved. We have added an explicit caveat about this point. Indeed, throughout the article, we have been careful to frame our results in a way that is appropriate given our evidence, e.g. saying “Those areas are similar to those active during number perception, arithmetic, geometric sequences, and the processing of high-level math concepts” and “The IPS areas activated by geometric shapes overlap with those active during the comprehension of elementary as well as advanced mathematical concepts”. We have rephrased the possibly ambiguous “geometric shapes activated math- and number-related areas, particular the right aIPS.” into “geometric shapes activated areas independently found to be activated by math- and number-related tasks, in particular the right aIPS”.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Weakness:</p><p>Perhaps the manuscript could emphasize that the areas recruited by geometric figures but not objects are spatial, with reduced processing in visual areas. It also seems important to say that the images of real objects are interpreted as representations of 3D objects, as they activate the same visual areas as real objects. By contrast, the images of geometric forms are not interpreted as representations of real objects but rather perhaps as 2D abstractions.</p></disp-quote><p>This is an interesting possibility. Geometric shapes are likely to draw attention to spatial dimensions (e.g. length) and to do so in a 2D spatial frame of reference rather than the 3D representations evoked by most other objects or images. However, this possibility would require further work to be thoroughly evaluated, for instance by comparing usual 3D objects with rare instances of 2D ones (e.g. a sheet of paper, a sticker etc). In the absence of such a test, we refrained from further speculation on this point.</p><disp-quote content-type="editor-comment"><p>The authors use the term &quot;symbolic.&quot; That use of that term could usefully be expanded here.</p></disp-quote><p>The reviewer is right in pointing out that “symbolic” should have been more clearly defined. We now added in the introduction:</p><p>(introduction) “[…] we sometimes refer to this model as “symbolic” because it relies on discrete, exact, rule-based features rather than continuous representations (Sablé-Meyer et al., 2022). In this representational format, geometric shapes are postulated to be represented by symbolic expressions in a “language-of-thought”, e.g. “a square is a four-sided figure with four equal sides and four right angles” or equivalently by a computer-like program from drawing them in a Logo-like language (Sablé-Meyer et al., 2022).”</p><p>Here, however, the present experiments do not directly probe this format of a representation. We have therefore simplified our wording and removed many of our use of the word “symbolic” in favor of the more specific “geometric features”.</p><disp-quote content-type="editor-comment"><p>Pigeons have remarkable visual systems. According to my fallible memory, Herrnstein investigated visual categories in pigeons. They can recognize individual people from fragments of photos, among other feats. I believe pigeons failed at geometric figures and also at cartoon drawings of things they could recognize in photos. This suggests they did not interpret line drawings of objects as representations of objects.</p></disp-quote><p>The comparison of geometric abilities across species is an interesting line of research. In the discussion, we briefly mention several lines of research that indicate that non-human primates do not perceive geometric shapes in the same way as we do – but for space reasons, we are reluctant to expand this section to a broader review of other more distant species. The referee is right that there is evidence of pigeons being able to perceive an invariant abstract 3D geometric shape in spite of much variation in viewpoint (Peissig et al., 2019) – but there does not seem to be evidence that they attend to geometric regularities specifically (e.g. squares versus non-squares). Also, the referee’s point bears on the somewhat different issue of whether humans and other animals may recognize the object depicted by a symbolic drawing (e.g. a sketch of a tree). Again, humans seem to be vastly superior in this domain, and research on this topic is currently ongoing in the lab. However, the point that we are making in the present work is specifically about the neural correlates of the representation of simple geometric shapes which by design were not intended to be interpretable as representations of objects.</p><disp-quote content-type="editor-comment"><p>Categories are established in part by contrast categories; are quadrilaterals, triangles, and circles different categories?</p></disp-quote><p>We are not sure how to interpret the referee’s question, since it bears on the definition of “category” (Spontaneous? After training? With what criterion?). While we are not aware of data that can unambiguously answer the reviewer’s question, categorical perception in geometric shapes can be inferred from early work investigating pop-out effects in visual search, e.g. (Treisman and Gormican, 1988): curvature appears to generate strong pop-out effects, and therefore we would expect e.g. circles to indeed be a different category than, say, triangles. Similarly, right angles, as well as parallel lines, have been found to be perceived categorically (Dillon et al., 2019).</p><p>This suggests that indeed squares would be perceived as categorically different from triangles and circles. On the other hand, in our own previous work (Sablé-Meyer et al., 2021) we have found that the deviants that we generated from our quadrilaterals did not pop out from displays of reference quadrilaterals. Pop-out is probably not the proper criterion for defining what a “category” is, but this is the extent to which we can provide an answer to the reviewer’s question.</p><disp-quote content-type="editor-comment"><p>It would be instructive to investigate stimuli that are on a continuum from representational to geometric, e.g., table tops or cartons under various projections, or balls or buildings that are rectangular or triangular. Building parts, inside and out. like corners. Objects differ from geometric forms in many ways: 3D rather than 2D, more complicated shapes, and internal texture. The geometric figures used are flat, 2-D, but much geometry is 3-D (e. g. cubes) with similar abstract features.</p></disp-quote><p>We agree that there is a whole line of potential research here. We decided to start by focusing on the simplest set of geometric shapes that would give us enough variation in geometric regularity while being easy to match on other visual features. We agree with the reviewer that our results should hold both for more complex 2-D shapes, but also for 3-D shapes. Indeed, generative theories of shapes in higher dimensions following similar principles as ours have been devised (I. Biederman, 1987; Leyton, 2003). We now mention this in the discussion:</p><p>“Finally, this research should ultimately be extended to the representation of 3-dimensional geometric shapes, for which similar symbolic generative models have indeed been proposed (Irving Biederman, 1987; Leyton, 2003).”</p><disp-quote content-type="editor-comment"><p>The feature space of geometry is more than parallelism and symmetry; angles are important, for example. Listing and testing features would be fascinating. Similarly, looking at younger or preferably non-Western children, as Western children are exposed to shapes in play at early ages.</p></disp-quote><p>We agree with the reviewer on all point. While we do not list and test the different properties separately in this work, we would like to highlight that angles are part of our geometric feature model, which includes features of “right-angle” and “equal-angles” as suggested by the reviewer.</p><p>We also agree about the importance of testing populations with limited exposure to formal training with geometric shapes. This was in fact a core aspect of a previous article of ours which tests both preschoolers, and adults with no access to formal western education – though no non-Western children (Sablé-Meyer et al., 2021). It remains a challenge to perform brain-imaging studies in non-Western populations (although see Dehaene et al., 2010; Pegado et al., 2014).</p><disp-quote content-type="editor-comment"><p>What in human experience but not the experience of close primates would drive the abstraction of these geometric properties? It's easy to make a case for elaborate brain processes for recognizing and distinguishing things in the world, shared by many species, but the case for brain areas sensitive to processing geometric figures is harder. The fact that these areas are active in blind mathematicians and that they are parietal areas suggests that what is important is spatial far more than visual. Could these geometric figures and their abstract properties be connected in some way to behavior, perhaps with fabrication and construction as well as use? Or with other interactions with complex objects and environments where symmetry and parallelism (and angles and curvature--and weight and size) would be important? Manual dexterity and fabrication also distinguish humans from great apes (quantitatively, not qualitatively), and action drives both visual and spatial representations of objects and spaces in the brain. I certainly wouldn't expect the authors to add research to this already packed paper, but raising some of the conceptual issues would contribute to the significance of the paper.</p></disp-quote><p>We refrained from speculating about this point in the previous version of the article, but share some of the reviewers’ intuitions about the underlying drive for geometric abstraction. As described in (Dehaene, 2026; Sablé-Meyer et al., 2022), our hypothesis, which isn’t tested in the present article, is that the emergence of a pervasive ability to represent aspects of the world as compact expressions in a mental “language-of-thought” is what underlies many domains of specific human competence, including some listed by the reviewer (tool construction, scene understanding) and our domain of study here, geometric shapes.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the Authors:</bold></p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>Overall, I enjoyed reading this paper. It is clearly written and nicely showcases the amount of work that has gone into conducting all these experiments and analyzing the data in sophisticated ways. I also thought the figures were great, and I liked the level of organization in the GitHub repository and am looking forward to seeing the shared data on OpenNeuro. I have some specific questions I hope the authors can address.</p><p>(1) Behavior</p><p>- Looking at Figure 1, it seemed like most shapes are clustering together, whereas square, rectangle, and maybe rhombus and parallelogram are slightly more unique. I was wondering whether the authors could comment on the potential influence of linguistic labels. Is it possible that it is easier to discard the intruder when the shapes are readily nameable versus not?</p></disp-quote><p>This is an interesting observation, but the existence of names for shapes does not suffice to explain all of our findings ; see our reply to the public comment.</p><disp-quote content-type="editor-comment"><p>(2) fMRI</p><p>- As mentioned in the public review, I was surprised that the authors went with an intruder task because I would imagine that performance depends on the specific combination of geometric shapes used within a trial. I assume it is much harder to find, for example, a &quot;Right Hinge&quot; embedded within &quot;Hinge&quot; stimuli than a &quot;Right Hinge&quot; amongst &quot;Squares&quot;. In addition, the rotation and scaling of each individual item should affect regular shapes less than irregular shapes, creating visual dissimilarities that would presumably make the task harder. Can the authors comment on how we can be sure that the differences we pick up in the parietal areas are not related to task difficulty but are truly related to geometric shape regularities?</p></disp-quote><p>Again, please see our public review response for a larger discussion of the impact of task difficulty. There are two aspects to answering this question.</p><p>First, the task is not as the reviewer describes: the intruder task is to find a deviant shape within several slightly rotated and scaled versions of the regular shape it came from. During brain imaging, we did not ask participants to find an exemplar of one of our reference shape amidst copies of another, but rather a deviant version of one shape against copies of its reference version. We only used this intruder task with all pairs of shapes to generate the behavioral RSA matrix.</p><p>Second, we agree that some of the fMRI effect may stem from task difficulty, and this motivated our use of RSA analysis in fMRI, and a passive MEG task. RSA results cannot be explained by task difficulty.</p><p>Overall, we have tried to make the limitations of the fMRI design, and the motivation for turning to passive presentation in MEG, clearer by stating the issues more clearly when we introduce experiment 4:</p><p>“The temporal resolution of fMRI does not allow to track the dynamic of mental representations over time. Furthermore, the previous fMRI experiment suffered from several limitations. First, we studied six quadrilaterals only, compared to 11 in our previous behavioral work. Second, we used an explicit intruder detection, which implies that the geometric regularity effect was correlated with task difficulty, and we cannot exclude that this factor alone explains some of the activations in figure 3C (although it is much less clear how task difficulty alone would explain the RSA results in figure 3D). Third, the long display duration, which was necessary for good task performance especially in children, afforded the possibility of eye movements, which were not monitored inside the 3T scanner and again could have affected the activations in figure 3C.”</p><disp-quote content-type="editor-comment"><p>- How far in the periphery were the stimuli presented? Was eye-tracking data collected for the intruder task? Similar to the point above, I would imagine that a harder trial would result in more eye movements to find the intruder, which could drive some of the differences observed here.</p></disp-quote><p>A 1-degree bar was added to Figure 3A, which faithfully illustrates how the stimuli were presented in fMRI. Eye-tracking data was not collected during fMRI. Although the participants were explicitly instructed to fixate at the center of the screen and avoid eye movements, we fully agree with the referee that we cannot exclude that eye movements were present, perhaps more so for more difficult displays, and would therefore have contributed to the observed fMRI activations in experiment 3 (figure 3C). We now mention this limitation explicity at the end of experiment 3. However, crucially, this potential problem cannot apply to the MEG data. During the MEG task, the stimuli were presented one by one at the center of screen, without any explicit task, thus avoiding issues of eye movements. We therefore consider the MEG geometrical regularity effect, which comes at a relatively early latency (starting at ~160 ms) and even in a passive task, to provide the strongest evidence of geometric coding, unaffected by potential eye movement artefacts.</p><disp-quote content-type="editor-comment"><p>- I was wondering whether the authors would consider showing some un-thresholded maps just to see how widespread the activation of the geometric shapes is across all of the cortex.</p></disp-quote><p>We share the uncorrected threshold maps in Fig. S3. for both adults and children in the category localizer, copied here as well. For the geometry task, most of the clusters identified are fairly big and survive cluster-corrected permutations; the uncorrected statistical maps look almost fully identical to the one presented in Fig. 3 (p&lt;.001 map).</p><disp-quote content-type="editor-comment"><p>- I'm missing some discussion on the role of early visual areas that goes beyond the RSA-CNN comparison. I would imagine that early visual areas are not only engaged due to top-down feedback (line 258) but may actually also encode some of the geometric features, such as parallel lines and symmetry. Is it feasible to look at early visual areas and examine what the similarity structure between different shapes looks like?</p></disp-quote><p>If early visual areas encoded the geometric features that we propose, then even early sensor-level RSA matrices should show a strong impact of geometric features similarity, which is not what we find (figure 4D). We do, however, appreciate the referee’s request to examine more closely how this similarity structure looks like. We now provide a movie showing the significant correlation between neural activity and our two models (uncorrected participants); indeed, while the early occipital activity (around 110ms) is dominated by a significant correlation with the CNN model, there are also scattered significant sources associated to the symbolic model around these timepoints already.</p><p>To test this further, we used beamformers to reconstruct the source-localized activity in calcarine cortex and performed an RSA analysis across that ROI. We find that indeed the CNN model is strongly significant at t=110ms (t=3.43, df=18, p=.003) while the geometric feature model is not (t=1.04, df=18, p=.31), and the CNN is significantly above the geometric feature model (t=4.25, df=18, p&lt;.001). However, this result is not very stable across time, and there are significant temporal clusters around these timepoints associated to each model, with no significant cluster associated to a CNN &gt; geometric (CNN: significant cluster from 88ms to 140ms, p&lt;.001 in permutation based with 10000 permutations; geometric features has a significant cluster from 80ms to 104ms, p=.0475; no significant cluster on the difference between the two).</p><disp-quote content-type="editor-comment"><p>(3) MEG</p><p>- Similar to the fMRI set, I am a little worried that task difficulty has an effect on the decoding results, as the oddball should pop out more in more geometric shapes, making it easier to detect and easier to decode. Can the authors comment on whether it would matter for the conclusions whether they are decoding varying task difficulty or differences in geometric regularity, or whether they think this can be considered similarly?</p></disp-quote><p>See above for an extensive discussion of the task difficulty effect. We point out that there is no task in the MEG data collection part. We have clarified the task design by updating our Fig. 4. Additionally, the fact that oddballs are more perceived more or less easily as a function of their geometric regularity is, in part, exactly the point that we are making – but, in MEG, even in the absence of a task of looking for them.</p><disp-quote content-type="editor-comment"><p>- The authors discuss that the inflated baseline/onset decoding/regression estimates may occur because the shapes are being repeated within a mini-block, which I think is unlikely given the long ISIs and the fact that the geometric features model is not &gt;0 at onset. I think their second possible explanation, that this may have to do with smoothing, is very possible. In the text, it said that for the non-smoothed result, the CNN encoding correlates with the data from 60ms, which makes a lot more sense. I would like to encourage the authors to provide readers with the unsmoothed beta values instead of the 100-ms smoothed version in the main plot to preserve the reason they chose to use MEG - for high temporal resolution!</p></disp-quote><p>We fully agree with the reviewer and have accordingly updated the figures to show the unsmoothed data (see below). Indeed, there is now no significant CNN effect before ~60 ms (up to the accuracy of identifying onsets with our method).</p><disp-quote content-type="editor-comment"><p>- In Figure 4C, I think it would be useful to either provide error bars or show variability across participants by plotting each participant's beta values. I think it would also be nice to plot the dissimilarity matrices based on the MEG data at select timepoints, just to see what the similarity structure is like.</p></disp-quote><p>Following the reviewer’s recommendation, we plot the timeseries with SEM as shaded area, and thicker lines for statistically significant clusters, and we provide the unsmoothed version in figure Fig. 4. As for the dissimilarity matrices at select timepoints, this has now been added to figure Fig. 4.</p><disp-quote content-type="editor-comment"><p>- To evaluate the source model reconstruction, I think the reader would need a little more detail on how it was done in the main text. How were the lead fields calculated? Which data was used to estimate the sources? How are the models correlated with the source data?</p></disp-quote><p>We have imported some of the details in the main text as follows (as well as expanding the methods section a little):</p><p>“To understand which brain areas generated these distinct patterns of activations, and probe whether they fit with our previous fMRI results, we performed a source reconstruction of our data. We projected the sensor activity onto each participant's cortical surfaces estimated from T1-images. The projection was performed using eLORETA and emptyroom recordings acquired on the same day to estimate noise covariance, with the default parameters of mne-bids-pipeline. Sources were spaced using a recursively subdivided octahedron (oct5). Group statistics were performed after alignement to fsaverage. We then replicated the RSA analysis […]”</p><disp-quote content-type="editor-comment"><p>- In addition to fitting the CNN, which is used here to model differences in early visual cortex, have the authors considered looking at their fMRI results and localizing early visual regions, extracting a similarity matrix, and correlating that with the MEG and/or comparing it with the CNN model?</p></disp-quote><p>We had ultimately decided against comparing the empirical similarity matrices from the MEG and fMRI experiments, first because the stimuli and tasks are different, and second because this would not be directly relevant to our goal, which is to evaluate whether a geometric-feature model accounts for the data. Thus, we systematically model empirical similarity matrices from fMRI and from MEG with our two models derived from different theories of shape perception in order to test predictions about their spatial and temporal dynamic. As for comparing the similarity matrix from early visual regions in fMRI with that predicted by the CNN model, this is effectively visible from our Fig. 3D where we perform searchlight RSA analysis and modeling with both the CNN and the geometric feature model; bilaterally, we find a correlation with the CNN model, although it sometimes overlap with predictions from the geometric feature model as well. We now include a section explaining this reasoning in appendix:</p><p>“Representational similarity analysis also offers a way to directly compared similarity matrices measured in MEG and fMRI, thus allowing for fusion of those two modalities and tentatively assigning a “time stamp” to distinct MRI clusters. However, we did not attempt such an analysis here for several reasons. First, distinct tasks and block structures were used in MEG and fMRI. Second, a smaller list of shapes was used in fMRI, as imposed by the slower modality of acquisition. Third, our study was designed as an attempt to sort out between two models of geometric shape recognition. We therefore focused all analyses on this goal, which could not have been achieved by direct MEG-fMRI fusion, but required correlation with independently obtained model predictions.”</p><disp-quote content-type="editor-comment"><p>Minor comments</p><p>- It's a little unclear from the abstract that there is children's data for fMRI only.</p></disp-quote><p>We have reworded the abstract to make this unambiguous</p><disp-quote content-type="editor-comment"><p>- Figures 4a &amp; b are missing y-labels.</p></disp-quote><p>We can see how our labels could be confused with (sub-)plot titles and have moved them to make the interpretation clearer.</p><disp-quote content-type="editor-comment"><p>- MEG: are the stimuli always shown in the same orientation and size?</p></disp-quote><p>They are not, each shape has a random orientation and scaling. On top of a task example at the top of Fig. 4, we have now included a clearer mention of this in the main text when we introduce the task:</p><p>“shapes were presented serially, one at a time, with small random changes in rotation and scaling parameters, in miniblocks with a fixed quadrilateral shape and with rare intruders with the bottom right corner shifted by a fixed amount (Sablé-Meyer et al., 2021)”</p><disp-quote content-type="editor-comment"><p>- To me, the discussion section felt a little lengthy, and I wonder whether it would benefit from being a little more streamlined, focused, and targeted. I found that the structure was a little difficult to follow as it went from describing the result by modality (behavior, fMRI, MEG) back to discussing mostly aspects of the fMRI findings.</p></disp-quote><p>We have tried to re-organize and streamline the discussion following these comments.</p><disp-quote content-type="editor-comment"><p>Then, later on, I found that especially the section on &quot;neurophysiological implementation of geometry&quot; went beyond the focus of the data presented in the paper and was comparatively long and speculative.</p></disp-quote><p>We have reexamined the discussion, but the citation of papers emphasizing a representation of non-accidental geometric properties in non-human animals was requested by other commentators on our article; and indeed, we think that they are relevant in the context of our prior suggestion that the composition of geometric features might be a uniquely human feature – these papers suggest that individual features may not, and that it is therefore compositionality which might be special to the human brain. We have nevertheless shortened it.</p><p>Furthermore, we think that this section is important because symbolic models are often criticized for lack of a plausible neurophysiological implementation. It is therefore important to discuss whether and how the postulated symbolic geometric code could be realized in neural circuits. We have added this justification to the introduction of this section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) If the authors want to specifically claim that their findings align with mathematical reasoning, they could at least show the overlap between the activation maps of the current study and those from prior work.</p></disp-quote><p>This was added to the fMRI results. See our answers to the public review.</p><disp-quote content-type="editor-comment"><p>(2) I wonder if the reason the authors only found aIPS in their first analysis (Figure 2) is because they are contrasting geometric shapes with figures that also have geometric properties. In other words, faces, objects, and houses also contain geometric shape information, and so the authors may have essentially contrasted out other areas that are sensitive to these features. One indication that this may be the case is that the geometric regularity effect and searchlight RSA (Figure 3) contains both anterior and posterior IPS regions (but crucially, little ventral activity). It might be interesting to discuss the implications of these differences.</p></disp-quote><p>Indeed, we cannot exclude that the few symmetries, perpendicularity and parallelism cues that can be presented in faces, objects or houses were processed as such, perhaps within the ventral pathway, and that these representations would have been subtracted out. We emphasize that our subtraction isolates the geometrical features that are present in simple regular geometric shapes, over and above those that might exist in other categories. We have added this point to the discussion:</p><p>“[…] For instance, faces possess a plane of quasi-symmetry, and so do many other man-made tools and houses. Thus, our subtraction isolated the geometrical features that are present in simple regular geometric shapes (e.g. parallels, right angles, equality of length) over and above those that might already exist, in a less pure form, in other categories.”</p><disp-quote content-type="editor-comment"><p>(3) I had a few questions regarding the MEG results.</p><p>a. I didn't quite understand the task. What is a regular or oddball shape in this context? It's not clear what is being decoded. Perhaps a small example of the MEG task in Figure 4 would help?</p></disp-quote><p>We now include an additional sub-figure in Fig. 4 to explain the paradigm. In brief: there is no explicit task, participants are simply asked to fixate. The shapes come in miniblocks of 30 identical reference shapes (up to rotation and scaling), among which some occasional deviant shapes randomly appear (created by moving the corner of the reference shape by some amount).</p><disp-quote content-type="editor-comment"><p>b. In Figure 4A/B they describe the correlation with a 'symbolic model'. Is this the same as the geometric model in 4C?</p></disp-quote><p>It is. We have removed this ambiguity by calling it “geometric model” and setting its color to the one associated to this model thought the article.</p><disp-quote content-type="editor-comment"><p>c. The author's explanation for why geometric feature coding was slower than CNN encoding doesn't quite make sense to me. As an explanation, they suggest that previous studies computed &quot;elementary features of location or motor affordance&quot;, whereas their study work examines &quot;high-level mathematical information of an abstract nature.&quot; However, looking at the studies the authors cite in this section, it seems that these studies also examined the time course of shape processing in the dorsal pathway, not &quot;elementary features of location or motor affordance.&quot; Second, it's not clear how the geometric feature model reflects high-level mathematical information (see point above about claiming this is related to math).</p></disp-quote><p>We thank the referee for pointing out this inappropriate phrase, which we removed. We rephrased the rest of the paragraph to clarify our hypothesis in the following way:</p><p>“However, in this work, we specifically probed the processing of geometric shapes that, if our hypothesis is correct, are represented as mental expressions that combine geometrical and arithmetic features of an abstract categorical nature, for instance representing “four equal sides” or “four right angles”. It seems logical that such expressions, combining number, angle and length information, take more time to be computed than the first wave of feedforward processing within the occipito-temporal visual pathway, and therefore only activate thereafter.”</p><disp-quote content-type="editor-comment"><p>One explanation may be that the authors' geometric shapes require finer-grained discrimination than the object categories used in prior studies. i.e., the odd-ball task may be more of a fine-grained visual discrimination task. Indeed, it may not be a surprise that one can decode the difference between, say, a hammer and a butterfly faster than two kinds of quadrilaterals.</p></disp-quote><p>We do not disagree with this intuition, although note that we do not have data on this point (we are reporting and modelling the MEG RSA matrix across geometric shapes only – in this part, no other shapes such as tools or faces are involved). Still, the difference between squares, rectangles, parallelograms and other geometric shapes in our stimuli is not so subtle. Furthermore, CNNs do make very fine grained distinctions, for instance between many different breeds of dogs in the IMAGENET corpus. Still, those sorts of distinctions capture the initial part of the MEG response, while the geometric model is needed only for the later part. Thus, we think that it is a genuine finding that geometric computations associated with the dorsal parietal pathway are slower than the image analysis performed by the ventral occipito-temporal pathway.</p><disp-quote content-type="editor-comment"><p>d. CNN encoding at time 0 is a little weird, but the author's explanation, that this is explained by the fact that temporal smoothed using a 100 ms window makes sense. However, smoothing by 100 ms is quite a lot, and it doesn't seem accurate to present continuous time course data when the decoding or RSA result at each time point reflects a 100 ms bin. It may be more accurate to simply show unsmoothed data. I'm less convinced by the explanation about shape prediction.</p></disp-quote><p>We agree. Following the reviewer’s advice, as well as the recommendation from reviewer 1, we now display unsmoothed plots, and the effects now exhibit a more reasonable timing (Figure 4D), with effects starting around ~60 ms for CNN encoding.</p><disp-quote content-type="editor-comment"><p>(4) I appreciate the author's use of multiple models and their explanation for why DINOv2 explains more variance than the geometric and CNN models (that it represents both types of features. A variance partitioning analysis may help strengthen this conclusion Bonner &amp; Epstein, 2018; Lescroart et al., 2015).</p><p>However, one difference between DINOv2 and the CNN used here is that it is trained on a dataset of 142 million images vs. the 1.5 million images used in ImageNet. Thus, DINOv2 is more likely to have been exposed to simple geometric shapes during training, whereas standard ImageNet trained models are not. Indeed, prior work has shown that lesioning line drawing-like images from such datasets drastically impairs the performance of large models (Mayilvahanan et al., 2024). Thus, it is unlikely that the use of a transformer architecture explains the performance of DINOv2. The authors could include an ImageNet-trained transformer (e.g., ViT) and a CNN trained on large datasets (e.g., ResNet trained on the Open Clip dataset) to test these possibilities. However, I think it's also sufficient to discuss visual experience as a possible explanation for the CNN and DINOv2 results. Indeed, young children are exposed to geometric shapes, whereas ImageNet-trained CNNs are not.</p></disp-quote><p>We agree with the reviewer’s observation. In fact, new and ongoing work from the lab is also exploring this; we have included in supplementary materials exactly what the reviewer is suggesting, namely the time course of the correlation with ViT and with ConvNeXT. In line with the reviewers’ prediction, these networks, trained on much larger dataset and with many more parameters, can also fit the human data as well as DINOv2. We ran additional analysis of the MEG data with ViT and ConvNeXT, which we now report in Fig. S6 as well as in an additional sentence in that section:</p><p>“[…] similar results were obtained by performing the same analysis, not only with another vision transformer network, ViT, but crucially using a much larger convolutional neural network, ConvNeXT, which comprises ~800M parameters and has been trained on 2B images, likely including many geometric shapes and human drawings. For the sake of completeness, RSA analysis in sensor space of the MEG data with these two models is provided in Fig. S6.”</p><p>We conclude that the size and nature of the training set could be as important as the architecture – but also note that humans do not rely on such a huge training set. We have updated the text, as well as Fig. S6, accordingly by updating the section now entitled “Vision Transformers and Larger Neural Networks”, and the discussion section on theoretical models.</p><disp-quote content-type="editor-comment"><p>(5) The authors may be interested in a recent paper from Arcaro and colleagues that showed that the parietal cortex is greatly expanded in humans (including infants) compared to non-human primates (Meyer et al., 2025), which may explain the stronger geometric reasoning abilities of humans.</p></disp-quote><p>A very interesting article indeed! We have updated our article to incorporate this reference in the discussion, in the section on visual pathways, as follows:</p><p>“Finally, recent work shows that within the visual cortex, the strongest relative difference in growth between human and non-human primates is localized in parietal areas (Meyer et al., 2025). If this expansion reflected the acquisition of new processing abilities in these regions, it might explain the observed differences in geometric abilities between human and non-human primates (Sablé-Meyer et al., 2021).”</p><disp-quote content-type="editor-comment"><p>Also, the authors may want to include this paper, which uses a similar oddity task and compelling shows that crows are sensitive to geometric regularity:</p><p>Schmidbauer, P., Hahn, M., &amp; Nieder, A. (2025). Crows recognize geometric regularity. Science Advances, 11(15), eadt3718. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/sciadv.adt3718">https://doi.org/10.1126/sciadv.adt3718</ext-link></p></disp-quote><p>We have ongoing discussions with the authors of this work and are have prepared a response to their findings (Sablé-Meyer and Dehaene, 2025)–ultimately, we think that this discussion, which we agree is important, does not have its place in the present article. They used a reduced version of our design, with amplified differences in the intruders. While they did not test the fit of their model with CNN or geometric feature models, we did and found that a simple CNN suffices to account for crow behavior. Thus, we disagree that their conclusions follow from their results and their conclusions. But the present article does not seem to be the right platform to engage in this discussion.</p><p>References</p><p>Ayzenberg, V., &amp; Behrmann, M. (2022). The Dorsal Visual Pathway Represents Object-Centered Spatial Relations for Object Recognition. The Journal of Neuroscience, 42(23), 4693-4710. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/jneurosci.2257-21.2022">https://doi.org/10.1523/jneurosci.2257-21.2022</ext-link></p><p>Bonner, M. F., &amp; Epstein, R. A. (2018). Computational mechanisms underlying cortical responses to the affordance properties of visual scenes. PLoS Computational Biology, 14(4), e1006111. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1006111">https://doi.org/10.1371/journal.pcbi.1006111</ext-link></p><p>Bueti, D., &amp; Walsh, V. (2009). The parietal cortex and the representation of time, space, number and other magnitudes. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1525), 1831-1840.</p><p>Dehaene, S., &amp; Brannon, E. (2011). Space, time and number in the brain: Searching for the foundations of mathematical thought. Academic Press.</p><p>Freud, E., Culham, J. C., Plaut, D. C., &amp; Bermann, M. (2017). The large-scale organization of shape processing in the ventral and dorsal pathways. eLife, 6, e27576.</p><p>Freud, E., Ganel, T., Shelef, I., Hammer, M. D., Avidan, G., &amp; Behrmann, M. (2017). Three-dimensional representations of objects in dorsal cortex are dissociable from those in ventral cortex. Cerebral Cortex, 27(1), 422-434.</p><p>Freud, E., Plaut, D. C., &amp; Behrmann, M. (2016). 'What 'is happening in the dorsal visual pathway. Trends in Cognitive Sciences, 20(10), 773-784.</p><p>Freud, E., Plaut, D. C., &amp; Behrmann, M. (2019). Protracted developmental trajectory of shape processing along the two visual pathways. Journal of Cognitive Neuroscience, 31(10), 1589-1597.</p><p>Han, Z., &amp; Sereno, A. (2022). Modeling the Ventral and Dorsal Cortical Visual Pathways Using Artificial Neural Networks. Neural Computation, 34(1), 138-171. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco_a_01456">https://doi.org/10.1162/neco_a_01456</ext-link></p><p>Janssen, P., Srivastava, S., Ombelet, S., &amp; Orban, G. A. (2008). Coding of shape and position in macaque lateral intraparietal area. Journal of Neuroscience, 28(26), 6679-6690.</p><p>Konen, C. S., &amp; Kastner, S. (2008). Two hierarchically organized neural systems for object information in human visual cortex. Nature Neuroscience, 11(2), 224-231.</p><p>Lescroart, M. D., Stansbury, D. E., &amp; Gallant, J. L. (2015). Fourier power, subjective distance, and object categories all provide plausible models of BOLD responses in scene-selective visual areas. Frontiers in Computational Neuroscience, 9(135), 1-20. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2015.00135">https://doi.org/10.3389/fncom.2015.00135</ext-link></p><p>Mayilvahanan, P., Zimmermann, R. S., Wiedemer, T., Rusak, E., Juhos, A., Bethge, M., &amp; Brendel, W. (2024). In search of forgotten domain generalization. arXiv Preprint arXiv:2410.08258.</p><p>Meyer, E. E., Martynek, M., Kastner, S., Livingstone, M. S., &amp; Arcaro, M. J. (2025). Expansion of a conserved architecture drives the evolution of the primate visual cortex. Proceedings of the National Academy of Sciences, 122(3), e2421585122. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.2421585122">https://doi.org/10.1073/pnas.2421585122</ext-link></p><p>Orban, G. A. (2011). The extraction of 3D shape in the visual system of human and nonhuman primates. Annual Review of Neuroscience, 34, 361-388.</p><p>Romei, V., Driver, J., Schyns, P. G., &amp; Thut, G. (2011). Rhythmic TMS over Parietal Cortex Links Distinct Brain Frequencies to Global versus Local Visual Processing. Current Biology, 21(4), 334-337. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2011.01.035">https://doi.org/10.1016/j.cub.2011.01.035</ext-link></p><p>Sereno, A. B., &amp; Maunsell, J. H. R. (1998). Shape selectivity in primate lateral intraparietal cortex. Nature, 395(6701), 500-503. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/26752">https://doi.org/10.1038/26752</ext-link></p><p>Summerfield, C., Luyckx, F., &amp; Sheahan, H. (2020). Structure learning and the posterior parietal cortex. Progress in Neurobiology, 184, 101717. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pneurobio.2019.101717">https://doi.org/10.1016/j.pneurobio.2019.101717</ext-link></p><p>Van Dromme, I. C., Premereur, E., Verhoef, B.-E., Vanduffel, W., &amp; Janssen, P. (2016). Posterior Parietal Cortex Drives Inferotemporal Activations During Three-Dimensional Object Vision. PLoS Biology, 14(4), e1002445. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002445">https://doi.org/10.1371/journal.pbio.1002445</ext-link></p><p>Xu, Y. (2018). A tale of two visual systems: Invariant and adaptive visual information representations in the primate brain. Annu. Rev. Vis. Sci, 4, 311-336.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>Bring into the discussion some of the issues outlined above, especially (a) the spatial rather than visual of the geometric figures and (b) the non-representational aspects of geometric form aspects.</p></disp-quote><p>We thank the reviewer for their recommendations – see our response to the public review for more details.</p></body></sub-article></article>