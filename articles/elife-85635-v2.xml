<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">85635</article-id><article-id pub-id-type="doi">10.7554/eLife.85635</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Evaluating hippocampal replay without a ground truth</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Takigawa</surname><given-names>Masahiro</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0162-9017</contrib-id><email>Masahiro.takigawa.17@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Huelin Gorriz</surname><given-names>Marta</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0281-0627</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tirole</surname><given-names>Margot</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0674-6690</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Bendor</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6621-793X</contrib-id><email>d.bendor@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Institute of Behavioural Neuroscience (IBN), University College London (UCL)</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>11</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e85635</elocation-id><history><date date-type="received" iso-8601-date="2022-12-16"><day>16</day><month>12</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-11-26"><day>26</day><month>11</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-12-13"><day>13</day><month>12</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.12.12.520040"/></event></pub-history><permissions><copyright-statement>© 2024, Takigawa et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Takigawa et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-85635-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-85635-figures-v2.pdf"/><abstract><p>During rest and sleep, memory traces replay in the brain. The dialogue between brain regions during replay is thought to stabilize labile memory traces for long-term storage. However, because replay is an internally driven, spontaneous phenomenon, it does not have a ground truth - an external reference that can validate whether a memory has truly been replayed. Instead, replay detection is based on the similarity between the sequential neural activity comprising the replay event and the corresponding template of neural activity generated during active locomotion. If the statistical likelihood of observing such a match by chance is sufficiently low, the candidate replay event is inferred to be replaying that specific memory. However, without the ability to evaluate whether replay detection methods are successfully detecting true events and correctly rejecting non-events, the evaluation and comparison of different replay methods is challenging. To circumvent this problem, we present a new framework for evaluating replay, tested using hippocampal neural recordings from rats exploring two novel linear tracks. Using this two-track paradigm, our framework selects replay events based on their temporal fidelity (sequence-based detection), and evaluates the detection performance using each event’s track discriminability, where sequenceless decoding across both tracks is used to quantify whether the track replaying is also the most likely track being reactivated.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>hippocampus</kwd><kwd>replay</kwd><kwd>sleep</kwd><kwd>memory</kwd><kwd>place cell</kwd><kwd>neural decoding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>Graduate student scholarship MR/N013867/1</award-id><principal-award-recipient><name><surname>Takigawa</surname><given-names>Masahiro</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Starter Grant CHIME</award-id><principal-award-recipient><name><surname>Bendor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>Young Investigator Award RGY0067/2016</award-id><principal-award-recipient><name><surname>Bendor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>Research Grant BB/T005475/1</award-id><principal-award-recipient><name><surname>Bendor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel tool enables cross-checking the quality of replay events and evaluating the effectiveness of a given replay detection method in the absence of a ground truth.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The hippocampus plays a central role in the encoding and consolidation of new memories (<xref ref-type="bibr" rid="bib11">Eichenbaum, 2000</xref>; <xref ref-type="bibr" rid="bib14">Frankland and Bontempi, 2005</xref>; <xref ref-type="bibr" rid="bib21">Klinzing et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Lewis and Bendor, 2019</xref>; <xref ref-type="bibr" rid="bib32">Squire, 1992</xref>). During locomotion in rodents, hippocampal place cells are active in specific regions of the animal’s environment (place fields), resulting in a sequential pattern of place cell firing when the animal runs along a spatial trajectory (<xref ref-type="bibr" rid="bib25">O’Keefe and Dostrovsky, 1971</xref>). During offline states, such as quiet restfulness and non-REM sleep, the sequential pattern of neural activity observed during behavior is spontaneously reactivated, a phenomenon referred to as ‘hippocampal replay’ (<xref ref-type="bibr" rid="bib22">Lee and Wilson, 2002</xref>; <xref ref-type="bibr" rid="bib40">Wilson and McNaughton, 1994</xref>). Offline replay of a neural memory trace is postulated to be a central mechanism by which memories recently encoded in the hippocampus can be consolidated by further stabilizing these memories in distributed cortico-hippocampal circuits for long-term storage (<xref ref-type="bibr" rid="bib4">Buzsáki, 1989</xref>; <xref ref-type="bibr" rid="bib21">Klinzing et al., 2019</xref>).</p><p>Evidence of replay was first discovered almost 30 years ago with the demonstration that during sleep, temporal correlations can be observed between the co-firing of place cell pairs with overlapping place fields (<xref ref-type="bibr" rid="bib40">Wilson and McNaughton, 1994</xref>). Since then, methods for large-scale chronic extracellular neural recordings have become more advanced, allowing the simultaneous recording of tens and even hundreds of neurons (<xref ref-type="bibr" rid="bib19">Ji and Wilson, 2007</xref>; <xref ref-type="bibr" rid="bib22">Lee and Wilson, 2002</xref>; <xref ref-type="bibr" rid="bib30">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib39">Widloski and Foster, 2022</xref>). In line with these developments, the analysis methods for replay have also become more sophisticated - shifting from the pairwise analysis of place cells to detecting sequential patterns within neuronal ensembles, using either spiking activity directly (<xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>) or decoding this activity to extrapolate the virtual spatial trajectories replaying (<xref ref-type="bibr" rid="bib8">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="bib42">Zhang et al., 1998</xref>). Commonly used replay scoring metrics for quantifying the fidelity of a replay sequence include: (1) a <italic>Spearman’s rank-order correlation</italic> of spike times, which quantifies the ordinal relationship between the temporal order of place cell firing during behavior and a replay event’s spike train (<xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>), but assumes that the place cell sequence is ordered accordingly to each place cell’s peak firing rate location alone, (2) a <italic>weighted correlation</italic> of the decoded replay event, which quantifies a generalized linear correlation in time and position weighted by the decoded posterior probabilities without any assumption about the temporal rigidity of the replayed trajectory (<xref ref-type="bibr" rid="bib17">Grosmark and Buzsáki, 2016</xref>; <xref ref-type="bibr" rid="bib31">Silva et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>), and (3) a <italic>linear fitting</italic> of the decoded replay event, which finds the linear path with the maximum summed decoded probability, assuming that the trajectory’s slope is constant (<xref ref-type="bibr" rid="bib8">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="bib16">Gomperts et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Ólafsdóttir et al., 2017</xref>). However, because replay is generated by an internal and spontaneous state of the brain, there is no external reference to indicate whether a given replay event is truly a reinstatement of a memory trace. Without a ground truth, the detection of replay events must be inferred based on whether the statistical likelihood of observing a match between the sequential structure of the replayed event and the original behavioral template is sufficiently low, by chance. To quantify this, each event’s replay score is compared to a distribution of scores obtained using randomized data, permutated in either the spatial or temporal domain (i.e. a shuffled distribution), where statistically significant replay scores must be greater than a certain percentage of this shuffled distribution, typically 95% for an alpha level (<italic>α</italic>)&lt;0.05.</p><p>While recent replay studies have relied predominately on these three major methods of scoring replay (<italic>rank-order</italic>, <italic>weighted correlation</italic>, or <italic>linear fit</italic>), there are still many variations in how these scores can be calculated and how the subsequent statistical significance of these scores are measured. This can lead to issues in reproducibility and a greater difficulty in interpreting conflicting results between studies (<xref ref-type="bibr" rid="bib34">Tingley and Peyrache, 2020</xref>). To overcome this problem, we need the ability to <italic>cross-check</italic> replay events (i.e. are the real events being correctly detected, and non-events being correctly rejected), in spite of not having a ground truth (<xref ref-type="bibr" rid="bib34">Tingley and Peyrache, 2020</xref>; <xref ref-type="bibr" rid="bib37">van der Meer et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">van der Meer et al., 2020</xref>). Hypothetically, any method of cross-checking should at the very least be able to pass a basic test of distinguishing between replay events detected from real data and spurious replay events detected from randomized data.</p><p>Given that we are detecting a replay sequence using a replay score (and its significance level <italic>α</italic>), we cannot use similar metrics for cross-checking due to a lack of independence. Here, we solve this problem by evaluating replay sequence events using a sequenceless decoding approach, which has been used in several recent replay studies when the rat’s behavior involved running different trajectories, either on a T-maze with two arms or on two different linear tracks (<xref ref-type="bibr" rid="bib6">Carey et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). This framework quantifies how well the sequence-based replay detector discriminates between two track-specific sequences, with a greater difference in summed trajectory likelihoods indicating a higher discriminability.</p><p>Several underlying assumptions are required for this framework: (1) for a given replay detection method, the proportion of spurious events (generated from randomized data) that are detected as significant events can be used to empirically estimate the proportion of non-replay events that are falsely labeled as significant replay events in real hippocampal data. However, this assumes that the null distribution used for creating randomized data is not used for detection, and that it does not underestimate the false-positive rates compared to the other null distribution(s) used for detection, (2) for neural data, pooled from multiple animals, with track-specific replay sequences but not spurious replay events, track discriminability should correlate with the sequenceness score of the track-specific replay, and (3) the empirically estimated false-positive rate can vary between methods, but can be adjusted by a scaling correction of the alpha level such that the performance of different replay methods can be evaluated and compared at an equivalent empirically estimated false-positive rate. Note that this method of scaling the alpha level is designed for method comparison only and should not be meant as a substitute for using appropriate shuffle methods to create a sufficient set of null distributions for detection.</p><p>Based on replay data pooled from five rats running on two novel tracks, we validate this framework and demonstrate how measures of sequence fidelity combined with measures of sequenceless track discriminability provide the means to evaluate and compare the performance of different sequence-based replay detection strategies and methods.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Extracellular signals from the dorsal CA1 region of the hippocampus were recorded in rats running back and forth on two novel linear tracks (male Lister-hooded, <italic>n</italic>=5, 10 sessions, dataset from <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). In addition to running on two linear tracks (RUN), rats also had a rest/sleep session in a remote location both before (PRE) and afterward (POST) (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To demonstrate how this framework quantifies replay detection performance in terms of dataset-specific detection rate, false-positive rate, and track discriminability, we started by detecting replay events using the weighted correlation of the decoded event, a common sequence-based replay detection approach (<xref ref-type="bibr" rid="bib17">Grosmark and Buzsáki, 2016</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Demonstration of novel replay analysis framework for comparing sequence fidelity with track discriminability.</title><p>(<bold>A</bold>) Experimental design. For each recording session, the animal ran back and forth on two novel linear tracks (RUN) with resting sessions before (PRE) and afterward (POST). Rat schematic in <bold>A</bold> was adapted with permission from <ext-link ext-link-type="uri" xlink:href="https://scidraw.io/">SciDraw.io</ext-link> (<xref ref-type="bibr" rid="bib2">Asiminas, 2020b</xref>; <xref ref-type="bibr" rid="bib1">Asiminas, 2020a</xref>). (<bold>B–E</bold>) Schematic of sequence-based and sequenceless decoding framework. (<bold>B</bold>) Each candidate replay event spike train was fed into a naïve Bayesian decoder to calculate the decoded posterior probabilities across time and space. (<bold>C</bold>) Then, for the sequence-based analysis, the sequence score for each candidate event was determined from the weighted correlation of the posterior probability matrix. Significance was determined by comparing the replay score relative to a shuffled distribution (alpha level = 0.05). (<bold>D</bold>) For sequenceless decoding, a Bayesian decoding similar to sequence-based approach was used, with the exception that only place cells with stable place fields on both tracks were used as template to avoid any track discrimination bias. Then, the logarithmic ratio of the summed posterior probabilities within each replay event for each track is calculated (log odds). The event log odds were z-scored relative to a shuffled distribution where each place cell’s track 1 and 2 place fields were randomly shuffled between tracks. (<bold>E</bold>) The difference between track 1 and track 2 replay events’ log odds can be used as a metric to cross-check the performance of sequence-based replay detection. (<bold>F,G</bold>) The relationship between the mean log odds difference and mean replay score for the significant events detected at different alpha levels (0.2–0.001) using a weighted correlation replay scoring with two different shuffling procedures (place field circular shuffle and time bin permutation shuffle). The shaded region indicates the 95% bootstrap confidence interval for the mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>F</bold>) Significant events detected during RUN using original candidate events (blue) and cell-id randomized spurious events (gray). (<bold>G</bold>) Significant events detected during POST using original candidate events (orange) and cell-id randomized spurious events (gray). (<bold>H</bold>) The relationship between the mean log odds difference and the proportion of significant events detected at different alpha levels (0.2–0.001) during RUN (blue) and POST (orange).The shaded region indicates the 95% bootstrap confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>I</bold>) The mean proportion of significant cell-id randomized events (mean false-positive rate) at different alpha levels (0.2–0.001). The error bar indicates the 95% bootstrap confidence interval for mean false-positive rates. (<bold>J</bold>) The replay detection performance at the original alpha level = 0.05 and the FPR-matched alpha level when the mean false-positive rate was 5%. The shaded box indicates a 95% bootstrap confidence interval for both proportion of significant events detected and mean log odds difference. The box with a light outline represents the values at an alpha level = 0.05 and the box with black outline represents the values at the FPR-matched alpha level. (Number of candidate replay events: RUN n = 4643 and POST n = 15283). The 95% confidence interval for the proportion of significant events, mean log odds difference, mean false-positive rates, and the FPR-matched alpha level for replay events detected during RUN and POST are available in <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Summary of replay detection performance for weighted correlation method with two shuffles.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig1-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Individual examples of decoded trajectories with different sequence fidelity and reactivation bias.</title><p>Each panel is the posterior probabilities of a candidate event decoded against track 1 template (left) or track 2 template (right), with the y axis representing position and the x axis representing time (each pixel corresponds to 20 cm and 20 ms in space and time). The four panels are arranged such that the events on the bottom left have low track 1 reactivation bias and low sequence fidelity and top right have high track 1 reactivation bias and high sequence fidelity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Mean log odds difference and proportion of significant events detected hold across 10 sessions.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at (<bold>A</bold>) an alpha level of 0.05 and (<bold>B</bold>) an FPR-matched alpha level with a mean false-positive rate of 5%.</p><p>The shaded box indicated 95% bootstrap confidence interval. Different colors are used to indicate different behavioral states: RUN (blue) and POST (orange). Different symbols are used to indicate different sessions. (Number of candidate replay events for sessions 1-10 : RUN n = 579, 559, 500, 388, 483, 400, 466, 459, 388 and 421 and POST n = 778, 1228, 969, 1770, 1578, 1986, 1793, 1687, 1278 and 1259, respectively)</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Schematics of shuffling procedures performed to obtain four null distributions for replay detection.</title><p>(<bold>A</bold>) A spike train circular shuffle (dark blue) in which each cell’s spike train was independently circularly shifted in time by a random amount within each replay event. (<bold>B</bold>) A place field circular shuffle (light blue) in which each cell’s ratemap was circularly shifted in space by a random amount. (<bold>C</bold>) A place bin circular shuffle (orange) in which the posterior probability distribution for each time bin was independently circularly shifted by a random amount. (<bold>D</bold>) A time bin permutation shuffle (red) in which the order of the time bin was permuted randomly.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig1-figsupp3-v2.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Proportion of false-positive events detected when using different shuffle methods and randomized datasets for a null distribution.</title><p>The detection during RUN and POST was based on single shuffle - (<bold>A</bold>) time bin permutation shuffle, (<bold>B</bold>) spike train circular shift shuffle, (<bold>C</bold>) place field circular shift shuffle, and (<bold>D</bold>) place bin circular shift shuffle. Methods for data randomization included - within experiment cell-id randomization (blue squares), and cross experiment cell-id randomization (orange squares), place field randomization (red squares) and spike train randomization (purple squares).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig1-figsupp4-v2.tif"/></fig></fig-group><p>Candidate replay events were population burst events (peak z-scored multi-unit activity [MUA]&gt;3) that occurred during periods of inactivity, when the animal’s velocity is less than 5 cm/s (see Methods). Across 10 sessions, we detected 8485, 4643, and 14,326 candidate events in PRE, RUN, and POST session, respectively. Detected replay events were required to also have a ripple power z-score greater than 3 and a significant replay score - i.e., higher than 95% of the scores obtained from each of two shuffle distributions, namely a place field circular shuffle and a time bin permutation shuffle (see Methods) (<xref ref-type="bibr" rid="bib17">Grosmark and Buzsáki, 2016</xref>). The replay score was obtained by performing a weighted correlation on the posterior probabilities (across place and time) obtained using a naïve Bayesian decoder (<xref ref-type="fig" rid="fig1">Figure 1B and C</xref>). Because the dataset includes more than one track, the posterior probabilities across the two available tracks were normalized at each time bin such that their combined sum was one (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; <xref ref-type="bibr" rid="bib3">Bendor and Wilson, 2012</xref>; <xref ref-type="bibr" rid="bib6">Carey et al., 2019</xref>).</p><sec id="s2-1"><title>Demonstrating a novel framework for cross-checking replay detection performance</title><p>We next developed a framework for evaluating and comparing replay detection methods, using a comparison between a replay event’s sequence fidelity and its track discriminability. For each replay event detected using a sequence-based detection approach, we also quantified the log odds of reactivation between tracks, a sequenceless metric <italic>based on only place cells with place fields on both tracks,</italic> to avoid any potential bias in track discrimination (<xref ref-type="fig" rid="fig1">Figure 1D and E</xref>, see Methods) (<xref ref-type="bibr" rid="bib6">Carey et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). Sequenceless decoding involved three steps: (1) computing the summed posteriors (across time and space) within the replay event for each track, (2) calculating log of the ratio between the summed posteriors for each track, and (3) taking the z-score of this value, based on a distribution of log odds computed by a track ID shuffle (<xref ref-type="bibr" rid="bib6">Carey et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). For each place cell in the track ID shuffle, the corresponding track 1 and 2 place fields were randomly assigned to the correct track or swapped. As a result, a more positive z-scored log odds would indicate a greater likelihood of track 1 reactivation whereas a more negative value would indicate a greater likelihood of track 2 reactivation (<xref ref-type="fig" rid="fig1">Figure 1E</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). To use sequenceless decoding to evaluate replay events, we computed the <italic>difference</italic> in mean log odds between track 1 and track 2 replay events across all sessions, which were originally detected using standard sequence-based replay detection methods. For this measurement, a more positive mean log odds difference would indicate a higher track discriminability in the replay content, and a higher confidence in replay quality. In contrast, a mean log odds difference of 0 would suggest that the quality of replay events is comparable to chance level track discriminability, and that the detected event population most likely consists of low fidelity events, indistinguishable from false-positives.</p><p>As mentioned earlier, any method of cross-checking should at least be able to pass a basic test of distinguishing between replay events detected from real data and spurious replay events detected from randomized data. We first compared the mean log-odds difference and mean weighted correlation score for replay events detected using our dataset and spurious replay events detected after the same dataset was randomized using a cell-id shuffle. Note that all sequence-based replay detection methods in this study did not use a cell-id shuffle, and as such this shuffling approach was sufficiently independent for generating random sequences as a negative control. When the alpha level, the significance level for detecting replay events, was varied from 0.2 to 0.001, we observed a close relationship between mean weighted correlation score and alpha level for both the original and randomized dataset (<xref ref-type="fig" rid="fig1">Figure 1F and G</xref>). However, there was a clear dissociation between the alpha level and mean log odds difference when using a randomized dataset, for both RUN and POST replay events. This demonstrated that while the weighted correlation score used for replay detection improved with alpha level even for the randomized dataset, the mean log odds difference (track discriminability) was able to still differentiate real from spurious replay events, acting independently from our selection criteria of replay events based on sequence fidelity.</p><p>While an alpha level of 0.05 is usually the chosen threshold for whether a statistical test rejects the null hypothesis, we predicted that as the alpha level became stricter, the mean log odds difference should increase due to a lower rate of false-positive events, albeit at the cost of also fewer detected replay events overall. To study the trade-off between the track discriminability and detection rate of detected replay events, we first analyzed our POST and RUN replay events, comparing the number of significant events detected on both tracks to the mean log odds difference, as the alpha level was adjusted between 0.2 and 0.001 (<xref ref-type="fig" rid="fig1">Figure 1H</xref>). For both POST and RUN, the mean log odds difference increased (higher track discriminability) as the alpha level threshold decreased, which also corresponded to a decrease in the number of detected events. In addition, we observed RUN replay events to be comparatively more prevalent and yielding a higher track discriminability than POST replay events, in line with previous reports (<xref ref-type="bibr" rid="bib20">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). This might be partly due to the presence of immediate sensory or other external inputs helping to direct hippocampal place cell ensembles toward representing the local current environment during awake replay.</p><p>To determine if this decrease in the detection rate was also associated with a decrease in the false-positive rate, we empirically measured the mean fraction of spurious replay events detected across both tracks after the dataset was randomized (by permuting the cell-id of each place cell) as a proxy for false-positive rate. We observed that as the alpha level was gradually reduced, so was the estimated false-positive rate using cell-id randomization (<xref ref-type="fig" rid="fig1">Figure 1I</xref>). We also found that the estimated false-positive rate was higher than the alpha level: using an alpha level of 0.05, 7.5% (lower CI = 7.2% and higher CI = 7.8%) of RUN replay events were false-positives, while 8.7% (lower CI = 8.5% and higher CI = 8.9%) of POST replay events were false-positives. Using this information, we could quantify the proportion of significant events for both tracks using sequence-based detection, and the corresponding mean log odds difference at the original alpha level of 0.05 and an FPR (false-positive rate)-matched alpha level (RUN: <italic>α</italic> = 0.032, POST: <italic>α</italic> = 0.028) that adjusted the estimated mean false-positive rate to approximately 5% (<xref ref-type="fig" rid="fig1">Figure 1J</xref>). These results were generally consistent when applied to individual sessions (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Such adjustment of the alpha level based on a matching false-positive rate is an important part of this novel framework as it provides an opportunity to compare different replay detection strategies and methods in a more equitable manner.</p><p>However, since most replay detection methods rely on comparing each event’s sequence score to one or more null distributions, it is important to make sure that cell-id randomization was not underestimating the false-positive rate of replay detection in a way that was biased toward specific null distributions used for detection. Therefore, we examined the influence of data randomization on empirically estimated false-positive rates by comparing four sets of randomized data: (1) cell-id randomization, (2) spike train circular shifted dataset, (3) place field circular shifted dataset, and (4) a cross-experiment shuffled dataset (where place fields from a different recording session were randomly assigned to each place cell during Bayesian decoding). We next performed replay detection using a weighted correlation with four single shuffle methods including two pre-decoding shuffles (spike train circular shuffle, place field circular shuffle) and two post-decoding shuffles (place bin circular shuffle, time bin permutation shuffle) (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). In most cases, both the cell-id randomized dataset and cross-experiment-shuffled dataset led to highest empirically estimated false-positive rates (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). However, using a shuffling procedure during detection that randomized the data along the same dimension (time or place) as the randomized dataset led to a substantial underestimation of the empirically estimated false-positive rate (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). Therefore, the cell-id randomization approach (and alternatively the cross-experiment shuffling approach) were the more optimal methods to create a randomized dataset for empirically estimating false-positive rates as the null distribution was not directly used in replay detection and was not underestimating the false-positive rates compared to alternative methods of data randomization.</p><p>We next used our framework to analyze and compare different strategies for replay detection. We did not attempt to precisely replicate any specific published replay detection method, as our goal was to see how general methodological differences can impact the track discriminability and quantity of detected events, and whether there is a preferred general approach for replay analysis. While it is important to note that there are many subtler, data-specific details to potentially consider (e.g. number of cells, smoothing of data, bin size, etc.) that can impact both the estimated false-positive rate and quality of detected replay events, our aim was to provide an overarching framework to help identify the more optimal replay detection approach.</p></sec><sec id="s2-2"><title>Ripple power is an important criterion for replay event selection during POST</title><p>During both quiet wakefulness and non-REM sleep, replay events preferentially occur during sharp-wave ripple events, transient high-frequency oscillations (150–250 Hz) in the local field potential (LFP) recorded near the cell layer of CA1 (<xref ref-type="bibr" rid="bib5">Buzsáki, 2015</xref>; <xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>; <xref ref-type="bibr" rid="bib22">Lee and Wilson, 2002</xref>). As such, a minimum ripple power has been used as a criterion for detecting candidate replay events prior to sequence detection, however the threshold has varied substantially across previous studies ranging from 2 SD (standard deviations) above baseline (<xref ref-type="bibr" rid="bib9">Diba and Buzsáki, 2007</xref>; <xref ref-type="bibr" rid="bib15">Gillespie et al., 2021</xref>) to 7 SD above baseline (<xref ref-type="bibr" rid="bib7">Csicsvari et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Ji and Wilson, 2007</xref>). However, because many replay events consist of the spontaneous reactivation of a large proportion of place cells within a short time window (more than typically occurs during active behavior), a substantial increase in MUA during immobility also provides a reasonable criterion for detecting candidate replay events (<xref ref-type="bibr" rid="bib8">Davidson et al., 2009</xref>). Because using only MUA as a threshold typically leads to more candidate replay events, and is believed to be more reliable for detecting both the start and end times of the replay event, many recent studies have opted to no longer use minimum ripple power as a criterion for candidate replay events (<xref ref-type="bibr" rid="bib16">Gomperts et al., 2015</xref>; <xref ref-type="bibr" rid="bib29">Ólafsdóttir et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Ólafsdóttir et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Silva et al., 2015</xref>).</p><p>We next applied our replay analysis framework to test how a stricter criterion for ripple power affects track discriminability. Candidate events were selected based on both elevated MUA (z-score&gt;3) and ripple power <italic>limited to a specific range</italic>, measured in SD above baseline (i.e. a z-score of 0–3, 3–5, 5–10, or &gt;10). For equivalent alpha levels, a stricter ripple threshold resulted in both a higher proportion of significant events and a higher mean log odds difference (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). The increase in track discriminability observed with higher ripple power was not associated with a change in the number of active place cells or total place cell spiking activity (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). However, one major difference between RUN and POST replay events was that even at the lowest ripple power threshold, RUN replay events had a non-zero mean log odds difference, which increased with stricter alpha levels. In contrast for POST replay events with a ripple power less than a z-score of 5, the mean log odds difference was near zero for all alpha levels tested. Furthermore, while the proportion of detected POST events with a ripple power less than a z-score of 5 was approximately 20% at an alpha level of 0.05, the proportion of detected POST events dropped to near chance levels of 10% when the alpha level was adjusted to match the mean false-positive rate of 5% (where 10% of spurious events were detected as significant events across both tracks) (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>, see Methods). For ripple thresholds above 5, the log odds difference increased as the proportion of events and the alpha level decreased (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>). Similar results were observed if stricter criteria were used in detecting candidate replay events - either lowering the speed threshold criterion (&lt;1 cm/s) or imposing an additional requirement for low theta power (z-score&lt;0) (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), suggesting that our finding was not confounded by the presence of other neuronal sequences during movement such as theta sequences. Next, we tested how shifting the ripple threshold (between 0 and 10 SD above baseline) could impact overall detection rates (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), calculating the proportion of significant events out of all of the candidate events based on MUA criteria alone. Consistent with our main finding, while the proportion of significant events dropped as a result of ripple thresholding, only replay during POST but not RUN improved its log odds difference with a stricter ripple threshold. These results emphasize that a ripple power threshold is not necessary for RUN replay events in our dataset but may still be beneficial, as long as it does not reject too many good replay events with low ripple power. In other words, depending on the experimental design, it is possible that a stricter alpha level with no ripple threshold can be used to detect more replay events than using a less strict alpha level combined with a strict ripple power threshold. However, for POST replay events, a threshold at least in the range of a z-score of 3–5 is recommended based on our dataset, to reduce inclusion of potential false-positives within the pool of detected replay events. For the remainder of this study, we incorporated a strict, albeit less conservative ripple power threshold (z-score&gt;3) for both RUN and POST, given the trade-off between an improved track discriminability and a decrease in the number of detected events. This also allowed a more direct comparison of awake and rest replay using identical criteria while still being consistent with previous replay studies (<xref ref-type="bibr" rid="bib20">Karlsson and Frank, 2009</xref>; <xref ref-type="bibr" rid="bib30">Pfeiffer and Foster, 2013</xref>; <xref ref-type="bibr" rid="bib36">Todorova and Zugaro, 2019</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Replay detection performance improves with ripple power.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) as ripple power increases (0–3, 3–5, 5–10, 10, and above). The shaded region indicates the 95% bootstrapped confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (<bold>C,D</bold>) The proportion of significant events and mean log odds differences at (<bold>C</bold>) an alpha level = 0.05 and (<bold>D</bold>) an FPR-matched alpha level with a mean false-positive rate of 5%. The shaded box indicates a 95% bootstrap confidence interval for both the proportion of significant events detected and mean log odds difference. The triangle symbol is used to represent replay events during RUN and the square symbol is used to represent replay events during POST. The dashed line represents the approximate chance level at mean false-positive rate of 5%. (Number of candidate replay events for ripple range 0-3, 3-5, 5-10 and 10 and above: RUN n = 782,1091,1982 and 788 and POST n = 1667, 2136, 4904 and 5619, respectively). The 95% confidence interval for the proportion of significant events, mean log odds difference, mean false-positive rates, and the FPR-matched alpha level for replay events with different ripple power range are available in <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Summary of replay detection performance at different ripple power thresholds.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig2-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>The replay event distribution, number of active place cells, and total spikes during replay event at different ripple powers.</title><p>(<bold>A</bold>) The distribution of candidate replay events across different ripple powers. (<bold>B</bold>) The proportion of significant events (out of all candidate events) at p-value≤0.05 at different ripple powers. (<bold>C</bold>) The mean number of active place cells during significant replay events. The shaded region indicated the standard deviation of the number of active place cells at each ripple power range. (<bold>D</bold>) The mean number of spikes fired by the active place cells during significant replay events. The shaded region indicated the standard deviation of the spike count at each ripple power range.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>The mean false-positive rate across both tracks for replay events detected with different ripple power range.</title><p>(<bold>A,B</bold>) The mean false-positive rate calculated at different alpha levels (i.e. 0.05, 0.02, 0.01, 0.005, 0.002, 0.001) as ripple power increased (i.e. 0–3, 3–5, 5–10,10, and above). The error bar indicated the 95% bootstrap confidence interval. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Replay detection performance improves with ripple power using stricter criteria for candidate events.</title><p>The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) as ripple power increases (0–3, 3–5, 5–10, 10, and above). (<bold>A,B</bold>) A stricter replay event criterion for the animal’s speed is used (&lt;1 cm/s). (Number of candidate replay events for ripple range 0-3, 3-5, 5-10 and 10 and above: RUN n = 256, 435, 878 and 421 and POST n = 1214, 1736, 4203 and 4746, respectively) (<bold>C,D</bold>) An additional replay event criterion (z-score of theta power &lt;0) is used. (Number of candidate replay events for ripple range 0-3, 3-5, 5-10 and 10 and above: RUN n = 389, 666, 1243 and 445 and POST n = 1061, 1287, 2347 and 2335, respectively). The shaded region indicates the 95% bootstrapped confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A,C</bold>) Replay events detected during RUN. (<bold>B,D</bold>) Replay events detected during POST.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Replay detection performance improves with ripple threshold for POST but not RUN.</title><p>(<bold>A,B</bold>) The proportion of significant events (out of all candidate events based on multi-unit activity [MUA] criteria alone) and mean log odds difference at different alpha level (0.2–0.001) as ripple threshold increases (0, 3, 5, and 10). The shaded region indicates the 95% bootstrapped confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (<bold>C</bold>) The proportion of significant events (out of all candidate events based on MUA criteria alone) and mean log odds differences at the alpha level of 0.05. The shaded box indicates a 95% bootstrap confidence interval for both the proportion of significant events detected and mean log odds difference. The triangle symbol is used to represent replay events during RUN and the square symbol is used to represent replay events during POST. (Number of candidate replay events: RUN n = 4643 and POST n = 15283).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig2-figsupp4-v2.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Replay detection is sensitive to the shuffle method used</title><p>Statistical significance of each replay event is calculated by comparing the replay score to one or more Monte-Carlo shuffle distributions, with each shuffle designed to randomize a specific aspect of sequential place cell firing while attempting to keep other factors intact. Some shuffles are applied to randomize the place fields or the spike train of the original data prior to Bayesian decoding, while other shuffles are applied to the spatial or temporal dimension of the decoded posterior probabilities. Because place cells often fire bursts of spikes, this creates non-independent samples, which <italic>violate the assumption of having independent samples for a statistical test</italic>. Thus, it is important for a shuffle method to preserve such aspects of the data that are not independent, to avoid adding type 1 errors in the statistical analysis, leading to a false-positive rate that exceeds the alpha level. Given this, we next examined how different shuffling procedures impact replay detection performance. Using the weighted correlation scoring method, we examined four types of shuffling procedures to see if they differed in how they detected replay during RUN and POST epochs. These consisted of two pre-decoding shuffles (spike train circular shuffle, place field circular shuffle) and two post-decoding shuffles (place bin circular shuffle, time bin permutation shuffle) (<xref ref-type="fig" rid="fig3">Figure 3A–D</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Replay detection performance was sensitive to the shuffling method applied.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) when using four different shuffling methods: (1) spike train circular shuffle (dark blue), (2) place field circular shuffle (light blue), (3) place bin circular shuffle (orange), and (4) time bin permutation shuffle (red). The shaded region indicates the 95% bootstrapped confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (<bold>C,D</bold>) The proportion of significant events and mean log odds difference at (<bold>C</bold>) an alpha level = 0.05 and (<bold>D</bold>) an FPR-matched alpha level with a mean false-positive rate of 5%. The shaded box indicates the 95% bootstrap confidence interval for both the proportion of significant events detected and mean log odds difference. The triangle symbol is used to represent replay events during RUN and the square symbol is used to represent replay events during POST. The dashed line represents the approximate chance level at mean false-positive rate of 5%. (Number of candidate replay events: RUN n = 4643 and POST n = 15283). The 95% confidence interval for the proportion of significant events, mean log odds difference, mean false-positive rates, and the FPR-matched alpha level for replay events detected using different shuffling methods are available in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Summary of replay detection performance using four different shuflling methods.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig3-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>The mean false-positive rate across both tracks for replay events detected using different shuffling methods.</title><p>(<bold>A</bold>) A spike train circular shuffle (dark blue) in which each cell’s spike train was independently circularly shifted in time by a random amount within each replay event. (<bold>B</bold>) A place field circular shuffle (light blue) in which each cell’s ratemap was circularly shifted in space by a random amount. (<bold>C</bold>) A place bin circular shuffle (orange) in which the posterior probability distribution for each time bin was independently circularly shifted by a random amount. (<bold>D</bold>) A time bin permutation shuffle (red) in which the order of the time bin was permuted randomly. (<bold>E,F</bold>) The mean false-positive rate calculated at different alpha levels (i.e. 0.05, 0.02, 0.01, 0.005, 0.002, 0.001) using four shuffles. The error bar indicated the 95% bootstrap confidence interval. (<bold>E</bold>) Replay events detected during RUN. (<bold>F</bold>) Replay events detected during POST.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig3-figsupp1-v2.tif"/></fig></fig-group><p>For both RUN and POST, we observed that pre-decoding shuffles performed better than post-decoding shuffles, with the time bin permutation shuffle showing a significantly lower log odd difference compared to the other three shuffles (<xref ref-type="fig" rid="fig3">Figure 3A and B</xref>). Although the post-decoding time bin permutation shuffle seemingly led to the highest detection rate at an alpha level of 0.05 (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), we observed that its mean false-positive rate at an alpha level of 0.05 was also substantially higher than other shuffling procedures. When we adjusted the alpha level to match a mean false-positive rate of 5%, the post-decoding time bin permutation shuffle had the poorest performance (the lowest proportion of significant events and the lowest mean log odds difference) across the four types of shuffles for both RUN and POST epochs (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). For both the RUN and POST epochs, the remaining three shuffling procedures had a similar mean log odds difference when using an FPR-matched alpha level. However, while the proportion of significant events were similar across the three shuffling methods for POST, the two pre-decoding shuffling procedures had a higher proportion of significant events for RUN replay events. These results suggested that the shuffling procedures that directly manipulated the place field data or spike train data before decoding may be more efficacious than procedures that randomized the posterior probability distributions after decoding.</p></sec><sec id="s2-4"><title>Replay detection can be improved by adding stricter detection criteria</title><p>Following our comparison of different shuffling procedures, we next investigated how the inclusion of additional criteria in replay detection could further improve performance. Commonly this is accomplished by using multiple types of shuffling procedures, with the requirement that each shuffling method must independently pass the same alpha level for the replay event to be classified as statistically significant. An alternative approach to using multiple shuffling procedures is to impose a criteria of jump distance - the number of spatial bins the decoded estimated position is allowed to jump across neighboring time bins during a spatial trajectory, with candidate replay events rejected if they fail this criterion (<xref ref-type="bibr" rid="bib31">Silva et al., 2015</xref>). One justification for using this metric is that a lower jump distance is correlated with a higher weighted correlation score, however not all events with higher scores are necessarily higher quality events.</p><p>Based on this, we compared the performance of four replay detection methods varying in their shuffling procedures: (1) a single place field circular shuffle, (2) a place field circular shuffle with a jump distance threshold (40% of track) (<xref ref-type="bibr" rid="bib31">Silva et al., 2015</xref>), (3) two shuffles (place field circular shuffle and time bin permutation shuffle) (<xref ref-type="bibr" rid="bib17">Grosmark and Buzsáki, 2016</xref>), and (4) three shuffles (place field circular shuffle, spike train circular shuffle, and place bin circular shuffle) (<xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). For both RUN and POST epochs, increasing the number of shuffles increased the mean log odd difference and decreased the proportion of significant events and the estimated false-positive rate, for similar alpha levels (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A and B</xref>). Adding a jump distance threshold as an additional criterion also increased the mean log odd difference, although this was accompanied by a marked reduction in the proportion of significant replay events detected (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>). Before controlling for the estimated false-positive rates associated with each method, as expected, the stricter methods would detect fewer replay events than more permissive methods. For instance, at alpha level of 0.05, around 50% of candidate events would be detected as significant events when using single shuffle, as opposed to 40% when using two shuffles (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). However, after adjusting the alpha level to a matching false-positive rate of 0.05, we found that stricter methods would detect more replay events. For example, at an FPR-matched alpha level, the proportion of significant events would reduce to 32% for a single shuffle (<italic>α</italic> = 0.007) but 35% for the stricter two-shuffle method (<italic>α</italic> = 0.032). This shows that controlling for the estimated false-positive rate can, almost paradoxically, result in stricter methods with additional shuffle types detecting more replay events. When using the original alpha level of 0.05, two methods (1 shuffle with a jump distance threshold and 3 shuffles) resulted in the highest mean log odd difference (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). However, if an FPR-matched alpha level was used (with the estimated false-positive rate fixed at 0.05), only the use of 2 or more shuffles achieved both the highest proportion of events and largest mean log odds difference, for both RUN and POST epochs (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). In contrast to this, a single shuffle with an added jump distance criterion performed the poorest (less events and lower track discriminability) out of the four methods when using the FPR-matched alpha level. This suggests that it is more beneficial to use a stricter alpha level than to impose a jump distance, an observation that extended to a range of different jump distances spanning 20–60% of the track (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Replay detection performance can be improved by adding stricter detection criteria.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) with four different detection criteria: (1) Only a post-decoding place bin circular shuffle, (2) a post-decoding place bin circular shuffle with jump distance threshold at normalized track length 0.4, (3) a pre-decoding place field circular shuffle and a post-decoding time bin permutation shuffle, (4) a pre-decoding place field circular shuffle, a pre-decoding spike train circular shuffle, and a post-decoding place bin circular shuffle. The shaded region indicates a 95% bootstrap confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (<bold>C,D</bold>) The proportion of significant events and mean log odds difference at (<bold>C</bold>) an alpha level = 0.05 and (<bold>D</bold>) an FPR-matched alpha level with a mean false-positive rate of 5%. The shaded box indicates a 95% bootstrap confidence interval for both the proportion of significant events detected and mean log odds difference. The triangle symbol represents replay events during RUN and the square symbol represents replay events during POST. The dashed line represents the approximate chance level at mean false-positive rate of 5%. (Number of candidate replay events: RUN n = 4643 and POST n = 15283). The 95% confidence interval for the proportion of significant events, mean log odds difference, mean false-positive rates, and the alpha level for replay events detected using different detection criteria are available in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Summary of replay detection performance when adding stricter detection criteria.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig4-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>The mean false-positive rate across both tracks for replay events detected using different detection criteria.</title><p>(<bold>A,B</bold>) The mean false-positive rate calculated at different alpha levels (i.e. 0.05, 0.02, 0.01, 0.005, 0.002, 0.001) using four different detection criteria: (1) Only a place bin circular shuffle, (2) a place bin circular shuffle with jump distance threshold (40% of the track length), (3) a place field circular shuffle and a time bin permutation shuffle, (4) a place bin circular shuffle, a spike train circular shuffle, and a place bin circular shuffle. The error bar indicated 95% bootstrap confidence interval. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Comparison of the replay detection performance when applying different jump distance thresholds.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) using place bin circular shuffle plus jump distance threshold ranging from 0.2 to 1 (normalized track length). The shaded region indicated 95% bootstrap confidence interval. The six dots with increasing color intensity for each distribution represented the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (<bold>C,D</bold>) The mean false-positive rate calculated at different alpha levels (i.e. 0.05, 0.02, 0.01, 0.005, 0.002, 0.001) using four different jump distance threshold. The error bar indicated 95% bootstrap confidence interval. For jump distance, due to the extremely low false-positive even at alpha level = 0.2, we quantified the replay detection performance up until alpha level = 1. (<bold>C</bold>) Replay events detected during RUN. (<bold>D</bold>) Replay events detected during POST. (<bold>E,F</bold>) The proportion of significant events and mean log odds difference at (<bold>E</bold>) an alpha level = 0.05 and (<bold>F</bold>) an FPR-matched alpha level with mean false-positive rate of 0.05. The shaded box indicated 95% bootstrap confidence interval. The triangle symbol was used to represent replay events during RUN and the square symbol was used to represent replay events during POST. The dashed line represented the approximate chance level at mean false-positive rate of 5%. (Number of candidate replay events: RUN n = 4643 and POST n = 15283). The 95% confidence interval for the proportion of significant events, mean log odds difference, mean false-positive rates, and the FPR-matched p-value across all methods are available in <xref ref-type="supplementary-material" rid="fig4s2sdata1">Figure 4—figure supplement 2—source data 1</xref>.</p><p><supplementary-material id="fig4s2sdata1"><label>Figure 4—figure supplement 2—source data 1.</label><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig4-figsupp2-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig4-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Rank-order-based replay detection is sensitive to spike selection</title><p>We next extended our framework to investigate other replay scoring methods - first focusing on the use of rank-order correlation. Instead of scoring the decoded trajectories, this approach directly quantifies the ordinal relationship within the spiking patterns (<xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>). In this method, the sequential order of place fields is ranked along the track, and compared with the corresponding rank of the spike times within the replay event. Rank-order correlation has been previously applied in several ways: (1) to all spikes within the replay event (<xref ref-type="bibr" rid="bib10">Dragoi and Tonegawa, 2011</xref>), (2) to all proto-events, each representing a bursts of spikes (<xref ref-type="bibr" rid="bib12">Foster and Wilson, 2006</xref>), or (3) only to one spike emitted by each place cell during a candidate replay event (e.g. median or first spike) (<xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). However, as mentioned previously, place cells often fire bursts of spikes which in effect creates non-independent samples. Therefore, in theory, the inclusion of all spikes for the Spearman’s rank-order based analysis would violate the statistical assumption of independent samples and lead to more type I errors (and a higher false-positive rate). We compared the rank-order-based analysis using either all spikes or median spike in our framework, and found that the estimated false-positive rate when using independent events (median spike time) remained around 5% (lower CI = 5.2% and upper CI = 5.8%), but increased to around 17% (lower CI = 16.3% and upper CI = 17.2%), when using all spikes (<xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). However, when using the FPR-matched alpha level (false-positive rate fixed at 5%), the trade-off between the proportion of significant events and their mean log odds difference was similar between the two methods (<xref ref-type="fig" rid="fig5">Figure 5D</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The performance of rank-order-based replay detection method depends on the selection of spikes for analysis.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) when (1) all spikes or (2) only the median spike fired by each place cell was included for rank-order-based replay analysis. The shaded region indicated 95% bootstrap confidence interval for mean log odds difference. The six dots with increasing color intensity for each distribution represent the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (<bold>C,D</bold>) The proportion of significant events and mean log odds difference at (<bold>C</bold>) an alpha level = 0.05 and (<bold>D</bold>) an FPR-matched alpha level with a mean false-positive rate of 5%. The shaded box indicates a 95% bootstrap confidence interval for both the proportion of significant events and mean log odds difference. The triangle symbol is used to represent replay events during RUN and the square symbol was is to represent replay events during POST. The dashed line represents the approximate chance level at mean false-positive rate of 5%. (Number of candidate replay events: RUN n = 4643 and POST n = 15283). The 95% confidence interval for the proportion of significant events, mean log odds difference, mean false-positive rates, and the FPR-matched alpha level for rank-order-based methods using all spikes or only median spike within the replay event are available in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Summary of replay detection performance for rank-order-based methods.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig5-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>The mean false-positive rate across both tracks when all spikes or median spike fired by each place cell was included for Spearman’s rank-order-based analysis.</title><p>(<bold>A,B</bold>) The mean false-positive rate calculated at different alpha levels (i.e. 0.05, 0.02, 0.01, 0.005, 0.002, 0.001) when (1) all spikes or (2) only the median spike fired by each place cell was included for rank-order-based replay analysis. The error bar indicated the 95% bootstrap confidence interval. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig5-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Replay events have a similar quality across different detection methods, when using an FPR-matched alpha level, but vary in their proportion</title><p>Finally, we expanded our analysis to directly compare every replay scoring and shuffling method already implemented in this report, with the addition one more commonly used method - a linear fit-based score of decoded trajectories with either one and two types of shuffles (<xref ref-type="bibr" rid="bib8">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="bib24">Muessig et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Ólafsdóttir et al., 2017</xref>). In the linear fit approach, the decoded posterior probability matrix, representing the probability of the virtual trajectory at each position on the track and at each time point, is compared to the best linear trajectory; the corresponding replay score is the sum of all posterior probabilities within a certain distance of this line, after maximizing this score across all possible lines that can pass through this matrix. While the linear fit approach is more stringent than the weighted correlation, it may also be less sensitive to non-linear replay trajectories (e.g. sigmoidal). When using a cell-id permuted randomized dataset to examine the chance-level detection for each method, we observed no bias in the mean log odds difference for every method, except for a linear fit approach with two shuffles (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). To avoid any possible intrinsic bias in our calculation of the mean log odds, we calculated the <italic>adjusted mean log odds difference</italic>, by subtracting any potential bias in log odds difference measured using spurious replay events (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Similar results were observed using an alternative within-track normalization approach in Bayesian decoding prior to replay detection (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). Finally, to provide a more comprehensive comparison across methods, we also included replay events detected in the PRE epochs, a phenomenon referred to as de novo preplay where statistically significant sequences are detected prior to any knowledge or experience running on the track (<xref ref-type="bibr" rid="bib10">Dragoi and Tonegawa, 2011</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparison of different replay detection methods for replay events during PRE, RUN, and POST.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at (<bold>A</bold>) an alpha level = 0.05 and (<bold>B</bold>) an FPR-matched alpha level with a mean false-positive rate of 5% using a range of different methods: (1) weighted correlation with place bin circular shuffle, (2) weighted correlation with place bin circular shuffle and jump distance threshold at normalized track length 0.4, (3) weighted correlation with place field circular shuffle and time bin permutation shuffle, (4) weighted correlation with place field circular shuffle, spike train circular shuffle, and time bin permutation, (5) Spearman’s rank-order based correlation using only median spike fired by each place cell, (6) Spearman’s rank-order based correlation using all spikes fired by each place cell, (7) linear fitting with place bin circular shuffle, (8) linear fitting with place field circular shuffle and time bin permutation shuffle. The shaded box indicated 95% bootstrap confidence interval. The circle symbol was used to represent replay events during PRE. The triangle symbol was used to represent replay events during RUN and the square symbol was used to represent replay events during POST. Inset plots are (<bold>A</bold>) the mean false-positive rate across shuffling methods and (<bold>B</bold>) the FPR-matched alpha level across shuffling methods. The dashed line in (<bold>B</bold>) represented the approximate chance level detection rate and track discriminability at mean false-positive rate of 5%. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283). The 95% confidence interval for the proportion of significant events, mean shuffled-corrected log odds difference, mean false-positive rates, and the FPR-matched p-value across all methods are available in <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref>.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Summary of replay detection performance across multiple detection methods.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-85635-fig6-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Comparison of different replay detection methods for replay events during PRE, RUN, and POST when log odds difference was not shuffle-subtracted.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at (<bold>A</bold>) an alpha level = 0.05 and (<bold>B</bold>) an FPR-matched alpha level with a mean false-positive rate of 5% using a range of different methods: (1) weighted correlation with place bin circular shuffle, (2) weighted correlation with place bin circular shuffle and jump distance threshold at normalized track length 0.4, (3) weighted correlation with place field circular shuffle and time bin permutation shuffle, (4) weighted correlation with place field circular shuffle, spike train circular shuffle, and time bin permutation, (5) Spearman’s rank-order-based correlation using only median spike fired by each place cell, (6) Spearman’s rank-order-based correlation using all spikes fired by each place cell, (7) linear fitting with place bin circular shuffle, (8) linear fitting with place field circular shuffle and time bin permutation shuffle. The shaded box indicated 95% bootstrap confidence interval. The circle symbol was used to represent replay events during PRE. The triangle symbol was used to represent replay events during RUN and the square symbol was used to represent replay events during POST. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Comparison of mean log odds difference using weighted correlation and linear fitting when the posterior probabilities were normalized within track or cross-track.</title><p>(<bold>A,B</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) when using weighted correlation and linear fitting where the posterior probabilities were normalized within track or across both tracks. The shaded region indicated 95% bootstrap confidence interval. The six dots with increasing color intensity for each distribution represented the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A</bold>) Replay events detected during RUN. (<bold>B</bold>) Replay events detected during POST. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Comparison of replay detection performance using weighted correlation for replay events during PRE, RUN, and POST.</title><p>(<bold>A–J</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) when using weighted correlation with different detection criteria. (<bold>A–C</bold>) Only post-decoding place bin circular shuffle. (<bold>D–F</bold>) Post-decoding place bin circular shuffle with jump distance threshold at normalized track length 0.4. (<bold>G–I</bold>) Pre-decoding place field circular shuffle and post-decoding time bin permutation shuffle. (<bold>J–L</bold>) Pre-decoding place field circular shuffle, pre-decoding spike train circular shuffle, and post-decoding time bin permutation. The shaded region indicated 95% bootstrap confidence interval. The six dots with increasing color intensity for each distribution represented the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A,D,G,J</bold>) Replay events detected during PRE. (<bold>B,E,H,L</bold>) Replay events detected during RUN. (<bold>C,F,I,L</bold>) Replay events detected during POST. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283)</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-figsupp3-v2.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 4.</label><caption><title>Comparison of replay detection performance using Spearman’s rank-order-based correlation for replay events during PRE, RUN, and POST.</title><p>(<bold>A–F</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) when (<bold>A–C</bold>) all spikes or (<bold>D–F</bold>) only the median spike fired by each place cell was included for rank-order-based replay analysis. The shaded region indicated 95% bootstrap confidence interval. The six dots with increasing color intensity for each distribution represented the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A,D</bold>) Replay events detected during PRE. (<bold>B,E</bold>) Replay events detected during RUN. (<bold>C,F</bold>) Replay events detected during POST. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-figsupp4-v2.tif"/></fig><fig id="fig6s5" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 5.</label><caption><title>Comparison of replay detection performance using linear fitting for replay events during PRE, RUN, and POST.</title><p>(<bold>A–F</bold>) The proportion of significant events and mean log odds difference at different alpha levels (0.2–0.001) when using linear fitting approach with (<bold>A–C</bold>) only post-decoding place bin circular shuffle or (<bold>D–F</bold>) both pre-decoding place field circular shuffle and post-decoding time bin permutation shuffle. The shaded region indicated 95% bootstrap confidence interval. The six dots with increasing color intensity for each distribution represented the data at an alpha level of 0.05, 0.02, 0.01, 0.005, 0.002, and 0.001. (<bold>A,D</bold>) Replay events detected during PRE. (<bold>B,E</bold>) Replay events detected during RUN. (<bold>C,F</bold>) Replay events detected during POST. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-figsupp5-v2.tif"/></fig><fig id="fig6s6" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 6.</label><caption><title>Comparison of the proportion of multi-track events and mean false-positive rate for different replay detection methods, and for replay events during PRE, RUN, and POST.</title><p>Multi-track events are replay events that pass all shuffling tests and/or additional replay criteria used for both tracks. At an alpha level = 0.05, shuffling methods tested include: (1) weighted correlation with place bin circular shuffle, (2) weighted correlation with place bin circular shuffle and jump distance threshold at normalized track length 0.4, (3) weighted correlation with place field circular shuffle and time bin permutation shuffle, (4) weighted correlation with place field circular shuffle, spike train circular shuffle, and time bin permutation, (5) Spearman’s rank-order-based correlation using only median spike fired by each place cell, (6) Spearman’s rank-order-based correlation using all spikes fired by each place cell, (7) linear fitting with place bin circular shuffle, (8) linear fitting with place field circular shuffle and time bin permutation shuffle. The shaded box indicated 95% bootstrap confidence interval. The circle symbol was used to represent replay events during PRE. The triangle symbol was used to represent replay events during RUN and the square symbol was used to represent replay events during POST. (Number of candidate replay events: PRE n = 8485, RUN n = 4643 and POST n = 15283)</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-fig6-figsupp6-v2.tif"/></fig></fig-group><p>Using our framework to compare replay detection methodologies, which incorporated (1) the corrected mean log odds difference, (2) the empirically estimated mean false-positive rate based on replay detection using a randomized dataset (cell-id permutation), and (3) the proportion of significant events on both tracks, we observed four main findings regarding replay during three different behavioral epochs (<xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplements 3</xref>–<xref ref-type="fig" rid="fig6s5">5</xref>). First, the proportion of significant events varied substantially across methods when using the original alpha level of 0.05, however this was consistent with the estimated false-positive rate, which ranged from 2% to 18%. Moreover, the proportion of mean false-positive rates estimated were also highly correlated with the proportion of events being classified as significant for both tracks (dual-track events), suggesting that most dual-track events we observed were likely due to false-positive detection on one of the two tracks rather than multiple track representations in a single replay event (<xref ref-type="fig" rid="fig6s6">Figure 6—figure supplement 6</xref>). Second, using the FPR-matched alpha level, all methods had a proportion of significant preplay events close to 10%, which was similar to the proportion of false-positive events expected by chance for two tracks. Furthermore, the shuffle-subtracted mean log odds difference for preplay was approximately zero for all methods, suggesting that regardless of the method used, if used correctly, preplay is indistinguishable from randomly generated spurious replay events. Third, using the FPR-matched alpha level for POST replay events, all methods produced a corrected mean log odds difference near 1, however linear and weighted correlation methods with 2 or more shuffles detected a higher proportion of significant events. Finally, using the FPR-matched alpha level for RUN replay events, the majority of methods produced a corrected mean log odds difference near 2. Despite using the FPR-matched alpha level to achieve the same mean false-positive rate, we observed that using two or more shuffling procedures would lead to significant improvement in the overall replay detection performance for both weighted correlation and linear fitting method. Replay detection methods will continue to evolve and be refined in the future; the quantification of any improvements is now possible with the framework outlined here.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Previous studies describing the phenomenon of replay have used a wide variety of methods and criteria for replay detection. The most common methods in the last decade have been a rank-order correlation (applied to spike trains), and the scoring of decoded trajectories (using a weighted correlation or linear fit) (<xref ref-type="bibr" rid="bib13">Foster, 2017</xref>; <xref ref-type="bibr" rid="bib34">Tingley and Peyrache, 2020</xref>; <xref ref-type="bibr" rid="bib38">van der Meer et al., 2020</xref>). However, even within these two classes of methods, there is substantial variation in the parameters and strategies for statistical testing, including the type and number of shuffling procedures, and the use of ripple power or jump distance as a threshold. The diversity in replay methods prevents an unbiased comparison between different replay findings. In particular, while replay detection statistical tests are commonly used with an alpha level of 0.05, this value does not necessarily match the actual false-positive detection rate for all methods. This issue is not just limited to when the entire spike train is used to calculate the rank-order correlation (as discussed above), but also applies to all methods using decoded neural activity. The use of either spatially smoothed place fields or spike bursts spanning multiple time bins similarly violates the assumption of independent samples. While the shuffling of neural patterns comprising the replayed trajectory may compensate for non-independent samples, its effectiveness varies substantially, especially given the wide range of parameters used across the published replay literature. Furthermore, while imposing stricter criteria for replay detection would theoretically improve the average fidelity of the events and reduce the false-positive rate, it comes with the caveat of potentially increasing the rejection of true replay events.</p><p>Quantifying and comparing replay detection performance has been an ongoing challenge due to the lack of a ground truth. Taking advantage of data collected from rats running on two linear tracks, here we introduce a novel framework for comparing different sequence-based replay detection methods - quantifying how well a given replay detection method correctly discriminates track-specific replay events. In particular, this framework quantifies the proportion of significant replay events (quantity) and the mean log odds difference (track discriminability) of a collection of track-specific replay events detected as well as adjusting the alpha level used for replay detection according to an estimated false-positive rate of 5% based on cell-id randomized dataset. Using this approach, we make six key observations: (1) For candidate events selected using an MUA threshold, the additional step of including a ripple power threshold is not necessary for RUN replay but is recommended for POST replay events. For our dataset, the POST replay events with ripple power below a z-score of 3–5 were indistinguishable from spurious events. While the exact ripple z-score threshold to implement may differ depending on the experimental condition (e.g. electrode placement, behavioral paradigm, noise level, etc.) and experimental aim, our findings highlight the benefit of using a ripple power threshold for detecting replay during POST. (2) More shuffles, with a preference for pre-decoding shuffles, lead to better replay detection, even after the alpha level is adjusted to match an estimated false-positive rate. (3) Without adjusting the alpha level, there is a very wide distribution of both the track discriminability and the proportion of significant events detected across methods. False-positive rates also vary substantially, and one should not assume that replay exists simply because the proportion of significant events detected is greater than the alpha level (typically 5%) used in detection. (4) After adjusting the alpha level to match an estimated false-positive rate, there is greater similarity observed between methods, however using either a weighted correlation or a linear fit replay scoring with only one shuffling procedure still results in the worst overall performance for our dataset. (5) Our metric for replay track discriminability yields a similar result across many methods once the alpha level is adjusted to fix the estimated false-positive rate at 5%, however the proportion of replay events detected remains more variable. (6) Regardless of the method used after the alpha level is adjusted, preplay events are at chance level for both track discriminability (mean log odds difference of 0) and proportion detected (10% of events across two tracks). Previous studies have used an alpha level of 0.05, which in our hands substantially changes the proportion of replay events detected across methods - overly strict methods such as weighted correlation with a jump distance criterion would have a proportion less than 5%, while more lenient methods such as rank-order correlation with all spikes included can have a proportion closer to 30%. Therefore, without actually measuring a false-positive rate, one cannot assume that a higher proportion of events relative to the alpha level is evidence, on its own, for the existence of replay. In all methods tested, we do not see any evidence of preplay events behaving differently from spurious replay events generated using randomized data. In contrast to this, both POST and RUN replay events have a proportion of significant events substantially above the chance level for detection, for any method used with an FPR-matched alpha level.</p><p>Here, we have focused on a number of common replay scoring methods and criteria, however there are many variations and nuanced differences between published methods, that were beyond the scope of this study to test. For instance, we did not examine the impact of spatial smoothing (place fields) and temporal smoothing (posterior probability matrix and/or spike train) on replay detection. For this study, we avoided any Gaussian smoothing of the posterior probability matrix in our analysis but opted to use relatively large bin sizes for Bayesian decoding (10 cm by 20 ms). It is possible that with smaller bin sizes, replay detection could improve, or yield a different outcome when comparing methodologies. However, as the size of the decoding bin decreases, noise in the decoded trajectory will likely increase or alternatively must be removed using another smoothing method, which in turn may change the false-positive rate of detection. Replay experiments can often vary along many other parameters (e.g. number of neurons, distribution of place fields, length of track, etc.), and it is possible that these differences could also affect the false-positive rate. As such, it is critical for replay studies to independently verify and report an estimated false-positive rate even for the experiments involving only one spatial context.</p><p>One caveat associated with this framework is that use of any kind of randomized dataset may potentially underestimate the false-positive rate of replay detection in a way that is biased toward specific shuffling distributions (i.e. place-based or time-based shuffling). In case of a cell-id randomization, each cell’s firing statistics of replay events is preserved, but the relationship between each cell’s spike train and its place fields is randomly swapped, which may be potentially biased more toward place-based shuffling. Generally, we observed that the cell-id randomized dataset did not underestimate the empirically estimated false-positive rates compared to place field shifted dataset and spike train shifted dataset and performed similarly to the cross-experiment shuffled dataset (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). Even for the case of place field shuffling where the spike train shifted dataset resulted in a higher false-positive rates when compared to the cell-id randomized dataset, the difference between cell-id randomized FPR and spike train randomized FPR is much smaller than that between the place field randomized and spike train randomized FPR at an alpha level of 0.05. This suggests that while cell-id randomization is relatively more sensitive to place-based shuffling compared to time-based shuffling, such bias was relatively minor. Taken together, our results suggest that, while imperfect, cell-id randomization (or alternatively a randomization using cross-experiment shuffled place fields) is the relatively more conservative and universally applicable null distribution for empirically estimating false-positive rates given the detection methods used here. Nevertheless, our finding about the performance of different replay detection methods should be interpreted with care. For example, while alpha level adjustment-based empirically estimated FPR allowed us to compare different detection methods, the comparison was made between distributions of candidate replay events rather than based on each individual event. Therefore, two methods with similar performance in terms of track discriminability and proportion of significant events do not necessarily detect the same sets of events as significant. It is also important to note that alpha-level adjustment implemented in this framework is not meant to be a way to optimize detection because it does not directly address method-specific issues and remove true false-positives caused by effects such as edge effects and neuronal spiking statistics.</p><p>The use of two linear tracks was critical for our current framework to calculate track discriminability. While this would limit the generality of our approach as most replay studies involved only one context rather than multiple contexts, with modification it is possible that this framework might be adapted to single-track data. For example, track discriminability could theoretically be calculated using a cell-id permutation to create a virtual second track or by comparing the discriminability across different portions of the track (depending on the experimental design), although exploring these options were beyond the scope of our current study.</p><p>One key consideration is that due to algorithmic constraints, some sequence detection methods may be biased to detecting certain types of replay trajectories. For instance, a linear fitting method may be more prone to detect linear replay trajectory compared to a sigmoidal-shaped or jumpy replay trajectory. On the other hand, a rank-order correlation analysis of spike trains is more relaxed in terms of the rigidity of the temporal structure, and therefore is more likely to include non-linear events with gaps. As such, there may be other factors to consider beyond track discriminability and the proportion of detected events when choosing a replay detection method. However, it is interesting to note that despite substantial differences in these algorithms used for replay detection, all methods had a similar track discriminability after adjusting the alpha level to a fixed estimated false-positive rate of 5%.</p><p>Last but not least, in addition to comparing the performance of different replay quantification strategies, this framework can also be applied to compare replay events detected across different behavioral states or experimental conditions. Previously, differences in replay across conditions have been measured using the rate or number of replay events (<xref ref-type="bibr" rid="bib15">Gillespie et al., 2021</xref>; <xref ref-type="bibr" rid="bib18">Huelin Gorriz et al., 2023</xref>; <xref ref-type="bibr" rid="bib26">Ólafsdóttir et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Xu et al., 2019</xref>), or decoding track bias (<xref ref-type="bibr" rid="bib6">Carey et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). However, combining these two approaches creates a more comprehensive metric for understanding the difference in replay across different conditions.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>All the behavioral and electrophysiological data used in this study were previously described in detail in <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>.</p><sec id="s4-1"><title>Behavioral paradigm and electrophysiological recording</title><p>Five male Lister-hooded rats (350–450 g) were implanted with 24 independently movable tetrodes, using a custom-made microdrive. In three rats, the microdrive was split into two 12 tetrodes groups, aimed at the right and left dorsal CA1 regions of the hippocampus respectively (AP: –3.48 mm; ML: ±2.4 mm from Bregma). In the remaining two rats, right dorsal CA1 was targeted with 16 tetrodes (AP: 3.72 mm, ML: 2.5 mm from Bregma), with the remaining 8 tetrodes targeting visual cortex.</p><p>Before the experimental sessions, each animal was trained for approximately 2 days (30 min sessions) to run back and forth on a linear track. The animal was motivated to run by receiving a reward (chocolate-flavored soy milk) at each end of the track. The training took place in a different room and the track configuration used for training was distinct from the experimental linear tracks.</p><p>Each recording session started with a 1 hr rest (PRE) epoch in which the rats were placed in a previously habituated rest pot at a remote location. The rest pot was made of circular enclosure (20 cm diameter) surrounded by a 50 cm tall black plastic wall, which prevented the rats from viewing rest of the environment. Following the PRE epoch, the rats went through one of the two protocols described below, for which both were followed by a POST rest epoch in the rest pot:</p><list list-type="order"><list-item><p>Rats were exposed to two novel linear tracks (2 m long).</p></list-item><list-item><p>Rats were exposed to three novel tracks (2 m long). Data from the first track is not included here to create a two-track experiment consistent with the remaining data.</p></list-item></list></sec><sec id="s4-2"><title>Place cell analysis</title><p>Place cells were required to have a minimum peak firing rate greater than 1 Hz, based on the unsmoothed ratemap. Spike trains used to compute the place field were speed filtered (4–50 cm/s).</p></sec><sec id="s4-3"><title>Bayesian decoding of spatial location</title><p>A naïve Bayesian decoder was used to estimate the brain’s estimation of animal’s position during behavior and virtual position during a replay event, using non-overlapping 20 ms time bins, and 10 cm position bins.<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>P</italic>(<italic>x</italic>|<italic>n</italic>) is the probability of the animal being at a specific position given the observed spiking activity, <italic>C</italic> is a normalization constant, <italic>x</italic> is the animal’s position, <italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic>) is the firing rate of the <italic>i</italic>th place field at a given location <italic>x</italic>, and <italic>n</italic> is the number of spikes in the time window <italic>τ</italic>. The normalization constant was defined as the summed posterior probabilities across both tracks.</p></sec><sec id="s4-4"><title>Detection of candidate replay event</title><p>Candidate replay events were first selected based on MUA, which was smoothed with a Gaussian kernel (sigma = 5 ms) and binned into 1 ms steps. Only events with MUA bursts with a maximum duration of 300 ms and z-scored activity over 3 were selected. Furthermore, ripple power was used as an additional criterion for candidate event selection. Ripple-band (125–300 Hz) LFP signal was smoothed with a 0.1 s moving average filter. Unless it was mentioned otherwise, the candidate events were discarded if the peak ripple power was less than z-score of 3. Moreover, the candidate events were further thresholded such that only events where the animal’s running speed was less than 5 cm/s, at least five active place cells and event duration over 100 ms and below 750 ms, were included for subsequent analysis. To maximize detection of replay events and avoid discarding minority events due to noisy probability decoding at the beginning or the end of the event, candidate replay events were split into two segments where the midpoint was determined based on the minimum MUA activity in the middle third of the candidate event. Both segments were decoded and analyzed for statistical significance independently with the alpha level adjusted to half of the alpha level used for ‘full’ event (e.g. <italic>α</italic> = 0.05 → <italic>α</italic> = 0.025). It should be noted that same criteria for candidate event inclusion such as minimum event duration and active place cell number still applied to these ‘half’ events. For this study, RUN replay was also defined as awake replay events when the animals were physically on the linear tracks (with moving speed less than 5 cm/s). PRE and POST replay was defined as replay events that took place during rest and sleep periods (not differentiated from each other) within the rest pot before and after experiencing both novel tracks, respectively.</p></sec><sec id="s4-5"><title>Sequence-based replay scoring</title><p>For this study, two different sequence quantification methods were used to quantify the sequenceness of the decoded posterior probability matrix for each event.</p><sec id="s4-5-1"><title>Weighted correlation</title><p>Weighted correlation method calculated the correlation coefficient between the change in decoded posterior probability bins across position and time by weighing each estimated position by its decoded posterior probability:</p><p>Weighted mean:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Weighted covariance:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Weighted correlation:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <italic>i</italic>th position bin, <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the <italic>j</italic>th time bin, and <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability at the position bin <italic>i</italic> and time bin <italic>j</italic>.</p></sec><sec id="s4-5-2"><title>Linear fitting</title><p>The linear fitting method finds the line of best fit that describes the decoded linear trajectory for each candidate event. In practice, this method would calculate all the possible linear kernel at different slopes (from 100 to 5000 cm/s) and intercept and then sum all the probabilities within 10 cm below or above each fitted line as a goodness of fit score:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>t</italic> is each time bin, <italic>prob</italic> is the decoded posterior, <italic>pos</italic> is the position bin, <italic>T</italic> is the time bin, <italic>d</italic> the maximum distance from the line of fit.</p><p>The combination of slope and intercept that maximizes the posterior decoded probabilities along the potential linear trajectory would be considered as the line of best fit.</p><p>A similar sequence-based analysis was applied to the randomized dataset (obtained by a cell-id permutation where the cell identity for each spike train in a <italic>candidate</italic> replay event was randomized before decoding). This shuffle was designed to preserve the firing statistics across place cells while randomizing the order of place fields along the track, in turn disrupting any relationship with the spike sequences produced during candidate events. For this study, to enhance the accuracy of our false-positive rate estimation, given that false-positive events were generally less frequent than replay events detected during POST and RUN in our original dataset, we generated three cell-id randomized candidate events for each real candidate event (with a new set of random seeds for every event) such that the total number of cell-id randomized events would equal three times the total number of real candidate events.</p><p>Following obtaining a sequence score (either a weighted correlation or linear fit-based score) for each event, we then calculated the p-value by comparing the event sequence score relative to the four shuffled distributions, each designed to randomize certain aspects of the sequential place cell firing.</p></sec></sec><sec id="s4-6"><title>Pre-decoding shuffling procedures</title><p>Shuffle 1: Spike train circular shuffle, in which the spike count vectors for each cell were independently circularly shifted in time by a random amount within each replay event, prior to decoding. This shuffle was designed to degrade the temporal order of the neuronal spiking with minimal disruption of each neuron’s spiking statistics and spatial template.</p><p>Shuffle 2: Place field circular shuffle, in which each ratemap was circularly shifted in space by a random amount of position bins prior to decoding. This shuffle was designed to randomize the preferred firing locations while preserving the temporal structure of the spiking patterns.</p></sec><sec id="s4-7"><title>Post-decoding shuffling procedures</title><p>Shuffle 3: Place bin circular shuffle, in which posterior probability distribution for each time bin (column) was independently circularly shifted by a random amount. This shuffle was designed to shift the decoded position at each time bin while preserving each neuron’s spiking properties.</p><p>Shuffle 4: Time bin permutation shuffle, in which the order of the time bins within each event was permutated randomly. This shuffle was designed to disrupt within-event temporal structure without interfering each neuron’s place field template.</p><p>Each type of shuffle was performed 1000 times for each event to obtain a shuffle distribution of sequence scores.</p></sec><sec id="s4-8"><title>Rank-order correlation</title><p>In addition to a probabilistic decoding-based approach, a rank-order correlation approach (Spearman’s correlation coefficient) was also performed directly to the spike train data during candidate replay events using all spikes (method 1) or only the median spike time (method 2) produced by each place cell:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>6</mml:mn><mml:mo>∑</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>d</italic> is the difference between the ranks of order of place cells that are active during a given event and the observed spike time, <italic>n</italic> is the number of spikes included for analysis (i.e. all spikes or only median spike produced by each place cell), and <inline-formula><mml:math id="inf4"><mml:mi>ρ</mml:mi></mml:math></inline-formula> is the Spearman’s rank correlation coefficient.</p><p>The p-value for Spearman’s <inline-formula><mml:math id="inf5"><mml:mi>ρ</mml:mi></mml:math></inline-formula> was computed by comparing the original <inline-formula><mml:math id="inf6"><mml:mi>ρ</mml:mi></mml:math></inline-formula> relative to the shuffled distribution where the order of spike was randomly permuted.</p></sec><sec id="s4-9"><title>Quantification of significant event proportion and false-positive rate</title><p>While standard replay analysis used p&lt;0.05 (i.e. score greater than 95% of the shuffled distribution) as the cut-off point for statistical significance, we calculated the proportion of significant events using alpha level ranging from 0.2 to 0.001. When more than one shuffle type was used together for detection, the sequence score was required to be below the alpha level for each shuffle type used, in order for the event to be considered significant. In cases when more than one shuffle was used to calculate the p-value, the highest p-value would be used to represent the sequenceness of each significant replay event. In cases where the candidate replay event was detected as statistically significant for both tracks (referred to as ‘multi-track event’), the candidate event was assigned as both track 1 and track 2 events. However, each multi-track event would be only counted once when calculating the proportion of significant events to avoid double counting. Assuming false-positive rate at 5%, we would expect approximately 10% detection rate (significant events labeled as either track 1 or track 2) with the caveat that this value can potentially decrease in the case where a high number of multi-track events were detected.<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>For cell-id randomized replay events, similar analysis was performed to calculate mean false-positive rate across both tracks.</p><p>The mean estimated false-positive rates across both tracks (later used for alpha level adjustment) were calculated by adding the number of false-positive events on both tracks divided by 2 (i.e. number of tracks), which was then divided by the total number of candidate events:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-10"><title>Sequenceless decoding and quantification of track discriminability</title><p>In order to cross-check the quality of replay events detected based on sequence-based approach, we used an independent metric that measure reactivation bias based on sequenceless decoding (<xref ref-type="bibr" rid="bib6">Carey et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Tirole et al., 2022</xref>). Only cells with stable place fields in the first and second half of the behavioral episode on both tracks (peak in-field firing rate &gt;1 Hz) were included in our sequenceless decoding analysis. Prior to Bayesian decoding, ratemaps for track 1 and track 2 were concatenated as a single matrix <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>C</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>nPos</italic> is the number of position bin on each track (i.e. 20×10 cm<sup>2</sup> position bins for both tracks). For a given replay event, the decoded posterior probabilities for each time bin were normalized across all position bins from both tracks to sum to 1. We quantified the logarithmic ratio between the summed posterior probabilities of both tracks, and then z-scored this ratio relative to a shuffled distribution computed by Track ID shuffle.<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For each place cell in the track ID shuffle, its place field templates for track 1 and track 2 were randomly assigned (i.e. each cell’s ratemap was either swapped or not swapped). Then, we quantify the track discriminability in terms of the difference in mean log odds between track 1 and track 2 replay events, originally detected using sequence-based replay detection methods, to cross-check the quality of replay content.</p></sec><sec id="s4-11"><title>Alpha level adjustment based on mean false-positive rate across both tracks</title><p>After calculating the mean false-positive rate across both tracks at alpha levels ranging from 0.2 to 0.001, we then identify the alpha level that leads to the mean false-positive rate closest to 0.05 for a given method, which would be referred to as FPR-matched alpha level. The log odds difference and proportion of significant events detected at the FPR-matched alpha level would be used for the subsequent method comparisons.</p></sec><sec id="s4-12"><title>Statistics</title><sec id="s4-12-1"><title>Bootstrapping for method comparisons</title><p>To determine if the differences between two methods (i.e. mean log odds or proportion of significant events) were statistically significant, we used a bootstrapping procedure where candidate replay events were resampled with replacement 1000 times. This created bootstrapped distributions of log odds differences, the proportion of significant events detected, and false-positive rates at each alpha level ranging from 0.2 to 0.001.</p><p>When comparing the proportion of significant events and log odds difference at the original alpha level = 0.05, we calculated the 95% confidence interval for both metrics obtained at an alpha level = 0.05. When comparing the proportion of significant events and log odds difference at the FPR-matched alpha level, we calculated the 95% confidence interval for both metrics obtained at the alpha level when the mean false-positive rate was closest to 5%. The difference between the two bootstrapped distributions was only considered statistically significant when the 95% confidence intervals did not overlap.</p></sec><sec id="s4-12-2"><title>Bootstrapping for log odds significance</title><p>To determine if the log odds difference of the replay events detected during PRE, RUN, and POST by a given method was statistically significant from chance level track discriminability, we calculated the confidence interval for the difference between bootstrapped distribution of the original replay data and the cell-id randomized replay data. The mean difference between two bootstrapped distributions was only considered statistically significant when the 95% confidence interval did not overlap with 0.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Funding acquisition, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures and post-operative care were approved and carried out in accordance with the UK Home Office, subject to the restrictions and provisions contained within the Animal Scientific Procedures Act of 1986. Experiments were conducted under PPL P61EA6A72 (Bendor).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-85635-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Analysis code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/bendor-lab/replay_detection_cross_validation">https://github.com/bendor-lab/replay_detection_cross_validation</ext-link> (copy archived at <xref ref-type="bibr" rid="bib33">Takigawa, 2024</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Tirole</surname><given-names>M</given-names></name><name><surname>Huelin</surname><given-names>M</given-names></name><name><surname>Takigawa</surname><given-names>M</given-names></name><name><surname>Kukovska</surname><given-names>L</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Experience-driven rate modulation is reinstated during hippocampal replay</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.ksn02v76h</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank members of the Bendor Lab and Henrik Singmann for valuable discussion and Tom Wills and Aman Saleem for their comments on the manuscript. Rat schematic in <xref ref-type="fig" rid="fig1">Figure 1A</xref> was adapted with permission from <ext-link ext-link-type="uri" xlink:href="https://scidraw.io/">SciDraw.io</ext-link> (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3926277">https://doi.org/10.5281/zenodo.3926277</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3926237">https://doi.org/10.5281/zenodo.3926237</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0">https://creativecommons.org/licenses/by/4.0</ext-link>). This work was supported by the Medical Research Council scholarship (MR/N013867/1) (MTak), the European Research Council starter grant (CHIME) (DB), the Human Frontiers Science Program Young Investigator Award (RGY0067/2016) (DB), and the Biotechnology and Biological Sciences Research Council Research grant (BB/T005475/1) (DB). The Titan Xp used for this research was donated by the NVIDIA Corporation.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Asiminas</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020a</year><data-title>Adult hooded rat running</data-title><version designator="v1">v1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3926237">https://doi.org/10.5281/zenodo.3926237</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Asiminas</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020b</year><data-title>Adult hooded rat running</data-title><version designator="v1">v1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3926277">https://doi.org/10.5281/zenodo.3926277</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Biasing the content of hippocampal replay during sleep</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1439</fpage><lpage>1444</lpage><pub-id pub-id-type="doi">10.1038/nn.3203</pub-id><pub-id pub-id-type="pmid">22941111</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Two-stage model of memory trace formation: a role for “noisy” brain states</article-title><source>Neuroscience</source><volume>31</volume><fpage>551</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1016/0306-4522(89)90423-5</pub-id><pub-id pub-id-type="pmid">2687720</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal sharp wave-ripple: a cognitive biomarker for episodic memory and planning</article-title><source>Hippocampus</source><volume>25</volume><fpage>1073</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1002/hipo.22488</pub-id><pub-id pub-id-type="pmid">26135716</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carey</surname><given-names>AA</given-names></name><name><surname>Tanaka</surname><given-names>Y</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reward revaluation biases hippocampal replay content away from the preferred outcome</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1450</fpage><lpage>1459</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0464-6</pub-id><pub-id pub-id-type="pmid">31427771</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Csicsvari</surname><given-names>J</given-names></name><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Allen</surname><given-names>K</given-names></name><name><surname>Senior</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Place-selective firing contributes to the reverse-order reactivation of CA1 pyramidal cells during sharp waves in open-field exploration</article-title><source>The European Journal of Neuroscience</source><volume>26</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2007.05684.x</pub-id><pub-id pub-id-type="pmid">17651429</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>TJ</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Hippocampal replay of extended experience</article-title><source>Neuron</source><volume>63</volume><fpage>497</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id><pub-id pub-id-type="pmid">19709631</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname><given-names>K</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname><given-names>G</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Preplay of future place cell sequences by hippocampal cellular assemblies</article-title><source>Nature</source><volume>469</volume><fpage>397</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1038/nature09633</pub-id><pub-id pub-id-type="pmid">21179088</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A cortical-hippocampal system for declarative memory</article-title><source>Nature Reviews. Neuroscience</source><volume>1</volume><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1038/35036213</pub-id><pub-id pub-id-type="pmid">11252767</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1038/nature04587</pub-id><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Replay comes of age</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>581</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031538</pub-id><pub-id pub-id-type="pmid">28772098</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>PW</given-names></name><name><surname>Bontempi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The organization of recent and remote memories</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1038/nrn1607</pub-id><pub-id pub-id-type="pmid">15685217</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>AK</given-names></name><name><surname>Astudillo Maya</surname><given-names>DA</given-names></name><name><surname>Denovellis</surname><given-names>EL</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Kastner</surname><given-names>DB</given-names></name><name><surname>Coulter</surname><given-names>ME</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Hippocampal replay reflects specific past experiences rather than a plan for subsequent choice</article-title><source>Neuron</source><volume>109</volume><fpage>3149</fpage><lpage>3163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.029</pub-id><pub-id pub-id-type="pmid">34450026</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomperts</surname><given-names>SN</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>VTA neurons coordinate with the hippocampal reactivation of spatial experience</article-title><source>eLife</source><volume>4</volume><elocation-id>e05360</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05360</pub-id><pub-id pub-id-type="pmid">26465113</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Diversity in neural firing dynamics supports both rigid and learned hippocampal sequences</article-title><source>Science</source><volume>351</volume><fpage>1440</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1126/science.aad1935</pub-id><pub-id pub-id-type="pmid">27013730</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huelin Gorriz</surname><given-names>M</given-names></name><name><surname>Takigawa</surname><given-names>M</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The role of experience in prioritizing hippocampal replay</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>8157</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-43939-z</pub-id><pub-id pub-id-type="pmid">38071221</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nn1825</pub-id><pub-id pub-id-type="pmid">17173043</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname><given-names>MP</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Awake replay of remote experiences in the hippocampus</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>913</fpage><lpage>918</lpage><pub-id pub-id-type="doi">10.1038/nn.2344</pub-id><pub-id pub-id-type="pmid">19525943</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinzing</surname><given-names>JG</given-names></name><name><surname>Niethard</surname><given-names>N</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mechanisms of systems memory consolidation during sleep</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1598</fpage><lpage>1610</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0467-3</pub-id><pub-id pub-id-type="pmid">31451802</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Memory of sequential experience in the hippocampus during slow wave sleep</article-title><source>Neuron</source><volume>36</volume><fpage>1183</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01096-6</pub-id><pub-id pub-id-type="pmid">12495631</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>PA</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How targeted memory reactivation promotes the selective strengthening of memories in sleep</article-title><source>Current Biology</source><volume>29</volume><fpage>R906</fpage><lpage>R912</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.08.019</pub-id><pub-id pub-id-type="pmid">31550479</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muessig</surname><given-names>L</given-names></name><name><surname>Lasek</surname><given-names>M</given-names></name><name><surname>Varsavsky</surname><given-names>I</given-names></name><name><surname>Cacucci</surname><given-names>F</given-names></name><name><surname>Wills</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coordinated emergence of hippocampal replay and theta sequences during post-natal development</article-title><source>Current Biology</source><volume>29</volume><fpage>834</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.01.005</pub-id><pub-id pub-id-type="pmid">30773370</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal place cells construct reward related sequences through unexplored space</article-title><source>eLife</source><volume>4</volume><elocation-id>e06063</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06063</pub-id><pub-id pub-id-type="pmid">26112828</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Carpenter</surname><given-names>F</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coordinated grid and place cell replay during rest</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>792</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1038/nn.4291</pub-id><pub-id pub-id-type="pmid">27089021</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Carpenter</surname><given-names>F</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Task demands predict a dynamic switch in the content of awake hippocampal replay</article-title><source>Neuron</source><volume>96</volume><fpage>925</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.035</pub-id><pub-id pub-id-type="pmid">29056296</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The role of hippocampal replay in memory and planning</article-title><source>Current Biology</source><volume>28</volume><fpage>R37</fpage><lpage>R50</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.073</pub-id><pub-id pub-id-type="pmid">29316421</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1038/nature12112</pub-id><pub-id pub-id-type="pmid">23594744</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>T</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Trajectory events across hippocampal place cells require previous experience</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1772</fpage><lpage>1779</lpage><pub-id pub-id-type="doi">10.1038/nn.4151</pub-id><pub-id pub-id-type="pmid">26502260</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans</article-title><source>Psychological Review</source><volume>99</volume><fpage>195</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.99.2.195</pub-id><pub-id pub-id-type="pmid">1594723</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Takigawa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Replay_detection_cross_validation</data-title><version designator="swh:1:rev:754a9efe01eae551a8e4766f1137c38b1365b2aa">swh:1:rev:754a9efe01eae551a8e4766f1137c38b1365b2aa</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:815c87241f246593b122a6e6df25a477162fd408;origin=https://github.com/bendor-lab/replay_detection_cross_validation;visit=swh:1:snp:c12aa78007454aed86fc1c2f4f382eeb6f117b2b;anchor=swh:1:rev:754a9efe01eae551a8e4766f1137c38b1365b2aa">https://archive.softwareheritage.org/swh:1:dir:815c87241f246593b122a6e6df25a477162fd408;origin=https://github.com/bendor-lab/replay_detection_cross_validation;visit=swh:1:snp:c12aa78007454aed86fc1c2f4f382eeb6f117b2b;anchor=swh:1:rev:754a9efe01eae551a8e4766f1137c38b1365b2aa</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tingley</surname><given-names>D</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the methods for reactivation and replay analysis</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>375</volume><elocation-id>20190231</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0231</pub-id><pub-id pub-id-type="pmid">32248787</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tirole</surname><given-names>M</given-names></name><name><surname>Huelin Gorriz</surname><given-names>M</given-names></name><name><surname>Takigawa</surname><given-names>M</given-names></name><name><surname>Kukovska</surname><given-names>L</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Experience-driven rate modulation is reinstated during hippocampal replay</article-title><source>eLife</source><volume>11</volume><elocation-id>e79031</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.79031</pub-id><pub-id pub-id-type="pmid">35993533</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorova</surname><given-names>R</given-names></name><name><surname>Zugaro</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Isolated cortical computations during delta waves support memory consolidation</article-title><source>Science</source><volume>366</volume><fpage>377</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1126/science.aay0616</pub-id><pub-id pub-id-type="pmid">31624215</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Meer</surname><given-names>MAA</given-names></name><name><surname>Carey</surname><given-names>AA</given-names></name><name><surname>Tanaka</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimizing for generalization in the decoding of internally generated activity in the hippocampus</article-title><source>Hippocampus</source><volume>27</volume><fpage>580</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1002/hipo.22714</pub-id><pub-id pub-id-type="pmid">28177571</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Meer</surname><given-names>MAA</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Diba</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Progress and issues in second-order analysis of hippocampal replay</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>375</volume><elocation-id>20190238</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0238</pub-id><pub-id pub-id-type="pmid">32248780</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Widloski</surname><given-names>J</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible rerouting of hippocampal replay sequences around changing barriers in the absence of global place field remapping</article-title><source>Neuron</source><volume>110</volume><fpage>1547</fpage><lpage>1558</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.02.002</pub-id><pub-id pub-id-type="pmid">35180390</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Reactivation of hippocampal ensemble memories during sleep</article-title><source>Science</source><volume>265</volume><fpage>676</fpage><lpage>679</lpage><pub-id pub-id-type="doi">10.1126/science.8036517</pub-id><pub-id pub-id-type="pmid">8036517</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Baracskay</surname><given-names>P</given-names></name><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Assembly responses of hippocampal CA1 place cells predict learned behavior in goal-directed spatial tasks on the radial eight-arm maze</article-title><source>Neuron</source><volume>101</volume><fpage>119</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.015</pub-id><pub-id pub-id-type="pmid">30503645</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Ginzburg</surname><given-names>I</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>1017</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.2.1017</pub-id><pub-id pub-id-type="pmid">9463459</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85635.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.12.12.520040" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.12.12.520040"/></front-stub><body><p>In this valuable work, the authors present a case for a new standard in replay detection, tackling the formidable problem that different methods can produce in vastly different results. The authors show compelling evidence about the source of this problem (which is that the true false positive rate can vary wildly between methods). The authors present a solution to the challenge of underestimation of the false positive rate and leverage new experimental data and novel analysis techniques to provide solid evidence that – under specific assumptions – their approach is effective.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85635.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Denovellis</surname><given-names>Eric L</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043mz5j54</institution-id><institution>University of California, San Francisco</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.12.12.520040">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.12.12.520040v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Evaluating hippocampal replay without a ground truth&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Laura Colgin as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another and multiple reviewing editors and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions (for the authors):</p><p>(1) The reviewers were unanimous in disagreeing with the idea that the cell-ID shuffle could be used as a method to estimate a false positive rate, and thus generally opposed the prospective premise of the work.</p><p>(1b) There was an opinion that &quot;it might still be a useful tool to compare popular replay scoring methods, as long as (1) instances where the threshold is incorrectly referred to as an &quot;adjusted p-value&quot; are removed and (2) it is not recommended as a novel replay detection approach but simply a tool to compare the methods.</p><p>(2) However, the reviewers felt that there was value in comparing track discriminability vs. sequence detection, particularly for analysis and comparisons including the PRE-RUN data. Reviewers were moderately enthusiastic about an alternative presentation that concentrated on reporting effect sizes rather than p-values alone, and shifted towards understanding the relationships between the factors.</p><p>(3) Towards this end, in the discussion, the reviewers were also concerned about the idea of restricting analyses only to those neurons that had fields in both environments. This makes sense as a potential way to avoid concerns about spike sorting, but a subsequent paper should include the obviously functionally important population of neurons that have place fields in only one environment.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. I would suggest showing the analysis on a single environment and three environments if possible for the log odds measure.</p><p>2. It would be ideal for the functions used for the main analysis to be clearly documented in the code repository. At the very least highlighted in the README for use by others.</p><p>3. Do these results hold over animals? It would be important to see the variability in conclusions if possible.</p><p>4. Would the conclusions about p-values hold if you applied a multiple comparisons correction like the Benjamini and Hochberg procedure or Storey's q-value first? I would suggest clarifying why the empirically determined adjusted p-value works better than the other multiple comparison measures.</p><p>5. I would clarify that the p-value threshold and significant events are determined by the replay score only in the text.</p><p>6. &quot;computing the summed posteriors (across time bins) within the replay event for each track&quot; – Please clarify or correct this line because I believe the posterior is summed across time and position.</p><p>7. &quot;difference in mean log odds between track 1 and track 2 replay events&quot; – It is unclear what this mean is over. Session? Over all animals?</p><p>8. &quot; ripple power threshold is not necessary for RUN replay, but should be applied to POST replay events&quot; – It is my understanding that a multiunit threshold is still applied. It would be important to clarify this if this is the case.</p><p>9. &quot;a more positive mean log odds difference would indicate a higher trajectory discriminability in the replay content, and a higher confidence in replay quality. In contrast, a mean log odds difference of 0 would suggest that the quality of replay events is comparable to chance level trajectory discriminability, and that the detected event population most likely consists of low fidelity events, indistinguishable from false-positives,&quot;- is there an absolute value being taken here? The measure can be both positive and negative, so unless the track selected by the replay score is always the positive going track, this cannot be quite correct. Please clarify if that is the case and note that the statistic can be negative.</p><p>10. The term &quot;cross-validation&quot; is being used improperly. Cross-validation has a specific meaning in statistics, implying a test and training set. I would suggest using validation, or orthogonal validation instead.</p><p>11. I would add individual examples of the log odds procedure and z-score (as in Tirole et al. 2022, Figure 4B) to help the readers understand the procedure better.</p><p>12. &quot;Based on the desire to maximize the statistical independence of data, we avoided any smoothing in our analysis, as this could increase the false-positive rate of replay detection. Because we did not use spatial or temporal smoothing, we opted to use larger bin sizes for Bayesian decoding (10 cm by 20 ms).&quot; – Using bins of any kind is a form of &quot;smoothing&quot; and choice of bin edges will lead to different answers. Furthermore smoothing is not inherently bad as it can reduce variance and potentially lead to more statistical power. Choosing not to smooth with a gaussian is fine as a choice but I would change this sentence.</p><p>13. &quot;To determine if the decrease in detection rate was associated with decrease in the false-positive rate for this detection method, we calculated the false-positive rate (mean fraction of spurious replay events detected across both tracks)&quot; – I would suggest clarifying that the spurious replay events detected are from the shuffled data. It is clear in the figure legend, but not in the main body of the text.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I would recommend the authors focus on the analysis of preplay in their dataset. They can better analyze their observation that no reactivation is evident in events with high replay scores, which will be of interest to the field. I believe that the rest of the work is highly problematic.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I have two fundamental suggestions.</p><p>1) While I agree that the sequenceless decoding is important to cross-validate the main findings of the paper, reading the paper for the first time gave me the (false) impression that the proposed framework of detecting replay somehow uses this measure to select the final replay events. After reading the methods multiple times, it seems to me that first impression was untrue, and the proposed replay detection only depends on adjusting the p-value based on the empirical false positive estimate using a cell-ID shuffle. If this is correct, then the sequenceless decoding only provides an x-axis as additional evidence to help convince the reader of the conclusions of the comparisons.</p><p>The way the paper is written, I think most readers would make the same initial assumption as I did. The first time false positive estimates are mentioned in the results of the paper, we are directed to Figure 1D which describes the sequenceless cross-validation. Moreover, given the emphasis of this sequenceless approach in the abstract and the entire manuscript, it is easy to conclude that the false-positive rate is estimated as the number of events that don't show sequenceless log-odds higher than the shuffle.</p><p>This is problematic because many people in the field might dismiss the findings by thinking &quot;Well that's nice but I don't have two tracks, so I can't do any of that&quot;. The only place in the manuscript that made me dig deeper was in the discussion, where the authors state &quot;As such, it is critical for replay studies to independently verify and report this false-positive rate even for the experiments involving only one spatial context.&quot; Only after reaching this sentence did I even consider that estimating the false positive rate might not require two tracks, and it took quite some time to make sure it doesn't, and yet even now a part of me is still unsure that I got it right. I would urge the authors to make it more clear that the false positive rate can be estimated readily using a simple shuffle and does not require having two tracks.</p><p>It is my opinion that estimating the false positive rate and taking it into account is very powerful and can be a game changer for replay detection, but it won't happen if people don't understand it was done and how it was done.</p><p>2) In my initial reading of the paper, I was confused by Figure 4D (and all the figures like it). It sounds counter-intuitive that a more strict method (e.g. 2 shuffles vs 1 shuffle) can end up detecting *more* replay events. It is not trivial point to grasp, and it is easy for readers to get lost here (e.g. the y-axis &quot;proportion of significant events&quot; may be considered by some confused readers to mean some measure of specificity like &quot;among the events selected by each method, how many of them were true positives&quot;). I think something can be added to this section to better guide readers and explain what is being proposed.</p><p>I'm thinking of something like &quot;Before controlling for the false positive rates associated with each method, as expected, stricter methods found fewer replay events than more permissive methods: for example, a single shuffle selected 50% of awake events as significant, as opposed to 40% that the stricter 2-shuffles method selected (Figure 4C). However, after adjusting the p-value to control for the false positive rate, we found that stricter methods actually resulted in detecting more replay events: the single shuffle awake replay adjusted p-value was 0.007 which brought down the proportion of significant events to 32%, while the stricter 2-shuffles awake replay adjusted p-value was 0.032, resulting in 35% of events detected as significant. This shows that controlling for the empirically measured false positive rate can, almost paradoxically, result in stricter methods detecting more replay events.&quot; The &quot;Place+jump distance&quot; is somewhat separate and can be presented after taking the time to explain this very critical point.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Evaluating hippocampal replay without a ground truth&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Laura Colgin (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The reviewers reached a consensus that the paper requires minor revisions. Please see individual reviewer comments below. In particular, there was a consensus that the discussion of the paper needs to clearly articulate a proper interpretation of the corrected p-values. Specifically, that using one shuffle (cell-ID) to correct the p-values of another (i.e., circular) on a collection of events is not the same as somehow correcting the replay detection for individual events.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Thank you to the authors for their thoughtful responses to the reviews. I do think the response clarified several aspects of the manuscript and improved the clarity of the manuscript overall. The documentation and improvement of the code associated with the manuscript is also improved.</p><p>As I understand the logic, the main claims of the framework to study sequence detection methods are:</p><p>1. A good sequence detection method should assign the same track as track decoding.</p><p>2. The significant replay events for a given method can be adjusted by matching the false positive rate resulting in greater track discriminability (and more conservatively identified replay events)</p><p>3. This can be used to compare replay detection methods or quality of replay event</p><p>The strengths of the work are:</p><p>1. it is more conservative in judging which sequence is a replay or &quot;spurious&quot; given that the two track model is true</p><p>2. it makes some interesting conclusions about the use of the ripple power and multiunit as criterion for detecting replay events.</p><p>The weaknesses of the work are:</p><p>1. The framework relies on an adjustment for track discriminability but, as the authors acknowledge, there are many ways in which the model can be misspecified. This only tests for a very specific way in which they are misspecified. And it doesn't seem to give one much insight into why a particular method is better. For example, the authors' response seems concerned about the lack of accounting for bursting in replay detection, but track discriminability does not circumvent this. The way to account for lack of independence in time bins is to explicitly account for the bursting, such as fitting the place field estimates with a self-history term. The track discriminability measure simply marginalizes over the time and position and already incorporates the misspecified encoding model.</p><p>2. This framework relies on classifying aspects of the replay event as significant or not significant at a given significance threshold. This is reliant on the particular null distribution specified.</p><p>3. The generality of the work is limited by their two track framing.</p><p>The work claims that their framework is a &quot;unifying approach to quantify and compare the replay detection performance&quot;. The work falls short of doing this in general but I do think it accomplishes this with a much narrower scope, namely in adjusting methods that fail to discriminate between tracks appropriately in the case of two tracks.</p><p>– Are the significant events being corrected for multiple comparisons? Could the authors explain why no corrections are needed?</p><p>– I do not think the authors sufficiently addressed the question of what happens if the track discrimination model is misspecified in their response. For example, if there are truly three tracks but you only account for two?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>While I appreciate the conceptual novelty of using the log odds ratio as a metric for testing the noisiness of replay events and comparing this metric with results from sequence analyses, unfortunately, I was not persuaded by the arguments presented in the authors' rebuttal. I think the study would benefit greatly if the authors consulted or collaborated closely with an expert on statistical methods, particularly on the application of resampling methods, to enhance the rigor of their approach. Without this, I remain concerned that this work can cause further confusion (and noise) rather than clarity in these analyses.</p><p>I believe I noted my concerns to great length in the initial review, but perhaps the authors will wish to look through the section &quot;The problem with replay decoding: shuffles&quot; section in Foster 2017 and note that each shuffle is permissive for some events and not others. E.g. if the circular shuffle is likely to pass through events that arise from edge effects, their study does not explain how a shift of the α level is going to fix that issue and remove false positives. Each type of shuffle will pass through different subsets of events, some of which may be &quot;false positives,&quot; but they will be different events in each case. Simply adjusting the α levels or p-values, the solution proposed by this study, will not fix the underlying reasons for the false positives, and lead to potentially mistaken confidence in the results.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I find that the revised manuscript has considerably improved. I have one outstanding issue with the randomization shuffle, which can actually be alleviated if the authors update Figure D to include the 3 remaining shuffles, and a minor comment.</p><p>Outstanding question regarding the randomization shuffle:</p><p>My concerns about the cell-id shuffle were that (1) any shuffle, including this one, may actually underestimate the true false positive rate, and (2) this tendency to underestimate the true false positive rate will be exacerbated when the randomization shuffle is similar to the shuffle used for detecting replay events, resulting in a biased comparison between methods.</p><p>I think (1) is unavoidable as it would be hard to disprove any structure that could be inadvertently erased by any given shuffle. The true false positive rate may always be higher than our best estimates captured by our surrogate datasets, and our estimates are simply a lower bound of the true false positive rate. In particular for the cell-id shuffle (including the &quot;cross experiment cell-id&quot; shuffle), there may be some underlying structure in biological data which violates assumptions of independence that is not captured by place-field swapping. This would undoubtedly be the case when including a variety of cell types with generally different place field properties (e.g. along the dorsoventral axis or the deep/superficial sublayer). Even without different anatomical cell types, more excitable cells may be more likely to have multiple place fields as well as firing more spikes within candidate events – a potential structure that is not captured by the cell-id shuffle.</p><p>While concern (1) only needs to be mentioned (in the discussion or in a public review), concern (2) is particularly problematic because the authors compare the (FRP-matched) performance of different methods to conclude which are best. Figure D shows a comparison of two methods: place field shifting and place field swapping (with place fields within the same dataset, the randomization used throughout the manuscript, or from a different dataset, which is almost identical to the original randomization) before trajectory decoding. Figure D convincingly argues that relative to field swapping (cell-id shuffle), field shifting (place field circular shifted shuffle) is a much poorer method of randomization as it considerably underestimates the false positive rate for detection methods using similar shuffles. Note that the bias is slightly attenuated for the &quot;place bin shuffle&quot; (it is worse for the &quot;place field shuffle&quot;), demonstrating that there is a confounding gradient with larger similarities between the randomization and the detection shuffles resulting in larger bias.</p><p>According to the authors, Figure D shows that &quot;cell-id based randomization is sufficiently independent from both place-based and temporally-based shuffles for replay detection&quot;. I cannot see how that can be concluded from the figure. The place field swapping methods may well be underestimating the false positive rate unevenly. Place field swapping is still a pre-decoding place-based shuffle, so the fact that its estimate agrees with the estimate of another pre-decoding place-based shuffle (place field shifting) does not demonstrate that these estimates are not a biased. I would like to stress that one of the authors' conclusions is that &quot;more shuffles, with a preference for pre-decoding shuffles, lead to better replay detection.&quot; All of the approaches using &quot;more shuffles, with a preference for pre-decoding shuffles&quot; included place field shifting, which in my opinion is similar to the place field swapping of the cell-id shuffle.</p><p>I commend the authors for the approach of Figure D, which is a good way to address this key concern. I recommend this figure to be included in the manuscript. Moreover, if the authors were to include the other 3 shuffles (it currently includes &quot;Shuffle 2&quot; from the methods) and show that the cell-id does not underestimate the false positive rate more than those shuffles in a biased way, that would practically disprove this issue. Indeed, the concern is that the cell-id shuffle may be underestimating the false positive rate particularly for detection methods using pre-decoding and place-based shuffles, but if none of the other 3 shuffles produce higher false positive estimates for the lower row in Figure D, that would be convincing evidence that the cell-id randomization procedure is not biased towards detection methods employing pre-decoding place-based shuffles and strengthen the conclusions.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Evaluating hippocampal replay without a ground truth&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Laura Colgin (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The introduction of the manuscript has already been significantly revised. However, the final revision requested by the reviewers is to spend a few sentences outlining not only sequence scoring, but also the fact that the manuscript utilises multiple types of surrogate data sets to establish null distributions. In particular, it is suggested that some assumptions about the utility of the cell-id shuffle surrogate be described, as well as the importance of track discriminability. Reviewer #2 (see below) is quite proscriptive; please consider their points and update the introduction to at least mention these issues up front.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The revision added some text regarding the limitations of the study. I was hoping that the revised manuscript would also be more explicit regarding the underlying assumptions. While I don't think reviewers should do the authors work for them, after such a lengthy review, perhaps this is the only way short of a rejection.</p><p>From what I can tell, these are the main assumptions. If I am wrong, perhaps the other reviewers, or the authors can correct me. Also perhaps there are others that I missed.</p><p>1) After recording from two tracks, events with low discriminability between these two tracks cannot be said to be replays, regardless of their sequence scores.</p><p>– Pooled across data, discriminability generally correlates with replay score, though this differs across datasets and timepoints of recording.</p><p>2) The cell-id shuffle provides the best null distribution to test against for replay.</p><p>– This is so because cell-id shuffled events show low discriminability even in instances where they have high sequence scores</p><p>3) The proportion of cell-id shuffled events that qualify as replays in any given period is similar to the proportion of events generated in the real data from hippocampus network that are likewise not actual replay but are labeled as significantly ordered.</p><p>– This therefore allows for a scaling correction to bring this proportion in line with the α level.</p><p>4) In the absence of performing cell-id shuffles, which is straightforward, the alternative prescription is to record from two tracks and adjust the apparent α to match those of cell-id shuffled events obtained during the recording</p><p>– This is so that the same relative number of events are labeled as replays, even though they may not be the actual replays.</p><p>In my view, these assumptions should be provided very clearly early in the manuscript, ideally in the introduction section, to guide readers towards a better understanding of the study.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85635.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions (for the authors):</p><p>1) The reviewers were unanimous in disagreeing with the idea that the cell-ID shuffle could be used as a method to estimate a false positive rate, and thus generally opposed the prospective premise of the work.</p></disp-quote><p>We fully understand the referees’ concerns with a cell-id randomization, and the caveats raised for using this approach for quantifying the false positive rate. However, we strongly feel that it is necessary to empirically estimate the false positive rate, given that the decoded data (or spike trains) are not generally composed of independent events, which may be inflating the false detection rate (and moreover by an unknown amount). We also believe that other methods suggested by reviewers may be problematic for this same reason. We will aim in this rebuttal to make a more sufficiently convincing argument that our original method, while imperfect, should remain in the manuscript, including a new validation of this method with an alternative approach (described below). We do not take this step lightly given that the reviewers were unanimous in disagreeing with our original approach. The response to the referee comments and our new analyses is described in detail below.</p><p>Reviewer #1 suggests an alternative multiple comparisons correction method, the Benjamini and Hochberg (BH) procedure, to adjust the α level rather than using an empirically-estimated false positive rate. To explore the use of this method we created a simple replay model- a true replay event was modelled as 10 neurons firing in a stereotyped order (e.g. A-B-C-D-E-F-G-H-I-J). Randomized events consisted of the same 10 neurons firing in a random order (e.g. C-F-J-A-H-D-E-B-G-I), from which we could empirically measure proportion of spurious events detected as a proxy for false positive rate. We next simulated 10000 different randomized events, varying the number of spikes produced each time the neuron fired (one spike or a burst of two or three spikes). Spikes within a burst always remained together, so for example a randomized sequence with bursts of two spikes could be- C-C-F-F-J-J-A-A-H-H-D-D-E-E-B-B-G-G-I-I. From this we quantified each event’s Spearman’s rank order correlation using all spikes (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>). We used this form of replay detection, as the statistical properties of a rank order correlation are well understood, and we can violate the assumption of independent events by replacing neurons firing single spikes with neurons firing bursts of 2 or 3 spikes. Note that this violation only occurs if we calculate the Spearman correlation coefficient with all spikes (rather than just the first or median spike for each neuron). We observed that while the distributions of correlation coefficients are similar, the distributions of p value are not uniform when neurons fire more than two spikes. Since the Benjamini and Hochberg procedure works by ranking the p value from the smallest to largest, such a skewed p value distribution due to spike non-independence (e.g. hippocampal neuronal bursting) can inflate detection of false events. Indeed, even if we now include 20% true events and 80% false events (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>), around 900 and 1700 false events would be detected as true events at a 5% false discovery rate (using the BH method) in the doublet and triplet condition, respectively. In contrast, a similar number of false events (i.e. 350-450) would be detected in all three conditions as false events, if the α level is lowered on the null distribution (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>) to reach a 5% empirically estimated false positive rate. We conclude with these analyses that traditional statistical methods may not work as intended with replay sequences, and it is important to find a way to empirically test the consequence of our dataset lacking independent samples. It is important to point out that this observation extends to multiple replay detection methods including those relying on decoding, due to a lack of independence between neighbouring bins in both the temporal and spatial dimension.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Distribution of Spearman’s rank order correlation score and p value for false events with random sequence where each neuron fires one (left), two (middle) or three (right) spikes.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-sa2-fig1-v2.tif"/></fig><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Distribution of Spearman’s rank order correlation score and p value for mixture of 20% true events and 80% false events where each neuron fires one (left), two (middle) or three (right) spikes.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-sa2-fig2-v2.tif"/></fig><fig id="sa2fig3" position="float"><label>Author response image 3.</label><caption><title>Number of true events (blue) and false events (yellow) detected based on α level 0.</title><p>05 (upper left), empirical false positive rate 5% (upper right) and false discovery rate 5% (lower left, based on BH method).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-sa2-fig3-v2.tif"/></fig><p>We understand that the reviewers are concerned that our method used to create a randomized dataset could theoretically influence the empirically estimated false positive rates and therefore α level threshold. To examine the effect of data randomization on detection, we created two new types of randomized datasets – a place field circular shifted dataset and a cross experiment cell-id randomized dataset. Place field circular shifted dataset was created in same way as described in the original manuscript, where it was used to create shuffled distributions during replay detection. Its purpose is to validate this concern, using a poor method of randomization (that is identical to the shuffle used in replay detection). The cross experiment cell-id randomized dataset was created by randomly selecting place cells from another experiment, eliminating any potential interaction between the randomization used for creating a sequence, and the shuffle distributions used for detection. We view this as an alternative and robust method of data randomization, which we can then compare to our cell-id shuffled approach used in our original manuscript.</p><p>We next performed replay detection using a weighted correlation with a single shuffle method (similar to Figure 3 in our original manuscript), but now applied to these two new randomized datasets. The shuffle methods tested included a time bin permutation shuffle, a spike train circular shift shuffle, a place field circular shift shuffle, or a place bin circular shift shuffle (<xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>). Indeed, using a place field shuffle on a place field randomized dataset leads to a 5% empirically estimated false positive rate due the null distribution of the randomized data being too similar to that used for detection. However, the cross experiment and within experiment cell-id randomisation leads to a very similar false positive rate (for each of the other four shuffles used). Interestingly, the estimated false positive rate for the place field randomized dataset overlaps with both cell-id randomized datasets when using shuffles that disrupt only temporal information (i.e. spike train shuffle and time bin shuffle). Together these results suggest that a cell-id based randomization is sufficiently independent from both place-based and temporally-based shuffles for replay detection.</p><p>Even though a cross experiment cell-id randomization would be in theory the most independent method of creating a null dataset, its result was extremely similar to that produced by a within experiment cell-id randomization. Given that it is less widely applicable (i.e. it needs multiple recording sessions for it to work) and may not necessarily capture the replay data’s distribution being analyzed, we decided to stay with our original within experiment cell id randomization method to empirically estimate false positive rates. Therefore, based on the additional analyses performed, we would like the reviewers to reconsider their decision about the validity of using a cell-id randomization, as we strongly feel this is an essential part of the study. If we do not account for the estimated false positive rate, we will unable to compare different replay detection methods. A prime example is the use of rank-order correlation to detect events, using either the median spike time or all spikes. On the surface, these two methods appear to have a trade-off – all spikes will detect more events but with a lower mean track discriminability. However, this is simply due to a higher false positive rate, which, when accounted for, show the two methods are very similar (Figure 5C,D). We are happy to include all the analyses in this rebuttal letter as supplementary figures in the manuscript if requested by the reviewers.</p><fig id="sa2fig4" position="float"><label>Author response image 4.</label><caption><title>Proportion of false events detected when using dataset with within and cross experiment cell-id randomization and place field randomization.</title><p>The detection was based on single shuffle including time bin permutation shuffle, spike train circular shift shuffle, place field circular shift shuffle, and place bin circular shift shuffle.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-sa2-fig4-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>(1b) There was an opinion that &quot;it might still be a useful tool to compare popular replay scoring methods, as long as (1) instances where the threshold is incorrectly referred to as an &quot;adjusted p-value&quot; are removed and (2) it is not recommended as a novel replay detection approach but simply a tool to compare the methods.</p></disp-quote><p>We understand that an ‘adjusted p-value’ is a term specifically reserved for multiple comparisons correction, which can confuse the readers about the kind of ‘adjustment’ we are performing. Therefore, as recommended by the reviewers, we have changed in the revised manuscript two terms- (1) ‘p-value threshold’ to ‘α level’ and (2) ‘adjusted’ to ‘FPR-matched’, to be more explicit about determining α level with matched false positive rates (0.05). We hope we have made a sufficient argument in our previous section for the necessity of including FPR-matched values. For the second point, we would like to clarify that it is not our intention to present this tool as a novel replay detection approach. It is indeed merely a novel tool for evaluating different replay detection methods or measuring the “quality” of a replay event (after more traditional sequence detection methods are employed). We have now modified the manuscript in the intro and discussion to make this point more explicit.</p><disp-quote content-type="editor-comment"><p>2) However, the reviewers felt that there was value in comparing track discriminability vs. sequence detection, particularly for analysis and comparisons including the PRE-RUN data. Reviewers were moderately enthusiastic about an alternative presentation that concentrated on reporting effect sizes rather than p-values alone, and shifted towards understanding the relationships between the factors.</p></disp-quote><p>We would like to thank the editors and reviewers for valuing the potential of our work. However, we are confused about where we are being requested to report effect size, and seek further clarification. While the effect size would be useful for comparing two groups, the p-value threshold (now referred to as α level) is for the selection of single replay events, based on a comparison of their replay score and the distribution of replay scores from shuffled events. For each method, we are reporting the estimated false-positive rate, log odds difference and the proportion of events detected at different α levels. To compare these values between methods, we are performing bootstrapping (1000 times with replacement) to calculate the 95% confidence interval of the distribution at each α level (however we are not measuring statistically significant differences between methods and obtaining a p-value).</p><p>While it is interesting to further explore how other factors such as behaviour, place field properties and linearity of the replayed trajectory can influence replay detection, it is beyond the scope of this manuscript which has focused on unresolved methodological debates. Our main aim is to introduce this tool to researchers studying replay, to allow the collective exploration of each factor’s contribution to replay detection.</p><disp-quote content-type="editor-comment"><p>3) Towards this end, in the discussion, the reviewers were also concerned about the idea of restricting analyses only to those neurons that had fields in both environments. This makes sense as a potential way to avoid concerns about spike sorting, but a subsequent paper should include the obviously functionally important population of neurons that have place fields in only one environment.</p></disp-quote><p>We would like to thank the reviewers for this suggestion, and may need to further clarify our reasons for using this approach. While sequence detection used all place fields (active on only one track or on both tracks), track discrimination required a restriction to only those neurons that had place fields in both environments. This was needed to avoid any potential bias in track discrimination arising when neurons with single-track fields were included. Every time a neuron with a place field on only one track participates in a detected replay sequence, it will increase the summed posterior probabilities for the decoded replayed track (but not the other track), automatically improving the track discriminability. Note that decoding was normalized across both tracks (so at any time point, the posterior probabilities for each place bin across both tracks summed to 1). Our goal was to make track discriminability as independent as possible from sequence detection, such that the fidelity of the sequence did not impact our assessment of how well a downstream brain region could determine whether track 1 or track 2 was reactivating.</p><p>It is worth noting that track discriminability has other potential uses, and in some cases it is desirable to use all the place fields to maximize the discriminability score. However, this is beyond the scope of this study, but something currently in development in the lab.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. I would suggest showing the analysis on a single environment and three environments if possible for the log odds measure.</p></disp-quote><p>Unfortunately, our current method is optimized for a two track comparison, because Bayesian decoding must be normalized across tracks to calculate track discriminability. While the quantification of log odds on a single environment with different subregions or multiple environments may be possible with modification, it is beyond the scope of this study. Three environments is possible by taking the ratio of one environment compared to the other two (summed), however chance level would no longer be a log odds of zero.</p><disp-quote content-type="editor-comment"><p>2. It would be ideal for the functions used for the main analysis to be clearly documented in the code repository. At the very least highlighted in the README for use by others.</p></disp-quote><p>We have now updated the README file to provide basic information about the codes used for main analysis. We have also provided a simplified version of code for calculating log odds, which can be easily customised by other users.</p><disp-quote content-type="editor-comment"><p>3. Do these results hold over animals? It would be important to see the variability in conclusions if possible.</p></disp-quote><p>We agree with the reviewer that it is important to demonstrate if the results hold for individual sessions. Therefore, we ran the log odds analysis for individual sessions using weighted correlation with two shuffles (place field shuffle and time bin shuffle) or Spearman correlation with inclusion of only median spike. Consistent with our original findings, we observed that replay events from different sessions but the same behavioural state (PRE, RUN or POST) would tend to cluster together especially when the α level with matching false positive rates were used (<xref ref-type="fig" rid="sa2fig5">Author response image 5</xref>). We are including subplots A-B from Figure E (without PRE) as Figure 1—figure supplement 2 for Figure 1 in the manuscript to demonstrate that our main findings hold over sessions.</p><fig id="sa2fig5" position="float"><label>Author response image 5.</label><caption><title>Mean log odds and proportion of significant events at α level 0.</title><p>05 or FDR-matched α level for individual sessions when replay events were detected using (A-B) weighted correlation with two shuffles and (C-D) Spearman correlation using only each cell’s median spike. Different color is used to indicate different behavioural states: PRE (black), RUN (blue) and POST (orange). Different symbol is used to indicate different sessions..</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85635-sa2-fig5-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>4. Would the conclusions about p-values hold if you applied a multiple comparisons correction like the Benjamini and Hochberg procedure or Storey's q-value first? I would suggest clarifying why the empirically determined adjusted p-value works better than the other multiple comparison measures.</p></disp-quote><p>Please refer to our earlier responses to this issue where we provided evidence for why we think empirically determined false-positive-rate-matched α level are more aligned to our goals than a multiple comparisons analysis with a BH procedure.</p><disp-quote content-type="editor-comment"><p>5. I would clarify that the p-value threshold and significant events are determined by the replay score only in the text.</p></disp-quote><p>Thank you for your suggestion. We have now made this point more explicit throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>6. &quot;computing the summed posteriors (across time bins) within the replay event for each track&quot; – Please clarify or correct this line because I believe the posterior is summed across time and position.</p></disp-quote><p>Thank you for your suggestion. Yes, the summed posterior probabilities were summed across time and position within the replay event for each track. We have corrected this line in the manuscript on page 3.</p><p>“Sequenceless decoding involved three steps: 1) computing the summed posteriors (across time and space) within the replay event for each track, 2)….”</p><disp-quote content-type="editor-comment"><p>7. &quot;difference in mean log odds between track 1 and track 2 replay events&quot; – It is unclear what this mean is over. Session? Over all animals?</p></disp-quote><p>The mean log odds for each track’s replay events is averaged across all 10 session. We have updated the manuscript to make this information more explicit on page 3:</p><p>“To use sequenceless decoding to evaluate replay events, we computed the difference in mean log odds between track 1 and track 2 replay events across all sessions,…”</p><disp-quote content-type="editor-comment"><p>8. &quot; ripple power threshold is not necessary for RUN replay, but should be applied to POST replay events&quot; – It is my understanding that a multiunit threshold is still applied. It would be important to clarify this if this is the case.</p></disp-quote><p>Yes, we agree that this is an important point to clarify. The multiunit threshold is always applied to select the candidate events before applying ripple power threshold. This was previously mentioned on page 7 (see below), but we will reiterate this point in the discussion for additional clarity:</p><p>“We next applied our replay analysis framework to test how a stricter criterion for ripple power affects trajectory discriminability. Candidate events were selected based on both elevated MUA (z-score&gt;3) and ripple power limited to a specific range, measured in SD above baseline (i.e. a z-score of 0-3, 3-5, 5-10, or &gt;10).”</p><disp-quote content-type="editor-comment"><p>9. &quot;a more positive mean log odds difference would indicate a higher trajectory discriminability in the replay content, and a higher confidence in replay quality. In contrast, a mean log odds difference of 0 would suggest that the quality of replay events is comparable to chance level trajectory discriminability, and that the detected event population most likely consists of low fidelity events, indistinguishable from false-positives,&quot;- is there an absolute value being taken here? The measure can be both positive and negative, so unless the track selected by the replay score is always the positive going track, this cannot be quite correct. Please clarify if that is the case and note that the statistic can be negative.</p></disp-quote><p>We believe the confusion is arising from using the mean log odds <italic>difference</italic> (not log odds). To summarise our method, each candidate replay event is decoded against track 1 and track 2 template to give replay scores for both templates. Depending on the α level, if the event is significant for track 1, then it is classified as track 1 and vice versa for track 2. The log odds for each event is calculated by taking the log of the ratio between the summed posteriors for track 1 and track 2 (always T1 over T2). The z-score log odds is then quantified based on a distribution of log odds computed by a track ID shuffle. As a result, a more positive z-scored log odds would indicate a greater likelihood of track 1 reactivation whereas a more negative value would indicate a greater likelihood of track 2 reactivation. If replay detection method can correctly identify track 1 events and track 2 events, then the mean log odds value across all track 1 events should be positive and mean log odds value across all track 2 events should be negative. Therefore the mean T1-T2 log odds difference should therefore be positive if the events are correctly detected. If the detection method is poor (many falsely classified events) or if there are no real replay events, then log odds would become noisy and random, which would lead to near zero mean log odds difference. This was indeed observed for the cell-id randomised datasets in our analysis. If the mean log-odds difference is negative, this would indicate that most track 1 events selected according to a sequence-based approach are consistently having higher probability bias towards track 2 and vice versa. While this mismatch could occur for a single event, this should never occur across large sample of replay events (from real data or a randomized dataset).</p><disp-quote content-type="editor-comment"><p>10. The term &quot;cross-validation&quot; is being used improperly. Cross-validation has a specific meaning in statistics, implying a test and training set. I would suggest using validation, or orthogonal validation instead.</p></disp-quote><p>Thank you for your feedback. We also agree that cross-validation can be misleading to the readers. We have now replaced it with ‘cross-checked’ instead in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>11. I would add individual examples of the log odds procedure and z-score (as in Tirole et al. 2022, Figure 4B) to help the readers understand the procedure better.</p></disp-quote><p>We agree with the reviewer that adding individual examples of log odds and sequenceness p value would help the readers to understand our framework better. We have added four individual examples with different level of sequence fidelity and reactivation bias in Figure 1—figure supplement 1.</p><disp-quote content-type="editor-comment"><p>12. &quot;Based on the desire to maximize the statistical independence of data, we avoided any smoothing in our analysis, as this could increase the false-positive rate of replay detection. Because we did not use spatial or temporal smoothing, we opted to use larger bin sizes for Bayesian decoding (10 cm by 20 ms).&quot; – Using bins of any kind is a form of &quot;smoothing&quot; and choice of bin edges will lead to different answers. Furthermore smoothing is not inherently bad as it can reduce variance and potentially lead to more statistical power. Choosing not to smooth with a gaussian is fine as a choice but I would change this sentence.</p></disp-quote><p>We agree with the reviewer that binning is also a form of smoothing and our original sentence could be misleading. However, we would like to point out that use of gaussian smoothing, especially when applied on posterior probability matrix, can cause neighbouring bins to share information, which may further increase the estimated false-positive rate (e.g. especially when a shuffle distribution is created from the smoothed posterior probability). We have modified the sentence to avoid the impression that we think binning is a better smoothing method than smoothing using a filter while still pointing out how different smoothing methods may influence false-positive rate of detection.</p><p>“For instance, we did not examine the impact of spatial smoothing (place fields) and temporal smoothing (posterior probability matrix and/or spike train) on replay detection. For this study, we avoided any gaussian smoothing of the posterior probability matrix in our analysis but opted to use relatively large bin sizes for Bayesian decoding (10 cm by 20 ms). It is possible that with smaller bin sizes, replay detection could improve, or yield a different outcome when comparing methodologies. However, as the size of the decoding bin decreases, noise in the decoded trajectory will likely increase or alternatively must be removed using another smoothing method, which in turn may change the false-positive rate of detection..”</p><disp-quote content-type="editor-comment"><p>13. &quot;To determine if the decrease in detection rate was associated with decrease in the false-positive rate for this detection method, we calculated the false-positive rate (mean fraction of spurious replay events detected across both tracks)&quot; – I would suggest clarifying that the spurious replay events detected are from the shuffled data. It is clear in the figure legend, but not in the main body of the text.</p></disp-quote><p>Thank you for your suggestions. We have updated the manuscript accordingly.</p><p>“To determine if this decrease in the detection rate was also associated with a decrease in the false-positive rate, we empirically measured the mean fraction of spurious replay events detected across both tracks after the dataset was randomized (by permuting the cell-id of each place cell) as a proxy for false-positive rate.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I would recommend the authors focus on the analysis of preplay in their dataset. They can better analyze their observation that no reactivation is evident in events with high replay scores, which will be of interest to the field. I believe that the rest of the work is highly problematic.</p></disp-quote><p>We would like to thank the reviewer for valuing our analysis about preplay. It is not our goal to single out a single debate in the field, and while this may be one of the more divisive ones, there is also sufficient diversity how candidate replay events are selected (ripple and/or multi-unit activity) and scored (line fitting or weighted correlation). Even if two methods are valid, it is important to understand how their use may impact the analysis.</p><p>We also strongly feel that it is necessary to use an independent metric like log odds to validate and compare the performance of different replay methods to understand the amount of discrepancy across methods in terms of track discriminability, proportion of significant events detected and mean estimated false positive rates. For more information, please refer to our response to the public review and essential revisions where we provided a detailed discussion about our choice of α level adjustment based on matching false positive rates empirically estimated using cell-id randomization.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I have two fundamental suggestions.</p><p>1) While I agree that the sequenceless decoding is important to cross-validate the main findings of the paper, reading the paper for the first time gave me the (false) impression that the proposed framework of detecting replay somehow uses this measure to select the final replay events. After reading the methods multiple times, it seems to me that first impression was untrue, and the proposed replay detection only depends on adjusting the p-value based on the empirical false positive estimate using a cell-ID shuffle. If this is correct, then the sequenceless decoding only provides an x-axis as additional evidence to help convince the reader of the conclusions of the comparisons.</p><p>The way the paper is written, I think most readers would make the same initial assumption as I did. The first time false positive estimates are mentioned in the results of the paper, we are directed to Figure 1D which describes the sequenceless cross-validation. Moreover, given the emphasis of this sequenceless approach in the abstract and the entire manuscript, it is easy to conclude that the false-positive rate is estimated as the number of events that don't show sequenceless log-odds higher than the shuffle.</p><p>This is problematic because many people in the field might dismiss the findings by thinking &quot;Well that's nice but I don't have two tracks, so I can't do any of that&quot;. The only place in the manuscript that made me dig deeper was in the discussion, where the authors state &quot;As such, it is critical for replay studies to independently verify and report this false-positive rate even for the experiments involving only one spatial context.&quot; Only after reaching this sentence did I even consider that estimating the false positive rate might not require two tracks, and it took quite some time to make sure it doesn't, and yet even now a part of me is still unsure that I got it right. I would urge the authors to make it more clear that the false positive rate can be estimated readily using a simple shuffle and does not require having two tracks.</p><p>It is my opinion that estimating the false positive rate and taking it into account is very powerful and can be a game changer for replay detection, but it won't happen if people don't understand it was done and how it was done.</p></disp-quote><p>We would like to than the reviewer for this suggestion. We have rewritten the manuscript for to clarify our methods, including emphasizing the fact that the empirically estimated false positive rates were derived by running the same sequence-based detection on cell-id randomized dataset rather than based on log odds metric. The reviewer is correct that this process can be done without the requirement of having two tracks.</p><disp-quote content-type="editor-comment"><p>2) In my initial reading of the paper, I was confused by Figure 4D (and all the figures like it). It sounds counter-intuitive that a more strict method (e.g. 2 shuffles vs 1 shuffle) can end up detecting *more* replay events. It is not trivial point to grasp, and it is easy for readers to get lost here (e.g. the y-axis &quot;proportion of significant events&quot; may be considered by some confused readers to mean some measure of specificity like &quot;among the events selected by each method, how many of them were true positives&quot;). I think something can be added to this section to better guide readers and explain what is being proposed.</p><p>I'm thinking of something like &quot;Before controlling for the false positive rates associated with each method, as expected, stricter methods found fewer replay events than more permissive methods: for example, a single shuffle selected 50% of awake events as significant, as opposed to 40% that the stricter 2-shuffles method selected (Figure 4C). However, after adjusting the p-value to control for the false positive rate, we found that stricter methods actually resulted in detecting more replay events: the single shuffle awake replay adjusted p-value was 0.007 which brought down the proportion of significant events to 32%, while the stricter 2-shuffles awake replay adjusted p-value was 0.032, resulting in 35% of events detected as significant. This shows that controlling for the empirically measured false positive rate can, almost paradoxically, result in stricter methods detecting more replay events.&quot; The &quot;Place+jump distance&quot; is somewhat separate and can be presented after taking the time to explain this very critical point.</p></disp-quote><p>This is an excellent suggestion, and we have modified the manuscript based on this feedback (page 12, and see below), and explain that while more shuffle methods will decrease the number of detected replay events, they also lower the false positive rate by an even larger amount. The consequence of this is that when the false positive rate made similar between methods by reducing the α level of the null distribution, the use of additional shuffle types will result in more detected replay events.</p><p>“Before controlling for the estimated false positive rates associated with each method, as expected, the stricter methods would detect fewer replay events than the more permissive methods. For instance, at α level of 0.05, around 50% of candidate events would be detected as significant events when using single shuffle, as opposed to 40% when using two shuffles (Figure 4C). However, after adjusting the α level to matching false positive rate of 0.05, we found that stricter methods would detect more replay events. For example, at FPR-matched α level, the proportion of significant events would reduce to 32% for single shuffle (α=0.007) but 35% for the stricter two-shuffle method (α=0.032). This shows that controlling for the empirically measured false positive rate can, almost paradoxically, result in stricter methods with additional shuffle types detecting more replay events.”</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The reviewers reached a consensus that the paper requires minor revisions. Please see individual reviewer comments below. In particular, there was a consensus that the discussion of the paper needs to clearly articulate a proper interpretation of the corrected p-values. Specifically, that using one shuffle (cell-ID) to correct the p-values of another (i.e., circular) on a collection of events is not the same as somehow correcting the replay detection for individual events.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>Thank you to the authors for their thoughtful responses to the reviews. I do think the response clarified several aspects of the manuscript and improved the clarity of the manuscript overall. The documentation and improvement of the code associated with the manuscript is also improved.</p><p>As I understand the logic, the main claims of the framework to study sequence detection methods are:</p><p>1. A good sequence detection method should assign the same track as track decoding.</p><p>2. The significant replay events for a given method can be adjusted by matching the false positive rate resulting in greater track discriminability (and more conservatively identified replay events)</p><p>3. This can be used to compare replay detection methods or quality of replay event</p><p>The strengths of the work are:</p><p>1. it is more conservative in judging which sequence is a replay or &quot;spurious&quot; given that the two track model is true</p><p>2. it makes some interesting conclusions about the use of the ripple power and multiunit as criterion for detecting replay events.</p></disp-quote><p>We would like to thank the reviewer for the constructive feedback and comments, and highlighting the value in our framework for comparing replay detection methods.</p><disp-quote content-type="editor-comment"><p>The weaknesses of the work are:</p><p>1. The framework relies on an adjustment for track discriminability but, as the authors acknowledge, there are many ways in which the model can be misspecified. This only tests for a very specific way in which they are misspecified. And it doesn't seem to give one much insight into why a particular method is better. For example, the authors' response seems concerned about the lack of accounting for bursting in replay detection, but track discriminability does not circumvent this. The way to account for lack of independence in time bins is to explicitly account for the bursting, such as fitting the place field estimates with a self-history term. The track discriminability measure simply marginalizes over the time and position and already incorporates the misspecified encoding model.</p></disp-quote><p>The log odds for each event quantified the relative probabilistic bias towards track 1 vs track 2 independently of the temporal fidelity of the replay sequence. While we relied on Track discriminability (computed from the log odds) to quantify how well a detected replay sequence was track-specific in its reactivation, we did not use log odds per se for any kind of adjustment or replay event selection. Track discriminability was only quantified using a distribution of replay events, with the expectation that the log odds separation between track 1+2 replay events increases with a better detection of track-specific replay events. This provided an opportunity to see how the log odds difference changed as a function of detection parameter such as ripple power, shuffle types and so on.</p><p>We adjusted the α level between replay detection methods, such that a similar proportion of false-positive events were obtained between methods, to compare the detection rate and track discriminability in a more equitable manner. Without doing this, stricter replay detection methods are expected to have a high track discriminability but lower detection rate; by matching the false positive rate, this “strictness” is matched in at least one respect. Nevertheless, even without any adjustment based on FPR, most findings comparing the mean log odds difference of different methods or behavioural states at an α level of 0.05 would still hold. This suggests that our results were not attributable to a specific method of adjustment.</p><p>We agree with the reviewer that one of the many factors that we were interested in was the impact of neuronal bursting on inflating the false positive rate of neuronal sequence detection. This was explored by comparing the detection performance using a rank-order correlation when either all spike times or only the median spike time from each neuron was included in the analysis. We found a significant increase in the false positive rate when including all spikes (presumably due to bursting). However, more generally for replay detection methods using decoding, it is the fact that correlations exist in the decoded bins for both the space and time dimension, and while the hope is that performing shuffles in these two dimension circumvents the problems of performing a statistical measure (e.g. weighted correlation) without having independent bins, we lack a ground truth to directly validate this assumption. While our log-odds metric was not used for replay detection, it is important to point out that it is less affected by neuronal bursting, as it is not measuring the “sequenceness” of a replay event.</p><p>We agree that there may be other possible methods (in addition to log odds) for measuring the “quality” of replay events. It is important to point out that our use of log-odds was not to optimize detection, but cross check events, to see which method of replay sequence detection was more optimal when comparing distributions of replay events detected.</p><disp-quote content-type="editor-comment"><p>2. This framework relies on classifying aspects of the replay event as significant or not significant at a given significance threshold. This is reliant on the particular null distribution specified.</p></disp-quote><p>We agree with the reviewer that replay sequence detection still relied on comparing sequence score of a given candidate relative to one or multiple null distribution specified. This is because this study was meant to evaluate existing replay detection methods (which all relied on this statistical approach) rather than designing novel approaches. Furthermore, to capture the performance of each method more comprehensively, we also showed the log odds difference and detection rate using a range of α levels (0.2 to 0.001).</p><disp-quote content-type="editor-comment"><p>3. The generality of the work is limited by their two track framing.</p></disp-quote><p>We agree with the reviewer that this work relies on a two track framework, which is less commonly used in replay studies than a single track. However, we do believe that differences between replay methods quantified here would hold for other studies regardless of whether two tracks or only a single track is used. It is worth noting that in the cases when only one track is used, it may be possible to quantify log odds by creating a virtual second track based on recorded place cell statistics or by comparing the discriminability across different portions/arms of the maze (depending on the experimental design), although this is beyond the scope of our current study. We have briefly discussed this on page 17 (discussion) in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>The work claims that their framework is a &quot;unifying approach to quantify and compare the replay detection performance&quot;. The work falls short of doing this in general but I do think it accomplishes this with a much narrower scope, namely in adjusting methods that fail to discriminate between tracks appropriately in the case of two tracks.</p></disp-quote><p>We agree with the reviewer that this sentence could be misleading and not well-phrased. We have removed the phrase “unifying framework”, and updated the manuscript to reflect the fact that this framework was accomplishing replay detection comparison within a narrower scope.</p><disp-quote content-type="editor-comment"><p>– Are the significant events being corrected for multiple comparisons? Could the authors explain why no corrections are needed?</p></disp-quote><p>While the likelihood of detecting any significant replay event increases with an additional track, our detection of replay (compared to a null distribution) is performed separately for each track. If each track forms an independent hypothesis (e.g. is this a track 1 replay event), there is only a single comparison being made, and no correction for multiple comparisons is needed.</p><disp-quote content-type="editor-comment"><p>– I do not think the authors sufficiently addressed the question of what happens if the track discrimination model is misspecified in their response. For example, if there are truly three tracks but you only account for two?</p></disp-quote><p>We agree with the reviewer that there are several assumptions about the track discrimination framework.</p><p>Firstly, it is assumed that only one track was usually replayed during a candidate ripple event, which might not be always true. However, based on our dataset, most methods would detect a rather low proportion of events being classified significant for both tracks. In addition, the proportion of multitrack events (out of all significant events at a given α level such as 0.05) were similar across PRE, RUN and POST and were comparable to the proportion of empirically measured false positive events obtained from a randomised dataset (see Figure 6- supplementary figure 6 in the revised manuscript). This suggests that most multi-track events we observed were probably mostly due to false positive detection (on one of the two tracks) rather than multiple track representations in a single replay event.</p><p>Furthermore, prior experience on other tracks not analysed in the log odds calculation should not pose any issue, given that the animal likely replays many experiences of the day (e.g. the homecage or rest pot). These “other” replay events likely contribute to candidate replay events that fail to have a statistically significant replay score on either track. Furthermore, even when the replay events of other track experiences were falsely detected based the sequential content, given that the representations of other experiences were sufficiently different from both tracks we analysed, the log odds measure for these events should be close to 0 as the representations should not be more similar to track 1 or track 2.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>While I appreciate the conceptual novelty of using the log odds ratio as a metric for testing the noisiness of replay events and comparing this metric with results from sequence analyses, unfortunately, I was not persuaded by the arguments presented in the authors' rebuttal. I think the study would benefit greatly if the authors consulted or collaborated closely with an expert on statistical methods, particularly on the application of resampling methods, to enhance the rigor of their approach. Without this, I remain concerned that this work can cause further confusion (and noise) rather than clarity in these analyses.</p><p>I believe I noted my concerns to great length in the initial review, but perhaps the authors will wish to look through the section &quot;The problem with replay decoding: shuffles&quot; section in Foster 2017 and note that each shuffle is permissive for some events and not others. E.g. if the circular shuffle is likely to pass through events that arise from edge effects, their study does not explain how a shift of the α level is going to fix that issue and remove false positives. Each type of shuffle will pass through different subsets of events, some of which may be &quot;false positives,&quot; but they will be different events in each case. Simply adjusting the α levels or p-values, the solution proposed by this study, will not fix the underlying reasons for the false positives, and lead to potentially mistaken confidence in the results.</p></disp-quote><p>We would like to thank the reviewer for the feedback and comment.</p><p>Firstly, we would like the reviewer for appreciating the novelty and potential usefulness of this crosschecking framework in evaluating replay under the constraints of having no ground truth for validation.</p><p>First, to address the concern about the use of an empirically estimated false positive rate based on cell Id randomization, we agree with the reviewer that an α level adjustment is not a quick fix to a poor method generally (although rank-order correlation did appear to be an exception to this). The most effective solution is using multiple, orthogonally-designed shuffles that directly address possible correlations in space and time found in place fields and spike trains (respectively). We have made this point clearer in our discussion, which is supported by our observation that decoding methods that use multiple shuffle types always perform better than an α-level adjusted single shuffle type method.</p><p>An ideal method will minimise false-positives and maximise true events, and by changing the α level threshold, you are reducing both, but not necessarily by the same amount (hopefully reducing false positives more than true events). But without matching the false-positive rate, it is impossible to directly compare methods.</p><p>Indeed, one of the main concerns about use of a cell-id randomized dataset is that it may be underestimating the false positive rate in a way that is biased towards specific shuffling distributions (I.e. place-based or time-based shuffling). In the original rebuttal letter, we discussed how use of cell id randomized dataset would be preferred over a place field randomized dataset as a cell id randomized dataset was not underestimating empirically-estimated false positive rates compared to place field randomized dataset. Here we extend the analysis to demonstrate that, in most cases, a cell id randomized dataset was also not underestimating empirically-estimated false positive rates compared to spike train shuffled dataset (Figure B). In particular, using shuffling procedures that disrupt place or temporal information causes a substantial reduction in the empirically-estimated false positive rates when they were applied to place field randomized dataset or spike train randomized dataset, respectively. In contrast, for most cases, a cell id randomized dataset would lead to highest empirically-estimated false positive rates, suggesting that, while imperfect, cell id randomization is the relatively more conservative null distribution for estimating false positive rates.</p><p>We have updated the manuscript to discuss further about the purpose and caveats/limitation of this FPR-based adjustment to avoid confusion and over-confidence about this false-positive estimates among the readers. We have also included Figure 1 – supplement 2 in the results and discussion of our manuscript</p><p>We have consulted with a statistician (Dr. Henrik Singmann) for this work, which is now noted in the acknowledgements.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I find that the revised manuscript has considerably improved. I have one outstanding issue with the randomization shuffle, which can actually be alleviated if the authors update Figure D to include the 3 remaining shuffles, and a minor comment.</p><p>Outstanding question regarding the randomization shuffle:</p><p>My concerns about the cell-id shuffle were that (1) any shuffle, including this one, may actually underestimate the true false positive rate, and (2) this tendency to underestimate the true false positive rate will be exacerbated when the randomization shuffle is similar to the shuffle used for detecting replay events, resulting in a biased comparison between methods.</p><p>I think (1) is unavoidable as it would be hard to disprove any structure that could be inadvertently erased by any given shuffle. The true false positive rate may always be higher than our best estimates captured by our surrogate datasets, and our estimates are simply a lower bound of the true false positive rate. In particular for the cell-id shuffle (including the &quot;cross experiment cell-id&quot; shuffle), there may be some underlying structure in biological data which violates assumptions of independence that is not captured by place-field swapping. This would undoubtedly be the case when including a variety of cell types with generally different place field properties (e.g. along the dorsoventral axis or the deep/superficial sublayer). Even without different anatomical cell types, more excitable cells may be more likely to have multiple place fields as well as firing more spikes within candidate events – a potential structure that is not captured by the cell-id shuffle.</p><p>While concern (1) only needs to be mentioned (in the discussion or in a public review), concern (2) is particularly problematic because the authors compare the (FRP-matched) performance of different methods to conclude which are best. Figure D shows a comparison of two methods: place field shifting and place field swapping (with place fields within the same dataset, the randomization used throughout the manuscript, or from a different dataset, which is almost identical to the original randomization) before trajectory decoding. Figure D convincingly argues that relative to field swapping (cell-id shuffle), field shifting (place field circular shifted shuffle) is a much poorer method of randomization as it considerably underestimates the false positive rate for detection methods using similar shuffles. Note that the bias is slightly attenuated for the &quot;place bin shuffle&quot; (it is worse for the &quot;place field shuffle&quot;), demonstrating that there is a confounding gradient with larger similarities between the randomization and the detection shuffles resulting in larger bias.</p><p>According to the authors, <xref ref-type="fig" rid="sa2fig4">Author response image 4</xref> shows that &quot;cell-id based randomization is sufficiently independent from both place-based and temporally-based shuffles for replay detection&quot;. I cannot see how that can be concluded from the figure. The place field swapping methods may well be underestimating the false positive rate unevenly. Place field swapping is still a pre-decoding place-based shuffle, so the fact that its estimate agrees with the estimate of another pre-decoding place-based shuffle (place field shifting) does not demonstrate that these estimates are not a biased. I would like to stress that one of the authors' conclusions is that &quot;more shuffles, with a preference for pre-decoding shuffles, lead to better replay detection.&quot; All of the approaches using &quot;more shuffles, with a preference for pre-decoding shuffles&quot; included place field shifting, which in my opinion is similar to the place field swapping of the cell-id shuffle.</p><p>I commend the authors for the approach of <xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>, which is a good way to address this key concern. I recommend this figure to be included in the manuscript. Moreover, if the authors were to include the other 3 shuffles (it currently includes &quot;Shuffle 2&quot; from the methods) and show that the cell-id does not underestimate the false positive rate more than those shuffles in a biased way, that would practically disprove this issue. Indeed, the concern is that the cell-id shuffle may be underestimating the false positive rate particularly for detection methods using pre-decoding and place-based shuffles, but if none of the other 3 shuffles produce higher false positive estimates for the lower row in Figure D, that would be convincing evidence that the cell-id randomization procedure is not biased towards detection methods employing pre-decoding place-based shuffles and strengthen the conclusions.</p></disp-quote><p>Thank you for your feedback.</p><p>[Editors’ note: what follows is the authors’ response to the third round of review.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>The introduction of the manuscript has already been significantly revised. However, the final revision requested by the reviewers is to spend a few sentences outlining not only sequence scoring, but also the fact that the manuscript utilises multiple types of surrogate data sets to establish null distributions. In particular, it is suggested that some assumptions about the utility of the cell-id shuffle surrogate be described, as well as the importance of track discriminability. Reviewer #2 (see below) is quite proscriptive; please consider their points and update the introduction to at least mention these issues up front.</p></disp-quote><p>We would like to thank the reviewers and editors for their feedback and comments. We agree with the reviewers that it is beneficial to include a section stating our framework’s assumptions including (1) the utility of using a randomized surrogate dataset to empirically estimate the false positive rate and (2) the value of measuring track discriminability for the events detected based on track-specific sequenceness. We have updated our manuscript based on the feedback here and feedback from Reviewer #2 (please see our detailed responses to Reviewer #2).</p><p>In particular, we included three main assumptions associated with this framework during introduction section.</p><p>Firstly, we stated that ‘for a given replay detection method, the proportion of spurious events (generated from randomized data) that are detected as significant events can be used to empirically estimate the proportion of non-replay events that are falsely labelled as significant replay events in real hippocampal data. However, this assumes that the null distribution used for creating randomized data is not used for detection, and that it does not underestimate the false-positive rates compared to the other null distribution(s) used for detection’. We wanted to highlight the utility of randomized dataset in estimating false positive rates with the caveat that this same method should not be used to create a null-distribution for replay detection and should not underestimate the false-positive rates compared to other null distributions. We hoped to introduce what a preferred null distribution for creating randomized dataset should look like without going too much into the specifics of shuffling procedures to early in the manuscript, which would detract from its readability. However, during the early part of the Results section when we first described our use of empiricallyestimated false positive rates, we now offer a more detailed description of the four potential null distributions for generating randomized datasets and explained why cell-id shuffling was the preferred choice in our study.</p><p>Secondly, we stated that ‘for neural data, pooled from multiple animals, with track-specific replay sequences but not spurious replay events, track discriminability should correlate with the sequenceness score of the track-specific replay’. We wanted to highlight that track discriminability measure should be able to reflect the overall fidelity of the replay events only when these events are detected from neural data that contain replay. When events were detected from a randomized dataset or from a real dataset that contains very few replay events, there should be a dissociation between the sequenceness score and track discriminability.</p><p>Lastly, we stated that ‘the empirically-estimated false positive rate can vary between methods, but can be adjusted by a scaling correction of the α level such that the performance of different replay methods can be evaluated and compared at an equivalent empirically-estimated false positive rate. Note that this method of scaling the α level is designed for method comparison only, and should not be meant as a substitute for using appropriate shuffle methods to create a sufficient set of null distributions for detection’. This assumption leads to our approach of applying a scaling correction to an equivalent empirically estimated false-positive rate, enabling more equitable comparisons of various replay detection methods. We wanted to emphasize that this approach was designed to facilitate fair comparison among methods, not to serve as a substitute for additional shuffle techniques in replay detection.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The revision added some text regarding the limitations of the study. I was hoping that the revised manuscript would also be more explicit regarding the underlying assumptions. While I don't think reviewers should do the authors work for them, after such a lengthy review, perhaps this is the only way short of a rejection.</p><p>From what I can tell, these are the main assumptions. If I am wrong, perhaps the other reviewers, or the authors can correct me. Also perhaps there are others that I missed.</p><p>1) After recording from two tracks, events with low discriminability between these two tracks cannot be said to be replays, regardless of their sequence scores.</p><p>– Pooled across data, discriminability generally correlates with replay score, though this differs across datasets and timepoints of recording.</p></disp-quote><p>We agree with reviewer that this framework assumed that detected replay events with low track discriminability are assumed to be false positive events, regardless of their sequence score. While this sequence score can be observed to correlate with track discriminability, this should not occur when using a randomized data set (composed only of false-positive events), or for a real dataset that does not contain replay events.</p><p>We have stated this assumption (sequence score correlating with track discriminability for real replay events but not spurious events) in the manuscript’s introduction</p><disp-quote content-type="editor-comment"><p>2) The cell-id shuffle provides the best null distribution to test against for replay.</p><p>– This is so because cell-id shuffled events show low discriminability even in instances where they have high sequence scores</p></disp-quote><p>We agree with reviewer that this framework assumed that a cell-id randomization can create a randomized dataset to measure the false-positive detection rate of a given replay detection method, with the caveat that this same method is not used to create a null-distribution for replay detection. It is true that cell-id randomized events show low track discriminability even in instances when they have high sequence scores. However, it is not a preferred null distribution for this reason, but rather because it is sufficiently different from the other shuffling methods, and as such does not underestimate the false positive rate.</p><p>We believe a discussion about why a particular null distribution is preferred over other null distributions during the early part of the introduction can distract the reader from the main points of the paper, and have moved this to the Results section. However, in the introduction section, we have decided to include our assumption that using a randomized surrogate dataset is useful for empirically estimating the false positive rate. This is based on the premise that the null distribution used to create the randomized dataset is not used for detection, and thus does not underestimate false positive rates compared to other null distributions used in detection. Furthermore, as mentioned in our response to the reviewer's first point, during the introduction, we have decided to introduce our assumption about how the sequence score would correlate with track discriminability for real replay events but not spurious events.</p><disp-quote content-type="editor-comment"><p>3) The proportion of cell-id shuffled events that qualify as replays in any given period is similar to the proportion of events generated in the real data from hippocampus network that are likewise not actual replay but are labeled as significantly ordered.</p><p>– This therefore allows for a scaling correction to bring this proportion in line with the α level.</p></disp-quote><p>We agree with reviewer that this framework assumed that the proportion of cell-id randomised events that were falsely detected as significant events would be similar to the proportion of nonreplay events that were falsely labelled as significant replay events in real hippocampal data. This assumption leads to our approach of applying a scaling correction to create an equivalent empirically-estimated false-positive rate, which would allow us to compare the performance of various replay detection methods. We want to emphasize that this approach is for comparing replay detection methods more equitably, but is not meant to be used as an alternative to using additional shuffle methods when performing replay detection.</p><p>We have included this assumption in the introduction section of the manuscript.</p><disp-quote content-type="editor-comment"><p>4) In the absence of performing cell-id shuffles, which is straightforward, the alternative prescription is to record from two tracks and adjust the apparent α to match those of cell-id shuffled events obtained during the recording</p><p>– This is so that the same relative number of events are labeled as replays, even though they may not be the actual replays.</p></disp-quote><p>We agree with the reviewer that this framework assumes that we can adjust the α level based on the empirically-estimated false positive rates such that the performance of various methods can be evaluated at a level that leads to similar relative number of false events. However, the adjustment is not designed to fix the detection issues and selectively remove true false positive events, rather it aims to permit an equitable comparison between methods. Furthermore, using a cell-id randomized null distribution does not solve the problem of not knowing equivalent falsepositive rates for each replay detection method, which would render any method comparison difficult.</p><p>We have included our assumption about the use of a scaling correction of α level such that the performance of various replay methods can be compared at an equivalent empirically-estimated false positive rate. However, we decided to keep the detailed discussion about the caveats of this approach still in the discussion part of the manuscript rather than move them to the introduction as we felt it would overload the readers too early on with detailed information in an unhelpful way.</p></body></sub-article></article>