<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89936</article-id><article-id pub-id-type="doi">10.7554/eLife.89936</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89936.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Task-specific invariant representation in auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Heller</surname><given-names>Charles R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9048-1201</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hamersky</surname><given-names>Gregory R</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>David</surname><given-names>Stephen V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4135-3104</contrib-id><email>davids@ohsu.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Neuroscience Graduate Program, Oregon Health and Science University</institution></institution-wrap><addr-line><named-content content-type="city">Portland</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Otolaryngology, Oregon Health &amp; Science University</institution></institution-wrap><addr-line><named-content content-type="city">Portland</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ding</surname><given-names>Nai</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Zhejiang University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>08</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP89936</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-06-15"><day>15</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-06-29"><day>29</day><month>06</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.29.547009"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-08-31"><day>31</day><month>08</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89936.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-18"><day>18</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89936.2"/></event></pub-history><permissions><copyright-statement>Â© 2023, Heller et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Heller et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89936-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89936-figures-v1.pdf"/><abstract><p>Categorical sensory representations are critical for many behaviors, including speech perception. In the auditory system, categorical information is thought to arise hierarchically, becoming increasingly prominent in higher-order cortical regions. The neural mechanisms that support this robust and flexible computation remain poorly understood. Here, we studied sound representations in the ferret primary and non-primary auditory cortex while animals engaged in a challenging sound discrimination task. Population-level decoding of simultaneously recorded single neurons revealed that task engagement caused categorical sound representations to emerge in non-primary auditory cortex. In primary auditory cortex, task engagement caused a general enhancement of sound decoding that was not specific to task-relevant categories. These findings are consistent with mixed selectivity models of neural disentanglement, in which early sensory regions build an overcomplete representation of the world and allow neurons in downstream brain regions to flexibly and selectively read out behaviorally relevant, categorical information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory cortex</kwd><kwd>population coding</kwd><kwd>behavior</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>GVPRS0015A2</award-id><principal-award-recipient><name><surname>Heller</surname><given-names>Charles R</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>DC0495</award-id><principal-award-recipient><name><surname>David</surname><given-names>Stephen V</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Engagement in auditory detection task leads to selective enhancement of task relevant information by neural populations in higher order auditory cortex.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Perceptual decision-making requires behavioral responses based on specific sensory patterns that ignore distracting and irrelevant information. In the auditory system, categorical sensory representation is essential to many natural behaviors (<xref ref-type="bibr" rid="bib7">Bizley and Cohen, 2013</xref>). For example, during language processing, vowels are perceived categorically, even though the formant frequencies that define them vary continuously across utterances (<xref ref-type="bibr" rid="bib25">Hillenbrand et al., 1995</xref>). Categorical perception is not limited to language, as subjects can learn to classify arbitrary, novel sounds according to one spectro-temporal feature while ignoring others (<xref ref-type="bibr" rid="bib54">Stilp and Kluender, 2010</xref>).</p><p>Neurophysiological studies in auditory cortex have shown that engaging in auditory behavior can enhance sensory discriminability at the level of single neurons (<xref ref-type="bibr" rid="bib10">Buran et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Niwa et al., 2012a</xref>) and neural populations (<xref ref-type="bibr" rid="bib4">Bagur et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Kuchibhotla et al., 2017</xref>). Most of this work has demonstrated a generalized, overall improvement in sensory coding without contrasting neural representations of task-relevant versus -irrelevant features. In frontal cortex, neurons often only encode sound categories (<xref ref-type="bibr" rid="bib19">Fritz et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Tsunada et al., 2011</xref>), suggesting that sound information is transformed into an invariant, categorical representation before exiting auditory cortex. Such representations require disentangling sensory features that are relevant for defining the object category from other features that are irrelevant to the category (<xref ref-type="bibr" rid="bib17">DiCarlo and Cox, 2007</xref>). Theory predicts that neural systems can produce these invariant representations through hierarchical computation. In early processing regions, mixed selectivity of single neurons produces high-dimensional, overcomplete representations of sensory inputs and behavioral variables. From this population activity, it is straightforward for neurons in downstream areas to decode information about a specific feature that is important to the current behavior and whose representation is invariant to irrelevant sensory information (<xref ref-type="bibr" rid="bib29">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Rigotti et al., 2013</xref>).</p><p>We hypothesized that invariant auditory representations supporting perceptual discrimination arise through a behavior-dependent hierarchical process, consistent with mixed selectivity models. According to this model, engaging in an auditory behavior leads to a non-specific enhancement of auditory representations at early stages, followed by a selective enhancement of task-relevant features at later stages. Previous work has shown that the effects of task engagement are larger in non-primary auditory fields (<xref ref-type="bibr" rid="bib2">Atiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Kline et al., 2023</xref>; <xref ref-type="bibr" rid="bib38">Niwa et al., 2013</xref>), as are the effects of selective attention, which may be related to invariant sound feature coding (<xref ref-type="bibr" rid="bib39">OâSullivan et al., 2019</xref>). Some studies have also reported that choice-related activity emerges in non-primary auditory cortex during a challenging perceptual discrimination behavior (<xref ref-type="bibr" rid="bib58">Tsunada et al., 2016</xref>), although factors affecting choice coding may be task-dependent (<xref ref-type="bibr" rid="bib8">Bizley et al., 2013</xref>). Together, these findings are consistent with the idea that behaviorally relevant neural representations are computed hierarchically in auditory cortex (<xref ref-type="bibr" rid="bib32">Lestang et al., 2023</xref>).</p><p>To investigate the emergence of invariant sound coding, we recorded neural population activity from primary and non-primary fields of the ferret auditory cortex while animals alternated between active tone-in-noise detection and passive listening to task stimuli. We designed the task so that behavioral sessions contained multiple different target and distractor sounds and used decoding analysis to measure how neural populations discriminate between both task-relevant and -irrelevant sound features. For this analysis, we developed decoding-based dimensionality reduction (dDR), which projects neural activity into a low-dimensional subspace spanning both changes in mean firing rate between categories and covariability across trials (<xref ref-type="bibr" rid="bib23">Heller and David, 2022</xref>) dDR prevents bias that can affect population decoding in behavioral studies with relatively limited numbers of trials (<xref ref-type="bibr" rid="bib28">Kanitscheider et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Moreno-Bote et al., 2014</xref>). Effects of task engagement were highly variable across individual neurons, but the population-level analysis revealed that sound coding in primary auditory cortex was broadly and non-specifically improved by task engagement. In contrast, an enhanced, selective representation of task-relevant features emerged in non-primary auditory cortex. The degree of task-relevant enhancement was correlated with behavioral performance, consistent with the hypothesis that categorical representations in non-primary auditory cortex inform behavioral choices.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Psychometric tone-in-noise detection behavior</title><p>To study how neural representations of sound category emerge in auditory cortex, we trained four male ferrets to perform a go/no-go tone-in-noise detection task. Animals reported the occurrence of a target tone in a sequence of narrowband noise distractors by licking a piezo spout (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, Methods: Behavioral paradigm, distractor stimulus sound level: 60 dB SPL). Targets were presented with variable signal-to-noise ratio (SNR), masked by noise centered at the same frequency. We describe SNR as the overall SPL of the target relative to the distractor noise level. Thus, an SNR of â5 dB corresponds to a target level of 55 dB SPL while an Inf dB SNR corresponds to a target tone presented without any masking noise. A subset of behavioral trials in each experiment included an explicit catch stimulus whose center frequency was matched to that of the target tone. This task design permitted us to probe neural coding of both task-relevant sound features (presence or absence of a target tone) and -irrelevant features (level of noise masking the target tone, <xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Tone-in-noise detection behavior.</title><p>(<bold>a</bold>) Schematic of go/no-go tone-in-noise detection task. Licking responses to target tones were rewarded, while responses to narrowband noise distractors were penalized with a timeout. Target tone frequency was fixed during a single behavior session and masked by narrowband (0.3 octave) noise centered at the same frequency with variable signal-to-noise ratio (SNR). Variable SNR was achieved by varying the overall SPL of the target relative to the fixed (60 dB SPL) distractor noise, e.g., â5 dB SNR corresponds to a 55 dB SPL target with 60 dB SPL masking noise. Infinite (inf) dB SNR corresponds to a target tone presented in isolation (60 dB SPL). The âCatchâ distractor was identical to the masking noise but with no tone. (<bold>b</bold>) Behavioral performance of individual animals as a function of SNR (<italic>d-prime</italic>=<italic>Z</italic>[target response rate] - <italic>Z</italic>[catch response rate], n=4 animals). Black lines and error bars indicate the mean and standard error of the mean across animals. (<bold>c</bold>) Left: Stimulus set for an example experiment where the target tone frequency was 2828 Hz. Right: both task-relevant (catch vs. target) and task-irrelevant (target vs. target, distractor vs. distractor) sound discriminations were studied.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1âfigure supplement 1.</label><caption><title>Behavioral performance of individual animals.</title><p>Cumulative reaction time histogram for each animal and target sound across all behavior sessions. Color indicates sound identity. Animals were rewarded for responses to all sounds except the catch.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig1-figsupp1-v1.tif"/></fig></fig-group><p>Behavioral performance was measured using d-prime (<xref ref-type="bibr" rid="bib21">Green and Swets, 1966</xref>; <xref ref-type="bibr" rid="bib46">Saderi et al., 2021</xref>), calculated as the z-scored hit rate for a given target minus the z-scored catch response rate. Across behavioral sessions and animals, performance showed a clear psychometric dependence on target SNR (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1âfigure supplement 1</xref>). All animals could easily discriminate between the pure tone (Inf dB) target and catch stimulus, while performance for lower SNR target stimuli approached the chance level.</p></sec><sec id="s2-2"><title>Diverse effects of task engagement on single neurons in primary and non-primary auditory cortex</title><p>We used linear 64-channel silicon probes (<xref ref-type="bibr" rid="bib51">Shobe et al., 2015</xref>) to record single-unit activity from primary (A1) and non-primary (dPEG) auditory cortex while animals performed the tone-in-noise detection task. Recordings were targeted to each respective region based on functional mapping of neural responses (Methods, <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). Behavior alternated between blocks of active task engagement and passive listening to task stimuli. During passive listening, licking responses were not rewarded and animals quickly disengaged from the task (<xref ref-type="bibr" rid="bib14">David et al., 2012</xref>).</p><p>Sound-evoked spiking activity was compared between active and passive states to study the impact of task engagement on sound representation. In both A1 and dPEG, responses to target and catch stimuli were significantly discriminable for a subset of single neurons (about 25% in both areas, <xref ref-type="fig" rid="fig2">Figure 2AâC</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplements 2</xref>â<xref ref-type="fig" rid="fig2s4">4</xref>, bootstrap test). This supports the idea that stimulus identity can be decoded in both brain regions, regardless of task performance. However, the fact that the responses of most neurons in both brain areas could not significantly discriminate target vs. catch stimuli also highlights the diversity of sound encoding observed at the level of single neurons. The accuracy of catch vs. target discrimination for each neuron was quantified using neural d-prime, the z-scored difference in target minus catch spiking response for each neuron (Methods: Single neuron PSTHs and d-prime <xref ref-type="bibr" rid="bib36">Niwa et al., 2012a</xref>). Task engagement was associated with significant changes in catch vs. target d-prime for roughly 10% of neurons in both A1 (40/481 neurons, bootstrap test) and dPEG (33/377 neurons, bootstrap test). This included neurons that both increased their discriminability and decreased their discriminability (<xref ref-type="fig" rid="fig2">Figure 2DâE</xref>). Thus, the effects of task engagement at the level of single neurons were relatively mild and inconsistent across the population; many neurons showed no significant change, and of those that did, the effects were bidirectional (<xref ref-type="fig" rid="fig2">Figure 2DâE</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>State-dependent modulation of single neuron target vs. catch discrimination.</title><p>(<bold>a</bold>) Example peristimulus time histogram (PSTH) responses from a single recording site in A1. Heatmap color in each row indicates the PSTH amplitude of one neuron. Dashed lines indicate sound onset/offset. Spikes were binned (20 ms), z-scored, and smoothed (Ï=30 ms Gaussian kernel). Example target responses are to the pure tone (Inf dB) target. Difference is computed as the z-scored response to the target minus the z-scored catch response (resulting in a difference shown in units of z-score). (<bold>bâc</bold>) Mean z-scored response evoked by-catch vs. Inf dB stimulus for each A1 neuron (n=481 neurons) across passive (<bold>b</bold>) and active (<bold>c</bold>) trials. Responses were defined as the total number of spikes recorded during the 300ms of sound presentation (area between dashed lines in panel A). Neurons with a significantly different response to the catch vs. target stimulus are indicated in black and quantified on the respective figure panel. (<bold>d</bold>) Histogram plots the state-dependent change in target vs. catch stimulus discriminability for each A1 neuron. Neural d-prime is defined |Z[target] - Z[catch]|, and Îd-prime is the difference of active minus passive d-prime. The distribution of neurons with a significant change in d-prime between passive and active conditions is overlaid in black. (<bold>e</bold>) Histogram of Îd-prime for dPEG neurons, plotted as in D.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Penetration map for one example animal.</title><p>Left hemisphere from one animal. Each marker indicates the anatomical location of a single electrode penetration. Color indicates the average best frequency (BF) of neurons recorded at each location and marker type indicates which brain region each penetration belongs to. Overlaid black lines represent a rough estimate of region boundaries. A1: Primary auditory cortex, PSF/PPF: Anterior/posterior fields of secondary auditory cortex (dPEG).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>State-dependent modulation of singe neuron target vs. catch discrimination in dPEG.</title><p>(<bold>a</bold>) Example peristimulus time histogram (PSTH) responses from a single recording site in dPEG. Heatmap color in each row indicates the PSTH amplitude of one neuron. Dashed lines indicate sound onset/offset. Spikes were binned (20 ms), z-scored, and smoothed (30 ms Gaussian kernel). Example target responses are to the pure tone (Inf dB) target. Difference is computed as the z-scored response to the target minus the z-scored catch response (resulting in a difference shown in units of z-score). (<bold>b, c</bold>) Mean z-scored response evoked by catch vs. Inf dB stimulus for each dPEG neuron across passive (<bold>b</bold>) and active (<bold>c</bold>) trials. Responses were defined as the total number of spikes recorded during the 300 ms of sound presentation (area between dashed lines in panel A). Neurons with a significantly different response to the catch vs. target stimulus are indicated in black and quantified on the respective figure panel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 3.</label><caption><title>Single neuron target vs. catch raster plots for all A1 recording sites.</title><p>Example peristimulus time histogram (PSTH) responses from all recording sites in A1. Each group of six panels corresponds to a single recording site, as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Heatmap color in each row of each panel indicates the PSTH amplitude of one neuron. Dashed lines indicate sound onset/offset. Spikes were binned (20 ms), z-scored, and smoothed (30 ms Gaussian kernel). Example target responses are to the pure tone (Inf dB) target. Difference is computed as the z-scored response to the target minus the z-scored catch response (resulting in a difference shown in units of z-score).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 4.</label><caption><title>Single neuron target vs. catch raster plots for all dPEG recording sites.</title><p>Example peristimulus time histogram (PSTH) responses from all recording sites in dPEG. Each group of six panels corresponds to a single recording site, as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Heatmap color in each row of each panel indicates the PSTH amplitude of one neuron. Dashed lines indicate sound onset/offset. Spikes were binned (20 ms), z-scored, and smoothed (30 ms Gaussian kernel). Example target responses are to the pure tone (Inf dB) target. Difference is computed as the z-scored response to the target minus the z-scored catch response (resulting in a difference shown in units of z-score).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig2-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Population coding of task-relevant features is selectively enhanced in non-primary auditory cortex</title><p>Given the diversity of task-related changes in neural activity, we asked if a clearer pattern of task-dependent changes could be observed at the population level. We performed optimal linear decoding of task stimuli from the single-trial activity of simultaneously recorded neurons at each recording site. We quantified decoding performance with neural population d-prime (Methods: Neural population d-prime <xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>; <xref ref-type="bibr" rid="bib36">Niwa et al., 2012a</xref>). To prevent overfitting and allow visualization of population responses, we first projected single trial activity into a low-dimensional subspace optimized for linear decoding of task stimuli (<xref ref-type="fig" rid="fig3">Figure 3</xref>; <xref ref-type="bibr" rid="bib23">Heller and David, 2022</xref>). In both A1 and dPEG, population d-prime for catch versus target stimuli consistently increased during task engagement. In A1, the increase in d-prime was consistent across all task categories; there was no difference between target vs. target and target vs. catch discrimination accuracy (<xref ref-type="fig" rid="fig3">Figure 3BâC</xref>). However, in dPEG the improvement of task-relevant catch vs. target discrimination was significantly larger than in any other category (<xref ref-type="fig" rid="fig3">Figure 3EâF</xref>). Unlike A1, discrimination of task-relevant sound categories was selectively enhanced in non-primary auditory cortex.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Selective enhancement of task-relevant category representation in the secondary auditory cortex.</title><p>(<bold>a</bold>) Left: Representative A1 population activity during passive listening projected into a two-dimensional space optimized for discriminating target versus catch responses. Each dot indicates the population response on a single trial, color indicates different noise (catch) or tone-in-noise (target) stimuli, and ellipses describe the standard deviation of responses across trials. The degree of ellipse overlap provides a visualization of the neural discriminability (d-prime) between the corresponding stimuli. Right: A1 population activity during active behavior. (<bold>b</bold>) Mean population d-prime between sounds from each category (target vs. catch, target vs. target, and distractor vs. distractor, <xref ref-type="fig" rid="fig1">Figure 1C</xref>) for each A1 recording site (n=18 sessions, n=3 animals). (<bold>c</bold>) Îd-prime is the difference between active and passive d-prime, normalized by their sum (D vs. D / T vs. T p=0.048, Wilcoxon signed-rank test). Each small dot represents the mean for a single A1 recording site, as in panel b (n=18 sessions, n=3 animals). Large dots and error bars represent the mean and standard error across sessions. (<bold>d</bold>) Single-trial population responses for a single site in non-primary auditory cortex (dPEG), plotted as in A. (<bold>e</bold>) Passive vs. Active category discriminability for dPEG recording sites, plotted as in B (n=12 sessions, n=4 animals). (<bold>f</bold>) Data shown as in panel c but for changes in discriminability per category in dPEG (n=12 sessions, n=4 animals). Îd-prime for target vs. catch pairs (T vs. C) was significantly greater than for the other categories (D vs. D: p=0.003; T vs. T: p=0.005, Wilcoxon signed-rank test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Pupil dynamics reflect both generalized arousal as well as trial outcome.</title><p>(<bold>a</bold>) Top: Histogram of pupil size across all experiments (n=39 sessions, n=4 animals) during active (orange) and passive (blue) trials. Pupil size is normalized to the max pupil size observed within each experiment. Bottom: Mean pupil size +/-the standard error under each condition. Data were first averaged within each experiment before computing the mean, and standard error across experiments, and before performing the significance test. Pupil size was significantly larger on active trials than on passive trials (p=1.48e-6, Wilcoxon signed rank test) (<bold>b</bold>) Top: Raw, trial-weighted average of per-trial pupil on hit (dark blue), correct reject (light blue), false alarm (dark red), and miss (light red) trials. Bottom: Summary of the mean pre-trial pupil for each type of trial outcome. Data were first averaged within each experiment before computing the mean, and the standard error, and before performing significance tests (hit vs. correct reject p=0.36, hit vs. miss p=8.81e-5, hit vs. false alarm p=2.83e-5, Wilcoxon signed-rank test). (<bold>c</bold>). Top: Same as in (<bold>b</bold>) after first normalizing pupil size to the pretrial mean. Bottom: Summary of the max change in pupil size during each trial for each type of trial outcome. Data were first averaged within each experiment before computing the mean, and the standard error, and before performing significance tests (hit vs. correct reject p=0.29, hit vs. miss p=3.01e-5, hit vs. false alarm p=1.62e-5, Wilcoxon signed-rank test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 2.</label><caption><title>Selective enhancement of task-relevant category representation in the secondary auditory cortex is not affected by global arousal.</title><p>Same as in <xref ref-type="fig" rid="fig3">Figure 3</xref>, without first correcting for pupil-indexed arousal explainable variance (i.e. decoding stimulus identity from raw neural activity, without controlling for global arousal). (<bold>a</bold>) Mean population d-prime between sounds from each category (target vs. catch, target vs. target, and distractor vs. distractor, <xref ref-type="fig" rid="fig1">Figure 1C</xref>) for each A1 recording site (n=18 sessions, n=3 animals). (<bold>b</bold>) Îd-prime is the difference between active and passive d-prime, normalized by their sum (no significant difference, Wilcoxon signed-rank test). Each small dot represents the mean for a single A1 recording site, as in panel b (n=18 sessions, n=3 animals). Large dots and error bars represent the mean and standard error across sessions. (<bold>c</bold>) Passive vs. Active category discriminability for dPEG recording sites, plotted as in A (n=12 sessions, n=4 animals). (<bold>d</bold>) Data plotted as in panel b for changes in discriminability per category in dPEG (n=12 sessions, n=4 animals). Îd-prime for target vs. catch pairs (T vs. C) was significantly greater than for the other categories (D vs. D: p=0.013; T vs. T: p=0.017, Wilcoxon signed-rank test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Prior work has demonstrated that generalized, pupil-indexed arousal can impact the responses of neurons in auditory cortex, independent of engagement in a specific task (<xref ref-type="bibr" rid="bib33">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Schwartz et al., 2020</xref>). Importantly, task engagement is often correlated with increased arousal (<xref ref-type="bibr" rid="bib15">de Gee et al., 2022</xref>; <xref ref-type="bibr" rid="bib46">Saderi et al., 2021</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>). To ensure that our results were not influenced by non-specific effects of arousal, decoding analysis was performed after first removing all spiking variability that could be explained using pupil-indexed arousal (Methods). Performing the same decoding analysis without first controlling for pupil size did not affect the selective enhancement that we observed in dPEG (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>). However, Î d-prime, in both A1 and dPEG, was higher overall. The absence of a pupil effect on selectivity suggests that pupil-indexed arousal primarily operates on an orthogonal subspace to the global task engagement axis and tends to non-specifically improve coding accuracy.</p><p>In addition to reflecting overall arousal level, pupil size has also been reported to reflect more nuanced cognitive variables such as, for example, listening effort (<xref ref-type="bibr" rid="bib60">Zekveld et al., 2014</xref>). Furthermore, rodent data suggests that optimal sensory detection is associated with intermediate pupil size (<xref ref-type="bibr" rid="bib33">McGinley et al., 2015</xref>), consistent with the hypothesis of an inverted-U relationship between arousal and behavioral performance (<xref ref-type="bibr" rid="bib60">Zekveld et al., 2014</xref>). To determine if this pattern was true for the animals in our task, we measured the dynamics of pupil size in the context of behavioral performance. Across animals, task stimuli evoked robust pupil dilation that varied with the trial outcome (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1bâc</xref>). Notably, pre-trial pupil size was significantly different between correct (hit and correct reject), hit, and miss trials (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1bâc</xref>), recapitulating the finding of an inverted-U relationship to performance in rodents (<xref ref-type="bibr" rid="bib33">McGinley et al., 2015</xref>). Since we focused only on correct trials in our decoding analysis, these outcome-dependent differences in pupil size are unlikely to contribute to the emergent decoding selectivity in dPEG.</p></sec><sec id="s2-4"><title>Behavioral performance is correlated with neural coding changes in non-primary auditory cortex only</title><p>If task-related changes in neural coding are linked to processes that guide behavior, then the changes in neural activity should be predictive of behavioral performance (<xref ref-type="bibr" rid="bib58">Tsunada et al., 2016</xref>). While selective enhancement of task-relevant discriminability was observed only in dPEG, both areas showed an overall increase in sensory discriminability. We asked if either of these changes in neural decoding performance were predictive of behavioral performance. For each tone-in-noise target stimulus, we compared the task-related change in neural d-prime to behavioral d-prime in the same experiment. We found a significant correlation for populations in dPEG, but not in A1 (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Thus, the task-specific changes in dPEG are coupled with the behavioral output reflecting those sound features.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Changes in neural decoding are correlated with behavior performance in dPEG, but not A1.</title><p>(<bold>a</bold>) Scatter plot compares neural Îd-prime (active minus passive) for all tone-in-noise target vs. catch noise combinations against the corresponding behavioral d-prime for that target vs. catch discrimination. Line shows the best linear fit, and shading represents a bootstrapped 95% confidence interval for slope. Left, data from A1 (n=60 unique target vs. catch combinations, n=3 animals, 18 recording sessions). Right, data from dPEG (n=44 unique target vs. catch combinations, n=4 animals, 12 recording sessions). (<bold>b</bold>) Pearson correlation between neural d-prime and behavioral d-prime in each brain region. Error bars indicate bootstrapped 95% confidence intervals (A1: p=0.082; dPEG: p=0.002, bootstrap test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>Choice decoding in auditory cortex primarily reflects impulsivity.</title><p>(<bold>a</bold>) Mean choice probability on hit vs. miss trials across all A1 experiments, split by target stimulus signal-to-noise ratio (SNR). Gray shading represents the time when the target sound stimulus is on. The vertical black line indicates when animals could begin licking in response to the target sound. Choice decoding is shown for the first distractor sound presentation on each trial (curves on the left) and for the target sound presentation (curves on the right). (<bold>b</bold>). Comparison of mean choice probability during the first distractor sound (Early) to the mean choice probability during the target sound (Late). Each gray line represents one experiment, and the black line is the mean across experiments. Only bins prior to the lick window onset were analyzed to avoid motor activity confounds. There was no significant difference between early and late choice probability (p&gt;0.05, Wilcoxon signed-rank test). (<bold>c, d</bold>) Same as a-b for dPEG experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig4-figsupp1-v1.tif"/></fig></fig-group><p>The previous analysis suggests that the task-dependent increase in stimulus information present in dPEG population activity is predictive of overall task performance. Next, we asked whether the population activity in either brain region was directly predictive of behavioral choice on single hit vs. miss trials. To do this, we conducted a choice probability analysis (Methods). We found that in both brain regions choice could be decoded well above chance level (<xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). Choice information was present throughout the entire trial and did not increase during the target stimulus presentation. This suggests that the difference in population activity primarily reflects a cognitive state associated with the probability of licking on a given trial, or âimpulsivityâ rather than âchoice.â This interpretation is consistent with our finding that baseline pupil size on each trial is predictive of trial outcome (<xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1b</xref>).</p></sec><sec id="s2-5"><title>Changes in evoked response gain, not shared population covariability, support the emergence of categorical representations in non-primary auditory cortex</title><p>The difference in task-dependent coding between A1 and dPEG could be explained by differential changes in the evoked responses of single neurons, patterns of covariability between neurons, or both (<xref ref-type="bibr" rid="bib12">Cohen and Maunsell, 2009</xref>; <xref ref-type="bibr" rid="bib13">Cowley et al., 2020</xref>). To measure task-dependent changes in covariability, we used factor analysis to model low-dimensional correlated activity in the neural population (Methods: Factor analysis). We found that covariability patterns changed significantly between the passive and active states (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). In both brain regions, task engagement caused a rotation of the principal covariability axis, consistent with an overall decorrelation of population activity (<xref ref-type="bibr" rid="bib59">Umakantha et al., 2021</xref>). In theory, a rotation could either help, or hurt, decoding accuracy, depending on its alignment with the sound discrimination axis (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Therefore, we measured the alignment of population covariability with the sound discrimination axis in both passive and task-engaged states (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Surprisingly, in A1, task engagement caused covariability to become more aligned with the sound discrimination axis. These results are consistent with a model in which the principal covariability axis does not represent information limiting noise in early sensory areas (<xref ref-type="bibr" rid="bib27">Kafashan et al., 2021</xref>), but instead reflects top-down, task-dependent gain modulation becoming more aligned with the task-relevant coding axis (<xref ref-type="bibr" rid="bib16">Denfield et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Goris et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Rabinowitz et al., 2015</xref>). Conversely, in dPEG alignment was low in both the passive and engaged states, consistent with covariability reflecting non-sensory variables that do not directly interact with the processing of the sensory stimulus (<xref ref-type="bibr" rid="bib55">Stringer et al., 2019a</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Task-related changes in shared population covariability do not impact the coding of task-relevant features.</title><p>(<bold>a</bold>) Schematic of population response over many trials to a catch stimulus (gray) and target stimulus (red), projected into a low-dimensional space. Dashed line indicates the sensory discrimination axis and the gray line indicates the axis of shared variability across trials during passive listening. Black lines indicate possible rotations in the axis of shared variability either toward or away from the discrimination axis during the task-engaged state. A larger angle (Î¸) between the shared variability and the discrimination axes leads to increased discrimination accuracy. (<bold>b</bold>) Alignment (cosine similarity) between the discrimination and shared variability axes during passive and active conditions. Error bars represent the standard error of the mean. The axes become more aligned during task engagement in A1 (p&lt;0.001, Wilcoxon signed-rank test) and do not change in dPEG. (<bold>c</bold>) Mean selective enhancement of neural target vs. catch discriminability across recording sites for simulated and actual data. Shading represents the standard error of the mean across experiments (A1: n=18 sessions, n=3 animals, dPEG: n=12 sessions, n=4 animals). Selective enhancement is the difference in Îd-prime for target vs. catch and target vs. target (inset). Simulations sequentially introduced task-dependent changes in mean sound-evoked response gain, single neuron variance, and population covariance matching changes in the actual neural data. (<bold>d</bold>) Model performance is defined as the correlation coefficient between simulated and actual selective enhancement across all sessions. Performance of each model was evaluated against the performance of the shared variance model to check for stepwise improvements in predictions. Stars indicate significance at alpha = 0.05 level, bootstrap test. Colors indicate brain regions: dPEG/black, A1 /gray.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>Modulation of population covariability metrics by task engagement.</title><p>Population metrics extracted from factor analysis model fit. Models were fit to the catch stimulus only to control for variability due to stimulus identity. <italic>P</italic>=passive, A=active. (<bold>a</bold>) Loading similarity is defined as the similarity of the neuronal loading weights onto the primary covariability axis (largest variance factor). Ranges from 0 (dissimilar) to 1 (similar). Loading similarity was significantly larger during passive conditions for both brain regions (A1: p=6.8e-5, n=18 sessions/3 animals; dPEG: p=6.9e-5, n=12 sessions/4 animals; Wilcoxon signed-rank test). (<bold>b</bold>) Dimensionality is defined as the number of significant dimensions of shared variability in the data determined by log-likelihood. No significant change in dimensionality was observed in either brain region. (<bold>c</bold>) % shared variance is defined as the percentage of single neuron variance that can be explained by the shared, covariability axes. Ranges from 0 to 1. No significant change in % shared variance was observed in either brain region. In all panels, dots represent the mean across sessions (A1: n=18 sessions, n=3 animals; dPEG: n=12 sessions, n=4 animals) and error bars show the standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>Modeling task-dependent changes in shared population covariability improves predictions of decoding changes in A1, but not dPEG.</title><p>(<bold>a</bold>) Mean Îd-prime across all target vs. target and target vs. catch stimulus pairs for each factor analysis based simulation: Null model, gain only model, independent variance model, and shared variance model. Actual Îd-prime is shown on the right. Shading represents the standard error of the mean across experiments (A1: n=18 sessions, n=3 animals, dPEG: n=12 sessions, n=4 animals). (<bold>b</bold>) Model performance is measured as the Pearson correlation between actual and predicted Îd-prime. In A1, model performance increases monotonically. In dPEG, including gain and independent variance both improve predictions but there is no significant improvement when including shared population covariability. Significance measured using the bootstrap test with alpha = 0.05. (<bold>c</bold>) Scatter plots show predicted vs. actual Îd-prime for each factor analysis simulation in both dPEG (black) and A1 (gray). Model performance (shown in B) was measured as the Pearson correlation of these scatter plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89936-fig5-figsupp2-v1.tif"/></fig></fig-group><p>To directly measure how these population-level changes relate to sound representation and emergent behavioral selectivity in non-primary auditory cortex, we performed simulations based on factor analysis model fits in which we sequentially introduced task-dependent changes in mean sound-evoked response gain, single neuron variance, and population covariance matching changes in the actual neural data (Methods: Factor analysis â Simulations). A simulation in which population covariability was fixed and only the evoked response gain changed between passive and active conditions (gain only) was sufficient to produce task-relevant selectivity in non-primary auditory cortex (<xref ref-type="fig" rid="fig5">Figure 5CâD</xref>). Thus, task-dependent changes in evoked response gain, not population covariability, support the emergence of a behaviorally relevant population code in non-primary auditory cortex.</p><p>This result is consistent with the fact that population covariability did not change in a systematic way with respect to the sound discrimination axis in non-primary auditory cortex (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). However, in A1 this was not the case. Therefore, we hypothesized that in A1 modeling changes in covariability would be required to explain the observed task-dependent changes in generalized sound discriminability. Indeed, we observed monotonic improvement in the modelâs ability to predict overall Îd-prime in A1, confirming that the shared variability model captured real aspects of shared population covariability that contribute to the accuracy of sound representation in A1 (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>).</p><p>Finally, we used the same simulation approach to determine what aspects of population activity carry the âchoiceâ related information we observed in A1 and dPEG (<xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>). Similar to our findings for stimulus decoding, we found that gain modulation alone was sufficient to recapitulate the choice information present in the raw data for this task. This helps frame prior work that pooled neurons across sessions to study population coding of choice in similar auditory discrimination tasks (<xref ref-type="bibr" rid="bib11">Christison-Lagay et al., 2017</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We observed distinct changes in how neural populations represent sound categories between primary (A1) and non-primary (dPEG) auditory cortex during a challenging tone-in-noise task. In A1, task engagement improved neural coding uniformly across all sound categories, both relevant and irrelevant to the current task. In dPEG, on the other hand, the neural population selectively enhanced the representation only of sound categories relevant to the tone-in-noise behavior. Task-dependent changes in neural response gain were sufficient to account for this emergent selectivity. In addition, we observed striking changes in population-level correlated activity that were strongly dependent on brain region. The pattern of task-related effects is consistent with a hierarchical, mixed selectivity model of sensory decision-making (<xref ref-type="bibr" rid="bib43">Rigotti et al., 2013</xref>). Neural populations in early brain areas form a sparse, overcomplete representation of sensory inputs, which supports a simple linear readout of task-relevant features in downstream areas. The selective changes are measurable only at the population level in dPEG, but a subsequent stage of processing would support category-specific coding by single neurons, as in frontal cortex (<xref ref-type="bibr" rid="bib19">Fritz et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Tsunada et al., 2011</xref>).</p><sec id="s3-1"><title>Emergent invariant, behaviorally relevant sound representations in non-primary auditory cortex</title><p>It has been proposed that auditory cortex dynamically computes representations of task-relevant sound features from non-specific spectro-temporal inputs to A1 (<xref ref-type="bibr" rid="bib32">Lestang et al., 2023</xref>). Prior work has established that task-dependent modulation of auditory responses is larger in non-primary versus primary fields of auditory cortex (<xref ref-type="bibr" rid="bib2">Atiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Kline et al., 2023</xref>; <xref ref-type="bibr" rid="bib38">Niwa et al., 2013</xref>). In our study, we asked how specific these changes in neural activity are to the encoding of task-relevant vs. irrelevant sound features. We found a clear dissociation between cortical fields; behaviorally relevant representations first emerged in the non-primary field, dPEG. In the frontal cortex (FC) of ferrets engaged in a similar tone detection behavior, single-neuron activity is behaviorally gated. Responses to a set of target tones are strongly enhanced when they require a behavioral response (<xref ref-type="bibr" rid="bib19">Fritz et al., 2010</xref>). In our task, the enhanced category representation in dPEG supports a simple linear readout of âgoâ versus âno-goâ categories that could provide input to category-specific neurons in FC.</p><p>Consistent with the hypothesis that the selective enhancement of category representation is causally related to behavior, we found that decoding accuracy in dPEG tracked the animalâs behavioral performance. This correlation between neural activity and behavioral performance is consistent with previous observations that choice-related activity is present or is stronger in non-primary versus primary auditory cortex (<xref ref-type="bibr" rid="bib8">Bizley et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Tsunada et al., 2016</xref>). However, some other studies have reported choice-related activity emerging as early as A1 (<xref ref-type="bibr" rid="bib37">Niwa et al., 2012b</xref>; <xref ref-type="bibr" rid="bib48">Selezneva et al., 2006</xref>), suggesting that the role of different cortical regions in decision-making may depend on aspects of the task, including the specifics of the auditory stimuli and the associated motor response. Future studies that precisely stimulate or suppress activity along the auditory pathway may definitively probe the causal role that each region plays in auditory perception.</p></sec><sec id="s3-2"><title>Population activity reveals hierarchically organized representations in the auditory system</title><p>Despite the diverse task-dependent changes in sound representations across individual neurons, analysis of sound discriminability at the population level revealed striking qualitative differences between A1 and dPEG. This finding highlights the value of studying auditory coding at the level of neural populations, which provides a more complete assessment of system-wide function than individual neurons (<xref ref-type="bibr" rid="bib4">Bagur et al., 2018</xref>; <xref ref-type="bibr" rid="bib32">Lestang et al., 2023</xref>). In our analysis, we defined population coding accuracy as the amount of stimulus information an ideal observer could extract from simultaneously recorded population activity. Thus, our results should be interpreted as an upper bound on the information transmitted by a group of neurons about a particular stimulus. Critically, these measures do not necessarily reflect the information utilized by the animal. In A1, for example, we found a decoding axis for every stimulus category along which task engagement improved sound representation. Despite this global improvement in A1, decoding downstream in dPEG was only improved for task-relevant sounds. This selective change indicates that dPEG does not always read out information optimally from A1. Instead, during the engaged state, it reads out activity along an axis of A1 population activity that is invariant to task-irrelevant stimuli.</p><p>At face value, these findings may seem paradoxical. If only one dimension of A1 activity is utilized downstream, why does task engagement improve sound representations so broadly? Theories of neural disentanglement and the formation of categorical representations posit that the brain must first build overcomplete, high-dimensional representations of the sensory world (<xref ref-type="bibr" rid="bib17">DiCarlo and Cox, 2007</xref>). From this high-dimensional activity, it is straightforward to build a linear decoder, tuned to the task at hand, that extracts only task-relevant information (<xref ref-type="bibr" rid="bib43">Rigotti et al., 2013</xref>). Our findings are consistent with this theory and describe a hierarchical network for computing sound categories. An overcomplete representation in A1 is selectively filtered at the population level in dPEG, and subsequently, this activity may provide input to category-specific neurons in areas such as FC.</p></sec><sec id="s3-3"><title>Implications for the role of correlated activity in sensory processing</title><p>An important advantage of our experimental setup was that we simultaneously recorded the activity of populations of neurons, contrasting with previous studies that built pseudo-populations from serial experiments (<xref ref-type="bibr" rid="bib4">Bagur et al., 2018</xref>). This approach allowed us to investigate how trial-by-trial covariability across the population depends on task engagement and contributes to sound encoding. Theoretical work has shown that trial-by-trial covariance can impair population coding accuracy (<xref ref-type="bibr" rid="bib3">Averbeck et al., 2006</xref>). Early experiments in visual cortex supported this idea, demonstrating that selective attention improves perceptual discriminations primarily by reducing covariance (<xref ref-type="bibr" rid="bib12">Cohen and Maunsell, 2009</xref>). We found task engagement modulated covariability patterns in both A1 and dPEG, broadly consistent with prior work in the auditory system (<xref ref-type="bibr" rid="bib18">Downer et al., 2017</xref>).</p><p>Strikingly, however, the changes in covariance had no impact on emergent behavioral selectivity in dPEG or on the mean generalized improvement in sound coding in A1. What, then, do these changes in correlated neural activity reflect? In A1, we found that covariability became more aligned with the behaviorally relevant sensory decoding axis during task engagement. This finding is in opposition to a model in which covariability reflects information-limiting noise (<xref ref-type="bibr" rid="bib5">Bartolo et al., 2020</xref>; <xref ref-type="bibr" rid="bib34">Moreno-Bote et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Rumyantsev et al., 2020</xref>). Instead, we hypothesize that covariability in A1 primarily reflects top-down gain modulation that drives changes in selectivity (<xref ref-type="bibr" rid="bib16">Denfield et al., 2018</xref>; <xref ref-type="bibr" rid="bib20">Goris et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Guo et al., 2017</xref>). During the task-engaged state, top-down signals selectively modulate sound-evoked responses of neurons tuned to task-relevant stimuli, thus boosting the representation of task-relevant sounds for downstream readout. If gain is not perfectly static, but varies in strength from trial to trial, this could explain the observed increase in covariability amongst task-relevant neurons (<xref ref-type="bibr" rid="bib16">Denfield et al., 2018</xref>). Simultaneous recordings from multiple auditory fields that permit analysis of the communication subspace between areas may provide further insight into the interaction between top-down signals and sound-evoked responses (<xref ref-type="bibr" rid="bib49">Semedo et al., 2019</xref>; <xref ref-type="bibr" rid="bib53">Srinath et al., 2021</xref>).</p><p>In contrast, the direction of covariability in dPEG changed randomly with respect to the behaviorally relevant decoding axis. During both passive and engaged states, covariability remained mostly orthogonal to the sensory decoding axis and, therefore, had little impact on population decoding accuracy. These findings are consistent with recent work suggesting that trial-by-trial covariability is primarily orthogonal to sensory coding dimensions and reflects non-sensory motor or cognitive variables, such as whisking, running, or arousal (<xref ref-type="bibr" rid="bib35">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Stringer et al., 2019b</xref>). Our results contribute to a growing body of evidence that covariability does not usually reflect information limiting noise, but instead reflects important cognitive processes active in different brain regions during sensory decision-making (<xref ref-type="bibr" rid="bib53">Srinath et al., 2021</xref>).</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Surgical procedures</title><p>All procedures were approved by the Oregon Health and Science University Institutional Animal Care and Use Committee (IACUC protocol IP1561) and conform to the standard of the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC). Adult male domestic ferrets were acquired from an animal supplier (Marshall BioResources). To permit head fixation during neurophysiological recordings and behavioral training, all animals underwent head-post implantation surgeries. Surgeries were performed as described previously (<xref ref-type="bibr" rid="bib46">Saderi et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Slee and David, 2015</xref>). Two stainless steel head-posts were fixed to the skull along the midline with bone cement (Palacos or Charisma). Additionally, 8â10 stainless steel screws were inserted into the skull and bonded to the bone cement to form the structure of the implant. After a two-week recovery period, animals were slowly habituated to a head-fixed posture and auditory stimulation. Following behavioral training on the tone-in-noise task (see below), a micro craniotomy (0.5â1 mm) was opened above either primary auditory cortex (A1) or the dorsal posterior ectosylvian gyrus (dPEG) to allow for the insertion of neurophysiology recording electrodes. Recording sites were targeted based on external landmarks and tonotopic maps (<xref ref-type="bibr" rid="bib2">Atiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Bizley et al., 2005</xref>). After recordings were complete at one location, the microcraniotomy was allowed to close and a new one was opened at a different location.</p></sec><sec id="s4-2"><title>Behavioral paradigm</title><p>Four adult male ferrets were trained on a positively reinforced, go/no-go tone-in-noise detection task. Throughout behavioral training, animals were provided with free access to water on weekends and placed on partial water restriction during the week. During restriction periods, animals were only able to receive liquid rewards during behavioral training. Supplemental water was provided after a training session, if necessary, to ensure that animals maintained at least 90% of their baseline body weight throughout training.</p><p>Single behavioral trials consisted of a sequence of narrow-band noise bursts (0.3-octave bandwidth, 0.3 s duration, 0.2 s ISI), followed by a target tone (0.3 s duration). Animals reported the presence of the target tone by licking a water spout. Licks were detected through a piezo electric sensor glued to the spout (two animals) or by a video camera monitoring the luminance change in a window around the spout (two animals). Licks occurring during a target window (0.2â1.0 s following target onset) were rewarded with a high-protein, high-calorie supplement (Ensure), while licks outside the window were penalized with a timeout (3â10 s). The number of distractor stimuli per trial was distributed randomly with a flat hazard function to prevent behavioral timing strategies. Each behavioral session consisted of 100â200 trials. A subset of trials contained an explicit catch stimulus â a noise burst with the same center frequency as the target tone and occurring with the same temporal distribution as targets. Trials containing a catch stimulus were always concluded by a pure tone reminder target, which was rewarded if the animal successfully withheld responding to the catch licked in response to the pure tone.</p><p>The center frequencies of distractor noise bursts spanned three octaves around the target tone frequency, which was varied randomly between days (0 .lâ20 kHz). Initially, training sessions contained only a single pure tone target (Inf dB SNR). As training progressed, masking noise was introduced to the target tone in order to increase task difficulty. By the end of training, a single behavioral session could consist of up to four different target stimuli (â10 dB, â5 dB, 0 dB, Inf dB). More difficult target stimuli (e.g. â10 dB) occurred more rarely than easier stimuli (e.g<italic>.</italic> Inf dB) during behavioral sessions to maintain motivation. In all cases, noise masking the target was exactly matched to the catch stimulus (centered at the target frequency). Target frequency was fixed within a session, and variable SNR was achieved by adjusting tone amplitude relative to the fixed masking noise. That is, the masking noise (and distractor stimuli) were always presented with an overall sound level of 60 dB SPL. Infinite (inf) dB trials corresponded to trials where the target tone was presented at 60 dB SPL without any masking noise present, 0 dB to trials where the target was 60 dB SPL, â5 dB to trials where the target was presented at 55 dB SPL etc. Neurophysiological recordings proceeded only after animals were able to perform the full, variable SNR task with consistently above chance level performance on â5 dB SNR target tones.</p></sec><sec id="s4-3"><title>Acoustic stimuli</title><p>All experiments were performed in a sound-attenuating chamber (Gretch-Ken). Sound presentation and behavioral control were provided by custom MATLAB software (<ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/lbhb/baphy">https://bitbucket.org/lbhb/baphy</ext-link>). Digital acoustic signals were transformed to analog (National Instruments), amplified (Crown), and delivered through free-field speakers (Manger, 50â35,000 Hz flat gain). Speakers were located 80 cm from the animal at +/-30 deg. azimuth. Stimuli were presented from a single speaker (left or right). During neurophysiology experiments, the speaker contralateral to the recording hemisphere was used. Sound level was equalized and calibrated against a standard reference (PCB Piezoelectronics).</p></sec><sec id="s4-4"><title>Neurophysiology</title><p>Neurophysiological recordings were performed using 64-channel silicon microelectrode arrays (<xref ref-type="bibr" rid="bib51">Shobe et al., 2015</xref>). Electrode contacts were spaced 20 Î¼m horizontally and 25 Î¼m vertically in three columns, collectively spanning 1.05 mm of cortex. Data were amplified (RHD 128-channel head stage, Intan Technologies), digitized at 30 kHz (Open Ephys <xref ref-type="bibr" rid="bib9">Black et al., 2017</xref>), and saved to disk for offline analysis.</p><p>Spike sorting was performed using Kilosort2 (<xref ref-type="bibr" rid="bib40">Pachitariu et al., 2016</xref>), followed by curation in phy (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link>; <xref ref-type="bibr" rid="bib44">Rossant, 2024</xref>). For all identified spike clusters, we quantified isolation as one minus a contamination percentage, defined based on the clusterâs isolation in feature space. We categorized spikes with isolation &gt;85% as isolated or nearly isolated units and included them in all analyses in this study.</p></sec><sec id="s4-5"><title>Auditory field localization</title><p>Initial recordings targeted A1 using external landmarks (<xref ref-type="bibr" rid="bib42">Radtke-Schuller, 2018</xref>). Tuning curves were calculated using pure tone stimuli (100ms duration, 200ms ISI, 3â7oc taves). Neurons were confirmed to be in A1 based on stereotypical response properties: short latency responses to sound onset, sharp and consistent frequency tuning across layers, and a characteristic dorsal-ventral tonotopic map across penetrations (<xref ref-type="bibr" rid="bib50">Shamma et al., 1993</xref>). Once A1 was located, subsequent craniotomies were opened in small lateral steps. Tuning was measured at each recording site, and the border between A1 and dPEG was defined as the location where the tonotopic map gradient reversed (<xref ref-type="bibr" rid="bib2">Atiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Bizley et al., 2005</xref>). After all recording sessions were completed, the best frequencies for each penetration were plotted according to their stereotactic coordinates for post-hoc confirmation of the boundary between A1 and dPEG. Ambiguous recording sites that could not be confidently placed into either area based on their frequency tuning were excluded from the analysis.</p></sec><sec id="s4-6"><title>Pupil recording</title><p>To account for spontaneous fluctuations in arousal that can modulate cortical activity (<xref ref-type="bibr" rid="bib33">McGinley et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">Schwartz et al., 2020</xref>), infrared video of the animalâs eye was recorded for offline analysis (camera: Adafruit TTL Serial Camera, lens: M12 Lenses PT-2514BMP 25.0 mm). The eye ipsilateral to the neurophysiological recording site was recorded so that camera hardware did not interfere with contralateral sound stimuli. To measure pupil size, we fit an ellipse to the boundary of the animalâs pupil on each frame using a custom machine-learning algorithm based on DenseNet201 (<xref ref-type="bibr" rid="bib26">Huang et al., 2018</xref>) and save the dimensions of the ellipse on each frame. Pupil size was shifted 750ms relative to spike times in order to account for the previously reported lagged relationship between neural activity and pupil in cortex (<xref ref-type="bibr" rid="bib33">McGinley et al., 2015</xref>).</p></sec><sec id="s4-7"><title>Analysis of behavioral performance</title><p>Behavioral performance was measured using d-prime (<xref ref-type="bibr" rid="bib21">Green and Swets, 1966</xref>), defined as the z-scored difference between the target hit rate and false alarm rate across a behavior session. We measured the false alarm rate from response to the catch stimulus, whose temporal distribution within a trial was explicitly balanced with target locations across trials. Thus, for each target SNR, d-prime described how well the animal could discriminate that target from the catch stimulus. A d-prime of 0 indicates chance level performance.</p></sec><sec id="s4-8"><title>Single neuron evoked activity and d-prime</title><p>Responses of single neurons to task stimuli were measured by computing each neuronâs peri-stimulus time histogram (PSTH) response to each stimulus. For visualization (e.g. <xref ref-type="fig" rid="fig2">Figure 2</xref>), spiking activity was binned at 20ms, normalized to its 100ms pre-stimulus baseline, z-scored, and smoothed with a gaussian kernel of width of 30ms. Single-trial responses were computed as a neuronâs mean z-scored activity during the 300ms sound evoked window. For active trials, we included responses measured on hit, correct reject, and miss trials. To quantify neural discriminability between catch and target sounds, we measured the difference between the mean z-scored response to target versus catch stimuli (neural d-prime), which is analogous to the behavioral d-prime described above.</p></sec><sec id="s4-9"><title>Neural population d-prime</title><p>To determine how well the activity of simultaneously recorded populations of neurons could discriminate between task stimuli, we measured neural population d-prime. Similar to the single neuron metric, population d-prime was defined as the z-scored difference in the population response to two distinct sound stimuli. We projected high-dimensional z-scored population activity onto a one-dimensional optimal linear discrimination axis to compute d-prime (<xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>), where ÎÂµ is equal to the mean difference in response to the two stimuli and Î£ is the stimulus-independent covariance matrix.<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo>â²</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:msup><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Î</mml:mi><mml:mi>Î¼</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Prior work has shown that finding an optimal discrimination axis for large neural populations can be unreliable because of overfitting to trial-limited data (<xref ref-type="bibr" rid="bib28">Kanitscheider et al., 2015</xref>). To avoid overfitting, we performed <italic>dDR</italic> prior to computing the discrimination axis (<xref ref-type="bibr" rid="bib23">Heller and David, 2022</xref>). In brief, this procedure projected the population activity into the two-dimensional space spanned by the population covariability axis (noise axis) and sound discrimination axis (signal axis), where <italic>e<sub>1</sub></italic> is the first eigenvector of the population covariance matrix.<disp-formula id="equ2"><mml:math id="m2"><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula><disp-formula id="equ3"><mml:math id="m3"><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi>Î¼</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula></p><p>The full dimensionality reduction and decoding procedure was repeated for each stimulus pair individually to avoid bias from stimuli that produced different magnitude responses. Results were grouped into behaviorally relevant (target versus catch) and behaviorally irrelevant (target versus target, distractor versus distractor) categories. To be included in the analysis, we required that a sound stimulus must have been presented in at least five active and five passive trials. Thus, the number of target stimuli analyzed per session depended on the animalâs performance and how long they remained engaged in the task; for shorter behavioral sessions, fewer repetitions of each stimulus were presented and target conditions with low repetition count were omitted.</p></sec><sec id="s4-10"><title>Choice probability analysis</title><p>We performed a choice decoding analysis on hit vs. miss trials. We followed the same procedure as described above for stimulus decoding, where instead of a pair of stimuli our two classes to be decoded were âhit trialâ vs. âmiss trial.â That is, for each target stimulus we computed the optimal linear discrimination axis separating hit vs. miss trials (<xref ref-type="bibr" rid="bib1">Abbott and Dayan, 1999</xref>) in the reduced dimensionality space identified with dDR (<xref ref-type="bibr" rid="bib23">Heller and David, 2022</xref>). For the sake of interpretability with respect to previous work, we reported choice probability as the percentage of correctly decoded trial outcomes rather than d-prime. Percent correct was calculated by projecting the population activity onto the optimal discrimination axis and using leave-one-out cross-validation to measure the number of correct classifications.</p></sec><sec id="s4-11"><title>Factor analysis â population metrics</title><p>To characterize population-wide covariability we used factor analysis (<xref ref-type="bibr" rid="bib59">Umakantha et al., 2021</xref>). Factor analysis is the simplest form of dimensionality reduction that explicitly separates shared variance across neurons from independent variance of single neurons, decomposing the spike count covariance matrix into two parts, a covariance matrix representing shared variance between neurons (Î£<sub>shared</sub>) and a diagonal matrix representing the independent variance of each single neuron (Î¨):<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi mathvariant="normal">Î£</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Î¨</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Because we were interested in stimulus-independent trial-trial variance and the role it played in behaviorally relevant sound decoding, we performed this analysis only on responses to the catch stimulus, as this was common to all measurements of pairwise target vs. catch discrimination accuracy. This way, spike-count covariance was not due to changing stimulus conditions and we could directly ask how it interacted with the behaviorally relevant, target vs. catch discrimination axis. We fit a unique factor analysis model for each behavior state (active verses passive) and experiment. The number of total factors was selected as the model which maximized log-likelihood. Following prior work, we quantified the properties of each factor analysis model using three metrics (<xref ref-type="bibr" rid="bib59">Umakantha et al., 2021</xref>):</p><p><italic>Loading similarity:</italic> Similarity of neuronal loading weights for the Factor that explained maximal shared variance. A value of 0 indicates maximal dissimilarity of weights and a value of 1 indicates that the weights for all neurons are identical.</p><p><italic>Percent shared variance (%sv):</italic> The percentage of each neuronâs variance that can be explained using other neurons in the population. Ranges from 0 to 100%.</p><p><italic>Dimensionality:</italic> The number of dimensions that maximized log-likelihood. In other words, the rank of the shared spike-count covariance matrix. Integer value.</p></sec><sec id="s4-12"><title>Factor analysis â simulations</title><p>We simulated neural population responses to each target and catch stimulus by drawing samples from a multivariate gaussian distribution (n=2000 responses were generated for each sound/behavior state). The mean response of each neuron was determined using the neuronâs actual PSTH and covariance between neurons was defined as the rank <italic>R</italic>&lt;N covariance matrix that maximized the likelihood of the Factor Analysis model for each sound stimulus. Data were simulated independently for each behavior state (passive listening vs. active task engagement). We simulated activity with four models:</p><sec id="s4-12-1"><title>Null</title><p>Mean response, independent variance, and covariance of the gaussian distribution were fixed to the active neuron PSTH, active independent variance (Î¨<sub>active</sub>), and active covariance matrix (Î£<sub>shared, active</sub>) for both passive and engaged states. Thus, simulated data (<italic>r<sub>sim</sub></italic>) were statistically identical between passive and engaged conditions.<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-12-2"><title>Gain only</title><p>Independent variance and covariance for both passive and engaged states were fixed to the active estimates, but the mean was matched to the actual conditionâs PSTH. Thus, variance was independent of task but mean evoked response magnitude was allowed to be modulated by task engagement.<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-12-3"><title>Independent variance</title><p>Mean evoked response and independent variance were modulated by task engagement. Covariance was fixed between states.<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-12-4"><title>Shared variance</title><p>All parameters of the gaussian distribution were matched to the state-dependent estimates.<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Î£</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Î¨</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s4-13"><title>Alignment of population covariability axes</title><p>To measure the alignment of two population covariability axes, we used the absolute value of their cosine similarity. Thus, alignment ranged from 0 (perfectly orthogonal) to 1 (perfectly aligned). In the noise axis versus discrimination axis alignment analysis, we defined the noise axis as the Factor that explained the most shared variance in the catch response (See above section: Factor analysis).</p></sec><sec id="s4-14"><title>Pupil-indexed arousal control</title><p>To control for changes in neural activity that were due to non-specific increases in arousal rather than task engagement (<xref ref-type="bibr" rid="bib46">Saderi et al., 2021</xref>), we used linear regression to remove variability in the activity of single neurons that could be explained by pupil size. The response of each neuron to each stimulus, <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, was modeled as a linear function of pupil size, <italic>p</italic>(<italic>t</italic>):<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>Î²</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Then, to remove the pupil-explainable variance from each neuronâs response but preserve any pupil-independent effect of task engagement on activity, we defined the corrected firing rate, <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, as the true response minus the pupil-dependent portion of the regression model:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">Â¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, the mean sound evoked response was preserved but changes correlated with pupil were removed. This procedure was performed prior to the analysis of task-dependent selectivity in dPEG (e.g<italic>.</italic> <xref ref-type="fig" rid="fig3">Figure 3</xref>). Results were similar for a control analysis that ignored pupil-dependent changes, indicating that the emergent selectivity in dPEG does not depend on this correction for generalized effects arousal (<xref ref-type="fig" rid="fig3s2">Figure 3âfigure supplement 2</xref>).</p></sec><sec id="s4-15"><title>Statistical tests</title><p>Given that the noise properties of neurons are not well understood, particularly around changes in behavioral state, a formal power analysis was not possible. Choice of sample size was based on standard practices in studies of cortex in ferrets and similarly sized species. For each experimental recording session, we measured the population decoding performance of multiple stimulus pairs. To control for any statistical dependencies between these data points within a recording session, we first took the mean projection across stimulus pairs within each recording session before measuring p-values. This procedure reduces our statistical power but provides more conservative estimates of statistical significance which are more robust to detecting spurious false positive results. For all pairwise statistical tests shown in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig5">5</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref> we performed a Wilcoxon signed rank test. Significance was determined at the alpha = 0.05 level. The number of unique recording sessions and animals that went into each comparison are listed in the main text/figure legends, along with the p-value for each analysis.</p><p>The one exception to this general approach is in <xref ref-type="fig" rid="fig2">Figure 2</xref>, where we analyzed the sound discrimination abilities of single neurons. In this case, we computed p-values for each neuron and stimulus independently. First, for each neuron and catch vs. target stimulus pair, we measured d-prime (see Methods: Single neuron evoked activity and d-prime). We generated a null distribution of d-prime values for each neuron-stimulus pair, under each experimental condition by shuffling stimulus identity across trials before computing d-prime (100 resamples). A neuron was determined to have a significant d-prime for a given target vs. catch pair if its actual measured d-prime was greater than the 95<sup>th</sup> percentile of the null d-prime distribution. Second, for each neuron and catch vs. target stimulus pair, we tested if d-prime was significantly different between active and passive conditions. To test this, we followed a similar procedure as above, however, rather than shuffle stimulus identity, we shuffled active vs. passive trial labels. This allowed us to generate a null distribution of active vs. passive d-prime difference for each neuron and stimulus pair. A neuron was determined to have a significant change in d-prime between conditions if the actual Î d-prime lay outside the 95% confidence interval of the null Î d-prime distribution.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4</xref>, we tested if the change in neural population d-prime was correlated with behavior performance on a per-target stimulus basis. Because each target had different behavioral performance (due to varying SNR), here we treated each stimulus as an independent sample. Therefore, correlation was measured between <italic>n</italic> sessions x <italic>n</italic> target stimuli neural vs. behavioral d-primes. To determine the significance of correlation in each brain region, we performed random bootstrap resampling to generate a null distribution of correlation values. The correlation for a given brain region was deemed significant if the actual observed correlation was greater than the 97.5-percentile of the null distribution.</p><p>To evaluate the performance of FA model simulations in predicting behavioral selectivity (<xref ref-type="fig" rid="fig5">Figure 5</xref>) and Î d-prime (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>), we measured the correlation between simulated and actual metrics for each model. To determine if stepwise changes in the FA model (e.g. adding task-dependent gain modulation) caused significant improvements in model performance, we compared correlation coefficients for each model to the correlation coefficient for the final model. To do this, we computed 1000 bootstrap resamples of the correlation coefficient for each model. If the 97.5-percentile of this distribution was greater than the observed correlation for the full model, we determined that it was not significantly different. That is, if the observed correlation for the full model lay within the 95% confidence interval of the null bootstrapped distribution for a given model, it was determined to not be significantly different than the full model.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Visualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Funding acquisition, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>This study was performed in accordance with the Oregon Health and Science University Institutional Animal Care and Use Committee (IACUC) and conforms to standards of the Association for Assessment and Accreditation of Laboratory Animal Care (AAALAC). (IACUC protocol IP1561).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89936-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data for generating figures are available in the associated data repository. All source code for reproducing the analyses and figures in the manuscript can be found on GitHub, along with instructions for downloading the data from Dryad: <ext-link ext-link-type="uri" xlink:href="https://github.com/crheller/eLife2024_Task">https://github.com/crheller/eLife2024_Task</ext-link>, copy archived at <xref ref-type="bibr" rid="bib24">Heller, 2024</xref>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Heller</surname><given-names>CR</given-names></name><name><surname>Hamersky</surname><given-names>GR</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data from: Task-specific invariant representation in auditory cortex</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.z08kprrp4</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The effect of correlated variability on the accuracy of a population code</article-title><source>Neural Computation</source><volume>11</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1162/089976699300016827</pub-id><pub-id pub-id-type="pmid">9950724</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atiani</surname><given-names>S</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Elgueda</surname><given-names>D</given-names></name><name><surname>Locastro</surname><given-names>M</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Emergent selectivity for task-relevant stimuli in higher-order auditory cortex</article-title><source>Neuron</source><volume>82</volume><fpage>486</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.02.029</pub-id><pub-id pub-id-type="pmid">24742467</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn1888</pub-id><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bagur</surname><given-names>S</given-names></name><name><surname>Averseng</surname><given-names>M</given-names></name><name><surname>Elgueda</surname><given-names>D</given-names></name><name><surname>David</surname><given-names>S</given-names></name><name><surname>Fritz</surname><given-names>J</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Go/No-Go task engagement enhances population representation of target stimuli in primary auditory cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2529</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04839-9</pub-id><pub-id pub-id-type="pmid">29955046</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolo</surname><given-names>R</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Mitz</surname><given-names>AR</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Information-limiting correlations in large neural populations</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>1668</fpage><lpage>1678</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2072-19.2019</pub-id><pub-id pub-id-type="pmid">31941667</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Nodal</surname><given-names>FR</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional organization of ferret auditory cortex</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1637</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi042</pub-id><pub-id pub-id-type="pmid">15703254</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The what, where and how of auditory-object perception</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>693</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1038/nrn3565</pub-id><pub-id pub-id-type="pmid">24052177</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Walker</surname><given-names>KMM</given-names></name><name><surname>Nodal</surname><given-names>FR</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auditory cortex represents both pitch judgments and the corresponding acoustic cues</article-title><source>Current Biology</source><volume>23</volume><fpage>620</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.03.003</pub-id><pub-id pub-id-type="pmid">23523247</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Black</surname><given-names>C</given-names></name><name><surname>Voigts</surname><given-names>J</given-names></name><name><surname>Agrawal</surname><given-names>U</given-names></name><name><surname>Ladow</surname><given-names>M</given-names></name><name><surname>Santoyo</surname><given-names>J</given-names></name><name><surname>Moore</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Open Ephys electroencephalography (Open Ephys+EEG): a modular, low-cost, open-source solution to human neural recording</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>035002</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa651f</pub-id><pub-id pub-id-type="pmid">28266930</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buran</surname><given-names>BN</given-names></name><name><surname>von Trapp</surname><given-names>G</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Behaviorally gated reduction of spontaneous discharge can improve detection thresholds in auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4076</fpage><lpage>4081</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4825-13.2014</pub-id><pub-id pub-id-type="pmid">24623785</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christison-Lagay</surname><given-names>KL</given-names></name><name><surname>Bennur</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Contribution of spiking activity in the primary auditory cortex to detection in noise</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>3118</fpage><lpage>3131</lpage><pub-id pub-id-type="doi">10.1152/jn.00521.2017</pub-id><pub-id pub-id-type="pmid">28855294</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention improves performance primarily by reducing interneuronal correlations</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1594</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1038/nn.2439</pub-id><pub-id pub-id-type="pmid">19915566</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Snyder</surname><given-names>AC</given-names></name><name><surname>Acar</surname><given-names>K</given-names></name><name><surname>Williamson</surname><given-names>RC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Slow drift of neural activity as a signature of impulsivity in macaque visual and prefrontal cortex</article-title><source>Neuron</source><volume>108</volume><fpage>551</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.021</pub-id><pub-id pub-id-type="pmid">32810433</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Task reward structure shapes rapid receptive field plasticity in auditory cortex</article-title><source>PNAS</source><volume>109</volume><fpage>2144</fpage><lpage>2149</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117717109</pub-id><pub-id pub-id-type="pmid">22308415</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>de Gee</surname><given-names>JW</given-names></name><name><surname>Mridha</surname><given-names>Z</given-names></name><name><surname>Hudson</surname><given-names>M</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Ramsaywak</surname><given-names>H</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name><name><surname>Karediya</surname><given-names>N</given-names></name><name><surname>Thompson</surname><given-names>M</given-names></name><name><surname>Jaspe</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mice regulate their attentional intensity and arousal to exploit increases in task utility</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.03.04.482962</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Shinn</surname><given-names>TJ</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attentional fluctuations induce shared variability in macaque primary visual cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2654</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05123-6</pub-id><pub-id pub-id-type="pmid">29985411</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downer</surname><given-names>JD</given-names></name><name><surname>Niwa</surname><given-names>M</given-names></name><name><surname>Sutter</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hierarchical differences in population coding within auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>118</volume><fpage>717</fpage><lpage>731</lpage><pub-id pub-id-type="doi">10.1152/jn.00899.2016</pub-id><pub-id pub-id-type="pmid">28446588</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Adaptive, behaviorally gated, persistent encoding of task-relevant auditory information in ferret frontal cortex</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1011</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1038/nn.2598</pub-id><pub-id pub-id-type="pmid">20622871</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goris</surname><given-names>RLT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Partitioning neuronal variability</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>858</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1038/nn.3711</pub-id><pub-id pub-id-type="pmid">24777419</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname><given-names>DM</given-names></name><name><surname>Swets</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Clause</surname><given-names>AR</given-names></name><name><surname>Barth-Maron</surname><given-names>A</given-names></name><name><surname>Polley</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A corticothalamic circuit for dynamic switching between feature detection and discrimination</article-title><source>Neuron</source><volume>95</volume><fpage>180</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.019</pub-id><pub-id pub-id-type="pmid">28625486</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heller</surname><given-names>CR</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Targeted dimensionality reduction enables reliable estimation of neural population coding accuracy from trial-limited data</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0271136</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0271136</pub-id><pub-id pub-id-type="pmid">35862300</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Heller</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>ELife 2024 task</data-title><version designator="swh:1:rev:1e9dea8e63d63f4f5fd2d18d0455ce215aad7e9a">swh:1:rev:1e9dea8e63d63f4f5fd2d18d0455ce215aad7e9a</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f16e9b310403a39adb6961a8d692f4c69e800682;origin=https://github.com/crheller/eLife2024_Task;visit=swh:1:snp:a00d01fd02836d00f65314c3089168c3689bcdbe;anchor=swh:1:rev:1e9dea8e63d63f4f5fd2d18d0455ce215aad7e9a">https://archive.softwareheritage.org/swh:1:dir:f16e9b310403a39adb6961a8d692f4c69e800682;origin=https://github.com/crheller/eLife2024_Task;visit=swh:1:snp:a00d01fd02836d00f65314c3089168c3689bcdbe;anchor=swh:1:rev:1e9dea8e63d63f4f5fd2d18d0455ce215aad7e9a</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillenbrand</surname><given-names>J</given-names></name><name><surname>Getty</surname><given-names>LA</given-names></name><name><surname>Clark</surname><given-names>MJ</given-names></name><name><surname>Wheeler</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Acoustic characteristics of American english vowels</article-title><source>The Journal of the Acoustical Society of America</source><volume>97</volume><fpage>3099</fpage><lpage>3111</lpage><pub-id pub-id-type="doi">10.1121/1.411872</pub-id><pub-id pub-id-type="pmid">7759650</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>van der</surname><given-names>L</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Densely connected convolutional networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1608.06993</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kafashan</surname><given-names>M</given-names></name><name><surname>Jaffe</surname><given-names>AW</given-names></name><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Nogueira</surname><given-names>R</given-names></name><name><surname>Arandia-Romero</surname><given-names>I</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Scaling of sensory information in large neural populations shows signatures of information-limiting correlations</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>473</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-20722-y</pub-id><pub-id pub-id-type="pmid">33473113</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Origin of information-limiting noise correlations</article-title><source>PNAS</source><volume>112</volume><fpage>E6973</fpage><lpage>E6982</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508738112</pub-id><pub-id pub-id-type="pmid">26621747</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kline</surname><given-names>AM</given-names></name><name><surname>Aponte</surname><given-names>DA</given-names></name><name><surname>Kato</surname><given-names>HK</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Distinct nonlinear spectrotemporal integration in primary and secondary auditory cortices</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.25.525588</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name><name><surname>Gill</surname><given-names>JV</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Field</surname><given-names>RE</given-names></name><name><surname>Sten</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallel processing by cortical inhibition enables context-dependent behavior</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>62</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/nn.4436</pub-id><pub-id pub-id-type="pmid">27798631</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lestang</surname><given-names>JH</given-names></name><name><surname>Cai</surname><given-names>H</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Functional network properties of the auditory cortex</article-title><source>Hearing Research</source><volume>433</volume><elocation-id>108768</elocation-id><pub-id pub-id-type="doi">10.1016/j.heares.2023.108768</pub-id><pub-id pub-id-type="pmid">37075536</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical membrane potential signature of optimal states for sensory signal detection</article-title><source>Neuron</source><volume>87</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.038</pub-id><pub-id pub-id-type="pmid">26074005</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Information-limiting correlations</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1410</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id><pub-id pub-id-type="pmid">25195105</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niwa</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>JS</given-names></name><name><surname>OâConnor</surname><given-names>KN</given-names></name><name><surname>Sutter</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Active engagement improves primary auditory cortical neuronsâ ability to discriminate temporal modulation</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>9323</fpage><lpage>9334</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5832-11.2012</pub-id><pub-id pub-id-type="pmid">22764239</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niwa</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>JS</given-names></name><name><surname>OâConnor</surname><given-names>KN</given-names></name><name><surname>Sutter</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Activity related to perceptual judgment and action in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3193</fpage><lpage>3210</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0767-11.2012</pub-id><pub-id pub-id-type="pmid">22378891</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niwa</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>JS</given-names></name><name><surname>OâConnor</surname><given-names>KN</given-names></name><name><surname>Sutter</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Differences between primary auditory cortex and auditory belt related to encoding and choice for AM sounds</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>8378</fpage><lpage>8395</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2672-12.2013</pub-id><pub-id pub-id-type="pmid">23658177</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>OâSullivan</surname><given-names>J</given-names></name><name><surname>Herrero</surname><given-names>J</given-names></name><name><surname>Smith</surname><given-names>E</given-names></name><name><surname>Schevon</surname><given-names>C</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Sheth</surname><given-names>SA</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical encoding of attended auditory objects in multi-talker speech perception</article-title><source>Neuron</source><volume>104</volume><fpage>1195</fpage><lpage>1209</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.007</pub-id><pub-id pub-id-type="pmid">31648900</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Kadir</surname><given-names>S</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Kenneth</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: Realtime spike-sorting for extracellular electrophysiology with hundreds of channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Goris</surname><given-names>RL</given-names></name><name><surname>Cohen</surname><given-names>M</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attention stabilizes the shared gain of V4 populations</article-title><source>eLife</source><volume>4</volume><elocation-id>e08998</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08998</pub-id><pub-id pub-id-type="pmid">26523390</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Cyto- and myeloarchitectural brain atlas of the ferret (Mustela Putorius) in MRI aided stereotaxic coordinates</source><publisher-name>Springer International Publishing</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-76626-3</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rossant</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Phy</data-title><version designator="7a2494b">7a2494b</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumyantsev</surname><given-names>OI</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Hernandez</surname><given-names>O</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Savall</surname><given-names>J</given-names></name><name><surname>Chrapkiewicz</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fundamental bounds on the fidelity of sensory cortical coding</article-title><source>Nature</source><volume>580</volume><fpage>100</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2130-2</pub-id><pub-id pub-id-type="pmid">32238928</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saderi</surname><given-names>D</given-names></name><name><surname>Schwartz</surname><given-names>ZP</given-names></name><name><surname>Heller</surname><given-names>CR</given-names></name><name><surname>Pennington</surname><given-names>JR</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dissociation of task engagement and arousal effects in auditory cortex and midbrain</article-title><source>eLife</source><volume>10</volume><elocation-id>e60153</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.60153</pub-id><pub-id pub-id-type="pmid">33570493</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>ZP</given-names></name><name><surname>Buran</surname><given-names>BN</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil-associated states modulate excitability but not stimulus selectivity in primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>123</volume><fpage>191</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1152/jn.00595.2019</pub-id><pub-id pub-id-type="pmid">31721652</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selezneva</surname><given-names>E</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name><name><surname>Brosch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dual time scales for categorical decision making in auditory cortex</article-title><source>Current Biology</source><volume>16</volume><fpage>2428</fpage><lpage>2433</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.10.027</pub-id><pub-id pub-id-type="pmid">17174917</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semedo</surname><given-names>JD</given-names></name><name><surname>Zandvakili</surname><given-names>A</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical areas interact through a communication subspace</article-title><source>Neuron</source><volume>102</volume><fpage>249</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.026</pub-id><pub-id pub-id-type="pmid">30770252</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Fleshman</surname><given-names>JW</given-names></name><name><surname>Wiser</surname><given-names>PR</given-names></name><name><surname>Versnel</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Organization of response areas in ferret primary auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>367</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.2.367</pub-id><pub-id pub-id-type="pmid">8459273</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shobe</surname><given-names>JL</given-names></name><name><surname>Claar</surname><given-names>LD</given-names></name><name><surname>Parhami</surname><given-names>S</given-names></name><name><surname>Bakhurin</surname><given-names>KI</given-names></name><name><surname>Masmanidis</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Brain activity mapping at multiple scales with silicon microprobes containing 1,024 electrodes</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>2043</fpage><lpage>2052</lpage><pub-id pub-id-type="doi">10.1152/jn.00464.2015</pub-id><pub-id pub-id-type="pmid">26133801</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slee</surname><given-names>SJ</given-names></name><name><surname>David</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rapid task-related plasticity of spectrotemporal receptive fields in the auditory midbrain</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13090</fpage><lpage>13102</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1671-15.2015</pub-id><pub-id pub-id-type="pmid">26400939</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinath</surname><given-names>R</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Cohen</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Attention improves information flow between neuronal populations without changing the communication subspace</article-title><source>Current Biology</source><volume>31</volume><fpage>5299</fpage><lpage>5313</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.09.076</pub-id><pub-id pub-id-type="pmid">34699782</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stilp</surname><given-names>CE</given-names></name><name><surname>Kluender</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cochlea-scaled entropy, not consonants, vowels, or time, best predicts speech intelligibility</article-title><source>PNAS</source><volume>107</volume><fpage>12387</fpage><lpage>12392</lpage><pub-id pub-id-type="doi">10.1073/pnas.0913625107</pub-id><pub-id pub-id-type="pmid">20566842</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>High-dimensional geometry of population responses in visual cortex</article-title><source>Nature</source><volume>571</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1346-5</pub-id><pub-id pub-id-type="pmid">31168101</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunada</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Representation of speech categories in the primate auditory cortex</article-title><source>Journal of Neurophysiology</source><volume>105</volume><fpage>2634</fpage><lpage>2646</lpage><pub-id pub-id-type="doi">10.1152/jn.00037.2011</pub-id><pub-id pub-id-type="pmid">21346209</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsunada</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>ASK</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal contribution of primate auditory cortex to auditory perceptual decision-making</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/nn.4195</pub-id><pub-id pub-id-type="pmid">26656644</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Umakantha</surname><given-names>A</given-names></name><name><surname>Morina</surname><given-names>R</given-names></name><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Snyder</surname><given-names>AC</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Bridging neuronal correlations and dimensionality reduction</article-title><source>Neuron</source><volume>109</volume><fpage>2740</fpage><lpage>2754</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.028</pub-id><pub-id pub-id-type="pmid">34293295</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>AA</given-names></name><name><surname>Heslenfeld</surname><given-names>DJ</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name><name><surname>Versfeld</surname><given-names>NJ</given-names></name><name><surname>Kramer</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The eye as a window to the listening brain: neural correlates of pupil size as a measure of cognitive listening load</article-title><source>NeuroImage</source><volume>101</volume><fpage>76</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.06.069</pub-id><pub-id pub-id-type="pmid">24999040</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89936.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ding</surname><given-names>Nai</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Zhejiang University</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study provides insights into how the brain constructs categorical neural representations during a difficult auditory target detection task. Through recordings of simultaneous single-unit activity in primary and secondary auditory areas, <bold>compelling</bold> evidence is provided that categorical neural representations emerge in a secondary auditory area, i.e., PEG. The study is of interest to neuroscientists and can also potentially shed light on human psychological studies.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89936.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This is a very interesting paper which addresses how auditory cortex represents sound while an animal is performing an auditory task. The study involves psychometric and neurophysiological measurements from ferrets engaged in a challenging tone in noise discrimination task, and relates these measurements using neurometric analysis. A novel neural decoding technique (decoding-based dimensionality reduction or dDR, introduced in a previous paper by two of the authors) is used to reduce bias so that stimulus parameters can be read out from neuronal responses.</p><p>The central finding of the study is that, when an animal is engaged in a task, non-primary auditory cortex represents task-relevant sound features in a categorical way. In primary cortex, task engagement also affects representations, but in a different way - the decoding is improved (suggesting that representations have been enhanced), but is not categorical in nature. The authors argue that these results are compatible with a model where early sensory representations form an overcomplete representation of the world, and downstream neurons flexibly read out behaviourally relevant information from these representations.</p><p>I find the concept and execution of the study very interesting and elegant. The paper is also commendably clear and readable. The differences between primary and higher cortex are compelling and I am largely convinced by the authors' claim that they have found evidence that broadly supports a mixed selectivity model of neural disentanglement along the lines of Rigotti et al (2013). I think that the increasing body of evidence for these kinds of representations is a significant development in our understanding of higher sensory representations. I also think that the dDR method is likely to be useful to researchers in a variety of fields who are looking to perform similar types of neural decoding analysis.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89936.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This study compares the activity of neural populations in the primary and non-primary auditory cortex of ferrets while the animals actively behaved or passively listened to a sound discrimination task. Using a variety of methods, the authors convincingly show differential effects of task engagement on population neural activity in primary vs non-primary auditory cortex; notably that in the primary auditory cortex, task-engagement (1) improves discriminability for both task-relevant and non-task relevant dimensions, and (2) improves the alignment between covariability and sound discrimination axes; whereas in the non-primary auditory cortex, task-engagement (1) improves discriminability for only task-relevant dimensions, and (2) does not affect the alignment between covariability and sound discrimination axes. They additionally show that task-engagement changes in gain can account for the selectivity noted in the discriminability of non-primary auditory neurons. They also admirably attempt to isolate task-engagement from arousal fluctuations, by using fluctuations in pupil size as a proxy for physiological arousal. This is a well-carried out study with thoughtful analyses which in large part achieves its aims to evaluate how task-engagement changes neural activity across multiple auditory regions . As with all work, there are several caveats or areas for future study/analysis. First, the sounds used here (tones, and narrow-band noise) are relatively simple sounds; previous work suggests that exactly what activity is observed within each region (e.g., sensory only, decision-related, etc) may depend in part upon what stimuli are used. Therefore, while the current study adds importance to the literature, future work may consider the use of more varied stimuli. Second, the animals here were engaged in a behavioral task; but apart from an initial calculation of behavioral d', the task performance (and its effect on neural activity) is largely unaddressed.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89936.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Heller</surname><given-names>Charles R</given-names></name><role specific-use="author">Author</role><aff><institution>Oregon Health and Science University</institution><addr-line><named-content content-type="city">Portland</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hamersky</surname><given-names>Gregory R</given-names></name><role specific-use="author">Author</role><aff><institution>Oregon Health and Science University</institution><addr-line><named-content content-type="city">Portland</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>David</surname><given-names>Stephen V</given-names></name><role specific-use="author">Author</role><aff><institution>Oregon Health &amp; Science University</institution><addr-line><named-content content-type="city">Portland</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>â¦I find the concept and execution of the study very interesting and elegant. The paper is also commendably clear and readable. The differences between primary and higher cortex are compelling and I am largely convinced by the authors' claim that they have found evidence that broadly supports a mixed selectivity model of neural disentanglement along the lines of Rigotti et al (2013). I think that the increasing body of evidence for these kinds of representations is a significant development in our understanding of higher sensory representations. I also think that the dDR method is likely to be useful to researchers in a variety of fields who are looking to perform similar types of neural decoding analysis.</p></disp-quote><p>Thanks! We agree that questions around population coding and high-level representations are critical in the field of sensory systems.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>... This is a well-carried out study with thoughtful analyses which in large part achieves its aims to evaluate how task-engagement changes neural activity across multiple auditory regions. As with all work, there are several caveats or areas for future study/analysis. First, the sounds used here (tones, and narrow-band noise) are relatively simple sounds; previous work suggests that exactly what activity is observed within each region (e.g., sensory only, decision-related, etc) may depend in part upon what stimuli are used. Therefore, while the current study adds importantly to the literature, future work may consider the use of more varied stimuli. Second, the animals here were engaged in a behavioral task; but apart from an initial calculation of behavioral d', the task performance (and its effect on neural activity) is largely unaddressed.</p></disp-quote><p>The reviewer makes several important points that we hope we addressed in the specific changes detailed below. Indeed, it is important to recognize the possibility that the specific stimuli involved in a task may interact with the effects of behavioral state and that variability in task performance should be considered as an important aspect of behavioral state.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>I have a few minor comments and criticisms:</p><p>(1) Figure 1c. The choice of low-contrast grey text (e.g. &quot;Target vs. target&quot; is unfortunate, especially when printed), and should be replaced (e.g. with dark grey).</p></disp-quote><p>We have edited the figure to use a higher contrast (dark grey). Thanks for catching this.</p><disp-quote content-type="editor-comment"><p>(2) Figure 2 and Supplementary Figure 3. I think some indication of error or significance is required in all panels. Without this, it's hard to interpret any of these panels.</p></disp-quote><p>Thank you for this feedback. Including significance here was clarifying and helps to strengthen our claim that state-dependent changes in neural activity were smaller and more diverse for single neurons than at the population level. We modified Figure 2b-c to indicate whether each neuronâs response to the target stimulus was significantly different than its response to the catch stimulus. The same test was performed in Supplementary Figure 3. Additionally, we added a statistical test in Figure 2d-e to indicate, for each pair of target/catch stimuli, whether discrimination (d-prime) changed significantly between active and passive conditions. Furthermore, we modified the text of the second paragraph under the results heading: âDiverse effects of task engagement on single neurons in primary and non-primary auditory cortexâ to reference and interpret the results of these significance tests. The new text reads as follows (L. 121):</p><p>âSound-evoked spiking activity was compared between active and passive states to study the impact of task engagement on sound representation. In both A1 and dPEG, responses to target and catch stimuli were significantly discriminable for a subset of single neurons (about 25% in both areas, Figure 2A-C, Supplemental Figures 3-5, bootstrap test). This supports the idea that stimulus identity can be decoded in both brain regions, regardless of task performance. However, the fact that the responses of most neurons in both brain areas could not significantly discriminate target vs. catch stimuli also highlights the diversity of sound encoding observed at the level of single neurons. The accuracy of catch vs. target discrimination for each neuron was quantified using neural d-prime, the z-scored difference in target minus catch spiking response for each neuron (Methods: Single neuron PSTHs and d-prime (Niwa et al., 2012a)). Task engagement was associated with significant changes in catch vs. target d-prime for roughly 10% of neurons in both A1 (40 / 481 neurons, bootstrap test) and dPEG (33 / 377 neurons, bootstrap test). This included neurons that both increased their discriminability and decreased their discriminability (Figure 2D-E). Thus, the effects of task engagement at the level of single neurons were relatively mild and inconsistent across the population; many neurons showed no significant change and of those that did, effects were bidirectional (Figure 2D-E).â</p><p>We also included an additional methods paragraph in the âStatistical testsâ section to describe the bootstrapping procedure used for these significance tests (L. 644):</p><p>âThe one exception to this general approach is in Figure 2, where we analyzed the sound discrimination abilities of single neurons. In this case, we computed p-values for each neuron and stimulus independently. First, for each neuron and catch vs. target stimulus pair, we measured d-prime (see Methods: Single neuron evoked activity and d-prime). We generated a null distribution of d-prime values for each neuron-stimulus pair, under each experimental condition by shuffling stimulus identity across trials before computing d-prime (100 resamples). A neuron was determined to have a significant d-prime for a given target vs. catch pair if its actual measured d-prime was greater than the 95th percentile of the null d-prime distribution. Second, for each neuron and catch vs. target stimulus pair, we tested if d-prime was significantly different between active and passive conditions. To test this, we followed a similar procedure as above, however, rather than shuffle stimulus identity, we shuffled active vs. passive trial labels. This allowed us to generate a null distribution of active vs. passive d-prime difference for each neuron and stimulus pair. A neuron was determined to have a significant change in d-prime between conditions if the actual Î d-prime lay outside the 95% confidence interval of the null Î d-prime distribution.â</p><p>For Figure 2a, we chose not to indicate significance on the figure to avoid clutter, since the significance for all neurons in the population are shown in panels b-c anyway. Additionally, the difference plot shown in panel a is in units of z-scores, which we believe already gives a raw sense of the significance of the target vs. catch response change per neuron in this example dataset.</p><disp-quote content-type="editor-comment"><p>(3) Figure 2 and Supplementary Figure 3. I would consider including some more examples as a Supplementary Figure (and perhaps combining Supp Fig 3 with Fig 2 as a main figure).</p></disp-quote><p>We found no significant or apparent difference in single-neuron properties between A1 and dPEG. Therefore, we decided it is not helpful to plot both A1 and PEG examples in the main text. However, we agree that the ability to see more examples of the raw data could be useful. Therefore, we compiled two supplementary figures (Supplementary Figures 4 and 5) that replicate Figure 2a for all datasets, encompassing A1 and PEG.</p><disp-quote content-type="editor-comment"><p>(4) Figure 2a and Supp Fig 3a. I was initially confused that the &quot;delta-spk/sec (z-score)&quot; values had themselves been z-scored, but now I think that they are simply the differences of the two left hand sub-panels. This could be made clear in the figure legend.</p></disp-quote><p>The figure legends have been modified to state the procedure for computing âdelta-spk/secâ more clearly. Specifically, we added the following information to the legend (L. 141):</p><p>âDifference is computed as the z-scored response to the target minus the z-scored catch response (resulting in a difference shown in units of z-score).â</p><disp-quote content-type="editor-comment"><p>(5) Figure 2b-e and Supp Fig 3b-e. Indicate the time window over which the responses were measured, and the number of neurons.</p></disp-quote><p>Figure legends have been modified to include a sentence clearly stating the time window over which responses were measured. The number of neurons is also now included in the legend and on the figure itself. Furthermore, a brief description of the new statistical testing procedure has been added here (L. 144).</p><p>âResponses were defined as the total number of spikes recorded during the 300 ms of sound presentation (area between dashed lines in panel A). Neurons with a significantly different response to the catch vs. target stimulus are indicated in black and quantified on the respective figure panel.â</p><disp-quote content-type="editor-comment"><p>(6) Figure 2. &quot;singe&quot; should read &quot;single&quot;</p></disp-quote><p>Typo in figure label has been fixed.</p><disp-quote content-type="editor-comment"><p>(7) Line 144. Figure number is missing (Figure 3B-C).</p></disp-quote><p>The missing figure number has been added to the text.</p><disp-quote content-type="editor-comment"><p>(8) Figure 3. Again, the low-contrast grey should be replaced.</p></disp-quote><p>The low-contrast grey has been replaced with dark grey.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>This study really nicely compares the activity and effects on activity in two areas of the auditory cortex in respect to task-engagement; I think it is, for the most part, very well done.</p><p>A couple of specific recommendations:</p><p>(1) Although I understand 'inf dB' as the SNR, including the actual dB level used in the experiments, would be useful, especially in the case of the inf dB.</p></disp-quote><p>Thank you for this feedback. We agree that clarification about the overall sound level used here would be helpful. We have modified the methods section âBehavioral paradigmâ to include the following sentence (L. 450):</p><p>âThat is, the masking noise (and distractor stimuli) were always presented with an overall sound level of 60 dB SPL. Infinite (inf) dB trials corresponded to trials where the target tone was presented at 60 dB SPL without any masking noise present, 0 dB to trials where the target was 60 dB SPL, -5 dB to trials where the target was presented at 55 dB SPL etc.â</p><p>In addition, we have modified the main text (L. 82):</p><p>âAnimals reported the occurrence of a target tone in a sequence of narrowband noise distractors by licking a piezo spout (Figure 1A, Methods: Behavioral paradigm, distractor stimulus sound level: 60 dB SPL). â¦ We describe SNR as the overall SPL of the target relative to distractor noise level. Thus, an SNR of â5 dB corresponds to a target level of 55 dB SPL while an Inf dB SNR corresponds to a target tone presented without any masking noise.â</p><p>And Figure legend 1 now explicitly states the sound level used in the experiments (L. 104):</p><p>âVariable SNR was achieved by varying overall SPL of the target relative to the fixed (60 dB SPL) distractor noise, e.g., -5 dB SNR corresponds to a 55 dB SPL target with 60 dB SPL masking noise. Infinite (inf) dB SNR corresponds to a target tone presented in isolation (60 dB SPL).â</p><disp-quote content-type="editor-comment"><p>(2) I very much appreciate the attempt to disentangle task engagement from generalized arousal state, and specifically, addressing this through the use of pupillometry. However, by focusing the discussion of pupil dynamics solely on the arousal-state aspects of pupil size, the paper doesn't address the increasing evidence suggests that pupil size may fluctuate based upon a lot of other things, including perceptual events (see Kronemer et al, 2022 for a recent human paper; for auditory: Zekveld et al 2018 (review) and Montes-Lourido et al, 2021; but many many others, too). It would be nice to see either a bit more nuanced discussion of what pupil size may be indicating (easier), or analyzing the behavior in the context of pupil dynamics (a heavier lift).</p></disp-quote><p>This is a good point. We agree that it is worth mentioning these more nuanced aspects of cognition that may be reflected by pupil size. Therefore, we also analyzed pupil size in the context of behavioral performance (see Supplemental Figure 6) and added the following text to the results (L. 193).</p><p>âIn addition to reflecting overall arousal level, pupil size has also been reported to reflect more nuanced cognitive variables such as, for example, listening effort (Zekveld et al., 2014). Furthermore, rodent data suggests that optimal sensory detection is associated with intermediate pupil size (McGinley et al., 2015), consistent with the hypothesis of an inverted-U relationship between arousal and behavioral performance (Zekveld et al., 2014). To determine if this pattern was true for the animals in our task, we measured the dynamics of pupil size in the context of behavioral performance. Across animals, task stimuli evoked robust pupil dilation that varied with trial outcome (Supplemental Figure 6b-c). Notably, pre-trial pupil size was significantly different between correct (hit and correct reject), hit, and miss trials (Supplemental Figure 6b-c), recapitulating the finding of an inverted-U relationship to performance in rodents (McGinley et al., 2015). Since we focused only on correct trials in our decoding analysis, these outcome-dependent differences in pupil size are unlikely to contribute to the emergent decoding selectivity in dPEG.â</p><disp-quote content-type="editor-comment"><p>(3) I think it would make this paper shine that much more if behavioral performance were not subsumed into the overall label of task engagement. You've already established you have performance that varies as a function of SNR; I would love to see the neural d' and covariability related to the behavioral d' (in the comparisons where this is possible). I would also love to see a more direct measure of choice for those stimuli that show variable behavior (e.g., a choice probability analysis or something of the like would seem to be easily applied to the target SNRs of -5 and 0 dB); and compare task engaged activity of hits vs misses vs passive listening to those same stimuli. You discuss previous studies looking at choice-related/decision-related activity and draw parallels to this work-given that there is the opportunity with this data set to *directly* assess choice-related activity, the absence of such an analysis seems like a missed opportunity.</p></disp-quote><p>Thank you for this feedback. We agree that âtask engagementâ is not a unimodal state and that a more fine-grained analysis of task-engaged neural activity, according to behavioral choice, could be informative.</p><p>First, we would like to point out that in Figure 4 we did already compare behavioral dâ to delta neural dâ. We found that the two were significantly correlated in dPEG, but not in A1. This suggests that task-dependent changes in stimulus decoding in dPEG, but not A1, are predictive of behavioral performance. This is consistent with the finding that task-relevant stimulus representations were selectively enhanced in dPEG, but not in A1.</p><p>Second, we added a choice decoding analysis to address whether auditory cortex represents the animalâs choice in our task. The results of this analysis are summarized in Supplemental Figure 8 and are discussed under the results section: âBehavioral performance is correlated with neural coding changes in non-primary auditory cortex only.â (L. 226):</p><p>âThe previous analysis suggests that the task-dependent increase in stimulus information present in dPEG population activity is predictive of overall task performance. Next, we asked whether the population activity in either brain region was directly predictive of behavioral choice on single hit vs. miss trials. To do this, we conducted a choice probability analysis (Methods). We found that in both brain regions choice could be decoded well above chance level (Supplemental Figure 8). Choice information was present throughout the entire trial and did not increase during the target stimulus presentation. This suggests that the difference in population activity primarily reflects a cognitive state associated with the probability of licking on a given trial, or âimpulsivityâ rather than âchoice.â This interpretation is consistent with our finding that baseline pupil size on each trial is predictive of trial outcome (Supplemental Figure 6b).â</p><p>To keep our decoding approach consistent throughout the manuscript, we followed the same approach for choice decoding as we did for stimulus decoding (perform dDR then calculate neural d-prime in the dimensionality reduced space). To make the results more interpretable, we converted choice d-prime to a choice probability (percent correctly decoded choices) using leave-one-out cross validation. (We note that d-prime and percent correct are very highly correlated statistics.) This is described in the methods as follows (L. 550):</p><p>âWe performed a choice decoding analysis on hit vs. miss trials. We followed the same procedure as described above for stimulus decoding, where instead of a pair of stimuli our two classes to be decoded were âhit trialâ vs. âmiss trialâ. That is, for each target stimulus we computed the optimal linear discrimination axis separating hit vs. miss trials (Abbott and Dayan, 1999) in the reduced dimensionality space identified with dDR (Heller and David, 2022). For the sake of interpretability with respect to previous work we reported choice probability as the percentage of correctly decoded trial outcomes rather than d-prime. Percent correct was calculated by projecting the population activity onto the optimal discrimination axis and using leave-one-out cross validation to measure the number of correct classifications.â</p><disp-quote content-type="editor-comment"><p>(4) It would also be interesting to look at population coding across sessions (although the point is taken that within a session allows the opportunity to assess covariability). Minorly self-servingly but very much related to the above point, Christison-Lagay et al, 2017 employed a similar detect-in-noise task, analyzed single neurons and population level activity, and looked at putative choice-related activity. The current study has the opportunity to expand on that kind of analysis that much more by looking across multiple sites vs within a given recording site; and compare across regions.</p></disp-quote><p>Thank you for highlighting this point, we agree that it is important. When studying population coding it is critical to consider the impact of covariability between neurons. Therefore, it is worthwhile to revisit our interpretations of prior results, e.g., Christison-Lagay et al, 2017, which studied population coding by combining neurons across different sessions, given that we now have access to simultaneously recorded population data.</p><p>First, we would like to point out that this was the primary motivation for our simulation analyses presented in Figure 5. Using simulations, we found that task-dependent gain modulation (which can be observed across sessions) was sufficient to explain our primary finding â selective enhancement in decoding of behaviorally relevant sound stimuli in dPEG.</p><p>Second, to address the question about how covariability affects choice-related information in auditory cortex and compare our findings with prior studies, we performed the same set of simulations for choice probability analysis. We found that, again, choice-dependent gain modulation was sufficient to explain our findings. That is, simulations with hit- vs. miss-dependent gain changes, but fixed covariability, closely mirrored the choice probability we observed in the raw data. An additional simulation where covariability between all neurons was set to zero also recapitulated our findings in the raw data. Collectively, this suggests that covariability does not play a significant role in shaping the choice information present in A1 and dPEG during this task. We have added the following text to the manuscript to summarize this finding (L. 293):</p><p>âFinally, we used the same simulation approach to determine what aspects of population activity carry the âchoiceâ related information we observed in A1 and dPEG (Figure 4 â figure supplement 1). Similar to our findings for stimulus decoding, we found that gain modulation alone was sufficient to recapitulate the choice information present in the raw data for this task. This helps frame prior work that pooled neurons across sessions to study population coding of choice in similar auditory discrimination tasks (Christison-Lagay et al, 2017).â</p></body></sub-article></article>