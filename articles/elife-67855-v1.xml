<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">67855</article-id><article-id pub-id-type="doi">10.7554/eLife.67855</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-228645"><name><surname>Goffinet</surname><given-names>Jack</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6729-0848</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-228646"><name><surname>Brudner</surname><given-names>Samuel</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6043-9328</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-8890"><name><surname>Mooney</surname><given-names>Richard</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3308-1367</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-157159"><name><surname>Pearson</surname><given-names>John</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9876-7837</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Computer Science</institution>, <institution>Duke University</institution>, <addr-line><named-content content-type="city">Durham</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution content-type="dept">Neurobiology</institution>, <institution>Duke University</institution>, <addr-line><named-content content-type="city">Durham</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><institution content-type="dept">Department of Neurobiology</institution>, <institution>Duke University</institution>, <addr-line><named-content content-type="city">Durham</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><institution content-type="dept">Biostatistics &amp; Bioinformatics, Neurobiology, Center for Cognitive Neuroscience, Psychology and Neuroscience, Electrical and Computer Engineering</institution>, <institution>Duke University</institution>, <addr-line><named-content content-type="city">Durham</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-3674"><name><surname>Goldberg</surname><given-names>Jesse H</given-names></name><role>Reviewing editor</role><aff><institution>Cornell University</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>john.pearson@duke.edu</email> (JP);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>14</day><month>05</month><year>2021</year></pub-date><volume>10</volume><elocation-id>e67855</elocation-id><history><date date-type="received"><day>24</day><month>02</month><year>2021</year></date><date date-type="accepted"><day>12</day><month>05</month><year>2021</year></date></history><permissions><copyright-statement>Â© 2021, Goffinet et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Goffinet et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-67855-v1.pdf"/><abstract><p>Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-by-moment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-dimensional vocal behavior.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH117778</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01-NS118424</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name><name><surname>Pearson</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000055</institution-id><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>R01-DC013826</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name><name><surname>Pearson</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01-NS099288</award-id><principal-award-recipient><name><surname>Mooney</surname><given-names>Richard</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009633</institution-id><institution>Eunice Kennedy Shriver National Institute of Child Health and Human Development</institution></institution-wrap></funding-source><award-id>F31-HD098772</award-id><principal-award-recipient><name><surname>Brudner</surname><given-names>Samuel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: All data generated in conjunction for this study were generated by experiments performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols of Duke University, protocol numbers A171-20-08 and A172-20-08.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Dataset 1---------Online, publicly available MUPET dataset: ~5GB Available at: https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki Figs: 2, 3, 4d-eDataset 2----------Single zebra finch data: ~200-400 MB of audio generated as part of work in progress in Mooney Lab. Figs: 2e-f, 4a-c, 5a, 5d, 6b-eDataset 3---------Mouse USV dataset: ~30-40 GB of audio generated as part of work in progress in Mooney Lab. Figs: 4fDataset 5---------This is a subset of dataset 3, taken from a single mouse: ~1GB of audio. Figs: 5b-e, 6aDataset 6---------10 zebra finch pupil/tutor pairs: ~60 GB of audio generated as part of work in progress in Mooney Lab. Figs: 7Upon acceptance, all Datasets 2-6 will be archived in the Duke Digital Repository (https://research.repository.duke.edu). DOI in process.</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Van Segbroeck et al</collab></person-group><year iso-8601-date="2017">2017</year><source>MUPET benchmark data</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki">https://github.com/mvansegbroeck/mupet/wiki/MUPET-wiki</ext-link><comment>NA</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-67855-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>