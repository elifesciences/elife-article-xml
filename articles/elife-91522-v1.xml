<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91522</article-id><article-id pub-id-type="doi">10.7554/eLife.91522</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91522.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>When abstract becomes concrete, naturalistic encoding of concepts in the brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kewenig</surname><given-names>Viktor Nikolaus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-5912-0676</contrib-id><email>ucjuvnk@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Vigliocco</surname><given-names>Gabriella</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Skipper</surname><given-names>Jeremy I</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5503-764X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Experimental Psychology, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Planck Institute for Psycholinguistics</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>05</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP91522</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-13"><day>13</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-01"><day>01</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.08.506944"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-10"><day>10</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91522.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-09-17"><day>17</day><month>09</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91522.2"/></event></pub-history><permissions><copyright-statement>© 2024, Kewenig et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Kewenig et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91522-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-91522-figures-v1.pdf"/><abstract><p>Language is acquired and processed in complex and dynamic naturalistic contexts, involving the simultaneous processing of connected speech, faces, bodies, objects, etc. How words and their associated concepts are encoded in the brain during real-world processing is still unknown. Here, the representational structure of concrete and abstract concepts was investigated during movie watching to address the extent to which brain responses dynamically change depending on visual context. First, across contexts, concrete and abstract concepts are shown to encode different experience-based information in separable sets of brain regions. However, these differences are reduced when multimodal context is considered. Specifically, the response profile of abstract words becomes more concrete-like when these are processed in visual scenes highly related to their meaning. Conversely, when the visual context is unrelated to a given concrete word, the activation pattern resembles more that of abstract conceptual processing. These results suggest that while concepts generally encode habitual experiences, the underlying neurobiological organisation is not fixed but depends dynamically on available contextual information.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>When we learn and use language, we deal with two main types of concepts. Concrete concepts, which refer to things we directly experience (like a chair, running or the colour blue), and abstract concepts, which refer to ideas that we are unable to sense directly (like truth, democracy or love).</p><p>Most studies have looked at how people process these concepts in isolation, such as by reading single words on a screen. This revealed that the human brain processes each concept differently, with concrete concepts typically activating brain regions involved in sensory and motor experiences, and abstract concepts activating regions involved in emotion and complex thinking.</p><p>However, the experiments conducted in these studies do not represent real life situations, where humans often encounter and process both concepts simultaneously. For instance, at the same time as processing language, someone may also be seeing, hearing, and experiencing other things in their environment.</p><p>Kewenig et al. wanted to understand whether the brain processes abstract and concrete concepts differently depending on what a person may be visualizing at the same time. To achieve this, they used a technique known as functional MRI to record which regions of the brain are activated as participants watched different movies.</p><p>The team found that when abstract concepts (such as love) appeared with related visual information (such as people kissing), the brain processed them more like concrete concepts, engaging sensory and motor regions. Conversely, when concrete concepts (like a chair) appeared without related visual information, the brain processed them more like abstract concepts, engaging regions involved in complex thinking. This suggests that the way the human brain processes meaning is very dynamic and constantly adapting to available contextual information.</p><p>These findings could help improve artificial intelligence systems that process language and visual information together, making them better at understanding context-dependent meaning. They might also benefit people with language disorders by informing the development of more effective therapies that consider how context affects understanding. However, more research is needed to confirm these findings and develop practical applications, particularly studies testing whether similar brain patterns occur in other natural situations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>concept representation</kwd><kwd>context</kwd><kwd>embodied cognition</kwd><kwd>naturalistic neuroimaging</kwd><kwd>semantic memory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>ECOLANG,743035</award-id><principal-award-recipient><name><surname>Vigliocco</surname><given-names>Gabriella</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>WRM\R3\170016</award-id><principal-award-recipient><name><surname>Vigliocco</surname><given-names>Gabriella</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>DS-2017-026</award-id><principal-award-recipient><name><surname>Kewenig</surname><given-names>Viktor Nikolaus</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A novel deep-learning-based computational method using object recognition to quantify visual context in naturalistic, multimodal stimuli demonstrates that a concept's perceived abstractness or concreteness dynamically depends on its visual context.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Humans acquire, and process language in situated multimodal contexts, through dynamic interactions with their environment. For example, children may learn what the word ‘tiger’ means primarily via sensory-motor experience: they see one on TV, or they are told that a tiger looks like a big cat. Conversely, the experience required for understanding the more abstract concept of ‘good’ will likely include an evaluation of the rational and emotional motives underscoring intentional actions. Consequently, while more concrete concepts have external physical references (they refer to objects or actions that are easily perceived in the world), more abstract concepts do not necessarily have such references (they generally refer more to cultural and societal constructs or peoples' inner states of mind) (<xref ref-type="bibr" rid="bib131">Villani et al., 2019</xref>). Is this difference reflected in concrete and abstract representations in the brain during naturalistic processing? And are these static or can they change as a function of the multimodal contexts in which processing occurs?</p><p>Most studies of concrete and abstract processing are not naturalistic in that they present words or sentences isolated from the rich contexts in which we usually process them. Collectively, these studies suggest that concrete and abstract concepts engage separate brain regions involved in processing different types of information. (<xref ref-type="bibr" rid="bib8">Bedny and Thompson-Schill, 2006</xref>; <xref ref-type="bibr" rid="bib11">Binder et al., 2009</xref>; <xref ref-type="bibr" rid="bib106">Sabsevitz et al., 2005</xref>; <xref ref-type="bibr" rid="bib135">Wang et al., 2010</xref>; <xref ref-type="bibr" rid="bib129">Vigliocco et al., 2014</xref>). Concrete words engage regions involved in experiential processing (<xref ref-type="bibr" rid="bib5">Barsalou et al., 2003</xref>). For example, motor-related cortices activate during the processing of action verbs like ‘throw’ (<xref ref-type="bibr" rid="bib52">Hauk et al., 2004</xref>), or action-related nouns like ‘hammer’ (<xref ref-type="bibr" rid="bib127">Vigliocco et al., 2006</xref>; <xref ref-type="bibr" rid="bib69">Kiefer and Pulvermüller, 2012</xref>), auditory cortices for sound-related words like ‘telephone’ (<xref ref-type="bibr" rid="bib42">Goldberg et al., 2006</xref>; <xref ref-type="bibr" rid="bib68">Kiefer et al., 2008</xref>), and visual cortices for color-related words like ‘yellow’ (<xref ref-type="bibr" rid="bib60">Hsu et al., 2011</xref>; <xref ref-type="bibr" rid="bib110">Simmons et al., 2007</xref>). These results are consistent with the view that we learn and neurobiologically encode concrete concepts in terms of the sensory and motor experiences associated with their referents.</p><p>In contrast, some studies of abstract concepts have found greater activation in brain regions associated with general linguistic processing (<xref ref-type="bibr" rid="bib10">Binder et al., 2005</xref>; <xref ref-type="bibr" rid="bib81">Mellet et al., 1998</xref>; <xref ref-type="bibr" rid="bib87">Noppeney et al., 2004</xref>; <xref ref-type="bibr" rid="bib106">Sabsevitz et al., 2005</xref>). These findings suggest that abstract concepts are learned by understanding their role in a linguistic context, including semantic relationships with other words (e.g. ‘democracy’ is understood through its relationships to words like ‘people,’ ‘parliament,’ ‘politics,’ etc., e.g. <xref ref-type="bibr" rid="bib64">Jones et al., 2012</xref>). However, neurobiological data also support the view that subcategories of abstract concepts retain sensorimotor information (<xref ref-type="bibr" rid="bib48">Harpaintner et al., 2022</xref>; <xref ref-type="bibr" rid="bib47">Harpaintner et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Harpaintner et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Fernandino et al., 2022</xref>) as well as social information (<xref ref-type="bibr" rid="bib131">Villani et al., 2019</xref>; <xref ref-type="bibr" rid="bib22">Conca et al., 2021a</xref>) and internal/interoceptive/affective experiences (<xref ref-type="bibr" rid="bib88">Oosterwijk et al., 2015</xref>; <xref ref-type="bibr" rid="bib129">Vigliocco et al., 2014</xref>), which are also important for learning abstract concepts (<xref ref-type="bibr" rid="bib95">Ponari et al., 2018</xref>). Thus, abstract concepts constitute a more heterogeneous category (<xref ref-type="bibr" rid="bib105">Roversi et al., 2013</xref>; <xref ref-type="bibr" rid="bib143">Zdrazilova et al., 2018</xref>; <xref ref-type="bibr" rid="bib131">Villani et al., 2019</xref>; <xref ref-type="bibr" rid="bib85">Muraki et al., 2020</xref>; <xref ref-type="bibr" rid="bib85">Muraki et al., 2020</xref>; <xref ref-type="bibr" rid="bib70">Kiefer et al., 2022</xref>).</p><p>A limitation of these studies is that they have only investigated the processing of decontextualized concepts (see e.g. <xref ref-type="table" rid="table1">Table 1</xref> in a recent review by <xref ref-type="bibr" rid="bib30">Del Maschio et al., 2022</xref>). That is, they (often implicitly) assume that conceptual representations in the brain are the product of a stable set of regions processing different types of information depending on whether a concept is concrete or abstract. However, this dichotomy may not account for the way in which we typically process concepts (<xref ref-type="bibr" rid="bib78">Lebois et al., 2015</xref>), given that the information encoded during conceptual processing depends on the contextual information available. For example, the situated (i.e. contextualized in the discourse but also in the physical setting in which processing occurs) processing of concrete concepts like ‘chair’ could be linked to many abstract internal elements like goals (‘I want to rest’), motivations (‘I have been standing for 2 hr’), emotions (‘I would like to feel comfortable’), and theory of mind (‘is that older person more in the need of this chair than me?’). Conversely, an abstract concept like ‘truth’ is no longer particularly abstract when used in reference to a perceived physical situation (such as ‘snowing’) that matches the utterance’s meaning (‘it is true that it is snowing’). Here, ‘truth’ refers to a concrete state of the world (<xref ref-type="bibr" rid="bib7">Barsalou et al., 2018</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Complete Meta-analytic description of clusters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dimension</th><th align="left" valign="bottom">Abstract Clusters(N=35)</th><th align="left" valign="bottom">Concrete Clusters(N=20)</th><th align="left" valign="bottom">Kruskal-Wallis Test</th></tr></thead><tbody><tr><td align="left" valign="bottom">Autob. Memory</td><td align="left" valign="bottom">9</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">H(2)=4, <italic>P</italic>=0.05</td></tr><tr><td align="left" valign="bottom">Valence</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=5.6, <italic>P</italic>=.01</td></tr><tr><td align="left" valign="bottom">Theory of Mind</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=4.8, <italic>P</italic>=0.03</td></tr><tr><td align="left" valign="bottom">Nausea</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=4.8, <italic>P</italic>=0.03</td></tr><tr><td align="left" valign="bottom">Pain</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=4, <italic>P</italic>=0.05</td></tr><tr><td align="left" valign="bottom">Movement</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">H(2)=12.4,<italic>P</italic>&lt;.001</td></tr><tr><td align="left" valign="bottom">Social/Empathy</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">H(2)=0.7, <italic>P</italic>=0.42</td></tr><tr><td align="left" valign="bottom">Touch</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=1.8, <italic>P</italic>=0.18</td></tr><tr><td align="left" valign="bottom">Speech</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">H(2)=1.1, <italic>P</italic>=0.29</td></tr><tr><td align="left" valign="bottom">Language</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">H(2)=0.08, <italic>P</italic>=0.78</td></tr><tr><td align="left" valign="bottom">Reading</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=1.8, <italic>P</italic>=0.18</td></tr><tr><td align="left" valign="bottom">Reward/Motivation</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">H(2)=0.24, <italic>P</italic>=0.63</td></tr><tr><td align="left" valign="bottom">Vision</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">H(2)=5.3, <italic>P</italic>=0.02</td></tr><tr><td align="left" valign="bottom">Listening</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">H(2)=0.02, <italic>P</italic>=0.88</td></tr><tr><td align="left" valign="bottom">Planning</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=1.7, <italic>P</italic>=0.19</td></tr><tr><td align="left" valign="bottom">Calculation</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=1.7, <italic>P</italic>=0.19</td></tr></tbody></table></table-wrap><p>Indeed, previous work has postulated flexible conceptual processing in experiential brain circuits (<xref ref-type="bibr" rid="bib12">Binder and Desai, 2011</xref>; <xref ref-type="bibr" rid="bib98">Pulvermüller, 2018a</xref>). Behavioral data support the view that contextual information can affect conceptual processing (e.g., <xref ref-type="bibr" rid="bib19">Chambers et al., 2004</xref>; <xref ref-type="bibr" rid="bib24">Cooper, 1974</xref>; <xref ref-type="bibr" rid="bib120">Tanenhaus et al., 1995</xref>). For example, when an object is depicted in a visual context consistent with its use, the action associated with using the object is more readily available than when the context is more consistent with picking the object up (<xref ref-type="bibr" rid="bib65">Kalénine et al., 2014</xref>). There is also neurobiological evidence that objects visually present in a situation can influence conceptual processing (<xref ref-type="bibr" rid="bib57">Hoffman et al., 2013</xref>; <xref ref-type="bibr" rid="bib141">Yee and Thompson-Schill, 2016</xref>). For example, task-related color-congruency of objects correlates with less activation of brain regions involved in color perception during processing – likely because less retrieval of detailed color knowledge was necessary (<xref ref-type="bibr" rid="bib60">Hsu et al., 2011</xref>). Dynamic, context-dependent recruitment of visual and motor-related areas during semantic processing has also been established (<xref ref-type="bibr" rid="bib56">Hoenig et al., 2008</xref>; <xref ref-type="bibr" rid="bib124">van Dam et al., 2012</xref>; <xref ref-type="bibr" rid="bib96">Popp et al., 2019</xref>). An understanding of conceptual knowledge as static and context-independent is insufficient to account for these dynamics (<xref ref-type="bibr" rid="bib99">Pulvermüller, 2018b</xref>).</p><p>However, no previous study has addressed whether the brain areas associated with concrete and abstract concepts are fixed or recruited in a more dynamic way during semantic processing. The present study aims to fill this gap and test the following two predictions. First, we submit that results from previous investigations of conceptual processing, which generally depict a stable dichotomy between concrete and abstract words, reflect the average experiential information of the type of situational context in which concepts are habitually experienced. Therefore, we predict that the neurobiological representation of concrete concepts, will be related to associated brain regions, because they retain experiences related to their physical referents that are predominantly characterized by sensory and motor information, (<xref ref-type="bibr" rid="bib99">Pulvermüller, 2018b</xref>; <xref ref-type="bibr" rid="bib139">Willems et al., 2010</xref>). In contrast, because their representations mostly reflect information related to internal/interoceptive/affective experience as well as linguistic information, we expect abstract concepts to activate brain regions associated with emotional, interoceptive, and general linguistic processing (<xref ref-type="bibr" rid="bib104">Reinboth and Farkaš, 2022</xref>).</p><p>Second, the reviewed work also suggests that these habitual representations are not necessarily stable and might change during naturalistic processing depending on the specific contextual information available. We specify two context conditions: a concept is <italic>displaced</italic> if its context offers little or no visual information related to the concept’s external sensory-motor features. In contrast, a concept is <italic>situated</italic>, if its visual context contains objects related to its meaning. We predict that when a concrete concept is processed in displaced situations (e.g. ‘cat’ processed when discussing the general character traits of cats vs dogs), response profiles will shift towards more internalized processing shared with abstract concepts. In contrast, when an abstract concept is processed in a situated context (for example the word ‘love’ processed in a scene with people kissing), its representation will more heavily draw on regions involved in processing external, visual information that otherwise characterize more concrete concepts.</p><p>We propose that this erosion of the concrete/abstract dichotomy for contextualized processing shows that both concrete and abstract concepts draw on information related to experience (external and internal) as well as linguistic association. Which associated neurobiological structures are engaged during processing depends dynamically on the contextual information available. This way of thinking about the representational nature of conceptual knowledge may help reconcile contradictory evidence concerning the involvement of different brain areas during conceptual processing (for example as discussed in <xref ref-type="bibr" rid="bib90">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="bib59">Hoffman et al., 2018</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Conceptual processing across contexts</title><p>Consistent with previous studies, we predicted that across naturalistic contexts, concrete and abstract concepts are processed in a separable set of brain regions. To test this, we contrasted concrete and abstract modulators at each time point of the IRF (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This showed that concrete produced more modulation than abstract processing in parts of the frontal lobes, including the right posterior inferior frontal gyrus (IFG) and the precentral sulcus (<xref ref-type="fig" rid="fig1">Figure 1, red</xref>). Known for its role in language processing and semantic retrieval, the IFG has been hypothesized to be involved in the processing of action-related words and sentences, supporting both semantic decision tasks and the retrieval of lexical semantic information (<xref ref-type="bibr" rid="bib14">Bookheimer, 2002</xref>; <xref ref-type="bibr" rid="bib44">Hagoort, 2005</xref>). The precentral sulcus is similarly linked to the processing of action verbs and motor-related words (<xref ref-type="bibr" rid="bib97">Pulvermüller, 2005</xref>). In the temporal lobes, greater modulation occurred in the bilateral transverse temporal gyrus and sulcus, planum polare, and temporale. These areas, including primary and secondary auditory cortices, are crucial for phonological and auditory processing, with implications for the processing of sound-related words and environmental sounds (<xref ref-type="bibr" rid="bib12">Binder and Desai, 2011</xref>) . The superior temporal gyrus (STG) and sulcus (STS) also showed greater modulation for concrete words and these are said to be central to auditory processing and the integration of phonological, syntactic, and semantic information, with a particular role in processing meaningful speech and narratives (<xref ref-type="bibr" rid="bib55">Hickok and Poeppel, 2007</xref>) . In the parietal and occipital lobes, more concrete modulated activity was found bilaterally in the precuneus, which has been associated with visuospatial imagery, episodic memory retrieval, and self-processing operations and has been said to contribute to the visualization aspects of concrete concepts (<xref ref-type="bibr" rid="bib18">Cavanna and Trimble, 2006</xref>) . More activation was also found in large swaths of the occipital cortices (running into the inferior temporal lobe), and the ventral visual stream. These regions are integral to visual processing, with the ventral stream (including areas like the fusiform gyrus) particularly involved in object recognition and categorization, linking directly to the visual representation of concrete concepts (<xref ref-type="bibr" rid="bib110">Simmons et al., 2007</xref>). Finally, subcortically, the dorsal and posterior medial cerebellum were more active bilaterally for concrete modulation. Traditionally associated with motor function, some studies also implicate the cerebellum in cognitive and linguistic processing, including the modulation of language and semantic processing through its connections with cerebral cortical areas (<xref ref-type="bibr" rid="bib119">Stoodley and Schmahmann, 2009</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Neurobiology of conceptual processing across contexts.</title><p>Colored regions show group-level results from a linear mixed effect model and subsequent general linear tests contrasting activity for concrete (red) versus abstract (blue) modulation at each of 20 timepoints after word onset. Overlapping regions (yellow) indicate a concrete and abstract difference at one of these timepoints. Results are thresholded and corrected for multiple comparisons at <italic>α</italic>=0.01 and displayed with a cluster size ≧ 20 voxels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Comparison between overlap and language regions.</title><p>Comparison between the language network (outline mask extracted from Neurosynth) and overlap activity for both concrete and abstract words across context. Results are thresholded and corrected for multiple comparisons at <italic>α</italic>=0.01 and displayed with a cluster size ≧ 20 voxels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Time course of activation.</title><p>Voxel activation based on BOLD signal (seconds from word-onset), correlated with abstract (blue) and concrete (red). Activation for concrete and abstract concepts both started at 3 s. Processing of concrete concepts correlated more with activity in vision and motor systems, while processing of abstract concepts correlated with activation in the thalamus and somatosensory cortex. Activation for concrete concepts then spread into the superior temporal lobe (bilaterally), during 3–8 s. On the other hand, activation for abstract concepts started in the somatosensory cortex, thalamus. Activation then spreads into the anterior cingulate cortex (5 s) and spreads into medial prefrontal (LH) areas as well as Inferior frontal and angular gyrus (LH) during 5–9 s.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig1-figsupp2-v1.tif"/></fig></fig-group><p>Conversely, activation for abstract was greater than concrete words in the following regions (<xref ref-type="fig" rid="fig1">Figure 1</xref>, blue): In the frontal lobes, this included the right anterior cingulate gyrus, lateral and medial aspects of the superior frontal gyrus. Being involved in cognitive control, decision-making, and emotional processing, these areas may contribute to abstract conceptualization by integrating affective and cognitive components (<xref ref-type="bibr" rid="bib109">Shenhav et al., 2013</xref>) . More left frontal activity was found in both lateral and medial prefrontal cortices, and in the orbital gyrus, regions which are key to social cognition, valuation, and decision-making, all domains rich in abstract concepts (<xref ref-type="bibr" rid="bib2">Amodio and Frith, 2006</xref>). In the parietal lobes, bilateral activity was greater in the angular gyri (AG) and inferior parietal lobules, including the postcentral gyrus. Central to the default mode network, these regions are implicated in a wide range of complex cognitive functions, including semantic processing, abstract thinking, and integrating sensory information with autobiographical memory (<xref ref-type="bibr" rid="bib108">Seghier, 2013</xref>). In the temporal lobes, activity was restricted to the STS bilaterally, which plays a critical role in the perception of intentionality and social interactions, essential for understanding abstract social concepts (<xref ref-type="bibr" rid="bib40">Frith and Frith, 2003</xref>). Subcortically, activity was greater, bilaterally, in the anterior thalamus, nucleus accumbens, and left amygdala for abstract modulation. These areas are involved in motivation, reward processing, and the integration of emotional information with memory, relevant for abstract concepts related to emotions and social relations (<xref ref-type="bibr" rid="bib43">Haber and Knutson, 2010</xref>; <xref ref-type="bibr" rid="bib93">Phelps and LeDoux, 2005</xref>).</p><p>Finally, there was an overlap in activity between modulation of both concreteness and abstractness (<xref ref-type="fig" rid="fig1">Figure 1</xref>, yellow). The overlap activity is due to the fact that we performed general linear tests for the abstract/concrete contrast at each of the 20 timepoints in our group analysis. Consequently, overlap means that activation in these regions is modulated by both concrete and abstract word processing but at different time-scales. In particular, we find that activity modulation associated with abstractness is generally processed over a longer time-frame (for a comparison of significant timing differences see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). In the frontal, parietal, and temporal lobes, this was primarily in the left IFG, AG, and STG, respectively. Left IFG is prominently involved in semantic processing, particularly in tasks requiring semantic selection and retrieval, and has been shown to play a critical role in accessing semantic memory and resolving semantic ambiguities, processes that are inherently time-consuming and reflective of the extended processing time for abstract concepts (<xref ref-type="bibr" rid="bib122">Thompson-Schill et al., 1999</xref>; <xref ref-type="bibr" rid="bib133">Wagner et al., 2001</xref>; <xref ref-type="bibr" rid="bib58">Hoffman et al., 2015</xref>). The STG, particularly its posterior portion, is critical for the comprehension of complex linguistic structures, including narrative and discourse processing. The processing of abstract concepts often necessitates the integration of contextual cues and inferential processing, tasks that engage the STG and may extend the temporal dynamics of semantic processing (<xref ref-type="bibr" rid="bib38">Ferstl et al., 2008</xref>; <xref ref-type="bibr" rid="bib125">Vandenberghe et al., 2002</xref>). In the occipital lobe, processing overlapped bilaterally around the calcarine sulcus, which is associated with primary visual processing (<xref ref-type="bibr" rid="bib66">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib73">Kosslyn et al., 2001</xref>).</p><sec id="s2-1-1"><title>Meta-analytic results</title><p>Overall, these results suggest that concrete modulation engages sensory and motor regions more, whereas abstract words engage regions more associated with semantic as well as internal/interoceptive/affective processing. Both categories overlap (though necessarily at different time points) in regions typically associated with word processing. However, these interpretations are based on informal reverse inference. To more formally and quantitatively evaluate this distinction between concrete and abstract words, we employed meta-analytic description and reverse correlation analyses. Both test whether brain regions involved in concrete and abstract conceptual processing reflect different types of habitual experience (i.e. sensory-motor vs internal/interoceptive/affective).</p><p>Term-based labeling demonstrates that significantly more concrete clusters are related to the term ‘Movement’ compared to abstract clusters (H(2) = 12.4, p&lt;0.001; <xref ref-type="fig" rid="fig2">Figure 2, red</xref>). In contrast, abstract clusters are more related to terms that are arguably associated with internal/interoceptive/affective processing compared to concrete activation clusters, i.e., ‘Autobiographical Memory,’ ‘Nausea,’ ‘Pain,’ ‘Reward/Motivation,’ and ‘Valence’ (all ps &lt;0.05; <xref ref-type="fig" rid="fig2">Figure 2</xref>). Finally, ‘Language’ was the only term more associated with overlapping clusters than either concrete (H(2) = 7, p&lt;0.001) or abstract clusters (H(2) = 4, p=0.045; <xref ref-type="fig" rid="fig2">Figure 2</xref>). For meta-analytic associations of each individual cluster, see <xref ref-type="table" rid="table1">Table 1</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Meta-analytic description of conceptual processing across contexts.</title><p>We used the Neurosynth meta-analysis package to find the terms associated with the centers of mass for each concrete (red), abstract (blue), and overlap (yellow) cluster from <xref ref-type="fig" rid="fig1">Figure 1</xref>. Numbers refer to the number of activation clusters associated with each meta-analytic term. There were significantly more concrete than abstract clusters for the term ‘Movement’ (p&lt;0.001), whereas there were more abstract compared to concrete clusters for ‘Autobiographical Memory,’ ‘Nausea,’ ‘Pain,’ ‘Theory of Mind,’ and ‘Valence’ (all p’s &lt;0.05). The term 'language' was significantly more associated with overlap clusters compared to concrete (p&lt;0.001) and abstract clusters (p=0.045).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig2-v1.tif"/></fig></sec><sec id="s2-1-2"><title>Peaks and valleys results</title><p>Comparing dimensions for abstract vs concrete modulated clusters, we found significantly more concrete compared to abstract clusters associated with the dimension ‘Torso’ H(2)=7, p&lt;0.001. Three concrete clusters were associated with ‘Haptic’ and ‘Mouth,’ which was also significantly more than for abstract clusters (all tests H(2)=5.2, all p’s = 0.02). Two concrete clusters with ‘Foot_Leg’ compared to 0 abstract clusters was not significant, but the mean was in the expected direction H(2)=3.4, p=0.06 All concrete clusters are displayed in <xref ref-type="fig" rid="fig3">Figure 3</xref> (red). Conversely, eight abstract clusters were significantly more associated with the dimension ‘Valence’ than concrete clusters (H(2)=8.3, p&lt;0.001). Three abstract clusters were associated with the dimension ‘Auditory,’ which was not significantly different from concrete clusters (H(2)=1.9, p=0.17). All abstract clusters are displayed in <xref ref-type="fig" rid="fig3">Figure 3</xref> (blue). Finally, five clusters in which modulation through concreteness and abstractness overlapped (though at different time points) were significantly more associated with the dimension ‘Mouth’ compared to two concrete clusters H(2) = 5.1, p=0.03 and 0 abstract clusters H(2)=7.8, p&lt;0.001. All overlap clusters are displayed in <xref ref-type="fig" rid="fig3">Figure 3</xref> (yellow). For all results of the peak and valley tests for each individual cluster, see <xref ref-type="table" rid="table2">Table 2</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Peak and valley analysis results for understanding conceptual processing across contexts.</title><p>We extract the type of information processed in each activation cluster by looking at experience-based features of movie words that are aligned with significantly more peaks than valleys (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Words highly rated on the sensorimotor dimensions ‘Haptic,’ ‘Hand_Arm,’ and ‘Torso’ were significantly more associated with concrete clusters (red, all p’s &lt;0.05), ‘Valence’ with abstract clusters (blue, p&lt;0.001) and ‘Mouth’ with overlap clusters (yellow, p’s &lt;0.05). For some features/terms, there were never significantly more words highly rated on that dimension occurring at peaks compared to valleys, so they do not have any significant clusters.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Peak and valley analysis for a 4 s lag.</title><p>A Kruskal-Wallis test shows that the distribution between sensorimotor and interoceptive/emotional dimensions for concrete and abstract words is significantly different from a 5 s H(2)=4,8, p=0.03, and 6 s (H(2)=5.3, p=0.02) lag.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Peak and valley analysis at the individual participant level (5s lag).</title><p>Peak and valley Analysis for a 5 s lag. Internal dimensions Valence and Arousal are significantly more associated with peaks in abstract compared to concrete clusters (Valence: H(2) = 5.8, p=0.02; Arousal: H(2) = 6.7, p=0.01). Conversely, concrete clusters are more associated with sensorimotor dimensions (Hand_Arm, Foot_Leg and Visual) – though not significantly so. Overlap is significantly more associated with ‘Mouth’ (H(2) = 6.2, p=0.2).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Peak and valley analysis after averaging results for individual participants for abstract clusters only.</title><p>Results show that the peaks in the BOLD response are most correlated with dimensions ‘A.Mean.Sum (Arousal),’ ‘V.Mean.Sum (Valence),’ and ‘Interoceptive.mean (Interoception).’</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Peak and valley analysis after averaging results for individual participants for concrete clusters only.</title><p>Results show that the peaks in the BOLD response are most correlated with dimensions ‘Visual.mean (Visual),’ ‘Foot_leg.mean (Foot_Leg),’ ‘Torso.mean (Torso),’ ‘Haptic.mean (Haptic),’ and ‘Hand_arm.mean (Hand_arm).’</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Peak and valley analysis after averaging results for individual participants for overlap clusters only.</title><p>Results show that the peaks in the BOLD response are most correlated with the dimension ‘Mouth.mean’ (Mouth).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp5-v1.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>Test of nonlinearity for peak and valley analysis.</title><p>p-values for all possible pairwise correlations of BOLD responses related to each feature. Only 3.85% of responses were significantly different.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp6-v1.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Overview of the features that showed nonlinear interactions.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig3-figsupp7-v1.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Peak and valley results between concrete and abstract activation clusters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dimension</th><th align="left" valign="bottom">Abstract clusters(N=35)</th><th align="left" valign="bottom">Concrete clusters(N=20)</th><th align="left" valign="bottom">Kruskal-Wallis test</th></tr></thead><tbody><tr><td align="left" valign="bottom">Valence</td><td align="left" valign="bottom">9</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">H(2)=4, p=0.05</td></tr><tr><td align="left" valign="bottom">Interoceptive</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=5.6, p=0.01</td></tr><tr><td align="left" valign="bottom">Arousal</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=4.8, p=0.03</td></tr><tr><td align="left" valign="bottom">Auditory</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=4.8, p=0.03</td></tr><tr><td align="left" valign="bottom">Visual</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">H(2)=4, p=0.05</td></tr><tr><td align="left" valign="bottom">Head</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">11</td><td align="left" valign="bottom">H(2)=12.4, p&lt;0.001</td></tr><tr><td align="left" valign="bottom">Haptic</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">H(2)=5.3, p=0.02</td></tr><tr><td align="left" valign="bottom">Foot_Leg</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">H(2)=3.5, p=0.06</td></tr><tr><td align="left" valign="bottom">Hand_Arm</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">H(2)=1.7, p=0.19</td></tr><tr><td align="left" valign="bottom">Torso</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">H(2)=7.3, p=0.01</td></tr><tr><td align="left" valign="bottom">Gustatory</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">H(2)=3.5, p=0.06</td></tr><tr><td align="left" valign="bottom">Mouth</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">3</td><td align="left" valign="bottom">H(2)=5.3, p=0.02</td></tr><tr><td align="left" valign="bottom">Head</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">/</td></tr></tbody></table></table-wrap></sec></sec><sec id="s2-2"><title>Conceptual processing in context</title><p>Activation associated with the main effect of word_type overlapped with processing of concrete and abstract words across context in superior temporal sulcus, superior temporal gyrus and middle temporal gyrus (bilateral), in angular gyrus (bilateral), in the central sulcus and precentral and postcentral gyrus (right hemisphere), in lateral and medial frontal cortices as well as in the occipital lobe (see <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Activation for the main effect of context was found bilaterally in posterior temporal lobe at the intersection with occipital lobe, as well as in nodes of the default mode network (DMN), including precuneus, medial prefrontal regions and angular gyrus (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The interaction between word_type and context-modulated activity in the main nodes of the DMN (amongst other regions), including precuneus, medial prefrontal regions, and angular gyrus (all bilaterally, see <xref ref-type="fig" rid="fig4">Figure 4C</xref>). Indeed, the thresholded interaction map with 1501 voxels was ‘decoded’ using the Neurosynth package, where the Pearson correlation is computed between the vectorized map and all the maps in the Neurosynth database. The top four associated terms (excluding brain regions or methodological terms) were the ‘DMN’ (r(1500)=0.194, p&lt;0.001), ‘Default Mode’ (r(1500)=0.219, p&lt;0.001), and ‘Default’ (r(1500)=0.226, p&lt;0.001) as well as ‘Semantic Control’ (r(1500)=0.206, p&lt;0.001).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Main effects of word_type, context, and their interaction.</title><p>(A) Main effect of word-type. Most significantly modulated areas include superior temporal sulcus, superior temporal gyrus and middle temporal gyrus (bilateral), angular gyrus (bilateral), the central sulcus and precentral and postcentral gyrus (right hemisphere), as well as lateral and medial frontal cortices and the occipital lobe. (<bold>B</bold>) Main effect of context. Most significantly modulated areas include the intersection between posterior temporal and occipital lobe, the Precuneus, Middle Prefrontal Cortex, as well as Angular Gyrus, and right inferior frontal gyrus. (<bold>C</bold>) Interaction between context (high/low) and word type (abstract/concrete). Most significantly modulated areas include the Precuneus, Middle Prefrontal Cortex as well as Middle Frontal Gyrus, Angular Gyrus, and Posterior Cingulate Cortex. These correspond to the nodes of the default mode network, as well as areas commonly associated with semantic control. This was confirmed by using the neurosynth decoder on the unthresholded brain image - top keywords were ‘Semantic Control’ and ‘DMN.’ All displayed results are thresholded and corrected for multiple comparisons at <italic>α</italic>=0.01 and displayed with a cluster size ≧ 20 voxels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig4-v1.tif"/></fig><p>To better understand the nature of this interaction and how it relates to response profiles associated with concreteness and abstractness across contexts, we contrasted concrete vs abstract modulation in situated and displaced conditions (collapsing across timepoints, as we had no prediction about timing differences). This comparison is displayed in <xref ref-type="fig" rid="fig5">Figure 5</xref>. We then extracted the resulting activation maps as masks and spatially correlated them with the brain mask obtained from the activation map contrasting concreteness and abstractness across contexts. This comparison is displayed in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Contrasts between situated abstract and displaced abstract (<bold>A</bold>) as well as situated concrete and displaced concrete (<bold>B</bold>).</title><p>The displaced concrete activation mask was later correlated with abstract processing across context (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). The situated abstract activation mask was later correlated with concrete processing across context (see <xref ref-type="fig" rid="fig6">Figure 6</xref>). Nodes of the default mode network (DMN) are especially active in the displaced condition for both abstract and concrete words. Visual and sensorimotor areas are especially active in situated conditions for both abstract and concrete words. Results are thresholded and corrected for multiple comparisons at <italic>α</italic>=0.01 and displayed with a cluster size ≧ 20 voxels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig5-v1.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Spatial overlap between thresholded statistical brain images of concrete and abstract conceptual processing obtained from the original analysis across contexts situatedness/displacement contrasts (on the left).</title><p>Original brain maps of our analysis across context are split into abstract (top) and concrete (bottom) on the right. The overlap between displaced concrete and abstract was (r(72,964) = 0.49, p&lt;0.001), the overlap between situated abstract and concrete was (r(72,964)=0.64, p&lt;0.001). All maps were thresholded at <italic>α</italic>=0.01 with a cluster size ≧ 20 voxels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig6-v1.tif"/></fig><p>To quantify this comparison, we used cosine similarity to calculate spatial correlation measures between the unthresholded results from contrasting concrete and abstract modulations in displaced and situated context with the unthresholded contrasts between concrete and abstract modulations across contexts (<xref ref-type="fig" rid="fig1">Figure 1</xref>, red is the thresholded version of this map). This shows that in situated context, the contrasted modulation by abstractness overlaps more with concreteness across context (r(72964)=0.64, p&lt;0.001) compared to displaced concreteness (r(72964)=0165, p=0.476). Concreteness across contexts overlaps with situated abstractness (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, red) bilaterally in the fusiform, occipital lobe, inferior and superior parietal lobules and with displaced concreteness bilaterally in the occipital lobe, as well as large swaths of the superior temporal gyrus (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>Conversely, in displaced context, the contrasted modulation by concreteness (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, blue) overlaps more with the pattern of activity modulated by abstractness across context (r(72,964) = 0.49, p&lt;0.001) compared to situated abstractness (r(72,964) = 0.21, p&lt;0.001). Abstractness across contexts overlaps with displaced concreteness bilaterally in large portions of the inferior parietal lobule, including supramarginal gyrus and post superior temporal sulcus, up to the intersection of the occipital and temporal lobes, lingual gyrus in particular (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Overlap activation could also be found bilaterally in anterior thalamus and medial prefrontal regions.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Conceptual processing is typically investigated in experiments where words are stripped away from their naturally occurring context: most studies use isolated words, and sometimes sentences (see <xref ref-type="table" rid="table1">Table 1</xref> in <xref ref-type="bibr" rid="bib30">Del Maschio et al., 2022</xref>). However, conceptual processing in its ecology occurs in rich multimodal contexts. Our study investigated naturalistic conceptual processing during movie-watching to begin to understand the effect of multimodal context on the neurobiological organization of real-world conceptual representation.</p><sec id="s3-1"><title>Conceptual processing across contexts</title><p>First, we asked where in the brain concrete and abstract concepts are processed across different contexts as well as the type of information they encode. Given the hypothesis that conceptual representations reflect contextual information, we expected a set of regions that correspond to the most typical set of experiences (e.g. as encountered during word learning in development) to activate across different contexts. Specifically, we expected concrete conceptual encoding to activate regions more involved in sensory and motor processing and abstract conceptual encoding to activate regions associated with more internal/interoceptive/affective as well as general linguistic processing (<xref ref-type="bibr" rid="bib3">Anderson et al., 2019</xref>; <xref ref-type="bibr" rid="bib83">Meteyard et al., 2012</xref>; <xref ref-type="bibr" rid="bib10">Binder et al., 2005</xref>).</p><p>Indeed, we found a general tendency for concrete and abstract words to activate regions associated with different experiences (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Consistent with prior work, concrete words were associated with multiple regions involved in sensory and motor processing (<xref ref-type="bibr" rid="bib84">Mkrtychian et al., 2019</xref>), including most of the visual system (<xref ref-type="bibr" rid="bib41">Gao et al., 2019</xref>) and the right frontal motor system (<xref ref-type="bibr" rid="bib97">Pulvermüller, 2005</xref>). In contrast, abstract words engaged regions typically associated with internal/interoceptive/affective processing (anterior thalamus, somatosensory cortex)(<xref ref-type="bibr" rid="bib46">Harpaintner et al., 2018</xref>; <xref ref-type="bibr" rid="bib132">Villani et al., 2021</xref>), autobiographical memory (anterior medial prefrontal regions) (<xref ref-type="bibr" rid="bib22">Conca et al., 2021a</xref>), and emotional processing and regulation (anterior medial prefrontal regions, orbital prefrontal cortex, dorsolateral prefrontal cortex, nucleus accumbens and amygdala) (<xref ref-type="bibr" rid="bib129">Vigliocco et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Conca et al., 2021b</xref>).</p><p>Consistent with this, both meta-analytic and peak and valley analyses showed that concrete regions were more associated with sensory-motor properties (e.g. ‘Movement’ and ‘Hand_Arm’) whereas abstract regions were more associated with internal/interoceptive/affective properties (e.g. ‘Valence;’ <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). Together, these results provide evidence from naturalistic processing that concrete and abstract concepts encode different types of experiences (<xref ref-type="bibr" rid="bib128">Vigliocco et al., 2009</xref>; <xref ref-type="bibr" rid="bib6">Barsalou and Wiemer-Hastings, 2005</xref>; <xref ref-type="bibr" rid="bib71">Kiehl et al., 1999</xref>).</p><p>At the level of brain regions, our study aligns with previous literature identifying distinct brain regions engaged in processing abstract versus concrete words. Specifically, our results show greater activation for concrete words in temporo-parieto-occipital regions. These areas include the bilateral middle temporal gyrus, the left fusiform gyrus, and the bilateral angular gyrus, among others. Conversely, our study found that abstract word processing preferentially engages a network of regions within the left hemisphere, including the inferior frontal gyrus (IFG), superior and middle temporal gyri, and the inferior parietal lobule.</p><p>However, the regions involved in processing concrete and abstract concepts across contexts did not imply a fully dichotomous encoding of experiences. First, we found that regions involved in sensory (mostly in visual cortices) and motor processing are involved in processing both types of words (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Moreover, we found overlap activation in regions associated with language processing in general (<xref ref-type="bibr" rid="bib121">Tang et al., 2022</xref>, <xref ref-type="fig" rid="fig1">Figure 1</xref>). Such results are in line with proposals in which both concrete and abstract representations rely on experiential information as well as their linguistic relationships with other words (e.g. <xref ref-type="bibr" rid="bib128">Vigliocco et al., 2009</xref>; <xref ref-type="bibr" rid="bib128">Vigliocco et al., 2009</xref>; <xref ref-type="bibr" rid="bib94">Piantadosi and Hill, 2022</xref>). This latter hypothesis is also supported by our Peaks and Valleys analysis, more specifically that information related to ‘Mouth’ (i.e. the language organ) drives activation in overlap clusters. This is furthermore evidence against hypotheses in which the mouth is specifically associated with abstract concepts (<xref ref-type="bibr" rid="bib15">Borghi and Zarcone, 2016</xref>).</p></sec><sec id="s3-2"><title>Conceptual processing in context</title><p>Though results across contexts presumably represent a form of experiential central tendency, the behavioral, neuroimaging, and electrophysiological literature suggests that conceptual representations might not be stable and may vary as a function of context (<xref ref-type="bibr" rid="bib34">Elman, 1995</xref>; <xref ref-type="bibr" rid="bib118">Spivey and Dale, 2006</xref>; <xref ref-type="bibr" rid="bib17">Cai and Vigliocco, 2018</xref>; <xref ref-type="bibr" rid="bib76">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="bib141">Yee and Thompson-Schill, 2016</xref>; <xref ref-type="bibr" rid="bib31">Deniz et al., 2023</xref>). For this reason, we conducted a second set of analyses with the goal of understanding the extent to which representations associated with concrete or abstract conceptual processing in the brain change as a function of context (<xref ref-type="bibr" rid="bib7">Barsalou et al., 2018</xref>).</p><p>We find that brain activation underlying concrete and abstract conceptual processing fundamentally changes as a function of visual context. We compared the activation profiles of concrete and abstract concepts in displaced and situated contexts with the activations obtained when collapsing across contexts. Our results show that concrete concepts become more abstract-like in displaced contexts with less relevant visual information (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Overlap between activation for concrete concepts in displaced conditions and abstract concepts across context (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) can be found in ACC, thalamus, and large swaths of the anterior, middle, and posterior temporal lobe. We propose that this is because, when a concrete concept is processed in a displaced context, its representation will relate more to internal/interoceptive variables and linguistic associations, which are usually encoded by abstract concepts. Conversely, abstract concepts become more concrete-like when they are highly situated (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Overlap between activation for abstract concepts in situated conditions and concrete concepts across context (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) can be found in fusiform and the occipital lobe (bilateral). We propose that this is because an abstract concept processed in a situated context relates more to external visual information, which is usually encoded by concrete concepts. A consequence of this finding is that the concrete/abstract distinction is neurobiologically less stable than might be assumed. Brain regions ’switch alliance' during concrete or abstract word processing depending on context.</p><p>What is the neurobiological mechanism behind contextual modulation of conceptual encoding in the brain? Our results indicate that variance in visual context interacted with word-type (both concrete and abstract) in regions commonly defined as the DMN, as well as a set of prefrontal regions associated with semantic control (<xref ref-type="bibr" rid="bib101">Ralph et al., 2017</xref>; <xref ref-type="bibr" rid="bib59">Hoffman et al., 2018</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref> and <xref ref-type="fig" rid="fig3s2">2</xref>). Recent literature on the role of the DMN suggests that these regions reactivate memories (<xref ref-type="bibr" rid="bib29">Crittenden et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Konishi et al., 2015</xref>; <xref ref-type="bibr" rid="bib86">Murphy et al., 2018</xref>; <xref ref-type="bibr" rid="bib117">Sormaz et al., 2018</xref>; <xref ref-type="bibr" rid="bib141">Yee and Thompson-Schill, 2016</xref>; <xref ref-type="bibr" rid="bib4">Andrews-Hanna et al., 2014</xref>, <xref ref-type="bibr" rid="bib126">Vatansever et al., 2017</xref>) and contexts-specific information (<xref ref-type="bibr" rid="bib45">Hahamy et al., 2023</xref>), possibly to form contextually relevant situation models (<xref ref-type="bibr" rid="bib21">Chen et al., 2015</xref>; <xref ref-type="bibr" rid="bib103">Raykov et al., 2020</xref>, <xref ref-type="bibr" rid="bib102">Ranganath and Ritchey, 2012</xref>, <xref ref-type="bibr" rid="bib116">Smith et al., 2021</xref>, <xref ref-type="bibr" rid="bib115">Smith et al., 2017</xref>), in order to guide semantic cognition (<xref ref-type="bibr" rid="bib9">Binder et al., 1999</xref>; <xref ref-type="bibr" rid="bib36">Fernandino et al., 2016</xref>; <xref ref-type="bibr" rid="bib142">Yeshurun et al., 2021</xref>; <xref ref-type="bibr" rid="bib123">Tong et al., 2022</xref>).</p><p>Breaking up the interaction between word_type and context, we find that the DMN is especially involved in displaced conditions for both concrete and abstract conceptual processing (see <xref ref-type="fig" rid="fig4">Figure 4A, B</xref> (blue)). These results fit well with evidence suggesting that the DMN supports conceptual processing especially when displaced from sensorimotor input (<xref ref-type="bibr" rid="bib86">Murphy et al., 2018</xref>; <xref ref-type="bibr" rid="bib77">Lanzoni et al., 2020</xref>; <xref ref-type="bibr" rid="bib135">Wang et al., 2010</xref>). Accordingly, the DMN is most strongly activated in the displaced conditions involving abstract concepts. Given their inherent lack of sensorimotor information, abstract concepts offer a greater degree of displacement than their concrete counterparts, thereby demanding a higher engagement of the DMN in these conditions.</p><p>In considering the impact of visual context on the neural encoding of concepts generally, it is furthermore essential to recognize that the mechanisms observed may extend beyond visual processing to encompass more general sensory processing mechanisms. The human brain is adept at integrating information across sensory modalities to form coherent conceptual representations, a process that is critical for navigating the multimodal nature of real-world experiences (<xref ref-type="bibr" rid="bib7">Barsalou et al., 2018</xref>; <xref ref-type="bibr" rid="bib114">Smith, 2007</xref>). While our findings highlight the role of visual context in modulating the neural representation of abstract and concrete words, similar effects may be observed in contexts that engage other sensory modalities. For instance, auditory contexts that provide relevant sound cues for certain concepts could potentially influence their neural representation in a manner akin to the visual contexts examined in this study. Future research could explore how different sensory contexts, individually or in combination, contribute to the dynamic neural encoding of concepts, further elucidating the multimodal foundation of semantic processing.</p></sec><sec id="s3-3"><title>Conceptual processing and language</title><p>The exact relationship between concepts and language remains an open question, but it is undisputed that, as determinants of meaning, concepts are necessary for language (<xref ref-type="bibr" rid="bib61">Jackendoff, 2002</xref>; <xref ref-type="bibr" rid="bib13">Bloom, 2000</xref>; <xref ref-type="bibr" rid="bib35">Fauconnier and Turner, 2002</xref>). The present study examined language-driven conceptual processing, as we looked at brain activation during word processing. Our results imply that the underlying neurobiological processes are dynamically distributed and contextually determined. This view fits well with models of ‘natural’ organization of language in the brain where it is argued that language processing more generally is a whole brain process whose patterns of activation are determined by available context (<xref ref-type="bibr" rid="bib112">Skipper and Willems, 2015</xref>; <xref ref-type="bibr" rid="bib112">Skipper and Willems, 2015</xref>). These more distributed regions may be averaged away when indiscriminately analyzed together and following thresholding because (i) they are more variable given they are associated with different experiences (as we have seen here), linguistic categories (e.g. ‘formulaic speech;’ see <xref ref-type="bibr" rid="bib113">Skipper et al., 2022</xref>), and processes (e.g. different types of syntax) (ii) there are individual differences in all of these (e.g. <xref ref-type="bibr" rid="bib113">Skipper et al., 2022</xref>; <xref ref-type="bibr" rid="bib112">Skipper and Willems, 2015</xref>). These suppositions are supported by the fact that concrete and abstract modulation overlaps in typical perisylvian ‘language regions’ (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s3-4"><title>Conclusions</title><p>Our work emphasizes the merits of investigating conceptual processing in naturalistic multimodal contexts. This paves the way for future analyses systematically quantifying different types of contexts (e.g. in terms of related objects, actions, emotions, or social interactions) and examining how these can affect conceptual processing in the brain. Such work might further our understanding of the neurobiology of conceptual processing in naturalistic settings by clarifying what type of contexts affect processing and how. This may inform the recent development of multimodal large language models, where processing depends on context beyond purely text-based information (<xref ref-type="bibr" rid="bib32">Driess et al., 2023</xref>) - especially in naturalistic settings (<xref ref-type="bibr" rid="bib67">Kewenig et al., 2023</xref>). Apart from commercial applications, gaining a better understanding of the mechanisms underlying naturalistic conceptual processing in the brain might bear important implications for clinical domains, e.g., by informing progress towards helping patients who lost the ability to speak by real-time semantic reconstruction of non-invasive brain recordings with the help of large language models (<xref ref-type="bibr" rid="bib121">Tang et al., 2022</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>The present study analyzed the ‘Naturalistic Neuroimaging Database (NNDb)’ (<xref ref-type="bibr" rid="bib1">Aliko et al., 2020</xref>). All code is made available on a designated repository under (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts">https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts</ext-link>, copy archived at <xref ref-type="bibr" rid="bib130">ViktorKewenig, 2024</xref>).</p><sec id="s4-1"><title>Participants and task</title><p>The Naturalistic Neuroimaging Database (<xref ref-type="bibr" rid="bib1">Aliko et al., 2020</xref>, <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds002837/versions/2.0.0">https://openneuro.org/datasets/ds002837/versions/2.0.0</ext-link>) includes 86 right-handed participants (42 females, range of age 18–58 years, M = 26.81, SD = 10.09 years) undergoing fMRI while watching one of 10 full-length movies selected across a range of genres. All had unimpaired hearing and (corrected) vision. None had any contraindication for magnetic resonance imaging (MRI), history of psychiatric or neurological disorder, or language-related learning disabilities. All participants gave informed consent, and the study was approved by the University College London Ethics Committee (Reference Number 143/003).</p></sec><sec id="s4-2"><title>Data acquisition and preprocessing</title><p>Functional and anatomical images were obtained using a 1.5T Siemens MAGNETOM Avanto, equipped with a 32-channel head coil. Whole-brain images were captured, each consisting of 40 slices per volume at an isotropic resolution of 3.2 mm. These were obtained using a multiband echo-planar imaging (EPI) sequence with no in-plane acceleration, a multiband factor of 4 x, a repetition time of 1 s, an echo time of 54.8 milliseconds, and a flip angle of 75 degrees. Each study participant yielded a number of brain volumes equivalent to movie runtime in seconds. Due to software constraints limiting the EPI sequence to 1 hr of continuous scanning, there were mandatory breaks during the movie for all participants.</p><p>The data were preprocessed with AFNI (<xref ref-type="bibr" rid="bib26">Cox, 1996</xref>) and included despiking, slice-time correction, coregistration, blurring, and nonlinear alignment to the MNI152 template brain. The time series underwent smoothing using an isotropic full-width half-maximum of 6 mm, with detrending accomplished through regressors for motion, white matter, cerebrospinal fluid, and run length. Adjustments were made to account for breaks in movie viewing, and artifacts identified by spatial independent component analysis were regressed out. Detailed information on data acquisition and preprocessing is available in <xref ref-type="bibr" rid="bib1">Aliko et al., 2020</xref> and on <ext-link ext-link-type="uri" xlink:href="https://openneuro.org/">openneuro.org</ext-link>.</p></sec><sec id="s4-3"><title>Materials</title><p>All words in the movies were annotated using automated approaches with a machine learning-based speech-to-text transcription tool from Amazon Web Services (AWS; <ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/transcribe/">https://aws.amazon.com/transcribe/</ext-link>). The resulting transcripts contained on and offset timings for individual words. However, as not all words were transcribed or accurately transcribed, timings were subsequently corrected manually.</p><p>Concrete and abstract words were selected for the present study from existing <xref ref-type="bibr" rid="bib136">Warriner et al., 2013</xref> norms. In this database, 37,058 words were rated for concreteness on a scale from 0 (not experience-based) to 5 (experience-based) by over 4000 participants. We median split only content words on this scale to yield our set of concrete and abstract words. These were matched for word frequency within 1 SD of mean log frequency (3.61), using the SUTBLEX (US) corpus <xref ref-type="bibr" rid="bib16">Brysbaert and New, 2009</xref>, which contains frequency counts for 72,286 words. Concrete and abstract words were also matched to be within 1SD from mean length measured as number of letters (4.81).</p><p>After this matching process, we were left with more concrete words than abstract words in all movies (783 on average for concrete words; 440 for abstract words). To maintain equal numbers in the subsequent analysis, we randomly selected a subset of 440 concrete words in each movie to match the amount of abstract words, leaving us with 880 words (half concrete, half abstract) per movie on average. Mean concreteness rating for the resulting set of concrete words was 3.22, compared to 1.83 for abstract words. The final mean log frequency and mean length for the final set of concrete words was 3.69 and 4.91 compared to 3.46 and 5.27 for the final set of abstract words and were not significantly different as determined by t-tests (all p’s&gt;0.45). Surprisal given preceding linguistic context (within the same time window) was extracted with a customized script making use of the predictive processing nature of GPT-2 (<xref ref-type="bibr" rid="bib100">Radford et al., 2019</xref>), which has been shown to be a good model of processing difficulty (<xref ref-type="bibr" rid="bib138">Wilcox et al., 2021</xref>; <xref ref-type="bibr" rid="bib33">Duan et al., 2020</xref>; <xref ref-type="bibr" rid="bib82">Merkx and Frank, 2021</xref>; <xref ref-type="bibr" rid="bib107">Schrimpf et al., 2021</xref>). Mean surprisal of concrete words was 22.19 bits, mean surprisal of abstract words was 21.86 bits. A t-test revealed that this difference was not significant (t=1.23, p=0.218). Mean semantic diversity of concrete words was 1.92 and 1.96 of abstract words. This difference was also not significant (t=−1.20, p=0.230).</p><p>We used luminance and loudness to control for visual and acoustic properties of the movies that might vary more or less for concrete or abstract words. These ‘low-level’ features might be correlated with other potentially confounding auditory and visual variables. For example, luminance correlates significantly with stimulus intensity and contrast (<xref ref-type="bibr" rid="bib63">Johnson and Casson, 1995</xref>) and loudness correlates with pitch (<xref ref-type="bibr" rid="bib137">Wengenroth et al., 2014</xref>), prosody (<xref ref-type="bibr" rid="bib25">Couper-Kuhlen, 2004</xref>), and speaking rate (<xref ref-type="bibr" rid="bib75">Kuhlmann et al., 2022</xref>). Thus, luminance and loudness for each frame in the movie was measured using the ‘Librosa’ package for music and audio analysis in Python (<xref ref-type="bibr" rid="bib80">McFee et al., 2015</xref>). We then averaged these measures across the full duration of each word. Mean luminance for concrete words was 0.72, compared to 0.65 for abstract words. These were significantly different (t(4798)=9.13 p&lt;0.001). The mean loudness for concrete words was 0.69, compared to 0.77 for abstract words. These were also significantly different (t(4798)=9.86, p&lt;0.001).</p><p>For the analysis looking at conceptual processing within context, we similarly wanted to check for collinear variables in the 2 s context window preceding each word, which could have confounding effects. In particular, we looked at surprisal given linguistic context, as well as the visual variables motion (optical flow), color saturation, and spatial frequency. We extracted the visual features for each frame in the 2 s context window preceding each label using the scikit-image package (<xref ref-type="bibr" rid="bib134">Walt et al., 2014</xref>).</p></sec><sec id="s4-4"><title>Conceptual processing across contexts</title><p>In this analysis, we tested the prediction that when contextual information is averaged away, the neurobiological organization of conceptual processing will reflect brain systems involved in experiential and linguistic information processing, broadly in line with previous studies. Specifically, sensory and motor system engagement for concrete concepts and internal/interoceptive/affective and more general linguistic processing system engagement for abstract concepts. All statistical analyses on the preprocessed NiFTI files were carried out in AFNI (<xref ref-type="bibr" rid="bib26">Cox, 1996</xref>; <xref ref-type="bibr" rid="bib27">Cox and Hyde, 1997</xref>). Individual AFNI programs used are indicated parenthetically or in italics in subsequent descriptions.</p><sec id="s4-4-1"><title>Deconvolution analysis</title><p>We used an amplitude (also known as parametric) modulated deconvolution regression to estimate activity associated with concrete and abstract words from the preprocessed fMRI data. Specifically, we estimated four sets of amplitude-modulated impulse response functions (IRF) for (1) abstract words; (2) concrete words; (3) remaining words; and (4) other time points. Both concrete and abstract words included word onset and five modulators, two of interest, and five nuisance modulators. These were the independent ratings of concreteness and abstractness and luminance, loudness, duration, word frequency, and speaking rate (calculated as the number of phonemes divided by duration) for each word. We also estimated the IRFs in the same manner and with the same amplitude modulators for all the remaining words in the movie that were not of interest to our hypothesis. Finally, we generated IRFs (without amplitude modulators) for all time points which did not include any speech. The deconvolution model also included general linear tests for (1) abstract words under the curve; (2) concrete words under the curve; (3) contrasts between concrete and abstract words at each timepoint (for a comparison of significant timing differences see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>In contrast to a standard convolution-based regression analysis, deconvolution does not assume a canonical hemodynamic response function. Instead, an IRF is estimated over a 20 s time-window from stimulus onset at 1 s steps using multiple basis functions. This produces a better understanding of shape differences between individual hemodynamic response functions and achieves higher statistical power at both individual and group-levels (<xref ref-type="bibr" rid="bib21">Chen et al., 2015</xref>). Furthermore, there might be differences in timing for the processing of concrete and abstract words (<xref ref-type="bibr" rid="bib74">Kroll and Merves, 1986</xref>). In particular, the ‘concreteness effect’ indicates that concrete words are processed faster and more accurately than abstract words (<xref ref-type="bibr" rid="bib89">Paivio et al., 1994</xref>; <xref ref-type="bibr" rid="bib62">Jessen et al., 2000</xref>; <xref ref-type="bibr" rid="bib39">Fliessbach et al., 2006</xref>). These timing differences can be captured by our approach. We chose a 20 s time window because this should be sufficient to capture the hemodynamic response function for each word. We selected ‘Csplin’ over the Tent function to deconvolve the BOLD signal because this function offers more interpolation between time points, which might result in a more precise estimate of the individual response function (but is computationally more costly).</p><p>Furthermore, traditional ‘main effect’ analysis confounds various non-specific processes, such as acoustic processing, which co-vary with each presented word (especially during dynamic, naturalistic stimuli). In contrast, amplitude modulation allows us to isolate regions that exhibit activational fluctuations specifically in relation to the concreteness modulator beyond the ‘main effect’ and fluctuations in the amplitude response caused by other modulators included in our model. Including nuisance modulators can help serve as controls, mitigating potentially confounding effects - in our case the significant differences between luminance and loudness. By adjusting for these sensory attributes, we ensure that the final betas from this analysis represent the estimated BOLD response specifically associated with concreteness.</p></sec><sec id="s4-4-2"><title>Group-level analysis</title><p>We then used the 20 amplitude-modulated beta-coefficients from the concrete-abstract contrasts in a linear mixed effects model for group-level analysis using ‘<italic>3dLME’</italic> (<xref ref-type="bibr" rid="bib20">Chen et al., 2013</xref>). The model included the factors ‘contrast’ with levels ‘abstract’ and ‘concrete’ and ‘time’ with 20 levels. The model also included a centered covariate for age of participant, and covariates for gender (two levels) and movie ID (10 levels). Besides a random intercept for participant we included a control implemented in '<italic>3dLME'</italic> for the potentially auto-correlative structure of residuals to make sure that we model the true effect estimates of the multiple basis function (<xref ref-type="bibr" rid="bib54">Hefley et al., 2017</xref>). The final model formula was: <italic>contrast + age + gender + movie</italic>. We included 20 general linear tests, one for each contrast between concrete and abstract activation at each of the 20 timepoints, because we wanted to see how the amplitude of the activation associated with concreteness and abstractness changes over time. We thought that the timing and/or amplitude of the response for concrete and abstract words might vary and that this might be particularly true of the subsequent context analysis. We provide information on timing differences in the supplementary material (<xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7</xref>).</p></sec><sec id="s4-4-3"><title>Correction for multiple comparisons</title><p>To correct for multiple comparisons in the LME, we used a multi-threshold approach rather than choosing an arbitrary p value at the individual voxel level threshold. In particular, we used a cluster simulation method to estimate the probability of noise-only clusters using the spatial autocorrelation function from the LME residuals (‘<italic>3dFWHMx’</italic> and ‘<italic>3dClustSim’</italic>). This resulted in the cluster sizes to achieve a corrected alpha value of 0.01 at 9 different p values (i.e. 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, and 0.0001). We thresholded each map at the corresponding z-value for each of these nine p-values and associated cluster sizes. We then combined the resulting maps, leaving each voxel with its original z-value. For additional protection and presentation purposes, we use a minimum cluster size 20 voxels for all results using ‘3dMerge’. For tables, we determined the center of mass for each of these clusters using ‘<italic>3dCM.’</italic> See <xref ref-type="bibr" rid="bib28">Cox et al., 2017</xref> for a validation of a related method and <xref ref-type="bibr" rid="bib113">Skipper et al., 2022</xref> for an earlier application.</p></sec><sec id="s4-4-4"><title>Analyses of experiential features</title><p>In order to more closely characterize the functional specificity of the spatial activation maps from the preceding LME analysis, we carried out the following two additional analyses. In both, the goal is to determine whether brain activity associated with concrete and abstract word modulation relates to separable experiential domains that roughly map onto the aforementioned sensory-motor vs internal/interoceptive/affective/linguistic distinction, respectively.</p><sec id="s4-4-4-1"><title>Meta-analytic descriptions</title><p>The resulting coordinates of the center of mass of each cluster associated with modulation of concreteness and abstractness were inputed into Neurosynth (<ext-link ext-link-type="uri" xlink:href="https://neurosynth.org/">https://neurosynth.org/</ext-link>, <xref ref-type="bibr" rid="bib140">Yarkoni et al., 2011</xref>), an online tool that includes activation maps of 14,371 neuroscientific studies (accessed April, 2023). Neurosynth automatically mines all words in titles and abstracts of these articles and performs a two-way ANOVA, testing for the presence of a non-zero association between terms reporting activation that overlaps with the input location. We scraped all terms with z scores above 3.12 (p&lt;0.001) (excluding those related to specific brain regions and nondescript terms related to methods, tasks, or results) and repeated this procedure for each concrete and abstract cluster to determine functionally associated terms. We then tested whether any of these terms were more important for concrete or abstract words across clusters using a Kruskal-Wallis test. We did not correct for multiple comparisons, as this analysis was exploratory in nature and we did not have a prediction about how many terms we would end up with.</p></sec><sec id="s4-4-4-2"><title>Peak and valley analysis</title><p>The meta-analytic approach can only provide relatively general functional descriptions of concrete and abstract words as it is based only on high frequency terms in published titles and abstracts. To provide more precise functional specificity, we used a variant of the ‘reverse correlation’ method (<xref ref-type="bibr" rid="bib50">Hasson et al., 2004</xref>), called the ‘Peaks and Valleys Analysis’ (<xref ref-type="bibr" rid="bib51">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="bib111">Skipper et al., 2009</xref>). For each participant, this analysis averaged the time series of voxels within clusters of modulated activity associated with concreteness and abstractness and relates this directly to features of the perceived stimulus. The approach assumes that, if a brain region encodes certain features, e.g. sensorimotor features, valence, or arousal, then activity will rise (creating peaks) in that region when the feature is present in the stimulus and fall (resulting in valleys) when it is absent.</p><p>We first extracted the averaged time series for each activation cluster for the concrete and abstract modulations across voxels using <italic>3dMerge</italic>. Next, we determined peaks and valleys by calculating the discrete difference ‘Δ’ along the time series ‘x’ for each value ‘i’ using the ‘<italic>Numpy’</italic> Python package (<xref ref-type="bibr" rid="bib49">Harris et al., 2020</xref>; <xref ref-type="fig" rid="fig7">Figure 7</xref>, (1)), where Δx[i]=x[i+1] - x[i]. Given that the canonical model of the hemodynamic response function is said to peak at around 6 s after stimulus onset for stimuli of our length, we extracted the words that were mentioned at each peak and valley in a given cluster’s time series with a 5- and 6 s lag (<xref ref-type="fig" rid="fig7">Figure 7</xref>, (2)). We then used the Lancaster sensorimotor norms (<xref ref-type="bibr" rid="bib79">Lynott et al., 2020</xref>) and norms for valence and arousal (<xref ref-type="bibr" rid="bib136">Warriner et al., 2013</xref>) to determine a 13-dimensional experience-based representation for each word (<xref ref-type="fig" rid="fig7">Figure 7</xref>, (3)), which included the dimensions: ‘Auditory,’ ‘Gustatory,’ ‘Haptic,’ ‘Interoception,’ ‘Visual,’ ‘Hand_Arm,’ ‘Foot_Leg,’ ‘Torso,’ ‘Mouth,’ ‘Head,’ ‘Olfactory,’ ‘Valence,’ and ‘Arousal.’</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Overview of the peak and valley analysis method.</title><p>First, we average the fMRI time series for each participant, for each abstract, concrete, and overlap cluster of activity from <xref ref-type="fig" rid="fig1">Figure 1</xref>. Then we label peaks and valleys in these (1) and map them onto word on- and off-set times (2). Finally, we estimate sensorimotor as well as valence and arousal representations for each abstract (blue frame) and concrete word (red frame) (3) and determine which dimensions are associated with significantly more peaks than valleys across participants in each cluster using a Kruskal-Wallis test (4).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig7-v1.tif"/></fig><p>Specifically, for concrete clusters, we expected significantly more sensory-motor features (i.e. ‘Foot_Leg,’ ‘Hand_Arm,’ ‘Haptic,’ ‘Visual,’ and ‘Torso’ in the corpora) to be associated with peaks rather than valleys in the time series compared to abstract clusters. Conversely, we expected significantly more experiential features related to internal/interoceptive/affective processing (i.e. ‘Interoception,’ ‘Valence,’ and ‘Arousal’ in the corpora) to be associated with peaks compared to valleys for abstract relative to concrete clusters. It was not clear to us whether the dimensions (‘Auditory,’ ‘Head,’ and ‘Gustatory’) were more related to internal/interoceptive/affective or sensory-motor processing. Therefore, we made no predictions for those.</p><p>For each of these dimensions, we created two categorical arrays, one for peaks and one for valleys, noting down 0 if the word mentioned at a peak or valley was not highly rated on the dimension and 1 if it was rated highly. This was defined as a deviation of at least one standard deviation from the mean. Given the distributional nature of this data, we then conducted a Kruskal-Wallis test between these arrays to determine whether a given experiential dimension occurred significantly more with peaks than valleys in the averaged time series of a cluster (<xref ref-type="fig" rid="fig7">Figure 7</xref>, (4)). We repeated this procedure for a 4 s, 5 s, and a 6 s time series lag and conducted a cosine-similarity test between each result using the ‘<italic>Sklearn’</italic> package in Python <xref ref-type="bibr" rid="bib91">Pedregosa et al., 2011</xref> in order to determine if they were significantly different. The results for the 5 s and 6 s lag converge but not for 4 s. This was expected, because the delay of the HRF is somewhere between 5 and 6 s. In the main results, we randomly decided between presenting the 5 s and 6 s lag. The 5 s lag is now displayed in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, the 4 s lag in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>.</p></sec></sec></sec><sec id="s4-5"><title>Conceptual processing in context</title><p>The previous analyses tested whether, when activation is considered across contexts, concrete and abstract words are processed in separable brain regions encoding different types of experiential information. Here, we test if these response profiles dynamically change depending on the objects present in the viewing context. We predict that when abstract concepts are situated in highly related contexts (e.g. the word ‘love’ is processed while watching two people kissing), they engage neurobiological regions that are usually involved in processing concrete concepts and are related to processing of external visual information. Conversely, when concrete concepts are displaced from the visual context (e.g. processing the word ‘apple’ while watching the interior of a house), we predict them to engage more abstract-like regions in the brain that are related to processing of internal/interoceptive/affective information. Note that we chose to do the analysis in two stages (first across context, then within context) because only a subset of the 440 words used in our analysis across context for concrete and abstract words are related to the objects present in the scene in a way that situates them in visual context (see below). We wanted to have as much power as possible for the first deconvolution/LME to look beyond the effects of context.</p><sec id="s4-5-1"><title>Estimating contextual situatedness</title><p>To test our predictions, we estimated a measure of contextual situatedness for each concrete and abstract word included in the first analysis. To that end, we utilized two pre-trained visual recognition models, Faster R-CNN (<xref ref-type="bibr" rid="bib53">He et al., 2015</xref>) and OmniSource (<xref ref-type="bibr" rid="bib33">Duan et al., 2020</xref>), to extract object features using computer vision toolboxes (<xref ref-type="bibr" rid="bib84">Mkrtychian et al., 2019</xref>), respectively. For each prediction frame (about every four frames, i.e. 4*0.04=0.16 s), the object recognition model generated a list of detected object labels and kept those that had a prediction confidence of 90% or greater (<xref ref-type="fig" rid="fig8">Figure 8</xref>, 1). Then, we excluded all objects that were recognized at least three standard deviations more often by the model compared to the mean recognition rate of objects (which was 682 appearances), because they would bias our measure of situatedness. These labels were ‘person’ (17,856 appearances per movie on average), ‘chair’ (9718 appearances per movie on average), and ‘tie’ (8123 appearances per movie on average). After exclusion of these labels, the final object features were represented as the average of the vectorized object labels using GloVe (<xref ref-type="bibr" rid="bib92">Pennington et al., 2014</xref>; <xref ref-type="fig" rid="fig8">Figure 8</xref>, 2), which represents the meaning of each label via global co-occurrence statistics.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Method for estimating contextual situatedness for each concrete and abstract word to model context-dependent modulation of conceptual encoding.</title><p>We use visual recognition models for automatically extracting labels that were visually present in the scene (60 frames, ~2 s) before a given word was mentioned in the movie (1). We then correlate an average GloVe Vector embedding of all these labels with a GloVe Vector embedding of that word to estimate how closely related the labels of objects in the scene are to the word (2). Displayed are four randomly extracted measures of situated abstract (blue frame) and concrete (red frame) words (3) together with the objects that were visually present in the scene.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91522-fig8-v1.tif"/></fig><p>We then estimated a representation of a 2 s (or 60 frames) context window, which should capture the immediate visual context leading up to each concrete and abstract word. We extracted all the labels of objects visually present in each frame within that window. Finally, we calculated the cosine similarity between the vector representation of each word and its context average, using the ‘<italic>Sklearn’</italic> package in Python (<xref ref-type="bibr" rid="bib91">Pedregosa et al., 2011</xref>), to estimate contextual situatedness for each concrete and abstract word (a value <italic>c</italic> between 0 and 1), <xref ref-type="fig" rid="fig8">Figure 8</xref>, (3). After separating into situated (<italic>c</italic>&gt;0.6) and displaced words (<italic>c</italic>&lt;0.4), we were left with (on average for each movie) 164 abstract situated words, 201 abstract displaced words, 215 concrete situated words, and 172 concrete displaced words. Given that, as concerns observations, the abstract situated condition was the limiting condition, we randomly selected words from the abstract displaced, concrete situated, and concrete displaced conditions to have an equal number of 164 words in each condition (on average per movie).</p><p>Though we use visual nuisance regressors, we note that there may be additional confounding visual information when estimating contextual situatedness: high situatedness may correlate positively with the number of objects present and, therefore, ‘naturally’ engage visual processing more for abstract situated concepts. To alleviate this concern, we counted the number of objects in the abstract situated (8315 objects across movies) and abstract displaced (7443 across movies) conditions. The difference between the two (872) is not statistically significant (H(2)=4.1, p&lt;0.09).</p></sec></sec><sec id="s4-6"><title>Deconvolution analysis</title><p>The deconvolution analysis was as described previously except that the sets of concrete and abstract words were broken into four equal subsets of regressors, i.e., situated concrete, displaced concrete, situated abstract, and displaced abstract words and modulators with 164 words per each set. The four contrasts included were: (1) abstract situated vs abstract displaced, (2) concrete situated vs concrete displaced, (3) abstract situated vs concrete situated, and (4) abstract displaced vs concrete displaced. Mean concreteness rating for the resulting sets of concrete words were 3.39 for displaced words and 3.35 for situated words, compared to 1.84 for abstract situated words and 1.71 for abstract displaced words. The mean log frequency and mean length for concrete words was 4.91 and 4.88 for situated words and 5.33 and 4.91 for displaced words, compared to 5.11 and 5.08 for abstract situated words and 5.20 and 5.27 for abstract displaced words. Mean surprisal ratings for concrete situated words were 21.98 bits, 22.02 bits for the displaced concrete words, 22.10 for the situated abstract words and 22.25 for the abstract displaced words. Mean semantic diversity ratings were 1.88 for the concrete situated words, 2.19 for the concrete displaced words, 2.03 for the abstract situated words, and 1.95 for the abstract displaced words. As concerns visual variables, mean optical flow was 0.85, mean color saturation was 0.33, and mean spatial frequency was 0.02 for the 2 s context windows before abstract situated words. Mean optical flow was 0.81, mean color saturation was 0.35 and mean spatial frequency was 0.04 for the 2 s context windows before abstract displaced words. Mean optical flow was 0.76, mean color saturation was 0.41 and mean spatial frequency was 0.10 for the 2 s context windows before concrete situated words. Mean optical flow was 0.86, mean color saturation was 0.33, and mean spatial frequency was 0.05 for the 2 s context windows before concrete displaced words. Pairwise T-tests between all of the mentioned measures for all groups revealed no significant differences.</p></sec><sec id="s4-7"><title>Group-level analysis</title><p>A linear mixed effects model for group-level analysis was conducted on the 20 amplitude-modulated betas from each condition (concrete situated, abstract situated, concrete displaced, situated displaced) using ‘<italic>3dLME’</italic> (<xref ref-type="bibr" rid="bib20">Chen et al., 2013</xref>). The model included factors ‘word_type’ (concrete and abstract), ‘context’ (displaced and situated), and ‘time’ (20 levels) and all possible interactions between these factors. We again included covariates for age, gender, and movie, a random intercept for participant, and a control of the auto-correlative structure of residuals. The final model formula was: <italic>word_type * context * time + age + gender + movie</italic>. We had no prediction on whether the timing and/or amplitude of the response for concrete and abstract words would vary in the present analysis. Therefore, we included four general linear tests, one for each contrast across time: one for the abstract situated vs. abstract displaced contrast, one for the concrete situated vs. concrete displaced contrast, one for the abstract situated vs abstract displaced contrast, and one for the concrete situated vs concrete displaced contrast.</p></sec><sec id="s4-8"><title>Code</title><p>We implemented our data analysis in Bash, Python, and R. Our code will be provided as online supplemental material upon publication and hosted openly on a dedicated Github repository under: (<ext-link ext-link-type="uri" xlink:href="https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts">https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts</ext-link>, copy archived at <xref ref-type="bibr" rid="bib130">ViktorKewenig, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Funding acquisition, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study was approved by the ethics committee of University College London and participants provided written informed consent to take part in the study and share their anonymised data. Ethics Committee (Reference Number 143/003).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91522-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code and data are publicly available.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Aliko</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Meliss</surname><given-names>S</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>A naturalistic neuroimaging database</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds002837.v2.0.0</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>VK would like to thank Bangjie Wang for his help in using image recognition software, and Dr. Sarah Aliko for help with neuroimaging analysis. This work was supported in part by the European Research Council Advanced Grant (ECOLANG, 743035); Royal Society Wolfson Research Merit Award (WRM\R3\170016) to GV; and Leverhulme award DS-2017–026 to VK and GV.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aliko</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Gheorghiu</surname><given-names>F</given-names></name><name><surname>Meliss</surname><given-names>S</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A naturalistic neuroimaging database for understanding the brain using ecological stimuli</article-title><source>Scientific Data</source><volume>7</volume><elocation-id>347</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-020-00680-2</pub-id><pub-id pub-id-type="pmid">33051448</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amodio</surname><given-names>DM</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Meeting of minds: the medial frontal cortex and social cognition</article-title><source>Nature Reviews. Neuroscience</source><volume>7</volume><fpage>268</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1038/nrn1884</pub-id><pub-id pub-id-type="pmid">16552413</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Fernandino</surname><given-names>L</given-names></name><name><surname>Humphries</surname><given-names>CJ</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Raizada</surname><given-names>RDS</given-names></name><name><surname>Lin</surname><given-names>F</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An integrated neural decoder of linguistic and experiential meaning</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>8969</fpage><lpage>8987</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2575-18.2019</pub-id><pub-id pub-id-type="pmid">31570538</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews-Hanna</surname><given-names>JR</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Spreng</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The default network and self-generated thought: component processes, dynamic control, and clinical relevance</article-title><source>Annals of the New York Academy of Sciences</source><volume>1316</volume><fpage>29</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1111/nyas.12360</pub-id><pub-id pub-id-type="pmid">24502540</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>LW</given-names></name><name><surname>Kyle Simmons</surname><given-names>W</given-names></name><name><surname>Barbey</surname><given-names>AK</given-names></name><name><surname>Wilson</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Grounding conceptual knowledge in modality-specific systems</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>84</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(02)00029-3</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>LW</given-names></name><name><surname>Wiemer-Hastings</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Situating abstract concepts</chapter-title><person-group person-group-type="editor"><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><source>Grounding Cognition</source><publisher-name>NIH</publisher-name><fpage>129</fpage><lpage>163</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>LW</given-names></name><name><surname>Dutriaux</surname><given-names>L</given-names></name><name><surname>Scheepers</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Moving beyond the distinction between concrete and abstract concepts</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>373</volume><elocation-id>20170144</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2017.0144</pub-id><pub-id pub-id-type="pmid">29915012</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname><given-names>M</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neuroanatomically separable effects of imageability and grammatical class during single-word comprehension</article-title><source>Brain and Language</source><volume>98</volume><fpage>127</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2006.04.008</pub-id><pub-id pub-id-type="pmid">16716387</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>J</given-names></name><name><surname>Frost</surname><given-names>JA</given-names></name><name><surname>Hammeke</surname><given-names>TA</given-names></name><name><surname>Bellgowan</surname><given-names>P</given-names></name><name><surname>Rao</surname><given-names>SM</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Conceptual processing during the conscious resting state. A functional MRI study</article-title><source>J Cogn Neurosci</source><volume>PMID</volume><fpage>80</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1162/089892999563265</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Westbury</surname><given-names>CF</given-names></name><name><surname>McKiernan</surname><given-names>KA</given-names></name><name><surname>Possing</surname><given-names>ET</given-names></name><name><surname>Medler</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Distinct brain systems for processing concrete and abstract concepts</article-title><source>Journal of Cognitive Neuroscience</source><volume>17</volume><fpage>905</fpage><lpage>917</lpage><pub-id pub-id-type="doi">10.1162/0898929054021102</pub-id><pub-id pub-id-type="pmid">16021798</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name><name><surname>Graves</surname><given-names>WW</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Where is the semantic system? A critical review and meta-analysis of 120 functional neuroimaging studies</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2767</fpage><lpage>2796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp055</pub-id><pub-id pub-id-type="pmid">19329570</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neurobiology of semantic memory</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.10.001</pub-id><pub-id pub-id-type="pmid">22001867</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bloom</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>How children learn the meanings of words</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bookheimer</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Functional MRI of language: new approaches to understanding the cortical organization of semantic processing</article-title><source>Annual Review of Neuroscience</source><volume>25</volume><fpage>151</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.25.112701.142946</pub-id><pub-id pub-id-type="pmid">12052907</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghi</surname><given-names>AM</given-names></name><name><surname>Zarcone</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Grounding abstractness: abstract concepts and the activation of the mouth</article-title><source>Frontiers in Psychology</source><volume>7</volume><elocation-id>1498</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.01498</pub-id><pub-id pub-id-type="pmid">27777563</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>New</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Moving beyond Kučera and Francis: A critical evaluation of current word frequency norms and the introduction of A new and improved word frequency measure for American English</article-title><source>Behavior Research Methods</source><volume>41</volume><fpage>977</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.3758/BRM.41.4.977</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>Z</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Word processing language and thought</source><publisher-name>Willey</publisher-name><pub-id pub-id-type="doi">10.1002/9781119170174</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanna</surname><given-names>AE</given-names></name><name><surname>Trimble</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The precuneus: A review of its functional anatomy and behavioural correlates</article-title><source>Brain</source><volume>129</volume><fpage>564</fpage><lpage>583</lpage><pub-id pub-id-type="doi">10.1093/brain/awl004</pub-id><pub-id pub-id-type="pmid">16399806</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>CG</given-names></name><name><surname>Tanenhaus</surname><given-names>MK</given-names></name><name><surname>Magnuson</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Actions and affordances in syntactic ambiguity resolution</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>30</volume><fpage>687</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.30.3.687</pub-id><pub-id pub-id-type="pmid">15099136</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Saad</surname><given-names>ZS</given-names></name><name><surname>Britton</surname><given-names>JC</given-names></name><name><surname>Pine</surname><given-names>DS</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Linear mixed-effects modeling approach to FMRI group analysis</article-title><source>NeuroImage</source><volume>73</volume><fpage>176</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.047</pub-id><pub-id pub-id-type="pmid">23376789</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Saad</surname><given-names>ZS</given-names></name><name><surname>Adleman</surname><given-names>NE</given-names></name><name><surname>Leibenluft</surname><given-names>E</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Detecting the subtle shape differences in hemodynamic responses at the group level</article-title><source>Frontiers in Neuroscience</source><volume>9</volume><elocation-id>375</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2015.00375</pub-id><pub-id pub-id-type="pmid">26578853</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conca</surname><given-names>F</given-names></name><name><surname>Borsa</surname><given-names>VM</given-names></name><name><surname>Cappa</surname><given-names>SF</given-names></name><name><surname>Catricalà</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>The multidimensionality of abstract concepts: a systematic review</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>127</volume><fpage>474</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2021.05.004</pub-id><pub-id pub-id-type="pmid">33979574</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conca</surname><given-names>F</given-names></name><name><surname>Catricalà</surname><given-names>E</given-names></name><name><surname>Canini</surname><given-names>M</given-names></name><name><surname>Petrini</surname><given-names>A</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name><name><surname>Cappa</surname><given-names>SF</given-names></name><name><surname>Della Rosa</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>In search of different categories of abstract concepts: a fMRI adaptation study</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>22587</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-02013-8</pub-id><pub-id pub-id-type="pmid">34799624</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>The control of eye fixation by the meaning of spoken language</article-title><source>Cognitive Psychology</source><volume>6</volume><fpage>84</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(74)90005-X</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Couper-Kuhlen</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><chapter-title>Prosody and sequence organization in english conversation: the case of new beginnings</chapter-title><person-group person-group-type="editor"><name><surname>Couper-Kuhlen</surname><given-names>E</given-names></name><name><surname>Ford</surname><given-names>CE</given-names></name></person-group><source>Sound Patterns in Interaction: Cross-Linguistic Studies from Conversation</source><publisher-name>John Benjamins Publishing</publisher-name><fpage>335</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1075/tsl.62.17cou</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">7656549</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR in Biomedicine</source><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(sici)1099-1492(199706/08)10:4/5&lt;171::aid-nbm453&gt;3.0.co;2-l</pub-id><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Glen</surname><given-names>DR</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Taylor</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>FMRI clustering in AFNI: false-positive rates redux</article-title><source>Brain Connect</source><volume>PMCID</volume><fpage>152</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1089/brain.2016.0475</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crittenden</surname><given-names>BM</given-names></name><name><surname>Mitchell</surname><given-names>DJ</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Recruitment of the default mode network during a demanding act of executive control</article-title><source>eLife</source><volume>4</volume><elocation-id>e06481</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06481</pub-id><pub-id pub-id-type="pmid">25866927</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Maschio</surname><given-names>N</given-names></name><name><surname>Fedeli</surname><given-names>D</given-names></name><name><surname>Garofalo</surname><given-names>G</given-names></name><name><surname>Buccino</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Evidence for the concreteness of abstract language: a meta-analysis of neuroimaging studies</article-title><source>Brain Sciences</source><volume>12</volume><elocation-id>32</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci12010032</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deniz</surname><given-names>F</given-names></name><name><surname>Tseng</surname><given-names>C</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name><name><surname>Dupré la Tour</surname><given-names>T</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Semantic representations during language comprehension are affected by context</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>3144</fpage><lpage>3158</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2459-21.2023</pub-id><pub-id pub-id-type="pmid">36973013</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Driess</surname><given-names>D</given-names></name><name><surname>Xia</surname><given-names>F</given-names></name><name><surname>Sajjadi</surname><given-names>MS</given-names></name><name><surname>Lynch</surname><given-names>C</given-names></name><name><surname>Chowdhery</surname><given-names>A</given-names></name><name><surname>Ichter</surname><given-names>B</given-names></name><name><surname>Wahid</surname><given-names>A</given-names></name><name><surname>Tompson</surname><given-names>J</given-names></name><name><surname>Vuong</surname><given-names>QH</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><name><surname>Chebotar</surname><given-names>Y</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Duckworth</surname><given-names>D</given-names></name><name><surname>Levine</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Hausman</surname><given-names>K</given-names></name><name><surname>Toussaint</surname><given-names>M</given-names></name><name><surname>Greff</surname><given-names>K</given-names></name><name><surname>Zeng</surname><given-names>A</given-names></name><name><surname>Mordatch</surname><given-names>I</given-names></name><name><surname>Florence</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>PaLM-E: An Embodied Multimodal Language Model</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2303.03378">https://arxiv.org/abs/2303.03378</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>H</given-names></name><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Xiong</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Omni-source webly-supervised learning for video recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.13042">https://arxiv.org/abs/2003.13042</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Language as a dynamical system</chapter-title><person-group person-group-type="editor"><name><surname>Port</surname><given-names>RF</given-names></name></person-group><source>Mind as Motion: Explorations in the Dynamics of Cognition</source><publisher-name>The MIT Press</publisher-name><fpage>195</fpage><lpage>225</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fauconnier</surname><given-names>G</given-names></name><name><surname>Turner</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>The way we think: conceptual blending and the mind’s hidden complexities</source><publisher-name>Basic Books</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandino</surname><given-names>L</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name><name><surname>Pendl</surname><given-names>SL</given-names></name><name><surname>Humphries</surname><given-names>CJ</given-names></name><name><surname>Gross</surname><given-names>WL</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Seidenberg</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Concept representation reflects multimodal abstraction: a framework for embodied semantics</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2018</fpage><lpage>2034</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv020</pub-id><pub-id pub-id-type="pmid">25750259</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandino</surname><given-names>L</given-names></name><name><surname>Tong</surname><given-names>J-Q</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Humphries</surname><given-names>CJ</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Decoding the information structure underlying the neural representation of concepts</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2108091119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2108091119</pub-id><pub-id pub-id-type="pmid">35115397</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferstl</surname><given-names>EC</given-names></name><name><surname>Neumann</surname><given-names>J</given-names></name><name><surname>Bogler</surname><given-names>C</given-names></name><name><surname>von Cramon</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The extended language network: A meta-analysis of neuroimaging studies on text comprehension</article-title><source>Human Brain Mapping</source><volume>29</volume><fpage>581</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1002/hbm.20422</pub-id><pub-id pub-id-type="pmid">17557297</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fliessbach</surname><given-names>K</given-names></name><name><surname>Weis</surname><given-names>S</given-names></name><name><surname>Klaver</surname><given-names>P</given-names></name><name><surname>Elger</surname><given-names>CE</given-names></name><name><surname>Weber</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The effect of word concreteness on recognition memory</article-title><source>Neuroimage</source><volume>PMID</volume><fpage>1413</fpage><lpage>1421</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.06.007</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Development and neurophysiology of mentalizing</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>358</volume><fpage>459</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1218</pub-id><pub-id pub-id-type="pmid">12689373</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>C</given-names></name><name><surname>Baucom</surname><given-names>LB</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wedell</surname><given-names>DH</given-names></name><name><surname>Shinkareva</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distinguishing abstract from concrete concepts in supramodal brain regions</article-title><source>Neuropsychologia</source><volume>102–110</volume><fpage>102</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.05.032</pub-id><pub-id pub-id-type="pmid">31175884</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberg</surname><given-names>RF</given-names></name><name><surname>Perfetti</surname><given-names>CA</given-names></name><name><surname>Schneider</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perceptual knowledge retrieval activates sensory brain regions</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>4917</fpage><lpage>4921</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5389-05.2006</pub-id><pub-id pub-id-type="pmid">16672666</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haber</surname><given-names>SN</given-names></name><name><surname>Knutson</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The reward circuit: linking primate anatomy and human imaging</article-title><source>Neuropsychopharmacology</source><volume>35</volume><fpage>4</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1038/npp.2009.129</pub-id><pub-id pub-id-type="pmid">19812543</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>On Broca, brain, and binding: a new framework</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>416</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.07.004</pub-id><pub-id pub-id-type="pmid">16054419</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahamy</surname><given-names>A</given-names></name><name><surname>Dubossarsky</surname><given-names>H</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The human brain reactivates context-specific past information at event boundaries of naturalistic experiences</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>1080</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01331-6</pub-id><pub-id pub-id-type="pmid">37248340</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harpaintner</surname><given-names>M</given-names></name><name><surname>Trumpp</surname><given-names>NM</given-names></name><name><surname>Kiefer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The semantic content of abstract concepts: a property listing study of 296 abstract words</article-title><source>Frontiers in Psychology</source><volume>9</volume><elocation-id>1748</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01748</pub-id><pub-id pub-id-type="pmid">30283389</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harpaintner</surname><given-names>M</given-names></name><name><surname>Sim</surname><given-names>EJ</given-names></name><name><surname>Trumpp</surname><given-names>NM</given-names></name><name><surname>Ulrich</surname><given-names>M</given-names></name><name><surname>Kiefer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The grounding of abstract concepts in the motor and visual system: An fMRI study</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>124</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2019.10.014</pub-id><pub-id pub-id-type="pmid">31821905</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harpaintner</surname><given-names>M</given-names></name><name><surname>Trumpp</surname><given-names>NM</given-names></name><name><surname>Kiefer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Time course of brain activity during the processing of motor- and vision-related abstract concepts: flexibility and task dependency</article-title><source>Psychological Research</source><volume>86</volume><fpage>2560</fpage><lpage>2582</lpage><pub-id pub-id-type="doi">10.1007/s00426-020-01374-5</pub-id><pub-id pub-id-type="pmid">32661582</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Fuhrmann</surname><given-names>G</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Intersubject synchronization of cortical activity during natural vision</article-title><source>Science</source><volume>303</volume><fpage>1634</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1126/science.1089506</pub-id><pub-id pub-id-type="pmid">15016991</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Wilde</surname><given-names>MJ</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Improving the analysis, storage and sharing of neuroimaging data using relational databases and distributed computing</article-title><source>NeuroImage</source><volume>39</volume><fpage>693</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.021</pub-id><pub-id pub-id-type="pmid">17964812</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Johnsrude</surname><given-names>I</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Somatotopic representation of action words in human motor and premotor cortex</article-title><source>Neuron</source><volume>41</volume><fpage>301</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00838-9</pub-id><pub-id pub-id-type="pmid">14741110</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hefley</surname><given-names>TJ</given-names></name><name><surname>Broms</surname><given-names>KM</given-names></name><name><surname>Brost</surname><given-names>BM</given-names></name><name><surname>Buderman</surname><given-names>FE</given-names></name><name><surname>Kay</surname><given-names>SL</given-names></name><name><surname>Scharf</surname><given-names>HR</given-names></name><name><surname>Tipton</surname><given-names>JR</given-names></name><name><surname>Williams</surname><given-names>PJ</given-names></name><name><surname>Hooten</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The basis function approach for modeling autocorrelation in ecological data</article-title><source>Ecology</source><volume>98</volume><fpage>632</fpage><lpage>646</lpage><pub-id pub-id-type="doi">10.1002/ecy.1674</pub-id><pub-id pub-id-type="pmid">27935640</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoenig</surname><given-names>K</given-names></name><name><surname>Sim</surname><given-names>EJ</given-names></name><name><surname>Bochev</surname><given-names>V</given-names></name><name><surname>Herrnberger</surname><given-names>B</given-names></name><name><surname>Kiefer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Conceptual flexibility in the human brain: dynamic recruitment of semantic maps from visual, motor, and motion-related areas</article-title><source>Journal of Cognitive Neuroscience</source><volume>20</volume><fpage>1799</fpage><lpage>1814</lpage><pub-id pub-id-type="doi">10.1162/jocn.2008.20123</pub-id><pub-id pub-id-type="pmid">18370598</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>P</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Semantic diversity: a measure of semantic ambiguity based on variability in the contextual usage of words</article-title><source>Behavior Research Methods</source><volume>45</volume><fpage>718</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.3758/s13428-012-0278-x</pub-id><pub-id pub-id-type="pmid">23239067</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>P</given-names></name><name><surname>Binney</surname><given-names>RJ</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Differing contributions of inferior prefrontal and anterior temporal cortex to concrete and abstract conceptual knowledge</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>63</volume><fpage>250</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2014.09.001</pub-id><pub-id pub-id-type="pmid">25303272</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>P</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Concepts, control, and context: a connectionist account of normal and disordered semantic cognition</article-title><source>Psychological Review</source><volume>125</volume><fpage>293</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1037/rev0000094</pub-id><pub-id pub-id-type="pmid">29733663</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>NS</given-names></name><name><surname>Kraemer</surname><given-names>DJM</given-names></name><name><surname>Oliver</surname><given-names>RT</given-names></name><name><surname>Schlichting</surname><given-names>ML</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Color, context, and cognitive style: variations in color knowledge retrieval as a function of task and subject variables</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>2544</fpage><lpage>2557</lpage><pub-id pub-id-type="doi">10.1162/jocn.2011.21619</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jackendoff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Foundations of language: brain, meaning, grammar, evolution</source><publisher-name>Oxford Academic</publisher-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jessen</surname><given-names>F</given-names></name><name><surname>Heun</surname><given-names>R</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Granath</surname><given-names>DO</given-names></name><name><surname>Klose</surname><given-names>U</given-names></name><name><surname>Papassotiropoulos</surname><given-names>A</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The concreteness effect: evidence for dual coding and context availability</article-title><source>Brain Lang</source><volume>PMID</volume><fpage>103</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1006/brln.2000.2340</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>CA</given-names></name><name><surname>Casson</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Effects of luminance, contrast, and blur on visual acuity</article-title><source>Optom Vis Sci</source><volume>PMID</volume><fpage>864</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1097/00006324-199512000-00004</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>MN</given-names></name><name><surname>Johns</surname><given-names>BT</given-names></name><name><surname>Recchia</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of semantic diversity in lexical organization</article-title><source>Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale</source><volume>66</volume><fpage>115</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1037/a0026727</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalénine</surname><given-names>S</given-names></name><name><surname>Shapiro</surname><given-names>AD</given-names></name><name><surname>Flumini</surname><given-names>A</given-names></name><name><surname>Borghi</surname><given-names>AM</given-names></name><name><surname>Buxbaum</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual context modulates potentiation of grasp types during semantic object categorization</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>645</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0536-7</pub-id><pub-id pub-id-type="pmid">24186270</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: A module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kewenig</surname><given-names>V</given-names></name><name><surname>Lampinen</surname><given-names>A</given-names></name><name><surname>Nastase</surname><given-names>S</given-names></name><name><surname>Edwards</surname><given-names>C</given-names></name><name><surname>D’Elascombe</surname><given-names>Q</given-names></name><name><surname>Rechardt</surname><given-names>A</given-names></name><name><surname>Skipper</surname><given-names>J</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Multimodality and Attention Increase Alignment in Natural Language Prediction between Humans and Computational Models</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2308.06035</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>M</given-names></name><name><surname>Sim</surname><given-names>EJ</given-names></name><name><surname>Herrnberger</surname><given-names>B</given-names></name><name><surname>Grothe</surname><given-names>J</given-names></name><name><surname>Hoenig</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The sound of concepts: four markers for a link between auditory and conceptual brain systems</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>12224</fpage><lpage>12230</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3579-08.2008</pub-id><pub-id pub-id-type="pmid">19020016</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>M</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Conceptual representations in mind and brain: theoretical developments, current evidence and future directions</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>48</volume><fpage>805</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2011.04.006</pub-id><pub-id pub-id-type="pmid">21621764</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>M</given-names></name><name><surname>Pielke</surname><given-names>L</given-names></name><name><surname>Trumpp</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Differential temporo-spatial pattern of electrical brain activity during the processing of abstract concepts related to mental states and verbal associations</article-title><source>NeuroImage</source><volume>252</volume><elocation-id>119036</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119036</pub-id><pub-id pub-id-type="pmid">35219860</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiehl</surname><given-names>KA</given-names></name><name><surname>Liddle</surname><given-names>PF</given-names></name><name><surname>Smith</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Neural pathways involved in the processing of concrete and abstract words</article-title><source>Human Brain Mapping</source><volume>7</volume><fpage>225</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)7:4&lt;225::AID-HBM1&gt;3.0.CO;2-P</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konishi</surname><given-names>M</given-names></name><name><surname>McLaren</surname><given-names>DG</given-names></name><name><surname>Engen</surname><given-names>H</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shaped by the past: the default mode network supports cognition that is independent of immediate perceptual input</article-title><source>PLOS ONE</source><volume>PMCID</volume><elocation-id>e0132209</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0132209</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Ganis</surname><given-names>G</given-names></name><name><surname>Thompson</surname><given-names>WL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural foundations of imagery</article-title><source>Nature Reviews. Neuroscience</source><volume>2</volume><fpage>635</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1038/35090055</pub-id><pub-id pub-id-type="pmid">11533731</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kroll</surname><given-names>JF</given-names></name><name><surname>Merves</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Lexical access for concrete and abstract words</article-title><source>Journal of Experimental Psychology</source><volume>12</volume><fpage>92</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1037//0278-7393.12.1.92</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kuhlmann</surname><given-names>M</given-names></name><name><surname>Seebauer</surname><given-names>F</given-names></name><name><surname>Ebbers</surname><given-names>J</given-names></name><name><surname>Wagner</surname><given-names>P</given-names></name><name><surname>Haeb-Umbach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Investigation into Target Speaking Rate Adaptation for Voice Conversion</article-title><conf-name>INTERSPEECH 2022</conf-name><pub-id pub-id-type="doi">10.21437/Interspeech.2022-10740</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id><pub-id pub-id-type="pmid">20809790</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanzoni</surname><given-names>L</given-names></name><name><surname>Ravasio</surname><given-names>D</given-names></name><name><surname>Thompson</surname><given-names>H</given-names></name><name><surname>Vatansever</surname><given-names>D</given-names></name><name><surname>Margulies</surname><given-names>D</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The role of default mode network in semantic cue integration</article-title><source>NeuroImage</source><volume>219</volume><elocation-id>117019</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117019</pub-id><pub-id pub-id-type="pmid">32522664</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebois</surname><given-names>LAM</given-names></name><name><surname>Wilson-Mendenhall</surname><given-names>CD</given-names></name><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Are automatic conceptual cores the gold standard of semantic processing? the context-dependence of spatial meaning in grounded congruency effects</article-title><source>Cognitive Science</source><volume>39</volume><fpage>1764</fpage><lpage>1801</lpage><pub-id pub-id-type="doi">10.1111/cogs.12174</pub-id><pub-id pub-id-type="pmid">25243925</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynott</surname><given-names>D</given-names></name><name><surname>Connell</surname><given-names>L</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>Brand</surname><given-names>J</given-names></name><name><surname>Carney</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Lancaster Sensorimotor Norms: multidimensional measures of perceptual and action strength for 40,000 English words</article-title><source>Behavior Research Methods</source><volume>52</volume><fpage>1271</fpage><lpage>1291</lpage><pub-id pub-id-type="doi">10.3758/s13428-019-01316-z</pub-id><pub-id pub-id-type="pmid">31832879</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McFee</surname><given-names>B</given-names></name><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Liang</surname><given-names>D</given-names></name><name><surname>Ellis</surname><given-names>DPW</given-names></name><name><surname>McVicar</surname><given-names>M</given-names></name><name><surname>Battenberg</surname><given-names>E</given-names></name><name><surname>Nieto</surname><given-names>O</given-names></name></person-group><article-title>librosa: Audio and music signal analysis in Python</article-title><conf-name>In Proceedings of the 14th Python in Science Conference (SciPy 2015)</conf-name><year iso-8601-date="2015">2015</year><fpage>18</fpage><lpage>25</lpage></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mellet</surname><given-names>E</given-names></name><name><surname>Tzourio</surname><given-names>N</given-names></name><name><surname>Denis</surname><given-names>M</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical anatomy of mental imagery of concrete nouns based on their dictionary definition</article-title><source>Neuroreport</source><volume>9</volume><fpage>803</fpage><lpage>808</lpage><pub-id pub-id-type="doi">10.1097/00001756-199803300-00007</pub-id><pub-id pub-id-type="pmid">9579669</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Merkx</surname><given-names>D</given-names></name><name><surname>Frank</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Human sentence processing: recurrence or attention?</article-title><conf-name>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</conf-name><fpage>12</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.18653/v1/2021.cmcl-1.2</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meteyard</surname><given-names>L</given-names></name><name><surname>Cuadrado</surname><given-names>SR</given-names></name><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Coming of age: A review of embodiment and the neuroscience of semantics</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>48</volume><fpage>788</fpage><lpage>804</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2010.11.002</pub-id><pub-id pub-id-type="pmid">21163473</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mkrtychian</surname><given-names>N</given-names></name><name><surname>Blagovechtchenski</surname><given-names>E</given-names></name><name><surname>Kurmakaeva</surname><given-names>D</given-names></name><name><surname>Gnedykh</surname><given-names>D</given-names></name><name><surname>Kostromina</surname><given-names>S</given-names></name><name><surname>Shtyrov</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Concrete vs. abstract semantics: from mental representations to functional brain mapping</article-title><source>Frontiers in Human Neuroscience</source><volume>13</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2019.00267</pub-id><pub-id pub-id-type="pmid">31427938</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muraki</surname><given-names>EJ</given-names></name><name><surname>Cortese</surname><given-names>F</given-names></name><name><surname>Protzner</surname><given-names>AB</given-names></name><name><surname>Pexman</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Heterogeneity in abstract verbs: An ERP study</article-title><source>Brain and Language</source><volume>211</volume><elocation-id>104863</elocation-id><pub-id pub-id-type="doi">10.1016/j.bandl.2020.104863</pub-id><pub-id pub-id-type="pmid">33039774</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>C</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Rueschemeyer</surname><given-names>SA</given-names></name><name><surname>Sormaz</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>HT</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distant from input: Evidence of regions within the default mode network supporting perceptually-decoupled and conceptually-guided cognition</article-title><source>Neuroimage</source><volume>PMCID</volume><fpage>393</fpage><lpage>401</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.017</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noppeney</surname><given-names>U</given-names></name><name><surname>Phillips</surname><given-names>J</given-names></name><name><surname>Price</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural areas that control the retrieval and selection of semantics</article-title><source>Neuropsychologia</source><volume>42</volume><fpage>1269</fpage><lpage>1280</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2003.12.014</pub-id><pub-id pub-id-type="pmid">15178178</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterwijk</surname><given-names>S</given-names></name><name><surname>Mackey</surname><given-names>S</given-names></name><name><surname>Wilson-Mendenhall</surname><given-names>C</given-names></name><name><surname>Winkielman</surname><given-names>P</given-names></name><name><surname>Paulus</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Concepts in context: Processing mental state concepts with internal or external focus involves different neural systems</article-title><source>Social Neuroscience</source><volume>10</volume><fpage>294</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1080/17470919.2014.998840</pub-id><pub-id pub-id-type="pmid">25748274</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paivio</surname><given-names>A</given-names></name><name><surname>Walsh</surname><given-names>M</given-names></name><name><surname>Bons</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Concreteness effects on memory: When and why?</article-title><source>Journal of Experimental Psychology</source><volume>20</volume><fpage>1196</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.20.5.1196</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Nestor</surname><given-names>PJ</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>976</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1038/nrn2277</pub-id><pub-id pub-id-type="pmid">18026167</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib92"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pennington</surname><given-names>J</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Manning</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Glove: global vectors for word representation</article-title><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name><conf-loc>Doha, Qatar</conf-loc><pub-id pub-id-type="doi">10.3115/v1/D14-1162</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>LeDoux</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Contributions of the amygdala to emotion processing: from animal models to human behavior</article-title><source>Neuron</source><volume>48</volume><fpage>175</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.09.025</pub-id><pub-id pub-id-type="pmid">16242399</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Piantadosi</surname><given-names>ST</given-names></name><name><surname>Hill</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Meaning Without Reference in Large Language Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2208.02957">https://arxiv.org/abs/2208.02957</ext-link></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponari</surname><given-names>M</given-names></name><name><surname>Norbury</surname><given-names>CF</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Acquisition of abstract concepts is influenced by emotional valence</article-title><source>Developmental Science</source><volume>21</volume><elocation-id>e12549</elocation-id><pub-id pub-id-type="doi">10.1111/desc.12549</pub-id><pub-id pub-id-type="pmid">28224689</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popp</surname><given-names>M</given-names></name><name><surname>Trumpp</surname><given-names>NM</given-names></name><name><surname>Kiefer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Processing of action and sound verbs in context: An FMRI Study</article-title><source>Translational Neuroscience</source><volume>10</volume><fpage>200</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1515/tnsci-2019-0035</pub-id><pub-id pub-id-type="pmid">31637047</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Brain mechanisms linking language and action</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>576</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1038/nrn1706</pub-id><pub-id pub-id-type="pmid">15959465</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Neurobiological mechanisms for semantic feature extraction and conceptual flexibility</article-title><source>Topics in Cognitive Science</source><volume>10</volume><fpage>590</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1111/tops.12367</pub-id><pub-id pub-id-type="pmid">30129710</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Neural reuse of action perception circuits for language, concepts and communication</article-title><source>Progress in Neurobiology</source><volume>160</volume><fpage>1</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2017.07.001</pub-id><pub-id pub-id-type="pmid">28734837</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Child</surname><given-names>R</given-names></name><name><surname>Luan</surname><given-names>D</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Language models are unsupervised multitask learners</source><publisher-name>Semantic Scholar</publisher-name></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname><given-names>MA</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural and computational bases of semantic cognition</article-title><source>Nat Rev Neurosci</source><volume>18</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.150</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Ritchey</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Two cortical systems for memory-guided behaviour</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="doi">10.1038/nrn3338</pub-id><pub-id pub-id-type="pmid">22992647</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raykov</surname><given-names>PP</given-names></name><name><surname>Keidel</surname><given-names>JL</given-names></name><name><surname>Oakhill</surname><given-names>J</given-names></name><name><surname>Bird</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The brain regions supporting schema-related processing of people’s identities</article-title><source>Cognitive Neuropsychology</source><volume>37</volume><fpage>8</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1080/02643294.2019.1685958</pub-id><pub-id pub-id-type="pmid">31710265</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinboth</surname><given-names>T</given-names></name><name><surname>Farkaš</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Ultimate grounding of abstract concepts: a graded account</article-title><source>Journal of Cognition</source><volume>5</volume><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.5334/joc.214</pub-id><pub-id pub-id-type="pmid">36072124</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roversi</surname><given-names>C</given-names></name><name><surname>Borghi</surname><given-names>AM</given-names></name><name><surname>Tummolini</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A Marriage is an artefact and not a walk that we take together: an experimental study on the categorization of artefacts</article-title><source>Review of Philosophy and Psychology</source><volume>4</volume><fpage>527</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1007/s13164-013-0150-7</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabsevitz</surname><given-names>DS</given-names></name><name><surname>Medler</surname><given-names>DA</given-names></name><name><surname>Seidenberg</surname><given-names>M</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Modulation of the semantic system by word imageability</article-title><source>NeuroImage</source><volume>27</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.04.012</pub-id><pub-id pub-id-type="pmid">15893940</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Blank</surname><given-names>IA</given-names></name><name><surname>Tuckute</surname><given-names>G</given-names></name><name><surname>Kauf</surname><given-names>C</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2105646118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id><pub-id pub-id-type="pmid">34737231</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seghier</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The angular gyrus: multiple functions and multiple subdivisions</article-title><source>The Neuroscientist</source><volume>19</volume><fpage>43</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1177/1073858412440596</pub-id><pub-id pub-id-type="pmid">22547530</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The expected value of control: an integrative theory of anterior cingulate cortex function</article-title><source>Neuron</source><volume>79</volume><fpage>217</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.007</pub-id><pub-id pub-id-type="pmid">23889930</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>WK</given-names></name><name><surname>Ramjee</surname><given-names>V</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>McRae</surname><given-names>K</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A common neural substrate for perceiving and knowing about color</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>2802</fpage><lpage>2810</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.05.002</pub-id><pub-id pub-id-type="pmid">17575989</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Goldin-Meadow</surname><given-names>S</given-names></name><name><surname>Nusbaum</surname><given-names>HC</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gestures orchestrate brain networks for language understanding</article-title><source>Current Biology</source><volume>19</volume><fpage>661</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.02.051</pub-id><pub-id pub-id-type="pmid">19327997</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Willems</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>The NOLB model: a model of the natural organization of language and the brain</chapter-title><person-group person-group-type="editor"><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><source>Cognitive Neuroscience of Natural Language Use</source><publisher-name>Cambridge University</publisher-name><fpage>101</fpage><lpage>134</lpage></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skipper</surname><given-names>JI</given-names></name><name><surname>Aliko</surname><given-names>S</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Jo</surname><given-names>YJ</given-names></name><name><surname>Lo</surname><given-names>S</given-names></name><name><surname>Molimpakis</surname><given-names>E</given-names></name><name><surname>Lametti</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reorganization of the neurobiology of language after sentence overlearning</article-title><source>Cerebral Cortex</source><volume>32</volume><fpage>2447</fpage><lpage>2468</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhab354</pub-id><pub-id pub-id-type="pmid">34585723</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Cognitive Psychology: Mind and Brain</source><publisher-loc>Upper Saddle River, NJ</publisher-loc><publisher-name>Prentice Hall</publisher-name></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SD</given-names></name><name><surname>Fredborg</surname><given-names>K</given-names></name><name><surname>Kornelsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An examination of the default mode network in individuals with autonomous sensory meridian response (ASMR)</article-title><source>Soc Neurosci</source><volume>PMID</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1080/17470919.2016.1188851</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>V</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Mitchell</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Roles of the default mode and multiple-demand networks in naturalistic versus symbolic decisions</article-title><source>J Neurosci</source><volume>01</volume><fpage>2214</fpage><lpage>2228</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1888-20.2020</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sormaz</surname><given-names>M</given-names></name><name><surname>Murphy</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Hymers</surname><given-names>M</given-names></name><name><surname>Karapanagiotidis</surname><given-names>T</given-names></name><name><surname>Poerio</surname><given-names>G</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Default mode network can support the level of detail in experience during active task states</article-title><source>PNAS</source><volume>115</volume><fpage>9318</fpage><lpage>9323</lpage><pub-id pub-id-type="doi">10.1073/pnas.1721259115</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spivey</surname><given-names>MJ</given-names></name><name><surname>Dale</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Continuous dynamics in real-time cognition</article-title><source>Current Directions in Psychological Science</source><volume>15</volume><fpage>207</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8721.2006.00437.x</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoodley</surname><given-names>CJ</given-names></name><name><surname>Schmahmann</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Functional topography in the human cerebellum: A meta-analysis of neuroimaging studies</article-title><source>NeuroImage</source><volume>44</volume><fpage>489</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.08.039</pub-id><pub-id pub-id-type="pmid">18835452</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanenhaus</surname><given-names>MK</given-names></name><name><surname>Spivey-Knowlton</surname><given-names>MJ</given-names></name><name><surname>Eberhard</surname><given-names>KM</given-names></name><name><surname>Sedivy</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Integration of visual and linguistic information in spoken language comprehension</article-title><source>Science</source><volume>268</volume><fpage>1632</fpage><lpage>1634</lpage><pub-id pub-id-type="doi">10.1126/science.7777863</pub-id><pub-id pub-id-type="pmid">7777863</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>J</given-names></name><name><surname>LeBel</surname><given-names>A</given-names></name><name><surname>Jain</surname><given-names>S</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Semantic reconstruction of continuous language from non-invasive brain recordings</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.09.29.509744</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name><name><surname>Kan</surname><given-names>IP</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of repetition and competition on activity in left prefrontal cortex during word generation</article-title><source>Neuron</source><volume>23</volume><fpage>513</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80804-1</pub-id><pub-id pub-id-type="pmid">10433263</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>J</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Humphries</surname><given-names>C</given-names></name><name><surname>Mazurchuk</surname><given-names>S</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Fernandino</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A distributed network for multimodal experiential representation of concepts</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>7121</fpage><lpage>7130</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1243-21.2022</pub-id><pub-id pub-id-type="pmid">35940877</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Dam</surname><given-names>WO</given-names></name><name><surname>van Dijk</surname><given-names>M</given-names></name><name><surname>Bekkering</surname><given-names>H</given-names></name><name><surname>Rueschemeyer</surname><given-names>S-A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Flexibility in embodied lexical-semantic representations</article-title><source>Human Brain Mapping</source><volume>33</volume><fpage>2322</fpage><lpage>2333</lpage><pub-id pub-id-type="doi">10.1002/hbm.21365</pub-id><pub-id pub-id-type="pmid">21976384</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandenberghe</surname><given-names>R</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The response of left temporal cortex to sentences</article-title><source>Journal of Cognitive Neuroscience</source><volume>14</volume><fpage>550</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1162/08989290260045800</pub-id><pub-id pub-id-type="pmid">12126497</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatansever</surname><given-names>D</given-names></name><name><surname>Menon</surname><given-names>DK</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Default mode contributions to automated information processing</article-title><source>PNAS</source><volume>114</volume><fpage>12821</fpage><lpage>12826</lpage><pub-id pub-id-type="doi">10.1073/pnas.1710521114</pub-id><pub-id pub-id-type="pmid">29078345</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vigliocco</surname><given-names>G</given-names></name><name><surname>Warren</surname><given-names>J</given-names></name><name><surname>Siri</surname><given-names>S</given-names></name><name><surname>Arciuli</surname><given-names>J</given-names></name><name><surname>Scott</surname><given-names>S</given-names></name><name><surname>Wise</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The role of semantics and grammatical class in the neural representation of words</article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1790</fpage><lpage>1796</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj115</pub-id><pub-id pub-id-type="pmid">16421329</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vigliocco</surname><given-names>G</given-names></name><name><surname>Meteyard</surname><given-names>L</given-names></name><name><surname>Andrews</surname><given-names>M</given-names></name><name><surname>Kousta</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Toward a theory of semantic representation</article-title><source>Language and Cognition</source><volume>1</volume><fpage>219</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1515/LANGCOG.2009.011</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vigliocco</surname><given-names>G</given-names></name><name><surname>Kousta</surname><given-names>ST</given-names></name><name><surname>Della Rosa</surname><given-names>PA</given-names></name><name><surname>Vinson</surname><given-names>DP</given-names></name><name><surname>Tettamanti</surname><given-names>M</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Cappa</surname><given-names>SF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neural representation of abstract words: the role of emotion</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1767</fpage><lpage>1777</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht025</pub-id><pub-id pub-id-type="pmid">23408565</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="software"><person-group person-group-type="author"><collab>ViktorKewenig</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Naturalistic_Encoding_Concepts</data-title><version designator="swh:1:rev:acbe3ba607dd92515f5dbf8921386f44d068d47d">swh:1:rev:acbe3ba607dd92515f5dbf8921386f44d068d47d</version><publisher-name>Software Heritage</publisher-name><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:20c8f318a32e5907b3b25f9c0aedbaa1128dff45;origin=https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts;visit=swh:1:snp:a49d7c532481d5cbcc83e045a582ecc704489bd7;anchor=swh:1:rev:acbe3ba607dd92515f5dbf8921386f44d068d47d">https://archive.softwareheritage.org/swh:1:dir:20c8f318a32e5907b3b25f9c0aedbaa1128dff45;origin=https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts;visit=swh:1:snp:a49d7c532481d5cbcc83e045a582ecc704489bd7;anchor=swh:1:rev:acbe3ba607dd92515f5dbf8921386f44d068d47d</ext-link></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villani</surname><given-names>C</given-names></name><name><surname>Lugli</surname><given-names>L</given-names></name><name><surname>Liuzza</surname><given-names>MT</given-names></name><name><surname>Borghi</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Varieties of abstract concepts and their multiple dimensions</article-title><source>Language and Cognition</source><volume>11</volume><fpage>403</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1017/langcog.2019.23</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villani</surname><given-names>C</given-names></name><name><surname>Lugli</surname><given-names>L</given-names></name><name><surname>Liuzza</surname><given-names>MT</given-names></name><name><surname>Nicoletti</surname><given-names>R</given-names></name><name><surname>Borghi</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sensorimotor and interoceptive dimensions in concrete and abstract concepts</article-title><source>Journal of Memory and Language</source><volume>116</volume><elocation-id>104173</elocation-id><pub-id pub-id-type="doi">10.1016/j.jml.2020.104173</pub-id><pub-id pub-id-type="pmid">32952286</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>AD</given-names></name><name><surname>Paré-Blagoev</surname><given-names>EJ</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Recovering meaning: left prefrontal cortex guides controlled semantic retrieval</article-title><source>Neuron</source><volume>31</volume><fpage>329</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00359-2</pub-id><pub-id pub-id-type="pmid">11502262</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walt</surname><given-names>S</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>the scikit-image contributors</article-title><source>Scikit-Image: Image Processing in Python. PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Conder</surname><given-names>JA</given-names></name><name><surname>Blitzer</surname><given-names>DN</given-names></name><name><surname>Shinkareva</surname><given-names>SV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural representation of abstract and concrete concepts: a meta-analysis of neuroimaging studies</article-title><source>Human Brain Mapping</source><volume>31</volume><fpage>1459</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1002/hbm.20950</pub-id><pub-id pub-id-type="pmid">20108224</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warriner</surname><given-names>AB</given-names></name><name><surname>Kuperman</surname><given-names>V</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Norms of valence, arousal, and dominance for 13,915 English lemmas</article-title><source>Behavior Research Methods</source><volume>45</volume><fpage>1191</fpage><lpage>1207</lpage><pub-id pub-id-type="doi">10.3758/s13428-012-0314-x</pub-id><pub-id pub-id-type="pmid">23404613</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wengenroth</surname><given-names>M</given-names></name><name><surname>Blatow</surname><given-names>M</given-names></name><name><surname>Heinecke</surname><given-names>A</given-names></name><name><surname>Reinhardt</surname><given-names>J</given-names></name><name><surname>Stippich</surname><given-names>C</given-names></name><name><surname>Hofmann</surname><given-names>E</given-names></name><name><surname>Schneider</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Increased volume and function of right auditory cortex as a marker for absolute pitch</article-title><source>Cereb Cortex</source><volume>24</volume><fpage>1127</fpage><lpage>1137</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs391</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wilcox</surname><given-names>E</given-names></name><name><surname>Vani</surname><given-names>P</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A targeted assessment of incremental processing in neural language models and humans</article-title><conf-name>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th international joint conference on natural language processing</conf-name><pub-id pub-id-type="doi">10.18653/v1/2021.acl-long.76</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willems</surname><given-names>RM</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Casasanto</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Body-specific representations of action verbs: neural evidence from right- and left-handers</article-title><source>Psychological Science</source><volume>21</volume><fpage>67</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1177/0956797609354072</pub-id><pub-id pub-id-type="pmid">20424025</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title><source>Nature Methods</source><volume>8</volume><fpage>665</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id><pub-id pub-id-type="pmid">21706013</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yee</surname><given-names>E</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Putting concepts into context</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>1015</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0948-7</pub-id><pub-id pub-id-type="pmid">27282993</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname><given-names>Y</given-names></name><name><surname>Nguyen</surname><given-names>M</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The default mode network: where the idiosyncratic self meets the shared social world</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-00420-w</pub-id><pub-id pub-id-type="pmid">33483717</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zdrazilova</surname><given-names>L</given-names></name><name><surname>Sidhu</surname><given-names>DM</given-names></name><name><surname>Pexman</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Communicating abstract meaning: concepts revealed in words and gestures</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>373</volume><elocation-id>20170138</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2017.0138</pub-id><pub-id pub-id-type="pmid">29915006</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91522.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Max Planck Institute for Psycholinguistics</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>Kewenig et al. present a timely and <bold>valuable</bold> study that extends prior research investigating the neural basis of abstract and concrete concepts by examining how these concepts are processed in a naturalistic stimulus: during movie watching. The authors provide <bold>convincing</bold> evidence that the varying strength of the relationship between a word and a particular visual scene is associated with a change in the similarity between the brain regions active for concrete and abstract words. This work makes a contribution that will be of general interest within any field that faces the inherent challenge of quantifying context in a multimodal stimulus.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91522.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this study, the authors investigate a very interesting but often overlooked aspect of abstract vs. concrete processing in language. Specifically, they study if the differences in processing of abstract vs. concrete concepts in the brain is static or dependent on the (visual) context in which the words occur. This study takes a two-step approach to investigate how context might affect the perception of concepts. First, the authors analyze if concrete concepts, expectedly, activate more sensory systems while abstract concepts activate higher-order processing regions. Second, they measure the contextual situatedness vs. displacement of each word with respect to the visual scenes in which it is spoken and then evaluate if this contextual measure correlates with more activation in the sensory vs. higher-order regions respectively.</p><p>Strengths:</p><p>This study raises a pertinent and understudied question in language neuroscience. It also combines both computational and meta-analytic approaches.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91522.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study tests a plausible and intriguing hypothesis that one cause of the differences in the neural underpinnings of concrete and abstract words is differences in their grounding in the current sensory context. The authors reasoned that, in this case, an abstract word presented with a relevant visual scene would be processed in a more similar way to a concrete word. Typically, abstract and concrete words are tested in isolation. In contrast, this study takes advantage of naturalistic movie stimuli to assess the neural effects of concreteness in both abstract and concrete words (the speech within the film), when the visual context is more or less tied to the word meaning (measured as the similarity between the word co-occurrence-based vector for the spoken word and the average of this vector across all present objects). This novel approach allows a test of the dynamic nature of abstract and concrete word processing, and as such provides a useful perspective accounting for differences in processing these word types.</p><p>The measure of contextual situatedness (how related a spoken word is to the average of the visually presented objects in a scene) is an interesting approach allowing parametric variation within naturalistic stimuli, which is a potential strength of the study. Additionally, the authors use an interesting peak and valley method and provide a rationale for this approach. This provided additional temporal information on the processing of spoken concrete and abstract words.</p><p>The authors predicted that sensory areas would be more active for concrete words, affective areas for abstract and language areas would be involved in both. The use of reverse inference to interpret areas such as the inferior frontal gyrus post hoc, as sensory, affective or language related deserves some caution. It is also important to remember that the different areas identified for each comparison do not necessarily have the same roles. As the number of clusters may therefore be a misleading way to assess the relationship of these areas with the sensory terms, the relationship between each area and the different sensory terms is provided in the supplemental to allow more nuanced interpretation. The study could benefit from being better situated in the prior literature on context and concrete vs abstract regional differences. Overall, the authors successfully demonstrate the context-dependent nature of abstract and concrete word processing.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91522.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The primary aim of this manuscript was to investigate how context, defined from visual object information in multimodal movies, impacts the neural representation of concrete and abstract conceptual knowledge. The authors first conduct a series of analyses to identify context independent regional response to concrete and abstract concepts in order to compare these results with the networks observed in prior research using non-naturalistic paradigms. The authors then conduct analyses to investigate whether regional response to abstract and concrete concepts changes when the concepts are either contextually situated or displaced. A concept is considered displaced if the visual information immediately preceding the word is weakly associated with the word whereas a concept is situated if the association is high. The results suggest that, when ignoring context, abstract and concrete concepts engage different brain regions with overlap in core language areas. When context is accounted for, however, similar brain regions are activated for processing concrete and situated abstract concepts and for processing abstract and displaced concrete concepts. The authors suggest that contextual information dynamically changes the brain regions that support the representation of abstract and concrete conceptual knowledge.</p><p>Strengths:</p><p>There is significant interest in understanding both the acquisition and neural representation of abstract and concrete concepts, and most of the work in this area has used highly constrained, decontextualized experimental stimuli and paradigms to do so. This manuscript addresses this limitation by using multimodal narratives which allows for an investigation of how context-sensitive the regional response to abstract and concrete concepts is. The authors characterize the regional response in a comprehensive way.</p><p>Weaknesses:</p><p>The edits made to the manuscript in response to the reviewer comments have clarified and strengthened the methodological concerns flagged by all reviewers, giving me greater confidence that the authors are capturing what they aimed to and are making appropriate inferences given the results.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91522.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kewenig</surname><given-names>Viktor Nikolaus</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Vigliocco</surname><given-names>Gabriella</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Skipper</surname><given-names>Jeremy I</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>We thank the reviewers and the editorial team for a thoughtful and constructive assessment. We appreciate all comments, and we try our best to respond appropriately to every reviewer’s queries below. It appears to us that one main worry was regarding appropriate modelling of the complex and rich structure of confounding variables in our movie task.</p><p>One recent approach fits large feature vectors that include confounding variables along the variable(s) of interest to the activity of each voxel in the brain to disentangle the contributions of each variable to the total recorded brain response. While these encoding models have yielded some interesting results, they have two major drawbacks which makes using them unfeasible for our purposes (as we explain in more detail below): first, by fitting large vectors to individual voxels, they tend to over-estimate effect size; second, they are very ineffective at unveiling group-level effects due to high variability between subjects. Another approach able to deal with at least the second of these worries is “inter-subject-correlation”. In this technique brain responses are recorded from multiple subjects while they are presented with natural stimuli. For each brain area, response time courses from different subjects are correlated to determine whether the responses are similar across subjects. Our “peak and valley” analysis is a special case of this analysis technique, as we explain in the manuscript and below.</p><p>For estimating individual-level brain-activation, we opted for an approach that adapts a classical method of analysing brain data – convolution - to naturalistic settings. Amplitude modulated deconvolution extends classical brain analysis tools in several ways to handle naturalistic data:</p><p>(1) The method does not assume a fixed hemodynamic response function (HRF). Instead, it estimates the HRF over a specified time window from the data, allowing it to vary in amplitude based on the stimulus. This flexibility is crucial for naturalistic stimuli, where the timing and nature of brain responses can vary widely.</p><p>(2) The method only models the modulation of the amplitude of the HRF above its average with respect to the intensity or characteristics of the stimulus.</p><p>(3) By allowing variation in the response amplitude, non-linear relationships between the stimulus and brain-response can be captured.</p><p>It is true that amplitude modulated deconvolution does not come without its flaws – for example including more than a few nuisance regressors becomes computationally very costly. Getting to grips with naturalistic data (especially with fMRI recordings) continuous to be an active area of research and presents a new and exciting challenge. We hope that we can convince reviewers and editors with this response and the additional analyses and controls performed, that the evidence presented for the visual context dependent recruitment of brain areas for abstract and concrete conceptual processing is not incomplete.</p><p>Overview of Additional Analyses and Controls Performed by the Authors:</p><p>(1) Individual-Level Peaks and Valleys Analysis (Supplementary Material, Figures S3, S4, and S5)</p><p>(2) Test of non-linear correlations of BOLD responses related to features used in the Peak and Valley Analysis (Supplementary Material, Figures S6, S7)</p><p>(3) Comparison of Psycholinguistic Variables Surprisal and Semantic Diversity between groups of words analysed (no significant differences found)</p><p>(4) Comparison of Visual Variables Optical Flow, Colour Saturation, and Spatial Frequency for 2s Context Window between groups of words analysed (no significant differences found)</p><p>These controls are in addition to the five low-level nuisance regressors included in our model, which are luminance, loudness, duration, word frequency, and speaking rate (calculated as the number of phonemes divided by duration) associated with each analysed word.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Peaks and Valleys Analysis:</p><p>(1) Doesn't this method assume that the features used to describe each word, like valence or arousal, will be linearly different for the peaks and valleys? What about non-linear interactions between the features and how they might modulate the response?</p></disp-quote><p>Within-subject variability in BOLD response delays is typically about 1 second at most (Neumann et al., 2003). As individual words are presented briefly (a few hundred Ms at most) and the BOLD response to these stimuli falls within that window (1s/TR), any nonlinear interactions between word features and a participant’s BOLD response within that window are unlikely to significantly affect the detection of peaks and valleys.</p><p>To quantitatively address the concern that non-linear modulations could manifest outside of that window, we include a new analysis in Figure S6, which compares the average BOLD responses of each participant in each cluster and each combination of features, showing that only a very few of all possible comparisons differ significantly from each other (~ 5000 combinations of features were significantly different from each other given an overall number of ~130.000 comparisons between BOLD responses to features, which amounts to 3.85%), suggesting that there are no relevant non-linear interactions between features. For a full list of the most non-linearly interacting features see Figure S7.</p><disp-quote content-type="editor-comment"><p>(2) Doesn't it also assume that the response to a word is infinitesimal and not spread across time? How does the chosen time window of analysis interact with the HRF? From the main figures and Figures S2-S3 there seem to be differences based on the timelag.</p></disp-quote><p>The Peak and Valley (P&amp;V) method does not assume that the response to a word is infinitesimal or confined to an instantaneous moment. The units of analysis (words) fall within one TR, as they are at most hundreds of Ms long – for this reason, we are looking at one TR only. The response of each voxel at that TR will be influenced by the word of interest, as well as all other words that have been uttered within the 1s TR, and the multimodal features of the video stimulus that fall within that timeframe. So, in our P&amp;V, we are not looking for an instantaneous response but rather changes in the BOLD signal that correspond to the presence of linguistic features within the stimuli.</p><p>The chosen time window of analysis interacts with the human response function (HRF) in the following way: the HRF unfolds over several seconds, typically peaking around 5-6 seconds after stimulus onset and returning to baseline within 20-30 seconds (Handwerker et al., 2004).</p><p>Our P&amp;V is designed to match these dynamics of fMRI data with the timing of word stimuli. We apply different lags (4s, 5s, and 6s) to account for the delayed nature of the HRF, ensuring that we capture the brain's response to the stimuli as it unfolds over time, rather than assuming an immediate or infinitesimal effect. We find that the P&amp;V yields our expected results for a 5s and a 6s lag, but not a 4s lag. This is in line with literature suggesting that the HRF for a given stimulus peaks around 5-6s after stimulus onset (Handwerker et al., 2004). As we are looking at very short stimuli (a few hundred ms) it makes sense that the distribution of features would significantly change with different lags. The fact that we find converging results for both a 5s and 6s lag, suggests that the delay is somewhere between 5s and 6s. There is no way of testing this hypothesis with the resolution of our brain data, however (1 TR).</p><disp-quote content-type="editor-comment"><p>(3) Were the group-averaged responses used for this analysis?</p></disp-quote><p>Yes, the response for each cluster was averaged across participants. We now report a participant-level overview of the Peak and Valley analysis (lagged at 5s) with similar results as the main analysis in the supplementary material see Figures S3, S4, and S5.</p><disp-quote content-type="editor-comment"><p>(4) Why don't the other terms identified in Figure 5 show any correspondence to the expected categories? What does this mean? Can the authors also situate their results with respect to prior findings as well as visualize how stable these results are at the individual voxel or participant level? It would also be useful to visualize example time courses that demonstrate the peaks and valleys.</p></disp-quote><p>The terms identified in figure 5 are sensorimotor and affective features from the combined Lancaster and Brysbaert norms. As for the main P&amp;V analysis, we only recorded a cluster as processing a given feature (or term) when there were significantly more instances of words highly rated in that dimension occurring at peaks rather than valleys in the HRF. For some features/terms, there were never significantly more words highly rated on that dimension occurring at peaks compared to valleys, which is why some terms identified in figure 5 do not show any significant clusters. We have now also clarified this in the figure caption.</p><p>We situate the method in previous literature in lines 289 – 296. In essence, it is a variant of the well-known method called “reverse correlation” first detailed in Hasson et al., 2004 (reference from the manuscript) and later adapter to a peak and valley analysis in Skipper et al., 2009 (reference from the manuscript).</p><p>We now present a more fine-grained characterisation of each cluster on an individual participant level in the supplementary material. We doubt that it would be useful to present an actual example time-course as it would only represent a fraction of over one hundred thousand analysed time-series. We do already present an exemplary time-course to demonstrate the method in Figure 1.</p><disp-quote content-type="editor-comment"><p>Estimating contextual situatedness:</p><p>(1) Doesn't this limit the analyses to &quot;visual&quot; contexts only? And more so, frequently recognized visual objects?</p></disp-quote><p>Yes, it was the point of this analysis to focus on visual context only, and it may be true that conducting the analysis in this way results in limiting it to objects that are frequently recognized by visual convolutional neural networks. However, the state-of-the-art strength of visual CNNs in recognising many different types of objects has been attested in several ways (He et al., 2015). Therefore, it is unlikely that the use of CNNs would bias the analysis towards any specific “frequently recognised” objects.</p><disp-quote content-type="editor-comment"><p>(2) The measure of situatedness is the cosine similarity of GloVe vectors that depend on word co-occurrence while the vectors themselves represent objects isolated by the visual recognition models. Expectedly, &quot;science&quot; and the label &quot;book&quot; or &quot;animal&quot; and the label &quot;dog&quot; will be close. But can the authors provide examples of context displacement? I wonder if this just picks up on instances where the identified object in the scene is unrelated to the word. How do the authors ensure that it is a displacement of context as opposed to the two words just being unrelated? This also has a consequence on deciding the temporal cutoff for consideration (2 seconds).</p></disp-quote><p>The cosine similarity is between the GloVe vectors of the <italic>word</italic> (that is situated or displaced) and the <italic>words</italic> referring to the objects identified by the visual recognition model. Therefore, the correlation is between more than just two vectors and both correlated representations depend on co-occurrence. The cosine similarity value reported is not from a comparison between GloVe vectors and vectors that are (visual) representations of objects from the visual recognition model.</p><p>A <italic>word</italic> is displaced if <italic>all</italic> the identified <italic>object-words</italic> in the defined context window (2s before word-onset) are unrelated to the _word (_see lines 105-110 (pg. 5); lines 371-380 pg. 1516 and Figure 2 caption). Thus, a word is considered to be displaced if <italic>all</italic> identified objects (not just two as claimed by the reviewer) in the scene are unrelated to the word. Given a context of 60 frames and an average of 5 identified objects per frame (i.e. an average candidate set of 300 objects that could be related) per word, the bar for “displacement” is set high. We provide some further considerations justifying the context window below in our responses to reviewers 2 and 3.</p><disp-quote content-type="editor-comment"><p>(3) While the introduction motivated the problem of context situatedness purely linguistically, the actual methods look at the relationship between recognized objects in the visual scene and the words. Can word surprisal or another language-based metric be used in place of the visual labeling? Also, it is not clear how the process identified in (2) above would come up with a high situatedness score for abstract concepts like &quot;truth&quot;.</p></disp-quote><p>We disagree with the reviewer that the introduction motivated the problem of context situatedness purely linguistically, as we explicitly consider <italic>visual context</italic> in the abstract as well as the introduction. Examples in text include lines 71-74 and lines 105-115. This is also reflected in the cited studies that use visual context, including Kalenine et al., 2014; Hoffmann et al., 2013; Yee &amp; Thompson-Schill, 2016; Hsu et al., 2011. However, we appreciate the importance of being very clear about this point, so we added various mentions of this fact at the beginning of the introduction to avoid confusion.</p><p>We know that prior linguistic context (e.g. measured by surprisal) does affect processing. The point of the analysis was to use a non-language-based metric of visual context to understand how this affects conceptual representation in naturalist settings. Therefore, it is not clear to us why replacing this with a language-based metric such as surprisal would be an adequate substitution. However, the reviewer is correct that we did not control for the influence of prior context. We obtained surprisal values for each of our words but could not find any significant differences between conditions and therefore did not include this factor in the analyses conducted. For considerations of differences in surprisal between each of the analysed sets of words, see the supplementary material.</p><p>The method would yield a high score of contextual situatedness for abstract concepts if there were objects in the scene whose GloVe embeddings have a close cosine distance to the GloVe embedding of that abstract word (e.g., “truth” and “book”). We believe this comment from the reviewer is rooted in a misconception of our method. They seem to think we compared GloVe vectors for the spoken word with vectors from a visual recognition model directly (in which case it is true that there would be a concern about how an abstract concept like “truth” could have a high situatedness). Apart from the fact that there would be concerns about the comparability of vectors derived from GloVe and a visual recognition model more generally, this present concern is unwarranted in our case, as we are comparing GloVe embeddings.</p><disp-quote content-type="editor-comment"><p>(4) It is a bit hard to see the overlapping regions in Figures 6A-C. Would it be possible to show pairs instead of triples? Like &quot;abstract across context&quot; vs. &quot;abstract displaced&quot;? Without that, and given (2) above, the results are not yet clear. Moreover, what happens in the &quot;overlapping&quot; regions of Figure 3?</p></disp-quote><p>To make this clearer, we introduced the contrasts (abstract situated vs displaced and concrete situated vs displaced) that were previously in the supplementary materials in the main text (now Figure 6, this was also requested by reviewer 2). We now show the overlap between the abstract situated (from the contrast in Figure 6) with concrete across context and the overlap between concrete displaced (from the contrast in Figure 6) with abstract across context separately in Figure 7.</p><p>The overlapping regions of Figure 3 indicate that both concrete and abstract concepts are processed in these regions (though at different time-points). We explain why this is a result of our deconvolution analysis on page 23:</p><p>“Finally, there was overlap in activity between modulation of both concreteness and abstractness (Figure 3, yellow). The overlap activity is due to the fact that we performed general linear tests for the abstract/concrete contrast at each of the 20 timepoints in our group analysis. Consequently, overlap means that activation in these regions is modulated by both concrete and abstract word processing but at different time-scales. In particular, we find that activity modulation associated with abstractness is generally processed over a longer time-frame. In the frontal, parietal, and temporal lobes, this was primarily in the left IFG, AG, and STG, respectively. In the occipital lobe, processing overlapped bilaterally around the calcarine sulcus.”</p><disp-quote content-type="editor-comment"><p>Miscellaneous comments:</p><p>(1) In Figure 3, it is surprising that the &quot;concrete-only&quot; regions dominate the angular gyrus and we see an overrepresentation of this category over &quot;abstract-only&quot;. Can the authors place their findings in the context of other studies?</p></disp-quote><p>The Angular Gyrus (AG) is hypothesised to be a general semantic hub; therefore it is not surprising that it should be active for general conceptual processing (and there is some overlap activation in posterior regions). We now situate our results in a wider range of previous findings in the results section under “Conceptual Processing Across Context”.</p><p>“Consistent with previous studies, we predicted that across naturalistic contexts, concrete and abstract concepts are processed in a separable set of brain regions. To test this, we contrasted concrete and abstract modulators at each time point of the IRF (Figure 3). This showed that concrete produced more modulation than abstract processing in parts of the frontal lobes, including the right posterior inferior frontal gyrus (IFG) and the precentral sulcus (Figure 3, red). Known for its role in language processing and semantic retrieval, the IFG has been hypothesised to be involved in the processing of action-related words and sentences, supporting both semantic decision tasks and the retrieval of lexical semantic information (Bookheimer, 2002; Hagoort, 2005). The precentral sulcus is similarly linked to the processing of action verbs and motor-related words (Pulvermüller, 2005). In the temporal lobes, greater modulation occurred in the bilateral transverse temporal gyrus and sulcus, planum polare and temporale. These areas, including primary and secondary auditory cortices, are crucial for phonological and auditory processing, with implications for the processing of sound-related words and environmental sounds (Binder et al., 2000). The superior temporal gyrus (STG) and sulcus (STS) also showed greater modulation for concrete words and these are said to be central to auditory processing and the integration of phonological, syntactic, and semantic information, with a particular role in processing meaningful speech and narratives (Hickok &amp; Poeppel, 2007). In the parietal and occipital lobes, more concrete modulated activity was found bilaterally in the precuneus, which has been associated with visuospatial imagery, episodic memory retrieval, and self-processing operations and has been said to contribute to the visualisation aspects of concrete concepts (Cavanna &amp; Trimble, 2006). More activation was also found in large swaths of the occipital cortices (running into the inferior temporal lobe), and the ventral visual stream. These regions are integral to visual processing, with the ventral stream (including areas like the fusiform gyrus) particularly involved in object recognition and categorization, linking directly to the visual representation of concrete concepts (Martin, 2007). Finally, subcortically, the dorsal and posterior medial cerebellum were more active bilaterally for concrete modulation. Traditionally associated with motor function, some studies also implicate the cerebellum in cognitive and linguistic processing, including the modulation of language and semantic processing through its connections with cerebral cortical areas (Stoodley &amp; Schmahmann, 2009).</p><p>Conversely, activation for abstract was greater than concrete words in the following regions (Figure 3, blue): In the frontal lobes, this included right anterior cingulate gyrus, lateral and medial aspects of the superior frontal gyrus. Being involved in cognitive control, decision-making, and emotional processing, these areas may contribute to abstract conceptualization by integrating affective and cognitive components (Shenhav et al., 2013). More left frontal activity was found in both lateral and medial prefrontal cortices, and in the orbital gyrus, regions which are key to social cognition, valuation, and decision-making, all domains rich in abstract concepts (Amodio &amp; Frith, 2006). In the parietal lobes, bilateral activity was greater in the angular gyri (AG) and inferior parietal lobules, including the postcentral gyrus. Central to the default mode network, these regions are implicated in a wide range of complex cognitive functions, including semantic processing, abstract thinking, and integrating sensory information with autobiographical memory (Seghier, 2013). In the temporal lobes, activity was restricted to the STS bilaterally, which plays a critical role in the perception of intentionality and social interactions, essential for understanding abstract social concepts (Frith &amp; Frith, 2003). Subcortically, activity was greater, bilaterally, in the anterior thalamus, nucleus accumbens, and left amygdala for abstract modulation. These areas are involved in motivation, reward processing, and the integration of emotional information with memory, relevant for abstract concepts related to emotions and social relations (Haber &amp; Knutson, 2010, Phelps &amp; LeDoux, 2005).</p><p>Finally, there was overlap in activity between modulation of both concreteness and abstractness (Figure 3, yellow). The overlap activity is due to the fact that we performed general linear tests for the abstract/concrete contrast at each of the 20 timepoints in our group analysis. Consequently, overlap means that activation in these regions is modulated by both concrete and abstract word processing but at different time-scales. In particular, we find that activity modulation associated with abstractness is generally processed over a longer time-frame (for a comparison of significant timing differences see figure S9). In the frontal, parietal, and temporal lobes, this was primarily in the left IFG, AG, and STG, respectively. Left IFG is prominently involved in semantic processing, particularly in tasks requiring semantic selection and retrieval and has been shown to play a critical role in accessing semantic memory and resolving semantic ambiguities, processes that are inherently time-consuming and reflective of the extended processing time for abstract concepts (Thompson-Schill et al., 1997; Wagner et al., 2001; Hofman et al., 2015). The STG, particularly its posterior portion, is critical for the comprehension of complex linguistic structures, including narrative and discourse processing. The processing of abstract concepts often necessitates the integration of contextual cues and inferential processing, tasks that engage the STG and may extend the temporal dynamics of semantic processing (Ferstl et al., 2008; Vandenberghe et al., 2002). In the occipital lobe, processing overlapped bilaterally around the calcarine sulcus, which is associated with primary visual processing (Kanwisher et al., 1997; Kosslyn et al., 2001).”</p><p>The finding that concrete concepts activate more brain voxels compared to abstract concepts is generally aligned with existing research, which often reports more extensive brain activation for concrete versus abstract words. This is primarily due to the richer sensory and perceptual associations tied to concrete concepts - see for example Binder et al., 2005 (figure 2 in the paper). Similarly, a recent meta-analysis by Bucur &amp; Pagano (2021) consistently found wider activation networks for the “concrete &gt; abstract” contrast compared to the “abstract &gt; concrete contrast”.</p><disp-quote content-type="editor-comment"><p>(2) The following line (Pg 21) regarding the necessary differences in time for the two categories was not clear. How does this fall out from the analysis method?</p><p>- Both categories overlap **(though necessarily at different time points)** in regions typically associated with word processing -</p></disp-quote><p>This is answered in our response above to point (4) in the reviewer’s comments. We now also provide more information on the temporal differences in the supplementary material (Figure S9).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>The critical contrasts needed to test the key hypothesis are not presented or not presented in full within the core text. To test whether abstract processing changes when in a situated context, the situated abstract condition would first need to be compared with the displaced abstract condition as in Supplementary Figure 6. Then to test whether this change makes the result closer to the processing of concrete words, this result should be compared to the concrete result. The correlations shown in Figure 6 in the main text are not focused on the differences in activity between the situated and displaced words or comparing the correlation of these two conditions with the other (concrete/abstract) condition. As such they cannot provide conclusive evidence as to whether the context is changing the processing of concrete/abstract words to be closer to the other condition. Additionally, it should be considered whether any effects reflect the current visual processing only or more general sensory processing.</p></disp-quote><p>The reviewer identifies the critical contrast as follows:</p><p>“The situated abstract condition would first need to be contrasted with the displaced abstract condition. Then, these results should be compared to the concrete result.”</p><p>We can confirm that this is indeed what had been done and we believe the reviewer’s confusion stems from a lack of clarity on our behalf. We have now made various clarifications on this point in the manuscript, and we changed the figures to make clear that our results are indeed based on the contrasts identified by this reviewer as the essential ones.</p><p>Figure 6 in the main text now reflects the contrast between situated and displaced abstract and concrete conditions (as requested by the reviewer, this was previously Figure S7 from the supplementary material). To compare the results from this contrast to conceptual processing across context, we use cosine similarity, and we mention these results in the text. We furthermore show the overlap between the conditions of interest (abstract situated x concrete across context; concrete displaced x abstract across context) in a new figure (Figure 7) to bring out the spatial distribution of overlap more clearly.</p><p>We also discussed the extent to which these effects reflect current visual processing only or more general sensory processing in lines 863 – 875 (pg. 33 and 34).</p><p>“In considering the impact of visual context on the neural encoding of concepts generally, it is furthermore essential to recognize that the mechanisms observed may extend beyond visual processing to encompass more general sensory processing mechanisms. The human brain is adept at integrating information across sensory modalities to form coherent conceptual representations, a process that is critical for navigating the multimodal nature of real-world experiences (Barsalou, 2008; Smith &amp; Kosslyn, 2007). While our findings highlight the role of visual context in modulating the neural representation of abstract and concrete words, similar effects may be observed in contexts that engage other sensory modalities. For instance, auditory contexts that provide relevant sound cues for certain concepts could potentially influence their neural representation in a manner akin to the visual contexts examined in this study. Future research could explore how different sensory contexts, individually or in combination, contribute to the dynamic neural encoding of concepts, further elucidating the multimodal foundation of semantic processing.”</p><disp-quote content-type="editor-comment"><p>Overall, the study would benefit from being situated in the literature more, including (a) a more general understanding of the areas involved in semantic processing (including areas proposed to be involved across different sensory modalities and for verbal and nonverbal stimuli), and (b) other differences between abstract and concrete words and whether they can explain the current findings, including other psycholinguistic variables which could be included in the model and the concept of semantic diversity (Hoffman et al.,). It would also be useful to consider whether difficulty effects (or processing effort) could explain some of the regional differences between abstract and concrete words (e.g., the language areas may simply require more of the same processing not more linguistic processing due to their greater reliance on word co-occurrence). Similarly, the findings are not considered in relation to prior comparisons of abstract and concrete words at the level of specific brain regions.</p></disp-quote><p>We now present an overview of the areas involved in semantic processing (across different sensory modalities for verbal and nonverbal stimuli) when we first present our results (section: “Conceptual Processing Across Context”).</p><p>We looked at surprisal as a potential cofound and found no significant differences between any of the set of words analysed. Mean surprisal of concrete words is 22.19, mean surprisal of abstract words is 21.86. Mean surprisal ratings for concrete situated words are 21.98 bits, 22.02 bits for the displaced concrete words, 22.10 for the situated abstract words and 22.25 for the abstract displaced words. We also calculated the semantic diversity of all sets of words and found now significant differences between the sets. The mean values for each condition are: abstract_high (2.02); abstract_low (1.95); concrete_high (1.88); concrete_low (2.19); abstract_original (1.96); concrete_original (1.92). Hence processing effort related to different predictability (surprisal), or greater semantic diversity cannot explain our findings.</p><p>We submit that difficulty effects do not explain any aspects of the activation found for conceptual processing, because we included word frequency in our model as a nuisance regressor and found no significant differences associated with surprisal. Previous work shows that surprisal (Hale, 2001) and word frequency (Brysbaert &amp; New, 2009) are good controls for processing difficulty.</p><p>Finally, we added considerations of prior findings comparing abstract and concrete words at the level of specific brain regions to the discussion (section: Conceptual Processing Across Context).</p><disp-quote content-type="editor-comment"><p>The authors use multiple methods to provide a post hoc interpretation of the areas identified as more involved in concrete, abstract, or both (at different times) words. These are designed to reduce the interpretation bias and improve interpretation, yet they may not successfully do so. These methods do give some evidence that sensory areas are more involved in concrete word processing. However, they are still open to interpretation bias as it is not clear whether all the evidence is consistent with the hypotheses or if this is the best interpretation of individual regions' involvement. This is because the hypotheses are provided at the level of 'sensory' and 'language' areas without further clarification and areas and terms found are simply interpreted as fitting these definitions. For instance, the right IFG is interpreted as a motor area, and therefore sensory as predicted, and the term 'autobiographical memory' is argued to be interoceptive. Language is associated with the 'both' cluster, not the abstract cluster, when abstract &gt;concrete is expected to engage language more. The areas identified for both vs. abstract&gt;concrete are distinguished in the Discussion through the description as semantic vs. language areas, but it is not clear how these are different or defined. Auditory areas appear to be included in the sensory prediction at times and not at others. When they are excluded, the rationale for this is not given. Overall, it is not clear whether all these areas and terms are expected and support the hypotheses. It should be possible to specify specific sensory areas where concrete and abstract words are predicted to be different based on (a) prior comparisons and/or (b) the known locations of sensory areas. Similarly, language or semantic areas could be identified using masks from NeuroSynth or traditional metaanalyses. A language network is presented in Supplementary Figure 7 but not interpreted, and its source is not given.</p></disp-quote><p>“The language network” was extracted through neurosynth and projected onto the “overlap” activation map with AFNI. We now specify this in the figure caption.</p><disp-quote content-type="editor-comment"><p>Alternatively, there could be a greater interpretation of different possible explanations of the regions found with a more comprehensive assessment of the literature. The function of individual regions and the explanation of why many of these areas are interpreted as sensory or language areas are only considered in the Discussion when it could inform whether the hypotheses have been evidenced in the results section.</p></disp-quote><p>We added extended considerations of this to the results (as requested by the reviewer) in the section “Conceptual Processing Across Contexts”.</p><p>“Consistent with previous studies, we predicted that across naturalistic contexts, concrete and abstract concepts are processed in a separable set of brain regions. To test this, we contrasted concrete and abstract modulators at each time point of the IRF (Figure 3). This showed that concrete produced more modulation than abstract processing in parts of the frontal lobes, including the right posterior inferior frontal gyrus (IFG) and the precentral sulcus (Figure 3, red). Known for its role in language processing and semantic retrieval, the IFG has been hypothesised to be involved in the processing of action-related words and sentences, supporting both semantic decision tasks and the retrieval of lexical semantic information <bold>(</bold>Bookheimer, 2002; Hagoort, 2005). The precentral sulcus is similarly linked to the processing of action verbs and motor-related words (Pulvermüller, 2005). In the temporal lobes, greater modulation occurred in the bilateral transverse temporal gyrus and sulcus, planum polare and temporale. These areas, including primary and secondary auditory cortices, are crucial for phonological and auditory processing, with implications for the processing of sound-related words and environmental sounds (Binder et al., 2000). The superior temporal gyrus (STG) and sulcus (STS) also showed greater modulation for concrete words and these are said to be central to auditory processing and the integration of phonological, syntactic, and semantic information, with a particular role in processing meaningful speech and narratives (Hickok &amp; Poeppel, 2007). In the parietal and occipital lobes, more concrete modulated activity was found bilaterally in the precuneus, which has been associated with visuospatial imagery, episodic memory retrieval, and self-processing operations and has been said to contribute to the visualisation aspects of concrete concepts (Cavanna &amp; Trimble, 2006). More activation was also found in large swaths of the occipital cortices (running into the inferior temporal lobe), and the ventral visual stream. These regions are integral to visual processing, with the ventral stream (including areas like the fusiform gyrus) particularly involved in object recognition and categorization, linking directly to the visual representation of concrete concepts (Martin, 2007). Finally, subcortically, the dorsal and posterior medial cerebellum were more active bilaterally for concrete modulation. Traditionally associated with motor function, some studies also implicate the cerebellum in cognitive and linguistic processing, including the modulation of language and semantic processing through its connections with cerebral cortical areas (Stoodley &amp; Schmahmann, 2009).</p><p>Conversely, activation for abstract was greater than concrete words in the following regions (Figure 3, blue): In the frontal lobes, this included right anterior cingulate gyrus, lateral and medial aspects of the superior frontal gyrus. Being involved in cognitive control, decisionmaking, and emotional processing, these areas may contribute to abstract conceptualization by integrating affective and cognitive components (Shenhav et al., 2013). More left frontal activity was found in both lateral and medial prefrontal cortices, and in the orbital gyrus, regions which are key to social cognition, valuation, and decision-making, all domains rich in abstract concepts (Amodio &amp; Frith, 2006). In the parietal lobes, bilateral activity was greater in the angular gyri (AG) and inferior parietal lobules, including the postcentral gyrus. Central to the default mode network, these regions are implicated in a wide range of complex cognitive functions, including semantic processing, abstract thinking, and integrating sensory information with autobiographical memory (Seghier, 2013). In the temporal lobes, activity was restricted to the STS bilaterally, which plays a critical role in the perception of intentionality and social interactions, essential for understanding abstract social concepts (Frith &amp; Frith, 2003). Subcortically, activity was greater, bilaterally, in the anterior thalamus, nucleus accumbens, and left amygdala for abstract modulation. These areas are involved in motivation, reward processing, and the integration of emotional information with memory, relevant for abstract concepts related to emotions and social relations (Haber &amp; Knutson, 2010, Phelps &amp; LeDoux, 2005).</p><p>Finally, there was overlap in activity between modulation of both concreteness and abstractness (Figure 3, yellow). The overlap activity is due to the fact that we performed general linear tests for the abstract/concrete contrast at each of the 20 timepoints in our group analysis. Consequently, overlap means that activation in these regions is modulated by both concrete and abstract word processing but at different time-scales. In particular, we find that activity modulation associated with abstractness is generally processed over a longer timeframe (for a comparison of significant timing differences see figure S9). In the frontal, parietal, and temporal lobes, this was primarily in the left IFG, AG, and STG, respectively. Left IFG is prominently involved in semantic processing, particularly in tasks requiring semantic selection and retrieval and has been shown to play a critical role in accessing semantic memory and resolving semantic ambiguities, processes that are inherently timeconsuming and reflective of the extended processing time for abstract concepts (ThompsonSchill et al., 1997; Wagner et al., 2001; Hofman et al., 2015). The STG, particularly its posterior portion, is critical for the comprehension of complex linguistic structures, including narrative and discourse processing. The processing of abstract concepts often necessitates the integration of contextual cues and inferential processing, tasks that engage the STG and may extend the temporal dynamics of semantic processing (Ferstl et al., 2008; Vandenberghe et al., 2002). In the occipital lobe, processing overlapped bilaterally around the calcarine sulcus, which is associated with primary visual processing (Kanwisher et al., 1997; Kosslyn et al., 2001).”</p><disp-quote content-type="editor-comment"><p>Additionally, these methods attempt to interpret all the clusters found for each contrast in the same way when they may have different roles (e.g., relate to different senses). This is a particular issue for the peaks and valleys method which assesses whether a significantly larger number of clusters is associated with each sensory term for the abstract, concrete, or both conditions than the other conditions. The number of clusters does not seem to be the right measure to compare. Clusters differ in size so the number of clusters does not represent the area within the brain well. Nor is it clear that many brain regions should respond to each sensory term, and not just one per term (whether that is V1 or the entire occipital lobe, for instance). The number of clusters is therefore somewhat arbitrary. This is further complicated by the assessment across 20 time points and the inclusion of the 'both' categories. It would seem more appropriate to see whether each abstract and concrete cluster could be associated with each different sensory term and then summarise these findings rather than assess the number of abstract or concrete clusters found for each independent sensory term. In general, the rationale for the methods used should be provided (including the peak and valley method instead of other possible options e.g., linear regression).</p></disp-quote><p>We included an assessment of whether each abstract and concrete cluster could be associated with each different sensory term and then summarised these findings on a participant level in the supplementary material (Figures S3, S4, and S5).</p><p>Rationales for the Amplitude Modulated Deconvolution are now provided on page 10 (specifically the first paragraph under “Deconvolution Analysis” in the Methods section) and for the P&amp;V on pages 13, 14 and 15 (under “Peaks and Valley” (particularly the first paragraph) in the Methods section).</p><disp-quote content-type="editor-comment"><p>The measure of contextual situatedness (how related a spoken word is to the average of the visually presented objects in a scene) is an interesting approach that allows parametric variation within naturalistic stimuli, which is a potential strength of the study. This measure appears to vary little between objects that are present (e.g., animal or room), and those that are strongly (e.g., monitor) or weakly related (e.g., science). Additional information validating this measure may be useful, as would consideration of the range of values and whether the split between situated (c &gt; 0.6) and displaced words (c &lt; 0.4) is sufficient.</p></disp-quote><p>The main validation of our measure of contextual situatedness derives from the high accuracy and reliability of CNNs in object detection and recognition tasks, as demonstrated in numerous benchmarks and real-world applications.</p><p>One reason for low variability in our measure of contextual situatedness is the fact that we compared the GloVe vector of each word of interest with an average GloVe vector of all object-words referring to objects present in 56 frames (~300 objects on average). This means that a lot of variability in similarity measures between individual object-words and the word of interest is averaged out. Notwithstanding the resulting low variability of our measure, we thought that this would be the more conservative approach, as even small differences between individual measures (e.g. 0.4 vs 0.6) would constitute a strong difference on average (across the 300 objects per context window). Therefore, this split ensures a sufficient distinction between words that are strongly related to their visual context and those that are not – which in turn allows us to properly investigate the impact of contextual relevance on conceptual processing.</p><disp-quote content-type="editor-comment"><p>Finally, the study assessed the relation of spoken concrete or abstract words to brain activity at different time points. The visual scene was always assessed using the 2 seconds before the word, while the neural effects of the word were assessed every second after the presentation for 20 seconds. This could be a strength of the study, however almost no temporal information was provided. The clusters shown have different timings, but this information is not presented in any way. Giving more temporal information in the results could help to both validate this approach and show when these areas are involved in abstract or concrete word processing.</p></disp-quote><p>We provide more information on the temporal differences of when clusters are involved in processing concrete and abstract concepts in the supplementary material (Figure S9) and refer to this information where relevant in the Methods and Results sections.</p><disp-quote content-type="editor-comment"><p>Additionally, no rationale was given for this long timeframe which is far greater than the time needed to process the word, and long after the presence of the visual context assessed (and therefore ignores the present visual context).</p></disp-quote><p>The 20-second timeframe for our deconvolution analysis is justified by several considerations. Firstly, the hemodynamic response function (HRF) is known to vary both across individuals and within different regions of the brain. To accommodate this variability and capture the full breadth of the HRF, including its rise, peak, and return to baseline, a longer timeframe is often necessary. The 20-second window ensures that we do not prematurely truncate the HRF, which could lead to inaccurate estimations of neural activity related to the processing of words. Secondly and related to this point, unlike model-based approaches that assume a canonical HRF shape, our deconvolution analysis does not impose a predefined form on the HRF, instead reconstructing the HRF from the data itself – for this, a longer time-frame is advantageous to get a better estimation of the true HRF. Finally, and related to this point, the use of the 'Csplin' function in our analysis provides a flexible set of basis functions for deconvolution, allowing for a more fine-grained and precise estimation of the HRF across this extended timeframe. The 'Csplin' function offers more interpolation between time points, which is particularly advantageous for capturing the nuances of the HRF as it unfolds over a longer time-frame.</p><p>Although we use a 20-second timeframe for the deconvolution analysis to capture the full HRF, the analysis is still time-locked to the onset of each visual stimulus. This ensures that the initial stages of the HRF are directly tied to the moment the word is presented, thus incorporating the immediate visual context. We furthermore include variables that represent aspects of the visual context at the time of word presentation in our models (e.g luminance) and control for motion (optical flow), colour saturation and spatial frequency of immediate visual context.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>The context measure is interesting, but I'm not convinced that it's capturing what the authors intended. In analysing the neural response to a single word, the authors are presuming that they have isolated the window in which that concept is processed and the observed activation corresponds to the neural representation of that word given the prior context. I question to what extent this assumption holds true in a narrative when co-articulation blurs the boundaries between words and when rapid context integration is occurring.</p></disp-quote><p>We appreciate the reviewer's critical perspective on the contextual measure employed in our study. We agree that the dynamic and continuous nature of narrative comprehension poses challenges for isolating the neural response to individual words. However, the use of an amplitude modulated deconvolution analysis, particularly with the CSPLIN function, is a methodological choice to specifically address these challenges. Deconvolution allows us to estimate the hemodynamic response function (HRF) without assuming its canonical shape, capturing nuances in the BOLD signal that may reflect the integration of rapid contextual shifts only beyond the average modulation of the BOLD signal. The CSPLIN function further refines this approach by offering a flexible basis set for modelling the HRF and by providing a detailed temporal resolution that can adapt to the variance in individual responses.</p><p>Our choice of a 20-second window is informed by the need to encompass not just the immediate response to a word but also the extended integration of the contextual information. This is consistent with evidence indicating that the brain integrates information over longer timescales when processing language in context (Hasson et al., 2015). The neural representation of a word is not a static snapshot but a dynamic process that evolves with the unfolding narrative.</p><disp-quote content-type="editor-comment"><p>Further, the authors define context based on the preceding visual information. I'm not sure that this is a strong manipulation of the narrative context, although I agree that it captures some of the local context. It is maybe not surprising that if a word, abstract or concrete, has a strong association with the preceding visual information then activation in the occipital cortex is observed. I also wonder if the effects being captured have less to do with concrete and abstract concepts and more to do with the specific context the displaced condition captures during a multimodal viewing paradigm. If the visual information is less related to the verbal content, the viewer might process those narrative moments differently regardless of whether the subsequent word is concrete or abstract. I think the claims could be tailored to focus less generally on context and more specifically on how visually presented objects, which contribute to the ongoing context of a multimodal narrative, influence the subsequent processing of abstract and concrete concepts.</p></disp-quote><p>The context measure, though admittedly a simplification, is designed to capture the local visual context preceding word presentation. By using high-confidence visual recognition models, we ensure that the visual information is reliably extracted and reflects objects that have a strong likelihood of influencing the processing of subsequent words. We acknowledge that this does not capture the full richness of narrative context; however, it provides a quantifiable and consistent measure of the immediate visual environment, which is an important aspect of context in naturalistic language comprehension.</p><p>With regards to the effects observed in the occipital cortex, we posit that while some activation might be attributable to the visual features of the narrative, our findings also reflect the influence of these features on conceptual processing. This is especially because our analysis only looks at the modulation of the HRF amplitude beyond the average response (so also beyond the average visual response) when contrasting between conditions of high and low visual-contextual association with important (audio-visual) control variables included in the model.</p><p>Lastly, we concur that both concrete and abstract words are processed within a multimodal narrative, which could influence their neural representation. We believe our approach captures a meaningful aspect of this processing, and we have refined our claims to specify the influence of visually presented objects on the processing of abstract and concrete concepts, rather than making broader assertions about multimodal context. We also highlight several other signals (e.g. auditory) that could influence processing.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(1) The approach taken here requires a lot of manual variable selection and seems a bit roundabout. Why not build an encoding model that can predict the BOLD time course of each voxel in a participant from the feature-of-interest like valence etc. and then analyze if (1) certain features better predict activity in a specific region (2) the predicted responses/regression parameters are more positive (peaks) or more negative (valleys) for certain features in a specific brain region (3) maybe even use contextual features use a large language model and then per word (like &quot;truth&quot;) analyze where the predicted responses diverge based on the associated context. This seems like a simpler approach than having multiple stages of analysis.</p></disp-quote><p>It is not clear to us why an encoding model would be more suitable for answering the question at hand (especially given that we tried to clarify concerns about non-linear relationships between variables). On the contrary, fitting a regression model to each individual voxel has several drawbacks. First, encoding models are prone to over-estimate effect sizes (Naselaris et al., 2011). Second, encoding models are not good at explaining group-level effects due to high variability between individual participants (Turner et al., 2018). We would also like to point out that an encoding model using features of a text-based LLM would not address the visual context question - unless the LLM was multimodal. Multimodal LLMs are a very recent research development in Artificial Intelligence, however, and models like LLaMA (adapter), Google’s Gemini, etc. are not truly multimodal in the sense that would be useful for this study, because they are first trained on text and later injected with visual data. This relates to our concern that the reviewer may have misunderstood that we are interested in purely visual context of words (not linguistic context).</p><disp-quote content-type="editor-comment"><p>(2) In multiple analyses, a subset of the selected words is sampled to create a balanced set between the abstract and concrete categories. Do the authors show standard deviation across these sets?</p></disp-quote><p>For the subset of words used in the context-based analyses, we give mean ratings of concreteness, log frequency and length and conduct a t-test to show that these variables are not significantly different between the sets. We also included the psycholinguistic control variables surprisal and semantic diversity, as well as the visual variables motion (optical flow), colour saturation and spatial frequency.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Figures S3-5 are central to the argument and should be in the main text (potentially combined).</p></disp-quote><p>These have been added to the main text</p><disp-quote content-type="editor-comment"><p>S5 says the top 3 terms are DMN (and not semantic control), but the text suggests the r value is higher for 'semantic control' than 'DMN'?</p></disp-quote><p>Fixed this in the text, the caption now reads:</p><p>“This was confirmed by using the neurosynth decoder on the unthresholded brain image - top keywords were “Semantic Control” and “DMN”.”</p><disp-quote content-type="editor-comment"><p>Fig. S7 is very hard to see due to the use of grey on grey. Not used for great effect in the final sentence, but should be used to help interpret areas in the results section (if useful). It has not been specified how the 'language network' has been identified/defined here.</p></disp-quote><p>We altered the contrast in the figure to make boundaries more visible and specified how the language network was identified in the figure caption.</p><disp-quote content-type="editor-comment"><p>In the Results 'This showed that concrete produced more modulation than abstract modulation in the frontal lobes,' should be parts of /some of the frontal lobes as this isn't true overall.</p></disp-quote><p>Fixed this in the text.</p><disp-quote content-type="editor-comment"><p>There are some grammatical errors and lack of clarity in the context comparison section of the results.</p></disp-quote><p>Fixed these in the text.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>• The analysis code should be shared on the github page prior to peer review.</p></disp-quote><p>The code is now shared under: https://github.com/ViktorKewenig/Naturalistic_Encoding_Concepts</p><disp-quote content-type="editor-comment"><p>• At several points throughout the methods section, information was referred to that had not yet been described. Reordering the presentation of this information would greatly improve interpretability. A couple of examples of this are provided below.</p><p>Deconvolution Analysis: the use of amplitude modulation regression was introduced prior to a discussion of using the TENT function to estimate the shape of the HRF. This was then followed by a discussion of the general benefits of amplitude modulation. Only after these paragraphs are the modulators/model structure described. Moving this information to the beginning of the section would make the analysis clearer from the onset.</p></disp-quote><p>Fixed this in the text</p><disp-quote content-type="editor-comment"><p>Peak and Valley Analysis: the hypotheses regarding the sensory-motor features and experiential features are provided prior to describing how these features were extracted from the data (e.g., using the Lancaster norms).</p></disp-quote><p>Fixed this in the text.</p><disp-quote content-type="editor-comment"><p>• The justification for and description of the IRF approach seems overdone considering the timing differences are not analyzed further or discussed.</p></disp-quote><p>We now present a further discussion of timing differences in the supplementary material.</p><disp-quote content-type="editor-comment"><p>• The need and suitability of the cluster simulation method as implemented were not clear. The resulting maps were thresholded at 9 different p values and then combined, and an arbitrary cluster threshold of 20 voxels was then applied. Why not use the standard approach of selecting the significance threshold and corresponding cluster size threshold from the ClustSim table?</p></disp-quote><p>We extracted the original clusters at 9 different p values with the corresponding cluster size from the ClustSim table, then only included clusters that were bigger than 20 voxels.</p><disp-quote content-type="editor-comment"><p>• Why was the center of mass used instead of the peak voxel?</p></disp-quote><p>Peak voxel analysis can be sensitive to noise and may not reliably represent the region's activation pattern, especially in naturalistic imaging data where signal fluctuations are more variable and outliers more frequent. The centre of mass provides a more stable and representative measure of the underlying neural activity. Another reason for using the center of mass is that it better represents the anatomical distribution of the data, especially in large clusters with more than 100 voxels where peak voxels are often located at the periphery.</p><disp-quote content-type="editor-comment"><p>• Figure 1 seems to reference a different Figure 1 that shows the abstract, concrete, and overlap clusters of activity (currently Figure 3).</p></disp-quote><p>Fixed this in the text.</p><disp-quote content-type="editor-comment"><p>• Table S1 seems to have the &quot;Touch&quot; dimension repeated twice with different statistics reported.</p></disp-quote><p>Fixed this in the text, the second mention of the dimension “touch” was wrong.</p><disp-quote content-type="editor-comment"><p>• It appears from the supplemental files that the Peaks and Valley analysis produces different results at different lag times. This might be expected but it's not clear why the results presented in the main text were chosen over those in the supplemental materials.</p></disp-quote><p>The results in the main text were chosen over those in the supplementary material, because the HRF is said to peak at 5s after stimulus onset. We added a specification of this rational to the “2. Peak and Valley Analysis” subsection in the Methods section.</p><p>References (in order of appearance)</p><p>(1) Neumann J, Lohmann G, Zysset S, von Cramon DY. Within-subject variability of BOLD response dynamics. Neuroimage. 2003 Jul;19(3):784-96. doi: 10.1016/s10538119(03)00177-0. PMID: 12880807.</p><p>(2) Handwerker DA, Ollinger JM, D'Esposito M. Variation of BOLD hemodynamic responses across subjects and brain regions and their effects on statistical analyses. Neuroimage. 2004 Apr;21(4):1639-51. doi: 10.1016/j.neuroimage.2003.11.029. PMID: 15050587.</p><p>(3) Binder JR, Westbury CF, McKiernan KA, Possing ET, Medler DA. Distinct brain systems for processing concrete and abstract concepts. J Cogn Neurosci. 2005 Jun;17(6):90517. doi: 10.1162/0898929054021102. PMID: 16021798</p><p>(4) Bucur, M., Papagno, C. An ALE meta-analytical review of the neural correlates of abstract and concrete words. Sci Rep 11, 15727 (2021). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-021-94506-9">https://doi.org/10.1038/s41598-021-94506-9</ext-link>.</p><p>(5) Hale., J. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies (NAACL '01). Association for Computational Linguistics, USA, 1–8. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3115/1073336.1073357">https://doi.org/10.3115/1073336.1073357</ext-link>.</p><p>(6) Brysbaert, M., New, B. Moving beyond Kučera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behavior Research Methods 41, 977–990 (2009). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BRM.41.4.977">https://doi.org/10.3758/BRM.41.4.977</ext-link>.</p><p>(7) Hasson, U., Nir, Y., Levy, I., Fuhrmann, G., &amp; Malach, R. (2004). Intersubject Synchronization of Cortical Activity During Natural Vision. <italic>Science</italic>, <italic>303</italic>(5664), 6.</p><p>(8) Naselaris T, Kay KN, Nishimoto S, Gallant JL. Encoding and decoding in fMRI. Neuroimage. 2011 May 15;56(2):400-10. doi: 10.1016/j.neuroimage.2010.07.073. Epub 2010 Aug 4. PMID: 20691790; PMCID: PMC3037423.</p><p>(9) Turner BO, Paul EJ, Miller MB, Barbey AK. Small sample sizes reduce the replicability of task-based fMRI studies. Commun Biol. 2018 Jun 7;1:62. doi: 10.1038/s42003-0180073-z. PMID: 30271944; PMCID: PMC6123695.</p><p>(10) He, K., Zhang, Y., Ren, S., &amp; Sun, J. (2015). Deep Residual Learning for Image Recognition. <italic>Bioarchive (Tech Report)</italic>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1512.03385">https://doi.org/10.48550/arXiv.1512.03385</ext-link>.</p><p>(11) Hasson, U., &amp; Egidi, G. (2015). What are naturalistic comprehension paradigms teaching us about language? In R. M. Willems (Ed.), <italic>Cognitive neuroscience of natural language use</italic> (pp. 228–255). Cambridge University Press. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/CBO9781107323667.011">https://doi.org/10.1017/CBO9781107323667.011</ext-link>.</p></body></sub-article></article>