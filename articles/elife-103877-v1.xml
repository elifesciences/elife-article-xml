<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">103877</article-id><article-id pub-id-type="doi">10.7554/eLife.103877</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.103877.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Cell Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>A differentiable Gillespie algorithm for simulating chemical kinetics, parameter estimation, and designing synthetic biological circuits</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Rijal</surname><given-names>Krishna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7236-7387</contrib-id><email>krishnarijal331@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Mehta</surname><given-names>Pankaj</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1290-5897</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qwgg493</institution-id><institution>Department of Physics, Boston University</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution></institution-wrap><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>17</day><month>03</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP103877</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-10-10"><day>10</day><month>10</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-09-25"><day>25</day><month>09</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.07.07.602397"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-07"><day>07</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103877.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-02-26"><day>26</day><month>02</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.103877.2"/></event></pub-history><permissions><copyright-statement>© 2025, Rijal and Mehta</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Rijal and Mehta</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-103877-v1.pdf"/><abstract><p>The Gillespie algorithm is commonly used to simulate and analyze complex chemical reaction networks. Here, we leverage recent breakthroughs in deep learning to develop a fully differentiable variant of the Gillespie algorithm. The differentiable Gillespie algorithm (DGA) approximates discontinuous operations in the exact Gillespie algorithm using smooth functions, allowing for the calculation of gradients using backpropagation. The DGA can be used to quickly and accurately learn kinetic parameters using gradient descent and design biochemical networks with desired properties. As an illustration, we apply the DGA to study stochastic models of gene promoters. We show that the DGA can be used to: (1) successfully learn kinetic parameters from experimental measurements of mRNA expression levels from two distinct <italic>Escherichia coli</italic> promoters and (2) design nonequilibrium promoter architectures with desired input–output relationships. These examples illustrate the utility of the DGA for analyzing stochastic chemical kinetics, including a wide variety of problems of interest to synthetic and systems biology.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>differentiable Gillespie algorithm</kwd><kwd>stochastic simulation</kwd><kwd>synthetic biology</kwd><kwd>parameter estimation</kwd><kwd>gene regulatory networks</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000057</institution-id><institution>National Institute of General Medical Sciences</institution></institution-wrap></funding-source><award-id>R35GM119461</award-id><principal-award-recipient><name><surname>Mehta</surname><given-names>Pankaj</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014989</institution-id><institution>Chan Zuckerberg Initiative</institution></institution-wrap></funding-source><award-id>Investigator grant</award-id><principal-award-recipient><name><surname>Mehta</surname><given-names>Pankaj</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A differentiable variant of the Gillespie algorithm enables gradient-based optimization for stochastic chemical kinetics, facilitating efficient parameter estimation and the design of biochemical networks with desired input–output relationships.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Randomness is a defining feature of our world. Stock market fluctuations, the movement of particles in fluids, and even the change of allele frequencies in organismal populations can all be described using the language of stochastic processes. For this reason, disciplines as diverse as physics, biology, ecology, evolution, finance, and engineering have all developed tools to mathematically model stochastic processes (<xref ref-type="bibr" rid="bib56">Van Kampen, 1992</xref>; <xref ref-type="bibr" rid="bib15">Gardiner, 2009</xref>; <xref ref-type="bibr" rid="bib47">Rolski et al., 2009</xref>; <xref ref-type="bibr" rid="bib60">Wong and Hajek, 2012</xref>). In the context of biology, an especially fruitful area of research has been the study of stochastic gene expression in single cells (<xref ref-type="bibr" rid="bib27">McAdams and Arkin, 1997</xref>; <xref ref-type="bibr" rid="bib13">Elowitz et al., 2002</xref>; <xref ref-type="bibr" rid="bib43">Raj and van Oudenaarden, 2008</xref>; <xref ref-type="bibr" rid="bib49">Sanchez and Golding, 2013</xref>). The small number of molecules involved in gene expression make stochasticity an inherent feature of protein production and numerous mathematical and computational techniques have been developed to model gene expression and relate mathematical models to experimental observations (<xref ref-type="bibr" rid="bib38">Paulsson, 2005</xref>; <xref ref-type="bibr" rid="bib59">Wilkinson, 2018</xref>).</p><p>One prominent computational algorithm for understanding stochasticity in gene expression is the Gillespie algorithm, with its Direct Stochastic Simulation Algorithm variant being the most commonly used method (<xref ref-type="bibr" rid="bib11">Doob, 1945</xref>; <xref ref-type="bibr" rid="bib17">Gillespie, 1977</xref>). The Gillespie algorithm is an extremely efficient computational technique used to simulate the time evolution of a system in which events occur randomly and discretely (<xref ref-type="bibr" rid="bib17">Gillespie, 1977</xref>). Beyond gene expression, the Gillespie algorithm is widely employed across numerous disciplines to model stochastic systems characterized by discrete, randomly occurring events including epidemiology (<xref ref-type="bibr" rid="bib41">Pineda-Krch, 2008</xref>), ecology (<xref ref-type="bibr" rid="bib36">Parker and Kamenev, 2009</xref>; <xref ref-type="bibr" rid="bib10">Dobramysl et al., 2018</xref>), neuroscience (<xref ref-type="bibr" rid="bib4">Benayoun et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Rijal et al., 2024</xref>), and chemical kinetics (<xref ref-type="bibr" rid="bib16">Gillespie, 1976</xref>; <xref ref-type="bibr" rid="bib18">Gillespie, 2007</xref>).</p><p>Here, we revisit the Gillespie algorithm in light of the recent progress in deep learning and differentiable programming by presenting a ‘fully differentiable’ variant of the Gillespie algorithm we dub the differentiable Gillespie algorithm (DGA). The DGA modifies the traditional Gillespie algorithm to take advantage of powerful automatic differentiation libraries for example, PyTorch (<xref ref-type="bibr" rid="bib37">Paszke et al., 2019</xref>), Jax (<xref ref-type="bibr" rid="bib7">Bradbury et al., 2018</xref>), and Julia (<xref ref-type="bibr" rid="bib5">Bezanson et al., 2017</xref>) and gradient-based optimization. The DGA allows us to quickly fit kinetic parameters to data and design discrete stochastic systems with a desired behavior. Our work is similar in spirit to other recent work that seeks to harness the power of differentiable programming to accelerate scientific simulations (<xref ref-type="bibr" rid="bib26">Liao et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Schoenholz and Cubuk, 2020</xref>; <xref ref-type="bibr" rid="bib58">Wei et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Degrave et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Arya et al., 2022</xref>; <xref ref-type="bibr" rid="bib3">Arya et al., 2023</xref>; <xref ref-type="bibr" rid="bib6">Bezgin et al., 2023</xref>). The DGA’s use of differential programming tools also complements more specialized numerical methods designed for performing parameter sensitivity analysis on Gillespie simulations such as finite-difference methods (<xref ref-type="bibr" rid="bib1">Anderson, 2012</xref>; <xref ref-type="bibr" rid="bib52">Srivastava et al., 2013</xref>; <xref ref-type="bibr" rid="bib54">Thanh et al., 2018</xref>), the likelihood ratio method (<xref ref-type="bibr" rid="bib19">Glynn, 1990</xref>; <xref ref-type="bibr" rid="bib28">McGill et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Núñez and Vlachos, 2015</xref>), and pathwise derivative methods (<xref ref-type="bibr" rid="bib51">Sheppard et al., 2012</xref>).</p><p>One of the difficulties in formulating a differentiable version of the Gillespie algorithm is that the stochastic systems it treats are inherently discrete. For this reason, there is no obvious way to take derivatives with respect to kinetic parameters without making approximations. As shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, in the traditional Gillespie algorithm both the selection of the index for the next reaction and the updates of chemical species are both discontinuous functions of the kinetic parameters. To circumnavigate these difficulties, the DGA modifies the traditional Gillespie algorithm by approximating discrete operations with continuous, differentiable functions, smoothing out abrupt transitions to facilitate gradient computation via automatic differentiation (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This significant modification preserves the core characteristics of the original algorithm while enabling integration with modern deep learning techniques.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Comparison between the exact Gillespie algorithm and the differentiable Gillespie algorithm (DGA) for simulating chemical kinetics.</title><p>(<bold>a</bold>) Example of kinetics with <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:math></inline-formula> reactions with rates <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. (<bold>b</bold>) Illustration of the DGA’s approximations: replacing the non-differentiable Heaviside and Kronecker delta functions with smooth sigmoid and Gaussian functions, respectively. (<bold>c</bold>) Flow chart comparing exact and differentiable Gillespie simulations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig1-v1.tif"/></fig><p>One natural setting for exploring the efficacy of the DGA is recent experimental and theoretical works exploring stochastic gene expression. Here, we focus on a set of beautiful experiments that explore the effect of promoter architecture on steady-state gene expression (<xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref>). An especially appealing aspect of <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref> is that the authors independently measured the kinetic parameters for these promoter architectures using orthogonal experiments. This allows us to directly compare the predictions of DGA to ground truth measurements of kinetic parameters. We then extend our considerations to more complex promoter architectures (<xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>) and illustrate how the DGA can be used to design circuits with a desired input–output relation.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A differentiable approximation to the Gillespie algorithm</title><p>Before proceeding to discussing the DGA, we start by briefly reviewing how the traditional Gillespie algorithm simulates discrete stochastic processes. For concreteness, in our exposition, we focus on the chemical system shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> consisting of three species, A, B, and C, whose abundances are described by a state vector <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. These chemical species can undergo <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:math></inline-formula> chemical reactions, characterized by rate constants, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a stoichiometric matrix <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>S</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> whose <italic>i</italic>th row encodes how the abundance <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> of species <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> changes due to reaction <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. Note that in what follows, we will often supress the dependence of the rates <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> on <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and simply write <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p><p>In order to simulate such a system, it is helpful to discretize time into small intervals of size <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. The probability that a reaction <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> with rate <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> occurs during such an interval is simply <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. By construction, we choose <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> to be small enough that <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and that the probability that a reaction occurs in any interval <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is extremely small and well described by a Poisson process. This means that naively simulating such a process is extremely inefficient because, in most intervals, no reactions will occur.</p></sec><sec id="s2-2"><title>Gillespie algorithm</title><p>The Gillespie algorithm circumnavigates this problem by: (1) exploiting the fact that the reactions are independent so that the rate at which <italic>any</italic> reaction occurs is also described by an independent Poisson process with rate <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and (2) the waiting time distribution <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> of a Poisson process with rate <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi></mml:mstyle></mml:math></inline-formula> is the exponential distribution <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. The basic steps of the Gillespie algorithm are illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><p>The simulation begins with the initialization of time and state variables:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>t</mml:mi></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> is the simulation time. One then samples the waiting time distribution <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for a reaction to occur to determine when the next the reaction occurs. This is done by drawing a random number <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi></mml:mstyle></mml:math></inline-formula> from a uniform distribution over <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> and updating<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that this time update is a fully differentiable function of the rates <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p><p>In order to determine which of the reactions <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> occurs after a time <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, we note that probability that reaction <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> occurs is simply given by <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, we can simply draw another random number <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and choose <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> such that <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> equals the smallest integer satisfying<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mo>&gt;</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The reaction abundances <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are then updated using the stoichiometric matrix<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Unlike the time update, both the choice of the reaction <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and the abundance updates are not differentiable since the choice of the reaction <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is a discontinuous function of the parameters <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s2-3"><title>Approximating updates in the Gillespie with differentiable functions</title><p>In order to make use of modern deep learning techniques and modern automatic differentiation packages, it is necessary to modify the Gillespie algorithm in such as way as to make the choice of reaction index (<xref ref-type="disp-formula" rid="equ3">Equation 2</xref>) and abundance updates (<xref ref-type="disp-formula" rid="equ4">Equation 3</xref>) differentiable functions of the kinetic parameters. To do so, we rewrite <xref ref-type="disp-formula" rid="equ3">Equation 2</xref> using a sum of Heaviside step function <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (recall <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ5">.<label>(4)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi mathvariant="normal">Θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>R</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This formulation of index selection makes clear the source of non-differentiability. The derivative of the <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> does not exist at the transition points where the Heaviside function jumps (see <xref ref-type="fig" rid="fig1">Figure 1b</xref>).</p><p>This suggests a natural modification of the Gillespie algorithm to make it differentiable – replacing the Heaviside function <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by a sigmoid function of the form<disp-formula id="equ6">,<label>(5)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have introduced a ‘hyper-parameter’ <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> that controls the steepness of the sigmoid and plays an analogous role to temperature in a Fermi function in statistical mechanics. A larger value of <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> results in a steeper slope for the sigmoid functions, thereby more closely approximating the true Heaviside functions which is recovered in the limit <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig1">Figure 1b</xref>). With this replacement, the index selection equation becomes<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>R</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that in making this approximation, our index is no longer an integer, but instead can take on all real values between 0 and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula>. However, by making <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> sufficiently small, <xref ref-type="disp-formula" rid="equ7">Equation 6</xref> still serves as a good approximation to the discrete jumps in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref>. In general, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> is a hyperparameter that is chosen to be as small as possible while still ensuring that the gradient of <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> with respect to the kinetic parameters <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> can be calculated numerically with high accuracy. For a detailed discussion, please see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and Appendix 1.</p><p>Since the index <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is no longer an integer but a real number, we must also modify the abundance update in <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> to make it fully differentiable. To do this, we start by rewriting <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> using the Kronecker delta <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (where <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) as<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is no longer an integer, we can approximate the Kronecker delta <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> by a Gaussian function, to arrive at the approximate update equation<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The hyperparameter <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula> is generally chosen to be as small as possible while still ensuring numerical stability of gradients (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Note by using an abundance update of the form <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>, the species abundances <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are now real numbers. This is in stark contrast with the exact Gillespie algorithm where the abundance update (<xref ref-type="disp-formula" rid="equ8">Equation 7</xref>) ensures that the <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are all integers.</p></sec><sec id="s2-4"><title>Combining the DGA with gradient-based optimization</title><p>The goal of making Gillespie simulations differentiable is to enable the computation of the gradient of a <italic>loss function</italic>, <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, with respect to the kinetics parameters <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. A loss function quantifies the difference between simulated and desired values for quantities of interest. For example, when employing the DGA in the context of fitting noisy gene expression models, a natural choice for <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the difference between the simulated and experimentally measured moments of mRNA/protein expression (or alternatively, the Kullback–Leibler divergence between the experimental and simulated mRNA/protein expression distributions if full distributions can be measured). When using the DGA to design gene circuits, the loss function can be any function that characterizes the difference between the simulated and desired values of the input–output relation.</p><p>The goal of the optimization using the DGA is to find parameters <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that minimize the loss. The basic workflow of a DGA-based optimization is shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. One starts with an initial guess for the parameters <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. One then uses DGA algorithm to simulate the systems and calculate the gradient of the loss function <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. One then updates the parameters, moving in the direction of the gradient using gradient descent or more advanced methods such as ADAM (<xref ref-type="bibr" rid="bib21">Kingma and Ba, 2014</xref>; <xref ref-type="bibr" rid="bib31">Mehta et al., 2019</xref>), which uses adaptive estimates of the first and second moments of the gradients to speed up convergence to a local minimum of the loss function.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Flow chart of the parameter optimization process using the differentiable Gillespie algorithm (DGA).</title><p>The process begins by initializing the parameters <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. Simulations are then run using the DGA to obtain statistics <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> like moments. These statistics are used to compute the loss <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the gradient of the loss <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>L</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained. Finally, parameters are updated using the ADAM optimizer, and the process iterates to minimize the loss.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig2-v1.tif"/></fig></sec><sec id="s2-5"><title>The price of differentiability</title><p>A summary of the DGA is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Unsurprisingly, differentiability comes at a price. The foremost of these is that unlike the Gillespie algorithm, the DGA is no longer exact. The DGA replaces the exact discrete stochastic system by an approximate differentiable stochastic system. This is done by allowing both the reaction index and the species abundances to be continuous numbers. Though in theory, the errors introduced by these approximations can be made arbitrarily small by choosing the hyper-parameters <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula> small enough (see <xref ref-type="fig" rid="fig1">Figure 1</xref>), in practice, gradients become numerically unstable when <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula> are sufficiently small (see Appendix 1 and <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>).</p><p>In what follows, we focus almost exclusively on steady-state properties that probe the ‘bulk’, steady-state properties of the stochastic system of interest. We find the DGA works well in this setting. However, we note that the effect of the approximations introduced by the DGA may be pronounced in more complex settings such as the calculation of rare events, modeling of tail-driven processes, or dealing with non-stationary time series.</p><p>In order to better understand the DGA in the context of stochastic gene expression, we benchmarked the DGA on a simple two-state promoter model inspired by experiments in <italic>Escherichia coli</italic> (<xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref>). This simple model had several advantages that make it well suited for exploring the performance of DGA. These include the ability to analytically calculate mRNA expression distributions and independent experimental measurements of kinetic parameters.</p></sec><sec id="s2-6"><title>Two-state promoter model</title><p>Gene regulation is tightly regulated at the transcriptional level to ensure that genes are expressed at the right time, place, and in the right amount (<xref ref-type="bibr" rid="bib39">Phillips et al., 2012</xref>). Transcriptional regulation involves various mechanisms, including the binding of transcription factors to specific DNA sequences, the modification of chromatin structure, and the influence of non-coding RNAs, which collectively control the initiation and rate of transcription (<xref ref-type="bibr" rid="bib39">Phillips et al., 2012</xref>; <xref ref-type="bibr" rid="bib48">Sanchez et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Phillips et al., 2019</xref>). By orchestrating these regulatory mechanisms, cells can respond to internal signals and external environmental changes, maintaining homeostasis and enabling proper development and function.</p><p>Here, we focus on a classic two-state promoter gene regulation (<xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref>). Two-state promoter systems are commonly studied because they provide a simplified yet powerful model for understanding gene regulation dynamics. These systems, characterized by promoters toggling between active and inactive states, offer insights into how genes are turned on or off in response to various stimuli (see <xref ref-type="fig" rid="fig3">Figure 3a</xref>). The two-state gene regulation circuit involves the promoter region, where RNA polymerase (RNAP) binds to initiate transcription and synthesize mRNA molecules at a rate <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>. A repressor protein can also bind to the operator site at a rate <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and unbind at a rate <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. When the repressor is bound to the operator, it prevents RNAP from accessing the promoter, effectively turning off transcription. mRNA is also degraded at a rate <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. An appealing feature of this model is that both mean mRNA expression and the Fano factor can be calculated analytically and there exist beautiful quantitative measurements of both these quantities (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). For this reason, we use this two-state promoters to benchmark the efficacy of DGA below.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Two-state gene regulation architecture.</title><p>(<bold>a</bold>) Schematic of gene regulatory circuit for transcriptional repression. RNA polymerase (RNAP) binds to the promoter region to initiate transcription at a rate <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, leading to the synthesis of mRNA molecules (red curvy lines). mRNA is degraded at a rate <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. A repressor protein can bind to the operator site, with association and dissociation rates <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. (<bold>b</bold>) Experimental data from <xref ref-type="bibr" rid="bib40">Phillips et al., 2019</xref>, showing the relationship between the mean mRNA level and the Fano factor for two different promoter constructs: <italic>lac</italic>UD5 (green squares) and 5DL1 (red squares).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig3-v1.tif"/></fig></sec><sec id="s2-7"><title>Characterizing errors due to approximations in the DGA</title><p>We begin by testing the DGA to do forward simulations on the two-state promoter system described above and comparing the results to simulations performed with the exact Gillespie algorithm (see Appendix 2 for simulation details). <xref ref-type="fig" rid="fig4">Figure 4a</xref> compares the probability distribution function (PDF) for the steady-state mRNA levels obtained from the DGA (in red) and the exact Gillespie simulation (in blue). The close overlap of these distributions demonstrates that the DGA can accurately replicate the results of the exact Gillespie simulation. This is also shown by the very close match of the first four moments <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> of the mRNA count between the exact Gillespie and the DGA in <xref ref-type="fig" rid="fig4">Figure 4b</xref>, though the DGA systematically overestimates these moments. As observed in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, the DGA also fails to accurately capture the tails of the underlying PDF. This discrepancy arises because rare events result from very frequent low-probability reaction events where the sigmoid approximation used in the DGA significantly impacts the reaction selection process and, consequently, the final simulation results.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Accuracy of the differentiable Gillespie algorithm (DGA) in simulating the two-state promoter architecture in <xref ref-type="fig" rid="fig3">Figure 3a</xref>.</title><p>Comparison between the DGA and exact simulations for (<bold>a</bold>) steady-state mRNA distribution, (<bold>b</bold>) moments of the steady-state mRNA distribution, and (<bold>d</bold>) the probability for the promoter to be in the ‘ON’ or ‘OFF’ state. (<bold>c</bold>) Ratio of the Jensen–Shannon divergence <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>JSD</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> between the differentiable Gillespie probability distribution function (PDF) <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and the exact steady-state PDF <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, and the Shannon entropy <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>H</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of the exact steady-state PDF. In all of the plots, 2000 trajectories are used. The simulation time used in panels (<bold>a</bold>), (<bold>b</bold>), and (<bold>d</bold>) is marked by blue ‘x’. Parameter values: <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig4-v1.tif"/></fig><p>Next, we compare the accuracy of the DGA in simulating mRNA abundance distributions across a range of simulation times (see <xref ref-type="fig" rid="fig4">Figure 4c</xref>). The accuracy is quantified by the ratio of the Jensen–Shannon divergence <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>JSD</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> between the differentiable Gillespie PDF <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and the exact steady-state PDF <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, and the entropy <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>H</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> of the exact steady-state PDF. For probability distributions <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula> over the same discrete space <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the JSD and H are defined as:<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mtext>JSD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>∥</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>∥</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>∥</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>H</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">X</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the Kullback–Leibler divergence<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>∥</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">X</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The ratio <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">J</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> normalizes divergence by entropy, enabling meaningful comparison across systems. As expected, the <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mtext>JSD</mml:mtext><mml:mtext>H</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> ratio decreases with increasing simulation time, indicating convergence toward the steady-state distribution of the exact Gillespie simulation. By ‘steady-state distribution’, we mean the long-term probability distribution of states that the exact Gillespie algorithm approaches after a simulation time of 10<sup>4</sup>. The saturation of the <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mtext>JSD</mml:mtext><mml:mtext>H</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> ratio at approximately 0.003 for long simulation times is due to the finite values of <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. In percentage terms, this ratio represents a 0.3% divergence, meaning that the DGA’s approximation introduces only a 0.3% deviation from the exact distribution, relative to the total uncertainty (entropy) in the exact system.</p><p>Finally, the bar plot in <xref ref-type="fig" rid="fig4">Figure 4d</xref> shows simulation results for the probability of the promoter being in the ‘OFF’ and ‘ON’ states as predicted by the DGA (in red) and the exact Gillespie simulation (in blue). The differentiable Gillespie overestimates the probability of being in the ‘OFF’ state and underestimates the probability of being in the ‘ON’ state. Nonetheless, given the discrete nature of this system, the DGA does a reasonable job of matching the results of the exact simulations.</p><p>As we will see below, despite these errors the DGA is able to accurately capture gradient information and hence works remarkably well at gradient-based optimization of loss functions.</p></sec><sec id="s2-8"><title>Parameter estimation using the DGA</title><p>In many applications, one often wants to estimate kinetic parameters from experimental measurements of a stochastic system (<xref ref-type="bibr" rid="bib55">Tian et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Munsky et al., 2009</xref>; <xref ref-type="bibr" rid="bib22">Komorowski et al., 2009</xref>; <xref ref-type="bibr" rid="bib57">Villaverde et al., 2019</xref>). For example, in the context of gene expression, biologists are often interested in understanding biophysical parameters such as the rate at which promoters switch between states or a transcription factor unbinds from DNA. However, estimating kinetic parameters in stochastic systems poses numerous challenges because the vast majority of methods for parameter estimation are designed with deterministic systems in mind. Moreover, it is often difficult to analytically calculate likelihood functions making it difficult to perform statistical inference. One attractive method for addressing these difficulties is to combine differentiable Gillespie simulations with gradient-based optimization methods. By choosing kinetic parameters that minimize the difference between simulations and experiments as measured by a loss function, one can quickly and efficiently estimate kinetic parameters and error bars.</p></sec><sec id="s2-9"><title>Loss function for parameter estimation</title><p>To use the DGA for parameter estimation, we start by defining a loss function <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> that measures the discrepancy between simulations and experiments. In the context of the two-state promoter model (<xref ref-type="fig" rid="fig3">Figure 3</xref>), a natural choice of loss function is the square error between the simulated and experimentally measured mean and standard deviations of the steady-state mRNA distributions:<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the mean and standard deviation obtained from DGA simulations, and <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the experimentally measured values of the same quantities. Having specified the loss function and parameters, we then use the gradient-based optimization to minimize the loss and find the optimal parameters <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Note that in general the solution to the optimization problem need not be unique (see below).</p></sec><sec id="s2-10"><title>Confidence intervals and visualizing loss landscapes</title><p>Given a set of learned parameters <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that minimize <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, one would also ideally like to assign a confidence interval (CI) to this estimate that reflect how constrained these parameters are. One natural way to achieve this is by examining the curvature of the loss function as the parameter <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> varies around its minimum value, <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>. Motivated by this, we define the 95% CIs for parameter <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> by:<disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. A detailed explanation of how to numerically estimate the CIs is given in Appendix 3.</p><p>One shortcoming of <xref ref-type="disp-formula" rid="equ14">Equation 13</xref> is that it treats each parameter in isolation and ignores correlations between parameters. On a technical level, this is reflected in the observation that the CIs only know about the diagonal elements of the full Hessian <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This shortcoming is especially glaring when there are many sets of parameters that all optimize the loss function (<xref ref-type="bibr" rid="bib12">Einav et al., 2018</xref>; <xref ref-type="bibr" rid="bib44">Razo-Mejia et al., 2018</xref>). As discuss below, this is often the case in many stochastic systems including the two-state promoter architecture in <xref ref-type="fig" rid="fig3">Figure 3</xref>. For this reason, it is often useful to make two dimensional plots of the loss function <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To do so, for each pair of parameters, we simply sample the parameters around their optimal value and forward simulate to calculate the loss function <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We then use this simulations to create two-dimensional heat maps of the loss function. This allows us to identify ‘soft directions’ in parameter space, where the loss function <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> changes slowly, indicating weak sensitivity to specific parameter combinations.</p></sec><sec id="s2-11"><title>Parameter estimation on synthetic data</title><p>Before proceeding to experiments, we start by benchmarking the DGA’s ability to perform parameter estimation on synthetic data generated using the two-state promoter model shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. This model nominally has four independent kinetic parameters: the rate at which repressors bind the promoter, <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>; the rate at which the repressor unbinds from the promoter, <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>; the rate at which mRNA is produced, <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>; and the rate at which mRNA degrades, <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. Since we are only concerned with steady-state properties of the mRNA distribution, we choose to measure time in units of the off rate and set <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> in everything that follows. In Appendix 4, we make use of exact analytical results for <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> to show that the solution to the optimization problem specified by loss function in <xref ref-type="disp-formula" rid="equ12">Equation 11</xref> is degenerate – there are many combinations of the three parameters <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> that all optimize <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. On the other hand, if one fixes the mRNA degradation rate <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>, this degeneracy is lifted and there is a unique solution to the optimization problem for the two parameters <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. We discuss both these cases below.</p></sec><sec id="s2-12"><title>Generating synthetic data</title><p>To generate synthetic data, we randomly sample the three parameters: <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> within the range <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, while keeping <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> fixed at 1. In total, we generate 20 different sets of random parameters. We then perform exact Gillespie simulations for each set of parameters. Using these simulations, we obtain the mean <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the mRNA levels, which are then used as input to the loss function in <xref ref-type="disp-formula" rid="equ12">Equation 11</xref>. We then use the DGA to estimate the parameters using the procedure outlined above and compare the resulting predictions with ground truth values for simulations.</p></sec><sec id="s2-13"><title>Estimating parameters in the non-degenerate case</title><p>We begin by considering the case where the mRNA degradation rate <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> is known and the goal is to estimate the two other parameters: the repressor binding rate <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and the mRNA production rate <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>. As discussed above, in this case, the loss function in <xref ref-type="disp-formula" rid="equ12">Equation 11</xref> has a unique minima, considerably simplifying the inference task. <xref ref-type="fig" rid="fig5">Figure 5a</xref> shows a scatter plot of the learned and the true parameter values for wide variety of choices of <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. As can be seen, there is very good agreement between the true parameters and learned parameters. <xref ref-type="fig" rid="fig5">Figure 5c</xref> shows that even when the true and learned parameters differ, the DGA can predict the mean <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the steady-state mRNA distribution almost perfectly (see Appendix 5 for discussion of how error bars were estimated). To better understand this, we selected a set of learned parameters: <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>3.83</mml:mn></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>2.43</mml:mn></mml:mstyle></mml:math></inline-formula>. We then plotted the loss function in the neighborhood of these parameters (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). As can be seen, the loss function around the true parameters is quite flat and the learned parameters live at the edge of this flat region. The flatness of the loss function reflects the fact that the mean and standard deviation of the mRNA distribution depend weakly on the kinetic parameters.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Gradient-based learning via differentiable Gillespie algorithm (DGA) is applied to the synthetic data for the gene expression model in <xref ref-type="fig" rid="fig3">Figure 3a</xref>.</title><p>Parameters <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are fixed at 1, with <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula> for a simulation time of 10. (<bold>a</bold>) Scatter plot of true versus inferred parameters (<inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) with <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> constant. Error bars are 95% confidence intervals (CIs). Panel (<bold>b</bold>) plots the logarithm of the loss function near a learned parameter set (shown in red circles in (a)), showing insensitivity regions. Panel (<bold>c</bold>) compares true and predicted mRNA mean and standard deviation with 95% CIs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig5-v1.tif"/></fig></sec><sec id="s2-14"><title>Estimating parameters for the degenerate case</title><p>We now estimate parameters for the two-state promoter model when all three parameters <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> are unknown. As discussed above, in this case, there are many sets of parameters that all minimize the loss function in <xref ref-type="disp-formula" rid="equ12">Equation 11</xref>. <xref ref-type="fig" rid="fig6">Figure 6a</xref> shows a comparison between the learned and true parameters along with a heat map of the loss function for one set of synthetic parameters (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). As can be seen in the plots, though the true parameters and learned parameter values differ significantly, they do so along ‘sloppy’ directions where loss function is flat. Consistent with this intuition, we performed simulations comparing the mean <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and standard deviation <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the steady-state mRNA levels using the true and learned parameters and found near-perfect agreement across all of the synthetic data (<xref ref-type="fig" rid="fig6">Figure 6c</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Gradient-based learning via differentiable Gillespie algorithm (DGA) is applied to the synthetic data for the gene expression model in <xref ref-type="fig" rid="fig3">Figure 3a</xref>.</title><p>Parameters <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are fixed at 1, with <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula> for a simulation time of 10. (<bold>a</bold>) Scatter plot of true versus inferred parameters (<inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>). Error bars are 95% confidence intervals (CIs). Panel (<bold>b</bold>) plots the logarithm of the loss function near a learned parameter set (shown in red circles in (a)), showing insensitivity regions. Panel (<bold>c</bold>) compares true and predicted mRNA mean and standard deviation with 95% CIs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig6-v1.tif"/></fig></sec><sec id="s2-15"><title>Parameter estimation on experimental data</title><p>In the previous section, we demonstrated that our DGA can effectively obtain parameters for synthetic data. However, real experimental data often contains noise and variability, which can complicate the parameter estimation process. To test the DGA in this more difficult setting, we reanalyze experiments by <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref> which measured how mRNA expression changes in a system well described by the two-state gene expression model in <xref ref-type="fig" rid="fig3">Figure 3</xref>. In these experiments, two constitutive promoters <italic>lac</italic>UD5 and 5DL1 (with different transcription rates <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>) were placed under the control of a LacI repressor through the insertion of a LacI binding site. By systematically varying LacI concentrations, the authors were able to adjust the repressor binding rate <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>. mRNA fluorescence in situ hybridization was employed to measure mRNA expression, providing data on both mean expression levels <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and the variability as quantified by the Fano factor <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> for both promoters (see <xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><p>Given a set of measurements of the mean and Fano factor <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> for a promoter (<italic>lac</italic>UD5 and 5DL1), we construct a loss function of the form:<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msqrt><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:msqrt><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> runs over data points (each with a different lac repressor concentration) and <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are the mean and standard deviation obtained from a sample of DGA simulations. This loss function is chosen because, at its minimum, <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. As above, we set <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>, and focus on estimating the other three parameters <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. When performing our gradient-based optimization, we assume that the transcription rate <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and the mRNA degradation rate <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> are the same for all data points <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, while allowing <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> to vary across data points <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. This reflects the fact that <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> is a function of the lac repressor concentration which, by design, is varied across data points (see Appendix 6 for details on how this optimization is implemented and calculation of error bars).</p><p>The results of this procedure are summarized in <xref ref-type="fig" rid="fig7">Figure 7</xref>. We find that for the <italic>lac</italic>UD5 promoter <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>90.25</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>6.20</mml:mn></mml:mstyle></mml:math></inline-formula> and that <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> varies from a minimum value of 0.18 to a maximum value of 99.0. For the 5DL1 promoters <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>87.48</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>9.80</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> varies between 3.64 and 99.0. Recall that we have normalized all rates to the repressor unbinding rate <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. These values indicate that mRNA transcription occurs much faster compared to the unbinding of the repressor, suggesting that once the promoter is in an active state, it produces mRNA rapidly. The relatively high mRNA degradation rates indicate a mechanism for fine-tuning gene expression levels, ensuring that mRNA does not persist too long in the cell, which could otherwise lead to prolonged expression even after promoter deactivation.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Fitting of experimental data from <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref> using the differentiable Gillespie algorithm (DGA).</title><p>(<bold>a</bold>) Comparison between theoretical predictions from the DGA (solid curves) and experimental values of mean and the Fano factor for the steady-state mRNA levels are represented by square markers, along with the error bars, for two different promoters, <italic>lac</italic>UD5 and 5DL1. Solid curves are generated by using DGA to estimate <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> and using this as input to exact analytical formulas. (<bold>b</bold>) Comparison between the inferred values of <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> using DGA with experimentally measured values of this parameter from <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref>. (<bold>c</bold>) Inferred <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> values as a function of the mean mRNA level.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig7-v1.tif"/></fig><p>As expected, the repressor binding rates decrease with the mean mRNA level (see <xref ref-type="fig" rid="fig7">Figure 7c</xref>). The broad range of repressor binding rates shows that the system can adjust its sensitivity to repressor concentration, allowing for both tight repression and rapid activation depending on the cellular context.</p><p><xref ref-type="fig" rid="fig7">Figure 7a</xref> shows a comparison between the predictions of the DGA (solid curves) and the experimental data (squares) for mean mRNA levels and the Fano factor . The theoretical curves are obtained by using analytical expression for and from <xref ref-type="bibr" rid="bib18">Gillespie, 2007</xref> with parameters estimated from the DGA. We find that for the lacUD5 and the 5DL1 promoters, the mean percentage errors for predictions of the Fano factor are 25% and 28%, respectively (see Appendix 6).</p><p>An appealing feature of <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref> is that the authors performed independent experiments to directly measure the normalized transcription rate <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (namely the ratio of the transcription rate and the mRNA degradation rate). This allows us to compare the DGA predictions for these parameters to ground truth measurements of kinetic parameters. In <xref ref-type="fig" rid="fig7">Figure 7b</xref>, the predictions of the DGA agree remarkably well for both the <italic>lac</italic>UD5 and 5DL1 promoters.</p></sec><sec id="s2-16"><title>Designing gene regulatory circuits with desired behaviors</title><p>Another interesting application of the DGA is to design stochastic chemical or biological networks that exhibit a particular behavior. In many cases, this design problem can be reformulated as identifying choices of parameter that give rise to a desired behavior. Here, we show that the DGA is ideally suited for such a task. We focus on designing the input–output relation of a four state promoter model of gene regulation (<xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>). We have chosen this more complex promoter architecture because, unlike the two-state promoter model analyzed above, it allows for nonequilibrium currents. In making this choice, we are inspired by numerous recent works have investigated how cells can tune kinetic parameters to operate out of equilibrium in order to achieve increased sharpness/sensitivity (<xref ref-type="bibr" rid="bib33">Nicholson and Gingrich, 2023</xref><xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>; <xref ref-type="bibr" rid="bib62">Zoller et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Wong and Gunawardena, 2020</xref>; <xref ref-type="bibr" rid="bib9">Dixit et al., 2024</xref>).</p></sec><sec id="s2-17"><title>Model of nonequilibrium promoter</title><p>We focus on designing the steady-state input–output relationship of the four-state promoter model of gene regulation model shown in <xref ref-type="fig" rid="fig8">Figure 8a</xref>; <xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>. The locus can be in either an ‘ON’ state where mRNA is transcribed at a rate <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> or an ‘OFF’ state where the locus is closed and there is no transcription. In addition, a transcription factor (assumed to be an activator) with concentration <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> can bind to the locus with a concentration dependent rate <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in the ‘OFF’ state and a rate <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in the ‘ON’ rate. The activator can also unbind at a rate <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in the ‘OFF’ state and a rate <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in the ‘ON’ state. The average mRNA production rate (averaged over many samples) in this model is given by<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) is the steady-state probability of finding the system in each of the ‘ON’ states (see <xref ref-type="fig" rid="fig8">Figure 8a</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Design of the four-state promoter architecture using the differentiable Gillespie algorithm (DGA).</title><p>(<bold>a</bold>) Schematic of four-state promoter model. (<bold>b</bold>) Target input–output relationships (solid curves) and learned input–output relationships (blue dots) between activator concentration <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> and average mRNA production rate. (<bold>c</bold>) Parameters learned by DGA for the two responses in (<bold>b</bold>). (<bold>d</bold>) The sharpness of the response <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the energy dissipated per unit time for two responses in (<bold>b</bold>). (<bold>e</bold>) Logarithm of the loss function for the learned parameter set for Response-2, revealing directions (or curves) of insensitivity in the model’s parameter space. The red circles are the learned parameter values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-fig8-v1.tif"/></fig><p>Such promoter architectures are often studied in the context of protein gradient-based development (<xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>; <xref ref-type="bibr" rid="bib14">Estrada et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Owen and Horowitz, 2023</xref>). One well-known example of such a gradient is the dorsal protein gradient in <italic>Drosophila</italic>, which plays a crucial role in determining the spatial boundaries of gene expression domains during early embryonic development. In this context, the sharpness of the response as a function of activator concentration is a critical aspect. High sharpness ensures that the transition between different gene expression domains occurs over a very narrow region, leading to well-defined and precise boundaries. Inspired by this, our objective is to determine the parameters such that the variation in <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> as a function of the activator concentration <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> follows a desired response. We consider the two target responses (shown in <xref ref-type="fig" rid="fig8">Figure 8b</xref>) of differing sharpness, which following <xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref> we quantify as <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For simplicity, we use sixth-degree polynomials to model the input–output functions, with the <italic>x</italic>-axis plotted on a logarithmic scale. We note that our results do not depend on this choice and any other functional form works equally well.</p></sec><sec id="s2-18"><title>Loss function</title><p>In order to use the DGA to learn a desired input–output relation, we must specify a loss function that quantifies the discrepancy between the desired and actual responses of the promoter network. To construct such a loss function, we begin by discretizing the activator concentration into <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula> logarithmically spaced points, <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula>. For each <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, we denote the corresponding average mRNA production rate <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ16">Equation 15</xref>). After discretization, the loss function is simply the square error between the desired response, <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and the current response, <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, of the circuit<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the predicted average mRNA production rates obtained from the DGA simulations given the current parameters <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To compute <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for a concentration <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, we perform <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>600</mml:mn></mml:mstyle></mml:math></inline-formula> DGA simulations (indexed by capital letters <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>) using the DGA and use these simulations to calculate the fraction of time spent in transcriptionally active states (states <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:math></inline-formula> in <xref ref-type="fig" rid="fig8">Figure 8a</xref>). If we denote the fraction of time spent in state <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> in simulation <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula> by <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, then we can calculate the probability <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of being in state <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> by<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>π</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mi>A</mml:mi></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and use <xref ref-type="disp-formula" rid="equ16">Equation 15</xref> to calculate <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>As before, we optimize this loss using gradient descent (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). We assume that the transcription rate <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> is known (this just corresponds to an overall scaling of mRNA numbers). Since we are concerned only with steady-state properties, we fix the activator binding rate to a constant value, <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:mstyle></mml:math></inline-formula>. This is equivalent to measuring time in units of <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>b</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>. We then use gradient descent to optimize the remaining seven parameters governing transitions between promoter states.</p></sec><sec id="s2-19"><title>Assessing circuits found by the DGA</title><p><xref ref-type="fig" rid="fig8">Figure 8b</xref> shows a comparison between the desired and learned input–output relations. This is good agreement between the learned and desired responses, showing that the DGA is able to design dose–response curves with different sensitivities and maximal values. <xref ref-type="fig" rid="fig8">Figure 8c</xref> shows the learned parameters for both response curves. Notably, the degree of activation resulting from transcription factor binding, denoted by <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, is substantially higher for the sharper response (Response-2). In contrast, the influence on transcription factor binding due to activation, represented by <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, is reduced for the sharper response curve. Additionally, the unbinding rate <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is observed to be lower for the sharper response. However, it is essential to approach these findings with caution, as the parameters are highly interdependent. These interdependencies can be visualized by plotting the loss function around the optimized parameter values. <xref ref-type="fig" rid="fig8">Figure 8e</xref> shows two dimensional heat maps of the loss function for Response-2. There are seven free parameters, resulting in a total of 21 possible 2D slices of the loss function within the seven-dimensional loss landscape.</p><p>The most striking feature of these plots is the central role played by the parameters <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>u</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> which must both be high, suggesting that the sharpness in Response-2 may result from creating a high-flux nonequilibrium cycle through the four promoter states (see <xref ref-type="fig" rid="fig8">Figure 8a</xref>). This observation is consistent with recent works suggesting that creating such nonequilibrium kinetics represents a general design principle for engineering sharp responses (<xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>; <xref ref-type="bibr" rid="bib62">Zoller et al., 2022</xref>; <xref ref-type="bibr" rid="bib61">Wong and Gunawardena, 2020</xref>; <xref ref-type="bibr" rid="bib9">Dixit et al., 2024</xref>). To better understand if this is indeed what is happening, we quantified the energy dissipation per unit time (power consumption), <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Φ</mml:mi></mml:mstyle></mml:math></inline-formula>, in the nonequilibrium circuit. The energetic cost of operating biochemical networks can be quantified using ideas from nonequilibrium thermodynamics using a generalized Ohm’s law of the form (<xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>; <xref ref-type="bibr" rid="bib42">Qian, 2007</xref>; <xref ref-type="bibr" rid="bib29">Mehta and Schwab, 2012</xref>; <xref ref-type="bibr" rid="bib24">Lan et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Lang et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Mehta et al., 2016</xref>)<disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>=</mml:mo><mml:mi>J</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>μ</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have defined a nonequilibrium drive<disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and the nonequilibrium flux<disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the probabilities of finding the system in state 0 and 1, respectively. <xref ref-type="fig" rid="fig8">Figure 8d</xref> shows a comparison between energy consumption and sharpness of the two learned circuits. Consistent with the results of <xref ref-type="bibr" rid="bib23">Lammers et al., 2023</xref>, we find that the sharper response curve is achieved by consuming more energy.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this paper, we introduced a fully differentiable variant of the Gillespie algorithm, the DGA. By integrating differentiable components into the traditional Gillespie algorithm, the DGA facilitates the use of gradient-based optimization techniques, such as gradient descent, for parameter estimation and network design. The ability to smoothly approximate the discrete operations of the traditional Gillespie algorithm with continuous functions facilitates the computation of gradients via both forward- and reverse-mode automatic differentiation, foundational techniques in machine learning, and has the potential to significantly expand the utility of stochastic simulations. Our work demonstrates the efficacy of the DGA through various applications, including parameter learning and the design of simple gene regulatory networks.</p><p>We benchmarked the DGA’s ability to accurately replicate the results of the exact Gillespie algorithm through simulations on a two-state promoter architecture. We found the DGA could accurately approximate the moments of the steady-state distribution and other major qualitative features. Unsurprisingly, it was less accurate at capturing information about the tails of distributions. We then demonstrated that the DGA could be accurately used for parameter estimation on both simulated and real experimental data. This capability to infer kinetic parameters from noisy experimental data underscores the robustness of the DGA, making it a potentially powerful computation tool for real-world applications in quantitative biology. Furthermore, we showcased the DGA’s application in designing biological networks. Specifically, for a complex four-state promoter architecture, we learned parameters that enable the gene regulation network to produce desired input–output relationships. This demonstrates how the DGA can be used to rapidly design complex biological systems with specific behaviors. We expect computational design of synthetic circuits with differentiable simulations to become an increasingly important tool in synthetic biology.</p><p>There remains much work still to be done. In this paper, we focused almost entirely on properties of the steady states. However, a powerful aspect of the traditional Gillespie algorithm is that it can be used to simulate dynamical trajectories. How to adopt the DGA to utilize dynamical data remains an extremely important open question. In addition, it will be interesting to see if the DGA can be adapted to understand the kinetic of rare events. It will also be interesting to compare the DGA with other recently developed approximation methods such as those based on tensor networks (<xref ref-type="bibr" rid="bib53">Strand et al., 2022</xref>; <xref ref-type="bibr" rid="bib33">Nicholson and Gingrich, 2023</xref>). Beyond the gene regulatory networks, extending the DGA to handle larger and more diverse datasets will be crucial for applications in epidemiology, evolution, ecology, and neuroscience. On a technical level, this may be facilitated by developing more sophisticated smoothing functions and adaptive algorithms to improve numerical stability and convergence.</p><p>The DGA could also be extended to stochastic spatial systems by incorporating reaction–diffusion master equations or lattice-based models. Its differentiability may enable efficient optimization of spatially heterogeneous reaction parameters. However, such extensions may need to address computational scalability and stability in high-dimensional spaces, especially in processes such as diffusion-driven pattern formation or spatial gene regulation.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>A detailed explanation of how the DGA is implemented using PyTorch is given in the Appendix. In addition, all code for the DGA is available on Github at our Github repository <ext-link ext-link-type="uri" xlink:href="https://github.com/Emergent-Behaviors-in-Biology/Differentiable-Gillespie-Algorithm">https://github.com/Emergent-Behaviors-in-Biology/Differentiable-Gillespie-Algorithm</ext-link> (copy archived at <xref ref-type="bibr" rid="bib46">Rijal, 2025</xref>).</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Validation, Investigation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-103877-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code for the Differentiable Gillespie Algorithm is freely available on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/Emergent-Behaviors-in-Biology/Differentiable-Gillespie-Algorithm">https://github.com/Emergent-Behaviors-in-Biology/Differentiable-Gillespie-Algorithm</ext-link> (copy archived at <xref ref-type="bibr" rid="bib46">Rijal, 2025</xref>). The study uses previously published experimental datasets from <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref>. This dataset is publicly available through Science at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1255301">https://doi.org/10.1126/science.1255301</ext-link>. No additional proprietary or restricted datasets were used in this study.</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH NIGMS R35GM119461 to PM and Chan-Zuckerburg Institute Investigator grant to PM. The authors also acknowledge support from the Shared Computing Cluster (SCC) administered by Boston University Research Computing Services. We would also like to thank the Mehta and Kondev groups for useful discussions.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>DF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An efficient finite difference method for parameter sensitivities of continuous time markov chains</article-title><source>SIAM Journal on Numerical Analysis</source><volume>50</volume><fpage>2237</fpage><lpage>2258</lpage><pub-id pub-id-type="doi">10.1137/110849079</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arya</surname><given-names>G</given-names></name><name><surname>Schauer</surname><given-names>M</given-names></name><name><surname>Schäfer</surname><given-names>F</given-names></name><name><surname>Rackauckas</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Automatic differentiation of programs with discrete randomness</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2210.08572">https://arxiv.org/abs/2210.08572</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arya</surname><given-names>G</given-names></name><name><surname>Seyer</surname><given-names>R</given-names></name><name><surname>Schäfer</surname><given-names>F</given-names></name><name><surname>Lew</surname><given-names>A</given-names></name><name><surname>Huot</surname><given-names>M</given-names></name><name><surname>Mansinghka</surname><given-names>VK</given-names></name><name><surname>Rackauckas</surname><given-names>C</given-names></name><name><surname>Chandra</surname><given-names>K</given-names></name><name><surname>Schauer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Differentiating Metropolis-Hastings to Optimize Intractable Densities</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2306.07961">https://arxiv.org/abs/2306.07961</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benayoun</surname><given-names>M</given-names></name><name><surname>Cowan</surname><given-names>JD</given-names></name><name><surname>van Drongelen</surname><given-names>W</given-names></name><name><surname>Wallace</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Avalanches in a stochastic model of spiking neurons</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000846</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000846</pub-id><pub-id pub-id-type="pmid">20628615</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bezanson</surname><given-names>J</given-names></name><name><surname>Edelman</surname><given-names>A</given-names></name><name><surname>Karpinski</surname><given-names>S</given-names></name><name><surname>Shah</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Julia: a fresh approach to numerical computing</article-title><source>SIAM Review</source><volume>59</volume><fpage>65</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1137/141000671</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bezgin</surname><given-names>DA</given-names></name><name><surname>Buhendwa</surname><given-names>AB</given-names></name><name><surname>Adams</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>JAX-Fluids: A fully-differentiable high-order computational fluid dynamics solver for compressible two-phase flows</article-title><source>Computer Physics Communications</source><volume>282</volume><elocation-id>108527</elocation-id><pub-id pub-id-type="doi">10.1016/j.cpc.2022.108527</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Frostig</surname><given-names>R</given-names></name><name><surname>Tkačik</surname><given-names>P</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Leary</surname><given-names>C</given-names></name><name><surname>Maclaurin</surname><given-names>D</given-names></name><name><surname>Necula</surname><given-names>G</given-names></name><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Wanderman-Milne</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>JAX: composable transformations of python+numpy programs</data-title><version designator="a67ab9f">a67ab9f</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/jax-ml/jax">https://github.com/jax-ml/jax</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degrave</surname><given-names>J</given-names></name><name><surname>Hermans</surname><given-names>M</given-names></name><name><surname>Dambre</surname><given-names>J</given-names></name><name><surname>Wyffels</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A differentiable physics engine for deep learning in robotics</article-title><source>Frontiers in Neurorobotics</source><volume>13</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2019.00006</pub-id><pub-id pub-id-type="pmid">30899218</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dixit</surname><given-names>S</given-names></name><name><surname>Middelkoop</surname><given-names>TC</given-names></name><name><surname>Choubey</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Governing principles of transcriptional logic out of equilibrium</article-title><source>Biophysical Journal</source><volume>123</volume><fpage>1015</fpage><lpage>1029</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2024.03.020</pub-id><pub-id pub-id-type="pmid">38486450</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobramysl</surname><given-names>U</given-names></name><name><surname>Mobilia</surname><given-names>M</given-names></name><name><surname>Pleimling</surname><given-names>M</given-names></name><name><surname>Täuber</surname><given-names>UC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stochastic population dynamics in spatially extended predator–prey systems</article-title><source>Journal of Physics A</source><volume>51</volume><elocation-id>063001</elocation-id><pub-id pub-id-type="doi">10.1088/1751-8121/aa95c7</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Doob</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1945">1945</year><source>Transactions of the American Mathematical Society</source><publisher-name>American Mathematical Society</publisher-name><pub-id pub-id-type="doi">10.1090/S0002-9947-1945-0013857-4</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Einav</surname><given-names>T</given-names></name><name><surname>Duque</surname><given-names>J</given-names></name><name><surname>Phillips</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Theoretical analysis of inducer and operator binding for cyclic-AMP receptor protein mutants</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0204275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0204275</pub-id><pub-id pub-id-type="pmid">30256816</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elowitz</surname><given-names>MB</given-names></name><name><surname>Levine</surname><given-names>AJ</given-names></name><name><surname>Siggia</surname><given-names>ED</given-names></name><name><surname>Swain</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Stochastic gene expression in a single cell</article-title><source>Science</source><volume>297</volume><fpage>1183</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1126/science.1070919</pub-id><pub-id pub-id-type="pmid">12183631</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Estrada</surname><given-names>J</given-names></name><name><surname>Wong</surname><given-names>F</given-names></name><name><surname>DePace</surname><given-names>A</given-names></name><name><surname>Gunawardena</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Information integration and energy expenditure in gene regulation</article-title><source>Cell</source><volume>166</volume><fpage>234</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.06.012</pub-id><pub-id pub-id-type="pmid">27368104</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gardiner</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Stochastic Methods</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>A general method for numerically simulating the stochastic time evolution of coupled chemical reactions</article-title><source>Journal of Computational Physics</source><volume>22</volume><fpage>403</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/0021-9991(76)90041-3</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Exact stochastic simulation of coupled chemical reactions</article-title><source>The Journal of Physical Chemistry</source><volume>81</volume><fpage>2340</fpage><lpage>2361</lpage><pub-id pub-id-type="doi">10.1021/j100540a008</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Stochastic simulation of chemical kinetics</article-title><source>Annual Review of Physical Chemistry</source><volume>58</volume><fpage>35</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1146/annurev.physchem.58.032806.104637</pub-id><pub-id pub-id-type="pmid">17037977</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glynn</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Likelihood ratio gradient estimation for stochastic systems</article-title><source>Communications of the ACM</source><volume>33</volume><fpage>75</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1145/84537.84552</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DL</given-names></name><name><surname>Brewster</surname><given-names>RC</given-names></name><name><surname>Phillips</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Promoter architecture dictates cell-to-cell variability in gene expression</article-title><source>Science</source><volume>346</volume><fpage>1533</fpage><lpage>1536</lpage><pub-id pub-id-type="doi">10.1126/science.1255301</pub-id><pub-id pub-id-type="pmid">25525251</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komorowski</surname><given-names>M</given-names></name><name><surname>Finkenstädt</surname><given-names>B</given-names></name><name><surname>Harper</surname><given-names>CV</given-names></name><name><surname>Rand</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian inference of biochemical kinetic parameters using the linear noise approximation</article-title><source>BMC Bioinformatics</source><volume>10</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1186/1471-2105-10-343</pub-id><pub-id pub-id-type="pmid">19840370</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lammers</surname><given-names>NC</given-names></name><name><surname>Flamholz</surname><given-names>AI</given-names></name><name><surname>Garcia</surname><given-names>HG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Competing constraints shape the nonequilibrium limits of cellular decision-making</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2211203120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2211203120</pub-id><pub-id pub-id-type="pmid">36862689</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>G</given-names></name><name><surname>Sartori</surname><given-names>P</given-names></name><name><surname>Neumann</surname><given-names>S</given-names></name><name><surname>Sourjik</surname><given-names>V</given-names></name><name><surname>Tu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The energy-speed-accuracy tradeoff in sensory adaptation</article-title><source>Nature Physics</source><volume>8</volume><fpage>422</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/nphys2276</pub-id><pub-id pub-id-type="pmid">22737175</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>AH</given-names></name><name><surname>Fisher</surname><given-names>CK</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Mehta</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thermodynamics of statistical inference by cells</article-title><source>Physical Review Letters</source><volume>113</volume><elocation-id>148103</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.113.148103</pub-id><pub-id pub-id-type="pmid">25325665</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>HJ</given-names></name><name><surname>Liu</surname><given-names>JG</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Xiang</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Differentiable programming tensor networks</article-title><source>Physical Review X</source><volume>9</volume><elocation-id>031041</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.9.031041</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>HH</given-names></name><name><surname>Arkin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Stochastic mechanisms in gene expression</article-title><source>PNAS</source><volume>94</volume><fpage>814</fpage><lpage>819</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.3.814</pub-id><pub-id pub-id-type="pmid">9023339</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGill</surname><given-names>JA</given-names></name><name><surname>Ogunnaike</surname><given-names>BA</given-names></name><name><surname>Vlachos</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Efficient gradient estimation using finite differencing and likelihood ratios for kinetic Monte Carlo simulations</article-title><source>Journal of Computational Physics</source><volume>231</volume><fpage>7170</fpage><lpage>7186</lpage><pub-id pub-id-type="doi">10.1016/j.jcp.2012.06.037</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>P</given-names></name><name><surname>Schwab</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Energetic costs of cellular computation</article-title><source>PNAS</source><volume>109</volume><fpage>17978</fpage><lpage>17982</lpage><pub-id pub-id-type="doi">10.1073/pnas.1207814109</pub-id><pub-id pub-id-type="pmid">23045633</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>P</given-names></name><name><surname>Lang</surname><given-names>AH</given-names></name><name><surname>Schwab</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Landauer in the age of synthetic biology: energy consumption and information processing in biochemical networks</article-title><source>Journal of Statistical Physics</source><volume>162</volume><fpage>1153</fpage><lpage>1166</lpage><pub-id pub-id-type="doi">10.1007/s10955-015-1431-6</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>CH</given-names></name><name><surname>Day</surname><given-names>AGR</given-names></name><name><surname>Richardson</surname><given-names>C</given-names></name><name><surname>Bukov</surname><given-names>M</given-names></name><name><surname>Fisher</surname><given-names>CK</given-names></name><name><surname>Schwab</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A high-bias, low-variance introduction to Machine Learning for physicists</article-title><source>Physics Reports</source><volume>810</volume><fpage>1</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.physrep.2019.03.001</pub-id><pub-id pub-id-type="pmid">31404441</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munsky</surname><given-names>B</given-names></name><name><surname>Trinh</surname><given-names>B</given-names></name><name><surname>Khammash</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Listening to the noise: random fluctuations reveal gene network parameters</article-title><source>Molecular Systems Biology</source><volume>5</volume><elocation-id>318</elocation-id><pub-id pub-id-type="doi">10.1038/msb.2009.75</pub-id><pub-id pub-id-type="pmid">19888213</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicholson</surname><given-names>SB</given-names></name><name><surname>Gingrich</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Quantifying rare events in stochastic reaction-diffusion dynamics using tensor networks</article-title><source>Physical Review X</source><volume>13</volume><elocation-id>041006</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.13.041006</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Núñez</surname><given-names>M</given-names></name><name><surname>Vlachos</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Steady state likelihood ratio sensitivity analysis for stiff kinetic Monte Carlo simulations</article-title><source>The Journal of Chemical Physics</source><volume>142</volume><elocation-id>044108</elocation-id><pub-id pub-id-type="doi">10.1063/1.4905957</pub-id><pub-id pub-id-type="pmid">25637970</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owen</surname><given-names>JA</given-names></name><name><surname>Horowitz</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Size limits the sensitivity of kinetic schemes</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>1280</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-36705-8</pub-id><pub-id pub-id-type="pmid">36890153</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>M</given-names></name><name><surname>Kamenev</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Extinction in the Lotka-Volterra model</article-title><source>Physical Review E</source><volume>80</volume><elocation-id>021129</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.80.021129</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Advances in Neural Information Processing Systems</source><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulsson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Models of stochastic gene expression</article-title><source>Physics of Life Reviews</source><volume>2</volume><fpage>157</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.plrev.2005.03.003</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>R</given-names></name><name><surname>Kondev</surname><given-names>J</given-names></name><name><surname>Theriot</surname><given-names>J</given-names></name><name><surname>Garcia</surname><given-names>HG</given-names></name><name><surname>Orme</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Physical Biology of the Cell</source><publisher-name>Garland Science</publisher-name><pub-id pub-id-type="doi">10.1201/9781134111589</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>R</given-names></name><name><surname>Belliveau</surname><given-names>NM</given-names></name><name><surname>Chure</surname><given-names>G</given-names></name><name><surname>Garcia</surname><given-names>HG</given-names></name><name><surname>Razo-Mejia</surname><given-names>M</given-names></name><name><surname>Scholes</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Figure 1 theory meets figure 2 experiments in the study of gene expression</article-title><source>Annual Review of Biophysics</source><volume>48</volume><fpage>121</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1146/annurev-biophys-052118-115525</pub-id><pub-id pub-id-type="pmid">31084583</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pineda-Krch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>GillespieSSA: implementing the gillespie stochastic simulation algorithm in r</article-title><source>Journal of Statistical Software</source><volume>25</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.18637/jss.v025.i12</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phosphorylation energy hypothesis: open chemical systems and their biological functions</article-title><source>Annual Review of Physical Chemistry</source><volume>58</volume><fpage>113</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1146/annurev.physchem.58.032806.104550</pub-id><pub-id pub-id-type="pmid">17059360</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raj</surname><given-names>A</given-names></name><name><surname>van Oudenaarden</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Nature, nurture, or chance: stochastic gene expression and its consequences</article-title><source>Cell</source><volume>135</volume><fpage>216</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2008.09.050</pub-id><pub-id pub-id-type="pmid">18957198</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razo-Mejia</surname><given-names>M</given-names></name><name><surname>Barnes</surname><given-names>SL</given-names></name><name><surname>Belliveau</surname><given-names>NM</given-names></name><name><surname>Chure</surname><given-names>G</given-names></name><name><surname>Einav</surname><given-names>T</given-names></name><name><surname>Lewis</surname><given-names>M</given-names></name><name><surname>Phillips</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Tuning transcriptional regulation through signaling: a predictive theory of allosteric induction</article-title><source>Cell Systems</source><volume>6</volume><fpage>456</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2018.02.004</pub-id><pub-id pub-id-type="pmid">29574055</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rijal</surname><given-names>K</given-names></name><name><surname>Müller</surname><given-names>NIC</given-names></name><name><surname>Friauf</surname><given-names>E</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Prasad</surname><given-names>A</given-names></name><name><surname>Das</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Exact distribution of the quantal content in synaptic transmission</article-title><source>Physical Review Letters</source><volume>132</volume><elocation-id>228401</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.132.228401</pub-id><pub-id pub-id-type="pmid">38877921</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rijal</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Differentiable-gillespie-algorithm</data-title><version designator="swh:1:rev:c9740a695c52c1d76f260a7a46bbbcab18354380">swh:1:rev:c9740a695c52c1d76f260a7a46bbbcab18354380</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:08774b99052909603a3bab334e7933a423ae2378;origin=https://github.com/Emergent-Behaviors-in-Biology/Differentiable-Gillespie-Algorithm;visit=swh:1:snp:2e545b727f97b3056f8e3886e6e345f9eb6b6c96;anchor=swh:1:rev:c9740a695c52c1d76f260a7a46bbbcab18354380">https://archive.softwareheritage.org/swh:1:dir:08774b99052909603a3bab334e7933a423ae2378;origin=https://github.com/Emergent-Behaviors-in-Biology/Differentiable-Gillespie-Algorithm;visit=swh:1:snp:2e545b727f97b3056f8e3886e6e345f9eb6b6c96;anchor=swh:1:rev:c9740a695c52c1d76f260a7a46bbbcab18354380</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rolski</surname><given-names>T</given-names></name><name><surname>Schmidli</surname><given-names>H</given-names></name><name><surname>Schmidt</surname><given-names>V</given-names></name><name><surname>Teugels</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Stochastic Processes for Insurance and Finance</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/9780470317044</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez</surname><given-names>A</given-names></name><name><surname>Choubey</surname><given-names>S</given-names></name><name><surname>Kondev</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Regulation of noise in gene expression</article-title><source>Annual Review of Biophysics</source><volume>42</volume><fpage>469</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1146/annurev-biophys-083012-130401</pub-id><pub-id pub-id-type="pmid">23527780</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez</surname><given-names>A</given-names></name><name><surname>Golding</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Genetic determinants and cellular constraints in noisy gene expression</article-title><source>Science</source><volume>342</volume><fpage>1188</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1126/science.1242975</pub-id><pub-id pub-id-type="pmid">24311680</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoenholz</surname><given-names>S</given-names></name><name><surname>Cubuk</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>JAX MD: A framework for differentiable physics</article-title><source>Advances in Neural Information Processing Systems</source><volume>33</volume><fpage>11428</fpage><lpage>11441</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheppard</surname><given-names>PW</given-names></name><name><surname>Rathinam</surname><given-names>M</given-names></name><name><surname>Khammash</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A pathwise derivative approach to the computation of parameter sensitivities in discrete stochastic chemical systems</article-title><source>The Journal of Chemical Physics</source><volume>136</volume><elocation-id>034115</elocation-id><pub-id pub-id-type="doi">10.1063/1.3677230</pub-id><pub-id pub-id-type="pmid">22280752</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>R</given-names></name><name><surname>Anderson</surname><given-names>DF</given-names></name><name><surname>Rawlings</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Comparison of finite difference based methods to obtain sensitivities of stochastic chemical kinetic models</article-title><source>The Journal of Chemical Physics</source><volume>138</volume><elocation-id>074110</elocation-id><pub-id pub-id-type="doi">10.1063/1.4790650</pub-id><pub-id pub-id-type="pmid">23445000</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strand</surname><given-names>NE</given-names></name><name><surname>Vroylandt</surname><given-names>H</given-names></name><name><surname>Gingrich</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Computing time-periodic steady-state currents via the time evolution of tensor network states</article-title><source>The Journal of Chemical Physics</source><volume>157</volume><elocation-id>054104</elocation-id><pub-id pub-id-type="doi">10.1063/5.0099741</pub-id><pub-id pub-id-type="pmid">35933195</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thanh</surname><given-names>VH</given-names></name><name><surname>Zunino</surname><given-names>R</given-names></name><name><surname>Priami</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Efficient finite-difference method for computing sensitivities of biochemical reactions</article-title><source>Proceedings of the Royal Society A</source><volume>474</volume><elocation-id>20180303</elocation-id><pub-id pub-id-type="doi">10.1098/rspa.2018.0303</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>T</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Burrage</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Simulated maximum likelihood method for estimating kinetic rates in gene expression</article-title><source>Bioinformatics</source><volume>23</volume><fpage>84</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl552</pub-id><pub-id pub-id-type="pmid">17068087</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Kampen</surname><given-names>NG</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Stochastic Processes in Physics and Chemistry</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villaverde</surname><given-names>AF</given-names></name><name><surname>Fröhlich</surname><given-names>F</given-names></name><name><surname>Weindl</surname><given-names>D</given-names></name><name><surname>Hasenauer</surname><given-names>J</given-names></name><name><surname>Banga</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Benchmarking optimization methods for parameter estimation in large kinetic models</article-title><source>Bioinformatics</source><volume>35</volume><fpage>830</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty736</pub-id><pub-id pub-id-type="pmid">30816929</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Chu</surname><given-names>X</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>K</given-names></name><name><surname>Deng</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Lei</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine learning in materials science</article-title><source>InfoMat</source><volume>1</volume><fpage>338</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1002/inf2.12028</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Stochastic Modelling for Systems Biology</source><publisher-name>Chapman and Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/9781351000918</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>E</given-names></name><name><surname>Hajek</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Stochastic Processes in Engineering Systems</source><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4612-5060-9</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>F</given-names></name><name><surname>Gunawardena</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Gene regulation in and out of equilibrium</article-title><source>Annual Review of Biophysics</source><volume>49</volume><fpage>199</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1146/annurev-biophys-121219-081542</pub-id><pub-id pub-id-type="pmid">32375018</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoller</surname><given-names>B</given-names></name><name><surname>Gregor</surname><given-names>T</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Eukaryotic gene regulation at equilibrium, or non?</article-title><source>Current Opinion in Systems Biology</source><volume>31</volume><elocation-id>100435</elocation-id><pub-id pub-id-type="doi">10.1016/j.coisb.2022.100435</pub-id><pub-id pub-id-type="pmid">36590072</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Balancing accuracy and stability: hyperparameter tradeoffs</title><p>In this section, we explore the tradeoffs involved in tuning the hyperparameters <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula> in the DGA. These hyperparameters are crucial for balancing the accuracy and numerical stability of the DGA in approximating the exact Gillespie algorithm.</p><p>The hyperparameter <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> controls the steepness of the sigmoid function used to approximate the Heaviside step function in reaction selection. Similarly, <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> determines the sharpness of the Gaussian function used to approximate the Kronecker delta function in abundance updates. A larger value of <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> results in a steeper sigmoid or Gaussian function, thus more closely approximating the discrete functions in the exact Gillespie algorithm.</p></sec><sec sec-type="appendix" id="s9"><title>Accuracy of the forward DGA simulations</title><p>To assess the impact of these hyperparameters on the accuracy of the DGA, we measure the ratio of the Jensen–Shannon divergence between the DGA-generated PDF <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and the exact PDF <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>exact</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, normalized by the entropy <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>H</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>exact</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> of the exact steady-state PDF (see <xref ref-type="disp-formula" rid="equ10">Equation 9</xref>). This ratio, <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>JSD</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mtext>H</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, provides a measure of how closely the DGA approximates the exact Gillespie algorithm.</p><p><xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> shows the ratio <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mtext>JSD</mml:mtext><mml:mtext>H</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of the sharpness parameters <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. In panel (a), <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is fixed at 20, and <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is varied. In panel (b), <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is fixed at 200, and <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is varied. Some key insights can be drawn from these plots.</p><p>First, as <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> increases, the ratio <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mtext>JSD</mml:mtext><mml:mtext>H</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> decreases, indicating that the DGA’s approximation becomes more accurate. This is because steeper sigmoid and Gaussian functions better mimics the discrete steps of the exact Gillespie algorithm. Interestingly, while the ratio decreases for both parameters, <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> plateaus at high values, whereas <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> rises at high values. This plateau occurs because the sigmoid function used for reaction selection becomes so steep that it effectively becomes a step function, beyond which further steepening has negligible impact. As <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> becomes very large, the width of the Gaussian function used for abundance updates becomes extremely narrow. In such a scenario, it becomes increasingly improbable for the chosen reaction index to fall within this narrow width, especially because the reaction indices are not exact integers but are instead near-integer continuous values. Therefore, as <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> becomes very large, the discrepancy between the DGA-generated probabilities and the exact probabilities widens, causing the ratio <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mtext>JSD</mml:mtext><mml:mtext>H</mml:mtext></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> to increase.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>In panels (<bold>a</bold>) and (<bold>b</bold>), we plot the ratio of the Jensen–Shannon divergence <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>JSD</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> between the differentiable Gillespie PDF <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>DGA</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and the exact steady-state PDF <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the Shannon entropy <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>H</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mtext>exact</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ss</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> of the exact steady-state PDF, as a function of the two sharpness parameters <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula>.</title><p>In panel (<bold>a</bold>), <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula>; in panel (<bold>b</bold>), <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mstyle></mml:math></inline-formula>. The simulation time is set to 10. In panels (<bold>c</bold>) and (<bold>d</bold>), for these same values, we show the gradient <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the loss function <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> with respect to the parameter <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> near the true parameter values. In all the plots, the values of the rates are: <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. 5000 trajectories are used to obtain the PDFs, while 200 trajectories are used to obtain the gradients.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-app1-fig1-v1.tif"/></fig></sec><sec sec-type="appendix" id="s10"><title>Stability of the backpropagation of DGA simulations</title><p>Numerical stability for gradient computation is crucial. Therefore, it is important to examine how the gradient behaves as a function of the hyperparameters. Panels (c) and (d) of <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> provide insights into this behavior. The plots show the gradient <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">∇</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the loss function <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> with respect to the parameter <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> near the true parameter values. The gradients are computed for the loss function in <xref ref-type="disp-formula" rid="equ12">Equation 11</xref> with <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> equal to 8 and 2.5, respectively, for the parameter values <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>. With these parameter values, the true mean and standard deviation are equal to 6.67 and 3.94, respectively.</p><p>As <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> increases, the gradients become more accurate, but their numerical stability can be compromised. This is evidenced by the increased variability and erratic behavior in the gradients at very high sharpness values (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Hence, very large values of <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> lead to oscillations and convergence issues, highlighting the need for a balance between accuracy and stability.</p><p>The tradeoff between accuracy and numerical stability is evident. This necessitates careful tuning of <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi></mml:mstyle></mml:math></inline-formula> to ensure stable and efficient optimization. In practice, this involves selecting values that provide sufficient approximation quality without compromising the stability of the gradients.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s11"><title>Implementation of the DGA in PyTorch</title><p>This section explains the implementation of the DGA used to simulate the two-state promoter model. The implementation can be adapted to any model where the stoichiometric matrix and propensities are known. The model involves promoter state switching and mRNA production/degradation (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), and the algorithm is designed to handle these sub-processes effectively.</p></sec><sec sec-type="appendix" id="s12"><title>Stoichiometric matrix:</title><p>The stoichiometric matrix is a key component in modeling state changes due to reactions. Each row represents a reaction, and each column corresponds to a state variable (promoter state and mRNA level). The matrix for the two-state promoter model includes:</p><list list-type="bullet"><list-item><p>Reaction 1: Promoter state transitions from the OFF state (–1) to the ON state (+1).</p></list-item><list-item><p>Reaction 2: Production of mRNA.</p></list-item><list-item><p>Reaction 3: Promoter state transitions from the ON state (+1) to the OFF state (–1).</p></list-item><list-item><p>Reaction 4: Degradation of mRNA.</p></list-item></list><p>The stoichiometric matrix <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi></mml:mstyle></mml:math></inline-formula> for this model is:<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this matrix, the rows correspond to the reactions listed above, and the columns represent the promoter state and mRNA level, respectively.</p></sec><sec sec-type="appendix" id="s13"><title>Propensity calculations:</title><p>The levels vector is a two-dimensional vector where the first element represents the promoter state (–1 or +1) and the second element represents the mRNA number <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi></mml:mstyle></mml:math></inline-formula>. Propensities are the rates at which reactions occur, given the current state of the system. In our PyTorch implementation, the propensities are calculated using the following expressions:</p><p><code xml:space="preserve">propensities = torch.stack([
   kon * torch.sigmoid(-c * levels[0]),
   # Promoter state switching from –1 to +1
   r * torch.sigmoid(-c * levels[0]),
   # mRNA production
   torch.sigmoid(c * levels[0]),
   # Promoter state switching from +1 to –1
   g * levels [1]
   # mRNA degradation
])</code></p><p>Each propensity corresponds to a different reaction:</p><list list-type="bullet"><list-item><p>Promoter state switching from –1 to +1: The value of <monospace>kon * torch.sigmoid(-c * levels[0])</monospace> is around <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> when the <monospace>levels[0]</monospace> (promoter state) is around –1 and close to zero when <monospace>levels[0]</monospace> is around +1. The sigmoid function ensures a smooth transition, allowing differentiability and preventing abrupt changes in rates. The constant <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> controls the sharpness of the sigmoid function.</p></list-item><list-item><p>mRNA production: The rate is proportional to <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and modulated by the same sigmoid function, <monospace>torch.sigmoid(-c * levels[0])</monospace>, such that the rate is equal to <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> only when the promoter is in –1 state.</p></list-item><list-item><p>Promoter state switching from +1 to –1: The rate is set to 1 and modulated by the sigmoid function, <monospace>torch.sigmoid(c * levels[0])</monospace>, such that the rate is equal to 1 only when the promoter is in +1 state.</p></list-item><list-item><p>mRNA degradation: The rate is proportional to the current mRNA level, <monospace>g * levels [1]</monospace>, reflecting the natural decay of mRNA with rate <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>m</mml:mi><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></list-item></list><p>Using the sigmoid function in the propensity calculations is crucial for ensuring smooth and differentiable transitions between states. This smoothness is essential for gradient-based optimization methods, which rely on continuous and differentiable functions to compute gradients effectively. Without the sigmoid function, the propensity rates could change abruptly, leading to numerical instability and difficulties in optimizing the model parameters.</p><p>Reaction selection function: We define a function <monospace>reaction_selection</monospace> that selects the next reaction to occur based on the transition points and a random number between [0,1]. The function basically implements using <xref ref-type="disp-formula" rid="equ7">Equation 6</xref>. The transition points are first calculated from the cumulative sum of reaction rates normalized to the total rate.</p><p>Gillespie simulation function: The main <monospace>gillespie_simulation</monospace> function uses the previously described functions, each with specific roles, to perform the actual simulation step-by-step, as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. This function iterates through the number of simulations, updating the system’s state and the propensities of each reaction at each step.</p><p>State jump function: We define another function <monospace>state_jump</monospace> that calculates the state change vector when a reaction occurs. It uses a Gaussian function to smoothly transition between states based on the selected reaction index and the stoichiometry matrix (see <xref ref-type="disp-formula" rid="equ9">Equation 8</xref>).</p></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s14"><title>Estimating confidence intervals for parameters</title><p>In this section, we describe the methodology used to estimate the confidence intervals for the parameters using polynomial fitting and numerical techniques. This approach uses the results of multiple simulations to determine the range within which the parameter <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is likely to lie, based on the curvature of the loss function around its minimum value. Specifically, we perform the following steps:</p><list list-type="order"><list-item><p>Parameter initialization: Set up and initialize the necessary parameters. This includes defining the number of evaluation points, the number of simulations, the simulation time, and the hyperparameters <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>.</p></list-item><list-item><p>Range generation: For each set of learned parameters <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, generate a range of values for the parameter of interest <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> while keeping the other parameters fixed at their learned values. Let the learned value of the parameter <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> be <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Then the range of evaluation is approximately <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">[</mml:mo><mml:mn>0.2</mml:mn><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, depending on the flatness of the loss landscape around its minimum.</p></list-item><list-item><p>Simulation and loss calculation: For each value in this range, perform forward DGA simulations to calculate the mean and standard deviation of the results, using which the loss function is computed and stored.</p></list-item><list-item><p>Polynomial fitting: Fit a polynomial of degree 6 to the computed loss values across the range of <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Compute the first and second derivatives of the fitted polynomial to identify the minimum and evaluate the curvature.</p></list-item><list-item><p>Identifying minimum: Identify the valid minimum <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> of the loss function by solving for the roots of the first derivative and filtering out the points where the second derivative is positive.</p></list-item><list-item><p>Curvature and standard deviation calculation: Calculate the curvature of the loss landscape at its minimum using its second derivative at the minimum. An estimation of the standard deviation <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the parameter <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is given by:</p></list-item></list><p><disp-formula id="equ23"><label>(C1)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>7. Confidence interval calculation: The loss function is typically asymmetric around its minimum, with the left side usually steeper than the right (see <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>). To determine the right error bar, we use <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the left error bar, we find the point where <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>). Therefore, the balanced 95% CI for the parameter <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by:<disp-formula id="equ24"><label>(C2)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This methodology allows for a robust estimation of the confidence intervals, providing insights into the reliability and precision of the parameter estimates.</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Error bars estimation for asymmetric loss function.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-103877-app3-fig1-v1.tif"/></fig></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s15"><title>Demonstrating parameter degeneracy in the two-state promoter model</title><p>We will now demonstrate the existence of degeneracy in the two-state promoter architecture. Setting <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> in the analytical expressions for the mean <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and the Fano factor <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> from <xref ref-type="bibr" rid="bib20">Jones et al., 2014</xref>, we have:<disp-formula id="equ25"><label>(D1)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mi>r</mml:mi><mml:mi>γ</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mi>r</mml:mi><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We want to solve <xref ref-type="disp-formula" rid="equ25">Equation D1</xref> for <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. By isolating <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> in the expression for <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula>, we obtain:<disp-formula id="equ26"><label>(D2)</label><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>⋅</mml:mo><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Next, we substitute the expression for <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ26">Equation D2</xref> into the expression for the <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ25">Equation D1</xref>:<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>⋅</mml:mo><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>⋅</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">⇒</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>⋅</mml:mo><mml:mi>γ</mml:mi></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Expanding and isolating terms involving <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>⋅</mml:mo><mml:mi>γ</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Rearranging to solve for <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, we obtain:<disp-formula id="equ30"><label>(D3)</label><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We substitute back <xref ref-type="disp-formula" rid="equ1">Equation 28</xref> in <xref ref-type="disp-formula" rid="equ1">Equation 23</xref> to obtain <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ31"><label>(D4)</label><mml:math id="m31"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ30">Equations D3 and D4</xref> indicate that for any given <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, there exists a corresponding pair of values for <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> that satisfy the equations. Therefore, the solutions are not unique, demonstrating a high degree of degeneracy in the parameter space. This degeneracy arises because multiple combinations of <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> can produce the same observable <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula>. The lack of unique solutions is a common issue in parameter estimation for complex systems, where different parameter sets can lead to similar system behaviors.</p></sec><sec sec-type="appendix" id="s16"><title>Resolving degeneracy with known <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula></title><p>To resolve the degeneracy, we can fix the value of <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. Now, if the rate <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> is known, we can solve <xref ref-type="disp-formula" rid="equ25">Equation D1</xref> for the other parameters. Starting by rearranging the mean expression from <xref ref-type="disp-formula" rid="equ25">Equation D1</xref>:<disp-formula id="equ32"><label>(D5)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>r</mml:mi><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Substituting <xref ref-type="disp-formula" rid="equ32">Equation D5</xref> in the expression of <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ25">Equation D1</xref>, we have<disp-formula id="equ33"><label>(D6)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">⇒</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">⇒</mml:mo></mml:mtd><mml:mtd><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">⇒</mml:mo></mml:mtd><mml:mtd><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Substituting <xref ref-type="disp-formula" rid="equ33">Equation D6</xref> into <xref ref-type="disp-formula" rid="equ32">Equation D5</xref>, we obtain:<disp-formula id="equ34"><label>(D7)</label><mml:math id="m34"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ33">Equations D6 and D7</xref> provide unique solutions for <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> given a known value of <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>. By fixing <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>, the degeneracy is resolved, and the remaining parameters can be accurately determined.</p></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s17"><title>Estimating confidence intervals for <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula></title><p>To assess the reliability of the statistics predicted through DGA-based optimization, we calculate error bars using the following procedure. For the non-degenerate situation, we use the learned parameter values <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, and perform forward DGA simulations with many different random seeds. This generates multiple samples of the mean mRNA level <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mstyle></mml:math></inline-formula> and the standard deviation <inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. These samples provide us with the variability due to the stochastic nature of the simulations. The 95% CIs are then determined using their standard deviations, denoted as <inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. For the mean mRNA level, the CI is calculated as:<disp-formula id="equ35"><label>(E1)</label><mml:math id="m35"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>CI</mml:mtext><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mn>1.96</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For the standard deviation of mRNA levels, the CI is calculated as:<disp-formula id="equ36"><label>(E2)</label><mml:math id="m36"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>CI</mml:mtext><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1.96</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1.96</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s18"><title>DGA-based optimization for experimental data and estimation of errors</title><sec sec-type="appendix" id="s18-1"><title>Optimization procedure</title><p>Given a set of measurements of the mean mRNA expression levels (<inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) and the Fano factor (<inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>) for promoters (<italic>lac</italic>UD5 and 5DL1), we construct a loss function as follows:<disp-formula id="equ37"><label>(F1)</label><mml:math id="m37"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msqrt><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:msqrt><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> runs over data points (each with a different lac repressor concentration), and <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are the mean and standard deviation obtained from a sample of DGA simulations. This loss function is chosen because, at its minimum, <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:msubsup><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><p>For the optimization, we set <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> and focus on estimating the parameters <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. During the gradient-based optimization, the transcription rate <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and the mRNA degradation rate <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> are assumed to be the same for all data points <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, while allowing <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> to vary across data points <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. This reflects the fact that <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> is a function of the lac repressor concentration, which is varied across data points.</p><p>Instead of <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>, we actually learn a transformed parameter <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, which is the probability for the promoter to be in the OFF state. This approach is based on our observation that the gradient of the loss function with respect to <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is more numerically stable compared to the gradient with respect to <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula>.</p><p>The parameters are initialized randomly as follows:</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> is initialized to a random number in [0, 100].</p></list-item><list-item><p><inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> is initialized to a random number between [0, 10].</p></list-item><list-item><p>The <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>off</mml:mtext></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> values, which depend on the index <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> of the data points, are initially set as linearly spaced points within the range [0.03, 0.97].</p></list-item></list><p>The hyperparameters used for the simulations are as follows:</p><list list-type="bullet"><list-item><p>Number of simulations: 200</p></list-item><list-item><p>Simulation time: <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0.2</mml:mn></mml:mstyle></mml:math></inline-formula></p></list-item><list-item><p>Steepness of the sigmoid function: <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>200.0</mml:mn></mml:mstyle></mml:math></inline-formula></p></list-item><list-item><p>Sharpness of the Gaussian function: <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>20.0</mml:mn></mml:mstyle></mml:math></inline-formula></p></list-item><list-item><p>Steepness of the sigmoid in propensities: <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>20.0</mml:mn></mml:mstyle></mml:math></inline-formula></p></list-item></list><p>The parameters are iteratively updated to minimize the loss function. During each iteration of the optimization, the following steps are performed:</p><list list-type="order"><list-item><p>Forward simulation: The DGA is used to simulate the system, generating predictions for the mean mRNA levels and their standard deviations.</p></list-item><list-item><p>Loss calculation: The loss function is computed based on the differences between the simulated and experimentally measured values of the mean mRNA levels and standard deviations (see <xref ref-type="disp-formula" rid="equ37">Equation F1</xref>).</p></list-item><list-item><p>Gradient calculation: The gradients of the loss function with respect to the parameters <inline-formula><mml:math id="inf392"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf393"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf394"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>k</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext>R</mml:mtext></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are calculated using backpropagation.</p></list-item><list-item><p>Parameter update: The ADAM optimizer updates the parameters in the direction that reduces the loss function. ADAM adjusts the learning rates based on the history of gradients and their moments. The learning rate used is 0.1.</p></list-item></list><p>The values of the parameters and the loss value are saved after each iteration. The parameter values corresponding to the minimum loss after convergence are picked at the end.</p></sec></sec><sec sec-type="appendix" id="s19"><title>Goodness of fit</title><p>To quantitatively assess the goodness of the fit, we define the mean percentage error (MPE) as follows:<disp-formula id="equ38"><label>(F2)</label><mml:math id="m38"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>MPE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf395"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is the predicted Fano factor for the <italic>i</italic>th data point. This metric provides a measure of the average discrepancy between the predicted and experimental Fano factors, expressed as a percentage of the experimental values.</p></sec><sec sec-type="appendix" id="s20"><title>Error bars</title><p>The error bars in the ratio <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> are obtained by applying error propagation to the standard deviations <inline-formula><mml:math id="inf397"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf398"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>γ</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> of the individual values <inline-formula><mml:math id="inf399"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>. The propagated error is given by:<disp-formula id="equ39"><label>(F3)</label><mml:math id="m39"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mfrac><mml:mi>r</mml:mi><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>δ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf402"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>γ</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the standard deviations of <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. These standard deviations are obtained using the curvature of the loss function, as discussed earlier.</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103877.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bitbol</surname><given-names>Anne-Florence</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Ecole Polytechnique Federale de Lausanne (EPFL)</institution><country>Switzerland</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study introduces a fully differentiable variant of the Gillespie algorithm as an approximate stochastic simulation scheme for complex chemical reaction networks, allowing kinetic parameters to be inferred from empirical measurements of network outputs using gradient descent. The concept and algorithm design are <bold>convincing</bold> and innovative. While the proofs of concept are promising, some questions are left open about implications for more complex systems that cannot be addressed by existing methods. This work has the potential to be of significant interest to a broad audience of quantitative and synthetic biologists.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103877.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This work introduces the differentiable Gillespie algorithm, DGA, which is a differentiable variant of the celebrated (and exact) Gillespie algorithm commonly used to perform stochastic simulations across numerous fields, notably in the life sciences. The proposed DGA approximates the exact Gillespie algorithm using smooth functions yielding a suitable approximate differentiable stochastic system as a proxy for the underlying discrete stochastic system, where DGA stochastic reactions have continuous reaction index and the species abundances. To illustrate their methodology, the authors specifically consider in detail the case of a well-studied two-state promoter gene regulation system that they analyze using a machine learning approach, and by combining simulation data with analytical results. For the two-state promoter gene system, the DGA is benchmarked by accurately reproducing the results of the exact Gillespie algorithm. For this same simple system, the authors also show how the DGA can be used for estimating kinetic parameters of both simulated and real noisy experimental data. This lets them argue convincingly that the DGA can become a powerful computation tool for applications in quantitative and synthetic biology. In order to argue that the DGA can be employed to design circuits with ad-hoc input-output relations, these considerations are then extended to a more complex four-state promoter model of gene regulation. The main strength of the paper is its clarity and its pedagogical presentation of the simulation methods.</p><p>Strengths:</p><p>The main strength of the paper is its clarity and its pedagogical presentation of the simulation methods.</p><p>Weaknesses:</p><p>It would have been useful to have a brief discussion, based on a concrete example, of what can be achieved with the DGA and is totally beyond the reach of the Gillespie algorithm and the numerous existing stochastic simulation methods. A more comprehensive and quantitative analysis of the limitations of the DGA, e.g. for rare events, and how it might be used for stochastic spatial systems would have also been helpful. However, this is arguably beyond the scope of this study whose primary goal is to introduce the DGA and demonstrate that it can achieve tasks like parameter estimation and network design.</p><p>Comments on revisions:</p><p>The authors have made a sound effort to address many of the comments raised in the previous reports. This has helped improve the clarity of the discussion.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103877.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this work, the authors present a differentiable version of the widely-used Gillespie Algorithm. The Gillespie Algorithm has been used for decades to simulate the behavior of stochastic biochemical reaction networks. But while the Gillespie Algorithm is a powerful tool for the forward simulation of biochemical systems given some set of known reaction parameters, it cannot be used for reverse process, i.e. inferring reaction parameters given a set of measured system characteristics. The Differentiable Gillespie Algorithm (&quot;DGA&quot;) overcomes this limitation by approximating two discontinuous steps in the Gillespie Algorithm with continuous functions. This makes it possible to calculate of gradients for each step in the simulation process which, in turn, allows the reaction parameters to be optimized via powerful backpropagation techniques. In addition to describing the theoretical underpinnings of DGA, the authors demonstrate different potential use-cases for the algorithm in the context of simple models of stochastic gene expression.</p><p>Overall, the DGA represents an important conceptual step forward for the field and should lay the groundwork for exciting innovations in the analysis and design of stochastic reaction networks. At the same time, significantly more work is needed to establish when the approximations made by DGA are valid and to demonstrate the viability of the algorithm in the context of complicated reaction networks.</p><p>Strengths:</p><p>This work makes an important conceptual leap by introducing a version of the Gillespie Algorithm that is end-to-end differentiable. This idea alone has the potential to drive a number of exciting innovations in the analysis, inference, and design of biochemical reaction networks. Beyond the theoretical adjustments, the authors also implement their algorithm in a Python-based codebase that combines DGA powerful optimization libraries like PyTorch. This codebase has the potential to be of interest to a wide range of researchers, even if the true scope of the method's applicability remains to be fully determined.</p><p>The authors also demonstrate how DGA can be used in practice both to infer reaction parameters from real experimental data (Figure 7) and to design networks with user-specified input-output characteristics (Figure 8). These illustrations should provide a nice roadmap for researchers interested in applying DGA to their own projects/systems.</p><p>Finally, although it does not stem directly from DGA, the exploration of pairwise parameter dependencies in different network architectures provides an interesting window into the design constraints (or lack thereof) that shape the architecture of biochemical reaction networks.</p><p>Weaknesses:</p><p>While it is clear that the DGA represents an important conceptual advancement, the authors do not do enough in the present manuscript to (i) validate the robustness of DGA inference and (ii) demonstrate that DGA inference works in the kinds of complex biochemical networks where it would actually be of legitimate use.</p><p>It is to the authors' credit that they are open and explicit about the potential limitations of DGA due to breakdowns in its continuous approximations. However they do not provide the reader with nearly enough empirical (i.e. simulation-based) or theoretical context to assess when, why, and to what extent DGA will fail in different situations. In Figure 2, they compare DGA to GA (i.e. ground-truth) in the context of a simple two state model of a stochastic transcription. Even in this minimal system, we see that DGA deviates notably from ground-truth both in the simulated mRNA distributions (Figure 2A) and in the ON/OFF state occupancy (Figure 2C). This begs the question of how DGA will scale to more complicated systems, or systems with non-steady state dynamics. Will the deviations become more severe? This is important because, in practice, there is really not much need for using DGA with a simple 2 state system-we have analytic solutions for this case. It is the more complex systems where DGA has the potential to move the needle.</p><p>A second concern is that the authors' present approach for parameter inference and error calculation does not seem to be reliable. For example, in Figure 5A, they show DGA inference results for the ON rate of a two-state system. We see substantial inference errors in this case, even though the inference problem should be non-degenerate in this case. One reason for this seems to be that the inference algorithm does not reliably find the global minimum of the loss function (Figure 2B). To turn DGA into a viable approach, it is paramount that the authors find some way to improve this behavior, perhaps by using multiple random initializations to better search the loss space.</p><p>Finally, the authors do a good job of illustrating how DGA might be used to infer biological parameters (Figure 7) and design reaction networks with desired input-output characteristics (Figure 8). However, analytic solutions exist for both of the systems they select for examples. This means that, in practice, there would be no need for DGA in these contexts, since one could directly optimize, e.g., the expressions for the mean and Fano Factor of the system in Figure 7A. I still believe that it is useful to have these examples, but it seems critical to add a use-case where DGA is the only option.</p><p>Comments on revisions:</p><p>I am concerned that the results in Figure 8D may not be correct, or that the authors may be mis-interpreting them. From my reading of the paper they cite (Lammers &amp; Flamholz 2023), the equilibrium sharpness limit for the network they consider in Figure 8 should be 0.25. But both solutions shown in Figure 8D fall below this limit, which means that they have sharpness levels that could have been achieved with no energy expenditure. If this is the case, then it would imply that while both systems do dissipate energy, they are not doing so productively; meaning that the same results could be achieved while holding Phi=0.</p><p>I acknowledge that this could be due to a difference in how they measure sharpness, but wanted to raise it here in case it is, in fact, a genuine issue with the analysis.</p><p>There should be an easy fix for this: just set the sharper &quot;desired response&quot; curve in 8b to be such that it demands non-equilibrium sharpness levels (0.25)</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103877.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This manuscript introduces a differentiable variant of the Gillespie algorithm (DGA) that allows gradient calculation using backpropagation. The most significant contribution of this work is the development of the DGA itself, a novel approach to making stochastic simulations differentiable. This is achieved by replacing discontinuous operations in the traditional Gillespie algorithm with smooth, differentiable approximations using sigmoid and Gaussian functions. This conceptual advance opens up new avenues for applying powerful gradient-based optimization techniques, prevalent in machine learning, to studying stochastic biological systems.</p><p>The method was tested on a simple two-state promoter model of gene expression. The authors found that the DGA accurately captured the moments of the steady-state distribution and other major qualitative features. However, it was less accurate at capturing information about the distribution's tails, potentially because rare events result from frequent low-probability reaction events where the approximations made by the DGA have a greater impact. The authors also used the DGA to design a four-state promoter model of gene regulation that exhibited a desired input-output relationship. The DGA could learn parameters that produced a sharper response curve, which was achieved by consuming more energy.</p><p>The authors conclude that the DGA is a powerful tool for analyzing and designing stochastic systems. The discussion lays several open questions in the field and constructively addresses shortcomings of the proposed method as well as potential ways forward.</p><p>Strengths:</p><p>The DGA allows gradient-based optimization techniques to estimate parameters and design networks with desired properties.</p><p>The DGA efficacy in estimating kinetic parameters from both synthetic and experimental data. This capability highlights the DGA's potential to extract meaningful biophysical parameters from noisy biological data.</p><p>The DGA's ability to design a four-state promoter architecture exhibits a desired input-output relationship. This success indicates the potential of the DGA as a valuable tool for synthetic biology, enabling researchers to engineer biological circuits with predefined behaviours.</p><p>Weaknesses:</p><p>The study primarily focuses on analysing the steady-state properties of stochastic systems.</p><p>Comments on revisions:</p><p>Thank you for addressing all the points raised. I am looking forward to seeing the next steps in DGAs development and performance!</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.103877.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rijal</surname><given-names>Krishna</given-names></name><role specific-use="author">Author</role><aff><institution>Boston University</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Mehta</surname><given-names>Pankaj</given-names></name><role specific-use="author">Author</role><aff><institution>Boston University</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the current reviews.</p><disp-quote content-type="editor-comment"><p><bold>Response to Reviewer 2’s comments:</bold></p><p>I am concerned that the results in Figure 8D may not be correct, or that the authors may be mis-interpreting them. From my reading of the paper they cite (Lammers &amp; Flamholz 2023), the equilibrium sharpness limit for the network they consider in Figure 8 should be 0.25. But both solutions shown in Figure 8D fall below this limit, which means that they have sharpness levels that could have been achieved with no energy expenditure. If this is the case, then it would imply that while both systems do dissipate energy, they are not doing so productively; meaning that the same results could be achieved while holding Phi=0.</p><p>I acknowledge that this could be due to a difference in how they measure sharpness, but wanted to raise it here in case it is, in fact, a genuine issue with the analysis.There should be an easy fix for this: just set the sharper &quot;desired response&quot; curve in 8b to be such that it demands non-equilibrium sharpness levels (0.25&lt;S&lt;0.5).</p></disp-quote><p>Thank you for raising this point regarding the interpretation of our results in Figure 8D. We agree that if the equilibrium sharpness limit for this particular network is around 0.25 (as shown by Lammers &amp; Flamholz 2023), then achieving a sharpness below this threshold could, in principle, be accomplished without any energy expenditure. However, in our current design approach, the loss function is solely designed to enforce agreement with a target mean mRNA level at different input concentrations; it does not explicitly constrain energy dissipation, noise, or other metrics. Consequently, the DGA has no built-in incentive to minimize or optimize energy consumption, which means the resulting solutions may dissipate energy without exceeding the equilibrium sharpness limit.</p><p>In other words, the same input–output relationship could theoretically be achieved with \Phi = 0 if an explicit constraint or regularization term penalizing energy usage had been included. As noted, adding such a term (e.g., penalizing \Phi^2) is conceptually straightforward but falls outside the scope of this study. Our primary goal is to demonstrate the flexibility of the DGA in designing a desired response, rather than to delve into energy–sharpness trade-offs or other biological considerations</p><p>While we appreciate the suggestion to set a higher target sharpness that exceeds the equilibrium limit, we believe the current example effectively demonstrates the DGA’s ability to design circuits with desired input-output relationships, which is the primary focus of this study. Researchers interested in optimizing energy efficiency, burst size, burst frequency, noise, response time, mutual information, or other system properties can easily extend our approach by incorporating additional terms into the loss function to target these specific objectives.</p><p>We hope this explanation addresses your concern and clarifies that the manuscript provides sufficient context for readers to interpret the results in Figure 8D correctly.</p><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p></disp-quote><p>We thank Reviewer #1 for their thoughtful feedback and appreciation of the manuscript's clarity. Our primary goal is to introduce the DGA as a foundational tool for integrating stochastic simulations with gradient-based optimization. While we recognize the value of providing detailed comparisons with existing methods and a deeper analysis of the DGA’s limitations (such as rare event handling), these topics are beyond the scope of this initial work. Our focus is on presenting the core concept and demonstrating its potential, leaving more extensive evaluations for future research.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p></disp-quote><p>We thank Reviewer #2 for their detailed and constructive feedback. We appreciate the recognition of the DGA as a significant conceptual advancement for stochastic biochemical network analysis and design.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) Validation of DGA robustness in complex systems:</p></disp-quote><p>Our primary goal is to introduce the DGA framework and demonstrate its feasibility. While validation on high-dimensional and non-steady-state systems is important, it is beyond the scope of this initial work. Future studies may improve scalability by employing techniques such as dynamically adjusting the smoothness of the DGA's approximations during simulation or using surrogate models that remain differentiable but more accurately capture discrete behaviors in critical regions, thus preserving gradient computation while improving accuracy.</p><disp-quote content-type="editor-comment"><p>(2) Inference accuracy and optimization:</p></disp-quote><p>We acknowledge that the non-convex loss landscape in the DGA can hinder parameter inference and convergence to global minima, as seen in Figure 5A. While techniques like multi-start optimization or second-order methods (e.g., L-BFGS) could improve performance, our focus here is on establishing the DGA framework. We plan to explore better optimization methods in future work to improve the accuracy of parameter inference in complex systems.</p><disp-quote content-type="editor-comment"><p>(3) Use of simple models for demonstration:</p></disp-quote><p>We selected well-understood systems to clearly illustrate the capabilities of the DGA. These examples were intended to demonstrate how the DGA can be applied, rather than to solve problems better addressed by analytical methods. Applying DGA to more complex, analytically intractable systems is an exciting avenue for future work, but introducing the method was our main objective in this study.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p></disp-quote><p>We thank the reviewer for their detailed and insightful feedback. We appreciate the recognition of the DGA as a significant advancement for enabling gradient-based optimization in stochastic systems.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) Application beyond steady-state analysis</p></disp-quote><p>We acknowledge the limitation of focusing solely on steady-state properties. To extend the DGA for analyzing transient dynamics, time-dependent loss functions can be incorporated to capture system evolution over time. This could involve aligning simulated trajectories with experimental time-series data or using moment-matching across multiple time points.</p><disp-quote content-type="editor-comment"><p>(2) Numerical instability in gradient computation</p></disp-quote><p>The reviewer correctly highlights that large sharpness parameters (a and b) in the sigmoid and Gaussian approximations can induce numerical instability due to vanishing or exploding gradients. To address this, adaptive tuning of a and b during optimization could balance smoothness and accuracy. Additionally, alternative smoothing functions (e.g., softmax-based reaction selection) and gradient regularization techniques (such as gradient clipping and trust-region methods) could improve stability and convergence.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (recommendations):</bold></p></disp-quote><p>We thank the reviewer for their thoughtful and constructive feedback on our manuscript. Below, we address each of the comments and suggestions raised.</p><disp-quote content-type="editor-comment"><p>Main points:</p><p>(1) It would have been useful to have a brief discussion, based on a concrete example, of what can be achieved with the DGA and is totally beyond the reach of the Gillespie algorithm and the numerous existing stochastic simulation methods.</p></disp-quote><p>Thank you for your comment. We would like to clarify that the primary aim of this work is to introduce the DGA and demonstrate its feasibility for tasks such as parameter estimation and network design. Unlike traditional stochastic simulation methods, the DGA’s differentiable nature enables gradient-based optimization, which is not possible with the classical Gillespie algorithm or its variants.</p><disp-quote content-type="editor-comment"><p>(2) As often with machine learning techniques, there is a sense of black box, with a lack of mathematical details of the proposed method: as opposite to the exact Gillespie algorithm, whose foundations lie on solid mathematical results (exponentially-distributed waiting times of continuous-time Markov processes), the DGA involves uncontrolled approximations, that are only briefly mentioned in the paper. For instance, it is currently simply noted that &quot;the approximations introduced by the DGA may be pronounced in more complex settings such as the calculation of rare events&quot;, without specifying how limiting these errors are. It would be useful to include a clearer and more comprehensive discussion of the limitations of the DGA: When does it work accurately? What are the approximations/errors and can they be controlled? When is it worth paying the price for those approximations/errors, and when is it better to stick to the Gillespie algorithm? Is this notably the case for problems involving rare events? Clearly, these are difficult questions, and the answers are problem specific. However, it would be important to draw the readers' attention on the issues, especially if the DGA is presented as a potentially significant tool in computational and synthetic biology.</p></disp-quote><p>We acknowledge the importance of discussing the limitations of the DGA in more detail. While we have noted that the approximations introduced by the DGA may impact its accuracy in certain scenarios, such as rare-event problems, a deeper exploration of these trade-offs is outside the scope of this work. Instead, we provide sufficient context in the manuscript to guide readers on when the DGA is appropriate.</p><disp-quote content-type="editor-comment"><p>(3) The DGA is here introduced and discussed in the context of non-spatial problems (simple gene regulatory networks). However, numerous problems in the life sciences and computational/synthetic biology, involve stochasticity and spatial degrees of freedom (e.g. for problems involving diffusion, migration, etc). It is notoriously challenging to use the Gillespie algorithm to efficiently simulate stochastic spatial systems, especially in the context of rare events (e.g., extinction or fixation problems). It would be useful to comment on whether, and possibly how, the DGA can be used to efficiently simulate stochastic spatial systems, and if it would be better suited than the Gillespie algorithm for this purpose.</p></disp-quote><p>Thank you for pointing this out. Although our current work centers on non-spatial systems, we agree that many biological contexts incorporate both stochasticity and spatial degrees of freedom. Extending the DGA to efficiently simulate such systems would indeed require substantial modifications—for instance, coupling it with reaction-diffusion frameworks or spatial master equations. We believe this is an exciting direction for future research and mention it briefly in the discussion as a potential extension.</p><disp-quote content-type="editor-comment"><p>Minor suggestions:</p><p>(1) After Eq.(10): it would be useful to explain and motivate the choice of the ratio JSD/H.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(2) On page 6, just below the caption of Fig.4: it would be useful to clarify what is actually meant by &quot;... convergence towards the steady-state distribution of the exact Gillespie simulation, which is obtained at a simulation time of 10^4&quot;.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(3) At the end of Section B on page 7: please clarify what is meant here by &quot;soft directions&quot;.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (recommendations):</bold></p></disp-quote><p>We thank the reviewer for their thoughtful comments and constructive feedback. Below, we address each of the comments/suggestions.</p><disp-quote content-type="editor-comment"><p>Main points:</p><p>(1) Enumerate the conditions under which DGA assumptions hold (and when they do not). There is currently not enough information for the interested reader to know whether DGA would work for their system of interest. Without this information, it is difficult to assess what the true scope of DGA's impact will be. One simple idea would be to test DGA performance along two axes: (i) increasing number of model states and (ii) presence/absence of non-steady state dynamics. I acknowledge that these are very open-ended directions, but looking at even a single instance of each would greatly strengthen this work. Alternatively, if this is not feasible, then the authors should provide more discussion of the attendant difficulties in the main text.</p></disp-quote><p>We agree that a detailed exploration of the conditions under which the DGA assumptions hold would be a valuable addition to the field. However, this paper primarily aims to introduce the DGA methodology and demonstrate its proof-of-concept applications. A comprehensive analysis along axes such as increasing model states or non-steady-state dynamics, while important, would require significant additional simulations and is beyond the scope of this work. In Appendix 1, we have discussed the trade-off between accuracy and numerical stability. Additionally, we encourage future users to tune the hyperparameters a and b for their specific systems.</p><disp-quote content-type="editor-comment"><p>(2) Demonstrate DGA performance in a more complex biochemical system. Clearly the authors were aware that analytic solutions exist for the 2-state system in Figure 7, but it this is actually also the case (I think) for mean mRNA production rate of the non-equilibrium system in Figure 8. To really demonstrate that DGA is practically viable, I encourage the authors to seek out an interesting application that is not analytically tractable.</p></disp-quote><p>We appreciate the suggestion to validate DGA on a more complex biochemical system. However, the goal of this study is not to provide an exhaustive demonstration of all possible applications but to introduce the DGA and validate it in systems where ground-truth comparisons are available. While the non-equilibrium system in Figure 8 might be analytically tractable, its complexity already provides a meaningful demonstration of DGA’s ability to optimize parameters and design systems. Extending this work to analytically intractable systems is an exciting direction for future studies, and we hope this paper will inspire others to explore these applications.</p><disp-quote content-type="editor-comment"><p>(3) Take steps to improve the robustness of parameter optimization and error bar calculations. (3a) When the loss landscape is degenerate, shallow, or otherwise &quot;difficult,&quot; a common solution is to perform multiple (e.g. 25-100) inference runs starting from different random positions in parameter space. Doing this, and then taking the parameter set that minimizes the loss should, in theory, lead to a more robust recovery of the optimal parameter set.</p><p>(3b) It seems clear that the Hessian approximation is underestimating the true error in your inference results. One alternative is to use a &quot;brute force&quot; approach like bootstrap resampling to get a better estimate for the statistical dispersion in parameter estimates. But I recognize that this is only viable if the inference is relatively fast. Simply recovering the true minimum will, of course, also help.</p></disp-quote><p>(3a) We acknowledge the challenge posed by degenerate or shallow loss landscapes during parameter optimization. While performing multiple inference runs from different initializations is a common strategy, this approach is computationally intensive. Instead, we rely on standard optimization techniques (e.g., ADAM) to find a robust local minimum.</p><p>(3b) Thank you for your comment. We agree that Hessian-based error bars can underestimate uncertainty, particularly in degenerate or poorly conditioned loss landscapes. While methods like bootstrap and Monte Carlo can provide more robust estimates, they can be computationally prohibitive for larger-scale simulations. A simpler reason for not using them is the high resource demand from repeated simulations, which quickly becomes infeasible for complex or high-dimensional models. We note these trade-offs between robust estimation and practicality as an important area for further exploration.</p><disp-quote content-type="editor-comment"><p>Moderate comments:</p><p>(1) Figure 7: is it possible to also show the inferred kon values? Specifically, it would be of interest to see how kon varies with repressor concentration.</p></disp-quote><p>Thank you for the suggestion. We have updated Figure 7 to include the inferred kon values, showing their variation with the mean mRNA copy number. However, we could not plot them against repressor concentration due to the lack of available data.</p><disp-quote content-type="editor-comment"><p>(2) Figure 8B &amp; D: the authors claim that the sharper system dissipates more energy, but doesn't 8D show the opposite of this? More importantly, it does not look like either network drives sharpness levels that exceed the upper equilibrium limit cited in [36]. So it is not clear that it is appropriate to look at energy dissipation here. In fact, it is likely that equilibrium networks could produce the curves in 8B, and might be worth checking.</p></disp-quote><p>Thank you for pointing this out. We realized that the plotted values in Figure 8D were incorrect, as we had mistakenly plotted noise instead of energy dissipation. The plot has now been corrected.</p><disp-quote content-type="editor-comment"><p>(3) Figure 8: I really like this idea of using DGA to &quot;design&quot; networks with desired input-output properties, but I wonder if you could explore more a biologically compelling use-case. Specifically, what about some kind of switch-like logic where, as the activator concentration increases, you have first 0 genes on, then 1 promoter on, then 2 promoters on. This would achieve interesting regulatory logic, and having DGA try to produce step functions would ensure that you force the networks to be maximally sharp (i.e. about double what you're currently achieving).</p></disp-quote><p>Thank you for this intriguing suggestion. While the proposed switch-like logic use case is indeed compelling, implementing such a system would require significant work. This goes beyond the scope of the current study, which focuses on demonstrating the feasibility of DGA for network design with simple input-output properties.</p><disp-quote content-type="editor-comment"><p>Minor comments:</p><p>(1) Figure 4B &amp; C: the bar plots do not do a good job conveying the points made by the authors. Consider alternatives, such as scatter plots or box plots that could convey inference uncertainty.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(2) Figure 4B: consider using a log y-axis.</p></disp-quote><p>The y-axis in Figure 4B is already plotted on a log scale.</p><disp-quote content-type="editor-comment"><p>(3) Figure 4D is mentioned prior to 4C in the text. Consider reordering.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(4) Figure 5B: it is difficult to assess from this plot whether or not the landscape is truly &quot;flat,&quot; as the authors claim. Flat relative to what? Consider alternative ways to convey your point.</p></disp-quote><p>Thank you for highlighting this ambiguity. By describing the loss landscape as “flat,” we intend to convey its relative insensitivity to parameter variations in certain regions, rather than implying a completely level surface. While we believe Figure 5B still provides a useful qualitative depiction of this behavior, we acknowledge that it does not quantitatively establish “flatness.” In future work, we plan to incorporate more rigorous measures—such as gradient magnitudes or Hessian eigenvalues—to more accurately characterize and communicate the geometry of the loss landscape.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (recommendations):</bold></p></disp-quote><p>We sincerely thank the reviewer for their thoughtful feedback and constructive suggestions, which have helped us improve the clarity and rigor of our manuscript. Below, we address each of the comments.</p><disp-quote content-type="editor-comment"><p>(1) Precision is lacking in the introduction section. Do the authors mean the Direct SSA, sorted SSA, which is usually faster, and how about rejection sampling methods?</p></disp-quote><p>Thank you for pointing this out. We have updated the introduction to explicitly mention the Direct SSA.</p><disp-quote content-type="editor-comment"><p>(2) When mentioning PyTorch and Jax, would be good to also talk about Julia, as they have fast stochastic simulators.</p></disp-quote><p>We have now mentioned Julia alongside PyTorch and Jax.</p><disp-quote content-type="editor-comment"><p>(3) Mentioned references 22-27. Reference 26 is an odd choice; a better reference is from the same author the Automatic Differentiation of Programs with Discrete Randomness, G Arya, M Schauer, F Schäfer, C Rackauckas, Advances in Neural Information Processing Systems, NeurIPS 2022</p></disp-quote><p>We have now cited the suggested reference.</p><disp-quote content-type="editor-comment"><p>(4) Page 1, Section: 'To circumnavigate these difficulties, the DGA modifies....' Have you thought about how you would deal with the bias that will be introduced by doing this?</p></disp-quote><p>Thank you for your insightful comment. We acknowledge the potential for bias due to the differentiable approximations in the DGA; however, our analysis has not revealed any systematic bias compared to the exact Gillespie algorithm. Instead, we observe irregular deviations from the exact results as the smoothness of the approximations increases.</p><disp-quote content-type="editor-comment"><p>(5) Page 2, first sentence '... traditional Gillespie...' be more precise here - the direct algorithm.</p></disp-quote><p>Thank you for your comment. We believe that the context of the paper, particularly the schematic in Figure 1, makes it clear that we are focusing on the Direct SSA.</p><disp-quote content-type="editor-comment"><p>(6) Page 2, second paragraph: ' In order to simulate such a system...' This doesn't fit here as this section is about tau-leaping. As this approach approximates discrete operations, it is unclear if it would work for large models, snap-shot data of larger scale and if it would be possible to extend it for time-lapse data</p></disp-quote><p>Thank you for your comment. We respectfully disagree that this paragraph is misplaced. The purpose of this paragraph is to explain why the standard Gillespie algorithm does not use fixed time intervals for simulating stochastic processes. By highlighting the inefficiency of discretizing time into small intervals where reactions rarely occur, the paragraph provides necessary context for the Gillespie algorithm’s event-driven approach, which avoids this inefficiency.</p><p>Regarding the applicability of the DGA to larger models, snapshot data, or time-lapse data, we acknowledge these are important directions and have noted them as potential extensions in the discussion section.</p><disp-quote content-type="editor-comment"><p>(7) Page 2 Section B: 'In order to make use of modern deep-learning techniques...' It doesn't appear from the paper that any modern deep learning is used.</p></disp-quote><p>Thank you for your comment. Although the DGA does not utilize deep learning architectures such as neural networks, it employs automatic differentiation techniques provided by frameworks like PyTorch and Jax. These tools allow efficient gradient computations, making the DGA compatible with modern optimization workflows.</p><disp-quote content-type="editor-comment"><p>(8) Page 3, Fig 1(a). S matrix last row, B and C should swap places: B should be 1 and C is -1.</p></disp-quote><p>Corrected the typo.</p><disp-quote content-type="editor-comment"><p>(9) Fig1 needs a more detailed caption.</p></disp-quote><p>Expanded the caption slightly for clarity.</p><disp-quote content-type="editor-comment"><p>(10) Page 3 last paragraph: 'The hyperparameter b...' Consequences of this are relevant, for example can we now go below zero. Also, we lose more efficient algorithms here. It would be good to discuss this in more detail that this is an approx.. algorithm that is good for our case study, but for other to use it more tests are needed.</p></disp-quote><p>Thank you for the comment. Appendix 1 discusses the trade-offs related to a and b, but we agree that more detailed analysis is needed. The hyperparameters are tailored to our case study and must be tuned for specific systems.</p><disp-quote content-type="editor-comment"><p>(11) Page 4, Section C, first paragraph, 'The goal of making...' This is snapshot data. Would the framework also translate to time-lapse data? Also, it would be better to make it clearer earlier which type of data are the target of this study.</p></disp-quote><p>Thank you for your suggestion. While the current study focuses on snapshot data and steady-state properties, we believe the DGA could be extended to handle time-lapse data by incorporating multiple recorded time points into its inference objective. Specifically, one could modify the loss function to penalize discrepancies across observed transitions between these time points, effectively capturing dynamic trajectories. We consider this an exciting area for future development, but it lies beyond our present scope.</p><disp-quote content-type="editor-comment"><p>(12) Page 4 Section C, sentence '...experimentally measured moments'. Should later be mentioned as error, as moments are imperfect</p></disp-quote><p>Thank you for your comment. We agree that experimentally measured moments are inherently noisy and may not perfectly represent the true system. However, within the context of the DGA, these moments serve as target quantities, and the discrepancy between simulated and measured moments is already accounted for in the loss function.</p><disp-quote content-type="editor-comment"><p>(13) Page 4 Section C, last sentence '...second-order...such as ADAM'. Another formulation would be better as second order can be confusing, especially in the context of parameter estimation</p></disp-quote><p>We have revised the language to avoid confusion regarding “second-order” methods.</p><disp-quote content-type="editor-comment"><p>(14) Fig 4(a) a density plot would fit better here</p></disp-quote><p>Fig. 4(a) has been updated to a scatter density plot as suggested.</p><disp-quote content-type="editor-comment"><p>(15) Fig 4(c) Would be interesting to see closer analysis of trade of between gradient and accuracy when changing a and b parameters</p></disp-quote><p>Thank you for this suggestion. We acknowledge that an in-depth exploration of these trade-offs could provide deeper insights into the method’s performance. However, for now, we believe the current analysis suffices to highlight the utility of the DGA in the contexts examined.</p><disp-quote content-type="editor-comment"><p>(16) Page 6 Section III, first sentence: This fits more to intro. Further the reference list is severely lacking here, with no comparison to other methods for actually fitting stochastic models.</p></disp-quote><p>Thank you for the suggestion. We have added a few references there.</p><disp-quote content-type="editor-comment"><p>(17) Page 6, Section A, sentence: '....experimental measured mean...' Why is it a good measure here (moment matching is not perfect), also do you have distribution data, would that not be better? How about accounting for measurement error?</p></disp-quote><p>Thank you for the comment. While we do not have full distribution data, we acknowledge that incorporating experimental measurement error could enhance the framework. A weighted loss function could model uncertainty explicitly, but this is beyond the scope of the current study.</p><disp-quote content-type="editor-comment"><p>(18) Page 7, section B, first paragraph: 'Motivated by this, we defined the...'Why using Fisher-Information when profile-likelihood have proven to be better, especially for systems with few parameters like this.</p></disp-quote><p>Thank you for the suggestion. While profile-likelihood is indeed a powerful tool for parameter uncertainty analysis, we chose Fisher Information due to its computational efficiency and compatibility with the differentiable nature of the DGA framework.</p><disp-quote content-type="editor-comment"><p>(19) Page 7, section C, sentence '...set kR/off=1..'. In this case, we cannot infer this parameter.</p></disp-quote><p>Thank you for the comment. You are correct that setting kR/off = 1 effectively normalizes the rates, making this parameter unidentifiable. In steady-state analyses, not all parameters can be independently inferred because observable quantities depend on relative—rather than absolute—rate values (as evident when setting the time derivative to zero in the master equation). To infer all parameters, one would need additional information, such as time-series data or moments at finite time.</p><disp-quote content-type="editor-comment"><p>(20) Page 7 Section 2. Estimating parameters .... Sentence: '....as can be seen, there is very good agreement..' How many times the true value falls within the CI (because corr 0.68 is not great).</p></disp-quote><p>Thank you for your comment. While a correlation coefficient of 0.68 indicates moderate agreement, the primary goal was to demonstrate the feasibility of parameter estimation using the DGA rather than achieving perfect accuracy. The coverage of the CI was not explicitly calculated, as the focus was on the overall trends and relative agreement.</p><disp-quote content-type="editor-comment"><p>(21) Page 7 Section 2. Estimating parameters .... Sentence: 'Fig5(c) shows....' Is this when using exact simulator?</p></disp-quote><p>Thank you for your question. Yes, the exact values in x-axis of Fig. 5(c) are obtained using the exact Gillespie simulation.</p><disp-quote content-type="editor-comment"><p>(22) Page 7 Section 3 Estimating parameters for the... Sentence: 'Fig6(a) shows...' Why Cis are not shown?</p></disp-quote><p>Thank you for your comment. CIs are not shown in Fig. 6(a) because this particular case is degenerate, making the calculation and meaningful representation of CIs challenging.</p><disp-quote content-type="editor-comment"><p>(23) Page 10, Sentence: 'As can be seen in Fig 7(b)...' Can you show uncertainty in measured value? It would be good to see something of a comparison against an exact method, at least on simulated synthetic data</p></disp-quote><p>Thank you for the comment. Fig. 7(a) already includes error bars for the experimental data, which account for measurement uncertainty. However, in Fig. 7(b), we do not include error bars for the experimental values due to limitations in the available data.</p><disp-quote content-type="editor-comment"><p>(24) Page 12, Section B Loss function '...n=600...' This is on a lower range. Have you tested with n=1000?</p></disp-quote><p>Yes, we have tested with n=1000 and observed no significant difference in the results. This indicates that n=600 is sufficient for the purposes of this study.</p><disp-quote content-type="editor-comment"><p>(25) Fig 8(c) why there are no CI shown?</p></disp-quote><p>Thank you for your comment. CIs were not included in Fig. 8(c) due to degeneracy, which makes meaningful confidence intervals difficult to compute.</p><disp-quote content-type="editor-comment"><p>(26) Page 12 Conclusion, sentence: '..gradients via backpropagation...' Actually, by making the function continuous, both forward and reverse mode might be used. And in this case, forward-mode would actually be the fastest by quite a margin</p></disp-quote><p>Thank you for your insightful comment. You are correct that by making the function continuous, both forward-mode and reverse-mode automatic differentiation can be used. We have now mentioned this point in the discussion.</p><disp-quote content-type="editor-comment"><p>(27) Overall comment for the Conclusion section: It would be good to discuss how this framework compares to other model-fitting frameworks for models with stochastic dynamics. The authors mention dynamic data and more discussion on this would be very welcomed. Why use ADAM and not something established like BFGS for model fitting? It would be interesting to discuss how this can fit with other SSA algorithms (e.g. in practice sorting SSA is used when models get larger). Also, inference comparison against exact approaches would be very nice. As it is now, the authors truly only check the accuracy of the SSA on 1 model -it would be interesting to see for other models.</p></disp-quote><p>Thank you for your detailed comments. While this study focuses on introducing the DGA and demonstrating its feasibility, we agree that comparisons with other model-fitting frameworks, testing on additional models, and integrating with other SSA variants like sorted SSA are important directions for future work. Similarly, extending the DGA to handle transient dynamics and exploring alternatives to ADAM, such as BFGS, are promising areas to investigate further.</p></body></sub-article></article>