<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86943</article-id><article-id pub-id-type="doi">10.7554/eLife.86943</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.86943.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Remapping in a recurrent neural network model of navigation and context inference</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-304799"><name><surname>Low</surname><given-names>Isabel IC</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6465-8459</contrib-id><email>il2419@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-29357"><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0416-2528</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-306302"><name><surname>Williams</surname><given-names>Alex H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5853-103X</contrib-id><email>alex.h.williams@nyu.edu</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Neurobiology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00sekdz59</institution-id><institution>Center for Computational Neuroscience, Flatiron Institute</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Center for Neural Science, New York University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>Ecole Normale Superieure Paris</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>07</month><year>2023</year></pub-date><volume>12</volume><elocation-id>RP86943</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-02-13"><day>13</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-01-27"><day>27</day><month>01</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.25.525596"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-05-09"><day>09</day><month>05</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.86943.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-06-16"><day>16</day><month>06</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.86943.2"/></event></pub-history><permissions><copyright-statement>© 2023, Low et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Low et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86943-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86943-figures-v1.pdf"/><abstract><p>Neurons in navigational brain regions provide information about position, orientation, and speed relative to environmental landmarks. These cells also change their firing patterns (‘remap’) in response to changing contextual factors such as environmental cues, task conditions, and behavioral states, which influence neural activity throughout the brain. How can navigational circuits preserve their local computations while responding to global context changes? To investigate this question, we trained recurrent neural network models to track position in simple environments while at the same time reporting transiently-cued context changes. We show that these combined task constraints (navigation and context inference) produce activity patterns that are qualitatively similar to population-wide remapping in the entorhinal cortex, a navigational brain region. Furthermore, the models identify a solution that generalizes to more complex navigation and inference tasks. We thus provide a simple, general, and experimentally-grounded model of remapping as one neural circuit performing both navigation and context inference.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Recurrent neural network models</kwd><kwd>dynamic coding</kwd><kwd>latent state</kwd><kwd>attractor manifolds</kwd><kwd>medial entorhinal cortex</kwd><kwd>navigation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Wu Tsai Neurosciences Institute, Stanford University</institution></institution-wrap></funding-source><award-id>Stanford Interdisciplinary Graduate Fellowship</award-id><principal-award-recipient><name><surname>Low</surname><given-names>Isabel IC</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00141812690</award-id><principal-award-recipient><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>SCGB 542987SPI</award-id><principal-award-recipient><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>1R01MH126904-01A1</award-id><principal-award-recipient><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>U19NS118284</award-id><principal-award-recipient><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007204</institution-id><institution>Vallee Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Williams</surname><given-names>Alex H</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Recurrent neural networks trained to navigate and infer latent states exhibit strikingly similar remapping patterns to those observed in navigational brain areas, inspiring new analyses of published data and suggesting a possible function for spontaneous remapping to support context-dependent navigation.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neural circuit computations throughout the brain, from the primary sensory cortex (<xref ref-type="bibr" rid="bib5">Bennett et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib57">Vinck et al., 2015</xref>; <xref ref-type="bibr" rid="bib62">Zhou et al., 2014</xref>) to higher cognitive areas, (<xref ref-type="bibr" rid="bib6">Boccara et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Butler et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Hardcastle et al., 2017b</xref>; <xref ref-type="bibr" rid="bib29">Hulse et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Pettit et al., 2022</xref>) are shaped by combinations of internal and external factors. Internal state changes, such as shifts in attention (<xref ref-type="bibr" rid="bib19">Fenton et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Kentros et al., 2004</xref>; <xref ref-type="bibr" rid="bib43">Pettit et al., 2022</xref>), thirst (<xref ref-type="bibr" rid="bib1">Allen et al., 2019</xref>), arousal (<xref ref-type="bibr" rid="bib54">Stringer et al., 2019</xref>), and impulsivity (<xref ref-type="bibr" rid="bib12">Cowley et al., 2020</xref>), can profoundly alter neural activity across multiple brain areas. This raises a question: how can individual brain regions with specialized functions integrate global state changes without compromising their local processing dynamics?</p><p>For example, neurons in the medial entorhinal cortex typically represent one or more features such as spatial position, heading direction, and environmental landmarks and are therefore thought to support navigation (<xref ref-type="bibr" rid="bib16">Diehl et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Gil et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Hafting et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">Hardcastle et al., 2017a</xref>; <xref ref-type="bibr" rid="bib28">Høydal et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Moser et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Sargolini et al., 2006</xref>; <xref ref-type="bibr" rid="bib52">Solstad et al., 2008</xref>). At the same time, these neurons change their firing rates and shift their spatial firing positions—or ‘remap’—under a variety of circumstances, even when navigational cues remain stable (<xref ref-type="bibr" rid="bib3">Bant et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Boccara et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Butler et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Campbell et al., 2021</xref>; <xref ref-type="bibr" rid="bib8">Campbell et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Hardcastle et al., 2017b</xref>; <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). It is difficult to pinpoint the reason for these <italic>spontaneous remapping events</italic>—i.e., remapping not driven by changes in navigational features. Theoretical models of this phenomenon propose that remapping occurs because these cells are responding to global contextual cues (like arousal or attention) in order to decorrelate related experiences with distinct contextual relevance (<xref ref-type="bibr" rid="bib10">Colgin et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Sanders et al., 2020</xref>). This process could enable animals to form distinct memories or choose appropriate actions for a given set of circumstances. However, these normative models (i.e. theories for <italic>why</italic> remapping occurs; <xref ref-type="bibr" rid="bib36">Levenstein et al., 2020</xref>) do not address how a biological system might implement this strategy.</p><p>To bridge the gap between existing theoretical models and biological observations of remapping in the entorhinal cortex, we sought to establish a minimal set of task constraints that could reproduce the essential dynamics of remapping in a computational model. Specifically, we tested the normative hypothesis that remapping occurs when a population of neurons must maintain its local navigational processing, while at the same time responding to global latent state changes (e.g. changes in behavioral state, task conditions, etc.; see <xref ref-type="bibr" rid="bib48">Sanders et al., 2020</xref>). We trained recurrent neural network models (RNNs) to maintain an estimate of position in a simple environment, while at the same time reporting a changing, transiently-cued latent state variable. In isolation, neither of these tasks is novel to the RNN literature—e.g., <xref ref-type="bibr" rid="bib13">Cueva and Wei, 2018</xref> trained RNNs to path integrate in complex environments while <xref ref-type="bibr" rid="bib55">Sussillo and Barak, 2013</xref> trained RNNs on a ‘1-bit flip-flop’ memory task akin to our latent state inference task. Here, we combine these two tasks to ask how a network would solve them simultaneously and to probe how this combination of tasks relates to remapping in navigational circuits.</p><p>We found that RNNs trained to navigate while inferring latent state changes exhibited network-wide activity patterns that were strikingly similar to those found in the brain (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>), suggesting a possible function for spontaneous remapping in the entorhinal cortex and other navigational brain areas. These activity patterns comprise a geometrically simple solution to the task of combining navigation with latent state inference. The RNN geometry and algorithmic principles readily generalized from a simple task to more complex settings. Furthermore, we performed a new analysis of experimental data published by <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> and found a similar geometric structure in neural activity from a subset of sessions with more than two stable spatial maps. Overall, these results provide an interpretable and experimentally grounded account of how a single neural population might flexibly represent global brain state changes (corresponding here to remapping) and localized circuit computations (corresponding here to navigation) in orthogonal subspaces (<xref ref-type="bibr" rid="bib33">Kaufman et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Rule et al., 2020</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A recurrent neural network model of 1D navigation and context inference remaps between aligned ring manifolds</title><p>To investigate a putative functional role for spontaneous remapping in an unchanging environment, we developed a task that requires simultaneous latent state inference and navigation in a single neural circuit. To ground our model in experimental data, we designed our task to reflect the basic structure of a recent study (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>; <xref ref-type="fig" rid="fig1">Figure 1A–D</xref>). In this study, Low et al. demonstrated that remapping in the medial entorhinal cortex simultaneously recruited large populations of neurons across the entorhinal cortical circuit. Remapping comprised discrete transitions between aligned neural activity manifolds, which each represented a distinct map of an unchanging, virtual reality 1D environment (<xref ref-type="fig" rid="fig1">Figure 1C–D</xref>). Remapping was not aligned to particular track positions, rewards, or landmarks. Instead, remapping correlated with transient decreases in running speed (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), which could correspond to discrete changes in a latent state (such as shifts in arousal, task engagement, or other behavioral states). Thus, we developed a task that requires a single neural circuit to navigate a 1D circular environment while inferring transiently cued, discrete latent state changes. We hypothesized that these task constraints would produce a manifold structure similar to that observed in Low et al. (<xref ref-type="fig" rid="fig1">Figure 1D</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Recurrent neural network (RNN) models and biological neural circuits remap between aligned spatial maps of a single 1D environment.</title><p>(<bold>A–D</bold>) are modified from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>. (<bold>A</bold>) Schematized task. Mice navigated virtual 1D circular-linear tracks with unchanging environmental cues and task conditions. Neuropixels recording probes were inserted during navigation. (<bold>B</bold>) Schematic: slower running speeds correlated with the remapping of neural firing patterns. (C, left) An example medial entorhinal cortex neuron switches between two maps of the same track (top, spikes by trial and track position; bottom, average firing rate by position across trials from each map; red, map 1; black, map 2). (C, right/top) Correlation between the spatial firing patterns of all co-recorded neurons for each pair of trials in the same example session (dark gray, high correlation; light gray, low correlation). The population-wide activity is alternating between two stable maps across blocks of trials. (C, right/bottom) K-means clustering of spatial firing patterns results in a map assignment for each trial. (<bold>D</bold>) Principal Components Analysis (PCA) projection of the manifolds associated with the two maps (color bar indicates track position). (<bold>E</bold>) RNN models were trained on a simultaneous 1D navigation (velocity signal, top) and latent state inference (transient, binary latent state signal, bottom) task. (<bold>F</bold>) Example showing high prediction performance for the position (top) and latent state (bottom). (<bold>G</bold>) As in (<bold>C</bold>), but for RNN units and network activity. Map is the predominant latent state on each trial. (<bold>H</bold>) Example PCA projection of the moment-to-moment RNN activity (colormap indicates track position). (<bold>I</bold>) Total variance explained by the principal components for network-wide activity across maps (top three principal components, red points). (<bold>J</bold>) Normalized manifold misalignment scores across models (0, perfectly aligned; 1, p=0.25 of shuffle). (<bold>K</bold>) Cosine similarity between the latent state and position input and output weights onto the remap dimension (left) and the position subspace (right) (error bars, sem; N = 15 models).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Single recurrent neural network (RNN) units remap heterogeneously.</title><p>(<bold>A</bold>) Absolute fold change in peak firing rate versus spatial dissimilarity across latent state changes for all units from 2-map RNN models (points, single units; histograms, density distributions for each variable). Median change in peak firing rate (horizontal red dashes)=1.9 fold; 95<sup>th</sup> percentile (horizontal gold dashes)=threefold. Median spatial dissimilarity (vertical red dashes)=0.025; 95<sup>th</sup> percentile (vertical gold dashes)=0.21. (<bold>B</bold>) Modified from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>. As in (<bold>A</bold>) but for biological neurons from 2-map sessions. Median change in peak firing rate (horizontal red dashes)=1.28 fold. Median spatial dissimilarity (vertical red dashes)=0.031; 95<sup>th</sup> percentile (vertical gold dashes)=0.22.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Recurrent neural network (RNN) geometry is interpretable.</title><p>(<bold>A</bold>) Schematic showing the relative orientation of the position output weights and the context input and output weights to the position and state tuning subspaces. (<bold>B</bold>) Reproduced from <xref ref-type="fig" rid="fig1">Figure 1K</xref>. (<bold>C–D</bold>) Schematic to interpret why the position input weights are orthogonal to the position-uning subspace. These schematics illustrate how a single velocity input (blue arrows) updates the position estimate (yellow to red points) from a given starting position (blue points). (C, not observed) Velocity input lies in the position tuning subspace (gray plane). Note that the same velocity input pushes the network clockwise or counterclockwise along the ring depending on the circular position. (D, observed) Velocity input is orthogonal to the position tuning subspace and pushes neural activity out of the subspace. (<bold>E</bold>) Schematic of possible flow fields in each of three planes (numbers correspond to planes in C and D). We conjecture that these dynamics would enable a given orthogonal velocity input to nonlinearly update the position estimate, resulting in the correct translation around the ring regardless of starting position (as in D).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig1-figsupp2-v1.tif"/></fig></fig-group><p>We trained RNNs with <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>248</mml:mn></mml:math></inline-formula> units to integrate a 1D velocity input along a circular environment (equivalent to a 1D virtual track with seamless teleportation, as in <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>; <xref ref-type="fig" rid="fig1">Figure 1E</xref>, top) and to concurrently remember a binary latent state signal (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, bottom). Trained RNNs achieve high performance in both tasks, indicating that they can correctly form a persistent representation of each latent state while maintaining a stable position estimate across states (100% correct state estimation; average angular position error after 300 steps, mean ± standard deviation: 8.13° ± 0.51°; <xref ref-type="fig" rid="fig1">Figure 1F</xref>). To visualize trial-by-trial RNN activity, we provided the trained model with nonnegative velocity inputs and divided the resulting session into track traversals, labeling each traversal by the predominant latent state (<xref ref-type="fig" rid="fig1">Figure 1G</xref>, red, context 1; black, context 2). As in biological data (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), single units and network-wide activity alternated between distinct maps of the same environment across the two latent states (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). Similar to biological neurons, single RNN units remapped heterogeneously (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Units changed their spatial firing field locations to a similar extent as biological neurons, but changes in firing rate were more common in the model units.</p><p>When we projected the hidden layer activity into the subspace defined by the first three principle components, the activity occupied two distinct rings, where position along each ring corresponded to position on the linear track (<xref ref-type="fig" rid="fig1">Figure 1H</xref>, red to blue color map). Together, these top three components explained ~50% of the variance (<xref ref-type="fig" rid="fig1">Figure 1I</xref>, red points). As in <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>, we used Procrustes shape analysis to demonstrate that these rings were more aligned in high-dimensional activity space than expected by chance for all trained models, such that the position along one ring matched the position on the second ring (<xref ref-type="fig" rid="fig1">Figure 1J</xref>). Thus, these task constraints are sufficient to organize randomly initialized synaptic weights into a network model that qualitatively reproduces the remapping dynamics and representational geometry that <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> observed in the entorhinal cortex.</p></sec><sec id="s2-2"><title>RNN geometry is interpretable</title><p>We next considered the network geometry in more detail, asking how much of the geometry arises necessarily from the task design. Some basic features of the network structure follow intuitively from the components of the task. First, the RNN must maintain an estimate of a 1D circular position, which is best achieved through approximate ring attractors (<xref ref-type="bibr" rid="bib14">Cueva et al., 2019</xref>). Thus, we expect the model to form two ring attractors, one for each of the two distinct latent state conditions. Second, the network must track two statistically independent information streams and should, therefore, develop separate orthogonal subspaces for each stream (<xref ref-type="bibr" rid="bib33">Kaufman et al., 2014</xref>). One subspace, the ‘position subspace,’ should contain the position tuning curves for all neurons, as well as the RNN readout weights for the position. The other subspace, the ‘remapping dimension,’ should be tuned to changes in the latent state and contain the readout weights for the state. We confirmed that these dimensions were orthogonal to one another (<xref ref-type="fig" rid="fig1">Figure 1K</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, Methods), such that changes in the latent state do not interfere with changes in position and vice versa.</p><p>However, as we show in detail below, the task does not require the two-ring manifolds to be strictly aligned in high-dimensional activity space. The RNN maintains a stable position estimate in spite of switches in the latent state—which we call ‘remapping events’—that may occur anywhere along the circular track. To do so, the RNN must implement an invertible remapping function to match track locations across the two ring manifolds. This remapping function could be complex and high dimensional (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), resulting in misaligned manifolds, or it could take the form of a simple translation in firing rate space (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), resulting in aligned manifolds. The RNN could implement a complex remapping function using its recurrent dynamics, but this implementation could lead to a delay between the latent state signal and remapping. We therefore reasoned that the RNN might converge to the simpler configuration, allowing the linear input layer to implement remapping and thereby enabling rapid remapping in a single timestep.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The ring attractors are more aligned than strictly required by the task.</title><p>(<bold>A</bold>) Schematic illustrating how the two attractor rings could be misaligned in the nullspace of the position output weights (left) while still allowing linear position decoding (right) (color map, track position; solid arrows, remapping vectors; dashed arrow, projection onto the position subspace). (<bold>B</bold>) Schematic illustrating perfect manifold alignment (colors as in A) (<bold>C</bold>) Normalized difference between the true remapping vectors for all position bins and all models and the ideal remapping vector (dashed line, p=0.025 of shuffle; n=50 position bins, 15 models). (<bold>D</bold>) Dimensionality of the remapping vectors for an example model. (Right) Total variance explained by the principal components for the remapping vectors (red points, top two PCs). (Left) Relative variance is explained by the remapping vectors (red) and the position rings (blue) (1=total network variance). (<bold>E</bold>) The remapping vectors vary smoothly over the position. (Right) Projection of the remapping vectors onto the first two PCs. (Left) Normalized covariance of the remapping vectors for each position bin (blue, min; yellow, max).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig2-v1.tif"/></fig><p>It is useful to mathematically formalize these ideas to show that the alignment of the two ring manifolds is not strictly imposed by the task. Consider an RNN with <inline-formula><mml:math id="inf2"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons and, for simplicity, consider a discrete grid of <inline-formula><mml:math id="inf3"><mml:mi>P</mml:mi></mml:math></inline-formula> position bins indexed by <inline-formula><mml:math id="inf4"><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>P</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf5"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> denote an <inline-formula><mml:math id="inf6"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vector corresponding to the neural firing rates in spatial bin <inline-formula><mml:math id="inf7"><mml:mi>p</mml:mi></mml:math></inline-formula> along the first ring attractor (i.e. position <inline-formula><mml:math id="inf8"><mml:mi>p</mml:mi></mml:math></inline-formula> in state 1). Likewise, let <inline-formula><mml:math id="inf9"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> denote the corresponding firing rate vector in the second ring attractor (i.e. position <inline-formula><mml:math id="inf10"><mml:mi>p</mml:mi></mml:math></inline-formula> in state 2). We can compute <inline-formula><mml:math id="inf11"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> by averaging the RNN activations across many simulated trials, similar to how spatial tuning curves are estimated in biological data.</p><p>Let <inline-formula><mml:math id="inf13"><mml:mi>W</mml:mi></mml:math></inline-formula> denote the <inline-formula><mml:math id="inf14"><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> matrix holding the readout layer weights used to decode angular position <inline-formula><mml:math id="inf15"><mml:mi>θ</mml:mi></mml:math></inline-formula> by predicting <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Methods). The linearity of this decoder imposes a constraint that <inline-formula><mml:math id="inf18"><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for all <inline-formula><mml:math id="inf19"><mml:mi>p</mml:mi></mml:math></inline-formula>; otherwise, the decoded position will erroneously depend on which latent state is active. Importantly, this constraint does not imply that the two rings must have the same shape nor that they must be aligned. To see this let <inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote any arbitrary set of <inline-formula><mml:math id="inf21"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional vectors in the nullspace of <inline-formula><mml:math id="inf22"><mml:mi>W</mml:mi></mml:math></inline-formula> (i.e. we have <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>W</mml:mi><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) and define <inline-formula><mml:math id="inf24"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Then it is easy to see that the constraint is satisfied,<disp-formula id="equ1"><mml:math id="m1"><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></disp-formula></p><p>Because each <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was chosen arbitrarily from the nullspace of <inline-formula><mml:math id="inf26"><mml:mi>W</mml:mi></mml:math></inline-formula>, an (<inline-formula><mml:math id="inf27"><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>)-dimensional subspace, there are many configurations of the two rings that are compatible with linear decoding of position (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). An alternative, lower-dimensional network geometry would instead remap along a constant <inline-formula><mml:math id="inf28"><mml:mi>N</mml:mi></mml:math></inline-formula>-dimensional translation vector <inline-formula><mml:math id="inf29"><mml:mi>v</mml:mi></mml:math></inline-formula>, such that we have <inline-formula><mml:math id="inf30"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula> (approximately) for all positions <inline-formula><mml:math id="inf31"><mml:mi>p</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>We now explore each of these intuitions in our data to see how well the trained RNN matches our expectations. First, how closely do the manifolds match the best alignment wherein <inline-formula><mml:math id="inf32"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula> ? We computed the empirical remapping vectors <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for each position bin and verified that <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>W</mml:mi><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> for all positions (mean ± sem: 9x10<sup>–6</sup> ± 10<sup>–5</sup>). We then defined <inline-formula><mml:math id="inf35"><mml:mi>v</mml:mi></mml:math></inline-formula> to be the average of these remapping vectors, <inline-formula><mml:math id="inf36"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mo>⟨</mml:mo><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:math></inline-formula>. If the manifolds were perfectly aligned then we would observe <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula> for all positions <inline-formula><mml:math id="inf38"><mml:mi>p</mml:mi></mml:math></inline-formula>.</p><p>We instead find that there is some variability in the remapping vectors, such that <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are not exactly equal to one another (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Indeed, when we perform PCA on the <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> matrix formed by concatenating the vectors <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi> </mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we find that remapping dimensions lie within a 2-dimensional subspace (<xref ref-type="fig" rid="fig2">Figure 2D and E</xref>), in contrast to our original conjecture that remapping vectors would be effectively zero-dimensional (i.e. <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula> for all positions). Nonetheless, the idealized model in which each <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mi>v</mml:mi></mml:math></inline-formula> is a much better fit to the observed RNN dynamics than would be expected by chance. When we randomly rotate the orientation of the two rings in the nullspace of <inline-formula><mml:math id="inf44"><mml:mi>W</mml:mi></mml:math></inline-formula>, we find that this approximation is much worse (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, dashed line).</p><p>Altogether, these findings suggest that RNN models trained on a simultaneous latent state inference and navigation task converge to a geometrically simple solution out of the space of all possible, high-dimensional solutions. This simpler solution recapitulates the geometry of entorhinal cortical dynamics during remapping in virtual reality environments (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). Notably, neither the RNN nor the biological data are consistent with the simplest 3-dimensional solution, as evidenced by the imperfect ring alignment (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> and <xref ref-type="fig" rid="fig1">Figure 1J</xref>), the variable remapping vectors (<xref ref-type="fig" rid="fig2">Figure 2C–D</xref>), and the dimensionality of the network dynamics (which is &gt;3; <xref ref-type="fig" rid="fig1">Figure 1I</xref>).</p></sec><sec id="s2-3"><title>RNN dynamics follow two-ring attractor manifolds</title><p>While neural manifold geometry can provide clues about the computational mechanisms at play in the system, one advantage of RNN models is that we can precisely characterize the structure and logic of their dynamics using tools from nonlinear systems analysis (<xref ref-type="bibr" rid="bib38">Maheswaranathan et al., 2019</xref>; <xref ref-type="bibr" rid="bib55">Sussillo and Barak, 2013</xref>). As we describe below, these tools reveal several insights into the underlying network computations that are not easy to experimentally demonstrate in biological networks.</p><p>Each RNN defines a nonlinear, discrete-time dynamical system, <inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf46"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is a nonlinear function parameterized by synaptic weight matrices and <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> defines a sequence of latent state cues and velocity inputs to the network. Using methods pioneered in <xref ref-type="bibr" rid="bib55">Sussillo and Barak, 2013</xref>, we used numerical optimization to identify <italic>approximate fixed points</italic>, which are N-dimensional vectors <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that satisfy <inline-formula><mml:math id="inf49"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for a specified input <inline-formula><mml:math id="inf50"><mml:mi>u</mml:mi></mml:math></inline-formula>. In particular, we studied the case where <inline-formula><mml:math id="inf51"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, corresponding to a situation where no velocity or context input is provided to the network. Intuitively, the network should approach a fixed point when no velocity or context input is provided because the position and latent state are unchanging.</p><p>The fixed points of the RNN provide a backbone for understanding its dynamics. While the global RNN dynamics are complex and nonlinear, the dynamics near any fixed point <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be approximated as a linear dynamical system governed by the <inline-formula><mml:math id="inf53"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> Jacobian matrix of partial derivatives <inline-formula><mml:math id="inf54"><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> evaluated at <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf56"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> (see Methods).</p><p>We computed these Jacobian matrices across 988 fixed points in the trained RNN shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Roughly 60% of these fixed points were located on one of the two previously described ring manifolds and largely had marginally stable linear dynamics (largest Jacobian eigenvalue ≈1; <xref ref-type="fig" rid="fig3">Figure 3</xref>, color-coded green). The remaining fixed points were located between the two ring manifolds and had unstable dynamics (largest Jacobian eigenvalue &gt;1; <xref ref-type="fig" rid="fig3">Figure 3</xref>, color-coded gold). In essence, this analysis confirms that the RNN dynamics indeed implemented a pair of ring attractors. Furthermore, a collection of unstable fixed points form a boundary between the two stable ring attractor basins. In line with observations by <xref ref-type="bibr" rid="bib55">Sussillo and Barak, 2013</xref> on a discrete flip-flop task, these intermediate fixed points are unstable along a small number of dimensions (i.e. saddle fixed points; <xref ref-type="fig" rid="fig3">Figure 3D</xref>, gold points) which ‘funnel’ neural activity to the appropriate location during a remapping event.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Recurrent neural network (RNN) dynamics follow stable ring attractor manifolds mediated by a ring of saddle points.</title><p>(<bold>A</bold>) Fixed points (gray) reside on the two ring manifolds and in a ring between them (gray, all fixed points; colors indicate example fixed points shown in B and D). (<bold>B</bold>) The maximum real component of each fixed point (shading, marginally stable or unstable points; colored points, examples from A). Dashed red lines indicate cut-off values for fixed points to be considered marginally stable (<inline-formula><mml:math id="inf57"><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>). (<bold>C</bold>) Projection of fixed points onto the remapping dimension (–1, map 1 centroid; 1, map 2 centroid; 0, midpoint between manifolds). (<bold>D</bold>) Distribution of eigenvalues (colored points) in the complex plane for each example fixed point from (<bold>A</bold>) (black line, unit circle). (<bold>E</bold>) Cosine similarity between the eigenvectors associated with the largest magnitude eigenvalue of each fixed point and the remap dimension (top) or the position subspace (bottom). (<bold>F</bold>) Eigenvector directions for 48 example fixed points (black lines, eigenvectors). (<bold>G</bold>) Projection of the example fixed points closest to each manifold onto the respective position subspace (top) and zoom on an example fixed point for each manifold (bottom, red box)(purple, estimated activity manifold; dashed line, approximate position estimate). Green indicates marginally stable points and gold indicates unstable points throughout.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig3-v1.tif"/></fig><p>This interpretation is supported by examining the principal eigenvector—i.e., the eigenvector associated with the largest magnitude eigenvalue—for each fixed point. For the fixed points along the two ring attractors, this eigenvector corresponds to a slow dimension along which <inline-formula><mml:math id="inf58"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> does not grow or decay (i.e. its associated eigenvalue <inline-formula><mml:math id="inf59"><mml:mi>λ</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>; <xref ref-type="fig" rid="fig3">Figure 3D</xref>, green points). Consistent with a mechanism for integrating positional information, these eigenvectors were nearly orthogonal to the remapping dimension and aligned with the position subspace (<xref ref-type="fig" rid="fig3">Figure 3E–G</xref>, green). Conversely, for the unstable fixed points, the principal eigenvector corresponds to a dimension along which <inline-formula><mml:math id="inf60"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> moves rapidly away from the fixed point (i.e. its associated eigenvalue <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig3">Figure 3D</xref>, gold points). Consistent with a mechanism for ‘funneling’ activity during remapping events, these eigenvectors were aligned with the remapping dimension and nearly orthogonal to the position subspace (<xref ref-type="fig" rid="fig3">Figure 3E–F</xref>, yellow).</p></sec><sec id="s2-4"><title>Aligned toroidal manifolds emerge in a 2D generalization of the task</title><p>Virtual 1D tracks are an ideal setting to experimentally study spontaneous remapping: the environmental cues can be tightly controlled and it is possible to sample an identical spatial trajectory hundreds of times, such that remapping events can be easily identified from the neural activity alone. But navigation is often studied in 2D environments, in which it is more difficult to control the animal’s experience and the animal can pursue an essentially infinite number of trajectories through the environment. Thus, while it is of interest to understand what remapping in the entorhinal cortex might look like in 2D spaces, it remains challenging to identify spontaneous remapping in biological data. In contrast, the RNN modeling framework that we have developed here can be readily generalized to 2D spatial environments. Are the computational solutions identified by the RNNs fundamentally different in this case? Or do RNNs use similar geometric structures and algorithmic principles across these related tasks?</p><p>To investigate this question, we again trained models to simultaneously integrate velocity inputs and estimate latent state from transient state cues, but this time we provided two velocity inputs and asked the models to estimate position on a 2D circular track (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, right). As before, the models performed well on both components of the task (mean loss ± sem: position estimate, 0.036 ± 1.1 × 10<sup>–3</sup>; latent state estimate, 0.002 ± 1.9 × 10<sup>–5</sup>; n = 15 models) (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), and single unit activity was modulated by both spatial position and latent state (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). When we projected the activity into a subspace defined by the first three principal components, the activity occupied two distinct toroidal manifolds with the position on each torus corresponding to the position in the 2D space (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Notably, each toroidal manifold alone is reminiscent of networks trained to store two circular variables without remapping (<xref ref-type="bibr" rid="bib15">Cueva et al., 2021</xref>). In keeping with these qualitative observations, four principal components explained ~50% of the variance in network activity (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), and the manifolds were again highly, though not perfectly, aligned in the full-dimensional activity space (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). By holding either the horizontal (X) or vertical (Y) position variable constant during the RNN simulation, we recover a pair of 1D-aligned ring manifolds (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). That is, we can recover the geometry of the original 1D task (see <xref ref-type="fig" rid="fig1">Figure 1</xref>) by taking ‘slices’ through the toroidal manifolds. The remapping and position dimensions were again orthogonalized in these models (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). Thus models trained on a 2D navigation task with latent state inference identified a geometrically similar solution to those trained on a 1D task. These findings demonstrate that spontaneous remapping is possible in 2D and may operate under similar mechanisms as in 1D.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>An recurrent neural network (RNN) model of 2D navigation and context inference remaps between aligned toroidal manifolds.</title><p>(<bold>A</bold>) (Left) Schematic illustrating the 2D navigation with simultaneous latent state inference task. (Right) As in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, but RNN models were trained to integrate two velocity inputs (X, Y) and output a 2D position estimate, in addition to simultaneous latent state inference. (<bold>B</bold>) Position-binned activity for three example units in latent state 1 (top) and latent state 2 (bottom)(colormap indicates normalized firing rate; blue, minimum; yellow, maximum). (<bold>C</bold>) Example principal components analysis (PCA) projection of the moment-to-moment RNN activity from a single session into three dimensions (colormap indicates position; left, X position; right, Y position). Note that the true tori are not linearly embeddable in 3 dimensions, so this projection is an approximation of the true torus structure. (<bold>D</bold>) Average cumulative variance explained by the principal components for network-wide activity across maps (top four principal components, red points). (<bold>E</bold>) Normalized manifold misalignment scores for all models (0, perfectly aligned; 1, p=0.25 of shuffle). (<bold>F</bold>) Example PCA projection of slices from the toroidal manifold where Y (left) or X (right) position is held constant, illustrating the substructure of RNN activity. (<bold>G</bold>) Cosine similarity between the latent state and position input and output weights onto the remap dimension (left) and the position subspace (right), defined for each pair of maps (error bars, sem; n=15 pairs, 15 models).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig4-v1.tif"/></fig></sec><sec id="s2-5"><title>Manifold alignment generalizes to three or more maps</title><p>It is simplest to consider remapping as switches between two maps, but neural activity can conceivably switch between any number of maps. Indeed, while Low et al. most commonly observed remapping between two maps of the same virtual track, they occasionally found transitions between more than two maps (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). Because these ‘multi-map’ sessions were rare, Low et al. predominantly limited their analysis to the more common ‘2-map’ sessions. Nonetheless, there were notable similarities between the 2-map and multi-map sessions. In particular, remapping was correlated with changes in running speed and position was preserved across remapping events (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). We reasoned that we could study the geometry of ‘multi-map’ sessions using RNN models to gain insight into what we might expect to see in biological data. In particular, do multiple ring manifolds corresponding to multiple spatial maps emerge and are these ring manifolds still geometrically aligned with each other?</p><p>We trained models to integrate a 1D velocity input, while tracking three (instead of two) binary state switch cues (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Models performed well on both task components (mean loss ± sem: position estimate, 0.013 ± 3.6 × 10<sup>–4</sup>; latent state estimate, 0.0039 ± 4.9 × 10<sup>–5</sup>; n=15 models) and single unit activity was modulated by both spatial position and latent state (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Importantly, we found that the same essential geometry of the original task was preserved. When we visualized each pair of maps using PCA, the ring manifolds were again qualitatively aligned (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, right). Projecting the activity from all three maps into the same subspace revealed that they were further organized as vertices of an equilateral triangle (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, left)—i.e., the acute angle between any two remapping dimensions was 60° (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Again, the network geometry was relatively low-dimensional (4 principal components explained ~60% of the variance; 14 principal components,~90% of the variance)(<xref ref-type="fig" rid="fig5">Figure 5E</xref>). Procrustes analysis revealed that all pairs of manifolds were highly aligned relative to chance, with a similar degree of alignment across ring manifold pairs and across RNNs (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). Finally, positional and latent state information were orthogonalized, as before (<xref ref-type="fig" rid="fig5">Figure 5G</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>An recurrent neural network (RNN) model of 1D navigation and identification of three latent states remaps between three aligned ring manifolds.</title><p>(<bold>A</bold>) (Left) RNN models were trained to navigate in 1D (velocity signal, top) and discriminate between three distinct contexts (transient, binary latent state signal, bottom). (Right) Example showing high prediction performance for the position (top) and context (bottom). (<bold>B</bold>) Position-binned activity for six examples single RNN units, split by context (colors as in (<bold>A</bold>)). (<bold>C</bold>) Example principal components analysis (PCA) projection of the moment-to-moment RNN activity into three dimensions (colormap indicates track position) for all three contexts (left) and for each pair of contexts (right). (<bold>D</bold>) Schematic: (Top) The hypothesized orthogonalization of the position and context input and output weights. (Bottom) Across maps, corresponding locations on the 1D track occupy a 2D remapping subspace in which the remapping dimensions between each pair are maximally separated (60°). (<bold>E</bold>) Total variance explained by the principal components for network-wide activity across maps (top four principal components, red points). (<bold>F</bold>) Normalized manifold misalignment scores between each pair of maps across all models (0, perfectly aligned; 1, p=0.25 of shuffle). (<bold>G</bold>) Cosine similarity between the latent state and position input and output weights onto the remap dimension (left) and the position subspace (right), defined for each pair of maps (error bars, sem; n=45 pairs, 15 models).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Manifold geometry generalizes for up to 10 latent states.</title><p>(<bold>A</bold>) (Left) Recurrent neural network (RNN) models were trained to navigate in 1D (velocity signal, top) and discriminate between many (2–10) distinct contexts (transient, binary latent state signal, bottom). (Right) Example showing high prediction performance for position (top) and context (bottom) for five latent states. (<bold>B</bold>) Position-binned activity for six example single RNN units, split by latent state (colors as in (<bold>A</bold>)). (<bold>C</bold>) Models accurately estimated position (top) and latent state (bottom) for different numbers of latent states or ‘maps.’ (<bold>D</bold>) Angle between remapping dimensions for different numbers of maps. The remapping dimensions were always maximally separated. (<bold>E</bold>) Manifold misalignment score for all pairs of maps across all models (0, perfectly aligned; 1, p=0.25 of shuffle). All pairs of manifolds were more aligned than expected by chance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig5-figsupp1-v1.tif"/></fig></fig-group><p>In <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, we show that RNNs are capable of solving this task with larger numbers of latent states (more than three; for simplicity, we consider up to 10 states). Furthermore, the RNN dynamics and geometry generalize accordingly: each latent state is associated with a different ring attractor and every pair of ring attractors is highly aligned. Motivated by these observations, we revisited a subset of experimental sessions from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> (N=4 sessions from 2 mice) that exhibited remapping between 3–4 stable maps of the same virtual track (<xref ref-type="fig" rid="fig6">Figure 6A, B</xref>; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) for a pilot comparison with the RNN models, which we hope will inspire future experimental analysis. As Low et al., we first confirmed that these remapping events did not reflect recording probe movement by comparing the waveforms from different maps across the session, which were highly stable (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, right; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Biological recordings with more than two maps recapitulate many geometric features of the recurrent neural network models.</title><p>(<bold>A - B</bold>) are modified from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>. (<bold>A</bold>) Schematic: Mice navigated virtual 1D circular-linear tracks with unchanging environmental cues and task conditions. Neuropixels recording probes were inserted during navigation. (<bold>B</bold>) Examples from the 3-map Session A. (Left/top) Network-wide trial-by-trial correlations for the spatial firing pattern of all co-recorded neurons in the same example session (color bar indicates correlation). (Left/bottom) k-means map assignments. (Middle) An example medial entorhinal cortex neuron switches between three maps of the same track (top, raster; bottom, average firing rate by position; teal, map 1; red, map 2; black, map 3). (Right) Overlay of average waveforms sampled from each of the three maps. (<bold>C</bold>) Principal components analysis (PCA) projection of the manifolds associated with each pair of maps from the 4-map Session D (color bar indicates virtual track position). (<bold>D</bold>) Normalized manifold misalignment scores between each pair of maps across all sessions (0, perfectly aligned; 1, p=0.25 of shuffle). (<bold>E</bold>) (Left) Schematic: maximal separation between all remapping dimensions for 3 and 4 maps. (Right) Angle between adjacent pairs of remapping dimensions for all sessions (dashes, ideal angle).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Medial entorhinal cortex can remap between 3 or 4 maps and many geometric features are preserved.</title><p>Experimental data for four example multi-map sessions from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>. (<bold>A</bold>) Examples from the 3-map Session A (n=142 cells). (A1) (Left) Network-wide trial-by-trial correlations for the spatial firing patterns of all co-recorded neurons in the same example session (color bar indicates correlation). (Left/bottom) k-means map assignments. (Right) Three example medial entorhinal cortex neurons switch between three maps of the same track (top, raster; bottom, average firing rate by position; teal, map 1; red, map 2; black, map 3). (<bold>A2</bold>) Comparisons of average spike waveforms sampled from each of the three maps (approximate sampling epochs indicated by black lines, A1 far right). (Left) Across map correlations for average spike waveforms from all cells (horizontal bar, median = 0.978; vertical line, 5<sup>th</sup> – 95<sup>th</sup> percentile, 5<sup>th</sup> percentile = 0.935). (Right) Overlay of average waveforms sampled from each map for the three example cells from (A2). (<bold>A3</bold>) Principal components analysis (PCA) projection of the manifolds associated with each pair of maps (color bar indicates virtual track position). Text: average normalized manifold misalignment score for all pairs of maps (0, perfectly aligned; 1, =0.25 of shuffle). (<bold>B–D</bold>) As in (<bold>A</bold>), but for the 3-map Session B (n=184 cells; median waveform correlation = 0.988), 4-map Session C (n=196 cells; median waveform correlation = 0.995), and 4-map Session D (n=162 cells; median waveform correlation = 0.99).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86943-fig6-figsupp1-v1.tif"/></fig></fig-group><p>To examine the manifold structure of the neural data, we projected the neural tuning curves associated with each pair of maps into the subspace defined by the first three principle components. In many sessions, population-wide activity across the two maps occupied distinct, qualitatively aligned rings, where the position along each ring corresponded to the position along the virtual track (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Procrustes analysis revealed that these pairs of maps were largely more aligned than expected by chance (<xref ref-type="fig" rid="fig6">Figure 6D</xref>; 13/18 map pairs more aligned than shuffle). Notably, five map pairs from one mouse (three in session A, two in session D) were not aligned (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, black and teal points), suggesting that manifold alignment does not always emerge in biological data. Finally, we asked whether the remapping dimensions from the biological sessions were organized symmetrically, as in the model (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, left). We found that there was a range of acute angles between pairs of remapping dimensions (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, right), suggesting that there was more asymmetry in the biological network geometry than in the model.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Previous experimental studies have found that neurons in the medial entorhinal cortex change their firing patterns in response to changes in task conditions, behavioral state, or visual and motor cues (<xref ref-type="bibr" rid="bib3">Bant et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Boccara et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Butler et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Campbell et al., 2021</xref>; <xref ref-type="bibr" rid="bib8">Campbell et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Hardcastle et al., 2017b</xref>). In virtual reality environments, these remapping events can recruit neurons across the entorhinal cortical circuit to rapidly switch between distinct maps of the same track (<xref ref-type="bibr" rid="bib9">Campbell et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). Here, we used RNN models to explore a normative hypothesis that these remapping dynamics reflect hidden state inference (<xref ref-type="bibr" rid="bib10">Colgin et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Sanders et al., 2020</xref>). We showed that RNNs initialized from random synaptic weights recapitulate the essential features of biological data—aligned ring manifolds (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>)—when trained to remember a binary latent state variable and to simultaneously integrate a velocity signal in a circular environment. RNNs learn to represent positional and state change information in orthogonal subspaces such that navigation and latent state inference co-occur without interference. Furthermore, we demonstrated that the geometry and algorithmic principles of this solution readily generalize to more complex tasks including navigation in 2D environments and tasks involving three or more latent states. These findings provide a jumping-off point for new analyses of remapping in neural data, which we demonstrated in a pilot analysis of neural data from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>.</p><p>These results complement an existing body of theoretical and experimental work on the neural basis of navigation. <xref ref-type="bibr" rid="bib19">Fenton et al., 2010</xref> proposed that the hippocampus constructs multiple spatial maps that are anchored to different landmarks; when the animal’s attention switches between these reference points, hippocampal cells remap (see also <xref ref-type="bibr" rid="bib35">Kubie et al., 2020</xref>). This proposal is consistent with the idea that the hippocampal circuit groups navigational episodes into discrete categories by combining internal context with external landmarks (<xref ref-type="bibr" rid="bib10">Colgin et al., 2008</xref>; <xref ref-type="bibr" rid="bib20">Fuhs and Touretzky, 2007</xref>; <xref ref-type="bibr" rid="bib48">Sanders et al., 2020</xref>). Related experimental work demonstrates that an animal’s prior experience with an environment can shape how the hippocampus delineates these categories (<xref ref-type="bibr" rid="bib44">Plitt and Giocomo, 2021</xref>). Each of these hypotheses can be seen as layering a discrete latent variable (e.g. changes in reference landmarks, task context, or prior experience) on top of a distributed neural code of position, which are the essential ingredients of our RNN task. While we draw explicit comparisons with spontaneous remapping in the entorhinal cortex (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>), <xref ref-type="bibr" rid="bib51">Sheintuch et al., 2020</xref> reported similar experimental findings in the hippocampus, highlighting the broad relevance of this remapping phenomenon and our modeling efforts. We explored these topics in a general modeling framework applicable to any circuit that supports navigation through physical space and even navigation of abstract cognitive spaces (<xref ref-type="bibr" rid="bib2">Aronov et al., 2017</xref>; <xref ref-type="bibr" rid="bib11">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Whittington et al., 2020</xref>).</p><p>In our task, discrete switches in the latent state are signaled by brief impulses to the RNN, such that the navigational circuit must maintain a persistent representation of the latent state based on these transient cues. This simple task design allowed us to clearly identify the minimal set of constraints that produces aligned ring attractors. In particular, our results suggest that aligned ring attractors could emerge even if upstream circuits trigger latent state changes and signal these changes to downstream navigational circuits. Indeed, Low et al. found that remapping was correlated with brief decreases in running speed (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>), suggesting that this temporary behavioral state change—which is known to have widespread impacts on global brain activity (<xref ref-type="bibr" rid="bib54">Stringer et al., 2019</xref>)—may serve as an external trigger of remapping in the entorhinal cortex. Extensions to our task could build on this basic framework by asking the network to infer state changes given a noisy input or using a more complex interaction with the environment (e.g. through reinforcement learning paradigms (<xref ref-type="bibr" rid="bib56">Uria et al., 2020</xref>).</p><p>The mechanisms of remapping in biological circuits are still poorly understood, but have been modeled using multistable attractor dynamics for several decades (<xref ref-type="bibr" rid="bib47">Samsonovich and McNaughton, 1997</xref>). Classically, these models were engineered and hand-tuned to produce the desired attractor dynamics. In contrast, RNN models are indirectly engineered by specifying task constraints and a learning algorithm (<xref ref-type="bibr" rid="bib61">Yang and Wang, 2021</xref>). Thus, our observation that trained RNNs produce multistable attractor manifolds is nontrivial because different solutions might have, in principle, emerged. Despite this key similarity, there are notable differences between our models and classical multistable attractor models. Classical models typically store completely decorrelated spatial maps (<xref ref-type="bibr" rid="bib47">Samsonovich and McNaughton, 1997</xref>), while our RNNs produce distinct maps that are, by construction, perfectly correlated in the position readout dimensions. <xref ref-type="bibr" rid="bib45">Romani and Tsodyks, 2010</xref> studied the effects of adding correlation to spatial maps in forward-engineered multistable attractor networks, as did <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>. Fundamentally, these and other forward-engineered models provide insights into <italic>how</italic> neural circuits may remap, but do not answer <italic>why</italic> they do so. We investigated the latter question in this work by identifying a minimal set of task constraints that provide a putative explanation for why the entorhinal cortex spontaneously remaps.</p><p>Other work has studied remapping in trained artificial networks performing navigation (<xref ref-type="bibr" rid="bib50">Schøyen et al., 2022</xref>; <xref ref-type="bibr" rid="bib56">Uria et al., 2020</xref>). Unlike our results, these papers typically consider remapping across different physical environments. <xref ref-type="bibr" rid="bib58">Whittington et al., 2020</xref> propose a normative model and a neural circuit that supports non-spatial remapping, which is perhaps most similar to the task constraints we studied. However, our investigation focused on a simpler and more targeted computational task to draw a tighter link to a specific biological finding and to perform a deeper examination of the resulting population geometry and dynamical structure.</p><p>While we were motivated to study remapping in the specific context of navigational circuits, our results have broader implications for understanding how RNNs perform complex, context-dependent computations. This topic has attracted significant interest. For example, RNNs trained in many computational tasks develop modular neural populations and dynamical motifs that are re-used across tasks (<xref ref-type="bibr" rid="bib17">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Yang et al., 2019</xref>). When RNN architecture is explicitly designed to include dedicated neural subpopulations, these subpopulations can improve model performance on particular types of tasks (<xref ref-type="bibr" rid="bib4">Beiran et al., 2021</xref>; <xref ref-type="bibr" rid="bib18">Dubreuil et al., 2022</xref>). Thus, there is an emerging conclusion that RNNs use simple dynamical motifs as building blocks for more general and complex computations, which our results support. In particular, aligned ring attractors are a recurring, dynamical motif in our results, appearing first in a simple task setting (two maps of a 1D environment) and subsequently as a component of RNN dynamics in more complex settings (e.g. as sub-manifolds of toroidal attractors in a 2D environment, see <xref ref-type="fig" rid="fig4">Figure 4</xref>). We can, therefore, conceptualize a pair of aligned ring manifolds as a dynamical ‘building block’ that RNNs utilize to solve higher-dimensional generalizations of the task. Intriguingly, our novel analysis of neural data from <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> revealed that similar principles may hold in biological circuits—when three or more spatial maps were present in a recording, the pairs of ring manifolds tended to be aligned.</p><p>Ultimately, our model provides a strong foundation for future experimental investigations of the functional role of remapping in navigational circuits. Our findings suggest that latent state changes can drive remapping; an experimental task that explicitly requires animals to report a latent internal state would provide substantial insight into this hypothesis. We also identify concrete predictions for how the representational geometry of neural populations generalizes from the dynamics found in 1D virtual reality environments (<xref ref-type="bibr" rid="bib9">Campbell et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>) to more complex settings. We found direct support for one of these predictions by re-analyzing an existing experimental dataset. Our work, therefore, provides a parsimonious, plausible, and testable model for the neural population geometry of remapping navigational circuits under a variety of task conditions.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">SciPy ecosystem of open-source Python libraries</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib27">Harris et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Hunter, 2007</xref>; <xref ref-type="bibr" rid="bib31">Jones et al., 2001</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://www.scipy.org/">https://www.scipy.org/</ext-link></td><td align="left" valign="bottom">libraries include numpy, matplotlib, scipy, etc.</td></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">scikit-learn</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib42">Pedregosa et al., 2012</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">PyTorch</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib41">Paszke et al., 2019</xref></td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://pytorch.org/docs/stable/nn.html">https://pytorch.org/docs/stable/nn.html</ext-link></td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Resource availability</title><sec id="s4-1-1"><title>Lead contact</title><p>Further information and requests for resources and reagents should be directed to and will be fulfilled by the Lead Contact, Alex H. Williams (alex.h.williams@nyu.edu).</p></sec><sec id="s4-1-2"><title>Materials availability</title><p>This study did not generate new unique reagents.</p></sec></sec><sec id="s4-2"><title>Experimental model and subject details</title><sec id="s4-2-1"><title>RNN model and training procedure</title><p>We examined Elman RNNs (‘vanilla’ RNNs), which are perhaps the simplest RNN architecture capable of theoretically representing any nonlinear dynamical system (<xref ref-type="bibr" rid="bib24">Hammer, 2000</xref>) and which can be viewed as an approximation to continuous time firing rate models of neural circuits (<xref ref-type="bibr" rid="bib53">Song et al., 2016</xref>). At each time index <inline-formula><mml:math id="inf62"><mml:mi>t</mml:mi><mml:mi> </mml:mi><mml:mo>∈</mml:mo><mml:mi> </mml:mi><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> the activation vector of <inline-formula><mml:math id="inf63"><mml:mi>N</mml:mi></mml:math></inline-formula> hidden units is denoted by <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Loosely, we can think of <inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the firing rates of <inline-formula><mml:math id="inf66"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons in a biological circuit at time <inline-formula><mml:math id="inf67"><mml:mi>t</mml:mi></mml:math></inline-formula>. The activation vector is updated according to:<disp-formula id="equ2"><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf68"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> denotes a rectifying linear unit function (i.e. an element-wise maximum between the vector <inline-formula><mml:math id="inf69"><mml:mi>x</mml:mi></mml:math></inline-formula> and a vector of zeros), <inline-formula><mml:math id="inf70"><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a matrix holding the recurrent synaptic connection weights, <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a vector of input signals at time <inline-formula><mml:math id="inf72"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf73"><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a matrix holding the input connection weights, and <inline-formula><mml:math id="inf74"><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a vector holding bias terms for each hidden unit. The output of the network at time <inline-formula><mml:math id="inf75"><mml:mi>t</mml:mi></mml:math></inline-formula> is defined by:<disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo></mml:math></inline-formula><inline-formula><mml:math id="inf77"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a vector of <inline-formula><mml:math id="inf78"><mml:mi>L</mml:mi></mml:math></inline-formula> output units, <inline-formula><mml:math id="inf79"><mml:msup><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a matrix holding output connection weights, and <inline-formula><mml:math id="inf80"><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a vector holding bias terms for each output unit. Finally, the initial condition <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for each dynamical sequence was set by:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a vector of <inline-formula><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> inputs used to define the initial condition, <inline-formula><mml:math id="inf84"><mml:msup><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> is a matrix holding connection weights, and <inline-formula><mml:math id="inf85"><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> is a vector holding bias terms. The connection weights were randomly initialized from the uniform distribution over <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is the default initialization scheme in PyTorch. As described below, the vector <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is used to define the initial position on the circular track, which is randomized in each trial. Altogether, these equations define an RNN model with trainable parameters <inline-formula><mml:math id="inf88"><mml:mo>{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>.</p><p>The number of inputs, <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and outputs, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, varied depending on the computational task the RNN was trained to perform. Specifically, <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by the number of latent states (‘contexts’) plus the number of spatial dimensions. Thus, for the 1D navigation task with binary state cues diagrammed in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, the number of network inputs was <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (whereas the tasks diagrammed in <xref ref-type="fig" rid="fig4">Figure 4A</xref> and <xref ref-type="fig" rid="fig5">Figure 5A</xref> each have <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> inputs). The number of network outputs, <inline-formula><mml:math id="inf94"><mml:mi>L</mml:mi></mml:math></inline-formula>, is given by the number of latent states plus two times the number of spatial dimensions. Thus, for the task diagrammed in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, <inline-formula><mml:math id="inf95"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:math></inline-formula> (for <xref ref-type="fig" rid="fig4">Figure 4A</xref>, <inline-formula><mml:math id="inf96"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:math></inline-formula>; for <xref ref-type="fig" rid="fig5">Figure 5A</xref>, <inline-formula><mml:math id="inf97"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula>). The additional spatial output dimensions can be understood as follows: Due to the periodic boundary conditions, the network must output a predicted spatial position <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> for each spatial dimension. Predicting this raw angular position would require the network to implement something akin to an <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>arctan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> function. Because this function is highly nonlinear and discontinuous, the linear readout layer of the RNN will struggle to predict <inline-formula><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> directly in a fashion that is numerically stable. We therefore trained the networks to predict <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each spatial dimension, which requires an extra factor of two spatial output dimensions. Similarly, for the initial condition <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the number of input variables <inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is given by two times the number of spatial dimensions, and the input vector <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is formed by concatenating <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each spatial dimension.</p><p>The network was trained by randomly generated input sequences with a ground truth target output. The input vector at each time step, <inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, contained the angular velocity along each spatial dimension as well as state change cues (see schematic in <xref ref-type="fig" rid="fig1">Figure 1E</xref>). The output vector at each time step, <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, contains disjoint dimensions that predict the spatial position and the latent state or ‘context’ (see schematic in <xref ref-type="fig" rid="fig1">Figure 1F</xref>). For each sequence, the overall loss function is a sum of two terms: (<italic>i</italic>) the mean-squared-error between the ground truth sine and cosine of angular position, <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the network’s prediction of these terms, and (<italic>ii</italic>) the cross-entropy of the true latent state and the network’s prediction (see <bold>torch.nn.CrossEntropyLoss</bold> class in the PyTorch library <xref ref-type="bibr" rid="bib41">Paszke et al., 2019</xref>).</p><p>We trained networks with <inline-formula><mml:math id="inf112"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>248</mml:mn></mml:math></inline-formula> hidden units using stochastic gradient descent with a batch size of 124 sequences and gradient clipping (gradient norm clipped to be less than or equal to 2). At the beginning of training, we trained RNNs on sequence lengths of <inline-formula><mml:math id="inf113"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and increased the sequence length by one every 50 parameter updates. We performed 30,000 parameter updates, so that by the end of training the RNNs were training on sequence lengths of <inline-formula><mml:math id="inf114"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>600</mml:mn></mml:math></inline-formula>. We found that this gradual increase in task complexity along with gradient clipping was necessary to achieve good performance. Intuitively, training on short sequences at the beginning helps the network learn suitable parameter values for <inline-formula><mml:math id="inf115"><mml:mo>{</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> without worrying about the typical challenges (e.g. exploding and vanishing gradients) associated with RNN training. Then, the remaining parameters <inline-formula><mml:math id="inf116"><mml:mo>{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> can be fine-tuned with gradually increasing sequence lengths.</p><p>Each sequence was randomly generated. For each spatial dimension, the initial angular position, <inline-formula><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, was sampled uniformly between <inline-formula><mml:math id="inf118"><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The angular velocity at each time step was given by <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the mean velocity and <inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was sampled randomly from a normal distribution with a mean of zero and a standard deviation of 0.3 radians. For each sequence the mean velocity, <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mstyle></mml:math></inline-formula>, was sampled from a normal distribution with a mean of zero and a standard deviation of 0.1 radians. The initial latent state was chosen randomly from the available states. State change cues occurred randomly according to a homogeneous Poisson process with an expected rate of one state change per 50-time steps. In <xref ref-type="fig" rid="fig5">Figure 5</xref>, we trained networks to switch between three or more states—for each state change one of the inactive states was chosen uniformly at random to be the new active state. State changes were cued by a pulse lasting two-time steps.</p><p>For <xref ref-type="fig" rid="fig1">Figure 1G</xref>, the trained model was provided velocity inputs with an initial position of <inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi> </mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and non-negative angular velocity at each time step, <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:munder><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:munder><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, so that the RNN output would follow a trial structure comparable to the biological data. Similarly, state changes occurred less frequently (at an expected rate of once per 500-time steps) to better match the biologically observed remapping rate. For comparison with the biological data, we truncated each sequence to remove incomplete track traversals and concatenated 50 sequences into a single session. For visualization purposes, we computed the smoothed, position-binned (n bins = 50) firing rates for 5 example units and labeled each track traversal according to the most commonly reported latent state for that traversal.</p></sec><sec id="s4-2-2"><title>Mice</title><p>All experimental data reported here were collected for a previous publication by <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>, and were approved by the Institutional Animal Care and Use Committee at Stanford University School of Medicine. More information on data collection and analyses can be found in Method Details, below, and in the Methods section of <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>.</p></sec></sec><sec id="s4-3"><title>Method details</title><sec id="s4-3-1"><title>Manifold geometry analyses</title><p>We used Procrustes shape analysis (<xref ref-type="bibr" rid="bib22">Gower and Dijksterhuis, 2004</xref>) according to the methods described by <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> to determine the degree to which manifolds from different maps were aligned in the high-dimensional activity space. Briefly, we divided the track into 50 position bins and computed the average activity for all units within each position bin for each latent state to obtain an estimate of the manifold associated with each map. We then mean-centered these manifolds and rescaled them to have unit norms. We compute the root-mean-squared error (RMSE) between these two manifolds (the ‘observed’ RMSE). We then find the rotation matrix that optimally aligns the two manifolds and calculate the RMSE between the optimally aligned manifolds. We report the observed RMSE relative to the RMSE after optimal (misalignment = 0) and random (misalignment = 1) rotation. For <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, which had more than two latent states, we computed this score for all pairs of manifolds.</p><p>In <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig3">3</xref>—<xref ref-type="fig" rid="fig5">5</xref> we consider the network in terms of subspaces tuned to the two task components—position and latent state. We define the ‘position subspace’ as a two-dimensional subspace containing the position-binned average firing rates of all units. We divided the track into 250 position bins and computed the average activity for all units within each position bin for each latent state. To find the position subspace across maps—as in <xref ref-type="fig" rid="fig1">Figure 1K</xref>, <xref ref-type="fig" rid="fig3">Figure 3E</xref>, <xref ref-type="fig" rid="fig4">Figure 4G</xref>, and <xref ref-type="fig" rid="fig5">Figure 5G</xref> —we performed a 2-factor Principal Components Analysis (PCA) on the position-binned activity across both latent states. To find the position subspace for a single map—as in <xref ref-type="fig" rid="fig3">Figure 3G</xref> —we performed 2-factor PCA on the average activity from just one latent state. We define the ‘remapping dimension’ as the dimension separating the manifold centroids, which we find by computing the average activity for each unit within each map and taking the difference across the two maps.</p><p>In <xref ref-type="fig" rid="fig1">Figures 1K</xref>, <xref ref-type="fig" rid="fig4">4G</xref> and <xref ref-type="fig" rid="fig5">5G</xref>, and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>, we calculate the angles between the input and output weights and the position subspace or remapping dimension. To find this angle, we calculated the cosine similarity between each weight vector and each subspace. Cosine similarity of 0 indicates that the weights were orthogonal to the subspace, while a similarity of 1 indicates that the weight vector was contained within the subspace.</p></sec><sec id="s4-3-2"><title>Fixed point analysis</title><p>We numerically identified fixed points according to the methods described in <xref ref-type="bibr" rid="bib55">Sussillo and Barak, 2013</xref> Briefly, we used stochastic gradient descent to minimize <inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> over hidden layer activation vectors <inline-formula><mml:math id="inf126"><mml:mi>x</mml:mi></mml:math></inline-formula>. Values of <inline-formula><mml:math id="inf127"><mml:mi>x</mml:mi></mml:math></inline-formula> that minimize the expression close to zero, correspond to approximate fixed points of the recurrent RNN dynamics when the input is held constant <inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. At each numerical fixed point <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, we use standard autodifferentiation tools in PyTorch to compute the <inline-formula><mml:math id="inf130"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> Jacobian matrix <inline-formula><mml:math id="inf131"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> evaluated at <inline-formula><mml:math id="inf132"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . The eigenvalues and eigenvectors of this matrix then provide a local linear dynamical approximation to the full system as explained in <xref ref-type="bibr" rid="bib55">Sussillo and Barak, 2013</xref> and in the main text.</p></sec><sec id="s4-3-3"><title>Single unit analysis</title><p>To characterize single unit remapping properties for <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>, we performed the rate remapping versus global remapping analysis described in <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> For each model, we computed the average firing rate for all units in each map, smoothing with a Gaussian filter (standard deviation, two position bins). We then calculated the percent change in peak firing rate (i.e. rate remapping). To compute a spatial dissimilarity score (i.e. global remapping), we subtracted 1 from the cosine similarity between firing rate vectors (a dissimilarity score of 0 indicates identical spatial firing, and 1 indicates orthogonal spatial representations).</p></sec><sec id="s4-3-4"><title>Experimental data</title><p>The experimental data included in <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig6">Figure 6</xref>, and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> were collected for a previous publication (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). Briefly, mice were trained to navigate a 1D virtual reality track with tower landmarks and floor cues to provide optic flow. The landmarks repeated seamlessly every 400 cm such that the track was circular-linear. The mice received randomly distributed, visually cued rewards within the middle 300 cm of the track. During behavior, neural activity was recorded using Neuropixels 1.0 silicon recording probes (<xref ref-type="bibr" rid="bib32">Jun et al., 2017</xref>), which were acutely inserted into the medial entorhinal cortex. Behavioral and neural data were processed as described by <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>; <xref ref-type="fig" rid="fig1">Figure 1A–D</xref> are modified from Low et al.</p><p>The pilot analyses in <xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> are performed on a subset of the data from Low et al. (n=684 cells from four sessions in two mice) (<xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>). As described in that publication, we used k-means clustering to divide these sessions into 3 or 4 maps. We then assessed the trial-by-trial spatial stability of the population-wide neural activity within each map in order to restrict our analysis to stable trials. We divided the session according to the k-means map labels and computed the Pearson correlation between the position-binned firing rates (n bins = 80) of all neurons for each pair of trials within each map. We excluded trials that were spatially unstable from our analysis (average correlation with all other trials &lt;0.25). (We performed the same analysis of trial-by-trial spatial stability to obtain the similarity matrices in <xref ref-type="fig" rid="fig1">Figure 1C and G</xref>).</p><p>To assess the geometry of the neural population activity, we used the k-means cluster centroids as an estimate for the neural activity manifold associated with each map. We then performed Procrustes shape analysis to assess manifold alignment and identified the remapping dimensions, as described above.</p><p>To ensure that remapping was not an artifact of probe movement or multi-unit activity, we compared the spike waveforms for all cells across remapping events, as described in <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref> Briefly, we identified the longest epoch of trials for each map and extracted waveforms for 100 spikes for each cell from each epoch. We then computed the average waveforms within each epoch. To determine waveform similarity, we computed the Pearson correlation between the vectorized average waveforms for each pair of maps and then calculated the average correlation across pairs. For all waveform analyses, we used waveforms from the 20 channels closest to the Kilosort2-identified depth for each cell.</p><p><xref ref-type="fig" rid="fig1">Figure 1A–D</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, and <xref ref-type="fig" rid="fig6">Figure 6A–B</xref> are modified from the following figure panels in <xref ref-type="bibr" rid="bib37">Low et al., 2021</xref>: the graphical abstract (left/middle panel—schematic of entorhinal neural activity), Figure 1A, Figure 4B, Figure 7E (left), Figure S1H (middle right), and Figure S5A (left) and B (top). These figures were originally published under an Elsevier user license. The copyright holder has granted permission to publish under a CC BY 4.0 license.</p></sec></sec><sec id="s4-4"><title>Quantification and statistical analysis</title><sec id="s4-4-1"><title>Statistics</title><p>All data were analyzed in Python, using the scipy stats library to compute statistics. Unless otherwise noted, all tests are two-sided, correlation coefficients represent Pearson’s correlation, and values are presented as mean ± standard error of the mean (SEM). Statistical tests are listed following the relevant result given in the Results, figure legend, or Method Details. Unless otherwise stated, p&lt;0.05 was taken as the criterion for significance.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86943-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>No new data were generated for this manuscript, as it is a computational study. Code to train the RNN models and reproduce the figures of the paper are provided in a GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/ahwillia/rnn_remapping_paper">repository</ext-link> (copy archived at <xref ref-type="bibr" rid="bib59">Williams and Low, 2023</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Giocomo</surname><given-names>LM</given-names></name><name><surname>Low</surname><given-names>IC</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Low et al. (2021) Dynamic and reversible remapping of network representations in an unchanging environment. Neuron</data-title><source>Mendeley Data</source><pub-id pub-id-type="doi">10.17632/hntn6m2pgk.1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Dmitriy Aronov and Selmaan Chettih for providing feedback on the manuscript. We thank Scott Linderman, members of the Aronov Lab, members of the Giocomo Lab, and members of the Williams Lab for discussions and feedback. This work was supported by funding from the Wu Tsai Neurosciences Institute under Stanford Interdisciplinary Graduate Fellowships (to IICL); the Office of Naval Research (N00141812690), the Simons Foundation (SCGB 542987SPI), NIMH (1R01MH126904-01A1 and U19NS118284), the Vallee Foundation, and the James S McDonnell Foundation (to LMG); and the Simons Foundation (to AHW).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>WE</given-names></name><name><surname>Chen</surname><given-names>MZ</given-names></name><name><surname>Pichamoorthy</surname><given-names>N</given-names></name><name><surname>Tien</surname><given-names>RH</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Thirst regulates motivated behavior through modulation of brainwide neural population dynamics</article-title><source>Science</source><volume>364</volume><elocation-id>aav3932</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav3932</pub-id><pub-id pub-id-type="pmid">30948440</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Nevers</surname><given-names>R</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping of a non-spatial dimension by the hippocampal-entorhinal circuit</article-title><source>Nature</source><volume>543</volume><fpage>719</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1038/nature21692</pub-id><pub-id pub-id-type="pmid">28358077</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bant</surname><given-names>JS</given-names></name><name><surname>Hardcastle</surname><given-names>K</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Topography in the bursting dynamics of entorhinal neurons</article-title><source>Cell Reports</source><volume>30</volume><fpage>2349</fpage><lpage>2359</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2020.01.057</pub-id><pub-id pub-id-type="pmid">32075768</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Shaping dynamics with multiple populations in low-rank recurrent networks</article-title><source>Neural Computation</source><volume>33</volume><fpage>1572</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01381</pub-id><pub-id pub-id-type="pmid">34496384</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bennett</surname><given-names>C</given-names></name><name><surname>Arroyo</surname><given-names>S</given-names></name><name><surname>Hestrin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Subthreshold mechanisms underlying state-dependent modulation of visual responses</article-title><source>Neuron</source><volume>80</volume><fpage>350</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.08.007</pub-id><pub-id pub-id-type="pmid">24139040</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boccara</surname><given-names>CN</given-names></name><name><surname>Nardin</surname><given-names>M</given-names></name><name><surname>Stella</surname><given-names>F</given-names></name><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The entorhinal cognitive map is attracted to goals</article-title><source>Science</source><volume>363</volume><fpage>1443</fpage><lpage>1447</lpage><pub-id pub-id-type="doi">10.1126/science.aav4837</pub-id><pub-id pub-id-type="pmid">30923221</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>WN</given-names></name><name><surname>Hardcastle</surname><given-names>K</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Remembered reward locations restructure entorhinal spatial maps</article-title><source>Science</source><volume>363</volume><fpage>1447</fpage><lpage>1452</lpage><pub-id pub-id-type="doi">10.1126/science.aav5297</pub-id><pub-id pub-id-type="pmid">30923222</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>MG</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Mallory</surname><given-names>CS</given-names></name><name><surname>Low</surname><given-names>IIC</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Principles governing the integration of landmark and self-motion cues in entorhinal cortical codes for navigation</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1096</fpage><lpage>1106</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0189-y</pub-id><pub-id pub-id-type="pmid">30038279</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>MG</given-names></name><name><surname>Attinger</surname><given-names>A</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Distance-tuned neurons drive specialized path integration calculations in medial entorhinal cortex</article-title><source>Cell Reports</source><volume>36</volume><elocation-id>109669</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109669</pub-id><pub-id pub-id-type="pmid">34496249</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colgin</surname><given-names>LL</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Understanding memory through hippocampal remapping</article-title><source>Trends in Neurosciences</source><volume>31</volume><fpage>469</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.06.008</pub-id><pub-id pub-id-type="pmid">18687478</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname><given-names>AO</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title><source>Science</source><volume>352</volume><fpage>1464</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id><pub-id pub-id-type="pmid">27313047</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Snyder</surname><given-names>AC</given-names></name><name><surname>Acar</surname><given-names>K</given-names></name><name><surname>Williamson</surname><given-names>RC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Slow drift of neural activity as a signature of impulsivity in macaque visual and prefrontal cortex</article-title><source>Neuron</source><volume>108</volume><fpage>551</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.021</pub-id><pub-id pub-id-type="pmid">32810433</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.07770">https://arxiv.org/abs/1803.07770</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Wang</surname><given-names>PY</given-names></name><name><surname>Chin</surname><given-names>M</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.10189">https://arxiv.org/abs/1912.10189</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Ardalan</surname><given-names>A</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Qian</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Recurrent neural network models for working memory of continuous variables: activity manifolds, connectivity patterns, and dynamic codes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2111.01275">https://arxiv.org/abs/2111.01275</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diehl</surname><given-names>GW</given-names></name><name><surname>Hon</surname><given-names>OJ</given-names></name><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Grid and nongrid cells in medial entorhinal cortex represent spatial location and environmental features with complementary coding schemes</article-title><source>Neuron</source><volume>94</volume><fpage>83</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.004</pub-id><pub-id pub-id-type="pmid">28343867</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.08.15.503870</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The role of population structure in computations through neural dynamics</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>783</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01088-4</pub-id><pub-id pub-id-type="pmid">35668174</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenton</surname><given-names>AA</given-names></name><name><surname>Lytton</surname><given-names>WW</given-names></name><name><surname>Barry</surname><given-names>JM</given-names></name><name><surname>Lenck-Santini</surname><given-names>P-P</given-names></name><name><surname>Zinyuk</surname><given-names>LE</given-names></name><name><surname>Kubík</surname><given-names>S</given-names></name><name><surname>Bures</surname><given-names>J</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Olypher</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention-like modulation of hippocampus place cell discharge</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>4613</fpage><lpage>4625</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5576-09.2010</pub-id><pub-id pub-id-type="pmid">20357112</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhs</surname><given-names>MC</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Context learning in the rodent hippocampus</article-title><source>Neural Computation</source><volume>19</volume><fpage>3173</fpage><lpage>3215</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.12.3173</pub-id><pub-id pub-id-type="pmid">17970649</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gil</surname><given-names>M</given-names></name><name><surname>Ancau</surname><given-names>M</given-names></name><name><surname>Schlesiger</surname><given-names>MI</given-names></name><name><surname>Neitz</surname><given-names>A</given-names></name><name><surname>Allen</surname><given-names>K</given-names></name><name><surname>De Marco</surname><given-names>RJ</given-names></name><name><surname>Monyer</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Impaired path integration in mice with disrupted grid cell firing</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>81</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0039-3</pub-id><pub-id pub-id-type="pmid">29230055</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gower</surname><given-names>JC</given-names></name><name><surname>Dijksterhuis</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Procrustes Problems</source><publisher-name>OUP Oxford</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198510581.001.0001</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammer</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>On the approximation capability of recurrent neural networks</article-title><source>Neurocomputing</source><volume>31</volume><fpage>107</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/S0925-2312(99)00174-5</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardcastle</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Cell types for our sense of location: where we are and where we are going</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1474</fpage><lpage>1482</lpage><pub-id pub-id-type="doi">10.1038/nn.4654</pub-id><pub-id pub-id-type="pmid">29073649</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardcastle</surname><given-names>K</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>A multiplexed, heterogeneous, and adaptive code for navigation in medial entorhinal cortex</article-title><source>Neuron</source><volume>94</volume><fpage>375</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.025</pub-id><pub-id pub-id-type="pmid">28392071</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with NumPy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Høydal</surname><given-names>ØA</given-names></name><name><surname>Skytøen</surname><given-names>ER</given-names></name><name><surname>Andersson</surname><given-names>SO</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Object-vector coding in the medial entorhinal cortex</article-title><source>Nature</source><volume>568</volume><fpage>400</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1077-7</pub-id><pub-id pub-id-type="pmid">30944479</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulse</surname><given-names>BK</given-names></name><name><surname>Lubenov</surname><given-names>EV</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brain state dependence of hippocampal subthreshold activity in awake mice</article-title><source>Cell Reports</source><volume>18</volume><fpage>136</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.11.084</pub-id><pub-id pub-id-type="pmid">28052244</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: A 2d Graphics environment 9:90–95</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Oliphant</surname><given-names>T</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><data-title>Scipy: open source scientific tools for python</data-title><source>SciPy</source></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cortical activity in the null space: permitting preparation without movement</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1038/nn.3643</pub-id><pub-id pub-id-type="pmid">24487233</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kentros</surname><given-names>CG</given-names></name><name><surname>Agnihotri</surname><given-names>NT</given-names></name><name><surname>Streater</surname><given-names>S</given-names></name><name><surname>Hawkins</surname><given-names>RD</given-names></name><name><surname>Kandel</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Increased attention to spatial context increases both place field stability and spatial memory</article-title><source>Neuron</source><volume>42</volume><fpage>283</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(04)00192-8</pub-id><pub-id pub-id-type="pmid">15091343</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname><given-names>JL</given-names></name><name><surname>Levy</surname><given-names>ERJ</given-names></name><name><surname>Fenton</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Is hippocampal remapping the physiological basis for context</article-title><source>Hippocampus</source><volume>30</volume><fpage>851</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1002/hipo.23160</pub-id><pub-id pub-id-type="pmid">31571314</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Levenstein</surname><given-names>D</given-names></name><name><surname>Alvarez</surname><given-names>VA</given-names></name><name><surname>Amarasingham</surname><given-names>A</given-names></name><name><surname>Azab</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>ZS</given-names></name><name><surname>Gerkin</surname><given-names>RC</given-names></name><name><surname>Hasenstaub</surname><given-names>A</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Jolivet</surname><given-names>RB</given-names></name><name><surname>Marzen</surname><given-names>S</given-names></name><name><surname>Monaco</surname><given-names>JD</given-names></name><name><surname>Prinz</surname><given-names>AA</given-names></name><name><surname>Quraishi</surname><given-names>S</given-names></name><name><surname>Santamaria</surname><given-names>F</given-names></name><name><surname>Shivkumar</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>MF</given-names></name><name><surname>Traub</surname><given-names>R</given-names></name><name><surname>Rotstein</surname><given-names>HG</given-names></name><name><surname>Nadim</surname><given-names>F</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On the role of theory and modeling in neuroscience</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.13825">https://arxiv.org/abs/2003.13825</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Low</surname><given-names>IIC</given-names></name><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Campbell</surname><given-names>MG</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dynamic and reversible remapping of network representations in an unchanging environment</article-title><source>Neuron</source><volume>109</volume><fpage>2967</fpage><lpage>2980</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.005</pub-id><pub-id pub-id-type="pmid">34363753</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics</article-title><source>Neural Information Processing Systems</source><volume>32</volume><fpage>15696</fpage><lpage>15705</lpage><pub-id pub-id-type="pmid">32782423</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Roudi</surname><given-names>Y</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Kentros</surname><given-names>C</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Grid cells and cortical representation</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>466</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1038/nrn3766</pub-id><pub-id pub-id-type="pmid">24917300</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Köpf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: An imperative style, high-performance deep learning library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Müller</surname><given-names>A</given-names></name><name><surname>Nothman</surname><given-names>J</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Brucher</surname><given-names>M</given-names></name><name><surname>Perrot</surname><given-names>M</given-names></name><name><surname>Duchesnay</surname><given-names>É</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Scikit-Learn: Machine Learning in Python</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1201.0490">https://arxiv.org/abs/1201.0490</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pettit</surname><given-names>NL</given-names></name><name><surname>Yuan</surname><given-names>XC</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Hippocampal place codes are gated by behavioral engagement</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>561</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01050-4</pub-id><pub-id pub-id-type="pmid">35449355</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plitt</surname><given-names>MH</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Experience-dependent contextual codes in the hippocampus</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>705</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00816-6</pub-id><pub-id pub-id-type="pmid">33753945</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Continuous attractors with morphed/correlated maps</article-title><source>PLOS Computational Biology</source><volume>6</volume><elocation-id>e1000869</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000869</pub-id><pub-id pub-id-type="pmid">20700490</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname><given-names>ME</given-names></name><name><surname>Loback</surname><given-names>AR</given-names></name><name><surname>Raman</surname><given-names>DV</given-names></name><name><surname>Driscoll</surname><given-names>LN</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>O’Leary</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stable task information from an unstable neural population. bioRxiv</article-title><source>eLife</source><volume>9</volume><elocation-id>e51121</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51121</pub-id><pub-id pub-id-type="pmid">32660692</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samsonovich</surname><given-names>A</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Path integration and cognitive mapping in a continuous attractor neural network model</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>5900</fpage><lpage>5920</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-15-05900.1997</pub-id><pub-id pub-id-type="pmid">9221787</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>H</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hippocampal remapping as hidden state inference</article-title><source>eLife</source><volume>9</volume><elocation-id>e51140</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51140</pub-id><pub-id pub-id-type="pmid">32515352</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargolini</surname><given-names>F</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title><source>Science</source><volume>312</volume><fpage>758</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1126/science.1125572</pub-id><pub-id pub-id-type="pmid">16675704</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schøyen</surname><given-names>VS</given-names></name><name><surname>Pettersen</surname><given-names>MB</given-names></name><name><surname>Holzhausen</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Navigating multiple environments with emergent grid cell remapping</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.08.18.504379</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheintuch</surname><given-names>L</given-names></name><name><surname>Geva</surname><given-names>N</given-names></name><name><surname>Baumer</surname><given-names>H</given-names></name><name><surname>Rechavi</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multiple maps of the same spatial context can stably coexist in the mouse hippocampus</article-title><source>Current Biology</source><volume>30</volume><fpage>1467</fpage><lpage>1476</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.02.018</pub-id><pub-id pub-id-type="pmid">32220328</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solstad</surname><given-names>T</given-names></name><name><surname>Boccara</surname><given-names>CN</given-names></name><name><surname>Kropff</surname><given-names>E</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representation of geometric borders in the entorhinal cortex</article-title><source>Science</source><volume>322</volume><fpage>1865</fpage><lpage>1868</lpage><pub-id pub-id-type="doi">10.1126/science.1166466</pub-id><pub-id pub-id-type="pmid">19095945</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: A simple and flexible framework</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004792</pub-id><pub-id pub-id-type="pmid">26928718</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Ibarz</surname><given-names>B</given-names></name><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Zambaldi</surname><given-names>V</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A model of egocentric to allocentric understanding in mammalian brains</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.11.378141</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Batista-Brito</surname><given-names>R</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Arousal and locomotion make distinct contributions to cortical activity patterns and visual encoding</article-title><source>Neuron</source><volume>86</volume><fpage>740</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.028</pub-id><pub-id pub-id-type="pmid">25892300</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The Tolman-Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation</article-title><source>Cell</source><volume>183</volume><fpage>1249</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.10.024</pub-id><pub-id pub-id-type="pmid">33181068</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Low</surname><given-names>IIC</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Rnn_Remapping_Paper</data-title><version designator="swh:1:rev:700b8f62d3ec04d0b9ad6bd036eea9104a770aea">swh:1:rev:700b8f62d3ec04d0b9ad6bd036eea9104a770aea</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:768e28a85bfb4493df790c2c2d9da00e62baf3c0;origin=https://github.com/ahwillia/rnn_remapping_paper;visit=swh:1:snp:c5fa47a05259deebdb8381a6953bb38772baad40;anchor=swh:1:rev:700b8f62d3ec04d0b9ad6bd036eea9104a770aea">https://archive.softwareheritage.org/swh:1:dir:768e28a85bfb4493df790c2c2d9da00e62baf3c0;origin=https://github.com/ahwillia/rnn_remapping_paper;visit=swh:1:snp:c5fa47a05259deebdb8381a6953bb38772baad40;anchor=swh:1:rev:700b8f62d3ec04d0b9ad6bd036eea9104a770aea</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0310-2</pub-id><pub-id pub-id-type="pmid">30643294</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Artificial neural networks for neuroscientists: a primer</article-title><source>Neuron</source><volume>109</volume><elocation-id>739</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.01.022</pub-id><pub-id pub-id-type="pmid">33600755</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Liang</surname><given-names>F</given-names></name><name><surname>Xiong</surname><given-names>XR</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Xiao</surname><given-names>Z</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>841</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1038/nn.3701</pub-id><pub-id pub-id-type="pmid">24747575</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86943.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Ecole Normale Superieure Paris</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> work provides evidence that artificial recurrent neural networks can be used to investigate neural mechanisms underlying reversible remapping of spatial representations. Authors perform <bold>convincing</bold> state of the art analyses showing how population activity preserves the encoding of spatial position despite remappings due to the tracking of an internal variable. This paper will be of interest to neuroscientists studying contextual computations, neural representation of space and links between artificial neural networks and the brain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86943.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Based on a recent report of spontaneous and reversible remapping of spatial representations in the enthorhinal cortex (Low et al 2021), this study sets out to examine possible mechanisms by which a network can simultaneously represent a positional variable and an uncorrelated binary internal state. To this end, the authors analyse the geometry of activity in recurrent neural networks trained to simultaneously encode an estimate of position in a one-dimensional track and a transiently-cued binary variable. They find that network activity is organised along two separate ring manifolds. The key result is that these two manifolds are significantly more aligned than expected by chance, as previously found in neural recordings. Importantly, the authors show that this is not a direct consequence of the design of the model, and clarify scenarios by which weaker alignment could be achieved. The model is then extended to a two-dimensional track, and to more than two internal variables. The latter case is compared with experimental data that had not been previously analysed.</p><p>Strengths:</p><list list-type="bullet"><list-item><p>rigorous and careful analysis of activity in trained recurrent neural networks</p></list-item><list-item><p>particular care is taken to show that the obtained results are not a necessary consequence of the design of the model</p></list-item><list-item><p>the writing is very clear and pleasant to read</p></list-item><list-item><p>close comparison with experimental data</p></list-item><list-item><p>extensions beyond the situations studied in experiments (two-dimensional track, more than two internal states)</p></list-item></list><p>Weaknesses:</p><list list-type="bullet"><list-item><p>no major weaknesses</p></list-item><list-item><p>(minor) the comparison with previous models of remapping could be expanded</p></list-item></list><p>Altogether the conclusions claimed by the authors seem to be strongly supported and convincing.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86943.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This important work presents an example of a contextual computation in a navigation task through a comparison of task driven RNNs and mouse neuronal data. Authors perform convincing state of the art analyses demonstrating compositional computation with valuable properties for shared and distinct readouts. This work will be of interest to those studying contextual computation and navigation in biological and artificial systems.</p><p>This work advances intuitions about recent remapping results. Authors trained RNNs to output spatial position and context given velocity and 1-bit flip-flops. Both of these tasks have been trained separately, but this is the first time to my knowledge that one network was trained to output both context and spatial position. This work is also somewhat similar to previous work where RNNs were trained to perform a contextual variation on the Ready-Set-Go with various input configurations (Remington et al. 2018). Additionally findings in the context of recent motor and brain machine interface tasks are consistent with these findings (Marino et al in prep). In all cases contextual input shifts neural dynamics linearly in state space. This shift results in a compositional organization where spatial position can be consistently decoded across contexts. This organization allows for generalization in new contexts. These findings in conjunction with the present study make a consistent argument that remapping events are the result of some input (contextual or otherwise) that moves the neural state along the remapping dimension.</p><p>The strength of this paper is that it tightly links theoretical insights with experimental data, demonstrating the value of running simulations in artificial systems for interpreting emergent properties of biological neuronal networks. For those familiar with RNNs and previous work in this area, these findings may not significantly advance intuitions beyond those developed in previous work. It's still valuable to see this implementation and satisfying demonstration of state of the art methods. The analysis of fixed points in these networks should provide a model for how to reverse engineer and mechanistically understand computation in RNNs.</p><p>I'm curious how the results might change or look the same if the network doesn't need to output context information. One prediction might be that the two rings would collapse resulting in completely overlapping maps in either context. I think this has interesting implications about the outputs of the biological system. What information should be maintained for potential readout and what information should be discarded? This is relevant for considering the number of maps in the network. Additionally, I could imagine the authors might reproduce their current findings in another interesting scenario: Train a network on the spatial navigation task without a context output. Fix the weights. Then provide a new contextual input for the network. I'm curious whether the geometric organization would be similar in this case. This would be an interesting scenario because it would show that any random input could translate the ring attractor that maintains spatial position information without degradation. It might not work, but it could be interesting to try!</p><p>I was curious and interested in the authors choice to not use activity or weight regularization in their networks. My expectation is that regularization might smooth the ring attractor to remove coding irrelevant fluctuations in neural activity. This might make Supplementary Figure 1 look more similar across model and biological remapping events (Line 74). I think this might also change the way authors describe potential complex and high dimensional remapping events described in Figure 2A.</p><p>Overall this is a nice demonstration of state-of-the-art methods to reverse engineer artificial systems to develop insights about biological systems. This work brings together concepts for various tasks and model organisms to provide a satisfying analysis of this remapping data.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86943.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This important work provides convincing evidence that artificial recurrent neural networks can be used to model neural activity during remapping events while an animal is moving along a one-dimensional circular track. This will be of interest to neuroscientists studying the neural dynamics of navigation and memory, as well as the community of researchers seeking to make links between artificial neural networks and the brain.</p><p>Low et al. trained artificial recurrent neural networks (RNNs) to keep track of their location during a navigation task and then compared the activity of these model neurons to the firing rates of real neurons recorded while mice performed a similar task. This study shows that a simple set of ingredients, namely, keeping track of spatial location along a one-dimensional circular track, along with storing the memory of a binary variable (representing which of the two spatial maps are currently being used), are enough to obtain model firing rates that reproduce features of real neural recordings during remapping events. This offers both a normative explanation for these neural activity patterns as well as a potential biological implementation.</p><p>One advantage of this modeling approach using RNNs is that this gives the authors a complete set of firing rates that can be used to solve the task. This makes analyzing these RNNs easier, and opens the door for analyses that are not always practical with limited neural data. The authors leverage this to study the stable and unstable fixed points of the model. However, in this paper there appear to be a few places where analyses that were performed on the RNNs were not performed on the neural data, missing out on an opportunity to appreciate the similarity, or identify differences and pose challenges for future modeling efforts. For example, in the neural data, what is the distribution of the differences between the true remapping vectors for all position bins and the average remapping vector? What is the dimensionality of the remapping vectors? Do the remapping vectors vary smoothly over position? Do the results based on neural data look similar to the results shown for the RNN models (Figures 2C-E)?</p><p>There are many choices that must be made when simulating RNNs and there is a growing awareness that these choices can influence the kinds of solutions RNNs develop. For example, how are the parameters of the RNN initialized? How long is the RNN trained on the task? Are the firing rates encouraged to be small or smoothly varying during training? For the most part these choices are not explored in this paper so I would interpret the authors' results as highlighting a single slice of the solution space while keeping in mind that other potential RNN solutions may exist. For example, the authors note that the RNN and biological data do not appear to solve the 1D navigation and remapping task with the simplest 3-dimensional solution. However, it seems likely that an RNN could also be trained such that it only encodes the task relevant dynamics of this 3-dimensional solution, by training longer or with some regularization on the firing rates. Similarly, a higher-dimensional RNN solution may also be possible and this would likely be necessary to explain the more variable manifold misalignment reported in the experimental data of Low et al. 2021 as opposed to the more tightly aligned distribution for the RNNs in this paper. However, thanks to the modeling work done in this paper, the door has now been opened to these and many other interesting research directions.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86943.3.sa4</article-id><title-group><article-title>Author Response:</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Low</surname><given-names>Isabel IC</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University Mortimer B Zuckerman Mind Brain Behaviour Institute</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford School of Medicine</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Williams</surname><given-names>Alex H</given-names></name><role specific-use="author">Author</role><aff><institution>New York University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors' response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>This is a list of suggestions the authors could use to improve the details of the manuscript:</p><p>- it is not immediately clear what is meant by &quot;modular&quot; on line 38 and the corresponding paragraph. This aspect is not mentioned or developed in the Results.</p><p>- the discussion of remapping vectors on lines 119-137 is particularly illuminating. It could have been interesting to generate surrogate manifolds separated by arbitrary remapping vectors and see how much the alignment metric (Procrustes shape) is sensitive to the dimensionality or amplitude of remapping vectors.</p><p>- A visual comparison between Fig 1 D and H suggests a difference between the manifold geometry in experiments and in the model. It seems that the embedding dimensionality of ring manifolds is higher in the data than in the model. Is that the case? It could have been interesting to explore how much embedding dimensionality influences the alignment metric.</p><p>- I could not find information about the initialization of the connectivity weights. An important possibility is that the degree of alignment (and the organization of remapping vectors) depends on the strength of initial random connectivity.</p><p>- It might have been interesting to comment on the relationship between the top three PCS in Fig1 and the three readout vectors. To which extent are they aligned?</p><p>- I found panels C and G in Fig 1 somewhat difficult to read. In panel C, the remapping seems to be aligned to the same position across all trials. This is not the case in panel G. I am not certain what the comparison is meant to convey, but it would help to have a similar alignment in C and G. Similarly, I was not sure what to conclude from the matrix in the right part of panel C, perhaps the legend should be expanded.</p><p>- the comparison with remapping models of Misha Tsodyks could be expanded. The current discussion implies that the model of Romani &amp; Tsodyks leads to less alignment than found in trained networks, but no direct evidence is given for that statement as far as I can tell.</p><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Minor points:</p><p>All mentions of 'modularity' should be replaced with 'compositionality'.</p><p>I found Supplementary Figure 2 highly confusing. I thought it was meant to help understand the analysis in Figure 1K and related figures. In the end, I never really understood what was happening in these figures. Do authors make perturbations along these different coding dimensions and compare the resulting maps? I wasn't sure what exactly the authors were calculating cosine similarity for. Maybe more exposition on this in the methods would help other readers as well.</p><p>Was there any behavioral difference when the maps were not aligned?</p><p>Why did the authors only go up to 10 contexts? Was this dependent on size of the network? Sorry if I missed this.</p><p>Are remapping event aligned to unit axes? Would this change with different nonlinearities? This could be interesting in the context of (Driscoll et all 2022) and (Wittington et al 2022).</p><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>Cueva, Ardalan, et al. 2021 arXiv:2111.01275 showed that RNNs trained to remember two circular variables develop a toroidal geometry to store this information, so consider citing this in your section on the toroidal manifolds.</p></disp-quote><p>We thank the reviewers for their thoughtful comments. We appreciate that all three reviewers affirmed the importance of our work and the rigor of our approach. We believe that no major weaknesses were identified by the reviews. In our view, the comparisons between recurrent neural network models and experimental data are one of the most important contributions of our work, and all reviewers agreed that this was a core strength of the manuscript.</p><p>The reviewers highlighted several future modeling directions that are raised by our results and that we did not explore in the manuscript. For example, Reviewer 2 suggests that we train networks on a navigation task alone, freeze the weights, and then train on a context discrimination task. We agree that this kind of contextual learning paradigm is of interest and could provide insight into biological remapping, such as that observed by Low et al. (2021). We also agree with Reviewer 3’s broader point that “There are many choices that must be made when simulating RNNs and there is a growing awareness that these choices can influence the kinds of solutions RNNs develop.” It is notable that we were able to reproduce the qualitative features of the experimental data without finely tuning hyperparameters (we used default settings in PyTorch layers), using a very basic training protocol (gradient descent with gradient clipping), and without adding any hand crafted regularization (though we agree that regularization could make the RNN solution look even more like the data).</p><p>We believe that readers will benefit from reading the reviewers' suggestions, which are insightful and well-motivated. Having weighed the reviewer comments carefully, we feel that our manuscript stands as a complete scientific story. We hope that the public reviewer comments will inspire future investigations to fully explore these possibilities and unpack their outcomes at a level of detail that would not be possible in the context of our manuscript.</p><p>Thus, we have chosen to implement the following minor changes suggested by the reviewers, which we hope will improve the clarity of the text and figures (summarized below). These changes do not alter the fundamental content of the manuscript.</p><p>Text:</p><list list-type="bullet"><list-item><p>We corrected a few minor typos.</p></list-item><list-item><p>We updated the citations to follow the eLife citation style.</p></list-item><list-item><p>To address comments from Reviewers 1 and 2: we reworded the final paragraph of the Introduction (p. 3) to remove the term “modularity” and clarify our main finding. Those sentences now read, “The RNN geometry and algorithmic principles readily generalized from a simple task to more complex settings. Furthermore, we performed a new analysis of experimental data published in Low et al.26 and found a similar geometric structure in neural activity from a subset of sessions with more than two stable spatial maps.”</p></list-item><list-item><p>To address comments from Reviewer 1: in the first paragraph of the Results section <italic>A recurrent neural network model of 1D navigation and context inference remaps between aligned ring manifolds</italic> (p. 3), we added the sentence, “Remapping was not aligned to particular track positions, rewards, or landmarks.” to clarify that experimental result from Low et al. (2021).</p></list-item><list-item><p>To address comments from Reviewer 3: in the final paragraph of the Results section <italic>Aligned toroidal manifolds emerge in a 2D generalization of the task</italic> (p. 11) we clarified that models were trained “to estimate position on a 2D circular track.” We also added a citation to Cueva, Ardalan et al. (2021) with the following sentence, “Notably, each toroidal manifold alone is reminiscent of networks trained to store two circular variables without remapping.”</p></list-item></list><list list-type="bullet"><list-item><p>To address a question from Reviewer 2: in the final paragraph of the Results section <italic>Manifold alignment generalizes to three or more maps</italic> (p. 13), we added the following clarification: “In Supplemental Figure 3, we show that RNNs are capable of solving this task with larger numbers of latent states (more than three<bold>;</bold> for simplicity, we consider up to 10 states).”</p></list-item><list-item><p>To address a comment from Reviewer 1: in the fourth paragraph of the Discussion (p. 17), we removed the sentence, “Notably, our model captured aspects of the data that these previous forward-engineered models did not explore—namely, that the ring manifolds corresponding to the correlated spatial maps were much more aligned than expected by chance and than strictly required by the task.” to focus on the key point in the following sentence that, “forward-engineered models provide insights into <italic>how</italic> neural circuits may remap, but do not answer <italic>why</italic> they do so.”</p></list-item><list-item><p>To address comments from Reviewers 1 and 2: we reworded the penultimate paragraph of the Discussion (p. 17–18) to clarify our findings and remove the term “modularity” (except when referencing papers that themselves use that term (Driscoll et al., 2022; Yang et al., 2019)). Those sentences now read:</p></list-item></list><p>“When RNN architecture is explicitly designed to include dedicated neural subpopulations, these subpopulations can improve model performance on particular types of tasks (Beiran et al., 2021; Dubreuil et al., 2022). Thus, there is an emerging conclusion that RNNs use simple dynamical motifs as building blocks for more general and complex computations, which our results support. In particular, aligned ring attractors are a recurring, dynamical motif in our results, appearing first in a simple task setting (2 maps of a 1D environment) and subsequently as a component of RNN dynamics in more complex settings (e.g., as sub-manifolds of toroidal attractors in a 2D environment, see Figure 4). We can therefore conceptualize a pair of aligned ring manifolds as a dynamical “building block” that RNNs utilize to solve higher-dimensional generalizations of the task. Intriguingly, our novel analysis of neural data from Low et al. (2021) revealed that similar principles may hold in biological circuits—when three or more spatial maps were present in a recording, the pairs of ring manifolds tended to be aligned.”</p><list list-type="bullet"><list-item><p>To address questions from Reviewers 2 and 3: in the first paragraph of the Methods section <italic>RNN Model and Training Procedure</italic> (p. 21), we added the sentence: “The connection weights were randomly initialized from the uniform distribution <italic>U</italic>(−√1/N, √1/N), which is the default initialization scheme in PyTorch.”</p></list-item></list><list list-type="bullet"><list-item><p>To address a question from Reviewer 2: we added a third paragraph to the Methods section <italic>Manifold Geometry Analysis</italic> (p. 23), as follows:</p></list-item></list><p>“In Figure 1K, 4G, 5G, and Supplementary Figure 2B, we calculate the angles between the input and output weights and the position subspace or remapping dimension. To find this angle, we calculated the cosine similarity between each weight vector and each subspace. Cosine similarity of 0 indicates that the weights were orthogonal to the subspace, while a similarity of 1 indicates that the weight vector was contained within the subspace.”</p><list list-type="bullet"><list-item><p>To address a question from Reviewer 1: we added the following sentence to the second paragraph of the Methods section <italic>Experimental Data</italic> (p. 24), “We performed the same analysis of trial-by-trial spatial stability to obtain the similarity matrices in Figure 1C and G.”</p></list-item></list><p>Figures and legends:</p><list list-type="bullet"><list-item><p>To address a question from Reviewer 1: in Figure 1C and G, we added x-axis labels to the similarity matrices to clarify that these are trial-by-trial correlations.</p></list-item><list-item><p>To address a question from Reviewer 1: we expanded the Figure 1C legend to clarify the experimental results as follows:</p></list-item></list><p><italic>Old legend:</italic></p><p>(C, left) An example medial entorhinal cortex neuron switches between two maps of the same track (top, raster; bottom, average firing rate by position; red, map 1; black, map 2). (C, right/top) Network-wide trial-by-trial correlations for the spatial firing pattern of all co-recorded neurons in the same example session (colorbar indicates correlation). (C, right/bottom) k-means map assignment.</p><p><italic>New legend:</italic></p><p>(C, left) An example medial entorhinal cortex neuron switches between two maps of the same track (top, spikes by trial and track position; bottom, average firing rate by position across trials from each map; red, map 1; black, map 2). (C, right/top) Correlation between the spatial firing patterns of all co-recorded neurons for each pair of trials in the same example session (dark gray, high correlation; light gray, low correlation). The population-wide activity is alternating between two stable maps across blocks of trials. (C, right/bottom) K-means clustering of spatial firing patterns results in a map assignment for each trial.</p><list list-type="bullet"><list-item><p>To address comments from Reviewer 3: in the legend of Figure 4C, we added the sentence “Note that the true tori are not linearly embeddable in 3 dimensions, so this projection is an approximation of the true torus structure.”</p></list-item><list-item><p>To address a question from Reviewer 2: we expanded the legend for Supplementary Figure 2 to clarify the purpose of the figure schematics as follows:</p></list-item></list><p><italic>Old legend:</italic></p><p>(A) Schematic showing the orthogonalization of the position and context input and output weights.</p><p>(B) Reproduced from Figure 1K.</p><p>(C-D) Schematic: How a single velocity input (blue arrows) updates the position estimate (yellow to red points) from the starting position (blue points).</p><p>(C) Velocity input lies in the position tuning subspace (gray plane)(hypothetical). Note that the same velocity input results in different final positions.</p><p>(D) Velocity input is orthogonal to the position tuning subspace (observed).</p><p>(E) Schematic of possible flow fields in each of the three planes (numbers correspond to planes in C and D), which would result in the correct positional estimate given orthogonal velocity inputs at different positions (D).</p><p><italic>New legend:</italic></p><p>(A) Schematic showing the relative orientation of the position output weights and the context input and output weights to the position and state tuning subspaces.</p><p>(B) Reproduced from Figure 1K.</p><p>(C-D) Schematic to interpret why the position input weights are orthogonal to the position tuning subspace. These schematics illustrate how a single velocity input (blue arrows) updates the position estimate (yellow to red points) from a given starting position (blue points).</p><p>(C, not observed) Velocity input lies in the position tuning subspace (gray plane). Note that the same velocity input pushes the network clockwise or counterclockwise along the ring depending on the circular position</p><p>(D, observed) Velocity input is orthogonal to the position tuning subspace and pushes neural activity out of the subspace.</p><p>(E) Schematic of possible flow fields in each of three planes (numbers correspond to planes in C and D). We conjecture that these dynamics would enable a given orthogonal velocity input to nonlinearly update the position estimate, resulting in the correct translation around the ring regardless of starting position (as in D).</p></body></sub-article></article>