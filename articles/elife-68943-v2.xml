<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">68943</article-id><article-id pub-id-type="doi">10.7554/eLife.68943</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Value signals guide abstraction during learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-232582"><name><surname>Cortese</surname><given-names>Aurelio</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4567-0924</contrib-id><email>cortese.aurelio@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-233455"><name><surname>Yamamoto</surname><given-names>Asuka</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-233456"><name><surname>Hashemzadeh</surname><given-names>Maryam</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196029"><name><surname>Sepulveda</surname><given-names>Pradyumna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0159-6777</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-3286"><name><surname>Kawato</surname><given-names>Mitsuo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-157784"><name><surname>De Martino</surname><given-names>Benedetto</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3555-2732</contrib-id><email>benedettodemartino@gmail.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Computational Neuroscience Labs, ATR Institute International</institution><addr-line><named-content content-type="city">Kyoto</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution>Institute of Cognitive Neuroscience, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>School of Information Science, Nara Institute of Science and Technology</institution><addr-line><named-content content-type="city">Nara</named-content></addr-line><country>Japan</country></aff><aff id="aff4"><label>4</label><institution>Department of Computing Science, University of Alberta</institution><addr-line><named-content content-type="city">Edmonton</named-content></addr-line><country>Canada</country></aff><aff id="aff5"><label>5</label><institution>RIKEN Center for Artificial Intelligence Project</institution><addr-line><named-content content-type="city">Kyoto</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>13</day><month>07</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e68943</elocation-id><history><date date-type="received" iso-8601-date="2021-03-30"><day>30</day><month>03</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-07-12"><day>12</day><month>07</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-10-30"><day>30</day><month>10</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.10.29.361469"/></event></pub-history><permissions><copyright-statement>© 2021, Cortese et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Cortese et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-68943-v2.pdf"/><abstract><p>The human brain excels at constructing and using abstractions, such as rules, or concepts. Here, in two fMRI experiments, we demonstrate a mechanism of abstraction built upon the valuation of sensory features. Human volunteers learned novel association rules based on simple visual features. Reinforcement-learning algorithms revealed that, with learning, high-value abstract representations increasingly guided participant behaviour, resulting in better choices and higher subjective confidence. We also found that the brain area computing value signals – the ventromedial prefrontal cortex – prioritised and selected latent task elements during abstraction, both locally and through its connection to the visual cortex. Such a coding scheme predicts a causal role for valuation. Hence, in a second experiment, we used multivoxel neural reinforcement to test for the causality of feature valuation in the sensory cortex, as a mechanism of abstraction. Tagging the neural representation of a task feature with rewards evoked abstraction-based decisions. Together, these findings provide a novel interpretation of value as a goal-dependent, key factor in forging abstract representations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>abstraction</kwd><kwd>vmpfc</kwd><kwd>confidence</kwd><kwd>multivoxel neural reinforcement</kwd><kwd>valuation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002241</institution-id><institution>Japan Science and Technology Agency</institution></institution-wrap></funding-source><award-id>JPMJER1801</award-id><principal-award-recipient><name><surname>Cortese</surname><given-names>Aurelio</given-names></name><name><surname>Kawato</surname><given-names>Mitsuo</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009619</institution-id><institution>Japan Agency for Medical Research and Development</institution></institution-wrap></funding-source><award-id>JP18dm0307008</award-id><principal-award-recipient><name><surname>Cortese</surname><given-names>Aurelio</given-names></name><name><surname>Kawato</surname><given-names>Mitsuo</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Chilean National Agency for Research and Development</institution></institution-wrap></funding-source><award-id>72180193</award-id><principal-award-recipient><name><surname>Sepulveda</surname><given-names>Pradyumna</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>102612/A/13/Z</award-id><principal-award-recipient><name><surname>De Martino</surname><given-names>Benedetto</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Valuation of sensory information in the ventromedial prefrontal cortex plays a key function in constructing the abstract representations that guide complex behaviours.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><sec id="s1-1"><title>‘All art is an abstraction to some degree.’ Henry Moore</title><p>Art is one of the best examples of abstraction, the unique ability of the human mind to organise information beyond the immediate sensory reality. Abstraction is by no means restricted to high-level cognitive behaviour such as art creation. It envelops every aspect of our interaction with the environment. Imagine that you are hiking in a park, and you need to cross a stream. Albeit deceptively simple, this scenario already requires the processing of a myriad visual (and auditory, etc.) features. For an agent that operates directly on each feature in this complex sensory space, any meaningful behavioural trajectory (such as crossing the stream) would quickly involve intractable computations. This is well exemplified in reinforcement learning (RL), where in complex and/or multidimensional problems, classic RL algorithms rapidly collapse (<xref ref-type="bibr" rid="bib7">Bellman, 1957</xref>; <xref ref-type="bibr" rid="bib37">Kawato and Samejima, 2007</xref>; <xref ref-type="bibr" rid="bib75">Sutton and Barto, 1998</xref>). If, on the other hand, the agent is able to first ‘abstract’ the current state to a lower dimensional manifold, representing only relevant features, behaviour becomes far more flexible and efficient (<xref ref-type="bibr" rid="bib34">Ho et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Konidaris, 2019</xref>; <xref ref-type="bibr" rid="bib60">Niv, 2019</xref>). Attention (<xref ref-type="bibr" rid="bib24">Farashahi et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Leong et al., 2017</xref>; <xref ref-type="bibr" rid="bib59">Niv et al., 2015</xref>), and more generally, the ability to act upon subspaces, concepts or abstract representations has been proposed as an effective solution to overcome computational bottlenecks arising from sensory-level operations in RL (<xref ref-type="bibr" rid="bib17">Cortese et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Hashemzadeh et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Ho et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Konidaris, 2019</xref>; <xref ref-type="bibr" rid="bib83">Wikenheiser and Schoenbaum, 2016</xref>). Abstractions can be thus thought of as simplified maps carved from higher dimensional space, in which details have been removed or transformed, in order to focus on a subset of interconnected features, that is, a higher order concept, category or schema (<xref ref-type="bibr" rid="bib29">Gilboa and Marlatte, 2017</xref>; <xref ref-type="bibr" rid="bib52">Mack et al., 2016</xref>).</p><p>How are abstract representations constructed in the human brain? For flexible deployment, abstraction should depend on task goals. From a psychological or neuroeconomic point of view, task goals generally determine what is valuable (<xref ref-type="bibr" rid="bib40">Kobayashi and Hsu, 2019</xref>; <xref ref-type="bibr" rid="bib49">Liu et al., 2017</xref>; <xref ref-type="bibr" rid="bib55">McNamee et al., 2013</xref>), such that if one needs to light a fire, matches are much more valuable than a glass of water. Hence, we hypothesised that valuation processes are directly related to abstraction.</p><p>Value representations have been linked to neural activity in the ventromedial prefrontal cortex (vmPFC) in the context of economic choices (<xref ref-type="bibr" rid="bib55">McNamee et al., 2013</xref>; <xref ref-type="bibr" rid="bib62">Padoa-Schioppa and Assad, 2006</xref>). More recently, the role of the vmPFC has also been extended to computation of confidence (<xref ref-type="bibr" rid="bib20">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Gherman and Philiastides, 2018</xref>; <xref ref-type="bibr" rid="bib45">Lebreton et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Shapiro and Grafton, 2020</xref>). While this line of work has been extremely fruitful, it has mostly focused on the hedonic and rewarding aspect of value instead of its broader functional role. In the field of memory, a large corpus of work has shown that the vmPFC is crucial for formation of schemas or conceptual knowledge (<xref ref-type="bibr" rid="bib14">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Gilboa and Marlatte, 2017</xref>; <xref ref-type="bibr" rid="bib44">Kumaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib52">Mack et al., 2016</xref>; <xref ref-type="bibr" rid="bib77">Tse et al., 2007</xref>), as well as generalisations (<xref ref-type="bibr" rid="bib11">Bowman and Zeithamova, 2018</xref>). The vmPFC also collates goal-relevant information from elsewhere in the brain (<xref ref-type="bibr" rid="bib9">Benoit et al., 2014</xref>). Considering its connectivity pattern (<xref ref-type="bibr" rid="bib58">Neubert et al., 2015</xref>), the vmPFC is well suited to serve a pivotal function in the circuit that involves the hippocampal formation (HPC) and the orbitofrontal cortex (OFC), dedicated to extracting latent task information and regularities important for navigating behavioural goals (<xref ref-type="bibr" rid="bib60">Niv, 2019</xref>; <xref ref-type="bibr" rid="bib66">Schuck et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Stachenfeld et al., 2017</xref>; <xref ref-type="bibr" rid="bib79">Viganò and Piazza, 2020</xref>; <xref ref-type="bibr" rid="bib84">Wilson et al., 2014</xref>). Thus, the aim of this study is twofold: (i) to demonstrate that abstraction emerges during the course of learning, and (ii) to investigate how the brain, and specifically the vmPFC, uses valuation upon low-level sensory features to forge abstract representations.</p><p>To achieve this, we leveraged a task in which human participants repeatedly learned novel association rules, while their brain activity was recorded with fMRI. Reinforcement learning (RL) modelling allowed us to track participants’ valuation processes and to dissociate their learning strategies (both at the behavioural and neural levels) based on the degree of abstraction. Participants’ confidence in having performed the task well was positively correlated with their ability to abstract. In a second experiment, we studied the causal role of value in promoting abstraction through directed effect in sensory cortices. To anticipate our results, we show that the vmPFC and its connection to the visual cortex construct abstract representations through a goal-dependent valuation process that is implemented as top-down control of sensory cortices.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experimental design</title><p>The goal of the learning task was to present a problem that could be solved according to two strategies, based on the sampled task-space dimensionality. A simple, slower strategy akin to pattern recognition, and a more sophisticated one that required abstraction to use the underlying structure. Participants (N = 33) learned the fruit preference of pacman-like characters formed by the combination of three visual features (colour, mouth direction, and stripe orientation, <xref ref-type="fig" rid="fig1">Figure 1A–B</xref>). The preference was governed by a combination of two features, selected randomly by our computer program for each block (<xref ref-type="fig" rid="fig1">Figure 1A–B</xref>). Learning the block rules essentially required participants to uncover hidden associations between features and fruits. Although participants were instructed that one feature was irrelevant, they did not know which. A block ended when a sequence of 8–12 (randomly set by our computer program) correct choices was detected or upon reaching its upper limit (80 trials). Variable stopping criteria were used to prevent participants from learning that a fixed sequence was predictive of block termination. During each trial, participants could see the outcome after selecting a fruit. A green box appeared around the chosen fruit if the choice was correct (red otherwise). Additionally, participants were instructed that the faster they learned a block rule, the higher the reward. At the end of the session, a final monetary reward was delivered, commensurate with participant performance (see Materials and methods). Participants failing to learn the association in three blocks or more (i.e. reaching a block limit of 80 trials without having learned the association), and / or failing to complete more than 10 blocks in the allocated time, were excluded (see Materials and methods). All main results reported in the paper are from the included sample of N = 33 participants.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Learning task and behavioural results.</title><p>(<bold>A</bold>) Task: participants learned the fruit preferences of pacman-like characters, which changed on each block. (<bold>B</bold>) Associations could form in three ways: colour – stripe orientation, colour – mouth direction, and stripe orientation – mouth direction. The left-out feature was irrelevant. Examples of the two types of fruit associations. The four combinations arising from two features with two levels were divided into symmetric (2x2) and asymmetric (3x1) cases. f1-3: features 1 to 3; fruit:rule refers to the fruit as being the association rule. Both block types were included to prevent participants from learning rules by simple deduction. If all blocks had symmetric association rules and participants knew this, they could simply learn one feature-fruit association (e.g. green-vertical), and from there deduce all other combinations. Both the relevant features and the association types varied on a block-by-block basis. (<bold>C</bold>), Trial-by-trial ratio-correct improved as a measure of within-block learning. Dots represent the mean across participants, while error bars indicate the SEM, and the shaded area represents the 95% CI (N = 33). Participant-level ratio correct was computed for each trial across all completed blocks. Source data is available in file <xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>. (<bold>D</bold>), Learning speed was positively correlated with time, among participants. Learning speed was computed as the inverse of the max-normalised number of trials taken to complete a block. Thin gray lines represent least square fits of individual participants, while the black line represents the group-average fit. The correlation was computed with group-averaged data points (N = 11). Average data points are plotted as coloured circles, the error bars are the SEM. (<bold>E</bold>), Confidence judgements were positively correlated with learning speed, among participants. Each dot represents data from one participant, and the thick line indicates the regression fit (N = 31 [2 missing data]). The experiment was conducted once (n = 33 biologically independent samples), **p&lt;0.01.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Csv: panel C.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig1-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig1sdata2"><label>Figure 1—source data 2.</label><caption><title>Csv: panel D.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig1-data2-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig1sdata3"><label>Figure 1—source data 3.</label><caption><title>Csv: panel E.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig1-data3-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Small (non-significant trends) influence of block/association type on learning speed.</title><p>Average learning speed was computed by pooling block-wise learning speed from all participants for each block or association type. None of the pairwise tests survived multiple comparison correction (FDR). Bars represent the population mean, error bars the SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Behaviour analysis of excluded participants.</title><p>Excluded participants made more mistakes overall (lower ratio of correct responses). Wilcoxon rank sum test, z = 2.76, p = 0.006.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig1-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Behavioural accounts of learning</title><p>We verified that participants learned the task sensibly. Within blocks, performance was higher than chance as early as the second trial (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, one-sample t-test against mean of 0.5, trial 2: <italic>t</italic><sub>32</sub> = 4.13, <italic>P<sub>(FDR)</sub></italic> &lt; 10<sup>−3</sup>, trial 3: <italic>t</italic><sub>32</sub> = 2.47, <italic>P<sub>(FDR)</sub></italic> = 0.014, all trials t&gt;3: <italic>P<sub>(FDR)</sub></italic> &lt; 10<sup>−3</sup>). Considering the whole experimental session, learning speed (i.e. how quickly participants completed a given block) increased significantly across blocks (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, N = 11 time points, Pearson’s <italic>r</italic> = 0.80, p = 0.003). These results not only confirmed that participants learned the task rule in each block, but also that they learned to use more efficient strategies. Notably, in this task, the only way to solve blocks faster was by using the correct subset of dimensions (the abstract representation). When at the end of a session, participants were asked about their degree of confidence in having performed the task well, their self-reports correlated with their learning speed (N = 31 [2 missing data], robust regression slope = 0.024, <italic>t</italic><sub>29</sub> = 3.27, p = 0.003, <xref ref-type="fig" rid="fig1">Figure 1E</xref>), but not with the overall number of trials, or the product of the proportion of successes (learning speed: Pearson’s <italic>r</italic> = 0.53, p = 0.002, total trials: <italic>r</italic> = −0.13, p = 0.47, test for difference in <italic>r</italic>: z = 2.71, p = 0.007; product of the proportion of successes: <italic>r</italic> = −0.06, p = 0.75, test for difference in <italic>r</italic>: z = 2.43, p = 0.015). We also confirmed that the block type (defined by relevant features, e.g., colour-orientation) or association type (e.g. symmetric 2x2) did not systematically affect learning speed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Excluded participants (see Materials and methods) had overall lower performance (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), although some had comparable ratios correct.</p></sec><sec id="s2-3"><title>Discovery of abstract representations</title><p>Was participants’ learning behaviour guided by the selection of accurate representations? To answer this question, we built upon a classic RL algorithm (Q-learning) (<xref ref-type="bibr" rid="bib82">Watkins and Dayan, 1992</xref>) in which state-action value functions (beliefs) used to predict future rewards, are updated according to the task state of a given trial and the action outcome. In this study, task states were defined by the number of feature combinations that the agent may track; hence, we devised algorithms that differed in their state-space dimensionality. The first algorithm, called Feature RL, explicitly tracked all combinations of three features, 2<sup>3</sup> = eight states (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, top left). This algorithm is anchored at a low feature level because each combination of the three features results in a unique fingerprint – one simply learns direct pairings between visual patterns and fruits (actions). Conversely, the second algorithm, called Abstract RL, used a more compact or <italic>abstract state representation</italic> in which only two features are tracked. These compressed representations reduce the explored state-space by half, 2<sup>2</sup> = four states (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, top right). Importantly, in this task environment as many as three Abstract RL in parallel were possible, one for each combination of two features.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Mixture of reinforcement learning (RL) experts and value computation.</title><p>(<bold>A</bold>) Outline of the representational spaces of each RL algorithm comprising the mixture-of-experts architecture. (<bold>B</bold>) Illustration of the model architecture. See Methods for a formal description of the model. All experts had the same number of hyperparameters: the learning rate α (how much the latest outcome affected agent beliefs), the forgetting factor γ (how much prior RPEs influenced current decisions), and the RPE variance v, modulating the sharpness with which the mixture-of-expert RL model should favour the best performing algorithm in the current trial. (<bold>C</bold>) The approach used for data analysis and model simulation. The model was first fitted to participant data with Hierarchical Bayesian Inference (<xref ref-type="bibr" rid="bib65">Piray et al., 2019</xref>). Estimated hyperparameters were used to compute value functions of participant data, as well as to generate new, artificial choice data and to compute simulated value functions. (<bold>D</bold>) Averaged expected value across all states for the chosen action in each RL expert, as well as responsibility signal for each model. Left: simulated data, right: participant empirical data. Dots represent individual agents (left) or participants (right). Bars indicate the mean and error bars depict the SEM. Statistical comparisons were performed with two-sided Wilcoxon signed rank tests. ***p&lt;0.001. AbRL: Abstract RL, FeRL: Feature RL, AbRL<sub>w1</sub>: wrong-1 Abstract RL, AbRL<sub>w2</sub>: wrong-2 Abstract RL. (<bold>E</bold>) RPE variance was negatively correlated with learning speed (outliers removed, N = 29). Dots represent individual participant data. The thick line shows the linear regression fit. The experiment was conducted once (n = 33 biologically independent samples), * p&lt;0.05.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Csv: panel D, mean expected value, model.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig2-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Csv: panel D, mean expected value, subjects.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig2-data2-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig2sdata3"><label>Figure 2—source data 3.</label><caption><title>Csv: panel D, lambda, model.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig2-data3-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig2sdata4"><label>Figure 2—source data 4.</label><caption><title>Csv: panel D, lambda, subjects.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig2-data4-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig2sdata5"><label>Figure 2—source data 5.</label><caption><title>Csv: panel E.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig2-data5-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Model comparison (accounting for model complexity).</title><p>Mixture-of-Experts RL (MoE-RL), Feature RL (FeRL), and Abstract RL (AbRL) where fit and compared to each other with Hierarchical Bayesian Inference [HBI] (<xref ref-type="bibr" rid="bib65">Piray et al., 2019</xref>), at the single participant and block level. (<bold>A</bold>) Percentage of participants in which the target model best fitted the choice data. The percentage was computed for each block separately (in time, block 1, 2, etc.), then averaged. Bars represent the mean, error bars the SEM. (<bold>B</bold>) Percentage of blocks (all participants and blocks pooled) in which the target model best fitted the choice data. Bars represent the percentage. (<bold>C</bold>) Block-specific break-down of the number of participants for which the MoE-RL, the FeRL, or the AbRL provided the best fit.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig2-figsupp1-v2.tif"/></fig></fig-group><p>The above four RL algorithms were combined in a mixture-of-experts architecture (<xref ref-type="bibr" rid="bib26">Frank and Badre, 2012</xref>; <xref ref-type="bibr" rid="bib35">Jacobs et al., 1991</xref>; <xref ref-type="bibr" rid="bib74">Sugimoto et al., 2012</xref>), <xref ref-type="fig" rid="fig2">Figure 2B</xref> and Materials and methods. The key intuition behind this approach was that at the beginning of a new block, the agent did not know which abstract representation was correct (i.e., which features were relevant). Thus, the agent needed to learn which representations were most predictive of reward, so as to exploit the best representation for action selection. Experts here denote the four learning algorithms (Feature RL, and three options of Abstract RL). While all experts competed for action selection, their learning uncertainty (RPE: reward prediction error) determined the strength in doing so (<xref ref-type="bibr" rid="bib23">Doya et al., 2002</xref>; <xref ref-type="bibr" rid="bib74">Sugimoto et al., 2012</xref>; <xref ref-type="bibr" rid="bib85">Wolpert and Kawato, 1998</xref>). This architecture allowed us to track the value function of each RL expert separately, while using a unique, global action in each trial.</p><p>Estimated hyperparameters (learning rate α, forgetting factor γ, RPE variance ν) were used to compute value functions of participant data, as well as to generate new, artificial choice data and value functions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, and Materials and methods). Simulations indicated that expected value and responsibility were highest for the appropriate Abstract RL, followed by Feature RL, and the two Abstract RLs based on irrelevant features as the lowest (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Participant empirical data displayed the same pattern, whereby the value function and responsibility signal of the appropriate Abstract RL were higher than in other RL algorithms (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, right side). Note that the large difference between appropriate Abstract RL and Feature RL was because the appropriate Abstract RL was an ‘oracle’: it had access to the correct low-dimensional state from the beginning. The RPE variance (hyperparameter ν) adjusted the sharpness with which each RL’s (un)certainty was considered for expert weighting. Crucially, the variance <bold>v</bold> was associated with participant learning speed, such that participants who learned block rules quickly were sharper in expert selection (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, N = 29, robust regression slope = −1.02, <italic>t</italic><sub>27</sub> = −2.59, p = 0.015). These modelling results provided a first layer of support for the hypothesis that valuation is related to abstraction.</p></sec><sec id="s2-4"><title>Behaviour shifts from Feature- to Abstraction-based reinforcement learning</title><p>The mixture-of-expert RL model revealed that participants who learned faster relied more on the best RL model value representations. Further, the modelling established that choices were mostly driven by either the appropriate Abstract RL or Feature RL, which had higher expected values (but note that the other Abstract RLs had mean values greater than a null level of 0.5), and higher responsibility λ. It is important to highlight though that the mixture-of-experts RL might not reflect the actual algorithmic computation used by the participant in this task, but it provides a conceptual solution to the arbitration between representations/strategies. Model comparison showed that Abstract RL and Feature RL in many cases offered a more parsimonious description of the participants behaviour. This is unsurprising since Feature RL is a simple model and Abstract RL is an oracle model – knowing which are the relevant feature (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for a direct model comparison between mixture-of-experts RL, Feature RL, and Abstract RL). Hence, we next sought to explicitly explain participant choices and learning according to either Feature RL or Abstract RL strategy. Given the task space (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), the only way to solve a block rule faster was to use abstract representations. As such, we expect to observe a shift from Feature RL toward Abstract RL to occur with learning.</p><p>Both algorithms had two hyperparameters: the learning rate α and greediness β (inverse temperature, the strength that expected value has on determining actions). Using the estimated hyperparameters, we generated new, synthetic data to evaluate how fast artificial agents, implementing either Feature RL or Abstract RL, solved the learning task (see Materials and methods). The simulations attested that Feature RL was slower and less efficient (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), yielding lower learning speed and a higher percentage of failed blocks.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Feature RL vs Abstract RL are related to learning speed and the use of abstraction increases with experience.</title><p>(<bold>A</bold>) Simulated learning speed and % of failed blocks for both Abstract RL and Feature RL. To make simulations more realistic, arbitrary noise was injected into the simulation, altering the state (see Materials and methods). N = 100 simulations of 45 agents. Right plot: bars represent the mean, error bars the SEM. (<bold>B</bold>) The relationship between the block-by-block, best-fitting model and learning speed of participants. Each dot represents one block from one participant, with data aggregated across all participants. Note that some dots fall beyond p=one or p=0. This effect occurs because dots were scattered with noise in their x-y coordinates for better visualisation. (<bold>C</bold>) Between participant correlations. Top: abstraction level vs learning speed. The abstraction level was computed as the average over all blocks completed by a given participant (code: Feature RL = 0, Abstract RL = 1). Bottom: confidence vs abstraction level. Dots represent individual participants (top: N = 33, bottom: N = 31, some dots are overlapping). (<bold>D</bold>) Learning rate was not symmetrically distributed across the two algorithms. (<bold>E</bold>) Greediness was not symmetrically distributed across the two algorithms. For both (<bold>D</bold> and <bold>E</bold>), each dot represents one block from one participant, with data aggregated across all participants. Histograms represent the distribution of data around the midline. (<bold>F</bold>) The number of participants for which Feature RL or Abstract RL best explained their choice behaviour in the first and last blocks of the experimental session. (<bold>G</bold>) Abstraction level was computed separately with blocks from the first half (early) and latter half (late) session. (<bold>H</bold>) Participants count for the best fitting model, in each block. The experiment was conducted once (n = 33 biologically independent samples), * p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Csv: panel A left, model simulations histogram of learning speed.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>Csv: panel A right, model simulations % failed blocks.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data2-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata3"><label>Figure 3—source data 3.</label><caption><title>Csv: panel B, scatter plot of model probabilities.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data3-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata4"><label>Figure 3—source data 4.</label><caption><title>Csv: panel B, violin plot of proportion Abstract RL.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data4-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata5"><label>Figure 3—source data 5.</label><caption><title>Csv: panel C.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data5-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata6"><label>Figure 3—source data 6.</label><caption><title>Csv: panel D.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data6-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata7"><label>Figure 3—source data 7.</label><caption><title>Csv: panel E.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data7-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata8"><label>Figure 3—source data 8.</label><caption><title>Csv: panels F and G.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data8-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig3sdata9"><label>Figure 3—source data 9.</label><caption><title>csv: panel H.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig3-data9-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Comparison of learning rate α and greediness β in Feature RL and Abstract RL best-fitting and worst-fitting blocks.</title><p>(<bold>A</bold>) The comparison was done across blocks derived from the model that provided a <italic>better</italic> fit. Here, the learning rate and greediness in Feature RL blocks was lower than in Abstract RL blocks. Statistical comparisons were performed with Wilcoxon rank-sum two-sided tests. Learning rate: z = −3.88, p = 0.0001; Greediness: z = −8.69, p = 3.5x10<sup>−18</sup>. (<bold>B</bold>) The comparison was done across blocks derived from the model that provided a <italic>worse</italic> fit. The pattern of results reversed, with lower learning rate and greediness for the Abstract RL model. Statistical comparisons were performed with Wilcoxon rank-sum two-sided tests. Learning rate: z = 13.65, p = 2x10<sup>−42</sup>; greediness: z = 12.91, p = 4.1x10<sup>−38</sup>. This discrepancy is likely due to the fact that in the blocks in which Abstract RL fits best the data, Feature RL would have very high learning rates / greediness to accommodate behaviour within fewer trials. Conversely, because the blocks in which Feature RL fits the data best also tend to have learning speed (more trials taken to complete), Abstract RL must display much lower learning speeds since the horizon is longer (but with few states). Bars correspond to the mean, error bars the SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Abstraction index for single blocks and expected value for the chosen action in Abstract RL and Feature RL.</title><p>(<bold>A</bold>) Abstraction index was computed by allocating a value of 0 to blocks labelled as ‘Feature RL’ and one to blocks labelled as ‘Abstract RL’, for each participant. This was done for the first 11 blocks, which were completed by all participants. The plot indicates that, at the group level, there was a tendency towards increasingly higher abstraction later in time. Each dot represents the population mean, error bars the SEM. The least square line fit, robust regression, and p-value were computed on the 11 mean data points. Robust regression slope = 0.022, t<sub>31</sub> = 2.34, p = 0.044. (<bold>B</bold>) Mean expected value computed from all blocks, when fitted with either Abstract RL or Feature RL. (<bold>C</bold>) Mean expected value computed from the best fitting algorithms on a given block. That is, Abstract RL (resp. Feature RL) refers to the expected value for the chosen action in blocks where Abstract RL (resp. Feature RL) was the best fitting algorithm. Two (<bold>B</bold>) and one (<bold>C</bold>) outliers were removed. Each coloured dot represents the average expected value for the chosen options for a single participant - in either Abstract RL (grey) or Feature RL (cyan). Shaded areas represent the density plot, central white dot the median, the dark central bar the interquartile range, and thin dark lines the lower and upper adjacent values. (<bold>B</bold>) two-sided t-test, t<sub>30</sub> = 35.66, p = 4.03x10<sup>−26</sup>. (<bold>C</bold>) two-sided t-test, t<sub>31</sub> = 2.31, p = 0.028.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Parameter recovery.</title><p>We first simulated choice data through the models, using the best-fitting parameters. Simulated data was then fed again to the fitting procedure using HBI, separately for each model, in the presence of noise (the update was sometimes not done for the real, correct state but rather for an alternative, random state). Parameters were recovered for each participant, block, and model. Recovered and original parameter values were then pooled across the models and plotted. Values were normalized in the interval [0, 1] for visualisation purposes (note that the normalisation does not affect the strength of the correlation).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Strategy analysis of excluded participants.</title><p>Excluded participants tended to rely more often on a Feature RL strategy. Binomial test Feature RL: base rate of 0.44 calculated from the main participants group, P(57|103) = 0.029. Binomial test Abstract RL: base rate of 0.52 calculated from the main participants group, P(41|103) = 0.014.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig3-figsupp4-v2.tif"/></fig></fig-group><p>Model comparison at the single participant and block levels (<xref ref-type="bibr" rid="bib65">Piray et al., 2019</xref>) provided a direct way to infer which algorithm was more likely to explain learning in any given block. Overall, similar proportions of blocks were classified as Feature RL and Abstract RL. This indicates that participants used both learning strategies (binomial test applied to all blocks: proportion of Abstract RL = 0.47 vs. equal level = 0.5, <italic>P</italic>(212|449) = 0.26, <xref ref-type="fig" rid="fig3">Figure 3B</xref>; two-sided t-test of participant-level proportions: lower, but close to 0.5, <italic>t</italic><sub>32</sub> = −2.87, p = 0.007, <xref ref-type="fig" rid="fig3">Figure 3B</xref> inset).</p><p>As suggested by the simulations (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), the strategy that best explained participant block data accounted for the distribution of learning speed measures in each block. Where learning proceeded slowly, Feature RL was consistently predominant (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), while the reverse happened in blocks where participants displayed fast learning (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Among participants, the degree of abstraction (propensity to use Abstract RL) correlated with the empirical learning speed (N = 33, robust regression, slope = 0.52, <italic>t</italic><sub>31</sub> = 4.56, p = 7.64x10<sup>−5</sup>, <xref ref-type="fig" rid="fig3">Figure 3C</xref> top). Participant confidence in having performed the task well was also significantly correlated with the degree of abstraction (N = 31, robust regression, slope = 0.026, <italic>t</italic><sub>29</sub> = 2.69, p = 0.012, <xref ref-type="fig" rid="fig3">Figure 3C</xref>, bottom). In addition to the finding that confidence related to learning speed (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), these results raise intriguing questions about the function of metacognition, as participants appeared to comprehend their own ability to construct and use abstractions (<xref ref-type="bibr" rid="bib18">Cortese et al., 2020</xref>).</p><p>The two RL algorithms revealed a second aspect of learning. Considering all blocks regardless of fit (paired comparison), feature RL appeared to have higher learning rates α compared with Abstract RL (two-sided Wilcoxon rank sum test against median 0, z = 14.33, p &lt; 10<sup>−30</sup>, <xref ref-type="fig" rid="fig3">Figure 3D</xref>). A similar asymmetry was found with greediness (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, two-sided Wilcoxon rank sum test against median 0, z = 7.14, p &lt; 10<sup>−10</sup>). Yet, more specifically, considering only the model (Feature RL or Abstract RL) which provided the best fit on a given block, resulted in Feature RL displaying <italic>lower</italic> learning rates and greediness (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). The order inverted entirely when considering the model which provided the <italic>worst</italic> fit: higher learning rates and greediness for Feature RL (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). These differences can be explained intuitively as follows. In Feature RL, exploration of the task state-space takes longer - in short blocks (best fit by the Abstract RL strategy) a higher learning rate is necessary for the Feature RL agent to make larger updates on states that are infrequently visited. Results also suggest that action selection tends to follow the same principles – more deterministic in blocks that are best fit by Abstract RL (i.e. large β for shorter blocks).</p><p>We predicted that use of abstraction should increase with learning, because the brain has to construct abstractions in the first place and must initially rely on Feature RL. To test this hypothesis, we quantified the number of participants using a Feature RL or Abstract RL strategy in their first and last blocks. On their first block, most participants relied on Feature RL, while the pattern reversed in the last block (two-sided sign test, z = −2.77, p = 0.006, <xref ref-type="fig" rid="fig3">Figure 3F</xref>). Computing the abstraction level separately for the session median split of early and late blocks also resulted in higher abstraction in late blocks (two-sided sign test, z = −2.94, p = 0.003, <xref ref-type="fig" rid="fig3">Figure 3G</xref>). These effects were complemented by two block-by-block analyses, displaying an increase in abstraction from early to late blocks (<xref ref-type="fig" rid="fig3">Figure 3H</xref>, and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>).</p><p>Supporting the current modelling framework, the mean expected value of the chosen action was higher for Abstract RL (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B–C</xref>), and model hyperparameters could be recovered in the presence of noise (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>; see Materials and methods) (<xref ref-type="bibr" rid="bib63">Palminteri et al., 2017</xref>). Given the lower learning speed in excluded participants, the distribution of strategies was also different among them, with a higher ratio of Feature RL blocks (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>).</p></sec><sec id="s2-5"><title>The role of vmPFC in constructing goal-dependent value from sensory features</title><p>The computational approach confirmed that participants relied on both a low-level feature strategy, and a more sophisticated abstract strategy (i.e. Feature RL and Abstract RL; <xref ref-type="fig" rid="fig2">Figures 2D</xref> and <xref ref-type="fig" rid="fig3">3B</xref>). Beside proving that abstract representations were generally associated with higher expected value, the modelling approach further allowed us to explicitly <italic>classify</italic> trials as belonging to either learning strategy. Here, our goal was to dissociate neural signatures of these distinct learning strategies in order to show how abstract representations are constructed by the human brain.</p><p>First, we reasoned that an anticipatory value signal might emerge in the vmPFC at stimulus presentation (<xref ref-type="bibr" rid="bib39">Knutson et al., 2005</xref>). We performed a general linear model (GLM) analysis of neuroimaging data with regressors for ‘High-value’ and ‘Low-value’ trials, selected by the block-level best fitting algorithm (Feature RL or Abstract RL, while controlling for other confounding factors such as time and strategy itself; see Materias and methods and Supplementary note one for the full GLM and regressors specification). As predicted, activity in the vmPFC strongly correlated with value magnitude (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). That is, the vmPFC indexed the anticipated value constructed from pacman features at stimulus presentation time. We used this signal to functionally define, for ensuing analyses, the subregion of the vmPFC that was maximally related to task computations about value when pacman visual features were integrated. Concurrently, activity in insular and dorsal prefrontal cortices increased under conditions of low expected value. This pattern of activity is consistent with previous studies on error monitoring and processing (<xref ref-type="bibr" rid="bib6">Bastin et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Carter et al., 1998</xref>; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural substrates of value construction during learning.</title><p>(<bold>A</bold>) Correlates of anticipated value at pacman stimulus presentation time. Trials were labelled according to a median split of the expected value for the chosen action, as computed by the best fitting model, Feature RL or Abstract RL, at the block level. Mass univariate analysis, contrast ‘High-value’ &gt; ‘Low-value’. vmPFC peaks at [2 50 -10]. The statistical parametric map was z-transformed and plotted at <italic>p</italic>(FWE) &lt; 0.05. (<bold>B</bold>) Psychophysiological interaction, using as seed a sphere (radius = 6 mm) centred around the participant-specific peak voxel, constrained within a 25 mm sphere centred around the group-level peak coordinate from contrast in (<bold>A</bold>). The statistical parametric map was z-transformed and plotted at <italic>p</italic>(fpr) &lt; 0.001 (one-sided, for positive contrast - increased coupling). (<bold>C</bold>) The strength of the interaction between the vmPFC and VC was positively correlated with participant’s ability to learn block rules. Dots represent individual participant data points, and the line is the regression fit. The experiment was conducted once (n = 33 biologically independent samples), * p&lt;0.05.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Csv: panel C.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig4-data1-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>‘Low value’ &gt; ‘High value’ GLM contrast.</title><p>Neural correlates of (predicted) low value at visual stimulus presentation time. Trials were labelled according to a median split of the expected value for the chosen option as computed by the best fitting model, at the participants and block level. The statistical parametric map was z-transformed, and false-positive means of cluster formation (fpr) correction was applied. p(fpr) &lt; 0.001, Z &gt; 3.09.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Neuro-behavioural correlation between VC-vmPFC coupling and abstraction.</title><p>The strength of the interaction between the vmPFC and VC showed a weak positive association with the abstraction level across participants. Dots represent individual participant data points, and the line is the regression fit. The experiment was conducted once (n = 33 biologically independent samples), robust regression fit: N=31, slope = 0.013, <italic>t</italic><sub>31</sub> = 1.56, p = 0.065 (one-sided).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig4-figsupp2-v2.tif"/></fig></fig-group><p>In order for the vmPFC to construct goal-dependent value signals, it should receive relevant feature information from other brain areas and specifically from visual cortices, given the nature of our task. Thus, we computed a psychophysiological interaction (PPI) analysis (<xref ref-type="bibr" rid="bib27">Friston et al., 1997</xref>), to isolate regions in which functional coupling with the vmPFC at the time of stimulus presentation was dependent on the magnitude of expected value. Supporting the idea that the vmPFC based its predictions on the integration of visual features, only connectivity between the visual cortex (VC) and vmPFC was higher on trials that carried large expected value, compared to low-value trials (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Strikingly, the strength of this VC - vmPFC interaction was associated with the overall learning speed among participants (N = 31, robust regression, slope = 0.016, <italic>t</italic><sub>29</sub> = 2.55, p = 0.016, <xref ref-type="fig" rid="fig4">Figure 4C</xref>), such that participants with stronger modulation of the coupling between the vmPFC and VC also learned block rules faster. The strength of the vmPFC - VC coupling showed a non-significant trend with the level of abstraction (N = 31, robust regression, slope = 0.013, t<sub>29</sub> = 1.56, p = 0.065 one-sided, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). However, this study was not optimised to detect between subject correlations that normally require a larger number of subjects. Therefore, future work is required to confirm or falsify this result.</p></sec><sec id="s2-6"><title>A value-sensitive vmPFC subregion prioritises abstract elements</title><p>Having established that the vmPFC computes a goal-dependent value signal, we evaluated whether the activity level of this region was sensitive to the strategies that participants used. To do so, we used the same GLM introduced earlier, and estimated two new statistical maps from the regressors ‘Abstract RL’ and ‘Feature RL’, while controlling for idiosyncratic features of the task, that is, high/low value and early/late trials (see Materials and methods and Supplementary note 1). We extracted the peak activity at the participant level, under Feature RL and Abstract RL conditions, in two regions-of-interest (ROI). Specifically, we focused on the vmPFC and the HPC, as both have been consistently linked with abstraction, and feature-based and conceptual learning. The HPC was defined anatomically (AAL atlas, <xref ref-type="fig" rid="fig5">Figure 5A</xref> top), while the vmPFC was defined as voxels sensitive to the orthogonal contrast ‘High value’ &gt; ‘Low value’ from the same GLM (<xref ref-type="fig" rid="fig5">Figure 5A</xref> bottom). A linear mixed effects model (LMEM) with fixed effects ‘ROI’ and ‘strategy’ [LMEM: ‘y ~ ROI * strategy + (1|participants)’, y: ROI activity] revealed significant main effects of ‘ROI’ (<italic>t</italic><sub>128</sub> = 2.16, <italic>p</italic> = 0.033), and ‘strategy’ (<italic>t</italic><sub>128</sub> = 3.07, p = 0.003), and a significant interaction (<italic>t</italic><sub>128</sub> = −2.29, p = 0.024), illustrating different HPC and vmPFC recruitment (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Post-hoc comparisons showed vmPFC activity levels distinguished Feature RL and Abstract RL cases well (LMEM: <italic>t</italic><sub>64</sub> = 2.94, <italic>p<sub>(FDR)</sub></italic> = 0.009), while the HPC remained agnostic to the style of learning (LMEM: <italic>t</italic><sub>64</sub> = 0.62, <italic>p<sub>(FDR)</sub></italic> = 0.54). Alternative explanations are unlikely, as there was no effect in terms of both the correlation between value-type trials and algorithms, and task difficulty, measured by reaction times (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Neural substrate of abstraction.</title><p>(<bold>A</bold>) Regions of interest for univariate and multivariate analyses. The HPC was defined through automated anatomical labelling (FreeSurfer). The vmPFC was functionally defined as the cluster of voxels found with the orthogonal contrast ‘High value’ &gt; ‘Low value’, at <italic>P</italic>(unc) &lt; 0.0001. (<bold>B</bold>) ROI activity levels corresponding to each learning mode were extracted from the contrasts ‘Feature RL’ &gt; ‘Abstract RL’, and ‘Abstract RL’ &gt; ‘Feature RL’. Coloured bars represent the mean, and error bars the SEM. (<bold>C</bold>) Multivariate (decoding) analysis in three regions of interest: VC, HPC, vmPFC. Binary decoding was performed for each feature (e.g. colour: red vs green), by using trials from blocks labelled as Feature RL or Abstract RL. Colour bars represent the mean, error bars the SEM, and grey dots represent individual data points (for each individual, taken as the average across all three classifications, i.e., of all features). Results were obtained from leave-one-run-out cross-validation. The experiment was conducted once (n = 33 biologically independent samples), * p&lt;0.05, ** p&lt;0.01. (<bold>D</bold>) Classification was performed for each feature pair (e.g. colour: red vs green), separately for blocks in which the feature in question was relevant or irrelevant to the block’s rule. The statistical map represents the strength of the reduction in accuracy between trials in which the feature was relevant compared to irrelevant, averaged over all features and participants. (<bold>E</bold>) Classification of the rule (2x2 blocks only). For each participant, classification was performed as fruit 1 vs fruit 2. In (<bold>D–E</bold> ), statistical parametric maps were z-transformed, false-positive means of cluster formation (fpr) correction was applied. p(fpr) &lt; 0.01, Z &gt; 2.33.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Csv: panel B.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig5-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Csv: panel C.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig5-data2-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Ratio correct in Feature RL and Abstract RL.</title><p>Mean ratio of correct choices total choices, for trials labelled as Feature RL and Abstract RL (i.e., the best fitting algorithm on a given block). A total of three (Abstract RL: two, Feature RL: one) outliers were removed. Each coloured dot represents the average across selected blocks for a single participant - in either Abstract RL (grey) or Feature RL (cyan). Shaded areas represent the density plot, central white dot the median, the dark central bar the interquartile range, and thin dark lines the lower and upper adjacent values. Two-sided t-test, t<sub>29</sub> = 3.23, p = 0.003 (**).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Value functions correlations and reaction time differences in Feature RL and Abstract RL trials.</title><p>(<bold>A</bold>) Fisher-transformed coefficients (Spearman ρ) of the correlation between high/low value and Feature RL / Abstract RL across trials. The coloured bar represents the population mean, the error bar the SEM, and dots individual participants’ data. The two labels used in the main GLM were uncorrelated. Wilcoxon sign rank test against median 0, z = 0.72, p = 0.47. (<bold>B</bold>) Reaction time (RT) data pooled over all participants for trials in blocks labelled as ‘Feature RL’ or ‘Abstract RL’. RT was not significantly different between the two strategies. Wilcoxon sign rank test between the two distributions, z = 1.48, p = 0.14.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>K-fold cross-validation in feature decoding, using Feature RL and Abstract RL trials separately.</title><p>The split of the data in training and test subsets was done (N=20) by randomly selecting 80% of the N<sub>min</sub> trials across the two levels of the target feature and the two conditions (Feature RL and Abstract RL). Thus, in each fold, the same number of trials for each feature and condition was used to train the classifier, avoiding possible accuracy confounds due to varying numbers of trials used for training. As in the primary analysis reported in the main text, classification accuracy was significantly higher in Abstract RL compared with Feature RL trials in both the HPC and vmPFC (two-sided Wilcoxon signed rank test, HPC: z = −4.21, p(FDR) &lt; 0.001, vmPFC: z = −3.15, p(FDR) = 0.002), but not in VC (z = −1.30, p(FDR) = 0.20). The difference in feature decodability was significantly larger in the HPC and vmPFC compared to the VC (LMEM model ‘y ~ ROI + (1|participants)’, y: difference in decodability, <italic>t</italic><sub>97</sub> = 2.52, p = 0.013).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig5-figsupp3-v2.tif"/></fig></fig-group><p>The next question we asked was, ‘Can we retrieve feature information from HPC and vmPFC activity patterns?’ In order to abstract and operate in the latent space, an agent is still bound to represent and use the features, because the rules are dictated by feature combinations. One possibility is that feature information is represented solely in sensory areas. What matters then is the connection with and/or the read out of vmPFC or HPC. Accordingly, neither HPC nor vmPFC should represent feature information, regardless of the strategy used. Alternatively, feature-level information could also be represented in higher cortical regions under Abstract RL to explicitly support (value-based) relational computations (<xref ref-type="bibr" rid="bib61">Oemisch et al., 2019</xref>). To resolve this question, we applied multivoxel pattern analysis to classify basic feature information (e.g. colour: red vs green) in three regions of interest: the VC, HPC, and vmPFC, separately for trials labelled as Feature RL or Abstract RL. We found that classification accuracy was significantly higher in Abstract RL trials compared with Feature RL trials in both the HPC and vmPFC (two-sided t-test, HPC: <italic>t</italic><sub>32</sub> = −2.37, <italic>p<sub>(FDR)</sub></italic> &lt; 0.036, vmPFC: <italic>t</italic><sub>32</sub> = −2.51, <italic>p<sub>(FDR)</sub></italic> = 0.036, <xref ref-type="fig" rid="fig5">Figure 5C</xref>), while the difference was of opposite sign in VC (<italic>t</italic><sub>32</sub> = 1.61, <italic>p<sub>(FDR)</sub></italic> = 0.12, <xref ref-type="fig" rid="fig5">Figure 5C</xref>). The increased feature decodability in Abstract RL was significantly larger in the HPC and vmPFC compared to the VC (LMEM model ‘y ~ ROI + (1|participants)’, y: difference in decodability, t<sub>97</sub> = 3.37, p = 0.001). Due to the nature of the task, the number of trials in each category could vary and thus confounds the analysis. A control analysis equating the number of training trials for each feature and condition replicated the original finding (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). These empirical results support the second hypothesis. In Abstract RL, features are represented in the neural circuitry incorporating the HPC and vmPFC, beyond a simple read out of sensory cortices. In Feature RL, representing feature-level information in sensory cortices alone should suffice because each visual pattern mapped to a task-state.</p><p>We expanded on this idea with two searchlight multivoxel pattern analyses. In short, we inquired which brain regions are sensitive to feature relevance, and whether we could recover representations of the latent rule itself (the fruit preference). Beside the occipital cortex, significant reduction in decoding accuracy was also detected in the OFC, ACC, vmPFC and dorsolateral PFC when a feature was irrelevant to the rule, compared to when it was relevant (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Multivoxel patterns in the dorsolateral PFC and lateral OFC further predicted fruit class (<xref ref-type="fig" rid="fig5">Figure 5E</xref>).</p></sec><sec id="s2-7"><title>Artificially injecting value in sensory representations with neurofeedback fosters abstraction</title><p>Our computational and neuroimaging results indicate that valuation serves a key function in abstraction. Two hypotheses on the underlying mechanism can be outlined here. On one hand, the effect of vmPFC value computations could remain localised within the prefrontal circuitry. For example, this could be achieved by representing and ranking incoming sensory information for further processing within the HPC-OFC circuitry. Alternatively, value computation could determine abstractions by directly affecting early sensory areas – that is, a top-down (attentional) effect to ‘tag’ relevant sensory information (<xref ref-type="bibr" rid="bib3">Anderson et al., 2011</xref>). Work in rodents has reported strong top-down modulation of sensory cortices by OFC neurons implicated in value computations (<xref ref-type="bibr" rid="bib5">Banerjee et al., 2020</xref>; <xref ref-type="bibr" rid="bib50">Liu et al., 2020</xref>). We thus hypothesised that abstraction could result from a direct effect of value in the VC. Therefore, artificially adding value to a neural representation of a task-relevant feature should result in enhanced behavioural abstraction.</p><p>Decoded neurofeedback is a form of neural reinforcement based on real time fMRI and multivoxel pattern analysis. It is the closest approximation to a non-invasive causal manipulation, with high specificity and administered without participant awareness (<xref ref-type="bibr" rid="bib51">Lubianiker et al., 2019</xref>; <xref ref-type="bibr" rid="bib57">Muñoz-Moldes and Cleeremans, 2020</xref>; <xref ref-type="bibr" rid="bib70">Shibata et al., 2019</xref>). Such reinforcement protocols can reliably lead to persistent behavioural or physiological changes (<xref ref-type="bibr" rid="bib15">Cortese et al., 2016</xref>; <xref ref-type="bibr" rid="bib41">Koizumi et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Shibata et al., 2011</xref>; <xref ref-type="bibr" rid="bib71">Sitaram et al., 2017</xref>; <xref ref-type="bibr" rid="bib76">Taschereau-Dumouchel et al., 2018</xref>). We used this procedure in a follow-up experiment (N = 22, a subgroup from the main experiment; see Materials and methods) to artificially add value (rewards) to neural representation in VC of a target task-related feature (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). At the end of two training sessions, participants completed 16 blocks of the pacman fruit preference task, outside of the scanner. Task blocks could be labelled as ‘relevant’ (eight blocks) if the feature tagged with value was relevant to the block rule, or ‘irrelevant’ otherwise (eight blocks).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Artificially adding value to a feature’s neural representation.</title><p>(<bold>A</bold>) Schematic diagram of the follow-up multivoxel neurofeedback experiment. During the neurofeedback procedure, participants were rewarded for increasing the size of a disc on the screen (max session reward 3000 JPY). Unbeknownst to them, disc size was changed by the computer program to reflect the likelihood of the target brain activity pattern (corresponding to one of the task features) measured in real time. (<bold>B</bold>) Blocks were subdivided based on the feature targeted by multivoxel neurofeedback as ‘relevant’ or ‘irrelevant’ to the block rules. Scatter plots replicate the finding from the main experiment, with a strong association between Feature RL / Abstract RL and learning speed. Each coloured dot represents a single block from one participant, with data aggregated from all participants. (<bold>C</bold>) Abstraction level was computed for each participant from all blocks belonging to: (1) left, the latter half of the main experiment (as in <xref ref-type="fig" rid="fig3">Figure 3G</xref>, but only selecting participants who took part in the multivoxel neurofeedback experiment); (2) centre, post-neurofeedback for the ‘relevant’ condition; (3) right, post-neurofeedback for the ‘irrelevant’ condition. Coloured dots represent participants. Shaded areas indicate the density plot. Central white dots show the medians. The dark central bar depicts the interquartile range, and dark vertical lines indicate the lower and upper adjacent values. (<bold>D</bold>) Bootstrapping the difference between model probabilities on each block, separately for ‘relevant’ and ‘irrelevant’ conditions. The experiment was conducted once (n = 22 biologically independent samples), * p&lt;0.05, *** p&lt;0.001.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Csv: panel B, irrelevant blocks.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig6-data1-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig6sdata2"><label>Figure 6—source data 2.</label><caption><title>Csv: panel B, relevant blocks.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig6-data2-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig6sdata3"><label>Figure 6—source data 3.</label><caption><title>Csv: panel C.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig6-data3-v2.csv"/></supplementary-material></p><p><supplementary-material id="fig6sdata4"><label>Figure 6—source data 4.</label><caption><title>Csv: panel D.</title></caption><media mime-subtype="octet-stream" mimetype="application" xlink:href="elife-68943-fig6-data4-v2.csv"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Neurofeedback experiment results.</title><p>(<bold>A</bold>) Neurofeedback (nfb) scores from each block of training, for sessions 1 and 2. Black dots represent data from individual participants, as the score obtained within a block of nfb training. Large green dots represent the mean, and the error bars the SEM. The red line is a linear fit on the mean data points. Robust regression fits, session 1: slope = 0.75, t<sub>7</sub> = 0.60, p = 0.57; session 2: slope = 1.67, t<sub>7</sub> = 1.98, p = 0.088. (<bold>B</bold>) Average nfb scores attained in each session. The red circles represent individual participants, the grey bars group means, and the black error bars the SEM. (<bold>C</bold>) Correlation between the sum of mean nfb scores (session 1 + session 2) and the subsequent increase in abstraction. The increase in abstraction was calculated as the difference between the abstraction level in the ‘relevant’ blocks and the abstraction level at the beginning of the main learning task. Black circles represent individual participants, and the black line a linear fit. N = 22, Spearman’s rho = 0.39, p = 0.036 (one-sided test for positive correlation). Source data files. Source data files for each figure are available with this manuscript.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68943-fig6-figsupp1-v2.tif"/></fig></fig-group><p>Data from the ‘relevant’ and ‘irrelevant’ blocks were analysed separately. The same model-fitting procedure used in the main experiment established whether participant choices in the new blocks were driven by a Feature RL or Abstract RL strategy. ‘Relevant’ blocks appeared to be associated with a behavioural shift toward Abstract RL, whereas there was no substantial qualitative effect in ‘irrelevant’ blocks (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). To quantify this effect, we first applied a binomial test, finding that behaviour in blocks where the targeted feature was relevant displayed markedly increased abstraction (base rate 0.5, number of Abstract RL blocks given total number of blocks; ‘relevant’: P(123|176) = 1.37x10<sup>−7</sup>, ‘irrelevant’: P(90|176) = 0.82). We then measured the abstraction level for each participant and directly compared it to the level attained by the same participants in the late blocks of main experiments (from <xref ref-type="fig" rid="fig3">Figure 3G</xref>). Participants increased their use of abstraction in ‘relevant’ blocks, whereas no significant difference was detected in the ‘irrelevant’ blocks (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, two-sided Wilcoxon signed rank test, ‘relevant’ blocks: z = 2.44, p = 0.015, ‘irrelevant’ blocks: z = −1.55, p = 0.12, ‘relevant’ vs ‘irrelevant’: z = 4.01, p = 6.03x10<sup>−5</sup>). Finally, we measured the difference between model probabilities P<sub>(Feature RL)</sub> - P<sub>(Abstract RL)</sub> for each block, and bootstrapped the mean over blocks (with replacement) 10,000 times to generate a distribution for ‘relevant’ and ‘irrelevant’ conditions. Replicating the results reported above, behaviour in ‘relevant’ blocks was more likely to be driven by Abstract RL (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, perm. test p &lt; 0.001), while Feature RL tended to appear more in ‘irrelevant’ blocks. Participants were successful at increasing the disk size in the neurofeedback task (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1A–B</xref>). Furthermore, those who were more successful were also more likely to display larger increases in abstraction in the subsequent behavioural test compared to their initial level (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1C</xref>).</p><p>A strategy shift toward abstraction, specific to blocks in which the target feature was tagged with reward, indicates that the effect of value in facilitating abstraction is likely to be mediated by a change in the early processing stage of visual information. In this experiment, by means of neurofeedback, value (in the form of external rewards) ‘primed’ a target feature. Hence, the brain used these ‘artificial’ values when constructing abstract representations by tagging certain sensory channels. Critically, this manipulation indicates that value tagging of early representation has a causal effect on abstraction and consequently on the learning strategy.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The ability to generate abstractions from simple sensory information has been suggested as crucial to support flexible and adaptive behaviours (<xref ref-type="bibr" rid="bib17">Cortese et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Ho et al., 2019</xref>; <xref ref-type="bibr" rid="bib83">Wikenheiser and Schoenbaum, 2016</xref>). Here, using computational, we found that value predictions drive participant selections of the appropriate representation to solve the task. Participants explored and used task dimensionality through learning, as they shifted from a simple feature-based strategy to using more sophisticated abstractions. The more participants used Abstract RL, the faster they became at solving the task. Note that in this task, structure, learning speed, and abstraction are linked. To learn faster, an agent must use Abstract RL, as other strategies would result in slower completion of task blocks.</p><p>These results build on the idea that efficient decision-making processes in the brain depend on higher-order, summarised representations of task-states (<xref ref-type="bibr" rid="bib60">Niv, 2019</xref>; <xref ref-type="bibr" rid="bib66">Schuck et al., 2016</xref>). Further, abstraction likely requires a functional link between sensory regions and areas encoding value predictions about task states (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, the VC-vmPFC coupling was positively correlated with participant’s learning speed). This is consistent with previous work that demonstrates how estimating reward value of individual features provides a reliable and adaptive mechanism in RL (<xref ref-type="bibr" rid="bib24">Farashahi et al., 2017</xref>). We extend this notion by showing that the mechanism may support formation of abstract representations to be further used for learning computations, for example selection of the appropriate abstract representation.</p><p>An interesting question concerns whether the brain uses abstract representations in isolation - operating in a hypothesis-testing regime - that is, favouring the current best model; or whether representations may be used to update multiple internal models, with behaviour determined by their synthesis (as in the mixture-of-experts architecture). The latter implementation may not be the most efficient computationally - the brain would have to run multiple processes in parallel, but it would be very data efficient since one data point can be used to update several models. Humans might (at least at the conscious level) engage with one hypothesis at the time. However, there is circumstantial evidence that multiple strategies might be computed in parallel but deployed one at the time (<xref ref-type="bibr" rid="bib21">Domenech et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Donoso et al., 2014</xref>). Furthermore, in many cases, the brain may have access to only limited data points, while parallel processing is a major feature of neural circuits (<xref ref-type="bibr" rid="bib2">Alexander and Crutcher, 1990</xref>; <xref ref-type="bibr" rid="bib47">Lee et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Spitmaan et al., 2020</xref>). In this study, we aimed to show that arbitration between feature and abstract learning may be achieved using a relatively simple algorithm (the mixture-of-experts RL) and then proceeded to characterise the neural underpinnings of these two types of learning (i.e. Feature RL and Abstract RL). Admittedly, in the present work the mixture-of-experts RL does not provide a solid account of the data when compared to the more parsimonious Feature RL and Abstract RL in isolation. Future work will need to establish the actual computational strategy employed by the human brain. Of particular importance will be further examining how such strategies vary across circumstances (tasks, contexts, or goals).</p><p>There is an important body of work considering how the HPC is involved in formation and update of conceptual information (<xref ref-type="bibr" rid="bib11">Bowman and Zeithamova, 2018</xref>; <xref ref-type="bibr" rid="bib44">Kumaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib52">Mack et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">McKenzie et al., 2014</xref>). Likely, the role of the HPC is to store, index, and update conceptual/schematic memories (<xref ref-type="bibr" rid="bib52">Mack et al., 2016</xref>; <xref ref-type="bibr" rid="bib78">Tse et al., 2011</xref>; <xref ref-type="bibr" rid="bib77">Tse et al., 2007</xref>). 'Creation' of new concepts or schemas may occur elsewhere. A good candidate could be the mPFC or the vmPFC in humans (<xref ref-type="bibr" rid="bib53">Mack et al., 2020</xref>; <xref ref-type="bibr" rid="bib78">Tse et al., 2011</xref>). Indeed, the vmPFC exhibits value signals directly modulated by cognitive requirements (<xref ref-type="bibr" rid="bib13">Castegnetti et al., 2021</xref>). Our results expose a potential mechanism of how the vmPFC interacts with the HPC in construction of goal-relevant abstractions. vmPFC-driven valuation of low-level sensory information serves to channel-specific features/components to higher order areas (e.g. the HPC, vmPFC, but also the dorsal prefrontal cortex, for instance). Congruent with this interpretation, we found that the vmPFC was more engaged in Abstract RL, while the HPC was equally active under both abstract and feature-based strategies (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). When a feature was irrelevant to the rule, its decodability from activity patterns in OFC/DLPFC decreased (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). These findings accord well with the role of prefrontal regions in constructing goal-dependent task states and abstract rules from relevant sensory information (<xref ref-type="bibr" rid="bib1">Akaishi et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Schuck et al., 2016</xref>; <xref ref-type="bibr" rid="bib80">Wallis et al., 2001</xref>). Furthermore, we found block rules were also encoded in multivoxel activity patterns within the OFC/DLPFC circuitry (<xref ref-type="fig" rid="fig5">Figure 5E</xref>; <xref ref-type="bibr" rid="bib8">Bengtsson et al., 2009</xref>; <xref ref-type="bibr" rid="bib56">Mian et al., 2014</xref>; <xref ref-type="bibr" rid="bib80">Wallis et al., 2001</xref>).</p><p>How these representations are actually used remains an open question. This study nevertheless suggests that there is no single region of the brain that maintains a fixed task state. Rather, the configuration of elements that determines a state is continuously reconstructed over time. At first glance, this may appear costly and inefficient. But this approach would provide high flexibility in noisy and stochastic environments, and where temporal dependencies exist (as in most real-world situations). By continuously recomputing task states, the agent can make more robust decisions because these are related to the subset of most relevant, up-to-date information. Such a computational coding scheme shares strong analogies with HPC neural coding, whereby neurons continuously generate representations of alternative future trajectories (<xref ref-type="bibr" rid="bib38">Kay et al., 2020</xref>), and replay past cognitive trajectories (<xref ref-type="bibr" rid="bib67">Schuck and Niv, 2019</xref>).</p><p>One significant topic for discussion concerns the elements used to construct abstractions. We leveraged simple visual features (colour, or stripe orientation), rather than more complex stimuli or objects that can be linked conceptually (<xref ref-type="bibr" rid="bib44">Kumaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib88">Zeithamova et al., 2019</xref>). Abstractions happen at several levels, from features, to exemplars, concepts/categories, and all the way to rules and symbolic representations. In this work, we effectively studied one of the lowest levels of abstraction. One may wonder whether the mechanism we identified here generalises to more complex scenarios. While our work cannot decisively support this, we believe it unlikely that the brain uses an entirely different strategy to generate new representations at different levels of abstraction. Rather, the internal source of information abstracted should be different, but the algorithm itself should be the same or, at the very least, highly similar. The fact our PPI analysis showed a link between the vmPFC and VC may point to this distinguishing characteristic of our study. Learning through abstraction of simple visual features should be related to early VC. Features in other modalities, for example, auditory, would involve functional coupling between the auditory cortex and the vmPFC. When learning about more complex objects or categories, we expect to see stronger reliance on the HPC (<xref ref-type="bibr" rid="bib44">Kumaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib52">Mack et al., 2016</xref>). Future studies could incorporate different levels of complexity, or different modalities, within a similar design so as to directly test this prediction and dissect exact neural contributions. Depending on which type of information is relevant at any point in time, we suspect that different areas will be coupled with the vmPFC to generate value representations.</p><p>In our second experiment, we implemented a direct assay to test our (causal) hypothesis that valuation of features guides abstraction in learning. Artificially adding value in the form of reward to a feature representation in the VC resulted in increased use of abstractions. Thus, the facilitating effect of value on abstraction can be directly linked to changes in the early processing stage of visual information. Consonant with this interpretation, recent work in mice has elegantly reported how value governs a functional remapping in the sensory cortex by direct lateral OFC projections carrying RPE information (<xref ref-type="bibr" rid="bib5">Banerjee et al., 2020</xref>), as well as by modulating the gain of neurons to irrelevant stimuli (<xref ref-type="bibr" rid="bib50">Liu et al., 2020</xref>). While these considerations clearly point to a central role of the vmPFC and valuation in abstraction by controlling sensory representations, it remains to be investigated whether this effect results in more efficient <italic>construction</italic> of abstract representations, or in better <italic>selection</italic> of internal abstract models.</p><p>Given the complex nature of our design, there are some limitations to this work. For example, there isn’t an applicable feature decoder to test actual feature representations (e.g. colour vs orientation), or the likelihood of one feature against another. In our task design, in every trial, all features were used to define pacman characters. Furthermore, we did not have a localiser session in which features were presented in isolation (see Supplementary note two for further discussion). Future work could investigate how separate feature representations emerge on the path to abstractions, for example in the parietal cortex or vmPFC, and their relation to feature levels (e.g. for colour: red vs green) as reported here. We speculate that parallel circuits linking the prefrontal cortex and basal ganglia could track these levels of abstraction, possibly in hierarchical fashion (<xref ref-type="bibr" rid="bib4">Badre and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib17">Cortese et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Haruno and Kawato, 2006</xref>).</p><p>Some may point out that what we call ‘Abstract RL’ is, in fact, just attention-mediated enhanced processing. Yet, if top-down attention were the sole driver in Abstract RL, we contend that the pattern of results would have been different. For example, we would expect to see a marked increase in feature decodability in VC (<xref ref-type="bibr" rid="bib30">Guggenmos et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Kamitani and Tong, 2005</xref>). This was not the case here, with only a minimal, non-significant, increase. More importantly, the results of the decoded neurofeedback manipulation question this interpretation. Because decoded neurofeedback operates unconsciously (<xref ref-type="bibr" rid="bib57">Muñoz-Moldes and Cleeremans, 2020</xref>; <xref ref-type="bibr" rid="bib70">Shibata et al., 2019</xref>), value was added directly at the sensory representation level (limited to the targeted region), precluding alternative top-down effects. That is not to say that attention does not significantly mediate this type of abstract learning; however, we argue that attention is most likely an effector of the abstraction and valuation processes (<xref ref-type="bibr" rid="bib43">Krauzlis et al., 2014</xref>). A simpler top-down attentional effect was indeed evident in the supplementary analysis comparing feature decoding in ‘relevant’ and ‘irrelevant’ cases (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Occipital regions displayed large effect sizes, irrespective of the learning strategy used to solve the task.</p><p>While valuation and abstraction appear tightly associated in reducing the dimensionality of the task space, what is the underlying mechanism? The degree of neural compression in the vmPFC has been shown to relate to features most predictive of positive outcomes, under a given goal (<xref ref-type="bibr" rid="bib53">Mack et al., 2020</xref>). Similarly, the geometry of neural activity in generalisation may be key here. Neural activity in the PFC (and HPC) explicitly generates representations that are simultaneously abstract and high dimensional (<xref ref-type="bibr" rid="bib10">Bernardi et al., 2020</xref>). An attractive view is that valuation may be interpreted as an abstraction in itself. Value could provide a simple and efficient way for the brain to operate on a dimensionless axis. Each point on this axis could index a certain task state, or even behavioural strategy, as a function of its assigned abstract value. Neuronal encoding of feature-specific value, or choice options, may help the system construct useful representations that can, in turn, inform flexible behavioural strategies (<xref ref-type="bibr" rid="bib60">Niv, 2019</xref>; <xref ref-type="bibr" rid="bib66">Schuck et al., 2016</xref>; <xref ref-type="bibr" rid="bib84">Wilson et al., 2014</xref>).</p><p>In summary, this work provides evidence for a function of valuation that exceeds the classic view in decision-making and neuroeconomics. We show that valuation subserves a critical function in constructing abstractions. One may further speculate that valuation, by generating a common currency across perceptually different stimuli, may be an abstraction in itself, and that it is tightly linked to the concept of task states in decision-making. We believe this work not only offers a new perspective on the role of valuation in generating abstract thoughts, but also reconciles apparently disconnected findings in decision-making and memory literature on the role of the vmPFC. In this context, value is not a simple proxy of a numerical reward signal, but is better understood as a conceptual representation or schema built on-the-fly to respond to a specific behavioural demand. Thus, we believe our findings provide a fresh view of the invariable presence of value signals in the brain that play an important algorithmic role in development of sophisticated learning strategies.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Forty-six participants with normal or corrected-to-normal vision were recruited for the main experiment (learning task). The sample size was chosen according to prior work and recommendations on model-based fMRI studies (<xref ref-type="bibr" rid="bib46">Lebreton et al., 2019</xref>). Based on pilot data and the available scanning time in one session (60 min), we set the following conditions of exclusion: failure to learn the association in three blocks or more (i.e. reaching a block limit of 80 trials without having learned the association), or failure to complete at least 11 blocks in the allocated time. Eleven participants were removed based on the above predetermined conditions, 2 of which did not go past the initial practice stages. Additionally, two more participants were removed due to head motion artifacts. Thus, 33 participants (22.4 ± 0.3 y.o.; eight females) were included in the main analyses. Of these, 22 participants (22.2 ± 0.3 y.o.; four females) returned for the follow-up experiment, based on their individual availability. All results presented up to <xref ref-type="fig" rid="fig5">Figure 5</xref> are from the 33 participants who completed the learning task. All results pertaining to the neurofeedback manipulation are from the subset of 22 participants that were called back. <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> reports a behavioural analysis of the excluded participants to investigate differences in performance or learning strategy compared to the 33 included participants.</p><p>All experiments and data analyses were conducted at the Advanced Telecommunications Research Institute International (ATR). The study was approved by the Institutional Review Board of ATR with ethics protocol numbers 18–122, 19–122, 20–122. All participants gave written informed consent.</p></sec><sec id="s4-2"><title>Learning task</title><p>The task consisted of learning the fruit preference of pacman-like characters. These characters had three features, each with two levels (colour: green vs red, stripe orientation: horizontal vs vertical, mouth direction: left vs right). On each trial, a character composed of a unique combination of the three features was presented. The experimental session was divided into blocks, for each of which a specific rule directed the association between features and preferred fruits. There were always two relevant features and one irrelevant feature, but these changed randomly at the beginning of each block. Blocks could thus be of three types: CO (colour-orientation), CD (colour-direction), and OD (orientation-direction). Furthermore, to avoid a simple logical deduction of the rule after one trial, we introduced the following pairings. The four possible combinations of two relevant features with two levels were paired with the two fruits in both a symmetric or asymmetric fashion - 2x2 or 3x1. The appearance of the two fruits was also randomly changed at the beginning of each new block (see <xref ref-type="fig" rid="fig1">Figure 1B,e</xref>.g., green-vertical: fruit 1, green-horizontal: fruit 2, red-vertical: fruit 1, red-horizontal: fruit two <italic>or</italic> green-vertical: fruit 2, green-horizontal: fruit 2, red-vertical: fruit 1, red-horizontal: fruit 2).</p><p>Each trial started with a black screen for 1 s, following which the character was presented for 2 s. Then, while the character continued to be present at the centre of the screen, the two fruit options appeared below, to the right and left sides. Participants had 2 s to indicate the preferred fruit by pressing a button (right for the fruit to the right, left for vice versa). Upon registering a participant’s choice, a coloured square appeared around the chosen fruit: green if the choice was correct, red otherwise. The square remained for 1 s, following which the trial ended with a variable ITI - bringing the trial to a fixed 8 s duration.</p><p>Participants were simply instructed that they had to learn the correct rule for each block, and the rule itself (relevant features + association type) was hidden. Each block contained up to 80 trials, but a block could end earlier if participants learned the target rule. Learning was measured as a set of correct trials (between 8 and 12, determined randomly in each block). Participants were instructed that each correct choice added one point, while incorrect choices did not alter the balance. They were further told that points obtained would be weighted by the speed of learning on that block. That is, the faster the learning, the greater the net worth of the points. The end of a block was explicitly signalled by presenting the reward obtained on the screen. Monetary reward was computed at the end of each block according to the formula:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>R</mml:mi><mml:mi/><mml:mo>=</mml:mo><mml:mi/><mml:mi>A</mml:mi><mml:mi/><mml:mi>*</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mi/><mml:mo>-</mml:mo><mml:mi/><mml:mo>(</mml:mo><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mi/><mml:mo>-</mml:mo><mml:mi/><mml:mi>m</mml:mi><mml:mi>c</mml:mi><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mi>*</mml:mi><mml:mi>a</mml:mi></mml:math></disp-formula>where <italic>R</italic> is the reward obtained in that block, <italic>A</italic> the maximum available reward (150JPY), <inline-formula><mml:math id="inf1"><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> the sum of correct responses, <inline-formula><mml:math id="inf2"><mml:mrow><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> the number of trials, <italic>mcs</italic> the maximum length of a correct strike (12 trials), and <italic>a</italic> is a scaling factor (<italic>a</italic> = 1.5). This formula ensures time-dependent decay of the reward that approximately follows a quadratic fit. In case participants completed a block in less than 12 trials, if the amount was larger than 150JPY, it was rounded to 150JPY.</p><p>The maximum terminal monetary reward over the whole session was set at 3,000 JPY. On average, participants earned 1251 ± 46 JPY (blocks in which participants failed to learn the association within the 80-trial limit were not rewarded). For each experimental session, there was a sequence of 20 blocks that was pre-generated pseudo-randomly, and on average, participants completed 13.6 ± 0.3 blocks. In the post-neurofeedback behavioural test, all participants completed 16 blocks, 8 of which had the targeted feature as relevant, while in the other half the targeted feature was irrelevant. The order was arranged pseudo-randomly such that in both halves of the session there were four blocks of each type. In the post-neurofeedback behavioural session, all blocks had only asymmetric pairings with preferred fruits.</p><p>For sessions done in the MR scanner, participants were instructed to use their dominant hand to press buttons on a dual-button response pad to register their choices. Concordance between responses and buttons was indicated on the display, and importantly, randomly changed across trials to avoid motor preparation confounds (i.e. associating a given preference choice with a specific button press).</p><p>The task was coded with PsychoPy v1.82.01 (<xref ref-type="bibr" rid="bib64">Peirce, 2008</xref>).</p></sec><sec id="s4-3"><title>Computational modelling part 1: mixture-of-experts RL model</title><p>We built on a standard RL model (<xref ref-type="bibr" rid="bib75">Sutton and Barto, 1998</xref>) and prior work in machine learning and robotics to derive the mixture-of-experts architecture (<xref ref-type="bibr" rid="bib23">Doya et al., 2002</xref>; <xref ref-type="bibr" rid="bib35">Jacobs et al., 1991</xref>; <xref ref-type="bibr" rid="bib74">Sugimoto et al., 2012</xref>). In this work, the mixture-of-experts architecture is composed of several ‘expert’ RL modules, each tracking a different representational space, and each with its own value function. In each trial, the action selected by the mixture-of-experts RL model is given by the weighted sum of the actions computed by the experts. The weight reflects the responsibility of each expert, which is computed from the SoftMax of the squared prediction error. In this section we define the general mixture-of-expert RL model, and in the next section we define each specific expert, based on the task-state representations being used.</p><p>Formally, the mixture-of-expert RL model global action is defined as:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>N</italic> is the number of experts, λ the responsibility signal, and <italic>a</italic> the action selected by the <italic>j</italic>th-model. Thus, λ is defined as:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msubsup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mfenced close="}" open="{" separators="|"><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:math></disp-formula>where <italic>N</italic> is the same as above, ν is the RPE variance. Expert uncertainty <inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>γ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula>where γ is the forgetting factor that controls the strength of the impact of prior experience on the current uncertainty estimate. The most recent RPE is computed as:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula>where <italic>O</italic> is the outcome (reward: 1, no reward: 0), <italic>Q</italic> is the value function, <italic>S</italic> the state for the current expert, and <italic>A</italic> is the global action computed with <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>. The update to the value function can therefore be computed as:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mi>Δ</mml:mi><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mi>α</mml:mi><mml:msubsup><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:math></disp-formula>where λ is the responsibility signal computed with <xref ref-type="disp-formula" rid="equ3">Equation (3)</xref>, α is the learning rate (assumed equal for all experts), and <italic>RPE</italic> is computed with <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>. Finally, for each expert, the action <italic>a</italic> at trial <italic>t</italic> is taken as the argmax of the value function, as follows:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:msubsup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:math></disp-formula>where <italic>Q</italic> is the value function, <italic>S</italic> the state at current trial, and <italic>a</italic> the two possible actions.</p><p>Hyperparameters estimated through likelihood maximisation were the learning rate α, the forgetting factor γ, and the RPE variance ν.</p></sec><sec id="s4-4"><title>Computational modelling part 2: Feature RL and Abstract RLs</title><p>Each (expert) RL algorithm is built on a standard RL model (<xref ref-type="bibr" rid="bib75">Sutton and Barto, 1998</xref>) to derive variants that differ in the number and type of states visited. Here, a state is defined as a combination of features. Feature RL has 2<sup>3</sup> = eight states, where each state was given by the combination of all three features (e.g. colour, stripe orientation, mouth direction: green, vertical, left). Abstract RL is designed with 2<sup>2</sup> = four states, where each state was given by the combination of two features.</p><p>Note that the number of states does not change for different blocks, only features used to determine them. These learning models create individual estimates of how participant action-selection depended on features they attended and their past reward history. Both RL models share the same underlying structure and are formally described as:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mi/><mml:mo>←</mml:mo><mml:mi/><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mi/><mml:mo>+</mml:mo><mml:mi/><mml:mi>α</mml:mi><mml:mi/><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mi/><mml:mo>-</mml:mo><mml:mi/><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf4"><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ8">Equation (8)</xref> is the value function of selecting either fruit-option <inline-formula><mml:math id="inf5"><mml:mi>a</mml:mi></mml:math></inline-formula> for packman-state <inline-formula><mml:math id="inf6"><mml:mi>s</mml:mi></mml:math></inline-formula>. The value of the action selected in the current trial is updated based on the difference between the expected value and the actual outcome (reward or no reward). This difference is called the reward prediction error (RPE). The degree to which this update affects the expected value depends on the learning rate α. For larger α, more recent outcomes will have a strong impact. On the contrary, for small α recent outcomes will have little effect on expectations. Only the value function of the selected action, which is state-contingent in <xref ref-type="disp-formula" rid="equ8">Equation (8)</xref>, is updated. Expected values of the two actions are combined to compute the probability <italic>p</italic> of predicting each outcome using a SoftMax (logistic) choice rule:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The greediness hyperparameter β controls how much the difference between the two expected values for <italic>a<sub>1</sub></italic> and <italic>a<sub>2</sub></italic> actually influence choices.</p><p>Hyperparameters estimated through likelihood maximisation were the learning rate α, and the greediness (inverse temperature) β.</p></sec><sec id="s4-5"><title>Procedures for model fitting, simulations, and hyperparameter recovery</title><p>Hierarchical Bayesian Inference (HBI) was used to fit the models to participant behavioural data, enabling precise estimates of hyperparameters at the block level for each participant (<xref ref-type="bibr" rid="bib65">Piray et al., 2019</xref>). Hyperparameters were selected by maximising the likelihood of estimated actions, given the true actions. For the mixture-of-experts architecture, we fit the model on all participants block-by-block to estimate hyperparameters at the single-block and single-participant level. For the subsequent direct comparison between Feature RL and Abstract RL models, we used HBI for concurrent model fitting and comparison at the single-block and single-participant basis. The model comparison provided the likelihood that each RL algorithm best explained participants’ choice data. That is, it was a proxy to whether learning followed a Feature RL or Abstract RL strategy. Because the fitting was done block-by-block, with a hierarchical approach considering all participants, blocks were first sorted according to their lengths, from longer to shorter, at the participant level. This ensured that each block of a given participant was at the most similar to the blocks of all other participants, thus avoiding unwanted effects in the fitting due to block length. The HBI procedure was then implemented on all participant data, proceeding block-by-block.</p><p>We also simulated model action-selection behaviours to benchmark their similarity to human behaviour, and in the case of Feature RL vs Abstract RL comparisons, to additionally compare their formal learning efficiency. In the case of the mixture-of-experts RL architecture, we simply used estimated hyperparameters to simulate 45 artificial agents, each completing 100 blocks. The simulation allowed us to compute, for each expert RL module, the mean responsibility signal, and the mean expected value across all states for the chosen action. Additionally, we also computed the learning speed (time to learn the rule of a block) and compared it with the learning speed of human participants.</p><p>In the case of the simple Feature RL and Abstract RL models, we added noise to the state information in order to have a more realistic behaviour (from the perspective of human participants). In the empirical data, the action (fruit selection) in the first trial of a new block was always chosen at random because participants did not have access to the appropriate representations (states). In later trials, participants may have followed specific strategies. For model simulations, we simply assumed that states were corrupted by a decaying noise function:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mi/><mml:mo>/</mml:mo><mml:mi/><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></disp-formula>where <italic>n<sub>t</sub></italic> is the noise level at trial <italic>t</italic>, <italic>n<sub>0</sub></italic> the initial noise level (randomly drawn from a uniform distribution within the interval [0.3 0.7]), and <italic>rte</italic> was the decay rate, which was set to 3. This meant that in early trials in a block, there was a higher chance of basing the action on the wrong representation (at random), rather than following the appropriate value function. Actions in later trials had a decreasing probability of being chosen at random. This approach is a combination of the classic ε-greedy policy and the standard SoftMax action-selection policy in RL. Hyperparameter values were sampled from obtained participant data maximum likelihood fits. We simulated 45 artificial agents solving 20 blocks each. The procedure was repeated 100 times for each block with new random seeds. We used two metrics to compare the efficiency of the two models: learning speed (same as above, the time to learn the rule of a block), as well as the fraction of failed blocks (blocks in which the rule was not learned with the 80-trials limit).</p><p>We performed a parameter recovery analysis for the simple Feature RL and Abstract RL models based on data from the main experiment. Parameter recovery analysis was done in order to confirm that fitted hyperparameters had sensible values and that the models themselves were a sensitive description of human choice behaviour (<xref ref-type="bibr" rid="bib63">Palminteri et al., 2017</xref>). Using the same noisy procedure described above, we generated one more simulated dataset, using the exact blocks that were presented to the 33 participants. The blocks from simulated data were then sorted according to their length, and the hyperparameters α and β were fitted by maximising the likelihood of the estimated actions, given the true model actions. We report in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> the scatter plot and correlation between hyperparameters estimated from participant data and recovered hyperparameters values, showing good agreement, notwithstanding the noise in the estimation.</p><p>For data from the behavioural session after multivoxel neurofeedback, blocks were first categorised as to whether the targeted feature was relevant or irrelevant to the rule of a given block. We then applied the HBI procedure as described above to all participants, with all blocks of the same type (e.g. targeted feature relevant) ordered by length. This allowed us to compute, based on whether the targeted feature was relevant or irrelevant, the difference in frequency between the models. We resampled with replacement to produce distributions of mean population bias for each block type.</p></sec><sec id="s4-6"><title>fMRI scans: acquisition and protocol</title><p>All scanning sessions employed a 3T MRI scanner (Siemens, Prisma) with a 64-channel head coil in the ATR Brain Activation Imaging Centre. Gradient T2*-weighted EPI (echoplanar) functional images with blood-oxygen-level-dependent (BOLD)-sensitive contrast and multi-band acceleration factor six were acquired (<xref ref-type="bibr" rid="bib25">Feinberg et al., 2010</xref>; <xref ref-type="bibr" rid="bib86">Xu et al., 2013</xref>). Imaging parameters: 72 contiguous slices (TR = 1 s, TE = 30 ms, flip angle = 60 deg, voxel size = 2×2×2 mm<sup>3</sup>, 0 mm slice gap) oriented parallel to the AC-PC plane were acquired, covering the entire brain. T1-weighted images (MP-RAGE; 256 slices, TR = 2 s, TE = 26 ms, flip angle = 80 deg, voxel size = 1×1×1 mm<sup>3</sup>, 0 mm slice gap) were also acquired at the end of the first session. For participants who joined the neurofeedback training sessions, the scanner was realigned to their head orientations with the same parameters for all sessions.</p></sec><sec id="s4-7"><title>fMRI scans: standard and parametric general linear models</title><p>BOLD-signal image analysis was performed with SPM12 [<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>], running on MATLAB v9.1.0.96 (r2016b) and v9.5.0.94 (r2018b). fMRI data for the initial 10 s of each block were discarded due to possible unsaturated T1 effects. Raw functional images underwent realignment to the first image of each session. Structural images were re-registered to mean EPI images and segmented into grey and white matter. Segmentation parameters were then used to normalise (MNI) and bias-correct the functional images. Normalised images were smoothed using a Gaussian kernel of 7 mm full-width at half-maximum.</p><p>GLM1: regressors of interest included ‘High value‘, ‘Low value’ (trials were labelled as such based on the median split of the trial-by-trial expected value for the chosen option computed from the best fitting algorithm - Feature RL or Abstract RL), ‘Feature RL’, ‘Abstract RL’ (trials were labelled as such based on the best fitting algorithm at the block level). For all, we generated boxcar regressors at the beginning of the visual stimulus (character) presentation, with duration 1 s. Contrast of [1 -1] or [−1 1] were applied to the regressors ‘High value’ - ‘Low value’, and ‘Feature RL’ - ‘Abstract RL’. Specific regressors of no interest included the time in the experiment: ‘early’ (all trials falling within the first half of the experiment), and ‘late’ (all trials falling in the second half of the experiment). The early/late split was done according to the total number of trials: taking as ‘early’, trials from the first block onward, adding blocks until the trial sum exceeded the total trials number divided by two.</p><p>GLM2 (PPI): the seed was defined as a sphere (radius = 6 mm) centred around the individual peak voxel from the ‘High value’ &gt; ‘Low value’ group-level contrast, within the vmPFC (peak coordinates [2 50 -10], radius 25 mm). The ROI mask was defined individually to account for possible differences among participants. Two participants were excluded from this analysis, because they did not show a significant cluster of voxels in the bounding sphere (even at very lenient thresholds). The GLM for the PPI included three regressors (the PPI, the mean BOLD signal of the seed region, and the psychological interaction), as well as nuisance regressors described below.</p><p>For all GLM analyses, additional regressors of no interest included a parametric regressor for reaction time, regressors for each trial event (fixation, fruit options presentation, choice, button press [left, right], ITI), block regressors, the six head motion realignment parameters, framewise displacement (FD) computed as the sum of the absolute values of the derivatives of the six realignment parameters, the TR-by-TR mean signal in white matter, and the TR-by-TR mean signal in cerebrospinal fluid.</p><p>Second-level group contrasts from all models were calculated as one-sample <italic>t</italic>-tests against zero for each first-level linear contrast. Statistical maps were z-transformed, and then reported at a threshold level of <italic>P(fpr)</italic> &lt; 0.001 (<italic>Z</italic> &gt; 3.09, false positive control meaning cluster forming threshold), unless otherwise specified. Statistical maps were projected onto a canonical MNI template with MRIcroGL [<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/mricrogl/">https://www.nitrc.org/projects/mricrogl/</ext-link>] or a glassbrain MNI template with Nilearn 0.7.1 [<ext-link ext-link-type="uri" xlink:href="https://nilearn.github.io/index.html">https://nilearn.github.io/index.html</ext-link>].</p></sec><sec id="s4-8"><title>fMRI scans: pre-processing for decoding</title><p>As above, the fMRI data for the initial 10 s of each run were discarded due to possible unsaturated T1 effects. BOLD signals in native space were pre-processed in MATLAB v7.13 (R2011b) (MathWorks) with the mrVista software package for MATLAB [<ext-link ext-link-type="uri" xlink:href="http://vistalab.stanford.edu/software/">http://vistalab.stanford.edu/software/</ext-link>]. All functional images underwent 3D motion correction. No spatial or temporal smoothing was applied. Rigid-body transformations were performed to align functional images to the structural image for each participant. One region of interest (ROI), the HPC, was anatomically defined through cortical reconstruction and volumetric segmentation using the Freesurfer software [<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>]. Furthermore, VC subregions V1, V2, and V3 were also automatically defined based on a probabilistic map atlas (<xref ref-type="bibr" rid="bib81">Wang et al., 2015</xref>). The vmPFC ROI was defined as the significant voxels from the GLM1 ‘High value’ &gt; ‘Low value’ contrast at <italic>p</italic>(fpr) &lt; 0.0001, within the OFC. All subsequent analyses were performed using MATLAB v9.5.0.94 (r2018b). Once ROIs were individually identified, time-courses of BOLD signal intensities were extracted from each voxel in each ROI and shifted by 6 s to account for the hemodynamic delay (assumed fixed). A linear trend was removed from time-courses, which were further z-score-normalised for each voxel in each block to minimise baseline differences across blocks. Data samples for computing individual feature identity decoders were created by averaging BOLD signal intensities of each voxel over two volumes, corresponding to the 2 s from stimulus (character) onset to fruit options onset.</p></sec><sec id="s4-9"><title>Decoding: multivoxel pattern analysis (MVPA)</title><p>All ROI-based MVP analyses followed the same procedure. We used sparse logistic regression (SLR) (<xref ref-type="bibr" rid="bib87">Yamashita et al., 2008</xref>), to automatically select the most relevant voxels for the classification problem. This allowed us to construct several binary classifiers (e.g. feature id.: colour - red vs green, stripes orientation - horizontal vs vertical, mouth direction - right vs left).</p><p>Cross-validation was used for each MVP analysis to evaluate the predictive power of the trained (fitted) model. In the primary analysis (reported in <xref ref-type="fig" rid="fig5">Figure 5C</xref>), cross-validation was done with a leave-one-run-out scheme, whereby each run was iteratively held out as a test set, and all other runs were used for training of the algorithm. The final accuracy was taken as the averaged accuracy across the runs. This approach is generally used because there may be subtle differences across runs: holding out one run as a test ensures higher generalizability of the results while avoiding within-run information leaks. Yet, because of the nature of our task and experiment, the leave-one-run-out cross-validation leads to other confounds due to varying number of training trials across classes (e.g. colour red vs green) or conditions (Feature RL vs Abstract RL blocks). To control for this idiosyncratic feature of our design, we performed a second cross-validation scheme. Here, we first merged the data from all blocks for each condition, and then computed the lowest bound of trial number from the minority class across conditions (e.g. if Feature RL had 128 trials labelled as ‘green’, and 109 as ‘red’, while Abstract RL had 94 trials labelled as ‘green’, and 99 labelled as ‘red’; then the minority class lowest bound was 94). In each fold (N folds = 20), a number of trials equivalent to 80% of the minority class lowest bound was assigned to the training set from each class, and the remaining trials to the test set. The training samples were randomly chosen in each fold. Furthermore, for all MVP analysis, SLR classification was optimised by using an iterative approach (<xref ref-type="bibr" rid="bib33">Hirose et al., 2015</xref>) In each fold of the cross-validation, the feature-selection process was repeated 10 times. In each iteration, selected features (voxels) were removed from pattern vectors, and only features with unassigned weights were used for the next iteration. At the end of the cross-validation, test accuracies were averaged for each iteration across folds, in order to evaluate accuracy at each iteration. The number of iterations yielding the highest classification accuracy was then used for the final computation. Results (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>) report the cross-validated average of the best yielding iteration.</p><p>For the multivoxel neurofeedback experiment, we used the entire dataset to train the classifier in VC. Thus, each classifier resulted in a set of weights assigned to the selected voxels. These weights could be used to classify any new data sample and to compute a likelihood of its belonging to the target class.</p></sec><sec id="s4-10"><title>Real-time multivoxel neurofeedback and fMRI pre-processing</title><p>As in previous studies (<xref ref-type="bibr" rid="bib16">Cortese et al., 2017</xref>; <xref ref-type="bibr" rid="bib15">Cortese et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Shibata et al., 2011</xref>), during the multivoxel neurofeedback manipulation, participants were instructed to modulate their brain activity, in order to enlarge a feedback disc and maximise their cumulative reward. Brain activity patterns measured through fMRI were used in real time to compute the feedback score. Unbeknownst to participants, the feedback score, ranging from 0 to 100 (empty to full disc), represented the likelihood of a target pattern occurring in their brains at measurement time. Each trial started with an induction period of 6 s, during which participants viewed a cue (a small grey circle) that instructed them to modulate their brain activity. This period was followed by a 6 s rest interval, and then by a 2 s feedback, during which the disc appeared on the screen. Finally, each trial ended with a 6 s inter-trial interval (ITI). Each block was composed of 12 trials, and one session could last up to 10 blocks (depending on time availability). Participants did two sessions on consecutive days. Within a session, the maximum monetary bonus was 3000 JPY.</p><p>Feedback was calculated through the following steps. In each block, the initial 10 s of fMRI data were discarded to avoid unsaturated T1 effects. First, newly measured, whole-brain functional images underwent 3D motion correction using Turbo BrainVoyager (Brain Innovation, Maastricht, Netherlands). Second, time-courses of BOLD signal intensities were extracted from each of the voxels identified in the decoder analysis for the target ROI (VC). Third, the time-course was detrended (removal of linear trends), and z-score-normalised for each voxel using BOLD signal intensities measured up to the last point. Fourth, the data sample to calculate the target likelihood was created by taking the average BOLD signal intensity of each voxel over the 6 s (6 TRs) ‘induction’ period as in previous studies (<xref ref-type="bibr" rid="bib15">Cortese et al., 2016</xref>; <xref ref-type="bibr" rid="bib69">Shibata et al., 2011</xref>). Finally, the likelihood of each feature level (e.g. right vs left mouth direction) being represented in the multivoxel activity pattern was calculated from the data sample using weights of the constructed classifier.</p></sec><sec id="s4-11"><title>Data and code availability</title><p>Behavioural data, group-level maps of brain activation, and custom code used to generate results and figures are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/BDMLab/Cortese_et_al_2021">https://github.com/BDMLab/Cortese_et_al_2021</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:88d680896aa54dc52629f4274001a6e529fb78fc;origin=https://github.com/BDMLab/Cortese_et_al_2021;visit=swh:1:snp:d5176536817595f8ae3061e468585b773abc696a;anchor=swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee">swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee</ext-link> (<xref ref-type="bibr" rid="bib19">Cortese et al., 2021</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Kaori Nakamura, Yasuo Shimada for experimental assistance; Drs. Hakwan Lau, Jessica Taylor for helpful comments on previous versions of this manuscript. <bold>Funding</bold>: JST ERATO (Japan, grant number JPMJER1801) to AC, AY, and MK; AMED (Japan, grant number JP18dm0307008) to AC and MK; the Chilean National Agency for Research and Development (ANID)/Scholarship Program/DOCTORADO BECAS CHILE/2017–72180193 to PS; the Royal Society and Wellcome Trust, Henry Dale Fellowship (102612/A/13/Z) to BDM.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Investigation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Formal analysis, Supervision, Validation, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experiments and data analyses were conducted at the Advanced Telecommunications Research Institute International (ATR). The study was approved by the Institutional Review Board of ATR with ethics protocol numbers 18-122, 19-122, 20-122. All participants gave written informed consent.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-68943-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files. Source data files have been provided for Figures 1-6. Behavioural data, group-level maps of brain activation, and custom code used to generate results and figures are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/BDMLab/Cortese_et_al_2021">https://github.com/BDMLab/Cortese_et_al_2021</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee">https://archive.softwareheritage.org/swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee</ext-link>.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akaishi</surname> <given-names>R</given-names></name><name><surname>Kolling</surname> <given-names>N</given-names></name><name><surname>Brown</surname> <given-names>JW</given-names></name><name><surname>Rushworth</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural Mechanisms of Credit Assignment in a Multicue Environment</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1096</fpage><lpage>1112</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3159-15.2016</pub-id><pub-id pub-id-type="pmid">26818500</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname> <given-names>GE</given-names></name><name><surname>Crutcher</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Functional architecture of basal ganglia circuits: neural substrates of parallel processing</article-title><source>Trends in Neurosciences</source><volume>13</volume><fpage>266</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(90)90107-l</pub-id><pub-id pub-id-type="pmid">1695401</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>BA</given-names></name><name><surname>Laurent</surname> <given-names>PA</given-names></name><name><surname>Yantis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Value-driven attentional capture</article-title><source>PNAS</source><volume>108</volume><fpage>10367</fpage><lpage>10371</lpage><pub-id pub-id-type="doi">10.1073/pnas.1104047108</pub-id><pub-id pub-id-type="pmid">21646524</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname> <given-names>D</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evidence from fMRI</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>527</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr117</pub-id><pub-id pub-id-type="pmid">21693491</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banerjee</surname> <given-names>A</given-names></name><name><surname>Parente</surname> <given-names>G</given-names></name><name><surname>Teutsch</surname> <given-names>J</given-names></name><name><surname>Lewis</surname> <given-names>C</given-names></name><name><surname>Voigt</surname> <given-names>FF</given-names></name><name><surname>Helmchen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Value-guided remapping of sensory cortex by lateral orbitofrontal cortex</article-title><source>Nature</source><volume>585</volume><fpage>245</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2704-z</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastin</surname> <given-names>J</given-names></name><name><surname>Deman</surname> <given-names>P</given-names></name><name><surname>David</surname> <given-names>O</given-names></name><name><surname>Gueguen</surname> <given-names>M</given-names></name><name><surname>Benis</surname> <given-names>D</given-names></name><name><surname>Minotti</surname> <given-names>L</given-names></name><name><surname>Hoffman</surname> <given-names>D</given-names></name><name><surname>Combrisson</surname> <given-names>E</given-names></name><name><surname>Kujala</surname> <given-names>J</given-names></name><name><surname>Perrone-Bertolotti</surname> <given-names>M</given-names></name><name><surname>Kahane</surname> <given-names>P</given-names></name><name><surname>Lachaux</surname> <given-names>JP</given-names></name><name><surname>Jerbi</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Direct recordings from human anterior insula reveal its leading role within the Error-Monitoring network</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>1545</fpage><lpage>1557</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv352</pub-id><pub-id pub-id-type="pmid">26796212</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bellman</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1957">1957</year><source>Dynamic Programming</source><publisher-loc>Princeton, New Jersey, USA</publisher-loc><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengtsson</surname> <given-names>SL</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Sakai</surname> <given-names>K</given-names></name><name><surname>Buckley</surname> <given-names>MJ</given-names></name><name><surname>Passingham</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The representation of abstract task rules in the human prefrontal cortex</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>1929</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn222</pub-id><pub-id pub-id-type="pmid">19047573</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benoit</surname> <given-names>RG</given-names></name><name><surname>Szpunar</surname> <given-names>KK</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Ventromedial prefrontal cortex supports affective future simulation by integrating distributed knowledge</article-title><source>PNAS</source><volume>111</volume><fpage>16550</fpage><lpage>16555</lpage><pub-id pub-id-type="doi">10.1073/pnas.1419274111</pub-id><pub-id pub-id-type="pmid">25368170</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardi</surname> <given-names>S</given-names></name><name><surname>Benna</surname> <given-names>MK</given-names></name><name><surname>Rigotti</surname> <given-names>M</given-names></name><name><surname>Munuera</surname> <given-names>J</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name><name><surname>Salzman</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The geometry of abstraction in the Hippocampus and prefrontal cortex</article-title><source>Cell</source><volume>183</volume><fpage>954</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id><pub-id pub-id-type="pmid">33058757</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowman</surname> <given-names>CR</given-names></name><name><surname>Zeithamova</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Abstract memory representations in the ventromedial prefrontal cortex and Hippocampus support concept generalization</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2605</fpage><lpage>2614</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2811-17.2018</pub-id><pub-id pub-id-type="pmid">29437891</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname> <given-names>CS</given-names></name><name><surname>Braver</surname> <given-names>TS</given-names></name><name><surname>Barch</surname> <given-names>DM</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Noll</surname> <given-names>D</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Anterior cingulate cortex, error detection, and the online monitoring of performance</article-title><source>Science</source><volume>280</volume><fpage>747</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1126/science.280.5364.747</pub-id><pub-id pub-id-type="pmid">9563953</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castegnetti</surname> <given-names>G</given-names></name><name><surname>Zurita</surname> <given-names>M</given-names></name><name><surname>De Martino</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How usefulness shapes neural representations during goal-directed behavior</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabd5363</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abd5363</pub-id><pub-id pub-id-type="pmid">33827810</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname> <given-names>AO</given-names></name><name><surname>O'Reilly</surname> <given-names>JX</given-names></name><name><surname>Behrens</surname> <given-names>TEJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title><source>Science</source><volume>352</volume><fpage>1464</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id><pub-id pub-id-type="pmid">27313047</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Amano</surname> <given-names>K</given-names></name><name><surname>Koizumi</surname> <given-names>A</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multivoxel neurofeedback selectively modulates confidence without changing perceptual performance</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13669</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13669</pub-id><pub-id pub-id-type="pmid">27976739</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Amano</surname> <given-names>K</given-names></name><name><surname>Koizumi</surname> <given-names>A</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoded fMRI neurofeedback can induce bidirectional confidence changes within single participants</article-title><source>NeuroImage</source><volume>149</volume><fpage>323</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.01.069</pub-id><pub-id pub-id-type="pmid">28163140</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>De Martino</surname> <given-names>B</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural and cognitive architecture for learning from a small sample</article-title><source>Current Opinion in Neurobiology</source><volume>55</volume><fpage>133</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.02.011</pub-id><pub-id pub-id-type="pmid">30953964</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unconscious reinforcement learning of hidden brain states supported by confidence</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4429</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17828-8</pub-id><pub-id pub-id-type="pmid">32868772</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Yamamoto</surname> <given-names>A</given-names></name><name><surname>Hashemzadeh</surname> <given-names>M</given-names></name><name><surname>Sepulveda</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Cortese_et_al_2021</data-title><version designator="swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee">swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:88d680896aa54dc52629f4274001a6e529fb78fc;origin=https://github.com/BDMLab/Cortese_et_al_2021;visit=swh:1:snp:d5176536817595f8ae3061e468585b773abc696a;anchor=swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee">https://archive.softwareheritage.org/swh:1:dir:88d680896aa54dc52629f4274001a6e529fb78fc;origin=https://github.com/BDMLab/Cortese_et_al_2021;visit=swh:1:snp:d5176536817595f8ae3061e468585b773abc696a;anchor=swh:1:rev:3ac5090fe0af132364bbf92b9b0dff95919d60ee</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname> <given-names>B</given-names></name><name><surname>Fleming</surname> <given-names>SM</given-names></name><name><surname>Garrett</surname> <given-names>N</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Confidence in value-based choice</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>105</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/nn.3279</pub-id><pub-id pub-id-type="pmid">23222911</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Domenech</surname> <given-names>P</given-names></name><name><surname>Redouté</surname> <given-names>J</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Dreher</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Neuro-Computational architecture of Value-Based selection in the human brain</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>585</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw396</pub-id><pub-id pub-id-type="pmid">28057725</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname> <given-names>M</given-names></name><name><surname>Collins</surname> <given-names>AG</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Human cognition. Foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="doi">10.1126/science.1252254</pub-id><pub-id pub-id-type="pmid">24876345</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Samejima</surname> <given-names>K</given-names></name><name><surname>Katagiri</surname> <given-names>K</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Multiple model-based reinforcement learning</article-title><source>Neural Computation</source><volume>14</volume><fpage>1347</fpage><lpage>1369</lpage><pub-id pub-id-type="doi">10.1162/089976602753712972</pub-id><pub-id pub-id-type="pmid">12020450</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farashahi</surname> <given-names>S</given-names></name><name><surname>Rowe</surname> <given-names>K</given-names></name><name><surname>Aslami</surname> <given-names>Z</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Soltani</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Feature-based learning improves adaptability without compromising precision</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>1768</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01874-w</pub-id><pub-id pub-id-type="pmid">29170381</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname> <given-names>DA</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Auerbach</surname> <given-names>E</given-names></name><name><surname>Ramanna</surname> <given-names>S</given-names></name><name><surname>Gunther</surname> <given-names>M</given-names></name><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Miller</surname> <given-names>KL</given-names></name><name><surname>Ugurbil</surname> <given-names>K</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiplexed echo planar imaging for sub-second whole brain FMRI and fast diffusion imaging</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e15710</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0015710</pub-id><pub-id pub-id-type="pmid">21187930</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Badre</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>509</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr114</pub-id><pub-id pub-id-type="pmid">21693490</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Buechel</surname> <given-names>C</given-names></name><name><surname>Fink</surname> <given-names>GR</given-names></name><name><surname>Morris</surname> <given-names>J</given-names></name><name><surname>Rolls</surname> <given-names>E</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Psychophysiological and modulatory interactions in neuroimaging</article-title><source>NeuroImage</source><volume>6</volume><fpage>218</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1006/nimg.1997.0291</pub-id><pub-id pub-id-type="pmid">9344826</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gherman</surname> <given-names>S</given-names></name><name><surname>Philiastides</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human VMPFC encodes early signatures of confidence in perceptual decisions</article-title><source>eLife</source><volume>7</volume><elocation-id>e38293</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38293</pub-id><pub-id pub-id-type="pmid">30247123</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilboa</surname> <given-names>A</given-names></name><name><surname>Marlatte</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neurobiology of Schemas and Schema-Mediated Memory</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>618</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.04.013</pub-id><pub-id pub-id-type="pmid">28551107</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname> <given-names>M</given-names></name><name><surname>Thoma</surname> <given-names>V</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Richardson-Klavehn</surname> <given-names>A</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spatial attention enhances object coding in local and distributed representations of the lateral occipital complex</article-title><source>NeuroImage</source><volume>116</volume><fpage>149</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.04.004</pub-id><pub-id pub-id-type="pmid">25865144</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haruno</surname> <given-names>M</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Heterarchical reinforcement-learning model for integration of multiple cortico-striatal loops: fMRI examination in stimulus-action-reward association learning</article-title><source>Neural Networks</source><volume>19</volume><fpage>1242</fpage><lpage>1254</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2006.06.007</pub-id><pub-id pub-id-type="pmid">16987637</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hashemzadeh</surname> <given-names>M</given-names></name><name><surname>Hosseini</surname> <given-names>R</given-names></name><name><surname>Ahmadabadi</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exploiting generalization in the subspaces for faster Model-Based reinforcement learning</article-title><source>IEEE Transactions on Neural Networks and Learning Systems</source><volume>30</volume><fpage>1635</fpage><lpage>1650</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2018.2869978</pub-id><pub-id pub-id-type="pmid">30307878</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirose</surname> <given-names>S</given-names></name><name><surname>Nambu</surname> <given-names>I</given-names></name><name><surname>Naito</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>An empirical solution for over-pruning with a novel ensemble-learning method for fMRI decoding</article-title><source>Journal of Neuroscience Methods</source><volume>239</volume><fpage>238</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.10.023</pub-id><pub-id pub-id-type="pmid">25445247</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname> <given-names>MK</given-names></name><name><surname>Abel</surname> <given-names>D</given-names></name><name><surname>Griffiths</surname> <given-names>TL</given-names></name><name><surname>Littman</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The value of abstraction</article-title><source>Current Opinion in Behavioral Sciences</source><volume>29</volume><fpage>111</fpage><lpage>116</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname> <given-names>RA</given-names></name><name><surname>Jordan</surname> <given-names>MI</given-names></name><name><surname>Nowlan</surname> <given-names>SJ</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Adaptive mixtures of local experts</article-title><source>Neural Computation</source><volume>3</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1162/neco.1991.3.1.79</pub-id><pub-id pub-id-type="pmid">31141872</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname> <given-names>Y</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Decoding the visual and subjective contents of the human brain</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>679</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1038/nn1444</pub-id><pub-id pub-id-type="pmid">15852014</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawato</surname> <given-names>M</given-names></name><name><surname>Samejima</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Efficient reinforcement learning: computational theories, neuroscience and robotics</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>205</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.03.004</pub-id><pub-id pub-id-type="pmid">17374483</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>K</given-names></name><name><surname>Chung</surname> <given-names>JE</given-names></name><name><surname>Sosa</surname> <given-names>M</given-names></name><name><surname>Schor</surname> <given-names>JS</given-names></name><name><surname>Karlsson</surname> <given-names>MP</given-names></name><name><surname>Larkin</surname> <given-names>MC</given-names></name><name><surname>Liu</surname> <given-names>DF</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Constant Sub-second cycling between representations of possible futures in the Hippocampus</article-title><source>Cell</source><volume>180</volume><fpage>552</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.01.014</pub-id><pub-id pub-id-type="pmid">32004462</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knutson</surname> <given-names>B</given-names></name><name><surname>Taylor</surname> <given-names>J</given-names></name><name><surname>Kaufman</surname> <given-names>M</given-names></name><name><surname>Peterson</surname> <given-names>R</given-names></name><name><surname>Glover</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Distributed neural representation of expected value</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>4806</fpage><lpage>4812</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0642-05.2005</pub-id><pub-id pub-id-type="pmid">15888656</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname> <given-names>K</given-names></name><name><surname>Hsu</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Common neural code for reward and information value</article-title><source>PNAS</source><volume>116</volume><fpage>13061</fpage><lpage>13066</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820145116</pub-id><pub-id pub-id-type="pmid">31186358</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koizumi</surname> <given-names>A</given-names></name><name><surname>Amano</surname> <given-names>K</given-names></name><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Shibata</surname> <given-names>K</given-names></name><name><surname>Yoshida</surname> <given-names>W</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fear reduction without fear through reinforcement of neural activity that bypasses conscious exposure</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0006</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-016-0006</pub-id><pub-id pub-id-type="pmid">28989977</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konidaris</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>On the necessity of abstraction</article-title><source>Current Opinion in Behavioral Sciences</source><volume>29</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2018.11.005</pub-id><pub-id pub-id-type="pmid">31440528</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauzlis</surname> <given-names>RJ</given-names></name><name><surname>Bollimunta</surname> <given-names>A</given-names></name><name><surname>Arcizet</surname> <given-names>F</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attention as an effect not a cause</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.05.008</pub-id><pub-id pub-id-type="pmid">24953964</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Summerfield</surname> <given-names>JJ</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Tracking the emergence of conceptual knowledge during human decision making</article-title><source>Neuron</source><volume>63</volume><fpage>889</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.030</pub-id><pub-id pub-id-type="pmid">19778516</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname> <given-names>M</given-names></name><name><surname>Abitbol</surname> <given-names>R</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Pessiglione</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automatic integration of confidence in the brain valuation signal</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/nn.4064</pub-id><pub-id pub-id-type="pmid">26192748</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname> <given-names>M</given-names></name><name><surname>Bavard</surname> <given-names>S</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Palminteri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Assessing inter-individual differences with task-related functional neuroimaging</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>897</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0681-8</pub-id><pub-id pub-id-type="pmid">31451737</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>GoodSmith</surname> <given-names>D</given-names></name><name><surname>Knierim</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Parallel processing streams in the hippocampus</article-title><source>Current Opinion in Neurobiology</source><volume>64</volume><fpage>127</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2020.03.004</pub-id><pub-id pub-id-type="pmid">32502734</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname> <given-names>YC</given-names></name><name><surname>Radulescu</surname> <given-names>A</given-names></name><name><surname>Daniel</surname> <given-names>R</given-names></name><name><surname>DeWoskin</surname> <given-names>V</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic interaction between reinforcement learning and attention in multidimensional environments</article-title><source>Neuron</source><volume>93</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.040</pub-id><pub-id pub-id-type="pmid">28103483</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>S</given-names></name><name><surname>Ullman</surname> <given-names>TD</given-names></name><name><surname>Tenenbaum</surname> <given-names>JB</given-names></name><name><surname>Spelke</surname> <given-names>ES</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ten-month-old infants infer the value of goals from the costs of actions</article-title><source>Science</source><volume>358</volume><fpage>1038</fpage><lpage>1041</lpage><pub-id pub-id-type="doi">10.1126/science.aag2132</pub-id><pub-id pub-id-type="pmid">29170232</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>D</given-names></name><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Z</given-names></name><name><surname>Zhang</surname> <given-names>ZY</given-names></name><name><surname>Sun</surname> <given-names>YG</given-names></name><name><surname>Yang</surname> <given-names>T</given-names></name><name><surname>Yao</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Orbitofrontal control of visual cortex gain promotes visual associative learning</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2784</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-16609-7</pub-id><pub-id pub-id-type="pmid">32493971</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lubianiker</surname> <given-names>N</given-names></name><name><surname>Goldway</surname> <given-names>N</given-names></name><name><surname>Fruchtman-Steinbok</surname> <given-names>T</given-names></name><name><surname>Paret</surname> <given-names>C</given-names></name><name><surname>Keynan</surname> <given-names>JN</given-names></name><name><surname>Singer</surname> <given-names>N</given-names></name><name><surname>Cohen</surname> <given-names>A</given-names></name><name><surname>Kadosh</surname> <given-names>KC</given-names></name><name><surname>Linden</surname> <given-names>DEJ</given-names></name><name><surname>Hendler</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Process-based framework for precise neuromodulation</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>436</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0573-y</pub-id><pub-id pub-id-type="pmid">30988481</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname> <given-names>ML</given-names></name><name><surname>Love</surname> <given-names>BC</given-names></name><name><surname>Preston</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dynamic updating of hippocampal object representations reflects new conceptual knowledge</article-title><source>PNAS</source><volume>113</volume><fpage>13203</fpage><lpage>13208</lpage><pub-id pub-id-type="doi">10.1073/pnas.1614048113</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname> <given-names>ML</given-names></name><name><surname>Preston</surname> <given-names>AR</given-names></name><name><surname>Love</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ventromedial prefrontal cortex compression during concept learning</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13930-8</pub-id><pub-id pub-id-type="pmid">31911628</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKenzie</surname> <given-names>S</given-names></name><name><surname>Frank</surname> <given-names>AJ</given-names></name><name><surname>Kinsky</surname> <given-names>NR</given-names></name><name><surname>Porter</surname> <given-names>B</given-names></name><name><surname>Rivière</surname> <given-names>PD</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal representation of related and opposing memories develop within distinct, hierarchically organized neural schemas</article-title><source>Neuron</source><volume>83</volume><fpage>202</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.019</pub-id><pub-id pub-id-type="pmid">24910078</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamee</surname> <given-names>D</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>O’Doherty</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>479</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1038/nn.3337</pub-id><pub-id pub-id-type="pmid">23416449</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mian</surname> <given-names>MK</given-names></name><name><surname>Sheth</surname> <given-names>SA</given-names></name><name><surname>Patel</surname> <given-names>SR</given-names></name><name><surname>Spiliopoulos</surname> <given-names>K</given-names></name><name><surname>Eskandar</surname> <given-names>EN</given-names></name><name><surname>Williams</surname> <given-names>ZM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Encoding of rules by neurons in the human dorsolateral prefrontal cortex</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>807</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs361</pub-id><pub-id pub-id-type="pmid">23172774</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muñoz-Moldes</surname> <given-names>S</given-names></name><name><surname>Cleeremans</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Delineating implicit and explicit processes in neurofeedback learning</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>118</volume><fpage>681</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.09.003</pub-id><pub-id pub-id-type="pmid">32918947</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neubert</surname> <given-names>FX</given-names></name><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Sallet</surname> <given-names>J</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Connectivity reveals relationship of brain areas for reward-guided learning and decision making in human and monkey frontal cortex</article-title><source>PNAS</source><volume>112</volume><fpage>2695</fpage><lpage>2704</lpage><pub-id pub-id-type="doi">10.1073/pnas.1410767112</pub-id><pub-id pub-id-type="pmid">25947150</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Daniel</surname> <given-names>R</given-names></name><name><surname>Geana</surname> <given-names>A</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Leong</surname> <given-names>YC</given-names></name><name><surname>Radulescu</surname> <given-names>A</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reinforcement learning in multidimensional environments relies on attention mechanisms</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>8145</fpage><lpage>8157</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2978-14.2015</pub-id><pub-id pub-id-type="pmid">26019331</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning task-state representations</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1544</fpage><lpage>1553</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0470-8</pub-id><pub-id pub-id-type="pmid">31551597</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oemisch</surname> <given-names>M</given-names></name><name><surname>Westendorff</surname> <given-names>S</given-names></name><name><surname>Azimi</surname> <given-names>M</given-names></name><name><surname>Hassani</surname> <given-names>SA</given-names></name><name><surname>Ardid</surname> <given-names>S</given-names></name><name><surname>Tiesinga</surname> <given-names>P</given-names></name><name><surname>Womelsdorf</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Feature-specific prediction errors and surprise across macaque fronto-striatal circuits</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>176</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08184-9</pub-id><pub-id pub-id-type="pmid">30635579</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name><name><surname>Assad</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Importance of Falsification in Computational Cognitive Modeling</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id><pub-id pub-id-type="pmid">28476348</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Generating stimuli for neuroscience using PsychoPy</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.010.2008</pub-id><pub-id pub-id-type="pmid">19198666</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname> <given-names>P</given-names></name><name><surname>Dezfouli</surname> <given-names>A</given-names></name><name><surname>Heskes</surname> <given-names>T</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical Bayesian inference for concurrent model fitting and comparison for group studies</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007043</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007043</pub-id><pub-id pub-id-type="pmid">31211783</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname> <given-names>NW</given-names></name><name><surname>Cai</surname> <given-names>MB</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human orbitofrontal cortex represents a cognitive map of state space</article-title><source>Neuron</source><volume>91</volume><fpage>1402</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.019</pub-id><pub-id pub-id-type="pmid">27657452</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname> <given-names>NW</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sequential replay of nonspatial task states in the human Hippocampus</article-title><source>Science</source><volume>364</volume><elocation-id>eaaw5181</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaw5181</pub-id><pub-id pub-id-type="pmid">31249030</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname> <given-names>AD</given-names></name><name><surname>Grafton</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Subjective value then confidence in human ventromedial prefrontal cortex</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0225617</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0225617</pub-id><pub-id pub-id-type="pmid">32040474</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shibata</surname> <given-names>K</given-names></name><name><surname>Watanabe</surname> <given-names>T</given-names></name><name><surname>Sasaki</surname> <given-names>Y</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perceptual learning incepted by decoded fMRI neurofeedback without stimulus presentation</article-title><source>Science</source><volume>334</volume><fpage>1413</fpage><lpage>1415</lpage><pub-id pub-id-type="doi">10.1126/science.1212003</pub-id><pub-id pub-id-type="pmid">22158821</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shibata</surname> <given-names>K</given-names></name><name><surname>Lisi</surname> <given-names>G</given-names></name><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Watanabe</surname> <given-names>T</given-names></name><name><surname>Sasaki</surname> <given-names>Y</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Toward a comprehensive understanding of the neural mechanisms of decoded neurofeedback</article-title><source>NeuroImage</source><volume>188</volume><fpage>539</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.12.022</pub-id><pub-id pub-id-type="pmid">30572110</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sitaram</surname> <given-names>R</given-names></name><name><surname>Ros</surname> <given-names>T</given-names></name><name><surname>Stoeckel</surname> <given-names>L</given-names></name><name><surname>Haller</surname> <given-names>S</given-names></name><name><surname>Scharnowski</surname> <given-names>F</given-names></name><name><surname>Lewis-Peacock</surname> <given-names>J</given-names></name><name><surname>Weiskopf</surname> <given-names>N</given-names></name><name><surname>Blefari</surname> <given-names>ML</given-names></name><name><surname>Rana</surname> <given-names>M</given-names></name><name><surname>Oblak</surname> <given-names>E</given-names></name><name><surname>Birbaumer</surname> <given-names>N</given-names></name><name><surname>Sulzer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Closed-loop brain training: the science of neurofeedback</article-title><source>Nature Reviews Neuroscience</source><volume>18</volume><fpage>86</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.164</pub-id><pub-id pub-id-type="pmid">28003656</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitmaan</surname> <given-names>M</given-names></name><name><surname>Seo</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Soltani</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multiple timescales of neural dynamics and integration of task-relevant signals across cortex</article-title><source>PNAS</source><volume>117</volume><fpage>22522</fpage><lpage>22531</lpage><pub-id pub-id-type="doi">10.1073/pnas.2005993117</pub-id><pub-id pub-id-type="pmid">32839338</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname> <given-names>KL</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The Hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1038/nn.4650</pub-id><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname> <given-names>N</given-names></name><name><surname>Haruno</surname> <given-names>M</given-names></name><name><surname>Doya</surname> <given-names>K</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>MOSAIC for multiple-reward environments</article-title><source>Neural Computation</source><volume>24</volume><fpage>577</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00246</pub-id><pub-id pub-id-type="pmid">22168558</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taschereau-Dumouchel</surname> <given-names>V</given-names></name><name><surname>Cortese</surname> <given-names>A</given-names></name><name><surname>Chiba</surname> <given-names>T</given-names></name><name><surname>Knotts</surname> <given-names>JD</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name><name><surname>Lau</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Towards an unconscious neural reinforcement intervention for common fears</article-title><source>PNAS</source><volume>115</volume><fpage>3470</fpage><lpage>3475</lpage><pub-id pub-id-type="doi">10.1073/pnas.1721572115</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname> <given-names>D</given-names></name><name><surname>Langston</surname> <given-names>RF</given-names></name><name><surname>Kakeyama</surname> <given-names>M</given-names></name><name><surname>Bethus</surname> <given-names>I</given-names></name><name><surname>Spooner</surname> <given-names>PA</given-names></name><name><surname>Wood</surname> <given-names>ER</given-names></name><name><surname>Witter</surname> <given-names>MP</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Schemas and memory consolidation</article-title><source>Science</source><volume>316</volume><fpage>76</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1126/science.1135935</pub-id><pub-id pub-id-type="pmid">17412951</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tse</surname> <given-names>D</given-names></name><name><surname>Takeuchi</surname> <given-names>T</given-names></name><name><surname>Kakeyama</surname> <given-names>M</given-names></name><name><surname>Kajii</surname> <given-names>Y</given-names></name><name><surname>Okuno</surname> <given-names>H</given-names></name><name><surname>Tohyama</surname> <given-names>C</given-names></name><name><surname>Bito</surname> <given-names>H</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Schema-dependent gene activation and memory encoding in neocortex</article-title><source>Science</source><volume>333</volume><fpage>891</fpage><lpage>895</lpage><pub-id pub-id-type="doi">10.1126/science.1205274</pub-id><pub-id pub-id-type="pmid">21737703</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname> <given-names>S</given-names></name><name><surname>Piazza</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Distance and direction codes underlie navigation of a novel semantic space in the human brain</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>2727</fpage><lpage>2736</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1849-19.2020</pub-id><pub-id pub-id-type="pmid">32060171</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Anderson</surname> <given-names>KC</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Single neurons in prefrontal cortex encode abstract rules</article-title><source>Nature</source><volume>411</volume><fpage>953</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1038/35082081</pub-id><pub-id pub-id-type="pmid">11418860</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname> <given-names>CJCH</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Q-learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1007/BF00992698</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikenheiser</surname> <given-names>AM</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Over the river, through the woods: cognitive maps in the hippocampus and orbitofrontal cortex</article-title><source>Nature Reviews. Neuroscience</source><volume>17</volume><fpage>513</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.56</pub-id><pub-id pub-id-type="pmid">27256552</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Takahashi</surname> <given-names>YK</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal cortex as a cognitive map of task space</article-title><source>Neuron</source><volume>81</volume><fpage>267</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.005</pub-id><pub-id pub-id-type="pmid">24462094</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname> <given-names>DM</given-names></name><name><surname>Kawato</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Multiple paired forward and inverse models for motor control</article-title><source>Neural Networks</source><volume>11</volume><fpage>1317</fpage><lpage>1329</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(98)00066-5</pub-id><pub-id pub-id-type="pmid">12662752</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name><name><surname>Auerbach</surname> <given-names>EJ</given-names></name><name><surname>Strupp</surname> <given-names>J</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Feinberg</surname> <given-names>DA</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Uğurbil</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Evaluation of slice accelerations using multiband echo planar imaging at 3 T</article-title><source>NeuroImage</source><volume>83</volume><fpage>991</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.055</pub-id><pub-id pub-id-type="pmid">23899722</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamashita</surname> <given-names>O</given-names></name><name><surname>Sato</surname> <given-names>MA</given-names></name><name><surname>Yoshioka</surname> <given-names>T</given-names></name><name><surname>Tong</surname> <given-names>F</given-names></name><name><surname>Kamitani</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sparse estimation automatically selects voxels relevant for the decoding of fMRI activity patterns</article-title><source>NeuroImage</source><volume>42</volume><fpage>1414</fpage><lpage>1429</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.05.050</pub-id><pub-id pub-id-type="pmid">18598768</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeithamova</surname> <given-names>D</given-names></name><name><surname>Mack</surname> <given-names>ML</given-names></name><name><surname>Braunlich</surname> <given-names>K</given-names></name><name><surname>Davis</surname> <given-names>T</given-names></name><name><surname>Seger</surname> <given-names>CA</given-names></name><name><surname>van Kesteren</surname> <given-names>MTR</given-names></name><name><surname>Wutz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Brain Mechanisms of Concept Learning</article-title><source>Journal of Neuroscience</source><volume>39</volume><fpage>8259</fpage><lpage>8266</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1166-19.2019</pub-id><pub-id pub-id-type="pmid">31619495</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Supplementary note 1</title><sec id="s8-1"><title>Target and control regressors for main GLM</title><p>On average Abstract RL blocks tended to be later blocks (<xref ref-type="fig" rid="fig3">Figure 3F–G</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>), and to be associated with a small but significantly higher ratio of correct to incorrect responses (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Moreover, although Abstract RL blocks were associated with higher expected value compared with Feature RL blocks (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref>), at the trial level value (high / low) and learning strategy (Feature RL or Abstract RL) were uncorrelated (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2A</xref>), thus confirming the regressors’ orthogonality. The main analysis that was used for the value contrast and for the strategy contrast thus included regressors for ‘early’, ‘late’, ‘High value’, ‘Low value’, ‘Feature RL’, ‘Abstract RL’, such that the GLM explicitly controlled for the idiosyncratic features of the task. Other regressors of no interest were motion parameters, mean white matter signal, mean cerebro-spinal fluid signal, block, constant.</p></sec></sec><sec id="s9" sec-type="appendix"><title>Supplementary note 2</title><sec id="s9-1"><title>Levels of multivoxel fMRI neurofeedback</title><p>It is worth noting that the neurofeedback procedure targeted one feature’s level, for example red colour, rather than colour overall. One might wonder why this approach would work nevertheless? Given previous work with fMRI-based decoded neurofeedback (1), the main driver of the effect was most likely due to change in processing in VC, leading to increased functional representation of task features also in PFC (particularly, in vmPFC). Because in the current work feature levels were intrinsically coupled in task space, for example if red-horizontal corresponded to fruit 1, then green-vertical too, enhanced processing of red should also directly influence the paired colour.</p></sec></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68943.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Soltani</surname><given-names>Alireza</given-names> </name><role>Reviewer</role><aff><institution>Dartmouth College</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This study combines a novel behavioral task, reinforcement learning modeling, functional imaging, and neurofeedback to show that learning to focus on what information is important for predicting choice outcomes (i.e., &quot;abstraction&quot;) is guided by value signals. Because &quot;abstraction&quot; is a key process underlying flexible behavior, understanding its neural and computational basis is of major importance for cognitive neuroscience.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Value signals guide abstraction during learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Alireza Soltani (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>All three reviewers agreed that your study is well-conducted and the results convincing. However, they also had specific questions and suggestions for improvement. Below is a list of 'essential' comments that we would expect you to address in a revised version of your manuscript. The reviewers also made additional comments in the individual critiques, which we would encourage you to consider when preparing your revision.</p><p>1. There were several questions about the modeling. Please add a formal model comparison (accounting for model complexity) and show how much the mixture of expert RL model improves the fit over purely abstract and/or feature RL models. In addition, please show the responsibility values for the mixture model. This information, in addition to 'mean expected value', is needed to draw conclusions on the importance of feature and abstract RL models.</p><p>2. Please add an analysis of the behavior of excluded subjects. Do they adopt a different strategy and that is why they could not learn fast/accurately enough?</p><p>3. Does VC-vmPFC coupling predict the abstraction level? This connection seems to be as important to the authors' claims as the discussed relationship between VC-vmPFC coupling and learning speed (Figure 4C).</p><p>4. Please discuss issues around efficiency and plausibility that result from running 4 models simultaneously. That is, would it not be better if the brain only implemented the most complex algorithm instead of this algorithm in addition to the 3 simpler models?</p><p>5. Could it be that participants are simply better (more correct choices) in the abstract blocks (which presumably are also the later blocks)? If so, what does that mean for the value contrast in vmPFC? DO they reflect performance or strategy?</p><p>6. Please plot performance separately for CO, CD and OD blocks as well as for 2x2 vs 3x1 blocks.</p><p>7. Please clarify how the variance over RPEs (v) was calculated.</p><p>8. Was the cross-validation done between runs? If not, if should be done between runs, if possible.</p><p>9. The specific difference between relevant and irrelevant features seems important. Please add Figure S6 into the main manuscript.</p><p>10. Please add the results of the neurofeedback experiment. Were participants successful at increasing the size of the disc? Was there a correlation between this success and subsequent performance on the association paradigm? Full results can be provided in the supplements but should be referenced in the main text.</p><p><italic>Reviewer #1:</italic></p><p>Overall, the question studied in this work is timely, interesting and important. More specifically, although previous modeling studies have been focused on explaining how humans and other animals can learn informative abstract representations at the behavioral level, the underlying neural mechanisms remained poorly understood. Cortese and colleagues performed modeling analyses using a mixture of experts RL that consist of abstract and feature RL models as well as behavioral analyses (analyses of choice in a learning task) to demonstrate that performance of human subjects in a multi-dimensional learning task depends on their adopted level of abstraction. Supporting their modeling and behavioral analyses, authors analyzed fMRI data to demonstrate that the connections between ventromedial prefrontal cortex (vmPFC), the brain area encoding value signals, and visual cortex (VC) can predict subjects' learning speed, which is an indicator for adoption of abstract representations. Lastly, to demonstrate the causal relationship between VC and adoption of abstract representations, authors used a multivoxel neurofeedback procedure and showed that artificially adding value to features in VC results in an increase in adoption of abstract representations.</p><p>Although provided analyses are thorough and results are convincing, further quantitative analyses could be included to strengthen the main claims of the study. More specifically, it is helpful to show that results from fitting the mixture of expert approach is fully consistent with analyses using purely Abstract and/or Feature RL models. Additionally, an analysis of excluded subjects' behavior is missing. This is important, because failure in performing the task could indicate alternative (but unsuccessful) representations adopted by some subjects.</p><p>Comments for the authors:</p><p>(1.1) Page 6-8 and Figure 2: I could have missed this, but authors don't seem to provide any formal comparison between the goodness-of-fit using the mixture of expert RL model and pure Feature RL or Abstract RL models. For example, a simple Abstract RL model with only the informative features could capture behavior of certain subjects. I asked this because, the mixture of expert RL model contains more parameters than the Feature and Abstract RL models. I wonder when accounting for the extra parameters, would the mixture of expert RL model still provide a better fit? Please clarify.</p><p>(1.2) Related to the previous point, if the mixture of expert RL model provides a better fit, how much of the captured variance is related to the Feature RL vs Abstract RL experts (e.g. Figure 3F, G)? Perhaps this could be answered by examining the weight assigned to each type of RL in this model (λ values).</p><p>(1.3) Authors don't show the values of responsibility signals in the mixture of expert model. This information, in addition to 'mean expected value', is needed to draw conclusions on the importance of Feature and Abstract RL models.</p><p>2) I feel that the decoding analysis can be further improved. For example, do authors see any changes happen as a result of experience in the task? Also, a relevant reference is a study by Oemisch et al., Nat. Comm 2019, in which the prevalence of feature encoding neurons is examined.</p><p>3) How did the excluded subjects perform the task? Do they adopt a different strategy and that is why they could not learn fast/accurately enough? For example, did they learn about the value of different features and combine these values to make decisions (as in feature-based RL in Farashahi et al., 2017 or Farashahi et al., 2020, which is different from Feature RL and closer to Abstract RL)? Please comment.</p><p>4) Does VC-vmPFC coupling predict the abstraction level? This connection seems to be as important to authors' claims as the discussed relationship between VC-vmPFC coupling and learning speed (Figure 4C).</p><p><italic>Reviewer #2:</italic></p><p>Cortese and colleagues report two experiments in which human subjects made choices based on cues that had three distinct visual features. Only two of the three visual features were needed to make a correct choice. Hence participants could safely ignore one feature and learn based only on the two relevant features (a process the authors call abstraction). The authors modelled how behaviour and ventromedial prefrontal cortex activity shifted from processing all features to only the two relevant features and sought to elucidate the role of feature valuation in this process. In a second experiment, the authors used a real-time neurofeedback approach to tag visual representations of features and showed how this feature valuation process shapes the feature selection described in Experiment 1.</p><p>Past research has investigated the process by which humans and other animals learn to attend relevant features during reinforcement learning (e.g., Niv et al., 2015; Leong et al., 2017). These studies have outlined how reward shapes which features we pay attention to, and how attention shapes how we process reward. While a true account of how the process of &quot;abstraction&quot; might occur is still outstanding in my opinion (see below), this study adds some important insights about this process. A main point is that the authors show changing representations of features directly in vmPFC, which co-occur and interact with values. They also provide insight into the unique roles vmPFC and the hippocampus might have in this process, and how vmPFC value signals interact with sensory areas.</p><p>One particularly interesting aspect is this study is the use of neurofeedback to achieve reward-tagging of visual representations. This approach is noteworthy as it does not require to pair reward with the visual features themselves, but rather with the occurrence of neural representations that reflect said features. The behavioural effects of this manipulation on later learning were impressively strong: if the task required to attend features that were tagged with reward, behaviour was guided more strongly by appropriate selective learning; if the task required to ignore the features that were previously tagged with reward, the learning process was unchanged. This suggests that the process of selecting relevant features during learning interacts with a neural mechanism that tracks the values associated with these features. This conclusion is also supported by the fact that the same brain area that tracked the expected values of the stimuli during the task, vmPFC, was modulated by participants' level of feature selection, i.e. abstraction.</p><p>One weakness of this study is that the mechanism of abstraction remains unclear. The authors use a mixture of experts architecture of 4 different RL models: one RL model that tries to learn the appropriate action as a function of all visual features of the cues, and 3 models that try to learn based on the possible subsets of only two features.</p><p>I have some concerns about this approach. One concern is that the modelling presumes that participants concurrently run all 4 RL models, and continuously decide which one is best. The whole purpose of using a lower dimensional model is that it is more efficient. Permanently using 4 models, including the highest dimensional one, seems to defy the purpose of why the search for a best model was initiated in the first place. Arguably, such a scheme would also not necessarily predict that vmPFC should come to selectively represent only the most relevant features, since the model that requires processing all features needs to be kept up to date. It also does not shed light on how participants could ever truly stop to pay attention to some features, as feature selection is only done by weighting the model with the lowest prediction errors relative to the variance most strongly in the action selection process. In other words: I am unsure if the manuscript presents a reasonable account how representations become transformed. Other models, which do not suffer from these shortcomings, such as a function approximation model, might have added important insights to this study. Ideally, the presented model could also explain another interesting observation made by the authors: that performance improves over blocks, even though the relevant features change. This probably reflects that participants might have learned something more global about the dimensionality of the relevant space, but such a learning process is not accounted for by the authors. On the positive side, while such a concurrent training of 4 models seems computationally inefficient, it is at least data efficient, as each experience is used to update all models at once. And, the mixture of experts approach may be considered a tool to investigate feature selection, rather than a cognitive model. This should be clarified in the manuscript.</p><p>Another weakness of the manuscript in my opinion is that the valuation process targeted in the neurofeedback experiment presupposes that visual features are predictive of reward. One important aspect of abstraction, however, is that they may not be, as the same feature could lead to different outcomes, for instance based on unobservable context.</p><p>Comments for the authors:</p><p>– It would be great to try to model how longer-term knowledge about rewarding features and dimensionality of the task influence performance. How does the change over blocks occur? How do biases, as introduced through the neurofeedback procedure, influence model selection in the mixture of experts approach?</p><p>– Figure 5: Could it be that participants are simply better (more correct choices) in the abstract blocks (which presumably are also the later blocks)? If so, wouldn't that mean that the contrast high-low value in vmPFC will necessarily be higher for abstract blocks, but it could reflect performance rather than strategy?</p><p>– It would be interesting to see performance separately for CO, CD and OD blocks as well as for 2x2 vs 3x1 blocks. Is there a difference between 2x2 vs 3x1 blocks?</p><p>– Please consider avoiding the word &quot;predict&quot; when reporting a regression analyses or other types of non-causal effects.</p><p>– I did not follow how the variance over RPEs (v) is calculated. It would be important to clarify that in the manuscript, and indicate how it changes as learning progresses.</p><p>Does a small variance imply that all models have similar RPEs? If so, I am not sure the statement that it is related to sharper model selection is the only way to view it. It seems it could also be related to more model similarity.</p><p>– Isn't the fact that the relevant AbRL has higher values and learns faster trivial, given the design of the task? Would there have been any possibility that these results would not have come out? If not, I believe all p values should be removed.</p><p>– It would be great to add the results from Figure S6 into the main manuscript. The specific different between relevant and irrelevant features seems important</p><p>– Decoding: was the cross validation done between runs? If not, if should be done between runs if possible</p><p>– Neurofeedback: can you provide more information about how good participants were, and how long the neurofeedback effect was presented in the later task blocks (did it diminish over time?).</p><p><italic>Reviewer #3:</italic></p><p>The authors of this study aimed to demonstrate that abstract representations occur during the course of learning and clarify the role of the vmPFC in this process. In a novel association learning paradigm it was shown that participants used abstract representations more as the experiment went on, and that these representations resulted in enhanced performance and confidence. Using decoded neurofeedback, (implicit) attention to certain features was reinforced monetarily and this led to these features being used more during the association task. They conclude that top down control (vmPFC control of sensory cortices) guides the use of abstract representations.</p><p>The strengths of this paper include an objective, model based assessment of reinforcement learning, a strong and simple experimental paradigm incorporating variable stopping criteria, and the incorporation of decoded neurofeedback to determine if these representations could be covertly reinforced and affect behavior.</p><p>The weaknesses include a small sample, the lack of subjective evaluation of strategies/learning, and the omission of neurofeedback learning results.</p><p>Overall the authors achieved their aims and the data supports their conclusions.</p><p>This work will be of significance to computational psychologists, those who study abstraction and decision making, and those interested in the role of the vmPFC. One exciting implication of this work is that the use of certain features can be reinforced via decoded neurofeedback.</p><p>Comments for the authors:</p><p>I am not an expert in computational methods, therefore my comments are largely restricted to the neurofeedback study. The neurofeedback task is well designed and the use of relevant and irrelevant features is a nice control condition. That the effects were only observed for relevant blocks and the finding of increased abstraction from the late blocks of the main experiment strengthens their conclusions regarding causality.</p><p>While this is not the main focus of the manuscript, a supplement should contain the results of the neurofeedback experiment. Were participants successful at increasing the size of the disc? Was there a correlation between this success and subsequent performance on the association paradigm?</p><p>6s of modulation seems short for neurofeedback studies, please justify this short modulation time.</p><p>Finally, I am curious as to whether subjects were interviewed regarding the strategies they were using during the association paradigm. Were they aware they were using abstraction?</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Value signals guide abstraction during learning&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Michael Frank (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>1. Please revise your paper such that a casual reader will not erroneously take away that the MoE model presents a solid account of the data. Right now, the MoE is still mentioned in the abstract and also presented prominently in one of the main figures. You can leave the model in the manuscript if you want, but please further tone down any claims related to it.</p><p>2. It would be important to mention that some of the excluded subjects had good overall performance and the distribution of strategies was different among them.</p><p><italic>Reviewer #1:</italic></p><p>Authors have adequately and thoroughly addressed my concerns and questions. The only remaining concern I have is related to point # 1. Based on the presented results, it seems that MoE model does not provide the best fit of data. However, authors clearly mention and discuss this limitation in the revised manuscript. I have no further comments or concerns.</p><p><italic>Reviewer #2:</italic></p><p>I thank the authors for their thorough response to our previous concerns. I have two main concerns left:</p><p>1. The model comparison seems to refute the MoE model. At the same time, it seems clear that neither the FeRL nor AbRL model alone can truly capture participants behavior, since participants switch from one model to the other during the course of behavior. I think this should be made very clear in the paper, and I wonder how useful including the MoE model is.</p><p>My main reason is as follows: the core benefit of the MoE model, its ability to flexibly mix the two strategies, is seemingly not implemented in a way that reflects participants behavior. Would there be any way to improve the MoE models flexibility? The fact that it provides a &quot;proof of concept that an algorithmic solution to arbitrate between representations / strategies exists&quot; alone does not convince me, since the arbitration itself seems to not capture behavior and the pure existence of some algorithm is hardly surprising. In addition, there are the concerns about how realistic the MoE model is, which were raised under point 4.</p><p>I am also wondering whether the bad fit of the MoE model reflects how the fits were calculated: within each block, and then averaged (if I understood correctly)? Does that mean there was a new set of parameters per block? Have the authors tried to fit over the entirety of the experiment, using one set of parameters?</p><p>Relatedly, I believe that the change between strategies over time should be presented in one of the main figures, as this is an important point (e.g. by putting the rightmost graph from the Figure shown in the point by point response in the main paper).</p><p>2. I am also not fully convinced by the explanations about exclusions. The fact that the excluded subjects showed a different distribution of strategies should not serve as a reason for exclusion, since the purpose of the paper is to elucidate, in an unbiased manner, the distribution as it exists in the general population. The reported accuracy also does not seem very low for some participants. To me it seems that including the overall high performing subjects (with e.g. avg % correct &gt; 70%) would provide a more unbiased sample.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68943.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>All three reviewers agreed that your study is well-conducted and the results convincing. However, they also had specific questions and suggestions for improvement. Below is a list of 'essential' comments that we would expect you to address in a revised version of your manuscript. The reviewers also made additional comments in the individual critiques, which we would encourage you to consider when preparing your revision.</p><p>1. There were several questions about the modeling. Please add a formal model comparison (accounting for model complexity) and show how much the mixture of expert RL model improves the fit over purely abstract and/or feature RL models. In addition, please show the responsibility values for the mixture model. This information, in addition to 'mean expected value', is needed to draw conclusions on the importance of feature and abstract RL models.</p></disp-quote><p>As per your suggestion we have now included a formal model comparison. Using a hierarchical Bayesian approach (Piray et al. 2019), we performed concurrent fitting and comparison of all 3 models mixture-of-experts (MoE-RL), Feature RL (FeRL), Abstract RL (AbRL). We now show in supplementary (Figure 2—figure supplement 1) the model frequency (i.e., goodness of fit) in the sample. More specifically, for each block, we report the number of participants for which the MoE-RL, FeRL, or AbRL best explain their learning/choice data. Furthermore, Figure 2D in the main text now includes the responsibility values for the mixture model, in the same format as the ‘mean expected value’.</p><p>Note the model comparison shows that the MoE-RL in itself improves the fit over the purely abstract or feature RL only in very few instances. It is important to remember that FeRL and AbRL are much simpler models and that AbRL is an oracle model (i.e. the relevant dimension – unknown to the participant, is set by the experimenter). Our rationale in devising the MoE-RL model in the first part of the study was not to show that it is superior to the purely abstract or feature RL. MoE-RL was introduced as a proof of concept that an algorithmic solution in the arbitration between strategies is possible setting the stage for the second part of the work that focused on direct comparison of the two simpler models – Feature RL and Abstract RL (that have been used for all the remaining neuroimaging data analysis). We have now amended the text in the manuscript to make this clearer.</p><p>Main text (pp. 8, lines 212 – 227):</p><p>“The mixture-of-expert RL model revealed that participants who learned faster relied more on the best RL model value representations. […] Hence, we next sought to explicitly explain participant choices and learning according to either Feature RL or Abstract RL strategy.”</p><disp-quote content-type="editor-comment"><p>2. Please add an analysis of the behavior of excluded subjects. Do they adopt a different strategy and that is why they could not learn fast/accurately enough?</p></disp-quote><p>Of the 13 subjects that were excluded, for 2 we did not have recorded data as the experiment never went past the preliminary stage. As per your request we have now added the analysis of the 11 remaining excluded subjects (these also included 2 subjects that were removed for high head motion in the scanner). Briefly, excluded subjects displayed lower response accuracy in choosing the preferred fruit, as well as significantly higher (resp. lower) proportion of blocks labelled as Feature RL (resp. Abstract RL). Based upon these results, it would appear that excluded participants tended to remain ‘stuck’ in a non-optimal learning regime (Feature RL). These results are now reported in Figure 1—figure supplement 2 and referenced in the main text.</p><p>Main text (pp. 24, lines 673 – 675):</p><p>“Figure 1—figure supplement 2 reports a behavioural analysis of the excluded participants to investigate differences in performance or learning strategy compared to the 33 included participants.”</p><disp-quote content-type="editor-comment"><p>3. Does VC-vmPFC coupling predict the abstraction level? This connection seems to be as important to the authors' claims as the discussed relationship between VC-vmPFC coupling and learning speed (Figure 4C).</p></disp-quote><p>We agree with the reviewer that the VC-vmPFC coupling with learning speed suggests that a similar coupling might also exist with abstraction level. We have conducted the analysis suggested – we don’t find an effect that passes the statistical threshold but only a non-significant trend (robust linear regression, p = 0.065 one-sided). However, we should also highlight the exploratory nature of these between subjects’ correlations since our study was not optimised to detect between subjects' effects (which generally requires much larger n of subjects). We have now added the plot as Figure 4—figure supplement 2, and report the result in the main text and mentioned this caveat.</p><p>Main text (pp. 12, lines 331 – 335):</p><p>“The strength of the vmPFC – VC coupling showed a non-significant trend with the level of abstraction (N = 31, robust regression, slope = 0.013, t<sub>29</sub> = 1.56, p = 0.065 one-sided, Figure 4—figure supplement 2). […] Therefore, future work is required to confirm or falsify this result.”</p><disp-quote content-type="editor-comment"><p>4. Please discuss issues around efficiency and plausibility that result from running 4 models simultaneously. That is, would it not be better if the brain only implemented the most complex algorithm instead of this algorithm in addition to the 3 simpler models?</p></disp-quote><p>As one of the reviewers astutely noticed, while the mixture-of-experts model is not the most computationally thrifty model, it is very data efficient (i.e., the same data points can be used to update multiple models / representations in parallel). This model was introduced in the manuscript not necessarily as the most realistic model but as a proof of concept of a cognitive architecture that can arbitrate between the abstract and feature-based learning strategies. This was to set the stage for comparing these 2 strategies in the neuroimaging data analysis that was the main scope of this work. We have now clarified this in the manuscript and added a formal model comparison in response to the first query. Note the model comparison shows that the MoE-RL in itself improves the fit over the purely abstract or feature RL only in very few instances. It is important to remember that FeRL and AbRL are much simpler models and that AbRL is an oracle model (i.e. the relevant dimension – unknown to the participant, is set by the experimenter). Our rationale in devising the MoE-RL model in the first part of the study was not to show that it is superior to the purely abstract or feature RL. MoE-RL was introduced as a proof of concept that a simple algorithmic solution in the arbitration between strategies is possible.</p><p>We agree with the reviewer that more work needs to be done to establish which is the actual computational basis in humans to select the correct strategy in each circumstance. We share their feeling that humans might (at least at the conscious level) engage with one hypothesis at a time. However, there is circumstantial evidence that multiple strategies might be computed in parallel but deployed one at a time (Domenech et al. 2014, Koechlin 2018). Given how little we know about how different algorithmic architectures to solve this kind of problem are implemented by the brain, we agree that these are important issues that need to be discussed. Making clear our goal to show that arbitration between feature and abstract learning can be achieved using a relatively simple algorithm (the MoE-RL) and then proceeded to characterise the neural underpinnings of these two types of learning (i.e. FeRL and AbRL). These points are also elaborated in the discussion.</p><p>Main text (pp. 18 – 19, lines 513 – 529):</p><p>“An interesting and open question concerns whether the brain uses abstract representations in isolation – operating in a hypothesis-testing regime – i.e., favouring the current best model; or whether representations may be used to update multiple internal models, with behaviour determined by their synthesis (as in the mixture-of-experts architecture). […] Future work will need to establish the actual computational strategy employed by the human brain, further examining how it may also vary across circumstances.</p><disp-quote content-type="editor-comment"><p>5. Could it be that participants are simply better (more correct choices) in the abstract blocks (which presumably are also the later blocks)? If so, what does that mean for the value contrast in vmPFC? DO they reflect performance or strategy?</p></disp-quote><p>The intuition is correct, in the Abstract blocks participants tended to make more correct choices on average (now reported in Figure 5—figure supplement 1), therefore Abstract RL blocks were associated with higher expected value compared with Feature RL blocks. As the reviewer correctly hinted this is probably due to the fact that abstract strategies were more frequent in late trials in which performance is usually higher. This small but significant result is now reported in Figure 3—figure supplement 2. Importantly for the GLM analysis at trial-by-trial level, value (high / low) and learning strategy (Feature RL or Abstract RL) were uncorrelated (Figure 5—figure supplement 2A) thus confirming the regressors’ orthogonality allowing us to include both regressors in the same GLM. We also included regressors for ‘early’, ‘late’.</p><p>To recapitulate we included regressors for ‘early’, ‘late’, ‘High value’, ‘Low value’, ‘Feature RL’, ‘Abstract RL’, such that the GLM explicitly controlled for the idiosyncratic features of the task.</p><p>We are therefore confident that our GLM was able to correctly disentangle the contribution of all these parameters on the neural signal.</p><p>We have updated the supplementary note 1 – we refer to it in the main text to better explain the idiosyncrasies of the task / conditions and their controls in the main GLM analysis. The text is reported here below.</p><p>Main text (pp. 12, lines 353 – 355):</p><p>“Having established that the vmPFC computes a goal-dependent value signal, we evaluated whether the activity level of this region was sensitive to the strategies that participants used. To do so, we used the same GLM introduced earlier, and estimated two new statistical maps from the regressors ‘Abstract RL’ and ‘Feature RL’ while controlling for idiosyncratic features of the task, i.e., high/low value and early/late trials (see Methods and Supplementary note 1).”</p><p>“Supplementary Note 1:</p><p>On average Abstract RL blocks tended to be later blocks (Figure 3F-G, Figure 3—figure supplement 2A), and to be associated with a slightly but significantly higher ratio of correct to incorrect responses (Figure 5—figure supplement 1). […] Other regressors of no interest were motion parameters, mean white matter signal, mean cerebro-spinal fluid signal, block, constant.”</p><disp-quote content-type="editor-comment"><p>6. Please plot performance separately for CO, CD and OD blocks as well as for 2x2 vs 3x1 blocks.</p></disp-quote><p>Thanks for the suggestion; we have done this. Figure S1 displays performance (learning speed) plotted separately for CO, CD, OD blocks, as well as for 2x2 and 3x1 blocks. There were no significant differences between these conditions.</p><disp-quote content-type="editor-comment"><p>7. Please clarify how the variance over RPEs (v) was calculated.</p></disp-quote><p>We apologize for our oversight – the variance over RPEs (v) was an hyperparameter estimated at the participant level, in each block. This has been clarified in the manuscript.</p><p>Main text (pp. 6, lines 177 – 179):</p><p>“Estimated hyperparameters (learning rate 𝛂, forgetting factor 𝛄, RPE variance 𝛎) were used to compute value functions of participant data, as well as to generate new, artificial choice data and value functions.”</p><disp-quote content-type="editor-comment"><p>8. Was the cross-validation done between runs? If not, if should be done between runs, if possible.</p></disp-quote><p>The cross-validation was done by repeatedly splitting the whole data in a training and test group, at random (N=20). We applied this procedure because the number of trials available for each class differed across conditions and best models (Feature RL and Abstract RL). For example, Feature RL may have had 128 trials labelled as ‘green’, 109 as ‘red’, while Abstract RL 94 as ‘green’, and 99 as ‘red’. We thus selected, in each fold, the number of trials representing 80% of the data in the condition with the lowest number of trials (in this example, 80% of 94). This procedure allowed us to avoid a situation in which different amounts of data are used to train the classifiers in different conditions, making comparisons or performance averages weaker to interpret. Nevertheless, since it has been rightfully pointed out that the procedure generally involves testing a classifier on data from a different run (leave-one-run-out cross-validation) such that even subtle differences across runs cannot be exploited by the algorithm.</p><p>Therefore, we have now implemented this procedure, recommended by the reviewer, as the primary analysis (reported in the new Figure 5C). Note that results from these two cross-validation approaches closely align, and we have now moved the original result to supplementary (Figure S10).</p><p>Main text (pp. 13, lines 380 – 389):</p><p>“We found that classification accuracy was significantly higher in Abstract RL trials compared with Feature RL trials in both the HPC and vmPFC (two-sided t-test, HPC: t<sub>32</sub> = -2.37, p<sub>(FDR)</sub> &lt; 0.036, vmPFC: t<sub>32</sub> = -2.51, p<sub>(FDR)</sub> = 0.036, Figure 5C), while the difference was of opposite sign in VC (t<sub>32</sub> = 1.61, p<sub>(FDR)</sub> = 0.12, Figure 5C). […] A control analysis equating the number of training trials for each feature and condition replicated the original finding (Figure 5—figure supplement 3).”</p><p>Methods (pp. 32 – 33, lines 937 – 961):</p><p>“Cross-validation was used for each MVP analysis to evaluate the predictive power of the trained (fitted) model. […] Results (Figures 5C, Figure 5—figure supplement 3) report the cross-validated average of the best yielding iteration.”</p><disp-quote content-type="editor-comment"><p>9. The specific difference between relevant and irrelevant features seems important. Please add Figure S6 into the main manuscript.</p></disp-quote><p>We have inserted the previous Figure S6 into the main manuscript, as panels D and E in Figure 5.</p><disp-quote content-type="editor-comment"><p>10. Please add the results of the neurofeedback experiment. Were participants successful at increasing the size of the disc? Was there a correlation between this success and subsequent performance on the association paradigm? Full results can be provided in the supplements but should be referenced in the main text.</p></disp-quote><p>Participants were successful at increasing the size of the disc, with similar levels of performance attained in the first and second session. We also show the relationship between the success in inducing the target pattern and the subsequent behavioural effect. There was a significant tendency in positive correlation between the cumulative session-averaged amount of reward obtained during the NFB manipulation and the strength of the subsequent behavioural effect. Results are reported in the main text and in supplementary, Figure 6—figure supplement 1.</p><p>Main text (pp. 16, lines 463 – 466):</p><p>“Participants were successful at increasing the disk size in the neurofeedback task (Figure 6—figure supplement 1A-B). Furthermore, those who were more successful were also more likely to display larger increases in abstraction in the subsequent behavioural test (Figure 6—figure supplement 1C).”</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>1. Please revise your paper such that a casual reader will not erroneously take away that the MoE model presents a solid account of the data. Right now, the MoE is still mentioned in the abstract and also presented prominently in one of the main figures. You can leave the model in the manuscript if you want, but please further tone down any claims related to it.</p></disp-quote><p>As per your suggestion we have toned down any claims related to the MoE model. To this end, we have amended the abstract, introduction, results and discussion. More specifically, we have removed any reference to the MoE model in the abstract and summary of results in the discussion to avoid giving the impression that the MoE model provides the best explanatory degree to the data. We further acknowledge in the discussion that with the current task the MoE model does not present a strong account of the data, and is not the main take-home message. Nevertheless, we have decided to keep the model in the manuscript and in Figure 2, as we still believe the MoE provides certain valuable pieces of information: (i) it provides a simple way to arbitrate between internal representations, (ii) it affords a data efficient approach (one data point can be used to update multiple strategies in parallel), (iii) it shows that participants who are more confident in their performance also have better selection of internal representations.</p><p>We report below excerpts of the main text that we have modified.</p><p>Abstract:</p><p>“Mixture-of-experts Reinforcement-learning algorithms revealed that, with learning, high-value abstract representations increasingly guided participant behaviour, resulting in better choices and higher subjective confidence.”</p><p>Main text, introduction (pp., lines):</p><p>“Reinforcement learning (RL) and mixture-of-experts (Jacobs et al., 1991; Sugimoto et al., 2012) modelling allowed us to track participant valuation processes and to dissociate their learning strategies (both at the behavioural and neural levels) based on the degree of abstraction.”</p><p>Main text, results (pp., lines):</p><p>Section title: “Mixture-of-experts reinforcement learning for Discovery of abstract representations”.</p><p>Main text, discussion (pp., lines):</p><p>“The ability to generate abstractions from simple sensory information has been suggested as crucial to support flexible and adaptive behaviours (Cortese et al., 2019; Ho et al., 2019; Wikenheiser and Schoenbaum, 2016). […] Of particular importance will be further examining how such strategies vary across circumstances (tasks, contexts, or goals).”</p><disp-quote content-type="editor-comment"><p>2. It would be important to mention that some of the excluded subjects had good overall performance and the distribution of strategies was different among them.</p></disp-quote><p>We agree that mentioning these aspects of the excluded subjects is important. We have thus added this information in the results, in the ‘Behavioural account of learning’ and ‘Behaviour shifts from Feature- to Abstraction-based reinforcement learning’ subsections.</p><p>To clarify, subjects were excluded independently of the distribution of strategies, which were computed at a later stage. The a priori criteria for exclusion were: failure to learn the association in 3 blocks or more (i.e., reaching a block limit of 80 trials without having learned the association), or failure to complete more than 10 blocks in the allocated time. These criteria were set to ensure a sufficient number of learning blocks for ensuing analyses.</p><p>To avoid any confusion, we have added this information to the main text, in results, subsection ‘Experimental design’.</p><p>Main text, results (pp., lines):</p><p>“Participants failing to learn the association in 3 blocks or more (i.e., reaching a block limit of 80 trials without having learned the association), and / or failing to complete more than 10 blocks in the allocated time, were excluded (see Methods). All main results reported in the paper are from the included sample of N = 33 participants.”</p><p>Main text, results (pp., lines):</p><p>“Excluded participants (see Methods) had overall lower performance (Figure 1 supplement 2), although some had comparable ratios correct.”</p><p>Main text, results (pp., lines):</p><p>“Given the lower learning speed in excluded participants, the distribution of strategies was also different among them, with a higher ratio of Feature RL blocks (Figure 3 supplement 4).”</p></body></sub-article></article>