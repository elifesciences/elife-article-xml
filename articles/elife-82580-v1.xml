<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82580</article-id><article-id pub-id-type="doi">10.7554/eLife.82580</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-99408"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7257-428X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290596"><name><surname>Contier</surname><given-names>Oliver</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2983-4709</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-225310"><name><surname>Teichmann</surname><given-names>Lina</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290593"><name><surname>Rockter</surname><given-names>Adam H</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2446-717X</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-203643"><name><surname>Zheng</surname><given-names>Charles Y</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290594"><name><surname>Kidder</surname><given-names>Alexis</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-290595"><name><surname>Corriveau</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-258535"><name><surname>Vaziri-Pashkam</surname><given-names>Maryam</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1830-2501</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28129"><name><surname>Baker</surname><given-names>Chris I</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6861-8964</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><institution content-type="dept">Vision and Computational Cognition Group</institution>, <institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution>, <addr-line><named-content content-type="city">Leipzig</named-content></addr-line>, <country>Germany</country></aff><aff id="aff2"><institution content-type="dept">Max Planck Institute for Human Cognitive and Brain Sciences</institution>, <institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution>, <addr-line><named-content content-type="city">Leipzig</named-content></addr-line>, <country>Germany</country></aff><aff id="aff3"><institution content-type="dept">Laboratory of Brain and Cognition</institution>, <institution>National Institute of Mental Health</institution>, <addr-line><named-content content-type="city">Bethesda</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><institution content-type="dept">Machine Learning Team</institution>, <institution>National Institute of Mental Health</institution>, <addr-line><named-content content-type="city">Bethesda</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-32294"><name><surname>Barense</surname><given-names>Morgan</given-names></name><role>Reviewing editor</role><aff><institution>University of Toronto</institution>, <country>Canada</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>hebart@cbs.mpg.de</email> (MH);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>27</day><month>02</month><year>2023</year></pub-date><volume>12</volume><elocation-id>e82580</elocation-id><history><date date-type="received"><day>09</day><month>08</month><year>2022</year></date><date date-type="accepted"><day>25</day><month>02</month><year>2023</year></date></history><permissions><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0</ext-link> public domain dedication.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82580-v1.pdf"/><abstract><p>Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here we present THINGS-data, a multimodal collection of large-scale neuroimaging and behavioral datasets in humans, comprising densely-sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-data is unique in its breadth of richly-annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-driven and data-driven applications. THINGS-data constitutes the core public release of the THINGS initiative (https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIA-MH-002909</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name><name><surname>Teichmann</surname><given-names>Lina</given-names></name><name><surname>Rockter</surname><given-names>Adam H</given-names></name><name><surname>Kidder</surname><given-names>Alexis</given-names></name><name><surname>Corriveau</surname><given-names>Anna</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>Maryam</given-names></name><name><surname>Baker</surname><given-names>Chris I</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIC-MH002968</award-id><principal-award-recipient><name><surname>Zheng</surname><given-names>Charles Y</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max-Planck-Gesellschaft</institution></institution-wrap></funding-source><award-id>Max Planck Research Group M.TN.A.NEPF0009</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name><name><surname>Contier</surname><given-names>Oliver</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>Starting Grant StG-2021-101039712</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003495</institution-id><institution>Hessisches Ministerium für Wissenschaft und Kunst</institution></institution-wrap></funding-source><award-id>LOEWE Start Professorship</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="par-6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100018668</institution-id><institution>Max Planck School of Cognition</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Contier</surname><given-names>Oliver</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf2"><p>Chris I Baker, Senior editor, <italic>eLife</italic>.</p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All research participants for the fMRI and MEG studies provided informed consent in participation and data sharing, and they received financial compensation for taking part in the respective studies. The research was approved by the NIH Institutional Review Board as part of the study protocol 93-M-0170 (NCT00001360).All research participants taking part in the online behavioral study provided informed consent for the participation in the study. The online study was conducted in accordance with all relevant ethical regulations and approved by the NIH Office of Human Research Subject Protection (OHSRP).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>All parts of the THINGS-data collection are freely available on scientific data repositories. We provide the raw MRI (https://openneuro.org/datasets/ds004192) and raw MEG (https://openneuro.org/datasets/ds004212) datasets in BIDS format98 on OpenNeuro109. In addition to these raw datasets, we provide the raw and preprocessed MEG data as well as the raw and derivative MRI data on Figshare110 (https://doi.org/10.25452/figshare.plus.c.6161151). The MEG data derivatives include preprocessed and epoched data that are compatible with MNE-python and CoSMoMVPA in MATLAB. The MRI data derivatives include single trial response estimates, category-selective and retinotopic regions of interest, cortical flatmaps, independent component based noise regressors, voxel-wise noise ceilings, and estimates of subject specific retinotopic parameters. In addition, we included the preprocessed and epoched eyetracking data that were recorded during the MEG experiment in the OpenNeuro repository. The behavioral triplet odd-one-out dataset can be accessed on OSF (https://osf.io/f5rn6/, https://doi.org/10.17605/OSF.IO/F5RN6).</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Hebart MN</collab><collab>Contier O</collab><collab>Teichmann L</collab><collab>Rockter AH</collab><collab>Zheng CY</collab><collab>Kidder A</collab><collab>Corriveau A</collab><collab>Vaziri-Pashkam M</collab><collab>Baker CI</collab></person-group><year iso-8601-date="2022">2022</year><source>THINGS-fMRI</source><ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004192/versions/1.0.2">https://openneuro.org/datasets/ds004192/versions/1.0.2</ext-link><comment>OpenNeuro doi:10.18112/openneuro.ds004192.v1.0.5</comment></element-citation><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Hebart MN</collab><collab>Contier O</collab><collab>Teichmann L</collab><collab>Rockter AH</collab><collab>Zheng CY</collab><collab>Kidder A</collab><collab>Corriveau A</collab><collab>Vaziri-Pashkam M</collab><collab>Baker CI</collab></person-group><year iso-8601-date="2023">2023</year><source>THINGS-MEG</source><ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds004212/versions/1.0.2">https://openneuro.org/datasets/ds004212/versions/1.0.2</ext-link><comment>OpenNeuro doi:10.18112/openneuro.ds004212.v2.0.0</comment></element-citation><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Hebart MN</collab><collab>Contier O</collab><collab>Teichmann L</collab><collab>Rockter AH</collab><collab>Zheng CY</collab><collab>Kidder A</collab><collab>Corriveau A</collab><collab>Vaziri-Pashkam M</collab><collab>Baker CI</collab></person-group><year iso-8601-date="2022">2022</year><source>THINGS-odd-one-out</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/f5rn6/?view_only=3eca2c272c4643e5b7c3f9a875fa9dc7">https://osf.io/f5rn6/?view_only=3eca2c272c4643e5b7c3f9a875fa9dc7</ext-link><comment>Open Science Foundation  doi:10.17605/OSF.IO/F5RN6</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-82580-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>