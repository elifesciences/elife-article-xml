<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86873</article-id><article-id pub-id-type="doi">10.7554/eLife.86873</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.86873.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>OpenApePose, a database of annotated ape photographs for pose estimation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-306524"><name><surname>Desai</surname><given-names>Nisarg</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3210-9409</contrib-id><email>desai054@umn.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-306530"><name><surname>Bala</surname><given-names>Praneet</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2144-1986</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-306531"><name><surname>Richardson</surname><given-names>Rebecca</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-248720"><name><surname>Raper</surname><given-names>Jessica</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0964-9944</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154524"><name><surname>Zimmermann</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-227364"><name><surname>Hayden</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>Department of Neuroscience and Center for Magnetic Resonance Research, University of Minnesota</institution></institution-wrap><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>Department of Computer Science, University of Minnesota</institution></institution-wrap><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory National Primate Research Center, Emory University</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kalan</surname><given-names>Ammie K</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04s5mat29</institution-id><institution>University of Victoria</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Perry</surname><given-names>George H</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04p491231</institution-id><institution>Pennsylvania State University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>11</day><month>12</month><year>2023</year></pub-date><volume>12</volume><elocation-id>RP86873</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-02-16"><day>16</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2022-11-30"><day>30</day><month>11</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2212.00741"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-07-13"><day>13</day><month>07</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.86873.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-10-31"><day>31</day><month>10</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.86873.2"/></event></pub-history><permissions><copyright-statement>© 2023, Desai et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Desai et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86873-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86873-figures-v1.pdf"/><abstract><p>Because of their close relationship with humans, non-human apes (chimpanzees, bonobos, gorillas, orangutans, and gibbons, including siamangs) are of great scientific interest. The goal of understanding their complex behavior would be greatly advanced by the ability to perform video-based pose tracking. Tracking, however, requires high-quality annotated datasets of ape photographs. Here we present <italic>OpenApePose</italic>, a new public dataset of 71,868 photographs, annotated with 16 body landmarks of six ape species in naturalistic contexts. We show that a standard deep net (HRNet-W48) trained on ape photos can reliably track out-of-sample ape photos better than networks trained on monkeys (specifically, the <italic>OpenMonkeyPose</italic> dataset) and on humans (<italic>COCO</italic>) can. This trained network can track apes almost as well as the other networks can track their respective taxa, and models trained without one of the six ape species can track the held-out species better than the monkey and human models can. Ultimately, the results of our analyses highlight the importance of large, specialized databases for animal tracking systems and confirm the utility of our new ape database.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>All animals carry out a wide range of behaviors in everyday life, such as feeding and communicating with one another. Understanding the complex behavior of non-human apes such as chimpanzees, bonobos, gorillas, orangutans, and various gibbons is of great interest to scientists due to their close relationship with humans.</p><p>Each behavior is made up of a string of poses that an animal makes with its body. To analyze them in a reliable and consistent way, scientists have developed automated pose estimation methods that determine the position of body parts from photographs and videos. While these systems require minimal external input to perform, they need to be trained on a large dataset of high-quality annotated images of the target animals to teach the system what to look for.</p><p>So far, scientists have relied on systems trained on monkey and human images to analyze ape data. However, apes are particularly challenging to track because their body textures are uniform, and they have a large number of poses. Therefore, for the most accurate tracking of ape behaviors, a dedicated training dataset of annotated ape images is required.</p><p>Desai et al. filled this gap by creating the “<italic>OpenApePose</italic>” dataset, which contains 71,868 photographs of apes from six species, annotated using 16 body landmarks. To test the dataset, the researchers trained an artificial intelligence network on separate monkey, human and ape datasets. The findings showed that the network is better at tracking apes when trained on ape images rather than those of monkeys or humans. It is also equally good at tracking apes as other monkey and human networks are at tracking their own species. This is contrary to optimistic expectations that monkey and human models could be generalized to apes. Training the network without images of one of the six ape species showed that it can still track the excluded species better than monkey and human models can. These experiments highlight the importance of species and family-specific datasets.</p><p><italic>OpenApePose</italic> is a valuable resource for researchers from various fields. It can aid tracking of animal behavior in the wild using large quantities of footage recorded by camera traps and drones. Artificial intelligence models trained on the <italic>OpenApePose</italic> dataset could also help scientists – such as neuroscientists – link movement with other types of data, including brain activity measurements, to gain deeper insights into behavior.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>apes</kwd><kwd>pose estimation</kwd><kwd>deep learning</kwd><kwd>behavior tracking</kwd><kwd>dataset</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH128177</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P30 DA048742</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name><name><surname>Hayden</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH125377</award-id><principal-award-recipient><name><surname>Hayden</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>2024581</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name><name><surname>Hayden</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007249</institution-id><institution>University of Minnesota</institution></institution-wrap></funding-source><award-id>UMN AIRP award</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name><name><surname>Hayden</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Minnesota Institute of Robotics</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Emory National Primate Research Center</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Raper</surname><given-names>Jessica</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P51-OD011132</award-id><principal-award-recipient><name><surname>Zimmermann</surname><given-names>Jan</given-names></name><name><surname>Hayden</surname><given-names>Benjamin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A dedicated dataset for non-human apes improves pose estimation and tracking accuracy in apes.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The ability to automatically track moving animals using video systems has been a great boon for the life sciences, including biomedicine (<xref ref-type="bibr" rid="bib8">Calhoun and Murthy, 2017</xref>; <xref ref-type="bibr" rid="bib32">Marshall et al., 2022</xref>; <xref ref-type="bibr" rid="bib34">Mathis and Mathis, 2020</xref>; <xref ref-type="bibr" rid="bib43">Pereira et al., 2020</xref>). Such systems allow data collected from digital video cameras to be used to infer the positions of body landmarks such as head, hands, and feet, without the use of specialized markers. In recent years, the field has witnessed the development of sophisticated tracking systems that can track and identify behavior in species important for biological research, including humans, worms, flies, and mice (e.g., <xref ref-type="bibr" rid="bib7">Bohnslav et al., 2021</xref>; <xref ref-type="bibr" rid="bib9">Calhoun et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Hsu and Yttri, 2021</xref>; <xref ref-type="bibr" rid="bib30">Marques et al., 2020</xref>). This problem is more difficult for monkeys, although, even here, significant progress has been made (<xref ref-type="bibr" rid="bib2">Bain et al., 2021</xref>; <xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib14">Dunn et al., 2021</xref>; <xref ref-type="bibr" rid="bib26">Labuguen et al., 2020</xref>; <xref ref-type="bibr" rid="bib29">Marks et al., 2022</xref>; reviewed in <xref ref-type="bibr" rid="bib16">Hayden et al., 2022</xref>).</p><p>In theory, species-general systems can achieve good performance with small numbers (hundreds or thousands) of hand-annotated sample images. In practice, however, such systems tend to be of limited functionality. That is, they may show brittle performance and may tend to perform poorly in edge cases, which may wind up being quite common. In general, large and precisely annotated databases (ones with tens of thousands of images or more) may be needed as training sets to achieve robust performance. The monkey tracking in our monkey-specific system (<italic>OpenMonkeyStudio</italic>), for example, required over 100,000 annotated images, and performance continued to improve even at larger numbers of images in the training set (<xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>).</p><p>However, there is no currently publicly available database specifically for non-human apes, which in turn means that readily usable tracking solutions specific to apes do not exist. Although there is hope that models built on related species, such as humans and/or monkeys may generalize to apes, transfer methods remain a work in progress (<xref ref-type="bibr" rid="bib47">Sanakoyeu et al., 2020</xref>). Like monkeys, apes are particularly challenging to track due to their homogeneous body texture and exponentially large number of pose configurations (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). We recently developed a novel system for tracking the pose of monkeys (<xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Bala et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). A critical ingredient of this system was the collection of high-quality annotated images of monkeys, which were used as raw material for training the model. Indeed, the need for high-quality training datasets is a major barrier to progress for much of machine learning (<xref ref-type="bibr" rid="bib12">Deng et al., 2009</xref>). Obtaining a database of annotated ape photographs is especially difficult due to apes’ relative rarity in captive settings and due to the proprietary oversight common among primatologists.</p><p>The lack of such tracking systems represents a critical gap due to the importance of apes in science. The ape (<italic>Hominoidea</italic>) superfamily includes the great apes (among them, humans, <italic>Hominidae</italic> family) and the lesser apes, gibbons and siamangs (<italic>Hylobatidae</italic> family). These species, which represent humans’ closest relatives in the animal kingdom, have complex social and foraging behavior, a high level of intelligence, and a behavioral repertoire characterized by flexibility and creativity (<xref ref-type="bibr" rid="bib48">Smuts et al., 2008</xref>; <xref ref-type="bibr" rid="bib49">Strier, 2016</xref>). The ability to perform sophisticated video tracking of apes would bring great benefits to primatology and comparative psychology, as well as to related fields like anthropology and kinesiology (<xref ref-type="bibr" rid="bib16">Hayden et al., 2022</xref>). Moreover, tracking systems could be deployed to improve ape welfare and to supplement in situ conservation efforts (<xref ref-type="bibr" rid="bib24">Knaebe et al., 2022</xref>).</p><p>Here we provide a dataset of annotated ape photographs, which we call <italic>OpenApePose</italic>. This dataset includes four species from the <italic>Hominidae</italic> family: bonobos, chimpanzees, gorillas, orangutans, and several species from the <italic>Hylobatidae</italic> family<italic>,</italic> pooled into two categories of gibbons and siamangs. This dataset consists primarily of photographs taken at zoos, and also includes images from online sources, including publicly available photographs and videos. Our database is designed to have a rich sampling of poses and backgrounds, as well as a range of image features. We provide high-precision annotation of 16 body landmarks. We show that tracking models built using this database do a good job tracking from a large sample of ape images, and do a better job than networks trained with monkey (<italic>OpenMonkeyPose</italic>, <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>) or human (COCO, <xref ref-type="bibr" rid="bib28">Lin et al., 2014</xref>) databases. We also show that tracking quality is comparable to these two databases tracking their own species (although performance lags slightly behind both). We believe this database will provide an important resource for future investigations of ape behavior.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>OpenApePose dataset</title><p>We collected several hundred thousand images of five species of apes: chimpanzee, bonobo, gorilla, orangutan, siamang, and a sixth category, including non-siamang gibbons (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Images were collected from zoos, sanctuaries, and field sites. We also added the ape images from the <italic>OpenMonkeyPose</italic> dataset (16,984 images) to our new dataset, which we call <italic>OpenApePose</italic>. Combined, our final dataset has 71,868 annotated ape images. Our image set contains 11,685 bonobos (<italic>Pan paniscus</italic>), 18,010 chimpanzees (<italic>Pan troglodytes</italic>), 12,905 gorillas (<italic>Gorilla gorilla</italic>), 12,722 orangutans (<italic>Pongo</italic> sp.), and 9274 gibbons (genus <italic>Hylobates</italic> and <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Nomascus">Nomascus</ext-link>) and 7272 siamangs (<italic>Symphalangus syndactylus</italic>, <xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Sampling of annotated images in the OpenApePose dataset.</title><p>Thirty-two photographs chosen to illustrate the range of photographs available in our larger set, illustrating the variety in species, pose, and background. Each annotated photograph contains an annotation for sixteen different body landmarks (shown here with connecting lines).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-fig1-v1.tif"/></fig><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Properties of the OpenApePose database.</title><p>(<bold>A</bold>) Number of annotated images per different species in the OpenApePose dataset. (<bold>B</bold>) Illustration of our annotations. All 16 annotated points are indicated and labeled on a gorilla image drawn from the database. (<bold>C</bold>) Histogram of bounding box sizes in the database as defined as length of the bounding box diagonal in pixels.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-fig2-v1.tif"/></fig><p>We manually sorted and cropped the images such that each cropped image contains the full body of at least one ape while minimizing repetitive poses to ensure a greater diversity of poses in the full dataset. We ensured that all cropped images have a resolution ≥300 × 300 pixels. Next, we used a commercial annotation service (Hive AI) to manually annotate the 16 landmarks (we used the same system in <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>; see ‘Methods’). The 16 landmarks together comprise a <italic>pose</italic> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>We used these landmarks to infer a bounding box, defined as the distance +20% pixels between the farthest landmarks on the two axes. We include a histogram of the bounding box sizes in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, where size is defined as the length of the diagonal of the bounding box. Our landmarks were (1) nose, (2–3) left and right eye, (4) head, (5) neck, (6–7) left and right shoulder, (8–9) elbows, (10–11) wrists, (12) sacrum, that is, the center point between the two hips, (13–14) knees, and (15–16) ankles. These are the same landmarks we used in our corresponding monkey dataset (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>), although in that set we also included a landmark for the tip of the tail. (We do not include that here because apes do not have tails.) Each data instance is made of image, species, bounding box, and pose.</p><p>Our previous monkey-centered dataset was presented in the form of a <italic>challenge</italic> (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). Our ape dataset, by contrast, is presented solely as a resource. The annotations and all 71,868 images are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib13">desai-nisarg, 2023</xref>).</p></sec><sec id="s2-2"><title>Overview of OpenApePose dataset</title><p>To illustrate the range of poses in the OpenApePose dataset, we visualize the space spanned by its poses using Uniform Manifold Approximation and Projection (UMAP, <xref ref-type="bibr" rid="bib36">McInnes et al., 2018</xref>, <xref ref-type="fig" rid="fig3">Figure 3</xref>). To obtain standard and meaningful spatial representations, we use normalized landmark coordinates based on image size—the x-coordinate normalized using image width and the y-coordinate normalized using the image height. We then center each pose to a reference root landmark (the sacrum), such that the normalized coordinate of each landmark is with respect to the sacrum landmark. We then create the UMAP visualizations by performing dimension reduction using the <monospace>UMAP()</monospace> function in the <monospace>umap-learn</monospace> Python package (<xref ref-type="bibr" rid="bib36">McInnes et al., 2018</xref>). We use the Euclidean distance metric with <monospace>n_neighbors=15</monospace> and <monospace>min_dist=0.001</monospace>, which allowed us a reasonable balance in combining similar poses and separating dissimilar ones.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Uniform Manifold Approximation and Projection (UMAP) visualization of the distribution of poses with the species IDs labeled.</title><p>X- and Y-dimensions indicate positions in a UMAP space. Each dot indicates a single photograph/pose. Dot colors indicate species (see inscribed legend, right). We include, as insets, example poses, with an arrow pointing to their position in the UMAP plot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-fig3-v1.tif"/></fig><p>We label the six different species in the database to visualize their distribution in the dimensions reduced using UMAP. We observe that the <italic>Hylobatidae</italic> family (gibbons and siamangs) form somewhat separate pose clusters from the <italic>Hominidae</italic> family (bonobos, chimpanzees, gorillas, and orangutans, <xref ref-type="fig" rid="fig3">Figure 3</xref>). These clusters likely reflect the differences in locomotion styles between these families, <italic>Hylobatidae</italic> being true brachiators, whereas <italic>Hominidae</italic> spend more time on moving on the ground. Of the <italic>Hominidae,</italic> the orangutans spend the most time in the trees like the <italic>Hylobatidae,</italic> and this is reflected in the overlap of their poses with the <italic>Hylobatidae</italic>.</p></sec><sec id="s2-3"><title>Demonstrating the effectiveness of the OpenApePose dataset</title><p>We next performed an assessment of the OpenApePose dataset for pose estimation. To do this, we used a standard deep net system HRNet-W48, which currently remains state of the art for pose estimation (<xref ref-type="bibr" rid="bib50">Sun et al., 2019</xref>). The deep high-resolution net (HRNet) architecture achieves superior performance as it works with high-resolution pose representations from the get-go compared to conventional architectures that work with lower resolution representations and extrapolate to higher resolutions from low resolutions (ibid.). We previously showed that this system does a good job tracking monkeys with a monkey database (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>).</p><p>We split the benchmark dataset into training (43,120 images, 60%), validation (14,374 images, 20%), and testing (14,374 images, 20%) datasets using the <monospace>train_test_split() </monospace>function in the<monospace> scikit-learn</monospace> Python library (<xref ref-type="bibr" rid="bib41">Pedregosa et al., 2011</xref>).</p><p>We first investigated the ability of a model trained on the ape training set to accurately predict landmarks on apes from the test set (i.e., a set that contains only images that were not used in training). To evaluate the performance of the HRNet-W48 models trained on this dataset, we used a standard approach of calculating <italic>percent correct keypoints</italic> (PCK) at a given threshold (here, 0.2, see ‘Methods’) and at a series of other thresholds (0.01–1, at 0.01 increments, <xref ref-type="fig" rid="fig4">Figure 4A</xref>). The PCK@0.2 for this model was 0.876, and the area under the curve of PCK at all thresholds (AUC) for this model was 0.897. We used a bootstrap procedure to estimate significance and compare the model performance across different datasets (see ‘Methods’). To assess significance, we calculated the AUCs of 100 random test subsets of 500 images each, sampled from the original held-out test set. We used the standard deviation of the AUCs as the error bars (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), performed pairwise <italic>t</italic>-tests on mean AUCs, and used Bonferroni-adjusted p-values to test for significance.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Keypoint detection performance of HRNet-W48 models on different datasets.</title><p>(<bold>A</bold>) Keypoint detection performance of HRNet-W48 models measured using percent correct keypoints (PCK) values at different thresholds. Left: models trained on the full training sets of COCO, OpenApePose (OAP), and OpenMonkeyPose (OMP), and tested on the same dataset, as well as across datasets. Right<bold>:</bold> models trained on different sizes of the full OAP training set, and tested on the OAP testing set. (<bold>B</bold>) Barplots showing the keypoint detection performance of state-of-the-art (HRNet-W48) models as measured using percent keypoints correct at 0.2 (PCK@0.2) and area under the curve (AUC) of the PCK curves at thresholds ranging from 0.01 to 1. Error bars: standard deviation of the performance metrics. Models are trained on different sizes of the full training set of OAP and tested on held-out OAP test sets. (<bold>C</bold>) Same as (<bold>B</bold>) but models are trained on full training sets of COCO, OAP, and OMP, and tested on the same dataset, as well as across datasets.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-fig4-v1.tif"/></fig><p>For comparison, we used a model trained on the dataset consisting of 94,550 monkeys, split into training (56,694 images, 60%), validation (18,928 images, 20%), and testing (18,928 images, 20%) to predict apes (specifically, we used OpenMonkeyPose, <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). (Note that the original OpenMonkeyPose dataset contained some apes; for fair cross-family comparison, we are using a version of OpenMonkeyPose with the apes removed; the 94,550 number above reflects the number of monkeys alone.) The monkey dataset showed poorer performance when it comes to estimating landmarks on photos of apes. Specifically, at a threshold of 0.2, the PCK was 0.584, which is lower than the analogous value for OpenApePose (PCK@0.2 = 0.876, p-adjusted&lt;0.001). Likewise, the AUC was also substantially lower (0.743, compared to 0.897 for OpenApePose, p-adjusted&lt;0.001). In other words, for tracking apes, models trained on monkey images have some value, but they are not nearly as good as models trained on apes.</p></sec><sec id="s2-4"><title>Comparison with human pose estimation</title><p>A long-term goal of primate pose estimation datasets such as OpenApePose and OpenMonkeyPose is to achieve performance comparable to that of human pose estimation. Hence, as a further comparison, we used a previously published standard model trained on the dataset consisting of 262,465 humans (<italic>COCO</italic>) to predict apes (<xref ref-type="bibr" rid="bib28">Lin et al., 2014</xref>). This dataset showed poorer performance at predicting landmarks on apes than the model trained on the OAP dataset. Specifically, the PCK@0.2 value of 0.569 was lower than the PCK@0.2 value of 0.876 for OAP (p-adjusted&lt;0.001) and the AUC value of 0.710 was lower than the AUC value of 0.897 for OAP (p-adjusted&lt;0.001).</p><p>COCO was worse at pose estimation for apes than the OpenMonkeyPose dataset was (PCK@0.2: 0.569 vs 0.584, p-adjusted&lt;0.001; and AUC: 0.710 vs 0.743, p-adjusted&lt;0.001), despite the fact that it is a much larger dataset (262,465 vs 56,694 training images). Moreover, humans are, biologically speaking, apes, so one may expect the COCO dataset to have an advantage on ape tracking over a monkey dataset such as OMP. This does not appear to be the case. However, it is interesting to note that the COCO model predicts landmarks on apes better than it predicts landmarks on monkeys (PCK@0.2 values: 0.568 vs 0.332, p-adjusted&lt;0.001; AUC values: 0.710 vs 0.578, p-adjusted&lt;0.001). This advantage, at least, does recapitulate phylogeny.</p><p>While the OpenApePose-trained model predicted apes at an AUC value of 0.897, the OpenMonkeyPose dataset predicted monkeys at an AUC value of 0.929. These values are close, but significantly different (p-adjusted&lt;0.001). We surmise that the superior performance of OpenMonkeyPose dataset may be due to the diversity of species and to its larger size. Finally, the model based on the COCO dataset predicted human poses even better still, at an AUC value of 0.956, than either the OMP or OAP within group predictions. This advantage presumably reflects, among other things, the larger size of the dataset.</p></sec><sec id="s2-5"><title>How big does an ape tracking dataset need to be?</title><p>We next assessed the performance of our ape dataset at different sizes (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). To do so, we used a decimation procedure in which we assessed the performance of the dataset after randomly removing different numbers of images. Specifically, we subsampled our OpenApePose dataset at a range of sizes (10, 30, 50, 70, and 90% of the full training set size). Note that our subsampling procedure was randomized to balance across different species. We then tested each of the resulting models on our independent test set.</p><p>We found a gradual increase in performance with training set size. Specifically, the performance at 30% was greater than the performance at 10% (PCK@0.2: 0.747 vs 0.617 and AUC: 0.824 vs 0.755). Likewise, the performance at 50% was greater than the performance at 30% (PCK@0.2: 0.776 vs 0.747 and AUC: 0.842 vs 0.824), performance at 70% was greater than the performance at 50% (PCK@0.2: 0.878 vs 0.776 and AUC: 0.899 vs 0.842), and the performance at 90% was comparable to 70% (PCK@0.2: 0.886 vs 0.878, AUC: 0.903 vs 0.899), although it too was significantly greater (p-adjusted&lt;0.001 for all comparisons above). However, the performance at 100% was not significantly greater than the performance at 70% (PCK@0.2: 0.876 vs 0.878, and AUC: 0.897 vs 0.899, p-adjusted&gt;0.9 for both). These results suggest that performance begins to saturate at around 70% size and that increasingly larger sets may not provide additional improvement in tracking and might lead to overfitting.</p><p>Interestingly, a similar pattern is observed when tracking monkey poses. While the Convolutional Pose Machines (CPM) models trained on different sizes of the OpenMonkeyPose training sets continue to show improvements as the training set size increases (see Figure 9A in <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>), the HRNet-W48 models show similar saturation beyond 80% training set size (Figure 9B in <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>), just like we observed in the OpenApePose models (see above). (Note that, for OpenMonkeyPose, the HRNet-W48 model performed better across the board, which is why we prefer it to the CPM approach here.) This difference between the two model classes points toward the arms race between dataset size and algorithmic development as the limiting factors for performance. Ultimately, for OpenApePose, future algorithmic developments may facilitate greater performance than increasing the dataset size beyond the number we offer here.</p></sec><sec id="s2-6"><title>What is the hardest ape species to track?</title><p>Finally, we assessed the performance of the model on each species of ape separately. We regenerated the OpenApePose model six times, each time with all images of one of the six taxonomic groups removed. We then tested the models on the images of that group in the OAP test set. Note that this procedure has a second benefit, which is that it automatically ensures that any similar images (such as those collected in the same zoo enclosure or of the same individual) are excluded, and therefore reduces the chance of overfitting artifacts. (However, as we show below, doing this does not markedly reduce performance, suggesting that this type of overfitting is not a major issue in our analyses presented above.).</p><p>We include a plot with performance of the full OpenApePose model on different species, performance of the models with one species removed at a time on that species, and of the OpenMonkeyPose model without apes on each of the species (<xref ref-type="fig" rid="fig5">Figure 5A–C</xref>). Not surprisingly, we find that all the models excluding a species perform worse than the full model on the same species (<xref ref-type="fig" rid="fig5">Figure 5A and B</xref>; PCK@0.2 and AUC for <bold>bonobos:</bold> 0.871 vs 0.881 and 0.896 vs 0.903; <bold>chimpanzees:</bold> 0.754 vs 0.882 and 0.836 vs 0.902; <bold>gibbons:</bold> 0.763 vs 0.855 and 0.827 vs 0.883; <bold>gorillas:</bold> 0.869 vs 0.893 and 0.896 vs 0.908; <bold>orangutans:</bold> 0.774 vs 0.859 and 0.839 vs 0.886; <bold>siamangs:</bold> 0.797 vs 0.869 and 0.848 vs 0.889; p-adjusted&lt;0.001 for all comparisons). We also include a plot including the performance of each of these models on all different species in the supplementary materials (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We also include in the plot the performance of the OpenMonkeyPose model on the species excluded from the OpenApePose dataset. We observe that the OpenApePose model with a specific species removed still performs better on that species than the OpenMonkeyPose model (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). This result suggests that there is indeed some species-specific information in the model that aids in tracking and raises the possibility that larger sets devoted to a single species may be superior to our more general multi-species dataset. At the same time, this finding highlights a major finding of this project—that, given current models, large tailored species-specific annotated sets are superior to large multispecies sets. In other words, current models have limited capacity of generalizing across species, even within taxonomic families.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Keypoint detection performance of HRNet-W48 models tested on each species from the OpenApePose (OAP) test set and trained on (<bold>A</bold>) the full OAP training set, (<bold>B</bold>) the OAP training set with the corresponding species excluded, and (<bold>C</bold>) the full OpenMonkeyPose (OMP) dataset with apes excluded.</title><p>Left panel includes the probability of correct keypoint (PCK) values at different thresholds ranging from 0 to 1. Middle panel indicates the mean area under the PCK curve for each species. Right panel indicates the mean PCK values at a threshold of 0.2 for each species.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>The probability of correct keypoint (PCK) values (y-axis) at different thresholds ranging from 0 to 1 (x-axis) of HRNet-W48 models tested on each species from the OpenApePose (OAP) test set and trained on the OAP training set with the corresponding species excluded.</title><p>Dotted lines indicate the performance on the species excluded from training in the case of OAP and the performance of the OpenMonkeyPose model trained on monkeys on the excluded species.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Comparing the different species, we find that the species are all very close in performance (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Among these close values, the dataset missing gorillas was the most accurate, suggesting that gorillas are the least difficult to track, perhaps because their bodies are the least variable (PCK@0.2: 0.869; AUC: 0.896). Conversely, the dataset missing gibbons was the least accurate, suggesting that gibbons are the most difficult to track (PCK@0.2: 0.763; AUC: 0.827). This observation is consistent with our own intuitions at hand-annotating images— gibbons’ habit of brachiation, combined with the variety of poses they exhibit, makes guessing their landmarks particularly tricky for human annotators as well. Overall, however, all ape species were relatively well tracked even when all members of their species were excluded from the dataset.</p><p>Note that models with one ape species removed still perform better at tracking the held-out species more accurately than the OpenMonkeyPose model on that species (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>; PCK@0.2 and AUC for <bold>bonobos</bold>: 0.871 vs 0.542 and 0.896 vs 0.727; <bold>chimpanzees:</bold> 0.754 vs 0.688 and 0.836 vs 0.803; <bold>gibbons:</bold> 0.763 vs 0.587 and 0.827 vs 0.730; <bold>gorillas:</bold> 0.869 vs 0.564 and 0.896 vs 0.744; <bold>orangutans:</bold> 0.774 vs 0.529 and 0.839 vs 0.707; <bold>siamangs:</bold> 0.797 vs 0.556 and 0.848 vs 0.711; p-adjusted&lt;0.001 for all comparisons). In other words, the close phylogenetic relationship between ape species does seem to bring about benefits in tracking.</p><p>All pairwise comparisons of different subsets of datasets tested are included in <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>.</p><boxed-text id="box1"><label>Box 1.</label><caption><title>Model card for the HRNet-W48 model.</title></caption><p>Model card—OpenApePose pose estimation</p><p>Model details</p><list list-type="bullet"><list-item><p>Developed by researchers from the University of Minnesota and Emory University</p></list-item><list-item><p>High-resolution networks (HRNet-W48) trained using toolkits in MMPose v. 0.26</p></list-item></list><p>Intended use</p><list list-type="bullet"><list-item><p>Trained to demonstrate the utility of the OpenApePose dataset</p></list-item><list-item><p>May be used for ape pose tracking in images and videos</p></list-item><list-item><p>Could be used as a backbone for training action recognition models; however, as it stands, it is insufficient for action recognition</p></list-item></list><p>Factors</p><list list-type="bullet"><list-item><p>Main factor evaluated includes the species of ape</p></list-item><list-item><p>Performance varies based on the species of ape</p></list-item><list-item><p>Factors not considered include background and other environmental conditions</p></list-item></list><p>Metrics</p><list list-type="bullet"><list-item><p>PCK@0.2: the probability of correct keypoint (PCK) at a threshold of 0.2</p></list-item><list-item><p>AUC: the area under the curve of PCK thresholds ranging from 0 to 1 in 0.01 increments</p></list-item></list><p>Evaluation data</p><list list-type="bullet"><list-item><p>We use the OpenApePose test set included on the GitHub page</p></list-item><list-item><p>We sample 100 unique test sets of 500 images each from the main test set</p></list-item><list-item><p>We evaluate the models by averaging AUC and PCK@0.2 across 100 different test sets of 500 images each to perform statistical significance testing</p></list-item></list><p>Training data</p><list list-type="bullet"><list-item><p>We use the OpenApePose training and validation sets included on the GitHub page</p></list-item><list-item><p>These splits are for proof of concept and users should feel free to use their own splits from the entire dataset</p></list-item></list><p>Ethical considerations</p><list list-type="bullet"><list-item><p>None</p></list-item></list><p>Caveats and recommendations</p><list list-type="bullet"><list-item><p>The model is based on images taken mostly from zoos and sanctuaries, so images from other settings, such as lab or the wild may have varied performance</p></list-item><list-item><p>Does not fully resolve sub-species and does not include all ape species (there are many species of gibbons that could not be collected)</p></list-item></list><fig position="float" id="box1fig1"><label>Box 1—figure 1.</label><caption><title>Quantitative analyses.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86873-box1-fig1-v1.tif"/></fig></boxed-text></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The ape superfamily is an especially charismatic clade, and one that has long been fascinating to both the lay public and to scientists. Here we present a new resource, a large (71,868 images) and fully annotated (16 landmarks) database of photographs of six species of non-human apes. These photographs were collected and curated with the goal of serving as a training set for machine vision learning models, especially ones designed to track apes in videos. As such, the apes in our dataset come in a range of poses; photographs are taken from a range of angles, and our photographs have a range of backgrounds. Our database can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib13">desai-nisarg, 2023</xref>).</p><p>To test and validate our set, we made use of the HRNet architecture, specifically HRNet-W48. As opposed to architectures such as CPM (<xref ref-type="bibr" rid="bib51">Wei et al., 2016</xref>), hourglass (<xref ref-type="bibr" rid="bib38">Newell et al., 2016</xref>), simple baselines (ResNet, <xref ref-type="bibr" rid="bib53">Xiao et al., 2018</xref>), HRNet works with higher resolution feature representations that facilitate better performance. In contrast, other systems, most famously DeepLabCut, uses ResNets, EfficientNets, and MobileNets V2 as backbones. Pose estimation studies often compare a variety of these architectures to test performance, but increasingly, studies find HRNet to outperform other architectures (<xref ref-type="bibr" rid="bib55">Yu et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Li et al., 2019</xref>). (Our own past work on monkey tracking finds this as well, <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>.) Because our goal here is not to evaluate these systems, but rather to introduce our annotated database, we provide data only for the HRNet system.</p><p>With growing interest in animal detection, pose estimation, and behavior classification (<xref ref-type="bibr" rid="bib2">Bain et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Sakib and Burghardt, 2020</xref>; <xref ref-type="bibr" rid="bib42">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Mathis et al., 2021</xref>), researchers have leveraged advances in human pose estimation and have made several animal datasets publicly available. For example, there are existing datasets on tigers (n ~ 8000, <xref ref-type="bibr" rid="bib27">Li et al., 2019</xref>), cheetahs (n ~ 7500, <xref ref-type="bibr" rid="bib20">Joska et al., 2021</xref>), horses (n ~ 8000, <xref ref-type="bibr" rid="bib35">Mathis et al., 2021</xref>), dogs (n ~ 22,000, <xref ref-type="bibr" rid="bib6">Biggs et al., 2020</xref>; <xref ref-type="bibr" rid="bib22">Khosla et al., 2011</xref>), cows (n ~ 2000, <xref ref-type="bibr" rid="bib45">Russello et al., 2022</xref>), 5 domestic animals (<xref ref-type="bibr" rid="bib10">Cao et al., 2019</xref>), and 54 species of mammals (<xref ref-type="bibr" rid="bib55">Yu et al., 2021</xref>), and there are large datasets containing millions of frames of rats enabling single and multianimal 3D pose estimation and behavior tracking (<xref ref-type="bibr" rid="bib14">Dunn et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Marshall et al., 2021</xref>). Relative to these other datasets (with the exception of the rat datasets), our ape dataset is much larger (n ~ 71,000). Moreover, our dataset contains multiple closely related species and a wide range of backgrounds and poses. Another major strength of our dataset is that it contains many different types of unique individuals, which is rare as most of such datasets include only a few unique individuals.</p><p>We anticipate that the main benefit of our database will be for future researchers to develop algorithms that can perform tracking of apes in photos and videos, including videos collected in field sites. We include an example of a video clip with the inferences from our model visualized in the supplementary materials (<xref ref-type="video" rid="video1">Video 1</xref>). Relative to simpler animals like worms and mice, primates are highly complex and have a great deal more variety in their poses. As such, in the absence of better deep learning techniques, the best way to come up with generalizable models is to have large and variegated datasets for each animal type of interest. Our results here indicate that even monkeys and apes—which are in the same order and have superficially similar body shapes and movements—are sufficiently different that monkey photos do not work as well for ape pose tracking. Likewise, despite the remarkable growth of human tracking systems, these systems do not readily generalize to apes in spite of our close phylogenetic similarity to them. While there is growing interest in leveraging human-tracking systems to develop better animal-tracking systems, such systems are still in their infancy (<xref ref-type="bibr" rid="bib47">Sanakoyeu et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Yu et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Mathis et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Arnkærn et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Kleanthous et al., 2022</xref>; <xref ref-type="bibr" rid="bib5">Bethell et al., 2022</xref>). At the same time, there are better and more usable general pose estimation systems for animals, such as DeepLabCut (<xref ref-type="bibr" rid="bib33">Mathis et al., 2018</xref>), SLEAP (<xref ref-type="bibr" rid="bib44">Pereira et al., 2022</xref>), LEAP (<xref ref-type="bibr" rid="bib42">Pereira et al., 2019</xref>), and DeepPoseKit (<xref ref-type="bibr" rid="bib15">Graving et al., 2019</xref>), that allow pose estimation with small numbers (thousands) of images. These poses can be combined with downstream analysis algorithms and software tools such as MoSeq (<xref ref-type="bibr" rid="bib52">Wiltschko et al., 2020</xref>), SimBA (<xref ref-type="bibr" rid="bib39">Nilsson et al., 2020</xref>), and B-SOiD (<xref ref-type="bibr" rid="bib19">Hsu and Yttri, 2021</xref>) for behavior tracking. However, it is clear that such systems can benefit from much larger stimulus sets.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86873-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Demonstration of the OpenApePose model capabilities on inferences on videos.</title><p>The video clip is analyzed using the mmpose and mmdetection libraries—mmdetection infers a bounding box around the ape and mmpose uses the OpenApePose model to infer the pose in each frame.</p></caption></media><p>While our dataset is readily usable for training pose estimation and behavior tracking models, it has several limitations that could be addressed in the future. First, while we have attempted to include as many backgrounds, poses, and individuals as possible, our dataset is mostly dominated by images taken in captive settings at zoos and sanctuaries. This may not reflect the conditions in wild settings accurately and may result in reduced performance for applications involving tracking apes in the wild from camera trap footage, etc. Nevertheless, OpenApePose still remains the most diverse of currently available datasets. Future attempts at building such datasets should aim to include more images from the wild. Second, this dataset only enables 2D pose tracking as it does not include simultaneous multiview images that are required for 3D pose estimation (<xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Kearney et al., 2020</xref>; <xref ref-type="bibr" rid="bib14">Dunn et al., 2021</xref>; <xref ref-type="bibr" rid="bib31">Marshall et al., 2021</xref>). Building a dataset that enables 3D pose estimation and has the strengths of OAP in terms of the diversity of individuals and poses would require building multiview camera setups outside of laboratories such as the one at Minnesota zoo by <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>. Third, while many images in our dataset include multiple individuals, we only have one individual labeled in each image. This limits, but does not eliminate, our ability to track multiple individuals simultaneously. Using OpenMMlab, we have had some success tracking multiple individuals using the OAP model. However, datasets with multiple individuals simultaneously will further facilitate multianimal tracking. Lastly, our dataset does not contain high-resolution tracking of finer features, such as face, hands, etc. Indeed, many primatologists would be interested in systems that can track facial expression and fine hand movements (<xref ref-type="bibr" rid="bib17">Hobaiter and Byrne, 2014</xref>; <xref ref-type="bibr" rid="bib18">Hobaiter et al., 2021</xref>). Because we have made our image database public, it can be used as a starting point for those researchers seeking to customize to their research goals. Indeed, it may be possible to add hand and face expression annotations to our system to serve these purposes.</p><p>There are several important ethical reasons why apes cannot—and should not—serve as subjects in invasive neuroscientific experiments. That does not mean, however, that we cannot draw inferences about their psychology and cognition based on careful observation of their behavior. Indeed, analysis of behavior is an important tool in neuroscience (<xref ref-type="bibr" rid="bib40">Niv, 2021</xref>; <xref ref-type="bibr" rid="bib25">Krakauer et al., 2017</xref>). In our previous work, we have argued for the virtues of primate tracking systems to work hand in hand with invasive neuroscience techniques to improve the reliability of neuroscientific data (<xref ref-type="bibr" rid="bib16">Hayden et al., 2022</xref>). However, we have also argued that tracking has another entirely different benefit—it has the potential ability to provide data of such high quality that it can, in some cases, serve to adjudicate between hypotheses that would otherwise require brain measures (<xref ref-type="bibr" rid="bib24">Knaebe et al., 2022</xref>). For this reason, tracking data has the potential to reduce the need for non-behavior neuroscientific tools and for invasive and/or stressful recording techniques. We are optimistic that better ape tracking systems will greatly expand the utility of apes in non-invasive studies of the mind and brain. We hope that our dataset will help advance such systems.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Data collection</title><p>The OpenApePose dataset consists of 71,868 photographs of apes. We collected images between August 2021 and September 2022 from zoos, sanctuaries, and internet videos. Note that a subset of these images (16,984 images from the train, validation, and test sets combined) also appeared in the OpenMonkeyPose dataset (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). The remainder are new here. We include a datasheet for this dataset in the supplementary materials (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><sec id="s4-1-1"><title>Zoos and sanctuaries</title><p>We obtained images of apes from several zoos. These include zoos in Atlanta, Chicago, Cincinnati, Columbus, Dallas, Denver, Detroit, Erie (Pennsylvania), Fort Worth, Houston, Indianapolis, Jacksonville, Kansas City, Madison, Memphis, Miami, Milwaukee, Minneapolis, Phoenix, Sacramento, San Diego, Saint Paul, San Francisco, Seattle, and Toronto, as well as sanctuaries including the Chimpanzee Conservation Center, Project Chimps, Chimp Haven, and the Ape Initiative (Des Moines). These zoo photographs were taken either by ourselves, our lab members, or by photographers hired on temporary contracts using TaskRabbit (<ext-link ext-link-type="uri" xlink:href="https://www.taskrabbit.com/">https://www.taskrabbit.com/</ext-link>) to take pictures at these zoos. Additionally, several other independent individuals contributed images: Esmay Van Strien, Jeff Whitlock, Jennifer Williams, Jodi Carrigan, Katarzyna Krolik, Lori Ellis, Mary Pohlmann, and Max Block. All photographs were carefully screened for quality and variety of poses first by a specially trained technician and then by ND.</p></sec><sec id="s4-1-2"><title>Internet sources</title><p>We also obtained a smaller number of images from internet sources including Facebook, Instagram, and YouTube. From YouTube videos, we took screenshots of apes exhibiting diverse poses during different behaviors. Use of photographic images from these sources is protected by Fair Use Laws and has been expressly approved by the legal office at the University of Minnesota. Specifically, our use of the images satisfies four properties of principles of Fair Use. First, our usage is transformative (a crucial part of their value is in their annotations, which improve their value to scientists); second, they were published in a public forum (YouTube or on public websites); third, we are using a small percentage of the frames in the videos (at 24 fps, we are using at most 1/24 of the frames); and fourth, our usage does not reduce the market value for the images, which are, after all, freely available.</p></sec></sec><sec id="s4-2"><title>Landmark annotation</title><p>We initially obtained hundreds of thousands of images from these sources. The majority of these images (&gt;75%) did not pass our quality checks. Specifically, they were either blurry or too small or were too similar to others or showed too much occlusion. This process led to 52,946 images in total.</p><p>We used a commercial service (Hive AI) to manually annotate 16 landmarks in these images, a process similar to the one we used previously (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). We include the instructions sent to the Hive annotators in the supplementary materials (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). We use the same set of landmarks as we did in our complementary monkey dataset, with the exception of the tip of the tail (apes do not have tails). The landmarks we used are (1) nose, (2–3) left and right eye, (4) crown of the head, (5) nape of the neck, (6–7) left and right shoulder, (8–9) left and right elbow, (10–11) left and right wrist, (12) sacrum, or center point between the hips, (13–14) left and right knee, and (15–16) left and right foot. An example image illustrating these annotations is shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. We ensured that the annotations were accurate by visualizing five random samples of 100 images with the annotations overlayed on the images, for each batch of 10,000 images, resulting in a total of ~2500 inspected images. Only one of the five batches showed errors, and we sent the batch back to Hive for correction. Ape images from OpenMonkeyPose were inspected as described in <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>. We converted the annotations in a JSON format that is consistent with our previous OpenMonkeyPose dataset, and similar to other common datasets such as COCO. More details on the annotations are on the <ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">GitHub</ext-link> page (copy archived at <xref ref-type="bibr" rid="bib13">desai-nisarg, 2023</xref>).</p></sec><sec id="s4-3"><title>Dataset evaluation</title><p>To facilitate the evaluation of generalizability of the OpenApePose dataset, we split the full dataset into three sets: training (60%: 43,120 images), validation (20%: 14,374 images), and testing (20%: 14,374 images) using the<monospace> train_test_split()</monospace> function in the <monospace>scikit-learn</monospace> Python library (<xref ref-type="bibr" rid="bib41">Pedregosa et al., 2011</xref>). We did not balance our training set for the species as we wanted to utilize the full variation in the dataset and assess models trained with the proportion of species as reflected in the dataset. We provide annotations including the entire dataset to allow others to create their own training/validation/test sets that suit their needs.</p><sec id="s4-3-1"><title>Model training</title><p>To train our models, we used the pipelines and tools available in the <monospace>OpenMMlab</monospace> Python library (<xref ref-type="bibr" rid="bib11">Chen et al., 2019</xref>). <monospace>OpenMMlab</monospace> includes a wide range of libraries for computer vision applications including, but not limited to, object detection, segmentation, action recognition, pose estimation, etc. For our project, we used the <monospace>MMPose</monospace> package in <monospace>OpenMMlab</monospace> (<xref ref-type="bibr" rid="bib37">MMPose Contributors, 2020</xref>). <monospace>MMPose</monospace> supports a range of pose estimation datasets on humans as well as many other animals, and includes pretrained models from these datasets that could be tuned for specific needs. It also provides tools for training a variety of neural network architectures from scratch on existing or new datasets.</p><p>In our previous work (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>), we tested different top-down neural network architectures for training pose estimation models on our OpenMonkeyPose database (Figure 9C in <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). This included CPM, Hourglass, ResNet101, ResNet152, HRNet-W32, and HRNet-W48. We found that the best performing architecture was the deep high-resolution net, HRNet-W48 (Table 2 in <xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>). As opposed to the conventional approaches where higher resolution representations are recovered from lower resolution representations, the deep high-resolution net architecture works with higher resolution representations during the whole learning process. This results in more accurate pose representations for human pose estimation as demonstrated in the original paper (<xref ref-type="bibr" rid="bib50">Sun et al., 2019</xref>), and also for primate pose estimation, as we observed for our monkey datasets (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>; <xref ref-type="bibr" rid="bib3">Bala et al., 2020</xref>). HRNet-W48 currently remains the best performing architecture for pose estimation, and hence, for this study, we train HRNet-W48 models for comparing the performance on our proposed dataset. We trained all models for 210 epochs.</p></sec><sec id="s4-3-2"><title>Other datasets tested</title><p>We compared the performance of the HRNet-W48 model trained on our OpenApePose dataset with the performance of the pretrained HRNet-W48 model on the COCO dataset (<xref ref-type="bibr" rid="bib50">Sun et al., 2019</xref>), as well as of the HRNet-W48 model trained from scratch on our OpenMonkeyPose dataset (<xref ref-type="bibr" rid="bib54">Yao et al., 2023</xref>) with apes removed. The original OpenMonkeyPose dataset included 16,984 images of apes—10,223 in the training, 3378 in the validation, and 3383 in the testing set. Hence, for a fair comparison between HRNet-W48 models trained on monkeys vs apes, we moved these ape images from the OpenMonkeyPose dataset to the OpenApePose dataset (we provide the annotations for the OpenApePose dataset with the ape images from OpenMonkeyPose included in OpenApePose, as well as separately, to enable future replications and comparisons).</p><p>On these datasets we performed the following comparisons. First, we performed within-dataset performance comparisons. We compared the performance of OpenApePose in predicting the poses of apes to the performance of OpenMonkeyPose in predicting the poses of monkeys. Second, we compare the performance of OpenApePose in predicting apes to the performance of the current state-of-the-art human pose estimation model (HRNet-W48 model trained on COCO human keypoint dataset, 2017). Third, we assess the importance of dataset size by systematically reducing the OpenApePose training set size while keeping the proportion to the species constant. We train an HRNet-W48 model from scratch on training sets 10% (4312 images), 30% (12,936 images), 50% (21,560 images), 70% (30,184 images), and 90% (38,808 images) of the size of the full training set of 43,120 images. Lastly, to assess if the models were overfitting on the species in the OpenApePose dataset over being generalizable to non-human apes, we train six separate HRNet-W48 models from scratch each with all images from one of the six species (bonobos, chimpanzees, gibbons, gorillas, orangutans, and siamangs) excluded from the training set. We test these models on the test set images of the species excluded from the training set and compare it with the performance of the OpenMonkeyPose model on that species.</p></sec><sec id="s4-3-3"><title>Performance metrics</title><p>To evaluate the performance of our models, we used two metrics: (i) the PCK at a threshold of 0.2, and (ii) the AUC of PCK thresholds ranging from 0 to 1 in 0.01 increments.</p><p>The PCK@ε is the PCK value at a given error threshold (ε), defined as <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>16</mml:mn><mml:mi>I</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>16</mml:mn></mml:mrow></mml:munderover><mml:mi>δ</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>W</mml:mi></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mi mathvariant="bold-italic">ε</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow/></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , where <italic>I</italic> is the number of images, <italic>i</italic> indicates the <italic>i</italic>th image instance, and <italic>j</italic> indicates the <italic>j</italic>th joint, <italic>W</italic> is the width of the bounding box, and <inline-formula><mml:math id="inf2"><mml:mi>δ</mml:mi><mml:mo>(</mml:mo></mml:math></inline-formula>.) is the function that returns 1 for a true statement and 0 for a false statement. This formulation ensures that the error tolerance accounts for the size of the image via the size of the bounding box, for example, for a bounding box that is 300 pixels wide, a PCK@0.2 value considers a prediction within 300 × 0.2 = 60 pixels, to be a correct prediction.</p><p>We calculate the PCK@ε value for ε ranging from 0 to 1 with 0.01 increments. We plot the PCK@ε values for different ε (normalized distances) and calculate the AUC to estimate the performance of the HRNet-W48 models.</p></sec><sec id="s4-3-4"><title>Statistical significance testing</title><p>To perform statistical significance tests of differences in model performance for the aforementioned performance metrics, we take a bootstrap approach. We simulate 100 different test sets by randomly sampling 500 images without replacement, 100 times, from a test set of interest. We then calculate the performance metrics of PCK@0.2 and the AUC of PCK@ε vs ε; for ε∈ [0, 1]. This allows us to simulate the variation in the performance of the HRNet-W48 models across different test sets. We test the differences in performance using pairwise <italic>t</italic>-tests for different training and testing set combinations. We report the p-values adjusted for multiple comparisons using Bonferroni correction.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Software, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Datasheet for the OpenApePose dataset.</title></caption><media xlink:href="elife-86873-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Tables S1 and S2.</title><p>(a) Pairwise <italic>t</italic>-tests comparing the AUC values for different training and testing set combinations. AUCs were obtained across 100 random samples of 500 images from the test sets. p-Values adjusted for multiple comparisons using Bonferroni correction. ** not significant. (b) Pairwise <italic>t</italic>-tests comparing the PCK@0.2 values for different training and testing set combinations. PCK@0.2 values were obtained across 100 random samples of 500 images from the test sets. p-Values adjusted for multiple comparisons using Bonferroni correction. ** not significant</p></caption><media xlink:href="elife-86873-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>List of instructions sent to Hive annotators.</title></caption><media xlink:href="elife-86873-supp3-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86873-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The dataset and model are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/desai-nisarg/OpenApePose">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib13">desai-nisarg, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Desai</surname><given-names>N</given-names></name><name><surname>Bala</surname><given-names>P</given-names></name><name><surname>Richardson</surname><given-names>R</given-names></name><name><surname>Raper</surname><given-names>J</given-names></name><name><surname>Hayden</surname><given-names>B</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>OpenApePose: a database of annotated ape photographs for pose estimation</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.c59zw3rds</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the Hayden/Zimmermann lab for valuable discussions and help with taking photographs. We thank Kriti Rastogi and Muskan Ali for their help with ape image collection. We thank Estelle Reballand from Chimpanzee Conservation Center, Fred Rubio from Project Chimps, Adam Thompson from Zoo Atlanta, Reba Collins from Chimp Haven, and Amanda Epping and Jared Taglialatela from Ape Initiative for permissions to take photographs from these sanctuaries as well as contributing images for the dataset. This work was supported by NIH MH128177 (to JZ), P30 DA048742 (JZ, BH), MH125377 (BH), NSF 2024581 (JZ, BH) and a UMN AIRP award from the Digital Technologies Initiative (JZ, BH), from the Minnesota Institute of Robotics (JZ), and Emory National Primate Research Center (JR), NIH Office of the Director (P51-OD011132) (JZ, BH).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnkærn</surname><given-names>B</given-names></name><name><surname>Schoeler</surname><given-names>S</given-names></name><name><surname>Ullah</surname><given-names>M</given-names></name><name><surname>Alaya Cheikh</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep learning-based multiple animal pose estimation</article-title><source>Electronic Imaging</source><volume>34</volume><elocation-id>276</elocation-id><pub-id pub-id-type="doi">10.2352/EI.2022.34.6.IRIACV-276</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bain</surname><given-names>M</given-names></name><name><surname>Nagrani</surname><given-names>A</given-names></name><name><surname>Schofield</surname><given-names>D</given-names></name><name><surname>Berdugo</surname><given-names>S</given-names></name><name><surname>Bessa</surname><given-names>J</given-names></name><name><surname>Owen</surname><given-names>J</given-names></name><name><surname>Hockings</surname><given-names>KJ</given-names></name><name><surname>Matsuzawa</surname><given-names>T</given-names></name><name><surname>Hayashi</surname><given-names>M</given-names></name><name><surname>Biro</surname><given-names>D</given-names></name><name><surname>Carvalho</surname><given-names>S</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Automated audiovisual behavior recognition in wild primates</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabi4883</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abi4883</pub-id><pub-id pub-id-type="pmid">34767448</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bala</surname><given-names>PC</given-names></name><name><surname>Eisenreich</surname><given-names>BR</given-names></name><name><surname>Yoo</surname><given-names>SBM</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4560</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18441-5</pub-id><pub-id pub-id-type="pmid">32917899</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bala</surname><given-names>PC</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Self-Supervised Secondary Landmark Detection via 3D Representation Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2110.00543">https://arxiv.org/abs/2110.00543</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bethell</surname><given-names>EJ</given-names></name><name><surname>Khan</surname><given-names>W</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A deep transfer learning model for head pose estimation in rhesus macaques during cognitive tasks: Towards A nonrestraint noninvasive 3Rs approach</article-title><source>Applied Animal Behaviour Science</source><volume>255</volume><elocation-id>105708</elocation-id><pub-id pub-id-type="doi">10.1016/j.applanim.2022.105708</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Biggs</surname><given-names>B</given-names></name><name><surname>Boyne</surname><given-names>O</given-names></name><name><surname>Charles</surname><given-names>J</given-names></name><name><surname>Fitzgibbon</surname><given-names>A</given-names></name><name><surname>Cipolla</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Who left the dogs out? 3d animal reconstruction with expectation maximization in the loop</article-title><conf-name>In European Conference on Computer Vision</conf-name><fpage>195</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-58621-8</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bohnslav</surname><given-names>JP</given-names></name><name><surname>Wimalasena</surname><given-names>NK</given-names></name><name><surname>Clausing</surname><given-names>KJ</given-names></name><name><surname>Dai</surname><given-names>YY</given-names></name><name><surname>Yarmolinsky</surname><given-names>DA</given-names></name><name><surname>Cruz</surname><given-names>T</given-names></name><name><surname>Kashlan</surname><given-names>AD</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name><name><surname>Orefice</surname><given-names>LL</given-names></name><name><surname>Woolf</surname><given-names>CJ</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>DeepEthogram, a machine learning pipeline for supervised behavior classification from raw pixels</article-title><source>eLife</source><volume>10</volume><elocation-id>e63377</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63377</pub-id><pub-id pub-id-type="pmid">34473051</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>AJ</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Quantifying behavior to solve sensorimotor transformations: advances from worms and flies</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>90</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.08.006</pub-id><pub-id pub-id-type="pmid">28850885</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>AJ</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unsupervised identification of the internal states that shape natural behavior</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>2040</fpage><lpage>2049</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0533-x</pub-id><pub-id pub-id-type="pmid">31768056</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>H</given-names></name><name><surname>Fang</surname><given-names>HS</given-names></name><name><surname>Shen</surname><given-names>X</given-names></name><name><surname>Tai</surname><given-names>YW</given-names></name><name><surname>Lu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cross-Domain Adaptation for Animal Pose Estimation</article-title><conf-name>2019 IEEE/CVF International Conference on Computer Vision (ICCV</conf-name><fpage>9498</fpage><lpage>9507</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2019.00959</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Pang</surname><given-names>J</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Xiong</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Sun</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MMDetection: open mmlab detection toolbox and benchmark</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.07155">https://arxiv.org/abs/1906.07155</ext-link></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>ImageNet: A large-scale hierarchical image database</article-title><conf-name>2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops</conf-name><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><collab>desai-nisarg</collab></person-group><year iso-8601-date="2023">2023</year><data-title>Openapepose</data-title><version designator="swh:1:rev:5ff5a6e9b4111920aed27098c6d9bae05cada950">swh:1:rev:5ff5a6e9b4111920aed27098c6d9bae05cada950</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:df3d23085490c65c58dec87d5c1b6f1b76929baf;origin=https://github.com/desai-nisarg/OpenApePose;visit=swh:1:snp:bd02a5ac4eabfc221e99f0a61af91b68e86c4b30;anchor=swh:1:rev:5ff5a6e9b4111920aed27098c6d9bae05cada950">https://archive.softwareheritage.org/swh:1:dir:df3d23085490c65c58dec87d5c1b6f1b76929baf;origin=https://github.com/desai-nisarg/OpenApePose;visit=swh:1:snp:bd02a5ac4eabfc221e99f0a61af91b68e86c4b30;anchor=swh:1:rev:5ff5a6e9b4111920aed27098c6d9bae05cada950</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>TW</given-names></name><name><surname>Marshall</surname><given-names>JD</given-names></name><name><surname>Severson</surname><given-names>KS</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Hildebrand</surname><given-names>DGC</given-names></name><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Wang</surname><given-names>WL</given-names></name><name><surname>Gellis</surname><given-names>AJ</given-names></name><name><surname>Carlson</surname><given-names>DE</given-names></name><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Geometric deep learning enables 3D kinematic profiling across species and environments</article-title><source>Nature Methods</source><volume>18</volume><fpage>564</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01106-6</pub-id><pub-id pub-id-type="pmid">33875887</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><name><surname>Chae</surname><given-names>D</given-names></name><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Koger</surname><given-names>B</given-names></name><name><surname>Costelloe</surname><given-names>BR</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id><pub-id pub-id-type="pmid">31570119</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Automated pose estimation in primates</article-title><source>American Journal of Primatology</source><volume>84</volume><elocation-id>e23348</elocation-id><pub-id pub-id-type="doi">10.1002/ajp.23348</pub-id><pub-id pub-id-type="pmid">34855257</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The meanings of chimpanzee gestures</article-title><source>Current Biology</source><volume>24</volume><fpage>1596</fpage><lpage>1600</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.066</pub-id><pub-id pub-id-type="pmid">24998524</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Badihi</surname><given-names>G</given-names></name><name><surname>Daly</surname><given-names>GB</given-names></name><name><surname>Eleuteri</surname><given-names>V</given-names></name><name><surname>Graham</surname><given-names>KE</given-names></name><name><surname>Grund</surname><given-names>C</given-names></name><name><surname>Henderson</surname><given-names>M</given-names></name><name><surname>Rodrigues</surname><given-names>ED</given-names></name><name><surname>Safryghin</surname><given-names>A</given-names></name><name><surname>Soldati</surname><given-names>A</given-names></name><name><surname>Wiltshire</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>The great ape dictionary Video database (1.0.0)</data-title><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5600472">https://doi.org/10.5281/zenodo.5600472</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>AI</given-names></name><name><surname>Yttri</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5188</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25420-x</pub-id><pub-id pub-id-type="pmid">34465784</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Joska</surname><given-names>D</given-names></name><name><surname>Clark</surname><given-names>L</given-names></name><name><surname>Muramatsu</surname><given-names>N</given-names></name><name><surname>Jericevich</surname><given-names>R</given-names></name><name><surname>Nicolls</surname><given-names>F</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Patel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>AcinoSet: A 3D pose estimation dataset and baseline models for cheetahs in the wild</article-title><conf-name>2021 IEEE International Conference on Robotics and Automation (ICRA)</conf-name><fpage>13901</fpage><lpage>13908</lpage><pub-id pub-id-type="doi">10.1109/ICRA48506.2021.9561338</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kearney</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Parsons</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>KI</given-names></name><name><surname>Cosker</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>RGBD-Dog: Predicting Canine Pose from RGBD Sensors</article-title><conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>8336</fpage><lpage>8345</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00836</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Jayadevaprakash</surname><given-names>N</given-names></name><name><surname>Yao</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>FF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Novel dataset for fine-grained image categorization: Stanford dogs</article-title><conf-name>In Proc. CVPR workshop on fine-grained visual categorization (FGVC)</conf-name><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleanthous</surname><given-names>N</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name><name><surname>Khan</surname><given-names>W</given-names></name><name><surname>Sneddon</surname><given-names>J</given-names></name><name><surname>Liatsis</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep transfer learning in sheep activity recognition using accelerometer data</article-title><source>Expert Systems with Applications</source><volume>207</volume><elocation-id>117925</elocation-id><pub-id pub-id-type="doi">10.1016/j.eswa.2022.117925</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knaebe</surname><given-names>B</given-names></name><name><surname>Weiss</surname><given-names>CC</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The promise of behavioral tracking systems for advancing primate animal welfare</article-title><source>Animals</source><volume>12</volume><elocation-id>1648</elocation-id><pub-id pub-id-type="doi">10.3390/ani12131648</pub-id><pub-id pub-id-type="pmid">35804547</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Gomez-Marin</surname><given-names>A</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuroscience needs behavior: correcting a reductionist bias</article-title><source>Neuron</source><volume>93</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.041</pub-id><pub-id pub-id-type="pmid">28182904</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Labuguen</surname><given-names>R</given-names></name><name><surname>Matsumoto</surname><given-names>J</given-names></name><name><surname>Negrete</surname><given-names>SB</given-names></name><name><surname>Nishimaru</surname><given-names>H</given-names></name><name><surname>Nishijo</surname><given-names>H</given-names></name><name><surname>Takada</surname><given-names>M</given-names></name><name><surname>Go</surname><given-names>Y</given-names></name><name><surname>Inoue</surname><given-names>K-I</given-names></name><name><surname>Shibata</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>MacaquePose: a novel “in the wild” macaque monkey pose dataset for markerless motion capture</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>14</volume><elocation-id>581154</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.581154</pub-id><pub-id pub-id-type="pmid">33584214</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>H</given-names></name><name><surname>Qian</surname><given-names>R</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ATRW: A Benchmark for Amur Tiger Re-Identification in the Wild</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.1145/3394171.3413569</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T-Y</given-names></name><name><surname>Maire</surname><given-names>M</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Microsoft COCO: Common Objects in Context</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>740</fpage><lpage>755</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>M</given-names></name><name><surname>Qiuhan</surname><given-names>J</given-names></name><name><surname>Sturman</surname><given-names>O</given-names></name><name><surname>von Ziegler</surname><given-names>L</given-names></name><name><surname>Kollmorgen</surname><given-names>S</given-names></name><name><surname>von der Behrens</surname><given-names>W</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Bohacek</surname><given-names>J</given-names></name><name><surname>Yanik</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Deep-learning based identification, tracking, pose estimation, and behavior classification of interacting primates and mice in complex environments</article-title><source>Nature Machine Intelligence</source><volume>4</volume><fpage>331</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00477-5</pub-id><pub-id pub-id-type="pmid">35465076</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname><given-names>JC</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Schaak</surname><given-names>D</given-names></name><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Internal state dynamics shape brainwide activity and foraging behaviour</article-title><source>Nature</source><volume>577</volume><fpage>239</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1858-z</pub-id><pub-id pub-id-type="pmid">31853063</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>JD</given-names></name><name><surname>Klibaite</surname><given-names>U</given-names></name><name><surname>Gellis</surname><given-names>A</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name><name><surname>Dunn</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The PAIR-R24M Dataset for Multi-Animal 3D Pose Estimation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.23.469743</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>JD</given-names></name><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Wu</surname><given-names>JH</given-names></name><name><surname>Dunn</surname><given-names>TW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Leaving flatland: Advances in 3D behavioral measurement</article-title><source>Current Opinion in Neurobiology</source><volume>73</volume><elocation-id>102522</elocation-id><pub-id pub-id-type="doi">10.1016/j.conb.2022.02.002</pub-id><pub-id pub-id-type="pmid">35453000</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>60</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.10.008</pub-id><pub-id pub-id-type="pmid">31791006</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Biasi</surname><given-names>T</given-names></name><name><surname>Schneider</surname><given-names>S</given-names></name><name><surname>Yuksekgonul</surname><given-names>M</given-names></name><name><surname>Rogers</surname><given-names>B</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pretraining boosts out-of-domain robustness for pose estimation</article-title><conf-name>2021 IEEE Winter Conference on Applications of Computer Vision (WACV</conf-name><fpage>1859</fpage><lpage>1868</lpage><pub-id pub-id-type="doi">10.1109/WACV48630.2021.00190</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Saul</surname><given-names>N</given-names></name><name><surname>Großberger</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>UMAP: uniform manifold approximation and projection</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>861</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="software"><person-group person-group-type="author"><collab>MMPose Contributors</collab></person-group><year iso-8601-date="2020">2020</year><data-title>Mmpose</data-title><version designator="v.0.26">v.0.26</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/open-mmlab/mmpose">https://github.com/open-mmlab/mmpose</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stacked hourglass networks for human pose estimation</article-title><conf-name>European conference on computer vision</conf-name></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>SR</given-names></name><name><surname>Goodwin</surname><given-names>NL</given-names></name><name><surname>Choong</surname><given-names>JJ</given-names></name><name><surname>Hwang</surname><given-names>S</given-names></name><name><surname>Wright</surname><given-names>HR</given-names></name><name><surname>Norville</surname><given-names>ZC</given-names></name><name><surname>Tong</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Bentzley</surname><given-names>BS</given-names></name><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>McLaughlin</surname><given-names>RJ</given-names></name><name><surname>Golden</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simple Behavioral Analysis (SimBA) – an open source toolkit for computer classification of complex social behaviors in experimental animals</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.19.049452</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The primacy of behavioral research for understanding the brain</article-title><source>Behavioral Neuroscience</source><volume>135</volume><fpage>601</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1037/bne0000471</pub-id><pub-id pub-id-type="pmid">34096743</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Aldarondo</surname><given-names>DE</given-names></name><name><surname>Willmore</surname><given-names>L</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>SSH</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Quantifying behavior to understand the brain</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1537</fpage><lpage>1549</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id><pub-id pub-id-type="pmid">33169033</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Tabris</surname><given-names>N</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>DM</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Ravindranath</surname><given-names>S</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Normand</surname><given-names>E</given-names></name><name><surname>Deutsch</surname><given-names>DS</given-names></name><name><surname>Wang</surname><given-names>ZY</given-names></name><name><surname>McKenzie-Smith</surname><given-names>GC</given-names></name><name><surname>Mitelut</surname><given-names>CC</given-names></name><name><surname>Castro</surname><given-names>MD</given-names></name><name><surname>D’Uva</surname><given-names>J</given-names></name><name><surname>Kislin</surname><given-names>M</given-names></name><name><surname>Sanes</surname><given-names>DH</given-names></name><name><surname>Kocher</surname><given-names>SD</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Publisher Correction: SLEAP: A deep learning system for multi-animal pose tracking</article-title><source>Nature Methods</source><volume>19</volume><fpage>486</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01495-2</pub-id><pub-id pub-id-type="pmid">35468969</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russello</surname><given-names>H</given-names></name><name><surname>van der Tol</surname><given-names>R</given-names></name><name><surname>Kootstra</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>T-LEAP: Occlusion-robust pose estimation of walking cows using temporal information</article-title><source>Computers and Electronics in Agriculture</source><volume>192</volume><elocation-id>106559</elocation-id><pub-id pub-id-type="doi">10.1016/j.compag.2021.106559</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sakib</surname><given-names>F</given-names></name><name><surname>Burghardt</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Visual Recognition of Great Ape Behaviours in the Wild</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.10759">https://arxiv.org/abs/2011.10759</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sanakoyeu</surname><given-names>A</given-names></name><name><surname>Khalidov</surname><given-names>V</given-names></name><name><surname>McCarthy</surname><given-names>MS</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Neverova</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Transferring dense pose to proximal animal classes</article-title><conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>5233</fpage><lpage>5242</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00528</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smuts</surname><given-names>BB</given-names></name><name><surname>Cheney</surname><given-names>DL</given-names></name><name><surname>Seyfarth</surname><given-names>RM</given-names></name><name><surname>Richard</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Primate Societies</source><publisher-name>University of Chicago Press</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strier</surname><given-names>KB</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Primate Behavioral Ecology</source><publisher-name>Routledge</publisher-name><pub-id pub-id-type="doi">10.4324/9781315657127</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>K</given-names></name><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep high-resolution representation learning for human pose estimation</article-title><conf-name>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>5693</fpage><lpage>5703</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2019.00584</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>SE</given-names></name><name><surname>Ramakrishna</surname><given-names>V</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name><name><surname>Sheikh</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Convolutional Pose Machines</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.511</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>AB</given-names></name><name><surname>Tsukahara</surname><given-names>T</given-names></name><name><surname>Zeine</surname><given-names>A</given-names></name><name><surname>Anyoha</surname><given-names>R</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Peterson</surname><given-names>RE</given-names></name><name><surname>Katon</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revealing the structure of pharmacobehavioral space through motion sequencing</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1433</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00706-3</pub-id><pub-id pub-id-type="pmid">32958923</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Wei</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Simple baselines for human pose estimation and tracking</article-title><conf-name>European conference on computer vision</conf-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Y</given-names></name><name><surname>Bala</surname><given-names>P</given-names></name><name><surname>Mohan</surname><given-names>A</given-names></name><name><surname>Bliss-Moreau</surname><given-names>E</given-names></name><name><surname>Coleman</surname><given-names>K</given-names></name><name><surname>Freeman</surname><given-names>SM</given-names></name><name><surname>Machado</surname><given-names>CJ</given-names></name><name><surname>Raper</surname><given-names>J</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>OpenMonkeyChallenge: dataset and benchmark challenges for pose estimation of non-human primates</article-title><source>International Journal of Computer Vision</source><volume>131</volume><fpage>243</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1007/s11263-022-01698-2</pub-id><pub-id pub-id-type="pmid">37576929</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>W</given-names></name><name><surname>Guan</surname><given-names>Z</given-names></name><name><surname>Tao</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Ap-10k: a benchmark for animal pose estimation in the wild</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2108.12617">https://arxiv.org/abs/2108.12617</ext-link></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86873.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kalan</surname><given-names>Ammie K</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Victoria</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The OpenApePose database presented in this manuscript will be <bold>important</bold> for many applications within primatology and the behavioural sciences, and a beneficial resource for developing additional tools using computer-vision based methods. The authors have rigorously tested the utility of this database to clearly demonstrate its <bold>convincing</bold> potential, especially in relation to current alternatives. The transparent and open nature of this work will surely be beneficial to advancing automated methods for pose estimation both in captive and wild settings, and for image and video processing.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86873.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This work provides a new dataset of 71,688 images of different ape species across a variety of environmental and behavioral conditions, along with pose annotations per image. The authors demonstrate the value of their dataset by training pose estimation networks (HRNet-W48) on both their own dataset and other primate datasets (OpenMonkeyPose for monkeys, COCO for humans), ultimately showing that the model trained on their dataset had the best performance (performance measured by PCK and AUC). In addition to their ablation studies where they train pose estimation models with either specific species removed or a certain percentage of the images removed, they provide solid evidence that their large, specialized dataset is uniquely positioned to aid in the task of pose estimation for ape species.</p><p>The diversity and size of the dataset make it particularly useful, as it covers a wide range of ape species and poses, making it particularly suitable for training off the shelf pose estimation networks or for contributing to the training of a large foundational pose estimation model. In conjunction with new tools focused on extracting behavioral dynamics from pose, this dataset can be especially useful in understanding the basis of ape behaviors using pose.</p><p>Overall this work is a terrific contribution to the field, and is likely to have a significant impact on both computer vision and animal behavior.</p><p>Strengths:</p><p>- Open source dataset with excellent annotations on the format, as well as example code provided for working with it</p><p>- Properties of the dataset are mostly well described</p><p>- Comparison to pose estimation models trained on humans vs monkeys, finding that models trained on human data generalized better to apes than the ones trained on monkeys, in accordance with phylogenetic similarity. This provides evidence for an important consideration in the field: how well can we expect pose estimation models to generalize to new species when using data from closely or distantly related ones.</p><p>- Sample efficiency experiments reflect an important property of pose estimation systems, which indicates how much data would be necessary to generate similar datasets in other species, as well as how much data may be required for fine tuning these types of models (also characterized via ablation experiments where some species are left out)</p><p>- The sample efficiency experiments also reveal important insights about scaling properties of different model architectures, finding that HRNet saturates in performance improvements as a function of dataset size sooner than other architectures like CPMs (even though HRNets still perform better overall).</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86873.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors present the OpenApePose database constituting a collection of over 70000 ape images which will be important for many applications within primatology and the behavioural sciences. The authors have also rigorously tested the utility of this database in comparison to available Pose image databases for monkeys and humans to clearly demonstrate its solid potential. However, the variation in the database with regards to individuals, background, source/setting is not clearly articulated and would be beneficial information for those wishing to make use of this resource in the future.</p></body></sub-article></article>