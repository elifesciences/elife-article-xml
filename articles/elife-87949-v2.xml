<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">87949</article-id><article-id pub-id-type="doi">10.7554/eLife.87949</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87949.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Developmental Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Deep learning for rapid analysis of cell divisions in vivo during epithelial morphogenesis and repair</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Turley</surname><given-names>Jake</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8553-4367</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Chenchiah</surname><given-names>Isaac V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8618-620X</contrib-id><email>Isaac.Chenchiah@bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Martin</surname><given-names>Paul</given-names></name><email>paul.martin@bristol.ac.uk</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Liverpool</surname><given-names>Tanniemola B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4376-5604</contrib-id><email>t.liverpool@bristol.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Weavers</surname><given-names>Helen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5383-6085</contrib-id><email>helen.weavers@bristol.ac.uk</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>School of Mathematics, University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>School of Biochemistry, University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tgyzw49</institution-id><institution>Mechanobiology Institute, National University of Singapore</institution></institution-wrap><addr-line><named-content content-type="city">Singapore</named-content></addr-line><country>Singapore</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>09</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP87949</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-05-10"><day>10</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-04-30"><day>30</day><month>04</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.20.533343"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-08-10"><day>10</day><month>08</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87949.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-14"><day>14</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87949.2"/></event></pub-history><permissions><copyright-statement>© 2023, Turley et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Turley et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-87949-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-87949-figures-v2.pdf"/><abstract><p>Cell division is fundamental to all healthy tissue growth, as well as being rate-limiting in the tissue repair response to wounding and during cancer progression. However, the role that cell divisions play in tissue growth is a collective one, requiring the integration of many individual cell division events. It is particularly difficult to accurately detect and quantify multiple features of large numbers of cell divisions (including their spatio-temporal synchronicity and orientation) over extended periods of time. It would thus be advantageous to perform such analyses in an automated fashion, which can naturally be enabled using deep learning. Hence, we develop a pipeline of deep learning models that accurately identify dividing cells in time-lapse movies of epithelial tissues in vivo. Our pipeline also determines their axis of division orientation, as well as their shape changes before and after division. This strategy enables us to analyse the dynamic profile of cell divisions within the <italic>Drosophila</italic> pupal wing epithelium, both as it undergoes developmental morphogenesis and as it repairs following laser wounding. We show that the division axis is biased according to lines of tissue tension and that wounding triggers a synchronised (but not oriented) burst of cell divisions back from the leading edge.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>deep learning</kwd><kwd>wound healing</kwd><kwd>epithelial cells</kwd><kwd>modelling</kwd><kwd>development</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/R014604/1</award-id><principal-award-recipient><name><surname>Turley</surname><given-names>Jake</given-names></name><name><surname>Chenchiah</surname><given-names>Isaac V</given-names></name><name><surname>Liverpool</surname><given-names>Tanniemola B</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/T031077/1</award-id><principal-award-recipient><name><surname>Turley</surname><given-names>Jake</given-names></name><name><surname>Chenchiah</surname><given-names>Isaac V</given-names></name><name><surname>Liverpool</surname><given-names>Tanniemola B</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/217169</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Paul</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Wellcome Trust / Royal Society</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/208762</award-id><principal-award-recipient><name><surname>Weavers</surname><given-names>Helen</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/N013794/1</award-id><principal-award-recipient><name><surname>Turley</surname><given-names>Jake</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>NE/W503174/1</award-id><principal-award-recipient><name><surname>Turley</surname><given-names>Jake</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>2284082</award-id><principal-award-recipient><name><surname>Turley</surname><given-names>Jake</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Deep Learning Artificial Intelligence tools can be developed to accurately identify cell divisions during development and wound repair of epithelial tissue.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Significant advancements in confocal microscopy mean it is now possible to collect vast quantities of time-lapse imaging data from living tissues as they develop in vivo and respond to genetic or environmental perturbations (such as wounding). In parallel with the development of these imaging technologies, new methodologies are required to efficiently analyse these movies and extract detailed information about how the various cell behaviours (e.g., cell movements, divisions, etc.) contribute to tissue development and expansion, and how they enable repair responses following tissue damage (<xref ref-type="bibr" rid="bib11">Etournay et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Nestor-Bergmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Olenik et al., 2023</xref>; <xref ref-type="bibr" rid="bib38">Park et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Scarpa et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Tetley et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Turley et al., 2022</xref>).</p><p>Computer vision (a form of artificial intelligence, AI) has progressed extensively in recent years, particularly with the development of deep learning models (<xref ref-type="bibr" rid="bib19">Guo et al., 2016</xref>; <xref ref-type="bibr" rid="bib53">Voulodimos et al., 2018</xref>). Such models can be trained to identify and classify objects in images, for example, enabling automated identification of tumours in MRI (Magnetic resonance imaging) scans or segmentation of cells according to their type (<xref ref-type="bibr" rid="bib24">Işın et al., 2016</xref>; <xref ref-type="bibr" rid="bib48">Tran et al., 2018</xref>). These algorithms are particularly useful when analysing medical and biological data (<xref ref-type="bibr" rid="bib26">Jones et al., 2017</xref>), because these data are often inherently ‘noisy’, as objects within a class can exhibit significant variation.</p><p>Deep learning algorithms excel at finding patterns in complex data (<xref ref-type="bibr" rid="bib19">Guo et al., 2016</xref>). These abstract patterns are often so complicated that they are difficult for the human eye to discern (<xref ref-type="bibr" rid="bib6">Bhatt et al., 2020</xref>). In order to operate, deep learning algorithms must ‘learn’ from ‘labelled data’, that is, data in which an expert has already performed the task that we require the model to automate (e.g., segmentation, classification, etc.). Using this ‘training’ data – which includes both the input and correctly annotated output (ground truth) – the algorithm then learns how to complete the same task (<xref ref-type="bibr" rid="bib23">Howard and Gugger, 2020</xref>). The resulting algorithms can be highly accurate at performing relatively simple vision tasks and are often much quicker than when the equivalent task is performed manually (<xref ref-type="bibr" rid="bib26">Jones et al., 2017</xref>). This automated process allows efficient analyses of large datasets without extensive time-consuming repetitive work by a clinician or researcher. Furthermore, the high consistency of the resulting models reduces operator bias (or error) and can guarantee the same level of accuracy across all experiments and studies.</p><p>In microscopy, deep learning has, so far, largely been applied to static images and has enabled relatively simple analyses, such as cell counting and quantification of object area (or volume), as well as more sophisticated tasks, such as the capacity to distinguish different cell types (<xref ref-type="bibr" rid="bib26">Jones et al., 2017</xref>), and for detection of mitotic indexes in biopsy tissue sections which have notoriously poor manual reproducibility (<xref ref-type="bibr" rid="bib5">Aubreville et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Piansaddhayanaon et al., 2023</xref>). AI approaches are increasingly employed in (and beginning to revolutionise) digital pathology (<xref ref-type="bibr" rid="bib7">Burlutskiy et al., 2020</xref>; <xref ref-type="bibr" rid="bib54">Wang et al., 2022</xref>), and whilst most current applications are to two-dimensional (2D) static images, there are opportunities for deep learning models to be applied to dynamic time-lapse videos.</p><p>The biological tissue we investigate – the <italic>Drosophila</italic> pupal epithelium – is densely packed with nuclei, and the developmental cell divisions are dispersed and rapid, each occurring over a period of only several minutes. Moreover, as with most fluorescently labelled live tissue, the pupal epithelium is somewhat prone to photo-bleaching, thus limiting the frequency at which sequential images can be collected whilst maintaining tissue health. All these factors need careful consideration as we attempt to develop a fully automatised algorithm to detect and analyse the divisions with a high degree of accuracy. We found that standard methods for tracking nuclei (such as TrackMate; <xref ref-type="bibr" rid="bib47">Tinevez et al., 2017</xref>) failed to cope with the constraints of our in vivo imaging data and routinely confused epithelial cell divisions with migrating immune cells (that often contain nuclear material from engulfed apoptotic corpses), whilst also missing many mitotic events that are clear to the eye. However, whilst cell divisions in time-lapse movie data are often too dynamic for current methods for cell tracking, they do produce unique and reproducible patterns of motion. Hence, we turned to deep learning algorithms that have the power and flexibility to learn and subsequently detect these patterns.</p><p>Previous work on automated methods for detecting cell divisions has largely been performed on lower-density tissues with cells spread relatively far apart and mostly in vitro (<xref ref-type="bibr" rid="bib17">Gilad et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Kitrungrotsakul et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Nie et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Phan et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Shi et al., 2020</xref>). Whilst considerable progress has been made over the years, many of these models are not accurate enough for biologists to analyse more complex in vivo data. Some existing approaches use unsupervised methods, which have the major advantage of not needing time-consuming hand labelling of data; however, these methods may struggle to cope with highly dense tissues and currently perform worse (i.e., exhibit a lower accuracy of detection) than supervised models (<xref ref-type="bibr" rid="bib17">Gilad et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Phan et al., 2019</xref>). This is likely to be particularly true after tissue wounding, where the algorithm needs to be able to accurately distinguish potential false positives (e.g., dynamic cellular debris or immune cells) from true mitotic events. One highly effective supervised method which is performed on low-density in vitro cells involves a series of three deep learning models: The first is a Low-Rank Matrix Recovery (LRMR) model that detects regions of interest where likely mitotic events occur (<xref ref-type="bibr" rid="bib31">Mao et al., 2019</xref>). The next step involves classifying these videos to determine whether a division has occurred or not, using a Hierarchical Convolutional Neural Network (HCNN). Lastly, a Two-Stream Bidirectional Long-Short Term Memory (TS-BLSTM) model determines the time in the video that the division occurred.</p><p>Here, we propose a simpler, direct single deep learning model that can detect cell divisions with high accuracy even with challenging conditions that require it to cope with very dense and dynamic (developing) tissues as well as with wound-induced debris. Despite these more challenging experimental imaging data, our relatively simple but highly effective model can complete the tasks well enough to be used to answer biological questions. This is achieved by using much deeper networks, based on highly effective and widely used image classifying models, and by increasing input information (e.g. using two independent fluorescent channels), which we show increases model accuracy. We have also developed a related deep learning model to compute the orientation of detected cell divisions.</p><p>Having established an effective mitosis detection model, we proceed to analyse cell divisions in time and space during epithelial morphogenesis and following wounding within living tissue in vivo. As expected, in the unwounded developing pupal epithelium, we observe that cell division density decreases linearly with time (<xref ref-type="bibr" rid="bib11">Etournay et al., 2015</xref>). However, wounding triggers a synchronous burst of cell divisions at 100 min post-wounding, in a ring of epithelial tissue beginning several cell diameters back from the wound edge; this ring of proliferation becomes broader with increased wound size. In parallel, we have generated a related deep learning algorithm to determine the orientation of these cell divisions. We anticipate this deep learning algorithm could be widely applicable to the analysis of dynamic cell behaviours in a range of tissues that are amenable to study over extended time-courses, and, for such purposes, we have developed a publicly available plugin for use by others.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A deep learning strategy efficiently identifies dividing epithelial cells in time-lapse imaging data</title><p>We chose to develop, and test the capability of, our model using the epithelium of the <italic>Drosophila</italic> pupal wing because of the optical translucency and genetic tractability of <italic>Drosophila</italic>, which makes it easy to generate tissues with fluorescently labelled nuclei and cell boundaries (<xref ref-type="bibr" rid="bib11">Etournay et al., 2015</xref>; <xref ref-type="bibr" rid="bib16">George and Martin, 2022</xref>; <xref ref-type="bibr" rid="bib30">Mao et al., 2011</xref>). The <italic>Drosophila</italic> pupal wing epithelium undergoes extensive growth through rapid cell divisions early in pupal life (<xref ref-type="bibr" rid="bib4">Athilingam et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Paci and Mao, 2021</xref>), and can be imaged with high spatio-temporal resolution using live confocal microscopy. <italic>Drosophila</italic> pupae at 18 hr after puparium formation (APF) are removed from their brittle, opaque puparium to reveal the transparent pupal wing (<xref ref-type="bibr" rid="bib56">Weavers et al., 2018</xref>; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The wing epithelium is a relatively flat 2D cell sheet, composed of two opposing cell layers, each one-cell thick. To analyse the cell behaviours involved in tissue repair, we use an ablation laser to generate sterile and reproducible wounds which heal rapidly within a few hours (<xref ref-type="bibr" rid="bib55">Weavers et al., 2016</xref>). We further enhance reproducibility by localising our imaging and wounding to a particular region of the wing (<xref ref-type="fig" rid="fig1">Figure 1B–D</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Live-imaging of <italic>Drosophila</italic> epithelial tissue dynamics in vivo.</title><p>(<bold>A</bold>) Translucent <italic>Drosophila</italic> pupa with the pupal wing highlighted in magenta. (<bold>B</bold>) The pupal wing (with magnified inset, B’, on the centre zone of the wing where we consistently image) with cell boundaries labelled using E-cadherin-GFP (green) and nuclei with Histone2Av-mRFP (magenta). (<bold>C</bold>) Magnified view of the pupal wing epithelium after wounding, with the white dashed line indicating the wound edge. (<bold>D</bold>) Schematic showing a cross-section through the upper layer of epithelium of the pupal wing, with haemolymph (insect blood containing haemocytes and adipocytes) beneath and rigid cuticle above (<bold>E</bold>) Multiple cell divisions (arrows) occur in the unwounded pupal wing epithelial tissue over the course of 8 min. (<bold>F</bold>) A cell division (arrow) occurs in a wounded epithelial tissue with the white dashed line indicating the wound edge.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig1-v2.tif"/></fig><p>To gather training data to build an algorithm that can reliably detect cell divisions, we performed time-lapse imaging of unwounded and wounded pupal wings, with each movie lasting 3 hr (<xref ref-type="fig" rid="fig1">Figure 1E, F</xref>). We generated a z-stack (with z-steps of 0.75 μm in depth) that encompassed the entire epithelial cell layer at each timepoint, which we then converted to a 2D image using a stack focuser tool (<xref ref-type="bibr" rid="bib51">Umorin, 2002</xref>). For the wounded imaging data, the wounds generated possessed a mean radius of 20 μm (ranging from 9 to 30 μm), with the smallest wounds closing 20 min after wounding and the largest wounds taking up to 90 min to fully close. Crucially, tissue wounding created several imaging complications that our algorithm needed to accommodate. Firstly, wounding led to the epithelium around the wound edge moving out of the original focal plane which reduced the image quality at the immediate wound edge. This loss of image quality was further exacerbated by a wound-associated reduction in the levels of the Ecadherin-GFP junctional reporter (<xref ref-type="fig" rid="fig1">Figure 1E, F</xref>), which might be a consequence of the previously reported loosening of junctions in the migratory wound epithelium (<xref ref-type="bibr" rid="bib32">Martin and Nunan, 2015</xref>; <xref ref-type="bibr" rid="bib42">Razzell et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Tetley et al., 2019</xref>). Secondly, wounding, by definition, leads to the accumulation of local tissue debris, including bright nuclear material. Motile immune cells and fat body cells, also with Histone2Av-mRFP-positive nuclei, are recruited to the wound and both of these cell lineages engulf tissue debris (<xref ref-type="bibr" rid="bib15">Franz et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Razzell et al., 2011</xref>); these motile and phagocytic (non-epithelial) cell types can be mistaken for dividing epithelial cells providing many opportunities for ‘false positives’. Finally, since pupae are living, developing organisms, they occasionally (and unavoidably) move during imaging, leading to sample drift in-between frames, and this also leads to the generation of false positives.</p><p>To limit photo-bleaching of our biological samples, we chose to capture images every 2 min (<xref ref-type="fig" rid="fig1">Figure 1E, F</xref>), which affords the sample over 1 min of photo-recovery in between scans. Since the chromosomal separation phase of cell division (termed anaphase) takes approximately 6 min in this specific tissue, the chosen imaging frequency captures some details of each division, but is insufficient for the application of a standard (non-deep learning) algorithm. Particle tracking algorithms, which link nuclei together by the distance travelled, are also inappropriate here, since the pupal epithelial cells (and thus nuclei) are packed close together and dividing daughter nuclei would frequently (and mistakenly) be linked to a neighbouring nucleus rather than being associated with the parental cell. All these factors together make developing a highly accurate method to detect the vast number of cell divisions across our movies very challenging.</p><p>We have overcome these various image analysis constraints by generating a deep learning model to locate cell divisions in space and time from complex 2D+T imaging data (<xref ref-type="fig" rid="fig2">Figure 2</xref>; <xref ref-type="bibr" rid="bib25">Ji et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Nie et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Villars et al., 2023</xref>). We use a ResNet34 model modified into a U-Net structure (<xref ref-type="bibr" rid="bib21">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Ronneberger et al., 2015</xref>). ResNet is a widely used architecture for red, green and blue (RGB) image classification. These deep learning models, with a convolutional neural network (CNN) architecture, are constructed of ‘residual’ layers (hence the name ResNet). Residual layers are specifically used to overcome the problem of degradation in which adding more layers makes optimising a model more difficult. These layers make it possible to construct networks with hundreds of convolutional layers, which allows deeper networks to be trained and thereby increases the networks’ ability to accurately classify images (<xref ref-type="bibr" rid="bib21">He et al., 2016</xref>). However, here we not only want to know <italic>whether</italic> a division has occurred in a given time period, but also to determine its <italic>location</italic> in space – and to do this we use a U-Net structure.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Deep learning detection of cell divisions, division orientation, and cell boundaries.</title><p>Four deep learning models were developed to analyse epithelial cell behaviours. (<bold>A</bold>) The first version of the division detection model receives three frames from the Histone2Av-mRFP channel, which can be combined into a single RGB image, as is standard for a U-Net model. (<bold>B</bold>) The second version of the model input has 10 frames, 5 each from the Histone2Av-mRFP and E-cadherin-GFP channels. The model produces a white circle (white spot) wherever it detects a division. (<bold>C</bold>) The cell division locations are then passed through the U-NetOrientation model to determine the division orientation. This model takes 10 frames of a division as the input. (<bold>D</bold>) Segmentation of the focussed cell boundaries without using a deep learning model. The focussed stack image is inputted to Tissue Analyzer for segmentation and the result is compared to a hand-labelled ground truth. Green cells are correctly segmented and red cells are incorrectly segmented. (<bold>E</bold>) The three-focal plane image is inputted into the U-NetBoundary model and then segmented using Tissue Analyzer; this result is then compared to the hand-labelled ground truth.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig2-v2.tif"/></fig><p>U-Nets were developed to segment images by classifying regions into categories. These neural networks have a U-shaped structure, with an encoder side that applies CNNs and other types of layers which decrease the spatial resolution whilst increasing features. The opposite happens on the decoder arm of the U-shaped structure, with the reintroduction of spatial information via skip connections allowing for the classification of individual pixels within the image (<xref ref-type="bibr" rid="bib43">Ronneberger et al., 2015</xref>). In our system, we classify epithelial cells into ‘dividing’ or ‘non-dividing’ (the latter being the vast majority) and by their location in space. We envisioned a U-Net structured model based on a ResNet that will be able to classify far more accurately than the standard U-Net model. To boost the model’s capacity to segment time-lapse videos, we used the fast.ai libraries Dynamic U-Net class which can create a U-Net structure from an image classifier (see Materials and methods for further details of the model architecture). This final model will therefore combine the properties of both models, enabling the training of high performing deeper networks with the U-Net structure. A key benefit of this method is that deeper/newer image classifier models can be swapped for more difficult tasks or to increase performance.</p></sec><sec id="s2-2"><title>Development of Deep Learning Model 1 (U-NetCellDivision3)</title><p>Both the standard ResNet and U-Net models use three channel RGB images as input. Here, our confocal z-stack images are composed of only two channels (E-cadherin-GFP, green, and Histone2Av-mRFP, red; <xref ref-type="fig" rid="fig1">Figure 1E, F</xref>), leaving a spare channel for other potential inputs. The clearest features of a dividing cell occur as the duplicated chromosomes separate and move in opposite directions (observed in the Histone2Av-mRFP channel, arrows, <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Hence, we started developing our model by focussing only on the Histone2Av-mRFP channel, and use three sequential time-lapse images of the Histone2Av-mRFP (nuclear) channel (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), the first frame being when the cell is still in metaphase (before chromosomal separation, <italic>t</italic> = 0 min) and the second and third in anaphase (during and after chromosome separation, <italic>t</italic> = 2 min and <italic>t</italic> = 4 min, respectively). Representing these three sequential frames in different colours and combining them into a single RGB image reveals a clear pattern with broadly symmetric stripes of red (centrally) followed by green and blue (extending outwards) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Crucially, there is a dramatic contrast between this triple-coloured division pattern and that of non-dividing cells that are relatively stationary and so appear as a white/grey circular shape (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>Our deep learning model is trained to distinguish between these different RGB patterns and thus to accurately detect and locate cell divisions. To train the model, we first manually identified dividing cells in 20 independent time-lapse videos of unwounded and wounded tissue (this generates ‘labelled’ training data); each training video consisted of 93 time frames (reflecting 186 min of footage). In this training data, we detected 4206 divisions in total across all movies. Next, we generated an ‘output’ that we required the model to be able to reproduce. For this, we generated a ‘mask’ video where every division was marked with a white circle (the same size as a cell about to divide) in the same location and at the correct time. The algorithm was then trained to reproduce this ‘mask’ (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><p>We trained this deep learning algorithm which we term ‘U-NetCellDivision3’. Next, we tested the model on data it had not previously seen; the results are shown in <xref ref-type="table" rid="table1">Table 1</xref>; it should be noted that there are no experimental differences between each of the labelled datasets; they are comprised only of different biological repeats. The results (outputs) are categorised into (1) true positives (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) where a cell division has correctly been identified, (2) false positives (<inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) where the model has incorrectly detected a cell division where one has not occurred, and (3) false negatives (<inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) where a cell division occurred but the model failed to detect it. We can then compute ‘Dice score’ (<italic>F</italic>1 score) as a measure of the model’s accuracy, by combining <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib9">Carass et al., 2020</xref>). The dice score is defined as:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Dice scores for the deep learning models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Model</th><th align="left" valign="bottom">True positives</th><th align="left" valign="bottom">False positive</th><th align="left" valign="bottom">False negative</th><th align="left" valign="bottom">Dice score</th></tr></thead><tbody><tr><td align="left" valign="bottom">U-NetCellDivision3</td><td align="char" char="." valign="bottom">797</td><td align="char" char="." valign="bottom">216</td><td align="char" char="." valign="bottom">310</td><td align="char" char="." valign="bottom">0.752</td></tr><tr><td align="left" valign="bottom">U-NetCellDivision10</td><td align="char" char="." valign="bottom">1057</td><td align="char" char="." valign="bottom">28</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">0.964</td></tr></tbody></table></table-wrap><p>A dice score of 1 is a perfect score, whereas scores progressively smaller than 1 indicate a poorer algorithm performance. Dice scores for our algorithm ‘U-NetCellDivision3’ indicate that this model detects only 78.7% of all cell divisions, and it led to many false positives (<xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s2-3"><title>Development of Deep Learning Model 2 (U-NetCellDivision10)</title><p>To overcome the false positives and negatives associated with our initial model, U-NetCellDivision3, we extended the deep learning model beyond a 3-frame input to increase the number of input frames to 10 (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Here, we included an additional timepoint either side of the original 3-frame images, taking our input data to 5 timepoints in total, and extended the analysis to include both the E-cadherin-GFP and Histone2Av-mRFP channels, thus incorporating the dynamics of both cell nuclei and cell boundaries. Consequently, two of these timepoints show the cell in metaphase and three timepoints show the cell moving through anaphase into telophase and cytokinesis (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Although there should be little nuclear movement in these first two frames, including these additional metaphase images will help filter out false positives due to dynamic non-mitotic nuclei. In this algorithm, to be identified as a dividing cell, the cell nuclei will need to be stationary in metaphase for two frames (2 min) before separating in a classical anaphase-like manner. Moreover, we included the E-cadherin-GFP channel to provide additional information on cell boundaries and further enable the model to identify dividing cells. Indeed, it is well documented that cell boundaries change prior to division as cells increase their size and become rounder (<xref ref-type="bibr" rid="bib29">Lancaster and Baum, 2014</xref>), and indeed this can be observed in the pupal wing tissue (<xref ref-type="fig" rid="fig1">Figure 1B and C</xref> and <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Inclusion of the E-cadherin-GFP channel should also help rule out false positives (such as nuclear debris within the wound centre), which will lack a clear GFP-positive boundary. Inclusion of the E-cadherin channel is particularly helpful in concert with the additional fifth timepoint, as the cells can be clearly observed to separate as they move through telophase and cytokinesis. A key finding of this study is that using multiple fluorescent channels can increase information about mitotic events which, in turn, leads to higher accuracy (fewer false positives and negatives).</p><p>We subsequently trained the model (Model 2) using the same data as previously used to train Model 1. As shown in <xref ref-type="table" rid="table1">Table 1</xref>, there is now a significant (over 80%) reduction in both false positive and false negatives using the 10-frame model. Most of the errors described previously have largely been resolved; a dice score above 0.95 means we can be far more confident in the results produced by U-NetCellDivision10. <xref ref-type="video" rid="video1">Video 1</xref> shows the cell divisions that the algorithm has correctly identified; the orientations of the divisions are also revealed (see later). Now, having established a deep learning algorithm that can accurately (and quickly) identify and quantify cell divisions from our in vivo imaging data, we used the model to explore how (and where) cell divisions occur within a living, developing epithelial tissue in normal conditions, and how this changes following an experimental perturbation such as wounding (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>). We also later extended this strategy to develop additional deep learning models to study different aspects of cell behaviour (shapes of cell boundaries and identification of cell division orientation planes, <xref ref-type="fig" rid="fig2">Figure 2C–E</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Analysis of cell division density in living epithelial tissue in vivo.</title><p>(<bold>A</bold>) The density of cell divisions in the unwounded tissue, with faded blue region showing the standard deviation. The red line is the line of ‘best fit’ of the unwounded data. (<bold>B</bold>) A heatmap of the division density correlation over distance and time in unwounded epithelial tissue. Red indicates positive, and blue negative correlation. (<bold>C</bold>) The density of cell divisions in the wounded tissue, with either small or large wounds, with faded regions showing associated standard deviation. The red line is the line of best fit of the unwounded data. The micrographs show representative divisions identified at two different timepoints post-wounding. (<bold>D</bold>) Diagram of the annular bands around a wound, each 10 m wide (white dashed line); white circles indicate cell divisions. (<bold>E, F</bold>) Heatmaps of the change in division density for small and large wounds compared with a best fit linear model of unwounded data. Red areas have more divisions, and blue less, than equivalent regions in the unwounded data. The dashed lines highlight areas in which cell divisions decrease and the dotted lines highlight areas in which divisions increase compared to unwounded data. Schematics below the heatmaps in E and F show the radial division densities 100 and 110 min after wounding, respectively (<italic>n</italic> = 14 unwounded, <italic>n</italic> = 8 small wounds, and <italic>n</italic> = 9 large wounds).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Further analysis of division density in living epithelial tissue.</title><p>(<bold>A</bold>) Heatmaps of the deviation in division density of unwounded tissue compared with a best fit linear model. The axes are time and distance from a virtual wound. Red areas have more division and blue less. (<bold>B, C</bold>) Heatmaps of the division density correlation for small and large wounds. Again, red areas have more divisions and blue less. Also see <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig3-figsupp1-v2.tif"/></fig></fig-group><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Analysis of division orientation in living epithelial tissue in vivo.</title><p>(<bold>A</bold>) Distribution of the division orientations with respect to the proximal–distal axis of the pupal wing in unwounded tissue. Cell division orientations of 0° and 90° are illustrated in the micrographs. (<bold>B</bold>) Distribution of the division orientations with respect to the wing in unwounded tissue (green) and the daughter cell orientations 20 min after dividing (magenta), with examples of the orientation of division before and after cell shuffling (<bold>B’</bold>). (<bold>C</bold>) Heatmap of the space–time correlation of division orientation. Red indicates positive correlation, blue negative, and white no correlation. (<bold>D</bold>) Diagram of cell division orientation with respect to a wound; lower values are dividing towards the wound and higher values away. (<bold>E</bold>) Mean division orientation towards the wound as a function of distance from wound for small and large wounds. For unwounded tissues an arbitrary point is chosen as a ‘virtual wound’. (<bold>F, G</bold>) Distribution of the division orientations with respect to small and large wounds. The spectrum of colours (same as in D) indicates the bias in orientation towards the wound. (<bold>H, I</bold>) Distribution of the division orientations with respect to the wound in small and large wounds (green), and the daughter cell orientation 20 min after dividing (magenta) (<italic>n</italic> = 14 unwounded, <italic>n</italic> = 8 small wounds, and <italic>n</italic> = 9 large wounds).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>UNetOrientation model diagram and error of test data.</title><p>(<bold>A</bold>) Diagram of the output of U-NetOrientation. The oval is elongated in the same direction as the division, thus calculating its <italic>q</italic>-tensor tells us the orientation of the cell division. (<bold>B</bold>) The error of the U-NetOrientation model on the test dataset; black line shows median error. Also see <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Further analysis of division orientation in living epithelial tissue.</title><p>(<bold>A</bold>) Distribution of the division orientations for small wounds. (<bold>B</bold>) Distribution of the division orientations with respect to the wing in small wounds (green), and the daughter cell orientation 20 min after dividing (magenta). (<bold>C</bold>) Distribution of the division orientations for large wounds. (<bold>D</bold>) Distribution of the division orientations with respect to the wing in large wounds (green), and the daughter cell orientation 20 min after dividing (magenta). Also see <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-fig4-figsupp2-v2.tif"/></fig></fig-group><media mimetype="video" mime-subtype="mp4" xlink:href="elife-87949-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Time-lapse imaging of the unwounded pupal epithelium over 3 hr.</title><p>Projected from a 3D stack using the stack focus algorithm with a radius of 5 pixels. Green indicates E-cadherin-GFP and magenta indicates Histone2Av-mRFP. The white circles show the divisions detected by the ‘U-NetCellDivision10’ and the white lines indicate the orientation of divisions determined by ‘U-NetOrientation’. Scale bar: 10 μm. Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption></media></sec><sec id="s2-4"><title>Cell divisions within unwounded epithelial tissue in vivo exhibit a ‘community effect’</title><p>We first explored whether the ‘U-NetCellDivision10’ algorithm can be used to quantify the numbers and locations of cell divisions within the unwounded pupal wing epithelium of <italic>Drosophila</italic>. We initially used our algorithm to compute ‘division density’ over space and time, that is, the number of divisions occurring in a given area at a given time (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Interestingly, in the unwounded pupal epithelium, we observed that cells are more likely to divide close to and soon after previous divisions (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). To explore this phenomenon and determine whether cell divisions occur randomly across the unwounded tissue or whether they are more likely to occur close to other divisions, we calculated a space–time correlation for the cell divisions (see Methods for details). The space–time correlation is shown as a heatmap (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), with more intense red reflecting higher correlation. There is a high correlation close to the origin (within a 30-μm radius and temporally, within 40 min), which implies that cells are more likely to divide close to others in both space and time; this effect reduces with both increasing distance and time between cells. Consistent with previous studies of pupal wing morphogenesis (<xref ref-type="bibr" rid="bib12">Etournay et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Milan et al., 1996</xref>), we also find that the density of cell divisions decreased linearly with time during the developmental process (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p></sec><sec id="s2-5"><title>Epithelial wounding triggers a spatio-temporal reprogramming of cell division</title><p>Analysis of wounded tissues reveals striking differences in cell division behaviour when compared to unwounded tissue (compare <xref ref-type="fig" rid="fig3">Figure 3A</xref> with <xref ref-type="fig" rid="fig3">Figure 3C</xref>). These altered behaviours are highly dependent on the size of the wound. For larger wounds (15–20 μm radius), there are initially significantly fewer cell divisions (i.e., a lower division density) in the wounded epithelium compared to unwounded tissues (<xref ref-type="fig" rid="fig3">Figure 3C</xref>); this wound-associated inhibition of cell division reaches its low point at 60–70 min post-wounding. In contrast, smaller wounds (8–12 μm radius) do not exhibit a similar reduction in cell divisions immediately following wounding (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). However, both small and large wounds exhibit a subsequent and dramatic synchronised burst of cell divisions at 100 min post-wounding, double that of unwounded tissue at the peak of this proliferative surge (<xref ref-type="fig" rid="fig3">Figure 3C</xref>); after 3 hr post-wounding, the division density of wounded tissue returns to unwounded levels (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). We calculated the space–time correlation for the cell divisions in the wounded tissue and found a similar high spatial correlation around the origin with the same range as unwounded tissue (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B, C</xref>); nevertheless, the temporal correlation was altered, due to the observed suppression and later synchronisation of divisions caused by wounding.</p><p>Since our model also identifies the spatial coordinates of the cell divisions, we can determine their distance from the wound edge, and this enables us to calculate the density of divisions in zones extending out from the wound (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). To analyse how the wounded division density varies over space and time, we have compared the wounded division data to that of unwounded tissue (by making a line of best fit for the unwounded data as a linear model and comparing the wounded data to this). This enables us to show the spatial-temporal change in division density in a heatmap, with blue indicating a decrease and red an increase in division density (<xref ref-type="fig" rid="fig3">Figure 3E, F</xref>). For small wounds, there is a clear decrease in divisions extending up to 20 μm (approximately 5-cell diameters) back from the wound edge until 70 min post-wounding. In large wounds, this reduction in division density extends much further back from the wound edge, beyond even the field of view (i.e., greater than 100 μm, approximately 25-cell diameters). The subsequent synchronised burst of divisions occurs between 20 and 70 μm back from the edge of small wounds and extends beyond 100 μm across the whole field of view for large wounds (<xref ref-type="video" rid="video2">Videos 2</xref> and <xref ref-type="video" rid="video3">3</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-87949-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Time-lapse imaging of a small wound in the pupal epithelium over 3 hr.</title><p>Projected from a 3D stack using the stack focus algorithm with a radius of 5 pixels. Green indicates E-cadherin-GFP and magenta indicates Histone2Av-mRFP. The white circles show the divisions detected by the ‘U-NetCellDivision10’ and the white lines indicate the orientation of divisions determined by ‘U-NetOrientation’. Scale bar: 10 μm. Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-87949-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Time-lapse imaging of a large wound in the pupal epithelium over 3 hr.</title><p>Projected from a 3D stack using the stack focus algorithm with a radius of 5 pixels. Green indicates E-cadherin-GFP and magenta indicates Histone2Av-mRFP. The white circles show the divisions detected by the ‘U-NetCellDivision10’ and the white lines indicate the orientation of divisions determined by ‘U-NetOrientation’. Scale bar: 10 μm. Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption></media></sec><sec id="s2-6"><title>The orientation of cell divisions might be biased by tissue tension but is not influenced by wounding</title><p>In addition to a general analysis of cell division density, we can also use our models to quantify the orientation of cell divisions in an automated manner. To achieve this, we developed a second deep learning model called ‘U-NetOrientation’. Whilst our earlier model reveals the locations of dividing cells, we retrained this algorithm to report division <italic>orientation</italic> using nuclear positioning. To achieve this, we used the same model architecture as U-NetCellDivision but retrained it to complete this new task. Our new workflow first uses U-NetCellDivision10 to find cell divisions. Secondly, U-NetOrientation is applied locally to determine the division orientation. The same cell divisions from the previous training videos were used to train the U-NetCellDivision model. We initially labelled the cell division orientations by hand and then trained the new deep learning model to extract <inline-formula><mml:math id="inf7"><mml:mi>θ</mml:mi></mml:math></inline-formula>, that is, the orientation of the division (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). After training, we tested the model’s accuracy by comparing the hand-labelled orientation with the one from the model. We found that the median difference between these values was <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>45</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>; <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video3">3</xref>).</p><p>Following this model training and validation, we used the U-NetCellDivision model to quantify division orientation in unwounded and wounded epithelial tissues (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In the unwounded pupal epithelium, we measured the division orientation relative to the proximal/distal (P/D) axis of the wing (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Previous work has demonstrated that hinge contraction in the proximal part of the wing causes tension, resulting in cells becoming elongated in the wing along the P/D axis (<xref ref-type="bibr" rid="bib4">Athilingam et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Etournay et al., 2016</xref>) and because of this, we anticipated that a bias of division orientation might occur along this axis. However, surprisingly, we observe a small orientation bias at 45° to this axis (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Interestingly, our subsequent analysis revealed that daughter cells undergo later ‘shuffling’ movements to rearrange their positions after cytokinesis so that the <italic>final</italic> daughter cell orientations (using centres of the cell shapes) consistently align along with the P/D axis (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). To analyse these ‘shuffling’ rearrangements, we needed to segment and track cell boundaries. However, applying traditional tools, such as the ImageJ Tissue Analyzer plugin (<xref ref-type="bibr" rid="bib12">Etournay et al., 2016</xref>), we found that our samples were too noisy to analyse without time-consuming manual editing of the segmentation. Hence, we automated this cell boundary segmentation by developing an additional (fourth) deep learning model to detect cell boundaries (<xref ref-type="fig" rid="fig2">Figure 2D</xref>; <xref ref-type="bibr" rid="bib1">Abedalla et al., 2021</xref>; <xref ref-type="bibr" rid="bib2">Aigouy et al., 2020</xref>; <xref ref-type="bibr" rid="bib14">Fernandez-Gonzalez et al., 2022</xref>; <xref ref-type="bibr" rid="bib57">Wolny et al., 2020</xref>). Here, we developed a model using multiple focal planes (individual slices of the z-stack) from the confocal imaging data. This allowed us to take advantage of the fact that E-cadherin is visible in the top few (2–3) z-slices of cells. Using this 3D data gives a clear signal in this otherwise noisy wounded tissue data (<xref ref-type="bibr" rid="bib57">Wolny et al., 2020</xref>). We therefore used a three-focal plane input to increase the amount of information available to the model, which we have called U-NetBoundary (<xref ref-type="fig" rid="fig2">Figure 2D, E</xref>). This utilised an algorithm which identifies the most focussed plane, and the planes immediately above and below it (see Materials and methods for further details), to provide sufficiently accurate automated identification of cell boundaries. After training, we tested the U-NetBoundary model on 12 images (12,514 cells) and ran the output through ImageJ’s Tissue Analyzer (<xref ref-type="bibr" rid="bib12">Etournay et al., 2016</xref>) to utilise the Watershed algorithm. <xref ref-type="table" rid="table2">Table 2</xref> shows that using U-NetBoundary leads to a much better dice score and so is more reliable than using a single focal plane without deep learning.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Dice scores for the segmentation methods.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Segmentation</th><th align="left" valign="top">True positives</th><th align="left" valign="top">False positive</th><th align="left" valign="top">False negative</th><th align="left" valign="top">Dice score</th></tr></thead><tbody><tr><td align="left" valign="top">Single focal plane + Tissue Analyzer</td><td align="char" char="." valign="top">8197</td><td align="char" char="." valign="top">313</td><td align="char" char="." valign="top">4317</td><td align="char" char="." valign="top">0.780</td></tr><tr><td align="left" valign="top">U-NetBoundary + Tissue Analyzer</td><td align="char" char="." valign="top">11,325</td><td align="char" char="." valign="top">501</td><td align="char" char="." valign="top">1189</td><td align="char" char="." valign="top">0.931</td></tr></tbody></table></table-wrap><p>Using U-NetBoundary, we are now able to track daughter cells post-division, with the required level of accuracy. Our algorithm automatically filters out any tracks that have large and/or sudden changes in size and shape of daughter cells, which indicates a likely segmentation mistake (see Methods for details). Once these anomalies have been identified and removed, our data is ready for analysis. To determine whether daughter cell orientation relative to one another changed during cell shuffling (in the period immediately after dividing), we measured the angle of a line drawn between the centres of two daughter nuclei 20 min after cell division (<xref ref-type="fig" rid="fig4">Figure 4B’</xref>) to find the change in orientation. We found that, on average, post-division shuffling shifted final daughter cell orientations by 14.8°. When we measured the post-shuffling orientation relative to the wing’s P/D axis, we found that the distribution had shifted to acquire a small bias in the direction of the tension in the tissue (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), and the mean orientation relative to the P/D axis had shifted by 8.5° to align with the global tension. If cell division orientation is also influenced by local tension in a developing tissue, then one might expect that cells about to divide in close proximity to one another will experience similar local forces and so might divide with the same orientation. To examine if this was the case, we measured the correlation between division orientation within space and time, but found no such correlation (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Therefore, we conclude that (1) global tension has a small influence on division orientation, but only after shuffling (repositioning) of the daughter cells and (2) local tension from the wound does not have a detectable effect. Other predictors of division orientation, such as cell shape, could dominate (<xref ref-type="bibr" rid="bib34">Nestor-Bergmann et al., 2019</xref>).</p><p>Next, we measured division orientation relative to a wound (<xref ref-type="fig" rid="fig4">Figure 4D–G</xref>). Here, the possible range of cell division orientations varies from 0° to 90°, with an orientation of 0° indicating that cells divide towards a wound (radially), and an orientation of 90° indicating that cells divide perpendicular to a wound (tangentially). To investigate whether cell division is biased towards the wound, we averaged the orientation of all divisions around the wound. If divisions are biased towards a wound, then their average orientation should be significantly less than 45° (or above 45° if significantly biased away from the wound); conversely, an average bias of 45° would suggest that cells divide in an unbiased manner. From the uniform distribution of division orientations, our data suggest that a rather unbiased orientation of cell divisions occurs in response to wounding (<xref ref-type="fig" rid="fig4">Figure 4D–G</xref>, <xref ref-type="video" rid="video4">Videos 4</xref> and <xref ref-type="video" rid="video5">5</xref>). Whilst these data suggest that there is no initial bias in the orientation of cell divisions in the epithelium following wounding, we wondered whether subsequent ‘shuffling’ of daughter cells might be influenced by tissue tensions within the repairing epithelium. We undertook the same tracking of daughter cell movements as described for unwounded tissue (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), but observed no significant shift in the cell orientations post-shuffling; rather, the distribution of post-division orientations is the same as for the divisions themselves (<xref ref-type="fig" rid="fig4">Figure 4H, I</xref>), suggesting that the local tension changes triggered by wound healing are not sufficient to have a measurable effect on the orientation of cell divisions, over and above those seen in unwounded tissue (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-87949-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Time-lapse imaging of a small wound in the pupal epithelium over 3 hr.</title><p>Projected from a 3D stack using the stack focus algorithm with a radius of 5 pixels. Greyscale background of epithelium with circles show the divisions detected by the ‘U-NetCellDivision10’, the lines indicate the orientation of divisions determined by ‘U-NetOrientation’ and the colour of labels display the orientations relative to wounds. Blue labelled divisions are orientated towards wounds, red away from wounds and white around 45°. The white dot is the centre of the wound and the closed wound site after closure. Scale bar: 10 μm. Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-87949-video5.mp4" id="video5"><label>Video 5.</label><caption><title>Time-lapse imaging of a large wound in the pupal epithelium over 3 hr.</title><p>Projected from a 3D stack using the stack focus algorithm with a radius of 5 pixels. Greyscale background of epithelium with circles show the divisions detected by the ‘U-NetCellDivision10’, the lines indicate the orientation of divisions determined by ‘U-NetOrientation’ and the colour of labels display the orientations relative to wounds. Blue labelled divisions are orientated towards wounds, red away from wounds and white around 45°. The white dot is the centre of the wound and the closed wound site after closure. Scale bar: 20 μm. Related to <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p></caption></media></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Deep learning is well suited to detecting variable but specific features in a dense field, such as face detection in a crowd. Hence, it is particularly useful for identifying patterns in noisy biological or clinical data. A key step with these data is identifying inputs for a given task, and this will be somewhat bespoke and dependent on the type of data being analysed. Here, we analyse confocal microscopy movies of translucent three-dimensional epithelial tissue, to identify and classify cellular behaviours in space and time during morphogenesis and tissue repair. To this end, we have developed deep learning tools for identifying and locating when and where cell divisions occur, as well as their division orientation and the post-division shuffling behaviour of daughter cells in unwounded and wounded tissue. This has allowed us to ask quite nuanced questions about cell division behaviours across an epithelial field, as well as investigate how an individual cell division might influence local cell behaviours by its close neighbours.</p><p>For such dynamic cell behaviours as cell division, there is a clear need to analyse imaging data from high-resolution confocal microscopy movies of living tissue. Because of the vast volume of this data, doing this task manually would not be possible, and so one must develop sophisticated deep learning strategies for the analysis. Our approach has been to generalise techniques currently used in computer vision for static images and adapt them to deal with dynamic data. Previous deep learning approaches have considerably improved the detection accuracy of mitotic indexes in static biopsy sections of clinical tissues for cancer diagnosis (<xref ref-type="bibr" rid="bib5">Aubreville et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Piansaddhayanaon et al., 2023</xref>). We have built on other existing 3D CNN networks (<xref ref-type="bibr" rid="bib25">Ji et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Nie et al., 2016</xref>) by making deeper models which can receive multiple florescent channels. In our study, we successfully applied this type of analysis to very dense in vivo tissues which are undergoing the highly dynamic events involved in tissue development and repair following wounding. Despite these additional difficulties, our model proved to be highly accurate. Furthermore, we could also determine the orientation of cell divisions.</p><sec id="s3-1"><title>What are the biological findings so far?</title><p>Our deep learning tools have enabled us to accurately quantify complex cell behaviours – such as cell divisions and subsequent daughter cell rearrangements – from large datasets which, in turn, has revealed trends that are otherwise hidden within the biology. Previous studies of wound healing in mammalian models have suggested that cell migration and cell division largely occur in separate epidermal domains following wounding (<xref ref-type="bibr" rid="bib3">Aragona et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Park et al., 2017</xref>) and our data support this. Our large wounds show a clear reduction in cell divisions, below pre-wound levels, in cells close to the leading epidermal wound edge where cells are actively migrating. Nevertheless, our data suggest that cell migration is not absolutely dependent on this ‘shut down’ in divisions because we see no observable cessation of cell division around small wounds as they are closing. For both large and small wounds, we observe a synchronised proliferative surge of cell divisions commencing 60 min post-wounding (and peaking shortly afterwards), but this is restricted to a domain beginning about 5-cell diameters back from the leading edge. These divisions are unlikely to be a major driver of wound closure because the rate of wound closure is the same before and after the proliferative surge. Indeed, cell divisions at the leading edge have largely halted during the most dramatic closure period. However, these cell divisions are likely to be a consequence of wounding, and the additional cells will help repopulate the tissue to restore near original numbers of epithelial cells and return tissue structure (and tensions) to pre-wound levels. This synchronised surge in cell proliferation in a band of cells back from the leading edge (to levels that are twice background levels for unwounded tissue) is potentially related to our observation of a strong correlation in the timing of cell divisions by close neighbours in unwounded epithelial tissue. Such a ‘community effect’ might be mediated by short-range chemical signals or local mechanical signals that operate locally in unwounded tissues and are recapitulated and expanded following wounding.</p><p>Once a cell has received signals driving it to divide, how do tissue tensions influence the orientation of this cell division in the unwounded or wounded epithelium? Previous studies of cells adjacent to the segmental boundaries in the <italic>Drosophila</italic> embryo show how local tissue tensions, driven by contractile actomyosin cables, can orient the plane of cell divisions adjacent to these boundaries (<xref ref-type="bibr" rid="bib44">Scarpa et al., 2018</xref>). Moreover, analyses of experimentally stretched Xenopus tissue revealed that whilst global cell division rates are regulated by tissue-level mechanical stress, division orientation is controlled more locally by cell shape (<xref ref-type="bibr" rid="bib34">Nestor-Bergmann et al., 2019</xref>). In our studies, we observe cells dividing with no specific orientation bias along the global P/D axis; however, subsequently, we do see the resulting daughter cells shuffle to adopt an alignment more biased along this P/D tension axis. We see no apparent bias in orientation of cell divisions following wounding; this was unexpected as one might presume there to be considerable tissue tension changes in the vicinity of a wound (<xref ref-type="bibr" rid="bib20">Guzmán-Herrera and Mao, 2020</xref>; <xref ref-type="bibr" rid="bib44">Scarpa et al., 2018</xref>). However, this effect might be partially explained by our observation that most cell divisions are distant from the main source of changing wound tensions, the contractile actomyosin purse-string that rapidly assembles in the leading epithelial wound edge cells (<xref ref-type="bibr" rid="bib46">Tetley et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">Wood et al., 2002</xref>), and that these divisions occur largely after the wound has closed.</p><p>To further extend these studies and to gain a more comprehensive understanding of how different cell behaviours, particularly beyond those directly related to cell division, coordinate in a repairing tissue, additional development of our deep learning algorithms might be useful to extract more information from the time-lapse imaging data. For example, this might enable us to correlate changes in the density or orientation of cell divisions at the wound site, with other contributing cell behaviours (such as cell shape changes and cell intercalations). Similarly, it would be possible to integrate our analyses of cell behaviour with tools that enable live-imaging of wound-induced signalling (e.g., calcium signalling at the wound edge using calcium sensitive GCaMP transgenic reporters), in order to determine how such signalling pathways might be integrating the various wound repair cell behaviours following injury.</p></sec><sec id="s3-2"><title>Future directions for our deep learning approaches</title><p>In this study, we have converted a suite of image classifiers (ResNets) into U-Net via the Dynamic UNET function from the fast.ai library. To analyse cell divisions in <italic>Drosophila</italic> pupal tissues we extended the dimension of the data being analysed to include multiple time steps to identify the dynamic features associated with individual cell division events. To achieve this, we have modified the architecture of these models by increasing the feature inputs in the first layer. With tweaks, the model can provide us with additional, but related, outputs, for example, detection of defective cell divisions, which might be relevant in studies of oogenesis or cancer. Our algorithms could also be extended further by altering the initial layers of the model; this would enable the generation of models which can identify much more complex dynamical features. Indeed, a major challenge is to generate AI (or deep learning) models that can be adapted to identify cellular (or pathological) features across a broad range of tissue types and in data generated through a range of different imaging modalities. The tissue employed in our current study was a relatively flat 3D epithelium with few other cell lineages present in the microscopy data (only migratory immune cells), but such AI approaches could be expanded to cater for mixed populations of cells existing in larger 3D volumes, for example, gastruloids or even whole embryos as they develop and undergo morphogenesis, or to study other complex cell:cell interactions or movements, for example, immune cell interactions or flagella beating. Incorporating LSTM architectures could also help detect these dynamic and complex behaviours (<xref ref-type="bibr" rid="bib30">Mao et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Shi et al., 2020</xref>). With any such methodology, there will be much interesting work to come, in optimising movie time resolution, fluorescent markers and model depth.</p><p>The development of the next generation of <italic>realistic</italic> theoretical models of tissue mechanics during morphogenesis and repair (and other physiological episodes such as cancer progression) in vivo, will require dealing with increasingly large and complex imaging datasets. To extract information from them will require the use of further deep learning tools to automate the process of data extraction (of, e.g., cell velocities, cell shapes, and cell divisions). The theories that must be developed will be founded on non-equilibrium statistical mechanics applied to describing stochastic equations of motion for many microscopic interacting degrees of freedom. Identifying the most important features of the dynamics and quantifying the fluctuations will be highly challenging. We envision promising approaches will include (1) inferring equations of motion based on optimising the parameters of Partial Differential Equations (PDEs) for continuum fields (e.g., nematic director fields for cell orientations) using deep learning (<xref ref-type="bibr" rid="bib10">Colen et al., 2021</xref>) or (2) reverse engineering the dynamics based on spatio-temporal correlation functions (of, e.g., cell shapes and cell velocities) that deep learning tools can elucidate. An advantage of the second approach is that one can also estimate the scale of fluctuations in the system.</p><p>The deep learning models that we present here can identify cell divisions and their orientations (as well as subsequent orientations of daughter cells) in dynamic movie data with high accuracy. We anticipate our models will have broad application and enable a similar analysis of tissues where cell nuclei and/or membranes have been labelled. To facilitate this, we have made a napari plugin that can run our model to detect cell divisions, which can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/turleyjm/cell-division-dl-plugin">https://github.com/turleyjm/cell-division-dl-plugin</ext-link> (copy archived at <xref ref-type="bibr" rid="bib50">Turley, 2024</xref>). All the data used to train/test the models plus the additional data used in the analysis cell divisions in wounded tissues can be found on our Zenodo dataset <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10846684">https://zenodo.org/records/10846684</ext-link>. Other researchers may wish to use different numbers of timepoints and/or fluorescent channels, leading to modifications in the number of frames inputted into the model; we provide clear instructions on how to do this in the code. Ultimately, we envisage that such deep learning approaches are an important step towards the development of AI tools for analysing dynamic cell behaviours, including cell divisions, in complex physiological as well as pathological processes occurring in a variety of organisms and tissue types.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title><italic>Drosophila</italic> stocks and husbandry</title><p><italic>Drosophila</italic> stocks were raised and maintained on Iberian food according to standard protocols (<xref ref-type="bibr" rid="bib18">Greenspan, 1997</xref>). All crosses were performed at 25°C. The following <italic>Drosophila</italic> stocks were used: <italic>E-cadherin-GFP</italic> and <italic>Histone2Av-mRFP</italic> (BDSC stock numbers #60584 and #23651, respectively, obtained from the Bloomington <italic>Drosophila</italic> Stock Centre, Indiana).</p></sec><sec id="s4-2"><title>Confocal imaging and data processing</title><p><italic>Drosophila</italic> pupae were aged to 18 hr APF at 25°C. Dissection, imaging, and wounding were all performed as previously described (<xref ref-type="bibr" rid="bib56">Weavers et al., 2018</xref>). The time-lapse movies were generated using a SP8 Leica confocal. Each z-stack slice consisted of a 123.26 × 123.26 μm image (512 × 512 pixels) with a slice taken every 0.75 μm. The z-stacks were converted to 2D using our own Python version of the ImageJ plugin stack focuser (<xref ref-type="bibr" rid="bib51">Umorin, 2002</xref>). Images were taken every 2 min for 93 timepoints (just over 3 hr of imaging). The data were manually labelled by making a database of the locations in space and time of the divisions and their orientations.</p><p>From the 93-frame full videos, we extracted 5 sequential timepoints to make a video clip (10 frames encompassing 2 different channels) or 3 timepoints and frames (for the U-NetCellDivision3). This was performed 89 times (one for each timepoint, apart from the last four timepoints). Our training data thus consisted of 979 video clips (11 full videos), our validation data consisted of 356 video clips (4 full videos) and our testing data consisted of 445 video clips (5 full videos). There are 4206 divisions across all the videos (on average, there are 2.38 divisions in each clip). There is no experimental difference between each of the labelled datasets as they are comprised of different biological repeats. Training data is used to directly teach the model to perform its given task. Validation data is used during the training process to determine whether the model is overfitting. If the model is performing well on the training data but not on the validating data, then this is a key signal that the model is overfitting and changes will need to be made to the network/training method to prevent this. The testing data is used after all the training has been completed and is used to test the performance of the model on fresh data it has not been trained on. We define the time a division occurred as the last timepoint of metaphase before anaphase starts. For each clip, we make a corresponding output mask (also 512 × 512 pixels) with divisions labelled with a white circle. This is generated using our hand-labelled database, which has the information about each division’s location in space and time. These video clips (plus their corresponding output masks) are the labelled data that will be used for training the U-NetCellDivision deep learning models.</p><p>For calculating the orientation of cell divisions, we used the 10-frame video clips. For each division, we made a cropped video clip that was a 14.4 × 14.4 μm square box around the centre of a division. The images in the cropped video were 60 × 60 pixels (which we rescaled to 120 × 120 as this improved the performance of the models). The same training dataset that was used for training the U-NetCellDivision models wasz used for U-NetOrientation, with 2638 cropped video clips from the 11 full videos. The validation data was 660 cropped video clips from 4 full videos, and testing had 1135 cropped clips from 5 full videos. The output for the model is an oval elongated in the same orientation as the division. The oval has a radius of 50 pixels in the long axis and 15 in the short axis. In the labelled data, each division’s orientation was measured by hand and the corresponding oval mask was generated. The mask is also 120 × 120 pixels.</p><p>For detecting cell boundaries, we maximise the information supplied to the model by using a modified stack focuser which identifies the most ‘in focus’ pixels in a stack. Our version also outputs the pixels above and below the most in-focus pixel and records this as an RGB image with colours corresponding to above (R), focussed (G), and below (B) pixels; the model will learn to use these upper and lower colours to identify if there is a genuine cell boundary or if focussed pixels are just noise within the image. We also rescaled our images from 512 × 512 pixels to 1024 × 1024 pixels, to increase the width of the boundaries so that they are large enough for the model to learn to detect them. The data was segmented using Tissue Analyzer to apply the Watershed algorithm on the original single focal plane data (then correcting by hand the boundaries on 59 images, finding a total of 58,582 cells). The boundaries are 1 pixel in width in the output labelled masks. To give the model a wider target to reproduce, we eroded the image to make the boundaries 3 pixels wide. As we have increased the scale of the images, this is around the same pixel thickness as the boundaries in the input.</p></sec><sec id="s4-3"><title>Network architecture</title><p>We converted a Resnet34 model into a U-Net architecture via the Dynamic UNET function from the fast.ai library (<xref ref-type="bibr" rid="bib21">He et al., 2016</xref>; <xref ref-type="bibr" rid="bib22">Howard, 2018</xref>; <xref ref-type="bibr" rid="bib43">Ronneberger et al., 2015</xref>). The weights from the Resnet 34 classifier were used to take advantage of transfer learning (<xref ref-type="bibr" rid="bib23">Howard and Gugger, 2020</xref>). For the second version of the model (U-NetCellDivision10), the first layer of the model was replaced with a Conv2d layer with 10 features in and 64 out. The inputs to the model were 512 × 512 × 3 or 10 × 512 × 512 voxels for U-NetCellDivision3 or U-NetCellDivision10, respectively. U-NetCellDivision3 has 41405589 parameters and both U-NetCellDivision10 and U-NetOrientation have 41,268,871, all have 54 layers. The U-NetOrientation has the same architecture as U-NetCellDivision10, but takes 10 × 120 × 120 videos as inputs. For U-NetBoundary we used the Resnet 101 classifier and converted it into a U-Net with Dynamic UNET function. U-NetBoundary has 318,616,725 parameters and has 121 layers. This model has inputs of 1024 × 1024 × 3. Source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/turleyjm/cell-division-dl-plugin">https://github.com/turleyjm/cell-division-dl-plugin</ext-link> (copy archived at <xref ref-type="bibr" rid="bib50">Turley, 2024</xref>).</p></sec><sec id="s4-4"><title>Data augmentation</title><p>The data were augmented using the albumentations library (<xref ref-type="bibr" rid="bib8">Buslaev et al., 2020</xref>). The transforms used were Rotate, HorizontalFlip, VerticalFlip, GaussianBlur, RandomBrightnessContrast, and Sharpen.</p></sec><sec id="s4-5"><title>Training models</title><p>Training our deep learning models requires that we split the data into three separate groups (<xref ref-type="bibr" rid="bib23">Howard and Gugger, 2020</xref>): (1) Training data: this is data from which the model directly learns (in this instance, the 11 videos described above); (2) Validation data: this data is used to test the model during the training process, to validate whether the algorithm is learning the patterns correctly and if it can perform on (similar but) unfamiliar videos. This ensures that the model has not simply ‘remembered’ the ‘answer’ in the training data (over-fitting); (3) Testing data: once we have fully trained the model, we run a final dataset through the model as our ultimate test of the algorithm (see <xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref>). Paperspace’s gradient ML Platform was used for training the models. The machine used was one with NVIDIA Quadra P5000 or P6000 GPU. We trained using an Adam optimisation.</p></sec><sec id="s4-6"><title>Detecting divisions from U-NetCellDivision outputs</title><p>After running a full video through our model in individual video clips, we have output masks with white circles in the same locations as the divisions (see <xref ref-type="fig" rid="fig2">Figure 2A, B</xref>). We detect the white circles using a Laplacian of Gaussian Filter (<xref ref-type="bibr" rid="bib28">Kong et al., 2013</xref>). The deep learning model is very accurate at finding divisions when they occur, but sometimes mistakenly detects them a frame before and/or after the actual division happens. This may be expected as the video clips still look similar after being shifted by one timepoint. The white circles in the frames before and after are normally not as intense as the timepoint of the division, reflecting the weaker confidence of the model in identifying them. To ensure we do not double count divisions, these are suppressed with the brightest circle taken as the timepoint when a cell divides. We have built in some tolerance into our evaluation of the model. When the algorithm detects one of these divisions and has a brighter spot in a timepoint ±1 frame of our labelled data, we still count this as a correctly detected division.</p></sec><sec id="s4-7"><title>Orientation from U-NetOrientation outputs</title><p>To determine the orientation of the oval shape produced by the U-NetOrientation deep learning model, we calculated a second-rank tensor (which we call the <italic>q</italic>-tensor) for the output image that stores information about the orientation of the oval shapes.<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf9"><mml:mi>A</mml:mi></mml:math></inline-formula> is the area of the image and <inline-formula><mml:math id="inf10"><mml:mi>d</mml:mi><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:math></inline-formula>. <inline-formula><mml:math id="inf11"><mml:mi mathvariant="bold-italic">q</mml:mi></mml:math></inline-formula> can be rewritten as<disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">q</mml:mi><mml:mo>=</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf12"><mml:mi>θ</mml:mi></mml:math></inline-formula> is the orientation of the shape. To calculate the orientation of a division, we apply these equations to our output image from U-NetOrientation and extract <inline-formula><mml:math id="inf13"><mml:mi>θ</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Using Tissue Analzyer for segmentation</title><p>We use the watershed algorithm from Tissue Analzyer (<xref ref-type="bibr" rid="bib12">Etournay et al., 2016</xref>), which is a plugin for ImageJ for segmentation both from the single focal plane data and from the output of the deep learning U-NetBoundary model. The ‘Detect bonds V3’ function was used to perform the segmentation. We found individual optimised settings for both single focal plane and U-NetBoundary output images. These were not the same settings, as the images are very different. To track cells after segmentation, we used the ‘Track cell (static tissue)’ algorithm. The U-NetBoundary outputs are resized back to 512 × 512 before being run through Tissue Analyzer.</p></sec><sec id="s4-9"><title>How to adapt this method for other cell division datasets</title><p>The models we have developed (optimised for <italic>Drosophila</italic> pupal wing epithelia) can be used on datasets from other systems. To be effective on a new tissue type, retraining will typically be needed. In our GitHub repository, we include the scripts for training new models (<ext-link ext-link-type="uri" xlink:href="https://github.com/turleyjm/cell-division-dl-plugin">https://github.com/turleyjm/cell-division-dl-plugin</ext-link>; <xref ref-type="bibr" rid="bib50">Turley, 2024</xref>). For best results, the user should load our model and weights, then train the model from this starting point (called transfer learning; <xref ref-type="bibr" rid="bib23">Howard and Gugger, 2020</xref>). The user will also need to generate labelled data (as done in the ‘Imaging and data processing’ section). Once this has been done, the user can utilise the training scripts to teach the model. Other researchers may wish to vary the number of input channels to use longer/shorter videos or different numbers of fluorescent channels. This can easily be changed in the code for the model, with comments on the GitHub repository highlighting where alterations need to be made. Additionally, the image classifier model, which is currently converted to a U-Net (currently ResNet34), can also be replaced by a different classification network. This allows for different models to be incorporated.</p></sec><sec id="s4-10"><title>Wound, division density, and orientation measurements</title><p>The epithelial wound was located using the 2D focussed E-cadherin channel. The ImageJ plugin Trainable Weka Segmentation (a machine learning algorithm) is trained to find areas of the images that are tissue or non-tissue. Non-tissue could be either a wound or parts of the tissue that are above or below our frame of reference. The tissue/non-tissue binary masks are then hand-edited to remove errors (mostly around the edges of wounds where the images are particularly noisy due to debris). To calculate the division density, we sum the number of cell divisions divided by the area of tissue during a defined time period. We find the number of divisions from our deep learning model, and using the tissue/non-tissue binary masks, we know the area of tissue observed in the video. For measuring division density in relation to a wound, the mask could then be used to extract the wound. We then calculated the distance from the edge of a wound to the divisions using a distance transform (<xref ref-type="bibr" rid="bib13">Fabbri and Da, 2008</xref>). Now we can find all the divisions in a band of a given radius and width. To quantify the density of divisions, we divide the number of divisions by the area of the band. Using both the distance transform and our tissue mask, we can work out the area of the tissue that is in each band. Once the wound has closed, we can no longer perform a distance transform using the wound edge, so we instead take the centre of the last timepoint before the wound closes. This point is the wound site and is where we take our distance transform from. As the tissue is still developing and moving, we track this point over time.</p><p>We track the tissue using the ImageJ plugin TrackMate (<xref ref-type="bibr" rid="bib47">Tinevez et al., 2017</xref>), which tracks the nuclei of cells as they move together in the tissue. Unlike the mitotic nuclei, these nuclei are slow moving, so trackable using a non-AI algorithm. By calculating the average velocity of the cells around the wound site, we can track this point and use this as our frame of reference to measure the distance from the wound site. The same method is used for unwounded tissue where we chose an arbitrary point as a ‘virtual wound’, which will flow with the local tissue. The starting point for the unwounded tissue is the centre of the image. This gives us our reference point to identify the bands we use for calculating the division density. We measure the orientation of division relative to the centroid of the polygon approximating the boundary of a wound (which we call the wound centre). The difference in angle between the vector from the wound centre to divisions and nomadic division vector is defined as the division orientation. Once the wound had closed, the wound centre point was used, whereas for the unwounded tissue, we used the ‘virtual wound’.</p></sec><sec id="s4-11"><title>Division density correlation function</title><p>We calculate the division density in our system as follows: we image a 123.26 × 123.26 µm section of the tissue for 186 min taking an image every 2 min. This video is converted into a 3D (<italic>x</italic>,<italic>y</italic>,<italic>t</italic>) matrix of dimensions 124 × 124 × 89, whose components are 1 where there is a division and 0 otherwise. Thus, each component represents a 1 µm<sup>2</sup>-2 min space–time slice. We defined the time of division as the moment that anaphase starts. We use only 89 (and not all 93) time slices because we have incomplete information about division at the beginning and end of the video.</p><p>We number each of the elements in the matrix <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf15"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1,368,464</mml:mn></mml:math></inline-formula> is the number of elements. For the <italic>i</italic>th element, we define the mean mitosis density, <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, to be the division density in a space–time annular tube spatially centred at the point corresponding to the <italic>i</italic>th element, with spatial radius <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>; temporally, the annular tube is of extent <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf19"><mml:mi>T</mml:mi></mml:math></inline-formula> is the time corresponding to the <italic>i</italic>th element, <inline-formula><mml:math id="inf20"><mml:mi>δ</mml:mi><mml:mi>r</mml:mi></mml:math></inline-formula> is 10 µm and <inline-formula><mml:math id="inf21"><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:math></inline-formula> is 10 min; this is the size of our bins. Consequently, <inline-formula><mml:math id="inf22"><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is defined for  <inline-formula><mml:math id="inf23"><mml:mi>t</mml:mi></mml:math></inline-formula> = 10, 20 min, etc. and similarly for  <inline-formula><mml:math id="inf24"><mml:mi>r</mml:mi></mml:math></inline-formula> = 10, 20 µm, etc. When calculating the density of a tube we take the number of divisions in the region and divide by the space–time volume, but we need to take into account the fact that often the annular tube will extend outside the microscope view. Therefore, we divide only by the volume that can be observed using the confocal. It is convenient to extend the definition of mean mitosis density also to  <inline-formula><mml:math id="inf25"><mml:mi>t</mml:mi></mml:math></inline-formula> = 0 and <inline-formula><mml:math id="inf26"><mml:mi>r</mml:mi></mml:math></inline-formula> = 0. When  <inline-formula><mml:math id="inf27"><mml:mi>t</mml:mi></mml:math></inline-formula> = 0, the annular tube has no temporal depth and is concerned only with the time <inline-formula><mml:math id="inf28"><mml:mi>T</mml:mi></mml:math></inline-formula>. Similarly, when  <inline-formula><mml:math id="inf29"><mml:mi>r</mml:mi></mml:math></inline-formula> = 0, the annular tube becomes a line. When both are 0, the annular tube becomes a single point in space time. We define  <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> = 1 if the<italic>i</italic>th element is a division and  <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0 otherwise.</p><p>We define the correlations between the divisions as:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The first term on the right hand side (RHS) is<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>∈</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>d</italic> is the subset of elements where <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is, corresponds to a division. This means that this term is looking only at the densities around the divisions. The second term is<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mtext> </mml:mtext><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of divisions in the video. The last term is<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mtext> </mml:mtext><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mtext> </mml:mtext><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mtext> </mml:mtext><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, since the computation would take an extremely long time as there are <inline-formula><mml:math id="inf34"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1,368,464</mml:mn></mml:math></inline-formula> elements, we approximate it by randomly choosing a subset, <inline-formula><mml:math id="inf35"><mml:mi>R</mml:mi></mml:math></inline-formula>, of  <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 1000 elements.</p><p>The resulting division density correlation function (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) shows that there is a positive correlation in space and time, so <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>&gt;</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>40</mml:mn><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. This means that if we find one mitotic event we are more likely to find others nearby and soon afterwards.</p></sec><sec id="s4-12"><title>Division orientation correlation function</title><p>We compute the orientation angle of each division using U-NetOrientation, and form the orientation vector:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:mfrac linethickness="0pt"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf40"><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:math></inline-formula> is used since cell division orientation is nematic and we need <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To compare two division orientations, we take the dot product of the orientation vectors: 1 indicates that the divisions are aligned, −1 that they are perpendicular, and 0 that their orientations differ by <inline-formula><mml:math id="inf42"><mml:mfrac bevelled="true"><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>.</p><p>The division orientation correlation function is defined as,<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean dot product comparing the orientation of every pair of divisions within a radius <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> time from each other. This is calculated as explained above and shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>. Values of <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> close to 1 indicate highly aligned divisions, 0 no correlation and −1 anti-correlated.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Supervision, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-87949-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data generated or analysed during this study has been deposited in Zenodo at this link <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/13819609">https://zenodo.org/records/13819609</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Turley</surname><given-names>J</given-names></name><name><surname>Chenchiah</surname><given-names>IV</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name><name><surname>Liverpool</surname><given-names>TB</given-names></name><name><surname>Weavers</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Deep learning for rapid analysis of cell divisions in vivo during epithelial morphogenesis and repair</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.13819609</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We would like to thank members of the Weavers, Martin, Chenchiah, and Liverpool groups for helpful discussion. We also thank the Wolfson Bioimaging Facility (Bristol, UK), particularly Stephen Cross, for help setting up pyimagej, for helpful conversations and sharing useful resources. We are grateful to the Drosophila research community, Flybase and the Bloomington Stock Centre (Indiana, US), for various fly lines/reagents. We thank Jack Dymond for helpful conversations and sharing useful resources. This research was funded by the MRC-GW4 DTP PhD programme (scholarship to JT) [MR/N013794/1, NE/W503174/1, studentship 2284082], a Wellcome Trust and Royal Society Sir Henry Dale Fellowship to HW [208762/Z/17/Z] and a Wellcome Trust Investigator Award to PM [217169/Z/19/Z], Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship to JT. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript arising from this submission.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abedalla</surname><given-names>A</given-names></name><name><surname>Abdullah</surname><given-names>M</given-names></name><name><surname>Al-Ayyoub</surname><given-names>M</given-names></name><name><surname>Benkhelifa</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Chest X-ray pneumothorax segmentation using U-Net with EfficientNet and ResNet architectures</article-title><source>PeerJ. Computer Science</source><volume>7</volume><elocation-id>e607</elocation-id><pub-id pub-id-type="doi">10.7717/peerj-cs.607</pub-id><pub-id pub-id-type="pmid">34307860</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aigouy</surname><given-names>B</given-names></name><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Prud’Homme</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>EPySeg: A coding-free solution for automated segmentation of epithelia using deep learning</article-title><source>Development</source><volume>147</volume><elocation-id>dev194589</elocation-id><pub-id pub-id-type="doi">10.1242/dev.194589</pub-id><pub-id pub-id-type="pmid">33268451</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aragona</surname><given-names>M</given-names></name><name><surname>Dekoninck</surname><given-names>S</given-names></name><name><surname>Rulands</surname><given-names>S</given-names></name><name><surname>Lenglez</surname><given-names>S</given-names></name><name><surname>Mascré</surname><given-names>G</given-names></name><name><surname>Simons</surname><given-names>BD</given-names></name><name><surname>Blanpain</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Defining stem cell dynamics and migration during wound healing in mouse skin epidermis</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14684</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14684</pub-id><pub-id pub-id-type="pmid">28248284</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Athilingam</surname><given-names>T</given-names></name><name><surname>Tiwari</surname><given-names>P</given-names></name><name><surname>Toyama</surname><given-names>Y</given-names></name><name><surname>Saunders</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mechanics of epidermal morphogenesis in the <italic>Drosophila</italic> pupa</article-title><source>Seminars in Cell &amp; Developmental Biology</source><volume>120</volume><fpage>171</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1016/j.semcdb.2021.06.008</pub-id><pub-id pub-id-type="pmid">34167884</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aubreville</surname><given-names>M</given-names></name><name><surname>Bertram</surname><given-names>CA</given-names></name><name><surname>Marzahl</surname><given-names>C</given-names></name><name><surname>Gurtner</surname><given-names>C</given-names></name><name><surname>Dettwiler</surname><given-names>M</given-names></name><name><surname>Schmidt</surname><given-names>A</given-names></name><name><surname>Bartenschlager</surname><given-names>F</given-names></name><name><surname>Merz</surname><given-names>S</given-names></name><name><surname>Fragoso</surname><given-names>M</given-names></name><name><surname>Kershaw</surname><given-names>O</given-names></name><name><surname>Klopfleisch</surname><given-names>R</given-names></name><name><surname>Maier</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning algorithms out-perform veterinary pathologists in detecting the mitotically most active tumor region</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>16447</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-73246-2</pub-id><pub-id pub-id-type="pmid">33020510</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhatt</surname><given-names>U</given-names></name><name><surname>Xiang</surname><given-names>A</given-names></name><name><surname>Sharma</surname><given-names>S</given-names></name><name><surname>Weller</surname><given-names>A</given-names></name><name><surname>Taly</surname><given-names>A</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Ghosh</surname><given-names>J</given-names></name><name><surname>Puri</surname><given-names>R</given-names></name><name><surname>Moura</surname><given-names>JMF</given-names></name><name><surname>Eckersley</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Explainable machine learning in deployment</article-title><conf-name>FAT* ’20-Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</conf-name><fpage>648</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1145/3351095.3375624</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burlutskiy</surname><given-names>N</given-names></name><name><surname>Waring</surname><given-names>P</given-names></name><name><surname>Hipp</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The revival of the H&amp;E with artificial intelligence</article-title><source>Journal of Clinical and Anatomic Pathology</source><volume>5</volume><elocation-id>108</elocation-id><pub-id pub-id-type="doi">10.47275/2332-4864-108</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buslaev</surname><given-names>A</given-names></name><name><surname>Iglovikov</surname><given-names>VI</given-names></name><name><surname>Khvedchenya</surname><given-names>E</given-names></name><name><surname>Parinov</surname><given-names>A</given-names></name><name><surname>Druzhinin</surname><given-names>M</given-names></name><name><surname>Kalinin</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Albumentations: Fast and flexible image augmentations</article-title><source>Information</source><volume>11</volume><elocation-id>125</elocation-id><pub-id pub-id-type="doi">10.3390/info11020125</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carass</surname><given-names>A</given-names></name><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Gherman</surname><given-names>A</given-names></name><name><surname>Reinhold</surname><given-names>JC</given-names></name><name><surname>Jesson</surname><given-names>A</given-names></name><name><surname>Arbel</surname><given-names>T</given-names></name><name><surname>Maier</surname><given-names>O</given-names></name><name><surname>Handels</surname><given-names>H</given-names></name><name><surname>Ghafoorian</surname><given-names>M</given-names></name><name><surname>Platel</surname><given-names>B</given-names></name><name><surname>Birenbaum</surname><given-names>A</given-names></name><name><surname>Greenspan</surname><given-names>H</given-names></name><name><surname>Pham</surname><given-names>DL</given-names></name><name><surname>Crainiceanu</surname><given-names>CM</given-names></name><name><surname>Calabresi</surname><given-names>PA</given-names></name><name><surname>Prince</surname><given-names>JL</given-names></name><name><surname>Roncal</surname><given-names>WRG</given-names></name><name><surname>Shinohara</surname><given-names>RT</given-names></name><name><surname>Oguz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Evaluating white matter lesion segmentations with refined sørensen-dice analysis</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>8242</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-64803-w</pub-id><pub-id pub-id-type="pmid">32427874</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colen</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Redford</surname><given-names>SA</given-names></name><name><surname>Lemma</surname><given-names>LM</given-names></name><name><surname>Morgan</surname><given-names>L</given-names></name><name><surname>Ruijgrok</surname><given-names>PV</given-names></name><name><surname>Adkins</surname><given-names>R</given-names></name><name><surname>Bryant</surname><given-names>Z</given-names></name><name><surname>Dogic</surname><given-names>Z</given-names></name><name><surname>Gardel</surname><given-names>ML</given-names></name><name><surname>de Pablo</surname><given-names>JJ</given-names></name><name><surname>Vitelli</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Machine learning active-nematic hydrodynamics</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2016708118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2016708118</pub-id><pub-id pub-id-type="pmid">33653956</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etournay</surname><given-names>R</given-names></name><name><surname>Popović</surname><given-names>M</given-names></name><name><surname>Merkel</surname><given-names>M</given-names></name><name><surname>Nandi</surname><given-names>A</given-names></name><name><surname>Blasse</surname><given-names>C</given-names></name><name><surname>Aigouy</surname><given-names>B</given-names></name><name><surname>Brandl</surname><given-names>H</given-names></name><name><surname>Myers</surname><given-names>G</given-names></name><name><surname>Salbreux</surname><given-names>G</given-names></name><name><surname>Jülicher</surname><given-names>F</given-names></name><name><surname>Eaton</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interplay of cell dynamics and epithelial tension during morphogenesis of the <italic>Drosophila</italic> pupal wing</article-title><source>eLife</source><volume>4</volume><elocation-id>e07090</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.07090</pub-id><pub-id pub-id-type="pmid">26102528</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etournay</surname><given-names>R</given-names></name><name><surname>Merkel</surname><given-names>M</given-names></name><name><surname>Popović</surname><given-names>M</given-names></name><name><surname>Brandl</surname><given-names>H</given-names></name><name><surname>Dye</surname><given-names>NA</given-names></name><name><surname>Aigouy</surname><given-names>B</given-names></name><name><surname>Salbreux</surname><given-names>G</given-names></name><name><surname>Eaton</surname><given-names>S</given-names></name><name><surname>Jülicher</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>TissueMiner: A multiscale analysis toolkit to quantify how cellular processes create tissue dynamics</article-title><source>eLife</source><volume>5</volume><elocation-id>e14334</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14334</pub-id><pub-id pub-id-type="pmid">27228153</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fabbri</surname><given-names>R</given-names></name><name><surname>Da</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>2D Euclidean distance transform algorithms: A comparative survey</article-title><source>ACM Computing Surveys</source><volume>40</volume><elocation-id>1322434</elocation-id><pub-id pub-id-type="doi">10.1145/1322432.1322434</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Gonzalez</surname><given-names>R</given-names></name><name><surname>Balaghi</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Hawkins</surname><given-names>R</given-names></name><name><surname>Rothenberg</surname><given-names>K</given-names></name><name><surname>McFaul</surname><given-names>C</given-names></name><name><surname>Schimmer</surname><given-names>C</given-names></name><name><surname>Ly</surname><given-names>M</given-names></name><name><surname>do Carmo</surname><given-names>AM</given-names></name><name><surname>Scepanovic</surname><given-names>G</given-names></name><name><surname>Erdemci-Tandogan</surname><given-names>G</given-names></name><name><surname>Castle</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>PyJAMAS: open-source, multimodal segmentation and analysis of microscopy images</article-title><source>Bioinformatics</source><volume>38</volume><fpage>594</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab589</pub-id><pub-id pub-id-type="pmid">34390579</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franz</surname><given-names>A</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fat body cells are motile and actively migrate to wounds to drive repair and prevent infection</article-title><source>Developmental Cell</source><volume>44</volume><fpage>460</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1016/j.devcel.2018.01.026</pub-id><pub-id pub-id-type="pmid">29486196</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>George</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Wound healing insights from flies and fish</article-title><source>Cold Spring Harbor Perspectives in Biology</source><volume>14</volume><elocation-id>a041217</elocation-id><pub-id pub-id-type="doi">10.1101/cshperspect.a041217</pub-id><pub-id pub-id-type="pmid">35817511</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilad</surname><given-names>T</given-names></name><name><surname>Reyes</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>JY</given-names></name><name><surname>Lahav</surname><given-names>G</given-names></name><name><surname>Riklin Raviv</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fully unsupervised symmetry-based mitosis detection in time-lapse cell microscopy</article-title><source>Bioinformatics</source><volume>35</volume><fpage>2644</fpage><lpage>2653</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty1034</pub-id><pub-id pub-id-type="pmid">30590471</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Greenspan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Fly Pushing: The Theory and Practice of Drosophila Genetics</source><publisher-name>Cold Spring Harbor Press</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Oerlemans</surname><given-names>A</given-names></name><name><surname>Lao</surname><given-names>S</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Lew</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning for visual understanding: A review</article-title><source>Neurocomputing</source><volume>187</volume><fpage>27</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2015.09.116</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzmán-Herrera</surname><given-names>A</given-names></name><name><surname>Mao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Polarity during tissue repair, a multiscale problem</article-title><source>Current Opinion in Cell Biology</source><volume>62</volume><fpage>31</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.ceb.2019.07.015</pub-id><pub-id pub-id-type="pmid">31514044</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Las Vegas, NV, USA</conf-loc><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Fast.ai—making neural nets uncool again</data-title><source>PyTorch</source><ext-link ext-link-type="uri" xlink:href="https://www.fast.ai/">https://www.fast.ai/</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>J</given-names></name><name><surname>Gugger</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fastai: A layered API for deep learning</article-title><source>Information</source><volume>11</volume><elocation-id>108</elocation-id><pub-id pub-id-type="doi">10.3390/info11020108</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Işın</surname><given-names>A</given-names></name><name><surname>Direkoğlu</surname><given-names>C</given-names></name><name><surname>Şah</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Review of MRI-based brain tumor image segmentation using deep learning methods</article-title><source>Procedia Computer Science</source><volume>102</volume><fpage>317</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2016.09.407</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>3D convolutional neural networks for human action recognition</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>35</volume><fpage>221</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.59</pub-id><pub-id pub-id-type="pmid">22392705</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>W</given-names></name><name><surname>Alasoo</surname><given-names>K</given-names></name><name><surname>Fishman</surname><given-names>D</given-names></name><name><surname>Parts</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computational biology: deep learning</article-title><source>Emerging Topics in Life Sciences</source><volume>1</volume><fpage>257</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1042/ETLS20160025</pub-id><pub-id pub-id-type="pmid">33525807</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitrungrotsakul</surname><given-names>T</given-names></name><name><surname>Han</surname><given-names>XH</given-names></name><name><surname>Iwamoto</surname><given-names>Y</given-names></name><name><surname>Takemoto</surname><given-names>S</given-names></name><name><surname>Yokota</surname><given-names>H</given-names></name><name><surname>Ipponjima</surname><given-names>S</given-names></name><name><surname>Nemoto</surname><given-names>T</given-names></name><name><surname>Xiong</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>YW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A cascade of 2.5D CNN and bidirectional CLSTM network for mitotic cell detection in 4D microscopy image</article-title><source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source><volume>18</volume><fpage>396</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2019.2919015</pub-id><pub-id pub-id-type="pmid">31144644</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>H</given-names></name><name><surname>Akakin</surname><given-names>HC</given-names></name><name><surname>Sarma</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A generalized Laplacian of Gaussian filter for blob detection and its applications</article-title><source>IEEE Transactions on Cybernetics</source><volume>43</volume><fpage>1719</fpage><lpage>1733</lpage><pub-id pub-id-type="doi">10.1109/TSMCB.2012.2228639</pub-id><pub-id pub-id-type="pmid">23757570</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lancaster</surname><given-names>OM</given-names></name><name><surname>Baum</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Shaping up to divide: coordinating actin and microtubule cytoskeletal remodelling during mitosis</article-title><source>Seminars in Cell &amp; Developmental Biology</source><volume>34</volume><fpage>109</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.semcdb.2014.02.015</pub-id><pub-id pub-id-type="pmid">24607328</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>Y</given-names></name><name><surname>Tournier</surname><given-names>AL</given-names></name><name><surname>Bates</surname><given-names>PA</given-names></name><name><surname>Gale</surname><given-names>JE</given-names></name><name><surname>Tapon</surname><given-names>N</given-names></name><name><surname>Thompson</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Planar polarization of the atypical myosin Dachs orients cell divisions in <italic>Drosophila</italic></article-title><source>Genes &amp; Development</source><volume>25</volume><fpage>131</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1101/gad.610511</pub-id><pub-id pub-id-type="pmid">21245166</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>Y</given-names></name><name><surname>Han</surname><given-names>L</given-names></name><name><surname>Yin</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cell mitosis event analysis in phase contrast microscopy images using deep learning</article-title><source>Medical Image Analysis</source><volume>57</volume><fpage>32</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.media.2019.06.011</pub-id><pub-id pub-id-type="pmid">31261018</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>P</given-names></name><name><surname>Nunan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cellular and molecular mechanisms of repair in acute and chronic wound healing</article-title><source>The British Journal of Dermatology</source><volume>173</volume><fpage>370</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1111/bjd.13954</pub-id><pub-id pub-id-type="pmid">26175283</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milan</surname><given-names>M</given-names></name><name><surname>Campuzano</surname><given-names>S</given-names></name><name><surname>Garcia-Bellido</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Cell cycling and patterned cell proliferation in the <italic>Drosophila</italic> wing during metamorphosis (imaginal disc/morphogenesis/evagination/vein patterning)</article-title><source>PNAS</source><volume>93</volume><fpage>11687</fpage><lpage>11692</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.21.11687</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nestor-Bergmann</surname><given-names>A</given-names></name><name><surname>Stooke-Vaughan</surname><given-names>GA</given-names></name><name><surname>Goddard</surname><given-names>GK</given-names></name><name><surname>Starborg</surname><given-names>T</given-names></name><name><surname>Jensen</surname><given-names>OE</given-names></name><name><surname>Woolner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoupling the roles of cell shape and mechanical stress in orienting and cueing epithelial mitosis</article-title><source>Cell Reports</source><volume>26</volume><fpage>2088</fpage><lpage>2100</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.01.102</pub-id><pub-id pub-id-type="pmid">30784591</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nie</surname><given-names>WZ</given-names></name><name><surname>Li</surname><given-names>WH</given-names></name><name><surname>Liu</surname><given-names>AA</given-names></name><name><surname>Hao</surname><given-names>T</given-names></name><name><surname>Su</surname><given-names>YT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>3D Convolutional Networks-Based Mitotic Event Detection in Time-Lapse Phase Contrast Microscopy Image Sequences of Stem Cell Populations</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW</conf-name><pub-id pub-id-type="doi">10.1109/CVPRW.2016.171</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olenik</surname><given-names>M</given-names></name><name><surname>Turley</surname><given-names>J</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Weavers</surname><given-names>H</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name><name><surname>Chenchiah</surname><given-names>IV</given-names></name><name><surname>Liverpool</surname><given-names>TB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Fluctuations of cell geometry and their nonequilibrium thermodynamics in living epithelial tissue</article-title><source>Physical Review. E</source><volume>107</volume><elocation-id>014403</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.107.014403</pub-id><pub-id pub-id-type="pmid">36797912</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paci</surname><given-names>G</given-names></name><name><surname>Mao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Forced into shape: Mechanical forces in <italic>Drosophila</italic> development and homeostasis</article-title><source>Seminars in Cell &amp; Developmental Biology</source><volume>120</volume><fpage>160</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.semcdb.2021.05.026</pub-id><pub-id pub-id-type="pmid">34092509</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>S</given-names></name><name><surname>Gonzalez</surname><given-names>DG</given-names></name><name><surname>Guirao</surname><given-names>B</given-names></name><name><surname>Boucher</surname><given-names>JD</given-names></name><name><surname>Cockburn</surname><given-names>K</given-names></name><name><surname>Marsh</surname><given-names>ED</given-names></name><name><surname>Mesa</surname><given-names>KR</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Rompolas</surname><given-names>P</given-names></name><name><surname>Haberman</surname><given-names>AM</given-names></name><name><surname>Bellaïche</surname><given-names>Y</given-names></name><name><surname>Greco</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tissue-scale coordination of cellular behaviour promotes epidermal wound repair in live mice</article-title><source>Nature Cell Biology</source><volume>19</volume><fpage>155</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1038/ncb3472</pub-id><pub-id pub-id-type="pmid">28248302</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phan</surname><given-names>HTH</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Fulham</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unsupervised Two-path neural network for cell event detection and classification using spatiotemporal patterns</article-title><source>IEEE Transactions on Medical Imaging</source><volume>38</volume><fpage>1477</fpage><lpage>1487</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2885572</pub-id><pub-id pub-id-type="pmid">30530316</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piansaddhayanaon</surname><given-names>C</given-names></name><name><surname>Santisukwongchote</surname><given-names>S</given-names></name><name><surname>Shuangshoti</surname><given-names>S</given-names></name><name><surname>Tao</surname><given-names>Q</given-names></name><name><surname>Sriswasdi</surname><given-names>S</given-names></name><name><surname>Chuangsuwanich</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>ReCasNet: Improving consistency within the two-stage mitosis detection framework</article-title><source>Artificial Intelligence in Medicine</source><volume>135</volume><elocation-id>102462</elocation-id><pub-id pub-id-type="doi">10.1016/j.artmed.2022.102462</pub-id><pub-id pub-id-type="pmid">36628784</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razzell</surname><given-names>W</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Swatting flies: modelling wound healing and inflammation in Drosophila</article-title><source>Disease Models &amp; Mechanisms</source><volume>4</volume><fpage>569</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1242/dmm.006825</pub-id><pub-id pub-id-type="pmid">21810906</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razzell</surname><given-names>W</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Recapitulation of morphogenetic cell shape changes enables wound re-epithelialisation</article-title><source>Development</source><volume>141</volume><fpage>1814</fpage><lpage>1820</lpage><pub-id pub-id-type="doi">10.1242/dev.107045</pub-id><pub-id pub-id-type="pmid">24718989</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1505.04597">http://arxiv.org/abs/1505.04597</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarpa</surname><given-names>E</given-names></name><name><surname>Finet</surname><given-names>C</given-names></name><name><surname>Blanchard</surname><given-names>GB</given-names></name><name><surname>Sanson</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Actomyosin-driven tension at compartmental boundaries orients cell division independently of cell geometry in vivo</article-title><source>Developmental Cell</source><volume>47</volume><fpage>727</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1016/j.devcel.2018.10.029</pub-id><pub-id pub-id-type="pmid">30503752</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Xin</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Lu</surname><given-names>M</given-names></name><name><surname>Cong</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Deep Framework for Cell Mitosis Detection in Microscopy Images</article-title><conf-name>2020 16th International Conference on Computational Intelligence and Security (CIS</conf-name><conf-loc>Guangxi, China</conf-loc><fpage>100</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1109/CIS52066.2020.00030</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetley</surname><given-names>RJ</given-names></name><name><surname>Staddon</surname><given-names>MF</given-names></name><name><surname>Heller</surname><given-names>D</given-names></name><name><surname>Hoppe</surname><given-names>A</given-names></name><name><surname>Banerjee</surname><given-names>S</given-names></name><name><surname>Mao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Tissue fluidity promotes epithelial wound healing</article-title><source>Nature Physics</source><volume>15</volume><fpage>1195</fpage><lpage>1203</lpage><pub-id pub-id-type="doi">10.1038/s41567-019-0618-1</pub-id><pub-id pub-id-type="pmid">31700525</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinevez</surname><given-names>JY</given-names></name><name><surname>Perry</surname><given-names>N</given-names></name><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Hoopes</surname><given-names>GM</given-names></name><name><surname>Reynolds</surname><given-names>GD</given-names></name><name><surname>Laplantine</surname><given-names>E</given-names></name><name><surname>Bednarek</surname><given-names>SY</given-names></name><name><surname>Shorte</surname><given-names>SL</given-names></name><name><surname>Eliceiri</surname><given-names>KW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>TrackMate: An open and extensible platform for single-particle tracking</article-title><source>Methods</source><volume>115</volume><fpage>80</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.ymeth.2016.09.016</pub-id><pub-id pub-id-type="pmid">27713081</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>T</given-names></name><name><surname>Kwon</surname><given-names>OH</given-names></name><name><surname>Kwon</surname><given-names>KR</given-names></name><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Kang</surname><given-names>KW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Blood Cell Images Segmentation using Deep Learning Semantic Segmentation</article-title><conf-name>2018 IEEE International Conference on Electronics and Communication Engineering (ICECE</conf-name><conf-loc>Xi’an, China</conf-loc><pub-id pub-id-type="doi">10.1109/ICECOME.2018.8644754</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turley</surname><given-names>J</given-names></name><name><surname>Chenchiah</surname><given-names>IV</given-names></name><name><surname>Liverpool</surname><given-names>TB</given-names></name><name><surname>Weavers</surname><given-names>H</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>What good is maths in studies of wound healing?</article-title><source>iScience</source><volume>25</volume><elocation-id>104778</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2022.104778</pub-id><pub-id pub-id-type="pmid">35996582</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Turley</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>cell-division-dl-plugin</data-title><version designator="swh:1:rev:f6c3290670f23524d47ccddc873ef5673eda4b3c">swh:1:rev:f6c3290670f23524d47ccddc873ef5673eda4b3c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:828bb9e7c1614c1e0534050cd6a6cb9df6bc84c7;origin=https://github.com/turleyjm/cell-division-dl-plugin;visit=swh:1:snp:939fe1ceeff9f734ad605b447dd8ed379af63455;anchor=swh:1:rev:f6c3290670f23524d47ccddc873ef5673eda4b3c">https://archive.softwareheritage.org/swh:1:dir:828bb9e7c1614c1e0534050cd6a6cb9df6bc84c7;origin=https://github.com/turleyjm/cell-division-dl-plugin;visit=swh:1:snp:939fe1ceeff9f734ad605b447dd8ed379af63455;anchor=swh:1:rev:f6c3290670f23524d47ccddc873ef5673eda4b3c</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Umorin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><data-title>Stack focuser plugin for imagej</data-title><source>Imagej</source><ext-link ext-link-type="uri" xlink:href="https://imagej.net/ij/plugins/stack-focuser.html">https://imagej.net/ij/plugins/stack-focuser.html</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villars</surname><given-names>A</given-names></name><name><surname>Letort</surname><given-names>G</given-names></name><name><surname>Valon</surname><given-names>L</given-names></name><name><surname>Levayer</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>DeXtrusion: automatic recognition of epithelial cell extrusion through machine learning in vivo</article-title><source>Development</source><volume>150</volume><elocation-id>dev201747</elocation-id><pub-id pub-id-type="doi">10.1242/dev.201747</pub-id><pub-id pub-id-type="pmid">37283069</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voulodimos</surname><given-names>A</given-names></name><name><surname>Doulamis</surname><given-names>N</given-names></name><name><surname>Doulamis</surname><given-names>A</given-names></name><name><surname>Protopapadakis</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning for computer vision: A brief review</article-title><source>Computational Intelligence and Neuroscience</source><volume>2018</volume><elocation-id>7068349</elocation-id><pub-id pub-id-type="doi">10.1155/2018/7068349</pub-id><pub-id pub-id-type="pmid">29487619</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Yan</surname><given-names>B</given-names></name><name><surname>Chang</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Zhao</surname><given-names>M</given-names></name><name><surname>Cui</surname><given-names>L</given-names></name><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Trends in the application of deep learning networks in medical image analysis: Evolution between 2012 and 2020</article-title><source>European Journal of Radiology</source><volume>146</volume><elocation-id>110069</elocation-id><pub-id pub-id-type="doi">10.1016/j.ejrad.2021.110069</pub-id><pub-id pub-id-type="pmid">34847395</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weavers</surname><given-names>H</given-names></name><name><surname>Liepe</surname><given-names>J</given-names></name><name><surname>Sim</surname><given-names>A</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name><name><surname>Stumpf</surname><given-names>MPH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Systems analysis of the dynamic inflammatory response to tissue damage reveals spatiotemporal properties of the wound attractant gradient</article-title><source>Current Biology</source><volume>26</volume><fpage>1975</fpage><lpage>1989</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.06.012</pub-id><pub-id pub-id-type="pmid">27426513</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weavers</surname><given-names>H</given-names></name><name><surname>Franz</surname><given-names>A</given-names></name><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Long-term in vivo tracking of inflammatory cell dynamics within <italic>Drosophila</italic> pupae</article-title><source>Journal of Visualized Experiments</source><volume>2018</volume><elocation-id>57871</elocation-id><pub-id pub-id-type="doi">10.3791/57871</pub-id><pub-id pub-id-type="pmid">29985351</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolny</surname><given-names>A</given-names></name><name><surname>Cerrone</surname><given-names>L</given-names></name><name><surname>Vijayan</surname><given-names>A</given-names></name><name><surname>Tofanelli</surname><given-names>R</given-names></name><name><surname>Barro</surname><given-names>AV</given-names></name><name><surname>Louveaux</surname><given-names>M</given-names></name><name><surname>Wenzl</surname><given-names>C</given-names></name><name><surname>Strauss</surname><given-names>S</given-names></name><name><surname>Wilson-Sánchez</surname><given-names>D</given-names></name><name><surname>Lymbouridou</surname><given-names>R</given-names></name><name><surname>Steigleder</surname><given-names>SS</given-names></name><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Bailoni</surname><given-names>A</given-names></name><name><surname>Duran-Nebreda</surname><given-names>S</given-names></name><name><surname>Bassel</surname><given-names>GW</given-names></name><name><surname>Lohmann</surname><given-names>JU</given-names></name><name><surname>Tsiantis</surname><given-names>M</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name><name><surname>Schneitz</surname><given-names>K</given-names></name><name><surname>Maizel</surname><given-names>A</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Accurate and versatile 3D segmentation of plant tissues at cellular resolution</article-title><source>eLife</source><volume>9</volume><elocation-id>e57613</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57613</pub-id><pub-id pub-id-type="pmid">32723478</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>W</given-names></name><name><surname>Jacinto</surname><given-names>A</given-names></name><name><surname>Grose</surname><given-names>R</given-names></name><name><surname>Woolner</surname><given-names>S</given-names></name><name><surname>Gale</surname><given-names>J</given-names></name><name><surname>Wilson</surname><given-names>C</given-names></name><name><surname>Martin</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Wound healing recapitulates morphogenesis in <italic>Drosophila</italic> embryos</article-title><source>Nature Cell Biology</source><volume>4</volume><fpage>907</fpage><lpage>912</lpage><pub-id pub-id-type="doi">10.1038/ncb875</pub-id><pub-id pub-id-type="pmid">12402048</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87949.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>École Normale Supérieure - PSL</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>In this <bold>valuable</bold> study, the authors use deep learning models to provide <bold>solid</bold> evidence that epithelial wounding triggers bursts of cell division at a characteristic distance away from the wound. The documentation provided by the authors should allow other scientists to readily apply these methods, which are particularly appropriate where unsupervised machine-learning algorithms have difficulties.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87949.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors present a number of deep learning models to analyse the dynamics of epithelia. In this way they want to overcome the time-consuming manual analysis of such data and also remove a potential operator bias. Specifically, they set up models for identifying cell division events and cell division orientation. They apply these tools to the epithelium of the developing <italic>Drosophila</italic> pupal wing. They confirm a linear decrease of the division density with time and identify a burst of cell division after healing of a wound that they had induced earlier. These division events happen a characteristic time after and a characteristic distance away from the wound. These characteristic quantities depend on the size of the wound.</p><p>Strengths:</p><p>The methods developed in this work achieve the goals set by the authors and are a very helpful addition to the toolbox of developmental biologists. They could potentially be used on various developing epithelia. The evidence for the impact of wounds on cell division is compelling.</p><p>The methods presented in this work should prove to be very helpful for quantifying cell proliferation in epithelial tissues.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87949.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this manuscript, the authors propose a computational method based on deep convolutional neural networks (CNNs) to automatically detect cell divisions in two-dimensional fluorescence microscopy timelapse images. Three deep learning models are proposed to detect the timing of division, predict the division axis, and enhance cell boundary images to segment cells before and after division. Using this computational pipeline, the authors analyze the dynamics of cell divisions in the epithelium of the <italic>Drosophila</italic> pupal wing and find that a wound first induces a reduction in the frequency of division followed by a synchronised burst of cell divisions about 100 minutes after its induction.</p><p>Comments on revised version:</p><p>Regarding the Reviewer's 1 comment on the architecture details, I have now understood that the precise architecture (number/type of layers, activation functions, pooling operations, skip connections, upsampling choice...) might have remained relatively hidden to the authors themselves, as the U-net is built automatically by the fast.ai library from a given classical choice of encoder architecture (ResNet34 and ResNet101 here) to generate the decoder part and skip connections.</p><p>Regarding the Major point 1, I raised the question of the generalisation potential of the method. I do not think, for instance, that the optimal number of frames to use, nor the optimal choice of their time-shift with respect to the division time (t-n, t+m) (not systematically studied here) may be generic hyperparameters that can be directly transferred to another setting. This implies that the method proposed will necessarily require re-labeling, re-training and re-optimizing the hyperparameters which directly influence the network architecture for each new dataset imaged differently. This limits the generalisation of the method to other datasets, and this may be seen as in contrast to other tools developed in the field for other tasks such as cellpose for segmentation, which has proven a true potential for generalisation on various data modalities. I was hoping that the authors would try themselves testing the robustness of their method by re-imaging the same tissue with slightly different acquisition rate for instance, to give more weight to their work.</p><p>In this regard, and because the authors claimed to provide clear instructions on how to reuse their method or adapt it to a different context, I delved deeper into the code and, to my surprise, felt that we are far from the coding practice of what a well-documented and accessible tool should be.</p><p>To start with, one has to be relatively accustomed with Napari to understand how the plugin must be installed, as the only thing given is a pip install command (that could be typed in any terminal without installing the plugin for Napari, but has to be typed inside the Napari terminal, which is mentioned nowhere). Surprisingly, the plugin was not uploaded on Napari hub, nor on PyPI by the authors, so it is not searchable/findable directly, one has to go to the Github repository and install it manually. In that regard, no description was provided in the copy-pasted templated files associated to the napari hub, so exporting it to the hub would actually leave it undocumented.</p><p>Regarding now the python notebooks, one can fairly say that the &quot;clear instructions&quot; that were supposed to enlighten the code are really minimal. Only one notebook &quot;trainingUNetCellDivision10.ipynb&quot; has actually some comments, the other have (almost) none nor title to help the unskilled programmer delving into the script to guess what it should do. I doubt that a biologist who does not have a strong computational background will manage adapting the method to its own dataset (which seems to me unavoidable for the reasons mentioned above).</p><p>Finally regarding the data, none is shared publicly along with this manuscript/code, such that if one doesn't have a similar type of dataset - that must be first annotated in a similar manner - one cannot even test the networks/plugin for its own information. A common and necessary practice in the field - and possibly a longer lasting contribution of this work - could have been to provide the complete and annotated dataset that was used to train and test the artificial neural network. The basic reason is that a more performant, or more generalisable deep-learning model may be developed very soon after this one and for its performance to be fairly compared, it requires to be compared on the same dataset. Benchmarking and comparison of methods performance is at the core of computer vision and deep-learning.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87949.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Turley</surname><given-names>Jake</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Chenchiah</surname><given-names>Isaac</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Martin</surname><given-names>Paul</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Liverpool</surname><given-names>Tanniemola</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Weavers</surname><given-names>Helen</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><addr-line><named-content content-type="city">Bristol</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the current reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>The authors present a number of deep learning models to analyse the dynamics of epithelia. In this way they want to overcome the time-consuming manual analysis of such data and also remove a potential operator bias. Specifically, they set up models for identifying cell division events and cell division orientation. They apply these tools to the epithelium of the developing <italic>Drosophila</italic> pupal wing. They confirm a linear decrease of the division density with time and identify a burst of cell division after healing of a wound that they had induced earlier. These division events happen a characteristic time after and a characteristic distance away from the wound. These characteristic quantities depend on the size of the wound.</p><p>Strengths:</p><p>The methods developed in this work achieve the goals set by the authors and are a very helpful addition to the toolbox of developmental biologists. They could potentially be used on various developing epithelia. The evidence for the impact of wounds on cell division is compelling.</p><p>The methods presented in this work should prove to be very helpful for quantifying cell proliferation in epithelial tissues.</p></disp-quote><p>We thank the reviewer for the positive comments!</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>In this manuscript, the authors propose a computational method based on deep convolutional neural networks (CNNs) to automatically detect cell divisions in two-dimensional fluorescence microscopy timelapse images. Three deep learning models are proposed to detect the timing of division, predict the division axis, and enhance cell boundary images to segment cells before and after division. Using this computational pipeline, the authors analyze the dynamics of cell divisions in the epithelium of the <italic>Drosophila</italic> pupal wing and find that a wound first induces a reduction in the frequency of division followed by a synchronised burst of cell divisions about 100 minutes after its induction.</p><p>Comments on revised version:</p><p>Regarding the Reviewer's 1 comment on the architecture details, I have now understood that the precise architecture (number/type of layers, activation functions, pooling operations, skip connections, upsampling choice…) might have remained relatively hidden to the authors themselves, as the U-net is built automatically by the fast.ai library from a given classical choice of encoder architecture (ResNet34 and ResNet101 here) to generate the decoder part and skip connections.</p><p>Regarding the Major point 1, I raised the question of the generalisation potential of the method. I do not think, for instance, that the optimal number of frames to use, nor the optimal choice of their time-shift with respect to the division time (t-n, t+m) (not systematically studied here) may be generic hyperparameters that can be directly transferred to another setting. This implies that the method proposed will necessarily require re-labeling, re-training and re-optimizing the hyperparameters which directly influence the network architecture for each new dataset imaged differently. This limits the generalisation of the method to other datasets, and this may be seen as in contrast to other tools developed in the field for other tasks such as cellpose for segmentation, which has proven a true potential for generalisation on various data modalities. I was hoping that the authors would try themselves testing the robustness of their method by re-imaging the same tissue with slightly different acquisition rate for instance, to give more weight to their work.</p></disp-quote><p>We thank the referee for the comments. Regarding this particular biological system, due to photobleaching over long imaging periods (and the availability of imaging systems during the project), we would have difficulty imaging at much higher rates than the 2 minute time frame we currently use. These limitations are true for many such systems, and it is rarely possible to rapidly image for long periods of time in real experiments. Given this upper limit in framerate, we could, in principle, sample this data at a lower framerate, by removing time points of the videos but this typically leads to worse results. With some pilot data, we have tried to use fewer time intervals for our analysis but they always gave worse results. We found we need to feed the maximum amount of information available into the model to get the best results (i.e. the fastest frame rate possible, given the data available). Our goal is to teach the neural net to identify dynamic space-time localised events from time lapse videos, in which the duration of an event is a key parameter. Our division events take 10 minutes or less to complete therefore we used 5 timepoints in the videos for the deep learning model. If we considered another system with dynamic events which have a duration T when we would use T/t timepoints where t is the minimum time interval (for our data t=2min). For example if we could image every minute we would use 10 timepoints. As discussed below, we do envision other users with different imaging setups and requirements may need to retrain the model for their own data and to help with this, we have now provided more detailed instructions how to do this (see later).</p><disp-quote content-type="editor-comment"><p>In this regard, and because the authors claimed to provide clear instructions on how to reuse their method or adapt it to a different context, I delved deeper into the code and, to my surprise, felt that we are far from the coding practice of what a well-documented and accessible tool should be.</p><p>To start with, one has to be relatively accustomed with Napari to understand how the plugin must be installed, as the only thing given is a pip install command (that could be typed in any terminal without installing the plugin for Napari, but has to be typed inside the Napari terminal, which is mentioned nowhere). Surprisingly, the plugin was not uploaded on Napari hub, nor on PyPI by the authors, so it is not searchable/findable directly, one has to go to the Github repository and install it manually. In that regard, no description was provided in the copy-pasted templated files associated to the napari hub, so exporting it to the hub would actually leave it undocumented.</p></disp-quote><p>We thank the referee for suggesting the example of (DeXtrusion, Villars et al. 2023). We have endeavoured to produce similarly-detailed documentation for our tools. We now have clear instructions for installation requiring only minimal coding knowledge, and we have provided a user manual for the napari plug-in. This includes information on each of the options for using the model and the outputs they will produce. The plugin has been tested by several colleagues using both Windows and Mac operating systems.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87949-sa3-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>Regarding now the python notebooks, one can fairly say that the &quot;clear instructions&quot; that were supposed to enlighten the code are really minimal. Only one notebook &quot;trainingUNetCellDivision10.ipynb&quot; has actually some comments, the other have (almost) none nor title to help the unskilled programmer delving into the script to guess what it should do. I doubt that a biologist who does not have a strong computational background will manage adapting the method to its own dataset (which seems to me unavoidable for the reasons mentioned above).</p></disp-quote><p>Within the README file, we have now included information on how to retrain the models with helpful links to deep learning tutorials (which, indeed, some of us have learnt from) for those new to deep learning. All Jupyter notebooks now include more comments explaining the models.</p><disp-quote content-type="editor-comment"><p>Finally regarding the data, none is shared publicly along with this manuscript/code, such that if one doesn't have a similar type of dataset - that must be first annotated in a similar manner - one cannot even test the networks/plugin for its own information. A common and necessary practice in the field - and possibly a longer lasting contribution of this work - could have been to provide the complete and annotated dataset that was used to train and test the artificial neural network. The basic reason is that a more performant, or more generalisable deep-learning model may be developed very soon after this one and for its performance to be fairly compared, it requires to be compared on the same dataset. Benchmarking and comparison of methods performance is at the core of computer vision and deep-learning.</p></disp-quote><p>We thank the referee for these comments. We have now uploaded all the data used to train the models and to test them, as well as all the data used in the analyses for the paper. This includes many videos that were not used for training but were analysed to generate the paper’s results. The link to these data sets is provided in our GitHub page (<ext-link ext-link-type="uri" xlink:href="https://github.com/turleyjm/cell-division-dl-plugin/tree/main">https://github.com/turleyjm/cell-division-dl-plugin/tree/main</ext-link>). In the folder for the data sets and in the GitHub repository, we have included the Jupyter notebooks used to train the models and these can be used for retraining. We have made our data publicly available at Zenodo dataset <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10846684">https://zenodo.org/records/10846684</ext-link> (added to last paragraph of discussion). We have also included scripts that can be used to compare the model output with ground truth, including outputs highlighting false positives and false negatives. Together with these scripts, models can be compared and contrasted, both in general and in individual videos. Overall, we very much appreciate the reviewer’s advice, which has made the plugin much more user- friendly and, hopefully, easier for other groups to train their own models. Our contact details are provided, and we would be happy to advise any groups that would like to use our tools.</p><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>The authors present a number of deep-learning models to analyse the dynamics of epithelia. In this way, they want to overcome the time-consuming manual analysis of such data and also remove a potential operator bias. Specifically, they set up models for identifying cell division events and cell division orientation. They apply these tools to the epithelium of the developing <italic>Drosophila</italic> pupal wing. They confirm a linear decrease of the division density with time and identify a burst of cell division after the healing of a wound that they had induced earlier. These division events happen a characteristic time after and a characteristic distance away from the wound. These characteristic quantities depend on the size of the wound.</p><p>Strength:</p><p>The methods developed in this work achieve the goals set by the authors and are a very helpful addition to the toolbox of developmental biologists. They could potentially be used on various developing epithelia. The evidence for the impact of wounds on cell division is solid.</p><p>Weakness:</p><p>Some aspects of the deep-learning models remained unclear, and the authors might want to think about adding details. First of all, for readers not being familiar with deep-learning models, I would like to see more information about ResNet and U-Net, which are at the base of the new deep-learning models developed here. What is the structure of these networks?</p></disp-quote><p>We agree with the Reviewer and have included additional information on page 8 of the manuscript, outlining some background information about the architecture of ResNet and U-Net models.</p><disp-quote content-type="editor-comment"><p>How many parameters do you use?</p></disp-quote><p>We apologise for this omission and have now included the number of parameters and layers in each model in the methods section on page 25.</p><disp-quote content-type="editor-comment"><p>What is the difference between validating and testing the model? Do the corresponding data sets differ fundamentally?</p></disp-quote><p>The difference between ‘validating’ and ‘testing’ the model is validating data is used during training to determine whether the model is overfitting. If the model is performing well on the training data but not on the validating data, this a key signal the model is overfitting and changes will need to be made to the network/training method to prevent this. The testing data is used after all the training has been completed and is used to test the performance of the model on fresh data it has not been trained on. We have removed refence to the validating data in the main text to make it simpler and add this explanation to the methods. There is no fundamental (or experimental) difference between each of the labelled data sets; rather, they are collected from different biological samples. We have now included this information in the Methods text on page 24.</p><disp-quote content-type="editor-comment"><p>How did you assess the quality of the training data classification?</p></disp-quote><p>These data were generated and hand-labelled by an expert with many years of experience in identifying cell divisions in imaging data, to give the ground truth for the deep learning model.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>You repeatedly use 'new', 'novel' as well as 'surprising' and 'unexpected'. The latter are rather subjective and it is not clear based on what prior knowledge you make these statements. Unless indicated otherwise, it is understood that the results and methods are new, so you can delete these terms.</p></disp-quote><p>We have deleted these words, as suggested, for almost all cases.</p><disp-quote content-type="editor-comment"><p>p.4 &quot;as expected&quot; add a reference or explain why it is expected.</p></disp-quote><p>A reference has now been included in this section, as suggested.</p><disp-quote content-type="editor-comment"><p>p.4 &quot;cell divisions decrease linearly with time&quot; Only later (p.10) it turns out that you think about the density of cell divisions.</p></disp-quote><p>This has been changed to &quot;cell division density decreases linearly with time&quot;.</p><disp-quote content-type="editor-comment"><p>p.5 &quot;imagine is largely in one plane&quot; while below &quot;we generated a 3D z-stack&quot; and above &quot;our in vivo 3D image data&quot; (p.4). Although these statements are not strictly contradictory, I still find them confusing. Eventually, you analyse a 2D image, so I would suggest that you refer to your in vivo data as being 2D.</p></disp-quote><p>We apologise for the confusion here; the imaging data was initially generated using 3D z-stacks but this 3D data is later converted to a 2D focused image, on which the deep learning analysis is performed. We are now more careful with the language in the text.</p><disp-quote content-type="editor-comment"><p>p.7 &quot;We have overcome (...) the standard U-Net model&quot; This paragraph remains rather cryptic to me. Maybe you can explain in two sentences what a U-Net is or state its main characteristics. Is it important to state which class you have used at this point? Similarly, what is the exact role of the ResNet model? What are its characteristics?</p></disp-quote><p>We have included more details on both the ResNet and U-Net models and how our model incorporates properties from them on Page 8.</p><disp-quote content-type="editor-comment"><p>p.8 Table 1 Where do I find it? Similarly, I could not find Table 2.</p></disp-quote><p>These were originally located in the supplemental information document, but have been moved to the main manuscript.</p><disp-quote content-type="editor-comment"><p>p.9 &quot;developing tissue in normal homeostatic conditions&quot; Aren't homeostatic and developing contradictory? In one case you maintain a state, in the other, it changes.</p></disp-quote><p>We agree with the Reviewer and have removed the word ‘homeostatic’.</p><disp-quote content-type="editor-comment"><p>p.9 &quot;Develop additional models&quot; I think 'models' refers to deep learning models, not to physical models of epithelial tissue development. Maybe you can clarify this?</p></disp-quote><p>Yes, this is correct; we have phrased this better in the text.</p><disp-quote content-type="editor-comment"><p>p.12 &quot;median error&quot; median difference to the manually acquired data?</p></disp-quote><p>Yes, and we have made this clearer in the text, too.</p><disp-quote content-type="editor-comment"><p>p.12 &quot;we expected to observe a bias of division orientation along this axis&quot; Can you justify the expectation? Elongated cells are not necessarily aligned with the direction of a uniaxially applied stress.</p></disp-quote><p>Although this is not always the case, we have now included additional references to previous work from other groups which demonstrated that wing epithelial cells do become elongated along the P/D axis in response to tension.</p><disp-quote content-type="editor-comment"><p>p.14 &quot;a rather random orientation&quot; Please, quantify.</p></disp-quote><p>The division orientations are quantified in Fig. 4F,G; we have now changed our description from ‘random’ to ‘unbiased’.</p><disp-quote content-type="editor-comment"><p>p.17 &quot;The theories that must be developed will be statistical mechanical (stochastic) in nature&quot; I do not understand. Statistical mechanics refers to systems at thermodynamic equilibrium, stochastic to processes that depend on, well, stochastic input.</p></disp-quote><p>We have clarified that we are referring to non-equilibrium statistical mechanics (the study of macroscopic systems far from equilibrium, a rich field of research with many open problems and applications in biology).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>In this manuscript, the authors propose a computational method based on deep convolutional neural networks (CNNs) to automatically detect cell divisions in two-dimensional fluorescence microscopy timelapse images. Three deep learning models are proposed to detect the timing of division, predict the division axis, and enhance cell boundary images to segment cells before and after division. Using this computational pipeline, the authors analyze the dynamics of cell divisions in the epithelium of the <italic>Drosophila</italic> pupal wing and find that a wound first induces a reduction in the frequency of division followed by a synchronised burst of cell divisions about 100 minutes after its induction.</p><p>In general, novelty over previous work does not seem particularly important. From a methodological point of view, the models are based on generic architectures of convolutional neural networks, with minimal changes, and on ideas already explored in general. The authors seem to have missed much (most?) of the literature on the specific topic of detecting mitotic events in 2D timelapse images, which has been published in more specialized journals or Proceedings. (TPMAI, CCVPR etc., see references below). Even though the image modality or biological structure may be different (non-fluorescent images sometimes), I don't believe it makes a big difference. How the authors' approach compares to this previously published work is not discussed, which prevents me from objectively assessing the true contribution of this article from a methodological perspective.</p><p>On the contrary, some competing works have proposed methods based on newer - and generally more efficient - architectures specifically designed to model temporal sequences (Phan 2018, Kitrungrotsakul 2019, 2021, Mao 2019, Shi 2020). These natural candidates (recurrent networks, long-short-term memory (LSTM) gated recurrent units (GRU), or even more recently transformers), coupled to CNNs are not even mentioned in the manuscript, although they have proved their generic superiority for inference tasks involving time series (Major point 2). Even though the original idea/trick of exploiting the different channels of RGB images to address the temporal aspect might seem smart in the first place - as it reduces the task of changing/testing a new architecture to a minimum - I guess that CNNs trained this way may not generalize very well to videos where the temporal resolution is changed slightly (Major point 1). This could be quite problematic as each new dataset acquired with a different temporal resolution or temperature may require manual relabeling and retraining of the network. In this perspective, recent alternatives (Phan 2018, Gilad 2019) have proposed unsupervised approaches, which could largely reduce the need for manual labeling of datasets.</p></disp-quote><p>We thank the reviewer for their constructive comments. Our goal is to develop a cell detection method that has a very high accuracy, which is critical for practical and effective application to biological problems. The algorithms need to be robust enough to cope with the difficult experimental systems we are interested in studying, which involve densely packed epithelial cells within in vivo tissues that are continuously developing, as well as repairing. In response to the above comments of the reviewer, we apologise for not including these important papers from the division detection and deep learning literature, which are now discussed in the Introduction (on page 4).</p><p>A key novelty of our approach is the use of multiple fluorescent channels to increase information for the model. As the referee points out, our method benefits from using and adapting existing highly effective architectures. Hence, we have been able to incorporate deeper models than some others have previously used. An additional novelty is using this same model architecture (retrained) to detect cell division orientation. For future practical use by us and other biologists, the models can easily be adapted and retrained to suit experimental conditions, including different multiple fluorescent channels or number of time points. Unsupervised approaches are very appealing due to the potential time saved compared to manual hand labelling of data. However, the accuracy of unsupervised models are currently much lower than that of supervised (as shown in Phan 2018) and most importantly well below the levels needed for practical use analysing inherently variable (and challenging) in vivo experimental data.</p><disp-quote content-type="editor-comment"><p>Regarding the other convolutional neural networks described in the manuscript:</p><p>(1) The one proposed to predict the orientation of mitosis performs a regression task, predicting a probability for the division angle. The architecture, which must be different from a simple Unet, is not detailed anywhere, so the way it was designed is difficult to assess. It is unclear if it also performs mitosis detection, or if it is instead used to infer orientation once the timing and location of the division have been inferred by the previous network.</p></disp-quote><p>The neural network used for U-NetOrientation has the same architecture as U-NetCellDivision10 but has been retrained to complete a different task: finding division orientation. Our workflow is as follows: firstly, U-NetCellDivision10 is used to find cell divisions; secondly, U-NetOrientation is applied locally to determine the division orientation. These points have now been clarified in the main text on Page 14.</p><disp-quote content-type="editor-comment"><p>(2) The one proposed to improve the quality of cell boundary images before segmentation is nothing new, it has now become a classic step in segmentation, see for example Wolny et al. eLife 2020.</p></disp-quote><p>We have cited similar segmentation models in our paper and thank the referee for this additional one. We had made an improvement to the segmentation models, using GFP-tagged E-cadherin, a protein localised in a thin layer at the apical boundary of cells. So, while this is primarily a 2D segmentation problem, some additional information is available in the z-axis as the protein is visible in 2-3 separate z-slices. Hence, we supplied this 3-focal plane input to take advantage of the 3D nature of this signal. This approach has been made more explicit in the text (Pages 14, 15) and Figure (Fig. 2D).</p><disp-quote content-type="editor-comment"><p>As a side note, I found it a bit frustrating to realise that all the analysis was done in 2D while the original images are 3D z-stacks, so a lot of the 3D information had to be compressed and has not been used. A novelty, in my opinion, could have resided in the generalisation to 3D of the deep-learning approaches previously proposed in that context, which are exclusively 2D, in particular, to predict the orientation of the division.</p></disp-quote><p>Our experimental system is a relatively flat 2D tissue with the orientation of the cell divisions consistently in the xy-plane. Hence, a 2D analysis is most appropriate for this system. With the successful application of the 2D methods already achieving high accuracy, we envision that extension to 3D would only offer a slight increase in effectiveness as these measurements have little room for improvement. Therefore, we did not extend the method to 3D here. However, of course, this is the next natural step in our research as 3D models would be essential for studying 3D tissues; such 3D models will be computationally more expensive to analyse and more challenging to hand label.</p><disp-quote content-type="editor-comment"><p>Concerning the biological application of the proposed methods, I found the results interesting, showing the potential of such a method to automatise mitosis quantification for a particular biological question of interest, here wound healing. However, the deep learning methods/applications that are put forward as the central point of the manuscript are not particularly original.</p></disp-quote><p>We thank the referee for their constructive comments. Our aim was not only to show the accuracy of our models but also to show how they might be useful to biologists for automated analysis of large datasets, which is a—if not the—bottleneck for many imaging experiments. The ability to process large datasets will improve robustness of results, as well as allow additional hypotheses to be tested. Our study also demonstrated that these models can cope with real in vivo experiments where additional complications such as progressive development, tissue wounding and inflammation must be accounted for.</p><disp-quote content-type="editor-comment"><p>Major point 1: generalisation potential of the proposed method.</p><p>The neural network model proposed for mitosis detection relies on a 2D convolutional neural network (CNN), more specifically on the Unet architecture, which has become widespread for the analysis of biology and medical images. The strategy proposed here exploits the fact that the input of such an architecture is natively composed of several channels (originally 3 to handle the 3 RGB channels, which is actually a holdover from computer vision, since most medical/biological images are gray images with a single channel), to directly feed the network with 3 successive images of a timelapse at a time. This idea is, in itself, interesting because no modification of the original architecture had to be carried out. The latest 10-channel model (U-NetCellDivision10), which includes more channels for better performance, required minimal modification to the original U-Net architecture but also simultaneous imaging of cadherin in addition to histone markers, which may not be a generic solution.</p></disp-quote><p>We believe we have provided a general approach for practical use by biologists that can be applied to a range of experimental data, whether that is based on varying numbers of fluorescent channels and/or timepoints. We envisioned that experimental biologists are likely to have several different parameters permissible for measurement based on their specific experimental conditions e.g., different fluorescently labelled proteins (e.g. tubulin) and/or time frames. To accommodate this, we have made it easy and clear in the code on GitHub how these changes can be made. While the model may need some alterations and retraining, the method itself is a generic solution as the same principles apply to very widely used fluorescent imaging techniques.</p><disp-quote content-type="editor-comment"><p>Since CNN-based methods accept only fixed-size vectors (fixed image size and fixed channel number) as input (and output), the length or time resolution of the extracted sequences should not vary from one experience to another. As such, the method proposed here may lack generalization capabilities, as it would have to be retrained for each experiment with a slightly different temporal resolution. The paper should have compared results with slightly different temporal resolutions to assess its inference robustness toward fluctuations in division speed.</p></disp-quote><p>If multiple temporal resolutions are required for a set of experiments, we envision that the model could be trained over a range of these different temporal resolutions. Of course, the temporal resolution, which requires the largest vector would be chosen as the model's fixed number of input channels. Given the depth of the models used and the potential to easily increase this by replacing resnet34 with resnet50 or resnet101 the model would likely be able to cope with this, although we have not specifically tested this. (page 27)</p><disp-quote content-type="editor-comment"><p>Another approach (not discussed) consists in directly convolving several temporal frames using a 3D CNN (2D+time) instead of a 2D, in order to detect a temporal event. Such an idea shares some similarities with the proposed approach, although in this previous work (Ji et al. TPAMI 2012 and for split detection Nie et al. CCVPR 2016) convolution is performed spatio-temporally, which may present advantages. How does the authors' method compare to such an (also very simple) approach?</p></disp-quote><p>We thank the Reviewer for this insightful comment. The text now discusses this (on Pages 8 and 17). Key differences between the models include our incorporation of multiple light channels and the use of much deeper models. We suggest that our method allows for an easy and natural extension to use deeper models for even more demanding tasks e.g. distinguishing between healthy and defective divisions. We also tested our method with ‘difficult conditions’ such as when a wound is present; despite the challenges imposed by the wound (including the discussed reduction in fluorescent intensities near the wound edge), we achieved higher accuracy compared to Nie et al. (accuracy of 78.5% compared to our F1 score of 0.964) using a low-density in vitro system.</p><disp-quote content-type="editor-comment"><p>Major point 2: innovatory nature of the proposed method.</p><p>The authors' idea of exploiting existing channels in the input vector to feed successive frames is interesting, but the natural choice in deep learning for manipulating time series is to use recurrent networks or their newer and more stable variants (LSTM, GRU, attention networks, or transformers). Several papers exploiting such approaches have been proposed for the mitotic division detection task, but they are not mentioned or discussed in this manuscript: Phan et al. 2018, Mao et al. 2019, Kitrungrotaskul et al. 2019, She et al 2020.</p><p>An obvious advantage of an LSTM architecture combined with CNN is that it is able to address variable length inputs, therefore time sequences of different lengths, whereas a CNN alone can only be fed with an input of fixed size.</p></disp-quote><p>LSTM architectures may produce similar accuracy to the models we employ in our study, however due to the high degree of accuracy we already achieve with our methods, it is hard to see how they would improve the understanding of the biology of wound healing that we have uncovered. Hence, they may provide an alternative way to achieve similar results from analyses of our data. It would also be interesting to see how LTSM architectures would cope with the noisy and difficult wounded data that we have analysed. We agree with the referee that these alternate models could allow an easier inclusion of difference temporal differences in division time (see discussion on Page 20). Nevertheless, we imagine that after selecting a sufficiently large input time/ fluorescent channel input, biologists could likely train our model to cope with a range of division lengths.</p><disp-quote content-type="editor-comment"><p>Another advantage of some of these approaches is that they rely on unsupervised learning, which can avoid the tedious relabeling of data (Phan et al. 2018, Gilad et al. 2019).</p></disp-quote><p>While these are very interesting ideas, we believe these unsupervised methods would struggle under the challenging conditions within ours and others experimental imaging data. The epithelial tissue examined in the present study possesses a particularly high density of cells with overlapping nuclei compared to the other experimental systems these unsupervised methods have been tested on. Another potential problem with these unsupervised methods is the difficulty in distinguishing dynamic debris and immune cells from mitotic cells. Once again despite our experimental data being more complex and difficult, our methods perform better than other methods designed for simpler systems as in Phan et al. 2018 and Gilad et al. 2019; for example, analysis performed on lower density in vitro and unwounded tissues gave best F1 scores for a single video was 0.768 and 0.829 for unsupervised and supervised respectively (Phan et al. 2018). We envision that having an F1 score above 0.9 (and preferably above 0.95), would be crucial for practical use by biologists, hence we believe supervision is currently still required. We expect that retraining our models for use in other experimental contexts will require smaller hand labelled datasets, as they will be able to take advantage of transfer learning (see discussion on Page 4).</p><p>References :</p><p>We have included these additional references in the revised version of our Manuscript.</p><p>Ji, S., Xu, W., Yang, M., &amp; Yu, K. (2012). 3D convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1), 221-231. &gt;6000 citations</p><p>Nie, W. Z., Li, W. H., Liu, A. A., Hao, T., &amp; Su, Y. T. (2016). 3D convolutional networks-based mitotic event detection in time-lapse phase contrast microscopy image sequences of stem cell populations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 55-62).</p><p>Phan, H. T. H., Kumar, A., Feng, D., Fulham, M., &amp; Kim, J. (2018). Unsupervised two-path neural network for cell event detection and classification using spatiotemporal patterns. IEEE Transactions on Medical Imaging, 38(6), 1477-1487.</p><p>Gilad, T., Reyes, J., Chen, J. Y., Lahav, G., &amp; Riklin Raviv, T. (2019). Fully unsupervised symmetry-based mitosis detection in time-lapse cell microscopy. Bioinformatics, 35(15), 2644-2653.</p><p>Mao, Y., Han, L., &amp; Yin, Z. (2019). Cell mitosis event analysis in phase contrast microscopy images using deep learning. Medical image analysis, 57, 32-43.</p><p>Kitrungrotsakul, T., Han, X. H., Iwamoto, Y., Takemoto, S., Yokota, H., Ipponjima, S., ... &amp; Chen, Y. W. (2019). A cascade of 2.5 D CNN and bidirectional CLSTM network for mitotic cell detection in 4D microscopy image. IEEE/ACM transactions on computational biology and bioinformatics, 18(2), 396-404.</p><p>Shi, J., Xin, Y., Xu, B., Lu, M., &amp; Cong, J. (2020, November). A Deep Framework for Cell Mitosis Detection in Microscopy Images. In 2020 16th International Conference on Computational Intelligence and Security (CIS) (pp. 100-103). IEEE.</p><p>Wolny, A., Cerrone, L., Vijayan, A., Tofanelli, R., Barro, A. V., Louveaux, M., ... &amp; Kreshuk, A. (2020). Accurate and versatile 3D segmentation of plant tissues at cellular resolution. Elife, 9, e57613.</p></body></sub-article></article>