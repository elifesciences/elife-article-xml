<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">52599</article-id><article-id pub-id-type="doi">10.7554/eLife.52599</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Hierarchical temporal prediction captures motion processing along the visual pathway</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-98202"><name><surname>Singer</surname><given-names>Yosef</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4480-0574</contrib-id><email>yossing@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-328401"><name><surname>Taylor</surname><given-names>Luke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4023-4958</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-38220"><name><surname>Willmore</surname><given-names>Ben DB</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2969-7572</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-14601"><name><surname>King</surname><given-names>Andrew J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5180-7179</contrib-id><email>andrew.king@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-19854"><name><surname>Harper</surname><given-names>Nicol S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7851-4840</contrib-id><email>nicol.harper@dpag.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Physiology, Anatomy and Genetics, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e52599</elocation-id><history><date date-type="received" iso-8601-date="2019-10-14"><day>14</day><month>10</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2023-10-04"><day>04</day><month>10</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2019-03-13"><day>13</day><month>03</month><year>2019</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/575464"/></event></pub-history><permissions><copyright-statement>© 2023, Singer, Taylor et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Singer, Taylor et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-52599-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-52599-figures-v2.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.31557" id="ra1"/><abstract><p>Visual neurons respond selectively to features that become increasingly complex from the eyes to the cortex. Retinal neurons prefer flashing spots of light, primary visual cortical (V1) neurons prefer moving bars, and those in higher cortical areas favor complex features like moving textures. Previously, we showed that V1 simple cell tuning can be accounted for by a basic model implementing temporal prediction – representing features that predict future sensory input from past input (Singer et al., 2018). Here, we show that hierarchical application of temporal prediction can capture how tuning properties change across at least two levels of the visual system. This suggests that the brain does not efficiently represent all incoming information; instead, it selectively represents sensory inputs that help in predicting the future. When applied hierarchically, temporal prediction extracts time-varying features that depend on increasingly high-level statistics of the sensory input.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>computational neuroscience</kwd><kwd>neural network model</kwd><kwd>normative model</kwd><kwd>vision</kwd><kwd>dorsal visual pathway</kwd><kwd>receptive fields</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>WT108369/Z/2015/Z</award-id><principal-award-recipient><name><surname>King</surname><given-names>Andrew J</given-names></name><name><surname>Harper</surname><given-names>Nicol S</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>University of Oxford Clarendon Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Singer</surname><given-names>Yosef</given-names></name><name><surname>Taylor</surname><given-names>Luke</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural network modeling shows that hierarchical application of the simple computational principle of predicting future sensory input from its past can capture features of visual motion processing from the retina to the visual cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The temporal prediction (<xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>) framework posits that sensory systems are optimized to represent features in natural stimuli that enable prediction of future sensory input. This would be useful for guiding future action, uncovering underlying variables, and discarding irrelevant information (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>). Temporal prediction relates to a class of principles, such as the predictive information bottleneck (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib98">Salisbury and Palmer, 2016</xref>) and slow feature analysis (<xref ref-type="bibr" rid="bib10">Berkes and Wiskott, 2005</xref>), that similarly involve selectively encoding <italic>only</italic> features that are efficiently predictive of the future. This class of principles has been contrasted (<xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib98">Salisbury and Palmer, 2016</xref>) with other principles that are more typically used to explain sensory coding – efficient coding (<xref ref-type="bibr" rid="bib4">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib6">Bell and Sejnowski, 1997</xref>), sparse coding (<xref ref-type="bibr" rid="bib79">Olshausen and Field, 1996</xref>), and predictive coding (<xref ref-type="bibr" rid="bib38">Huang and Rao, 2011</xref>; <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>) – that aim instead to efficiently represent <italic>all</italic> current and perhaps past input. Although these principles have been successful in accounting for various visual receptive field (RF) properties in V1 (<xref ref-type="bibr" rid="bib6">Bell and Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bib10">Berkes and Wiskott, 2005</xref>; <xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib79">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>), no single principle has so far been able to explain the diverse spatiotemporal tuning involved in motion processing that emerges along the visual stream, from retina to V1 to the extrastriate middle temporal area (MT).</p><p>Any general principle of visual encoding needs to explain temporal aspects of neural tuning – the encoding of visual scenes in motion rather than static images. It is also important that any general principle is largely unsupervised. Some features of the visual system have been reproduced by deep supervised network models optimized for image classification using large labeled datasets (e.g. images labeled as cat, dog, car; <xref ref-type="bibr" rid="bib136">Yamins and DiCarlo, 2016</xref>). While these models can help to explain the RF properties of the likely hard-wired retina (<xref ref-type="bibr" rid="bib62">Lindsey et al., 2019</xref>), they are less informative if neuronal tuning is influenced by experience, as in cortex, since most sensory input is unlabeled except for sporadic reinforcement signals. The temporal prediction approach is unsupervised (i.e. it requires no labeled data), and inherently applies to the temporal domain.</p><p>Several theoretical studies have proposed prediction of future input as a primary function of neural systems (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="bib34">Hawkins and Blakeslee, 2004</xref>; <xref ref-type="bibr" rid="bib82">O’Reilly et al., 2014</xref>; <xref ref-type="bibr" rid="bib116">Softky, 1996</xref>). However, models incorporating temporal prediction that explicitly attempt, with varying degrees of success, to explain features of sensory representations from natural inputs have only very recently started to emerge (<xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>; <xref ref-type="bibr" rid="bib77">Ocko et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">Palm, 2012</xref>; <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>). We have previously shown that a simple non-hierarchical model instantiating temporal prediction can account for temporal aspects of V1 simple cell RFs (<xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>). This model finds features in its inputs that are predictive of future input. However, the pattern of occurrence of these particular predictive features over time may itself have a predictable structure. Hence a second temporal prediction model could be stacked atop the first to find this structure. This could be done repeatedly so as to find the predictable structure of increasingly high-level statistics of the input (<xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>An illustration of the general architecture of the hierarchical temporal prediction model.</title><p>(<bold>a</bold>) Architecture of the hierarchical temporal prediction model. Each stack of the model (labeled ‘predictive feature extractor’) receives input from the stack below and finds features of its input that are predictive of its future input, with the lowest stack predicting the raw natural inputs. The predictive signal is fed forward to the next layer. (<bold>b</bold>) Operations performed by each stack of the temporal prediction model. Each stack receives a time-varying input vector <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which in all but the first stack is the hidden unit activity vector <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> from the lower stack. For the first stack, the input <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is raw visual input (i.e. video). The input, also with some additional delayed input, passes through feedforward input weight matrix <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and undergoes a nonlinear transformation <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to generate hidden unit activity <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each stack is optimized so that a linear transformation <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> from its hidden unit activity <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and then a delay, generates an accurate estimate <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of its future input (at time <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Because of the delay, the predicted input is in the future relative to the input that generated the hidden unit activity. This delay is shown at the end of processing, but is likely distributed throughout the system. For each stack, the prediction error is only used to train the input and output weights <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The prediction error can equivalently be written as  <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To see how this hierarchical temporal prediction model differs from predictive coding (<xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>) and PredNet (<xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>) see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Architecture and operations performed by temporal prediction, predictive coding and PredNet models.</title><p>(<bold>a</bold>) Architecture of the hierarchical temporal prediction model. Each stack of the model (labeled ‘predictive feature extractor’) receives input from the stack below and finds features of its input that are predictive of its future input, with the lowest stack predicting the raw natural inputs. The predictive signal is fed forward to the next layer. (<bold>b</bold>) Operations performed by each stack of the temporal prediction model. See the caption of <xref ref-type="fig" rid="fig1">Figure 1</xref> for more details. (<bold>c</bold>) General architecture of <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref> hierarchical predictive coding model. At each layer (labeled ‘predictive estimator’), the model generates an estimate of its input which is fed back down to the previous layer. Top-down predictions from the layer above are subtracted from the current estimate and the error signal is fed forward to the next layer. (<bold>d</bold>) Operations performed at each layer of Rao and Ballard’s predictive coding model. The input <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> undergoes a linear transformation <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mo>⊺</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> to generate hidden unit representation <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is used to generate an estimate <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of the input at the same time step. An error neuron computes the difference between the hidden representation <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and its estimate <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> from the layer above. This error is passed forward as input to the next layer. (<bold>e</bold>), General architecture of the PredNet model (<xref ref-type="bibr" rid="bib64">Lotter et al., 2016</xref>). At each layer, the model takes in the prediction error from the previous layer as input, with the bottom layer receiving the raw video frame. This layer then tries to predict the future of this input given the past error signal, past hidden unit activity and top-down input from the layer above. The prediction error (between the true future input and the model’s estimate of the input) are fed forward to the next layer, which in turn attempts to predict its future in the same manner. The layer above feeds back its hidden unit activity to the layer below. (<bold>f</bold>) Operations performed by each layer (labeled ‘Predictive estimator’ in (<bold>e</bold>)) of the PredNet model. In brief, the model receives a time-varying input <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which at the lowest layer is the raw video input and at all other layers is the prediction error from the layer below. The model then generates a hidden representation <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> from a combination of the prediction error at the previous time-step <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the hidden representation at the previous time-step <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and the current hidden unit activity in the layer above <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. This hidden unit activity is used to generate a prediction <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of the input <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is in the future relative to the past hidden unit activity and prediction error. The prediction error <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is fed forward to the next layer, where it serves as the input to be predicted. The hidden unit activity <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is fed down to the layer below, where it becomes <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>c, d</bold>) adapted from <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Here, we have developed such a hierarchical form of the temporal prediction model that predicts increasingly high-level statistics of natural dynamic visual input (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This hierarchical model stands in contrast to the influential hierarchical predictive-coding related approaches that transmit forward the prediction <italic>error</italic> (<xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>; <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>), rather than passing forward the predictive <italic>signal</italic> (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Our model accounts not only for linear tuning properties of V1 simple cells, as in previous non-hierarchical temporal prediction models (<xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>), but also for the diversity of linear and non-linear spatiotemporal tuning properties that emerge along the visual pathway. In particular, the temporal tuning properties in successive hierarchical stages of the model progress from those resembling magnocellular and parvocellular neurons at early levels of visual processing, to direction-selective simple and complex cells in V1, and finally to a few units that are sensitive to two-dimensional features of motion, as seen in pattern-selective cells in higher visual cortex. The capacity of this model to explain spatiotemporal tuning properties at several levels of the visual pathway using iterated application of a single process, suggests that optimization for temporal prediction may be a fundamental principle of sensory neural processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The hierarchical temporal prediction model</title><p>We instantiated temporal prediction as a hierarchical model consisting of stacked feedforward single-hidden-layer convolutional neural networks (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Image statistics tend to be similar across space, so convolution allows the networks to be efficiently trained and implemented while providing units in deeper stacks the capacity to receive input from a wide span of the image. The first stack was trained using backpropagation to predict the immediate future frame (40ms) of unfiltered natural video inputs from the previous 5 frames (200ms). Each subsequent stack was then trained to predict the future hidden-unit activity of the stack below it from the past activity in response to the natural video inputs. L<sub>1</sub> regularization was applied to the weights of each stack, akin to a constraint on neural ‘wiring’.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Hierarchical temporal prediction model in detail, with convolution.</title><p>Schematic of model architecture, illustrating the stacking and convolution. This model is essentially the same as the model in <xref ref-type="fig" rid="fig1">Figure 1</xref> but using convolutional networks; the notation used here is slightly different from <xref ref-type="fig" rid="fig1">Figure 1</xref> due to this. Each stack is a single hidden-layer feedforward convolutional network, which is trained to predict the future time-step of its input from the previous 5 time-steps. Each stack consists of a bank of <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> convolutional hidden units, where a convolutional hidden unit is a set of hidden units that each have the same weights (the same convolution kernels). The first stack is trained to predict future pixels of natural video inputs, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, from their past. Subsequent stacks are trained to predict future time-steps of the hidden-layer activity in the stack below, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, based on their past responses to the same natural video inputs. For each convolutional hidden unit <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in a stack, its input <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from convolutional hidden unit <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the stack below is convolved with an input weight kernel <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, summed over <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and then rectified, to produce its hidden unit activity <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This activity is next convolved with output weight kernels <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and summed over <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to provide predictions <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of the immediate future input <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of each convolutional hidden unit, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><italic>,</italic> in the stack below. Note, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is just <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> shifted one time step into the future, with <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The weights (<inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) of each stack are then trained by backpropagation to minimize the difference between the predicted future <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the true future, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each stack is trained separately, starting with the lowest stack, which, once trained, provides input for the next stack, which is then trained, and so on. The input and hidden stages of all the stacks when stacked together form a deep feedforward network. Note, to avoid clutter, only one time slice <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are shown. In the convolution model presented here, every bold capitalized variable is a 3D-tensor over space (<inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), space (<inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and time (<inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), and every italicized variable is a scalar, either a real number or an integer. The bracketed superscript on variables denotes the stack number <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> up to top stack <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig2-v2.tif"/></fig><p>The video inputs consisted of animals moving in natural environments, filmed with panning, still and occasionally zooming cameras. Unlike in <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>, we did not bandpass filter the videos in the manner of <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>. The four stacks respectively used 50, 100, 200, and 400 convolutional units, each having a particular convolutional kernel. The convolutional units can also be seen as a set of hidden units, with each hidden unit having the same weights relative to its position in the space of the video frame. However, because we only use valid convolutions, the number of spatial positions decreases with each stack (see <xref ref-type="table" rid="table1">Table 1</xref>, Methods). This results in 14,450, 22,500, 33,800, and 48,400 hidden units over all spatial positions in each stack, respectively. Hereafter, the term model units refers to the properties of the convolutional units of the model, unless noted otherwise.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Model parameter settings for each stack.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Stack</th><th align="left" valign="bottom">Input size (X,Y,T,I)</th><th align="left" valign="bottom">Hidden layer size</th><th align="left" valign="bottom">Kernel size (X’,Y’,T’)</th><th align="left" valign="bottom">Spatial and temporal extent</th><th align="left" valign="bottom">Stride (s<sub>1</sub>,s<sub>2</sub>,s<sub>3</sub>)</th><th align="left" valign="bottom">Number of convolutional hidden units</th><th align="left" valign="bottom">Learning rate (α)</th><th align="left" valign="bottom">L<sub>1</sub> regularization strength (λ)</th></tr></thead><tbody><tr><td align="left" valign="bottom">1</td><td align="left" valign="bottom">181x18x20x1</td><td align="left" valign="bottom">17x17x16x50</td><td align="left" valign="bottom">21x21x5</td><td align="left" valign="bottom">21x21x5</td><td align="left" valign="bottom">10,10,1</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">10<sup>–2</sup></td><td align="left" valign="bottom">10<sup>-4.5</sup></td></tr><tr><td align="left" valign="bottom">2</td><td align="left" valign="bottom">17x17x16x50</td><td align="left" valign="bottom">15x15x12x100</td><td align="left" valign="bottom">3x3x5</td><td align="left" valign="bottom">41x41x9</td><td align="left" valign="bottom">1,1,1</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">10<sup>–4</sup></td><td align="left" valign="bottom">10<sup>–6</sup></td></tr><tr><td align="left" valign="bottom">3</td><td align="left" valign="bottom">15x15x12x100</td><td align="left" valign="bottom">13x13x8x200</td><td align="left" valign="bottom">3x3x5</td><td align="left" valign="bottom">61x61x13</td><td align="left" valign="bottom">1,1,1</td><td align="left" valign="bottom">200</td><td align="left" valign="bottom">10<sup>–4</sup></td><td align="left" valign="bottom">10<sup>–6</sup></td></tr><tr><td align="left" valign="bottom">4</td><td align="left" valign="bottom">13x13x8x200</td><td align="left" valign="bottom">11x11x4x400</td><td align="left" valign="bottom">3x3x5</td><td align="left" valign="bottom">81x81x17</td><td align="left" valign="bottom">1,1,1</td><td align="left" valign="bottom">400</td><td align="left" valign="bottom">10<sup>–4</sup></td><td align="left" valign="bottom">10<sup>–6</sup></td></tr></tbody></table></table-wrap><p>After training, we examined the response properties of the units in the model and compared them to measured response properties in the visual processing pathway. The trained model is very rich, with diverse properties. Here, we have chosen to focus mostly on those properties relating to motion sensitivity. However, we expect there to be many other response properties of the model units that could also be examined and compared to other aspects of the biology. The construction of this model has not been tailored to any specific species and aims to capture general features common across mammals. However, for consistency, throughout the Results, we compare the tuning properties of our model units mostly to those of macaque visual neurons.</p></sec><sec id="s2-2"><title>Temporal prediction of natural inputs produces retinal-like units</title><p>After training, we first examined the input weights of the units in the first stack. Each hidden unit can be viewed as a linear non-linear (LN) model (<xref ref-type="bibr" rid="bib18">Chichilnisky, 2001</xref>; <xref ref-type="bibr" rid="bib108">Simoncelli et al., 2004</xref>), as commonly used to describe neuronal RFs. With L<sub>1</sub> regularization slightly above the optimum for prediction, the RFs of the units showed spatially localized, center-surround tuning with a decaying temporal envelope, characteristic of retinal and lateral geniculate nucleus (LGN) neurons (<xref ref-type="bibr" rid="bib3">Barlow, 1953</xref>; <xref ref-type="bibr" rid="bib58">Kuffler, 1953</xref>; <xref ref-type="bibr" rid="bib105">Shapley and Perry, 1986</xref>). The model units’ RFs have either an excitatory (ON) or inhibitory (OFF) blob-like structure at the 0 ms time-step, often with a surround of opposing sign in the same or previous time-step (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Both ON and OFF units can have either small RFs that do not change polarity over time (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, units 1–4; <xref ref-type="fig" rid="fig3">Figure 3b</xref>, bottom left), or large RFs that switch polarity over time (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, units 5–6; <xref ref-type="fig" rid="fig3">Figure 3b</xref>, top right). This is reminiscent of the four main cell types in the primate retina and LGN: the parvocellular pathway ON/OFF neurons and the more change-sensitive magnocellular pathway ON/OFF neurons, respectively (<xref ref-type="bibr" rid="bib105">Shapley and Perry, 1986</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>RFs of trained first stack of the model show retina-like tuning.</title><p>(<bold>a</bold>) Example RFs with center-surround tuning characteristic of neurons in retina and LGN. RFs are small and do not switch polarity over time (units 1–4) or large and switch polarity (units 5–6), resembling cells along the parvocellular and magnocellular pathways, respectively. (<bold>b</bold>) RF size plotted against proportion of the weights (pixels) in the RF that switch polarity over the course of the most recent two timesteps. Units in <bold>a</bold> labeled and shown in orange. (<bold>c</bold>) Effect of changing regularization strength on the qualitative properties of RFs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig3-v2.tif"/></fig><p>Interestingly, simply decreasing L<sub>1</sub>-regularization strength causes the model RFs to change from center-surround tuning to Gabor-like tuning, resembling localized, oriented bars that shift over time (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). It is possible that this balance, between a code that is optimal for prediction and one that prioritizes efficient wiring, might underlie differences in the retina and LGN of different species. The retina of mice and rabbits contains many neurons with oriented and direction-tuned RFs, whereas cats and macaques mostly have center-surround RFs (<xref ref-type="bibr" rid="bib101">Scholl et al., 2013</xref>). Efficient retinal wiring may be more important in some species, due, for example, to different constraints on the width of the optic nerve or different impacts of light scattering by superficial retinal cell layers.</p></sec><sec id="s2-3"><title>Hierarchical temporal prediction produces simple and complex cell tuning</title><p>Using the trained center-surround-tuned network as the first stack, a second stack was added to the model and trained. The output of each second stack unit results from a linear-nonlinear-linear-nonlinear transformation of the visual input, and hence we estimated their RFs by reverse correlation with binary noise input. Many of the resulting RFs were Gabor-like over space (<xref ref-type="fig" rid="fig4">Figure 4a and b</xref>, I-II), resembling those of V1 simple cells (<xref ref-type="bibr" rid="bib39">Hubel and Wiesel, 1959</xref>; <xref ref-type="bibr" rid="bib40">Hubel and Wiesel, 1965</xref>; <xref ref-type="bibr" rid="bib41">Hubel and Wiesel, 1968</xref>). The RF envelopes decayed into the past, and often showed spatial shifts or polarity changes over time, indicating direction or flicker sensitivity, as is also seen in V1 (<xref ref-type="bibr" rid="bib23">DeAngelis et al., 1993</xref>; <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>, I; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Using full-field drifting sinusoidal gratings (<xref ref-type="fig" rid="fig4">Figure 4a and b</xref>, III-IV), we found that most units were selective for stimulus orientation, spatial and temporal frequency (<xref ref-type="fig" rid="fig4">Figure 4a and b</xref>, V-VII), and some were also direction selective (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). Responses to the optimal grating typically oscillate over time between a maximum when the grating is in phase with the RF and 0 when the grating is out of phase (<xref ref-type="fig" rid="fig4">Figure 4a and b</xref>, IV). These response characteristics are typical of V1 simple cells (<xref ref-type="bibr" rid="bib69">Movshon et al., 1978a</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Qualitative tuning properties of example model units in stack 2.</title><p>(<bold>a, b</bold>) Tuning properties of two example units from the 2nd stack of the model, including (I) the linear RF and (II) the Gabor fitted to the most recent time-step of the linear RF. (III) the drifting grating that best stimulates this unit. (IV) the amplitude of the unit’s response to this grating over time. Red line: unit response; blue line: best-fitting sinusoid; gray dashed line: response to blank stimulus, which is zero and hence obscured by the x-axis. Note that the response (red line) is sometimes obscured by a perfectly overlapping best-fitting sinusoid (blue line). (V) The unit’s mean response over time plotted against orientation (in degrees) for gratings presented at its optimal spatial and temporal frequency. (VI,VII) Tuning curves showing the joint distribution of responses to (VI) orientation (in degrees) and spatial frequency (in cycles/pixel) at the preferred temporal frequency and to (VII) orientation and temporal frequency (in Hz) at the preferred spatial frequency. In VI and VII the color represents the mean response over time to the grating presented. For more example units, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Tuning properties of example units in stack 2.</title><p>(<bold>a–h</bold>), I-VII as in <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig4-figsupp1-v2.tif"/></fig></fig-group><p>In the third and fourth stack, we followed the same procedures as in the second stack. Most of these units are also tuned for orientation, spatial frequency, temporal frequency and in some cases for direction (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>, V-VII; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref> and <xref ref-type="fig" rid="fig5s2">2</xref>). However, while some units resembled simple cells, most resembled the complex cells of V1 and extrastriate visual areas (such as V2/V3/V4/MT; <xref ref-type="bibr" rid="bib40">Hubel and Wiesel, 1965</xref>; <xref ref-type="bibr" rid="bib37">Hu et al., 2018</xref>). Complex cells are tuned for orientation and spatial and temporal frequency, but are relatively invariant to the phase of the optimal grating <xref ref-type="bibr" rid="bib70">Movshon et al., 1978b</xref>; each cell’s response to its optimal grating has a high mean value and changes little with the grating’s phase (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>, III-IV). Whether a neuron is assigned as simple or complex is typically based on the modulation ratio in such plots (&lt;1 indicates complex; <xref ref-type="bibr" rid="bib114">Skottun et al., 1991</xref>). Model units with low modulation ratios had little discernible structure in their RFs (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>, I-II), another characteristic feature of V1 complex cells (<xref ref-type="bibr" rid="bib95">Rust et al., 2005</xref>; <xref ref-type="bibr" rid="bib104">Schwartz et al., 2006</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Qualitative tuning properties of example model units in stack 3.</title><p>(<bold>a, b</bold>)<italic>,</italic> As in <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>. Note that <bold>a</bold>(II) and <bold>b</bold>(II) when blank indicate that a Gabor could not be well fitted to the RF. Also, note that in <bold>a</bold>(IV) and <bold>b</bold>(IV) the response to a blank stimulus is visible as a gray line, whereas in the corresponding plots in <xref ref-type="fig" rid="fig4">Figure 4</xref> this response is zero. For more example units, see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. Example units from stack 4 are shown in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Tuning properties of example units in stack 3.</title><p>(<bold>a–h</bold>), I-VII as in <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Tuning properties of example units in stack 4.</title><p>(<bold>a–h</bold>), I-VII as in <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig5-figsupp2-v2.tif"/></fig></fig-group><p>We do not claim that any stack in the model corresponds directly to any specific stage along the visual pathway, rather that the order that tuning properties emerge as one moves up the stacks resembles the order that they emerge as one moves up the visual pathway. For example, it can be argued that at least some of the units in stack 4 may correspond to units seen in higher visual areas. With this caveat in mind, we quantified the tuning characteristics of units in stacks 2–4 and compared them to published V1 data (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>; <xref ref-type="fig" rid="fig6">Figure 6a–j</xref>). Simple cells have high modulation ratios and typically have RFs that can be well approximated by Gabor functions (<xref ref-type="bibr" rid="bib48">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="bib114">Skottun et al., 1991</xref>), while complex cells have low modulation ratios and typically have unstructured RFs (<xref ref-type="bibr" rid="bib95">Rust et al., 2005</xref>; <xref ref-type="bibr" rid="bib104">Schwartz et al., 2006</xref>). We included all putative simple cell-like units that responded to drifting gratings (model units with modulation ratios &gt;1, whose RFs could be fitted by Gabors; <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>; see Methods) and all putative complex cell-like units that responded to drifting gratings (model units with modulation ratios &lt;1) from stacks 2–4 in this analysis.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Quantitative tuning properties of model units in stacks 2–4 in response to drifting sinusoidal gratings and corresponding measures of macaque V1 neurons.</title><p>(<bold>a–d</bold>) Histograms showing tuning properties of model and macaque V1 (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>) neurons as measured using drifting gratings. Units from stacks 2–4 are plotted on top of each other to also show the distribution over all stacks; this is because no stack can be uniquely assigned to V1. Modulation ratio measures how simple or complex a cell is; orientation bandwidth and circular variance are measures of orientation selectivity. (<bold>e, f, g</bold>) Joint distributions of tuning measures for model units. (<bold>h, i</bold>) Joint distributions of tuning measures for V1 data; note similarity to <bold>e</bold> and <bold>f</bold>. (<bold>j</bold>) Distribution of direction selectivity for model units (using direction selectivity index 1, see Methods). (<bold>k,l</bold>) Distribution of direction selectivity for model units and macaque V1 (<bold>k</bold> from <xref ref-type="bibr" rid="bib25">De Valois et al., 1982</xref>; <bold>l</bold> from <xref ref-type="bibr" rid="bib100">Schiller et al., 1976</xref>) using direction selectivity index 2. (<bold>m</bold>) Distribution of direction selectivity for model units and for cat V1 (from <xref ref-type="bibr" rid="bib33">Gizzi et al., 1990</xref>) using direction selectivity index 3. In all cases, for units with modulation ratios &gt;1, only units whose linear RFs could be well fitted by Gabors were included in the analysis. To see the spatial RFs of these units and their Gabor fits, see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, and for those units not well fitted by Gabors see <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>. Some units had very small responses to drifting gratings compared to other units in the population. Units whose mean response was &lt;0.1% of the maximum for the population were excluded from analysis. To see the RFs and Gabor fits and quantitative properties of the units of the present-predicting and the weight-shuffled control models, see <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplements 3</xref>–<xref ref-type="fig" rid="fig6s6">6</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Linear RFs of model units that could be well fitted by Gabors and corresponding Gabors.</title><p>(<bold>a</bold>), Most recent time-step of linear RFs of model units in stack 2. (<bold>b</bold>), Corresponding Gabor fits to each RF shown in <bold>a</bold>. (<bold>c, d</bold>) as in <bold>a</bold>,<bold>b</bold> for units in stack 3. (<bold>e, f</bold>) as in <bold>a</bold>,<bold>b</bold> for units in stack 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Linear RFs of model units that could not be well fitted by Gabors and corresponding Gabors.</title><p>(<bold>a</bold>), Most recent time-step of linear RFs of model units in stack 2. (<bold>b</bold>), Corresponding Gabor fits to each RF shown in <bold>a</bold>. (<bold>c, d</bold>) as in <bold>a</bold>,<bold>b</bold> for units in stack 3. (<bold>e, f</bold>) as in (<bold>a,b</bold>) for units in stack 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Linear RFs of model units and corresponding Gabors for stacked autoencoding (present-predicting) model.</title><p>(<bold>a</bold>) Most recent time-step of linear RFs of model units in stack 2. (<bold>b</bold>) Corresponding Gabor fits to each RF shown in <bold>a</bold>. (<bold>c, d</bold>) as in <bold>a</bold>,<bold>b</bold> for units in stack 3. (<bold>e, f</bold>) as in <bold>a</bold>,<bold>b</bold> for units in stack 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-figsupp3-v2.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 4.</label><caption><title>Quantitative tuning properties of stacked autoencoding (present-predicting) model units in stacks 2–4 in response to drifting sinusoidal gratings and corresponding measures of macaque V1 neurons.</title><p>(<bold>a–d</bold>) Histograms showing tuning properties of model and macaque V1 neurons as measured using drifting gratings. Units from stacks 2–4 are plotted on top of each other to also show the distribution over all stacks; this is because no stack can be uniquely assigned to V1. Modulation ratio measures how simple or complex a cell is; orientation bandwidth and circular variance are measures of orientation selectivity. (<bold>e, f</bold>) Joint distributions of tuning measures for model units. (<bold>g</bold>) Distribution of direct selectivity for model units. (<bold>h, i</bold>) Joint distributions of tuning measures for V1 data; note similarity to e and f. (<bold>j</bold>) Joint distribution of direction selectivity and modulation ratio for model units.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-figsupp4-v2.tif"/></fig><fig id="fig6s5" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 5.</label><caption><title>Linear RFs of model units and corresponding Gabors for the model with the input weights to each unit in the network shuffled across space and time.</title><p>(<bold>a</bold>) Most recent time-step of linear RFs of model units in stack 2. (<bold>b</bold>) Corresponding Gabor fits to each RF shown in a. (<bold>c, d</bold>) as in a,b for units in stack 3. (<bold>e, f</bold>) as in a,b for units in stack 4.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-figsupp5-v2.tif"/></fig><fig id="fig6s6" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 6.</label><caption><title>Quantitative tuning properties of the model with input weights to each unit shuffled across space and time for units in stacks 2–4 in response to drifting sinusoidal gratings and corresponding measures of macaque V1 neurons.</title><p>(<bold>a–d</bold>) Histograms showing tuning properties of model and macaque V1 neurons as measured using drifting gratings. Units from stacks 2–4 are plotted on top of each other to also show the distribution over all stacks; this is because no stack can be uniquely assigned to V1. Modulation ratio measures how simple or complex a cell is; orientation bandwidth and circular variance are measures of orientation selectivity. (<bold>e, f</bold>) Joint distributions of tuning measures for model units. (<bold>g</bold>) Distribution of direct selectivity for model units. (<bold>h, i</bold>) Joint distributions of tuning measures for V1 data; note similarity to e and f. (<bold>j</bold>) Joint distribution of direction selectivity and modulation ratio for model units.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig6-figsupp6-v2.tif"/></fig></fig-group><p>The distribution of modulation ratios is bimodal in both macaque monkey V1 (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>; <xref ref-type="bibr" rid="bib114">Skottun et al., 1991</xref>) and our model (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). Both model and real neurons were typically orientation selective, but with the model units having weaker tuning as measured by orientation bandwidth (median data [<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>]: 23.5°, model: 37.5°; <xref ref-type="fig" rid="fig6">Figure 6b</xref>) and circular variance (median data [<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>]: simple cells 0.45, complex cells 0.66; median model: simple cells 0.47, complex cells 0.85; <xref ref-type="fig" rid="fig6">Figure 6c and d</xref>). Orientation-tuned units (circular variance &lt;0.9 or orientation bandwidth &lt;150°) and direction-tuned units (direction selectivity &gt;0.1) in the second stack were mostly simple (modulation ratios &gt;1), whereas those in subsequent stacks became increasingly complex (<xref ref-type="fig" rid="fig6">Figure 6a and c–f</xref>). Circular variance varied with the modulation ratio in a similar way in both model (<xref ref-type="fig" rid="fig6">Figures 6c–e</xref>) and recorded V1 data (<xref ref-type="fig" rid="fig6">Figure 6c–d and h</xref>). A similar trend between model and data was also seen for orientation bandwidth and modulation ratio (<xref ref-type="fig" rid="fig6">Figure 6f and i</xref>).</p><p>Model units showed a range of direction selectivity preferences (<xref ref-type="fig" rid="fig6">Figure 6g and j</xref>) using a common direction selectivity index (DSI1, <xref ref-type="bibr" rid="bib91">Rochefort et al., 2011</xref>). Simple cell-like units (60% with DSI1 ≥0.5; n=95; <xref ref-type="fig" rid="fig6">Figure 6g</xref>) tended to be more direction tuned than complex cell-like units (19% with DSI1 ≥0.5, n=205; <xref ref-type="fig" rid="fig6">Figure 6g</xref>), as is seen in cat V1 (<xref ref-type="bibr" rid="bib54">Kim and Freeman, 2016</xref>). The distribution of direction selectivity in the model units was dominated by peaks at each extreme, with cells tending to be highly direction selective or non-direction selective, although also with a slight central peak of moderately selective units (<xref ref-type="fig" rid="fig6">Figure 6j</xref>). A similar trend is seen for a related measure of directionality selectivity (DSI2), and this trend is also evident in macaque V1 data (<xref ref-type="bibr" rid="bib100">Schiller et al., 1976</xref>; <xref ref-type="bibr" rid="bib25">De Valois et al., 1982</xref>; <xref ref-type="fig" rid="fig6">Figure 6k and l</xref>) with peaks at the two extremes, but a less evident central peak. Using a third direction selectivity index (DSI3), which is more sensitive due to subtracting out the response to a null stimulus, a dominant peak of direction-selective neurons is seen in the model units (at a value of ~1) and some units are even seen to be inhibited by the direction orthogonal to the preferred direction (DSI3 values &gt;1). This is also observed in cat V1 data (<xref ref-type="bibr" rid="bib33">Gizzi et al., 1990</xref>; <xref ref-type="fig" rid="fig6">Figure 6m</xref>).</p><p>We can summarize the kinds of units found as we progress from stack 1 to stack 4 by assigning each unit to a category. Taking all units that responded to drifting gratings, we defined orientation-selective simple-cell units as above (modulation ratio &gt;1, Gabor fit correlation coefficient &gt;0.4), but also with circular variance &lt;0.9, and orientation-selective complex-cell units as above (modulation ratio &lt;1), but also with circular variance &lt;0.9; any remaining units are defined as non-orientation-selective (e.g. the center-surround units of stack 1). A clear progression from non-orientation-selective, to simple-cell-like, to complex-cell-like is seen in the units as one progresses up the stacks of the model (<xref ref-type="fig" rid="fig7">Figure 7</xref>). This bears some resemblance to the progression from the retina and LGN, which has few orientation-selective neurons, at least in cats and monkeys (<xref ref-type="bibr" rid="bib3">Barlow, 1953</xref>; <xref ref-type="bibr" rid="bib58">Kuffler, 1953</xref>; <xref ref-type="bibr" rid="bib105">Shapley and Perry, 1986</xref>), to the geniculorecipient granular layer of V1, which tends to contain more simple cells, to the superficial layers of V1, where more complex cells have been found (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>; <xref ref-type="bibr" rid="bib19">Cloherty and Ibbotson, 2015</xref>). This is also consistent with the substantial increase in the proportion of complex cells from V1 to extrastriate visual areas, such as V2 (<xref ref-type="bibr" rid="bib19">Cloherty and Ibbotson, 2015</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Progression of tuning properties across stacks of the hierarchical temporal prediction model.</title><p>Proportion of simple (red), complex (purple) and non-orientation-selective (gray) units in each stack of the model as measured by responses to full-field drifting sinusoidal gratings. Simple cells are defined as those that can be well fitted by Gabors (pixel-wise correlation coefficient &gt;0.4), are orientation-tuned (circular variance &lt;0.9) and have a modulation ratio &gt;1. Complex cells are defined as those that are orientation-tuned (circular variance &lt;0.9) and have a modulation ratio &lt;1. Crossed lines show number of direction-tuned simple (red) and complex (purple) units in each stack of the model. Direction-tuned units are simple and complex cells (as defined above) that additionally have a direction selectivity index &gt;0.5. Also shown is the proportion of pattern-motion-selective units (yellow) as measured by drifting plaids.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig7-v2.tif"/></fig></sec><sec id="s2-4"><title>Some model units are tuned to two-dimensional features of visual motion</title><p>Simple and complex cells extract many dynamic features from natural scenes. However, their small bandpass RFs prevent individual neurons from tracking the motion of objects because of the aperture problem; the direction of motion of an edge is ambiguous, with only the component of motion perpendicular to the cell’s preferred orientation being represented. Pattern-selective neurons solve this problem, likely by integrating over input from many direction-selective V1 complex cells (<xref ref-type="bibr" rid="bib71">Movshon et al., 1985</xref>; <xref ref-type="bibr" rid="bib72">Movshon and Newsome, 1996</xref>; <xref ref-type="bibr" rid="bib96">Rust et al., 2006</xref>; <xref ref-type="bibr" rid="bib107">Simoncelli and Heeger, 1998</xref>; <xref ref-type="bibr" rid="bib137">Zeki, 1974</xref>), and hence respond selectively to the motion of patterns as a whole. These neurons have been found in macaque extrastriate cortical area MT (<xref ref-type="bibr" rid="bib71">Movshon et al., 1985</xref>; <xref ref-type="bibr" rid="bib92">Rodman and Albright, 1989</xref>; <xref ref-type="bibr" rid="bib96">Rust et al., 2006</xref>), cat anterior ectosylvian visual area (AEV) (<xref ref-type="bibr" rid="bib99">Scannell et al., 1996</xref>), ferret posterior suprasylvian sulcus (PSS) (<xref ref-type="bibr" rid="bib60">Lempel and Nielsen, 2019</xref>), and even in mouse V1 (<xref ref-type="bibr" rid="bib85">Palagina et al., 2017</xref>).</p><p>To investigate pattern selectivity in our model units, we measured their responses to drifting plaids, comprising two superimposed drifting sinusoidal gratings with different orientations. The net direction of the plaid movement lies midway between these two orientations (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). In V1 and MT, component-selective cells respond maximally when the plaid is oriented such that either one of its component gratings moves in the preferred direction of the cell (as measured by a drifting grating). This results in two peaks in plaid-direction tuning curves (<xref ref-type="bibr" rid="bib71">Movshon et al., 1985</xref>; <xref ref-type="bibr" rid="bib96">Rust et al., 2006</xref>; <xref ref-type="bibr" rid="bib115">Smith et al., 2005</xref>). Conversely, pattern-selective cells have a single peak in their direction tuning curves, when the plaid’s direction of movement aligns with the preferred direction of the cell (<xref ref-type="bibr" rid="bib71">Movshon et al., 1985</xref>; <xref ref-type="bibr" rid="bib96">Rust et al., 2006</xref>; <xref ref-type="bibr" rid="bib115">Smith et al., 2005</xref>). We see examples of both component-selective units (in stacks 2–4) and pattern-selective units (only in stack 4) in our model, as indicated by plaid-direction tuning curves (<xref ref-type="fig" rid="fig8">Figure 8b</xref>) and plots of response as a function of the directions of each component (<xref ref-type="fig" rid="fig8">Figure 8c</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Pattern sensitivity.</title><p>(<bold>a</bold>) Example plaid stimuli used to measure pattern selectivity. Black arrow, direction of pattern motion. Colored arrows, directions of component motion. (<bold>b</bold>) Direction tuning curves showing the response of an example component-motion-selective (left, from stack 2) and pattern-motion-selective (right, from stack 4) unit to grating and plaid stimuli. Colored lines, response to plaid stimuli composed of gratings with the indicated angle between them. Black solid line, unit’s response to double intensity grating moving in the same direction as plaids. Black dashed line, response to single intensity grating moving in the same direction. Horizontal gray line, response to blank stimulus. (<bold>c</bold>) Surface contour plots showing response of units in <bold>b</bold> to plaids as a function of the direction of the grating components. Colored lines denote loci of plaids whose responses are shown in the same colors in <bold>b</bold>. Contour lines range from 20% of the maximum response to the maximum in steps of 10%. For clarity, all direction tuning curves are rotated so that the preferred direction of the response to the optimal grating is at 180°. Responses are mean amplitudes over time.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig8-v2.tif"/></fig><p>The hierarchical temporal prediction model is not explicitly designed to model the pathway in which MT lies (i.e. the dorsal pathway) over other regions of the visual pathway (such as those comprising the ventral visual stream). Some units in the higher stacks of the model could also represent features we have not explored here, but which are represented in V1 or other higher areas. However, in this study we have chosen to focus on motion sensitivity, as found in the dorsal pathway. Only a few (8%, n=3/37) of the direction-selective units in stack 4 are pattern-motion selective. This is a lower proportion than is seen in MT (~23%, n=179/792 [<xref ref-type="bibr" rid="bib128">Wang and Movshon, 2016</xref>]), where the vast majority (&gt;90%, [<xref ref-type="bibr" rid="bib128">Wang and Movshon, 2016</xref>]) of neurons are direction sensitive (DSI &gt;0.5). However, there are also direction-selective neurons in V1 and in higher cortical regions other than MT (e.g. V2, V3 [<xref ref-type="bibr" rid="bib37">Hu et al., 2018</xref>]) that show little or no pattern-selectivity. Hence, if we consider the 4th stack to be not exclusively MT-like, but rather to comprise units that fall in all of these regions collectively, this will dilute the proportion of direction-selective units that are pattern-motion selective. We explore further possible reasons for the low proportion of pattern-motion selective units in the Discussion.</p><p>We also investigated what role the stimulus statistics play in the appearance of pattern-motion-selective units. Although the videos we use have a fair amount of camera motion in them, they also included clips with little or no visual motion. We introduced additional motion into the videos by creating versions that panned left or right at various speeds (see Methods). When we trained the temporal prediction model on this augmented dataset, we observed a substantial increase in the proportion of direction-selective units in all stacks, as well as an increase in the number of pattern-motion-selective units to 19, with some of them emerging in stacks 2 and 3 as well as stack 4. While there are phenomenological models (i.e. that are fitted to neural data) of pattern selectivity (<xref ref-type="bibr" rid="bib96">Rust et al., 2006</xref>; <xref ref-type="bibr" rid="bib107">Simoncelli and Heeger, 1998</xref>), to our knowledge, our model is the first normative model (i.e. where properties emerge from first principles applied to natural statistics) that produces this phenomenon.</p></sec><sec id="s2-5"><title>Determining the model properties critical for the units to resemble visual neurons</title><p>It is interesting to ask whether the tuning properties that emerge are due to the convolutional hierarchical structure of the model, the temporal prediction objective, or both together. At least some hierarchy or recurrency in model structure is required to obtain complex-cell-like units - the simple linear-nonlinear structure of hidden units in our previous non-hierarchical temporal prediction model is incapable of producing units resembling complex cells, although it can produce simple-cell-like units (<xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>).</p><p>To investigate whether a model with the same convolutional hierarchical structure as our model, but with a different objective function than temporal prediction, could produce cells with tuning properties resembling those of visual neurons, we instead trained the model at each stack to efficiently estimate the present input given present and past input, rather than to predict its future input. In other words, we trained each stack of this control model (which has some similarities to a stacked autoencoder, <xref ref-type="bibr" rid="bib7">Bengio et al., 2006</xref>) to reproduce the most recent time step of its input from the same time step and the preceding 4 time-steps. We would expect such a model to have very limited capacity to represent the temporal structure of the movies. When we trained this model with the same form and dataset as the temporal prediction model, we indeed found that while the first stack could reproduce the center-surround spatial tuning seen in the retina, the model unit RFs had no temporal structure – they only depended on the present time step. Furthermore, this control model could not produce magnocellular-like units that switch polarity, nor could it produce units at any stack that resembled simple cells or complex cells by inspection, either spatially or temporally (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplements 3</xref> and <xref ref-type="fig" rid="fig6s4">4</xref>). By our definitions (<xref ref-type="fig" rid="fig7">Figure 7</xref>), there were only ~2% as many simple cells and no complex cells, compared to our standard temporal prediction model. Furthermore, none of the units in this control model showed pattern-motion-selectivity. This suggests that both hierarchy and temporal prediction acting together produce the diverse tuning properties we observe in our model.</p><p>As a further control model, we shuffled the input weights across space and time for each stack of the trained hierarchical temporal prediction model, and then probed the network with stimuli. In this case, the RFs in all stacks lacked discernable structure (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5</xref>), and could not be fitted by Gabor functions, being neither center-surround nor simple-cell-like (only ~1% relative to the standard model met the criteria for simple-cell-like). To look for complex-cell-like units, we measured the model units’ responses to drifting gratings (<xref ref-type="fig" rid="fig6s6">Figure 6—figure supplement 6</xref>). They showed only patchy selectivity for spatial and temporal frequency and orientation, and there were almost no (&lt;1% relative to standard model) units responding like complex cells in any stack. In addition, none of the units showed pattern-motion-selectivity. The fact that we do not see center-surround, simple cells, complex cells or pattern-motion-selective cells shows that the precise ordering of the weights learned through temporal prediction (as opposed to these weights being randomly shuffled) is important for capturing the tuning phenomena observed in the brain.</p><p>Finally, we examined the importance of temporal structure in the stimuli for the results seen in the model. To test this, we shuffled the order of the frames in each clip, reducing their similarity over time. In the first stack, center-surround spatial tuning remained, but temporal structure of the RFs spanned the full five frames available and without temporal asymmetry or increased weighting near the present. By inspection, no RFs resembled simple cells, and by our definitions, simple-cell-like units and complex-cell-like units were much rarer across stacks (~4% and~13% relative to the standard model). None of the units showed pattern-motion-selectivity.</p></sec><sec id="s2-6"><title>Predicting neural responses to natural stimuli</title><p>A standard method to assess a model’s capacity to explain neural responses is to measure how well it can predict neural responses to natural stimuli. We did this for our model for two datasets. The first comprised single-unit neural responses recorded from awake macaque V1 to images of natural scenes (<xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>). The second, particularly relevant to our model, consisted of multi-unit responses from anesthetized macaque V1 to movies of natural scenes (<xref ref-type="bibr" rid="bib73">Nauhaus and Ringach, 2007</xref>; <xref ref-type="bibr" rid="bib90">Ringach and Nauhaus, 2009</xref>). Estimating neural responses to such dynamic natural stimuli has received less attention in models of visual processing.</p><p>We measured the capacity of the temporal prediction model to predict the neural responses of these datasets, and compared it to three other published models of the visual processing hierarchy. The first, the visual geometry group (VGG) model, is a deep convolutional neural network optimized for object recognition that has been trained using many labeled images (<xref ref-type="bibr" rid="bib109">Simonyan and Zisserman, 2014</xref>). This supervised model has been commonly used for explaining responses in visual cortex (<xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>). The second, the Berkeley wavelet transform (BWT) model, is a descriptive model that encapsulates the classical approach to modeling simple and complex receptive fields by using Gabor-like wavelets (<xref ref-type="bibr" rid="bib133">Willmore et al., 2008</xref>). The third model is the PredNet model (<xref ref-type="bibr" rid="bib64">Lotter et al., 2016</xref>; <xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>), which is an unsupervised model trained to predict the immediate future frame of natural movies from recent past frames. This model differs from our temporal prediction approach in that it only applies this frame prediction at the first layer, with subsequent layers seeking an efficient representation of the activity in the lower layer.</p><p>Furthermore, we also modified our model so that comparison could be made with alternative unsupervised objective functions to temporal prediction. One variant of our model had the same architecture, but each stack was instead optimized to reproduce the current (rather than future) input along with a sparsity constraint on the activity (a sparse autoencoder model). Another variant also had the same architecture, but each stack was instead optimized to find slowly varying features while decorrelating activity across units (a slowness model). We also reduced the kernel spacing (stride) and made a few other small modifications (see Methods) to these two models and to the standard temporal prediction model to make them more directly comparable to the BWT, VGG and PredNet models, which use a very fine spacing of kernels in their first layer.</p><p>The method we adopted to assess the capacity of the different models to predict responses to natural stimuli has previously been used to assess models of recording data from macaque monkeys and mice (<xref ref-type="bibr" rid="bib103">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="bib120">Storrs et al., 2021</xref>; <xref ref-type="bibr" rid="bib139">Zhuang et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Mineault et al., 2021</xref>; <xref ref-type="bibr" rid="bib20">Conwell et al., 2021</xref>; <xref ref-type="bibr" rid="bib74">Nayebi et al., 2023</xref>). This involved taking a model’s unit activity in a layer or stack (over a span of two time-steps, 66ms), performing dimensionality reduction on it to a standard size (500 dimensions), and then finding the best linear-nonlinear fit (using cross validation) from this activity to the neural responses over time. Finally, the model and the linear-nonlinear fit were used to predict neural responses to a held-out fraction of the natural stimuli, and the accuracy of this prediction was measured. The assessment measure that we used (CC<sub>norm</sub>) was the Pearson correlation coefficient between the neural response and the model’s prediction of the neural response, normalized to take into account intrinsic neural variability (<xref ref-type="bibr" rid="bib102">Schoppe et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Hsu et al., 2004</xref>).</p><p>On predicting the neural responses to images (<xref ref-type="fig" rid="fig9">Figure 9A</xref>), relative to the temporal prediction model, the BWT and autoencoder models performed slightly worse (bootstrap, n = 166, p &lt;10<sup>–4</sup> and p &lt;10<sup>–4</sup>, respectively, see Methods) and the VGG, PredNet and slowness models performed slightly better (bootstrap, n = 166, p &lt;10<sup>–4</sup>, p &lt;10<sup>–4</sup> and p = 5.3 × 10<sup>–3</sup>, respectively). However, for the dynamic visual stimuli, the results were quite different (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). The BWT, PredNet, autoencoder, and slowness models all performed worse than the temporal prediction model (bootstrap, n = 23, p = 1.6 × 10<sup>–2</sup>, p = 2 × 10<sup>–4</sup>, p = 2.5 × 10<sup>–3</sup> and p &lt;10<sup>–4</sup>, respectively), with only the VGG model being not significantly different (bootstrap, n = 23, n.s.). Hence, the temporal prediction model, an unsupervised model trained on unlabeled data with no explicit constraints imposed by either neural or behavioral data, rivals a supervised model (VGG) trained using a labeled behavioral dataset in predicting neural responses in V1 to dynamic stimuli. Thus, the temporal prediction model is a reasonable contender as an unsupervised model of V1 responses to natural stimuli, particularly dynamic natural stimuli.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Assessment of different models in their capacity to predict neural responses in macaque V1 to natural stimuli.</title><p>(<bold>a</bold>) Prediction performance for neural responses to images (<italic>n</italic> = 166 neurons). Neural data from <xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>. (<bold>b</bold>) <italic>Prediction performance for neural responses to movies (n</italic> = 23 neurons). Neural data from <xref ref-type="bibr" rid="bib73">Nauhaus and Ringach, 2007</xref> and <xref ref-type="bibr" rid="bib90">Ringach and Nauhaus, 2009</xref>. The error bars are standard errors for the population of recorded neurons. Prediction performance that does not differ significantly by a bootstrap method (see Methods) from the temporal prediction model is marked by n.s., while significantly different CC<sub>norm</sub> values are denoted by *p &lt;0.05, **p &lt;0.01, ***p &lt;0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-52599-fig9-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have presented a simple model that hierarchically instantiates temporal prediction – the hypothesis that sensory neurons are optimized to efficiently predict the immediate future from the recent past. This model has several advantages: it is unsupervised, it operates over spatiotemporal inputs, and its hierarchical implementation is general, allowing the same network model to learn features resembling each level of the visual processing hierarchy with little fine-tuning or modification to the model structure at each stage. This simple model accounts for many spatial and temporal tuning properties of cells in the dorsal visual pathway, from the center-surround tuning of cells in the retina and LGN, to the spatiotemporal tuning and direction selectivity of V1 simple and complex cells, and to some degree the motion processing of pattern-selective cells. Furthermore, it can predict neural responses in V1 to movies of natural scenes to a level equal to or better than other models of visual processing, such as supervised image recognition models, predictive coding models, and hierarchical sparse or slow activity models.</p><sec id="s3-1"><title>Relation to biology</title><p>Although this work suggests that temporal prediction may explain why many features of sensory neurons take the form that they do, we are agnostic as to whether the features are hard-wired by evolution or learned over the course of development. This is likely to depend on the region in question, with retina more hard-wired and cortex a mixture of innate tuning and learning from sensory input (<xref ref-type="bibr" rid="bib42">Huberman et al., 2008</xref>; <xref ref-type="bibr" rid="bib56">Kiorpes, 2015</xref>). If the predictive features are learned, this suggests that while some neurons represent these features, a fraction of neurons in the cortex (or elsewhere) might represent the prediction error used to train the network. Indeed, there is evidence that some cortical neurons may represent prediction error (<xref ref-type="bibr" rid="bib2">Auksztulewicz et al., 2023</xref>; <xref ref-type="bibr" rid="bib28">Fiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib93">Rubin et al., 2016</xref>).</p><p>Our model is trained using backpropagation over a single hidden layer for each stack. Although the biological plausibility of backpropagation has been questioned, increasingly biologically realistic methods for training networks are being developed (<xref ref-type="bibr" rid="bib132">Whittington and Bogacz, 2019</xref>). One advantage of our model, for comparison to the biology, is that it does not require backpropagation across more than one hidden layer and isolates learning within each stack, reducing the need to find biologically plausible alternatives to backpropagation that scale with depth (<xref ref-type="bibr" rid="bib5">Bartunov et al., 2018</xref>). This stands in contrast to deep supervised models.</p><p>There are a number of further developments that could be made to our model that may even better capture features of the biology, notably the inclusion of recurrent connections within and between layers or using spiking units. Incorporating recurrent processing may lead to an improvement in the match between the number of pattern-selective units in the model and the proportion of these units seen in MT, as this has been argued to be important by some (<xref ref-type="bibr" rid="bib84">Pack et al., 2001</xref>), but not all (<xref ref-type="bibr" rid="bib96">Rust et al., 2006</xref>) physiological studies of pattern-selectivity in MT. It could also help capture complex spatiotemporal tuning properties of other types of visual neurons, for example those exhibited by some retinal ganglion cells (<xref ref-type="bibr" rid="bib67">Meister and Berry, 1999</xref>). Furthermore, the videos used to train our model typically involve animals moving in natural environments filmed with still or slowly panning cameras. Incorporating other types of motion into the training videos, such as self-motion or eye movements, may also produce a closer match between the response properties of our model units and those seen in the biology. Indeed, when additional panning was incorporated into the training videos, the proportion of pattern-motion-selective units in the model increased, providing an experimentally testable prediction from our model. Finally, although we propose that temporal prediction is an important method for extracting potentially useful features of natural inputs, additional constraints are likely to further refine those features that are of direct relevance to specific behavioral goals.</p></sec><sec id="s3-2"><title>Applications of the model</title><p>Algorithms involving prediction have a long history of use in engineering and signal processing (<xref ref-type="bibr" rid="bib24">de Jager, 1952</xref>; <xref ref-type="bibr" rid="bib49">Kalman, 1960</xref>; <xref ref-type="bibr" rid="bib50">Kalman and Bucy, 1961</xref>). Our model also has potential applications for such use, for example, as an unsupervised learning algorithm to initialize deep networks for supervised tasks. In semi-supervised and transfer learning, models are trained using unlabeled data to produce useful representations that help in the performance of specific tasks (such as object classification etc.) and then fine-tuned using few labeled training examples. Sparse coding models, independent component analysis (ICA) and denoising autoencoders have been widely used in these paradigms (<xref ref-type="bibr" rid="bib7">Bengio et al., 2006</xref>; <xref ref-type="bibr" rid="bib8">Bengio, 2012</xref>; <xref ref-type="bibr" rid="bib9">Bengio et al., 2012</xref>; <xref ref-type="bibr" rid="bib35">Hinton et al., 2006</xref>; <xref ref-type="bibr" rid="bib59">Lee et al., 2009</xref>; <xref ref-type="bibr" rid="bib87">Raina et al., 2007</xref>). More recently, models performing next-frame video prediction have been used in an attempt to find representations that are useful for goals such as action recognition (<xref ref-type="bibr" rid="bib119">Srivastava et al., 2015</xref>; <xref ref-type="bibr" rid="bib126">Vondrick et al., 2016</xref>), object classification (<xref ref-type="bibr" rid="bib15">Canziani and Culurciello, 2017</xref>; <xref ref-type="bibr" rid="bib127">Vondrick and Torralba, 2017</xref>) and action-generation in video games (<xref ref-type="bibr" rid="bib78">Oh et al., 2015</xref>). Very few studies (<xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>; <xref ref-type="bibr" rid="bib86">Palm, 2012</xref>), however, have made any attempt to compare the representations learned by these models to those of sensory neurons in the brain.</p></sec><sec id="s3-3"><title>Predictions of the model</title><p>There are a number of biological predictions suggested by our model. One prediction common to all unsupervised models, which distinguishes them from supervised and reinforcement models, is that neurons in sensory cortex should learn stimulus feature selectivity that is dependent on stimulus statistics even in the absence of reinforcement signals. Some studies have shown that altering stimulus statistics can change cortical representations; for example, rearing cats with goggles that restrict the range of visible orientations influences the orientation tuning of V1 neurons (<xref ref-type="bibr" rid="bib121">Tanaka et al., 2006</xref>). However, the animals could still be seeking rewards and avoiding punishments in such altered environments. To more securely test whether feature selectivity can be learned without reinforcement signals, an experiment would be required that separates reinforcement from stimulus statistics.</p><p>There are also a number of predictions that distinguish hierarchical temporal prediction from other unsupervised frameworks, such as predictive coding (<xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>). Although many possible variants of predictive coding exist (<xref ref-type="bibr" rid="bib117">Spratling, 2017</xref>), by focusing on the framework of <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>, we can draw distinctions from hierarchical temporal prediction. First, predictive coding suggests that the majority of feedforward projections between cortical regions should transmit prediction error, whereas temporal prediction suggests that these projections should transmit the presence of predictive features. This could be directly tested using physiological measurements.</p><p>Second, feedback connectivity is integral to perception in predictive coding, but not for temporal prediction. Indeed, many aspects of visual perception can occur so rapidly that there may not be time for feedback to operate (<xref ref-type="bibr" rid="bib27">Fabre-Thorpe, 2011</xref>; <xref ref-type="bibr" rid="bib123">Thorpe et al., 1996</xref>), supportive of predominantly feed-forward models such as the one we present here. This does, however, raise the question of what role feedback connections would play in the temporal prediction framework. We would suggest that they may be important for contextually modulating neuronal selectivity (<xref ref-type="bibr" rid="bib22">Crist et al., 2001</xref>; <xref ref-type="bibr" rid="bib32">Gilbert et al., 2001</xref>), perhaps using higher level statistics to help predict lower level statistics. Model differences could be tested by optogenetically targeting cortico-cortical feedback connections (<xref ref-type="bibr" rid="bib76">Nurminen et al., 2018</xref>), and then investigating the behavioural consequences.</p><p>Third, temporal correlational structure in sensory input is extremely important for the temporal prediction model, but not for predictive coding, which can learn from still images. This implies that an animal reared in a visual environment, perhaps a virtual environment using goggles, where the frame order of natural visual inputs is randomly shuffled, should develop altered cortical tuning, not just in the temporal domain, but also in the spatial domain. If no changes were seen this would argue against temporal prediction as a learning mechanism. A recent study (<xref ref-type="bibr" rid="bib66">Matteucci and Zoccolan, 2020</xref>) reported that rearing rats in a box with frame-scrambled natural movies played upon the walls led to a substantial degradation and reduction in complex cells, consistent with the importance of temporal prediction, at least for those cells. No alterations in the simple cell population were found, but this may be because much of simple cell tuning is hard-wired before eye opening (<xref ref-type="bibr" rid="bib42">Huberman et al., 2008</xref>). Hence simple cell spatiotemporal tuning in rats may still be optimized for temporal prediction, but by evolutionary processes rather than learning processes in the animal’s lifetime. However, the movies used in this study were shown at a lower frame rate than rats’ flicker fusion frequency, and some aspects of their visual inputs were not frame-scrambled (screen edges and the rat’s own body when still and under self-motion), implying that they did experience some temporal continuity. This may account for the lack of changes in simple cells and the residual fraction of complex cells. Further studies of this sort in different species will help settle these questions.</p></sec><sec id="s3-4"><title>Comparison to other normative models</title><p>Normative models of visual processing derive visual neuron tuning properties from natural scene statistics and optimization for one or more normative principle. As we have mentioned, there are several normative models of visual processing, based on a range of principles, which can account for a number of properties of visual neurons. These theories include predictive coding, sparse coding, independent component analysis (ICA) and temporal coherence.</p><p>The predictive coding framework postulates that sensory systems learn the statistical regularities present in natural inputs, feeding forward the errors caused by deviations from these regularities to higher areas (<xref ref-type="bibr" rid="bib38">Huang and Rao, 2011</xref>; <xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib118">Srinivasan et al., 1982</xref>). In this process, the predictable components of the input signal are subtracted and the prediction errors are fed forward through the hierarchy. This should be distinguished from temporal prediction, which performs selective coding, where predictive elements of the input are explicitly represented in neuronal responses and fed forward, and non-predictable elements are discarded (<xref ref-type="bibr" rid="bib12">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib98">Salisbury and Palmer, 2016</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Sparse coding (<xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib79">Olshausen and Field, 1996</xref>), which shares similarities with predictive coding (<xref ref-type="bibr" rid="bib38">Huang and Rao, 2011</xref>), is built on the idea that an overcomplete set of neurons is optimized to represent inputs as accurately as possible using only a few active neurons for a given input. ICA (<xref ref-type="bibr" rid="bib6">Bell and Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bib125">van Hateren and van der Schaaf, 1998</xref>) is a related framework that finds maximally independent features of the inputs. Sparse coding and ICA are practically identical in cases where a critically complete code is used. In these frameworks, as with predictive coding, the aim is to encode all current or past input, whereas in temporal prediction, only features that are predictive of the future are encoded and other features are discarded.</p><p>Another set of approaches, slow feature analysis (<xref ref-type="bibr" rid="bib134">Wiskott and Sejnowski, 2002</xref>) (SFA) and slow subspace analysis (<xref ref-type="bibr" rid="bib52">Kayser et al., 2001</xref>), stem from the idea of temporal coherence (<xref ref-type="bibr" rid="bib29">Földiák, 1991</xref>), which suggests that a key goal of sensory processing is to identify slowly varying features of natural inputs. SFA is closely related to temporal prediction because features that vary slowly are likely to be predictive of the future. However, SFA and temporal prediction may give different weighting to the features that they find (<xref ref-type="bibr" rid="bib21">Creutzig and Sprekeler, 2008</xref>), and SFA could also fail to capture features that do not vary slowly, but are predictive of the future. One notable study suggests that the features found by SFA can be comparably predictable over time to those features found by networks trained to find predictable features (<xref ref-type="bibr" rid="bib131">Weghenkel and Wiskott, 2018</xref>). However, predictable features are not the same as predictive features. For example, if a horizontally oriented feature is always followed by a vertically oriented feature in the next frame, but the horizontally oriented features are randomly scattered over time, then the horizontally oriented features, while being predictive of the future, are not themselves predictable.</p><p>In the following, we focus on unsupervised normative models trained on natural inputs, because they are the most relevant to our model. Broadly, these models can be divided into several categories: local models, trained to represent features of a specific subset of neurons, such as simple cells in V1, and hierarchical models, which attempt to explain features of more than one cell type (such as simple and complex cells) in a single model. These two categories can be further divided into models that are trained on natural spatial inputs (images) and those that are trained on natural spatiotemporal inputs (movies).</p></sec><sec id="s3-5"><title>Local models of V1 simple or complex cells</title><p>Among local models, sparse coding and ICA are the standard normative models of V1 simple cell RFs (<xref ref-type="bibr" rid="bib6">Bell and Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bib81">Olshausen, 2002</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib79">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib124">van Hateren and Ruderman, 1998</xref>; <xref ref-type="bibr" rid="bib125">van Hateren and van der Schaaf, 1998</xref>). Typically, these models are trained using still natural images and have had remarkable success in accounting for the spatial features of V1 simple cell RFs (<xref ref-type="bibr" rid="bib6">Bell and Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bib80">Olshausen and Field, 1997</xref>; <xref ref-type="bibr" rid="bib79">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib125">van Hateren and van der Schaaf, 1998</xref>). However, models trained on static images are unable to account for the temporal aspects of neuronal RFs, such as direction selectivity or two-dimensional motion processing.</p><p>The ICA and sparse coding frameworks have been extended to model features of spatiotemporal inputs (<xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Olshausen, 2002</xref>; <xref ref-type="bibr" rid="bib124">van Hateren and Ruderman, 1998</xref>). While these models capture many of the spatial tuning properties of simple cells, they tend to produce symmetric temporal envelopes that do not match the asymmetric envelopes of real neurons (<xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>). Capturing temporal features is especially important when building a normative model of the dorsal visual stream, which is responsible for processing cues related to visual motion.</p><p>When trained to find slowly varying features in natural video inputs, SFA models (<xref ref-type="bibr" rid="bib10">Berkes and Wiskott, 2005</xref>) find features with tuning properties that closely resemble those of V1 complex cells, including phase invariance to drifting sinusoidal gratings and end- and side-inhibition. A sparse prior must be applied to the activities of the model units in order to produce spatial localization – a key feature of V1 complex cells (<xref ref-type="bibr" rid="bib61">Lies et al., 2014</xref>). Although SFA can account for complex cell tuning, on its own this framework does not provide a normative explanation for simple cells.</p></sec><sec id="s3-6"><title>Hierarchical models trained on natural images</title><p>Predictive coding (<xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>) provides a powerful framework for learning hierarchical structure from visual inputs in an unsupervised learning paradigm. When applied to natural images, predictive coding has been used to explain some nonlinear tuning properties of neurons in V1, such as end-stopping (<xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>), and, when constrained to have sparse responses, reproduces orientation tuning that resembles the spatial tuning of simple cells. However, it is not clear whether this framework can reproduce the phase-invariant tuning of complex cells, nor has it been shown to account for direction selectivity or pattern-motion sensitivity (<xref ref-type="bibr" rid="bib71">Movshon et al., 1985</xref>).</p><p>Hierarchical ICA models (and related models) provide another approach. These consist of two-layer networks that are trained on natural images with an independence prior placed on the unit activities (<xref ref-type="bibr" rid="bib44">Hyvärinen and Hoyer, 2000</xref>; <xref ref-type="bibr" rid="bib45">Hyvärinen and Hoyer, 2001</xref>; <xref ref-type="bibr" rid="bib51">Karklin and Lewicki, 2009</xref>; <xref ref-type="bibr" rid="bib83">Osindero et al., 2006</xref>). They have been shown to produce simple-cell-like subunits in the first layer of the network and phase-invariant tuning reminiscent of complex cells in the second layer. However, these models typically incorporate aspects that specifically encourage complex cell-like characteristics in the form of a quadratic nonlinearity resembling the complex cell energy model (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>). In some models, phase invariance is enforced by requiring the outputs of individual subunits to be uncorrelated (<xref ref-type="bibr" rid="bib44">Hyvärinen and Hoyer, 2000</xref>; <xref ref-type="bibr" rid="bib57">Körding et al., 2004</xref>). This is in contrast to our model where the phase invariance is learned as a consequence of the general principle of finding features that can efficiently predict future input. An advantage to learning the invariance is that the identical model architecture can then be applied hierarchically to explain features in higher visual areas without changing the form of the model.</p></sec><sec id="s3-7"><title>Hierarchical models trained on natural spatiotemporal inputs</title><p>Some hierarchical models based on temporal coherence have been trained on spatiotemporal inputs (videos of natural scenes). These models have been shown to capture properties of both simple and complex cells (<xref ref-type="bibr" rid="bib43">Hurri and Hyvärinen, 2003</xref>; <xref ref-type="bibr" rid="bib52">Kayser et al., 2001</xref>; <xref ref-type="bibr" rid="bib57">Körding et al., 2004</xref>). Typically, they share a similar structure and assumptions with the hierarchical ICA models outlined above, consisting of a two-layer model where the outputs of one or more simple cell subunits are squared and passed forward to a complex cell layer. Other models have combined sparsity/independence priors with a temporal slowness constraint in a hierarchical model (<xref ref-type="bibr" rid="bib11">Berkes et al., 2009</xref>; <xref ref-type="bibr" rid="bib14">Cadieu and Olshausen, 2012</xref>; <xref ref-type="bibr" rid="bib17">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Hyvärinen et al., 2003</xref>). Since sparsity constraints tend to produce simple cell tuning and slowness constraints result in complex cell tuning, these models produce units with both types of selectivity. This contrasts with our model, which produces both types of selectivity with a single objective and also accounts for some tuning features of higher visual areas.</p><p>Recently, a hierarchical predictive coding model (PredNet [<xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>; <xref ref-type="bibr" rid="bib64">Lotter et al., 2016</xref>]) has been developed, which combines features of Rao and Ballard’s (<xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>) predictive coding model and temporal prediction (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). This shares some similarities with our hierarchical temporal prediction model, but also has several important differences and is substantially more complex (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Similar to our temporal prediction model, the PredNet model tries to predict the future input, rather than compress current input like classical predictive coding. However, a notable and substantial difference from our model is that in PredNet, as in predictive coding, it is the prediction error, rather than the predictive signal contained in the hidden unit activity, that is passed up as input to the next layer of the model. PredNet captures some interesting physiological or psychophysical aspects of visual processing, such as end-stopping, sequence learning, illusory contours and the flash-lag effect (<xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>). However, it has not been demonstrated that PredNet captures the phenomena that we describe, namely the parvocellular/magnocellular division, orientation- and direction-tuned simple and complex cells, and pattern-direction selectivity.</p></sec><sec id="s3-8"><title>Quantitative comparison to different models</title><p>Because the visual system has evolved to process natural stimuli, an important consideration in whether a computational model captures visual processing is whether it can predict neural responses to such stimuli. To this end, we estimated how well the hierarchical temporal prediction model can predict responses in awake macaque V1 to movies and images of natural scenes. Of particular interest are responses to movies, as our model is focused on the neural processing of dynamic stimuli. We compared the temporal prediction model to other models of visual system organization. Two of these models have identical structure to the temporal prediction model, but implement different underlying principles; one model estimates the current rather than future input alongside a sparse activity constraint (sparse coding principle) and the other finds slowly-varying features (slowness principle). We also made comparisons with three other key models of visual processing, the PredNet model, based on the principle of predictive coding, a wavelet model (BWT), inspired by the classic Gabor model, and a deep supervised model trained on image recognition (VGG).</p><p>It is important to note that it can be difficult to determine whether the success or failure of a given model to predict neural responses is a consequence of the key principles underlying the model or the details of its implementation. Making different models comparable in the way they are implemented is a challenge, and while we have striven to do this, there is always scope for further improvement. Furthermore, there are also other classes of models that could be examined (e.g. <xref ref-type="bibr" rid="bib130">Wang et al., 2021</xref>; <xref ref-type="bibr" rid="bib129">Wang et al., 2018</xref>), training datasets that could be used (e.g. <ext-link ext-link-type="uri" xlink:href="http://moments.csail.mit.edu">http://moments.csail.mit.edu</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://cmd.rcc.uchicago.edu">https://cmd.rcc.uchicago.edu</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://deepmind.com/research/open-source/kinetics">https://deepmind.com/research/open-source/kinetics</ext-link>), or neural datasets to which the models could be compared (e.g. a comparison to the primate ventral pathway data as in <xref ref-type="bibr" rid="bib53">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib135">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib139">Zhuang et al., 2021</xref>).</p><p>Thus, our analysis should not be seen as definitively ruling out the principles underlying any of the other models, but rather an attempt to ascertain whether temporal prediction is a good contender for an underlying principle. It is also important to point out that while the sparsity, slowness and PredNet models are, like temporal prediction, unsupervised normative models, the wavelet model is a descriptive model, and the VGG model is a supervised model fitted to human behavioral data (labeled images). The behavioral data provide quantitative measurements of the outputs of the visual system that constrain the VGG model, which is not the case for the other models in this comparison. Thus, in some senses, the VGG model is a fitted descriptive encoding model, rather than a normative unsupervised model.</p><p>We found that the performance of the hierarchical temporal prediction model at predicting the responses of V1 neurons to images was not substantially different from that of leading models, such as the VGG model. Importantly, for the V1 responses to movies, the temporal prediction model outperformed all the other models, except for the VGG model which it performed as well as. However, the VGG model has the advantage of labeled data, and was also trained on far more data than our model, whereas the temporal prediction model is unsupervised. Our findings therefore indicate that temporal prediction remains a leading contender as an underlying normative unsupervised principle governing the neural processing of natural visual scenes.</p></sec><sec id="s3-9"><title>Conclusions</title><p>Previous studies using principles related to temporal prediction have produced non-hierarchical models of the retina or primary sensory cortex and demonstrated retina-like RFs (<xref ref-type="bibr" rid="bib77">Ocko et al., 2018</xref>), simple-cell like RFs (<xref ref-type="bibr" rid="bib16">Chalk et al., 2018</xref>; <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>), or RFs resembling those found in primary auditory cortex (<xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>). However, any general principle of neural representation should be extendable to a hierarchical form and not tailored to the region it is attempting to explain. Here, we show that temporal prediction can indeed be made hierarchical and that it reproduces the major motion-tuning properties that emerge along the visual pathway from retina and LGN to V1 granular layers to V1 superficial layers and potentially some properties of the extrastriate dorsal cortical pathway. This model captures not only linear tuning features, such as those seen in center-surround retinal neurons and direction-selective simple cells, but also nonlinear features seen in complex cells, with some indication that it may also reproduce properties of the pattern-sensitive neurons that are found in area MT. The capacity of our hierarchical temporal prediction model to account for many tuning features at several levels of the visual system suggests that the same framework may well explain many more features of the brain than we have so far investigated. Iterated application of temporal prediction, as performed by our model, could be used to make predictions about the tuning properties of neurons in brain pathways that are much less well understood than those of the visual system. Our results suggest that, by learning behaviorally useful features from dynamic unlabeled data, temporal prediction may represent a fundamental coding principle in the brain.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Data used for model training and testing</title><sec id="s4-1-1"><title>Visual inputs</title><p>Videos (grayscale, without sound, sampled at 25 frames-per-second) of wildlife in natural settings were used to create visual stimuli for training the artificial neural network. The videos were obtained from <ext-link ext-link-type="uri" xlink:href="http://www.arkive.org/species">http://www.arkive.org/species</ext-link> and contributed by: BBC Natural History Unit, <ext-link ext-link-type="uri" xlink:href="http://www.gettyimages.co.uk/footage/bbcmotiongallery">http://www.gettyimages.co.uk/footage/bbcmotiongallery</ext-link>; BBC Natural History Unit &amp; Discovery Communications Inc, <ext-link ext-link-type="uri" xlink:href="http://www.bbcmotiongallery.com">http://www.bbcmotiongallery.com</ext-link>; Granada Wild, <ext-link ext-link-type="uri" xlink:href="http://www.itnsource.com">http://www.itnsource.com</ext-link>; Mark Deeble &amp; Victoria Stone Flat Dog Productions Ltd., <ext-link ext-link-type="uri" xlink:href="http://www.deeblestone.com">http://www.deeblestone.com</ext-link>; Getty Images, <ext-link ext-link-type="uri" xlink:href="http://www.gettyimages.com">http://www.gettyimages.com</ext-link>; National Geographic Digital Motion, <ext-link ext-link-type="uri" xlink:href="http://www.ngdigitalmotion.com">http://www.ngdigitalmotion.com</ext-link>. The videos consisted of animals moving in natural environments, filmed with panning, still and occasionally zooming cameras. The longest dimension of each video frame was clipped to form a square image. Each frame was then down-sampled (using bilinear interpolation) over space, to provide 181x181 pixel frames. The video patches were cut into non-overlapping clips, each of 20 frames duration (800ms). We used a training set of <italic>N</italic> = ~1305 clips from around 17 min of video, and a validation set of <italic>N</italic> = ~145 clips. Finally, each clip was normalized by subtracting the mean and dividing by the standard deviation of that clip.</p></sec><sec id="s4-1-2"><title>Adding panning to videos</title><p>To investigate the effect of using videos with additional motion, we separately trained the model on an augmented dataset where additional panning was incorporated into each of the videos. To do this, we shifted each frame horizontally by a given number of pixels left or right relative to the previous frame. The dataset that included panning was comprised of the original videos, plus these videos with each of left and right panning of 1, 2, and 3 pixels per frame. The results of training the model on the dataset with additional panning are mentioned in relation to patternmotion selectivity, but it should be noted that the main results presented are from the model when trained on the original dataset.</p></sec></sec><sec id="s4-2"><title>Hierarchical temporal prediction model</title><sec id="s4-2-1"><title>The basic model and cost function</title><p>First, we will describe the basic hierarchical temporal prediction model without convolution (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Although this model is not used in this paper, it is the basis for the convolutional hierarchical temporal prediction model presented here. The basic temporal prediction model consists of stacked feedforward single-hidden-layer neural networks. The first stack receives as input the movie, and predicts future frames of the movie from a number of past frames. The second stack receives as input the activities of the hidden units of the first stack, and predicts the future activity of the first stack from a number of time steps of past activity of the first stack. The third stack receives as input the activities of the second stack, and so on.</p><p>Each stack is optimized independently, starting with the lowest stack and moving up the stacks, the output of the trained lowest stack providing input for the next stack which was then trained, and so on. The cost function of each stack, minimized by backpropagation, is the sum of the squared error between the prediction and the target, plus an L<sub>1</sub>-norm regularization term, which is proportional to the sum of absolute values of all weights in the network. This regularization tends to drive redundant weights to near zero and provides a parsimonious network. Note, a single stack for this model is almost exactly the same as the non-hierarchical model from <xref ref-type="bibr" rid="bib110">Singer et al., 2018</xref>.</p></sec><sec id="s4-2-2"><title>The convolutional model and cost function</title><p>The convolutional hierarchical temporal prediction model used in the present study (<xref ref-type="fig" rid="fig2">Figure 2</xref>) is the same as the basic hierarchical temporal prediction model outlined above, except that the networks involved are convolutional neural networks. Convolutional networks recognize that neural tuning for similar sets of spatially-restricted features is likely to be found at every point in visual space, due to the relatively local and translation-invariant statistics of natural scenes. Hence, rather than learn the feature separately for every point in space, convolutional networks ensure that the same set of features is learned at every point in space by using convolution, aiding and accelerating learning. Formulating the model as convolutions of tensors provides a compact notation.</p><p>The convolutional hierarchical temporal prediction model consists of stacked feedforward single-hidden-layer 3D convolutional neural networks. Each stack consists of an input layer, a convolutional hidden layer and a convolutional output layer. Note, there are no sub-sampling layers. Each hidden layer consists of a number of convolutional hidden units, each convolutional hidden unit being the set of hidden units tuned to the same feature at every point in space. Each convolutional hidden unit (over time and 2D space; <xref ref-type="fig" rid="fig2">Figure 2</xref>) performs 3D convolution over its inputs using its particular convolutional kernels, and its output is determined by passing the result of this operation through a rectified linear function. Thus, hidden units have non-negative activity. Following the hidden layer there is a convolutional output layer, which performs linear convolution. Each stack is trained to minimize the difference between its output layer and its target. The target is the input at the immediate future time-step.</p><p>The first stack of the model is trained to predict the immediate future frame (40ms) of unfiltered natural video inputs from the previous 5 frames (200ms). This 5-frame span was chosen after earlier exploration of the trained model indicated that most of the non-negligible weights tended to fall within this span. Each subsequent stack is then trained to predict the immediate future hidden-unit activity of the stack below it from the past hidden-unit activity in response to the natural video inputs. This process is repeated until 4 stacks have been trained. The first stack uses 50 convolutional hidden units and this number is doubled with each added stack, until we have 400 units in the 4th stack.</p><p>More formally, each stack of the model can be described by a network of the same form. Throughout this section on the convolutional model, bold capitalized variables are rank-3 tensors over time and 2D-space, otherwise variables are scalars. The network has an input layer <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> input channels, a single hidden layer of <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> convolutional hidden units, and an output layer of <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> output channels.</p><p>For input channel <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the input <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a rank-3 tensor spanning time and 2D-space with <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> spatial positions, and <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> time steps. Each input channel <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the output <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of convolutional hidden unit <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the stack below (with <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), except for the first stack which has only a single input channel (<inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), which is the grayscale video. Hence, each subsequent stack has as many input channels (<inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) as the number of convolutional hidden units (feature maps) in the previous stack.</p><p>For a given convolutional hidden unit <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the output of the unit is a rank-3 tensor over time and 2D-space, <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and is given by,<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the input weights kernel between each input channel <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and hidden unit <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the bias of hidden unit <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the rectified linear function. The operator <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the 3D convolutional operator over the two spatial and one temporal dimensions of the input. Each hidden layer kernel <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is 3D with size <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. No zero padding is applied to the input.</p><p>The output of the network predicts the future activity of the input. Hence, the number of input channels (<inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) always equals the number of output channels (<inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) for each stack. The activity <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of each output channel <italic>k</italic> is the estimate of the true future <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> given the past <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is simply <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> shifted one time-step into the future, and <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This prediction <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is estimated from the hidden unit output <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The parameters in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> are the output weight kernels <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (the weights between each hidden unit <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and output channel <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the bias <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each output kernel <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is 3D with size <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and predicts a single time-step into the future based on hidden layer activity in that portion of space.</p><p>There is a slight exception for the first stack, where to cope with the size of the images the spatial stride value of the convolution in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> was greater than one (it was 10). To ensure that the predicted output provided by <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> has the same size as the input when a stride of &gt;1 is used, the hidden layer representation is first dilated over by adding <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> zeros between adjacent input elements, where <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the stride of the convolutional operator in the hidden layer. This dilation makes <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> into what is known as a fractionally-strided convolutional layer (<xref ref-type="bibr" rid="bib26">Dumoulin and Visin, 2018</xref>).</p><p>Each stack is optimized independently, starting with the lowest stack and moving up the stacks, the hidden unit activity <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of the trained lowest stack providing the input for the next stack, which is then trained, and so on. For each stack, the parameters of every <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> were optimized for the training set by minimizing the cost function given by,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>n</italic> indicates the video clip number and <inline-formula><mml:math id="inf112"><mml:msub><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow/></mml:mfenced></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the entrywise <italic>p</italic>-norm of the tensor over time and 2D-space, where <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> provides the square root of the sum of squares of all values in the tensor and <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> provides the sum of absolute values. Thus, <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the sum of the squared error between the prediction <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the target <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, plus an <italic>L</italic><sub>1</sub>-norm regularization term, which is proportional to the sum of absolute values of all weights in the network and its strength is determined by the hyper-parameter <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The resulting set of stacked networks can then operate as a single feed-forward convolutional network, with <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> providing the transformation at each stack, using the weights and biases learned for that stage. <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref> are simply used for training the stacks. In <xref ref-type="fig" rid="fig2">Figure 2</xref>, we use a bracketed superscript to indicate the weight matrices associated with each stack; stack 1, <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, stack 2, <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, stack 3, <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and stack 4, <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-3"><title>Implementation details</title><p>The networks were implemented in Python (<ext-link ext-link-type="uri" xlink:href="https://lasagne.readthedocs.io/en/latest/">https://lasagne.readthedocs.io/en/latest/</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</ext-link>). The objective function for each stack was minimized using backpropagation as performed by the Adam optimization method (<xref ref-type="bibr" rid="bib55">Kingma and Ba, 2014</xref>) with hyperparameters <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> kept at their default settings of 0.9 and 0.999, respectively, and the learning rate (<inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) varied as detailed below. Training examples were split into minibatches of 32 training examples each.</p><p>During model network training, several hyperparameters were varied, including the regularization strength (<inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the learning rate (<inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). For each hyperparameter setting, the training algorithm was run for 1000 iterations. The effect of varying <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on the prediction error (the first term of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>) and receptive field structure of the first stack is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. For all subsequent stacks, we varied <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> between 10<sup>–5</sup> and 10<sup>–7</sup> and picked the network with the lowest prediction error (mean squared error) on a held-out validation set. We measured the predictive capacity of each network by taking the average prediction error on the validation set over the final 50 iterations. We also explored the time span of frames into the past and set it sufficiently long (5 frames) that in the trained model there was typically very little weighting for the frame furthest into the past. The settings for each stack are presented in <xref ref-type="table" rid="table1">Table 1</xref>.</p></sec></sec><sec id="s4-3"><title>Model unit spatiotemporal extent and receptive fields</title><p>Due to the convolutional form of the hidden layer, each hidden unit can potentially receive from a certain span over space and time. We call this the unit’s spatial and temporal extent. For stack 1, this this extent is given by the kernel size (21x21 x 5, space x space x time). For stack 2, the extent of each hidden unit is a function of its kernel size and the kernel size and stride of the hidden units in the previous stack, resulting in an extent of 41x41 x 9. Similarly, the extent of each hidden unit in stack 3 is 61x61 x 13 and in stack 4 is 81x81 x 17. The RF size of a unit can be considerably smaller than the hidden unit’s extent if weights assume a value close to zero.</p><p>In the first stack of the model, the combination of linear weights and nonlinear activation function are similar to the basic linear non-linear (LN) model (<xref ref-type="bibr" rid="bib18">Chichilnisky, 2001</xref>; <xref ref-type="bibr" rid="bib108">Simoncelli et al., 2004</xref>) commonly used to describe neuronal RFs. Hence, the input weights between the input layer and a hidden unit of the model network are taken directly to represent the unit’s linear RF, indicating the features of the input that are important to that unit. The output activities of hidden units in stacks 2–4 are transformations with multiple linear and nonlinear stages, and hence we estimated their linear RFs by applying reverse correlation to 100,000 responses to binary noise input with amplitude ±3 to stack 1.</p></sec><sec id="s4-4"><title>Fitting Gabors to model unit receptive fields</title><p>We fitted 2-dimensional Gabor functions to the most recent time-step of each of model unit’s linear RF. The Gabor function has been shown to provide a good approximation for most spatial aspects of simple cell visual RFs (<xref ref-type="bibr" rid="bib48">Jones and Palmer, 1987</xref>). The 2D Gabor is given as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>⋅</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where the spatial coordinates <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are given by the following transformation of the RF center <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with rotation of the RF by its spatial orientation <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> provide the width of the Gaussian envelope in the <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> directions, while <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameterize the spatial frequency and phase of the sinusoid along the <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> axis. <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> parameterizes the height of the Gaussian envelope. For each RF, the parameters <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of the Gabor were fitted by minimizing the mean squared error between the Gabor model and the RF using Python code modified from: <ext-link ext-link-type="uri" xlink:href="https://github.com/JesseLivezey/gabor_fit">https://github.com/JesseLivezey/gabor_fit</ext-link>, copy archived at <xref ref-type="bibr" rid="bib63">Livezey, 2019</xref>.</p><p>Units that could be well fitted by Gabor functions, that is, where the pixel-wise correlation coefficient between the model unit RF and the fitted Gabor was &gt;0.4, were included as putative simple cells (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref> and <xref ref-type="fig" rid="fig6s2">2</xref>).</p></sec><sec id="s4-5"><title>In vivo V1 receptive field data</title><p>Responses to drifting gratings measured using recordings from V1 simple and complex cells were compared against the model (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The in vivo data were taken from <xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>.</p></sec><sec id="s4-6"><title>Receptive field size and polarity</title><p>We measured the size of the RFs of the units in the first stack and examined the relationship between the RF size and the proportion of the RFs switching polarity. For each unit, all RF pixels (weights) in the most recent time-step with absolute values ≥50% of the maximum absolute value in that time-step are included in the RF. The RF size was determined by counting the number of pixels fitting this criterion. We then counted the proportion of pixels included in the RF that changed sign (either positive to negative or vice versa) between the two most recent timesteps. The relationship between these two properties for the units in the first stack is shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>.</p></sec><sec id="s4-7"><title>Drifting sinusoidal gratings</title><p>In order to characterize the tuning properties of the model’s visual RFs, we measured the responses of each unit to full-field drifting sinusoidal gratings. For each unit, we measured the response to gratings with a wide range of orientations, spatial frequencies and temporal frequencies until we found the parameters that maximally stimulated that unit (giving rise to the highest mean response over time). We define this as the optimal grating for that unit. In cases where orientation or tuning curves were measured, the gratings with optimal spatial and temporal frequency for that unit were used and were varied over orientation. Each grating alternated between an amplitude of ± 3 on a gray (0) background. Some units, especially in higher stacks, had weak or no responses to drifting sinusoidal gratings. To account for this, we excluded any units with a mean response (over time) of &lt;1% of the maximum mean response of all the units in that stack. As a result of this, 17/100, 99/200 and 286/400 units were excluded from the 2nd, 3rd, and 4th stacks, respectively.</p><p>We measured several aspects of the V1 neuron and model unit responses to the drifting gratings. For each unit, we measured the circular variance, orientation bandwidth, modulation ratio and direction selectivity.</p><sec id="s4-7-1"><title>Circular variance</title><p>Circular variance (CV) is a global measure of orientation selectivity. For a unit with mean response-over-time <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to a grating with angle <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, with angles spanning the range of 0–360° in equally spaced intervals of 5° and measured in radians, the circular variance is defined as (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-7-2"><title>Orientation bandwidth</title><p>We also measured the orientation bandwidth (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>), which provides a more local measure of orientation selectivity. First, we smoothed the direction tuning curve with a Hanning window filter with a half-width at half-height of 13.5°. We then determined the peak of the orientation tuning curve. The orientation angles closest to the peak for which the response was <inline-formula><mml:math id="inf144"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> (or 70.7%) of the peak response were measured. The orientation bandwidth was defined as half of the difference between these two angles. We limited the maximum orientation bandwidth to 180°.</p></sec><sec id="s4-7-3"><title>Modulation ratio</title><p>We measured the modulation ratio of each unit’s response to its optimal sinusoidal grating. The modulation ratio is defined as:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the amplitude of the best-fitting sinusoid to the unit’s response over time to the drifting grating. <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean response to the grating over time.</p></sec><sec id="s4-7-4"><title>Direction selectivity index</title><p>To measure the direction selectivity index, we obtained each unit’s direction tuning curve at its optimal spatial and temporal frequency. We measured the peak of the direction tuning curve, indicating the unit’s response to gratings presented in the preferred direction (<inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), the response to the grating presented in the opposite (non-preferred) direction (<inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the response to a blank gray image (<inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We examined three different direction selectivity indexes based on measures that have been used in neurophysiological studies. Direction selectivity index 1 (DSI1) (for example, in <xref ref-type="bibr" rid="bib91">Rochefort et al., 2011</xref>) is defined as:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mn mathvariant="italic">1</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Direction selectivity index 2 is defined as:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mn mathvariant="italic">2</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>This is a simple transformation of the measures used by <xref ref-type="bibr" rid="bib100">Schiller et al., 1976</xref> and <xref ref-type="bibr" rid="bib25">De Valois et al., 1982</xref>. The measure in <xref ref-type="bibr" rid="bib100">Schiller et al., 1976</xref> is calculated as <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>100</mml:mn><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; to convert their data to DSI2 we divided by 100 and then subtracted the values from one. Another difference between the DSI2 measure we used with the model and the measure of direction selectivity in <xref ref-type="bibr" rid="bib100">Schiller et al., 1976</xref> is that they took the average responses to moving light and dark bars, whereas we used drifting gratings. <xref ref-type="bibr" rid="bib25">De Valois et al., 1982</xref> used <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, so to convert their data to DSI2 we subtracted their measure from one. We performed these transformations to provide a measure that goes from 0 to 1 to be more consistent with the other direction selectivity indexes.</p><p>Direction selection index 3 (as used, for example in <xref ref-type="bibr" rid="bib33">Gizzi et al., 1990</xref>) is defined as:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mn mathvariant="italic">3</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The direction tuning data were extracted from the relevant papers using WebPlotDigitizer (<ext-link ext-link-type="uri" xlink:href="https://automeris.io/WebPlotDigitizer">https://automeris.io/WebPlotDigitizer</ext-link>).</p></sec></sec><sec id="s4-8"><title>Drifting plaid stimuli</title><p>In order to test whether units were pattern selective, we measured their responses to drifting plaid stimuli. Each plaid stimulus was composed of two superimposed half-intensity (amplitude 1.5) sinusoidal gratings with different orientations. The net direction of the plaid movement lies midway between these two orientations (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). Plaids with a variety of orientations, spatial frequencies, temporal frequencies and spatial extents (as defined by the extent of a circular mask) were tested. For each unit, the direction tuning curves of the optimal plaid stimulus (that giving rise to the largest mean response over time) were measured (<xref ref-type="fig" rid="fig8">Figure 8b and c</xref>).</p></sec><sec id="s4-9"><title>Quantitative model comparison for neural response prediction</title><p>We compared how well the temporal prediction model and five other models could predict neural responses. The models that we compared to the temporal prediction model were the following: the visual geometry group (VGG) model – a spatial deep convolutional model trained in a supervised manner on labeled data for image recognition, which has been shown to predict well neural responses in V1 (<xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>); the Berkeley wavelet transform model – a descriptive spatial model of V1, and three unsupervised spatiotemporal models – a leading predictive coding model (PredNet), a slowness model and an autoencoder model with sparse activity. The models were compared for their capacity to predict neural responses to images and movies of natural scenes.</p><sec id="s4-9-1"><title>V1 neural response datasets</title><p>We used two publicly available neural response datasets recorded in V1 of awake non-human primates. The first dataset contains recordings in response to natural images and the second dataset contains recordings in response to natural videos. For simplicity and to avoid confusion with model units, we will refer to single-unit and multi-unit recordings simply as ‘neurons’. For those recordings, all stimuli were presented in grayscale with the animals trained to fixate on a small target. For both datasets, we used 80% of the data as a cross-validation set for training and setting hyperparameters (validation) and the remaining 20% of the data was held out for testing the models. For the videos, we split the dataset by video files rather than slicing videos in time. We also normalized all data by subtracting the mean and dividing by the standard deviation of each respective cross-validation dataset.</p><sec id="s4-9-1-1"><title>Spatial V1 dataset</title><p>The image dataset contains the responses of 166 single-unit recordings in V1 to 7250 natural images (repeated 4 times) (<xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>). The natural images were from the ImageNet dataset (<xref ref-type="bibr" rid="bib94">Russakovsky et al., 2015</xref>), with some further augmented using a textural synthesis model (<xref ref-type="bibr" rid="bib30">Gatys et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>) (to further increase higher-order correlations within the dataset). We resampled the images to 112×112 pixels as input to the models.</p><p>The electrophysiological recordings came from two awake adult male macaque monkeys (<italic>Macaca mulatta</italic>) using a 32-channel linear silicon probe (approved by the Baylor College of Medicine Institutional Animal Care and Use Committee). The images were presented for 60ms (covering 2 degrees of visual angle) with spike counts extracted in the window 40–100ms after image onset. Following spike sorting, the dataset comprised 166 single-unit recordings that were driven by the visual stimuli (with at least 15% of the total variance accounted for from the stimulus). For a more in-depth description of the dataset acquisition see <xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>.</p></sec><sec id="s4-9-1-2"><title>Spatiotemporal V1 dataset</title><p>The video dataset comprised 23 multi-unit recordings in V1 in response to 10 videos of 30s duration each (repeated 10 times) (<xref ref-type="bibr" rid="bib73">Nauhaus and Ringach, 2007</xref>; <xref ref-type="bibr" rid="bib90">Ringach and Nauhaus, 2009</xref>). The videos were snippets from four different movies (Sleeper, Benji, Goldfinger and Shakespeare in Love). We resampled all spatial dimensions to 112×112 pixels as input to the models.</p><p>Electrophysiological recordings were obtained from anesthetized adult male macaque monkeys (<italic>Macaca fascicularis</italic>) using a 10×10 extracellular electrode array (approved by the UCLA Animal Research Committee). Videos were displayed at 30 frames-per-second (covering 2–6 degrees of visual angle) with spike counts binned in 33.33ms intervals. For a more in-depth description of the dataset acquisition see <xref ref-type="bibr" rid="bib90">Ringach and Nauhaus, 2009</xref>.</p></sec></sec></sec><sec id="s4-10"><title>Models</title><sec id="s4-10-1"><title>Visual Geometry Group (VGG)</title><p>The VGG model is a large convolutional neural network (CNN) with 16 convolutional and 3 fully connected layers (<xref ref-type="bibr" rid="bib109">Simonyan and Zisserman, 2014</xref>) trained on the large ImageNet dataset containing more than a million images categorized into 1000 classes. As done by <xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>, we used the activation maps in layer 6 of the CNN model to predict the real neural responses.</p></sec><sec id="s4-10-2"><title>Berkeley Wavelet Transform (BWT)</title><p>The Berkeley wavelet transform (BWT, <xref ref-type="bibr" rid="bib133">Willmore et al., 2008</xref>) is a static wavelet model that is inspired by the Gabor-like tuning (<xref ref-type="bibr" rid="bib48">Jones and Palmer, 1987</xref>) of simple cells in V1. It performs a multiscale wavelet decomposition of each input image. Like Gabor filters, the wavelets are tuned in position, orientation and spatial frequency, and they can be divided into even and odd pairs with similar tuning. Unlike Gabor filters, they also form a complete orthonormal basis, making the decomposition inexpensive to compute. The sum of the square of the activities of corresponding pairs is used to model complex cells. Due to the wavelets being scaled by a power of 3, we zero-padded the spatial input dimensions of the input data to the closest power of 3. We used the simple- and complex-like units together to predict the neural responses.</p></sec><sec id="s4-10-3"><title>PredNet</title><p>The PredNet model is a multi-layered network, optimized to predict future frames from past frames in a movie (<xref ref-type="bibr" rid="bib64">Lotter et al., 2016</xref>; <xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>). We used the pre-trained model from these publications, which was trained using the KITTI dataset (sampled at 10 frames per second) comprising footage from a roof-mounted camera on a car driving around a city (<xref ref-type="bibr" rid="bib31">Geiger et al., 2013</xref>). We recognize that the PredNet model has been trained on movies with a different frame rate than the movies that were used as stimuli for the neural recordings, but even movies with identical frame rates can have different temporal characteristics, depending on their content. We therefore decided to use PredNet as optimized by the authors, rather than a potentially less well optimized version of PredNet retrained by us. The PredNet model is inspired by predictive coding theory (<xref ref-type="bibr" rid="bib88">Rao and Ballard, 1999</xref>), reformulated into a modern deep learning context (i.e. built with convolutional and long short-term memory (LSTM) layers and optimized using backpropagation). As stipulated by predictive coding, the model has feedforward units that transmit the errors between predictions and lower-level activity and feedback units that transmit representations of the predictions of lower-level activity. As suggested by <xref ref-type="bibr" rid="bib65">Lotter et al., 2020</xref>, for each stack (layer), we used the feedforward error representation units to fit to the real neural responses, as these units have been described to be most brain-like.</p></sec><sec id="s4-10-4"><title>Modifications to the temporal prediction model</title><p>We made certain modifications to the temporal prediction model for the neural response prediction tasks. First, to be more consistent with the comparison models, we employed more units within the first stack (100 instead of 50) and used a smaller convolutional stride (4 instead of 10). Second, we employed the softplus activation function (<xref ref-type="bibr" rid="bib138">Zheng et al., 2015</xref>) (with  <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 1 and threshold = 20) instead of the rectified linear activation function for all units. Softplus is similar to a rectified linear function, but has a smooth rather than abrupt change in slope. Third, we predicted the input at time <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> rather than at time <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Fourth, for speed of training, instead of training on a stack-to-stack basis, we trained all stacks at once, and detached gradients from flowing between the stacks (to emulate the stack-to-stack training). We used the Adam optimizer with default parameters and trained the model for 300 epochs with a learning rate of 10<sup>–4</sup>. A value of  <inline-formula><mml:math id="inf155"><mml:mi>λ</mml:mi></mml:math></inline-formula> = 10<sup>–4</sup> was used for stack one. Unless noted otherwise, all other hyperparameters were the same (<xref ref-type="table" rid="table1">Table 1</xref>).</p><p>Finally, we used a different and more diverse natural stimulus dataset for training (which we also used to train the slowness and autoencoder with activity sparsity models). We recorded this dataset ourselves using an iPhone 7, and it consisted of 32 diverse movies (each of 9.6s duration sampled at 120 frames-per-second) of everyday objects and scenes (e.g. a fish swimming, a dog walking, a ball rolling, and trees moving in the wind) and different recording techniques (e.g. visual flow from forward motion, panning, still and moving content). We spatially downsampled the recordings from 720×1280 pixels to 144×256 pixels using the bilinear method (and then cropped to 140×240 pixels) and temporally downsampled the recordings from 120 frames-per-second to 30 frames-per-second by taking every 4th frame. We trained the model using batches of 15 clips of 30 frames in length (corresponding to approximately 1s of stimulus). We also employed data augmentation (<xref ref-type="bibr" rid="bib122">Taylor and Nitschke, 2018</xref>) to artificially double the training dataset, by having a 50% chance of left-right flipping each clip when each batch is loaded.</p></sec><sec id="s4-10-5"><title>Stacked slowness model</title><p>The slow feature analysis (SFA) model extracts slowly-varying features from an input signal (<xref ref-type="bibr" rid="bib134">Wiskott and Sejnowski, 2002</xref>; <xref ref-type="bibr" rid="bib131">Weghenkel and Wiskott, 2018</xref>). We implemented a variant hereof using the same architecture of the modified temporal prediction model (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, four hierarchical stacks) with the cost function replaced by one with a slowness objective. Each stack finds the slowly-varying features of the hidden unit activity of the stack below (or the value of the stimuli for the first stack). There is no <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> in this model. This new cost function of each stack minimizes the mean-squared difference between consecutive time-steps of model unit activity, subject to activity decorrelation and sparse weight regularization.<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>J</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a rank-3 tensor over 2D-space (of width <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and height <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and time (of duration <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). It is the difference in activity between consecutive time steps in the tensor <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of model hidden unit activity, for channel <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and video clip <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. As stipulated within the SFA algorithm, we applied an additional constraint of decorrelating hidden unit activity, by minimizing the Pearson correlation coefficient <inline-formula><mml:math id="inf163"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between the activity-over-time of the <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><sup>th</sup> and <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula><sup>th</sup> channel over the <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><sup>th</sup> clip. Before calculating the correlation coefficients, we added a small random variable (drawn from a uniform distribution between 0 and 10<sup>–4</sup>) to the activity of each hidden unit to avoid the numeric instability of division by zero. Lastly, the activity of every hidden unit was normalized using batch normalization (<xref ref-type="bibr" rid="bib47">Ioffe and Szegedy, 2015</xref>). Again, we used the Adam optimizer with default parameters to train the model and we did so for 300 epochs with a learning rate of 10<sup>–4</sup>. Unlike the temporal prediction model, we cannot use the prediction performance to set <inline-formula><mml:math id="inf167"><mml:mi>λ</mml:mi></mml:math></inline-formula>, so instead we chose the value of <inline-formula><mml:math id="inf168"><mml:mi>λ</mml:mi></mml:math></inline-formula> (from those explored) that gave the receptive fields that most closely resembled those of real neurons. Unless noted otherwise, all other hyperparameters were the same as <xref ref-type="table" rid="table1">Table 1</xref> and all other aspects of the model and its training were the same as the modified temporal prediction model described above.</p></sec><sec id="s4-10-6"><title>Stacked autoencoder with activity sparsity</title><p>As another control model, we trained a model in which each stack estimates the input at the current time step <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (rather than at the future time step <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) of the spatiotemporal input signal, and with a sparse penalty on the hidden unit activity as well as on the weights. Again, we employed the same architecture (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, four hierarchical stacks) as the modified temporal prediction model. Each stack estimates the current activity of the stack below (or the value of the stimuli for the first stack). <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> also remains the same, but as it now estimates <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (which is the input <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> simply indexed with <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> rather than <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), the output of the equation becomes <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We trained each stack with the following cost function.<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>J</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Again, we used the Adam optimizer with default parameters to train the model and we did so for 300 epochs with a learning rate of 10<sup>–4</sup>. Unlike the temporal prediction model, we cannot use the prediction capacity to set <inline-formula><mml:math id="inf177"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , so instead we chose the values (from those values explored) that gave the receptive fields that most closely resembled those of real neurons. Unless noted otherwise, all other hyperparameters were the same as <xref ref-type="table" rid="table1">Table 1</xref> and all other aspects of the model and its training were the same as the modified temporal prediction model described above.</p></sec></sec><sec id="s4-11"><title>Measuring the capacity of the models to predict neural responses to natural stimuli</title><p>For the spatial or spatiotemporal neural datasets, we ran the stimuli of the cross-validation set through each of the above models, obtaining the activity of their hidden units. One slight complication is that the spatial models (VGG and BWT) are designed to work with images, whereas the spatiotemporal models (all other models) are designed to work with videos. To pass the spatiotemporal stimuli (videos) into the spatial models (VGG and BWT), we fed in each frame individually as an image and concatenated the resulting unit activity to provide a time-series of activity. To pass the spatial stimuli (images) into the spatiotemporal models, we used a spatiotemporal sequence of repeated identical input frames.</p><p>For each model, or stack within a model, we estimated the neural responses by fitting a linear-nonlinear mapping from the latent unit activity (the first 500 principal components of the unit activity of the cross-validation set) of that model or stack. The minimum neural response latency from retina to V1 in macaque monkey is approximately 30ms, and there is a considerable spread of latencies (<xref ref-type="bibr" rid="bib75">Nowak et al., 1995</xref>). To account for these transduction and conductance latencies we computed the latent unit activity <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> using the model unit activity (over space and channels) at time <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is a latency span of approximately 33–99ms. We used this latent unit activity to predict neural responses at time <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We used the principal components instead of directly fitting neural responses from unit activity as this standardizes the comparison between all the models, keeps the fitting procedure manageable (<xref ref-type="bibr" rid="bib103">Schrimpf et al., 2018</xref>), and circumvents overfitting (<xref ref-type="bibr" rid="bib120">Storrs et al., 2021</xref>). This technique of fitting on dimensionality-reduced unit activity has also been used in other studies (<xref ref-type="bibr" rid="bib103">Schrimpf et al., 2018</xref>; <xref ref-type="bibr" rid="bib139">Zhuang et al., 2021</xref>; <xref ref-type="bibr" rid="bib68">Mineault et al., 2021</xref>; <xref ref-type="bibr" rid="bib20">Conwell et al., 2021</xref>).</p><p>More formally, for each neuron <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the dataset of <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons, we fitted the following linear-nonlinear readout, mapping latent model unit activity to the responses of the neuron:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mi>l</mml:mi><mml:mo>⊺</mml:mo></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the neuron's readout weight vector, <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a bias, and we used the softplus function as the non-linearity. We fitted the readouts by minimizing the negative Poisson likelihood of the model predictions <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the neurons' responses <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , and employed a L<sub>1</sub> penalty on the readout weights to avoid overfitting (<xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>).<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow/></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">m</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msub><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We used the Adam optimizer (with default parameters) for fitting, using a batch size of 1024, a learning rate of 0.002, and training for 300 epochs.</p><p>To do the readout fitting, we divided the neural response cross-validation set (whether for images or videos) and the corresponding model latent activity into a training set (80%) and a validation set (20%). We fitted the readout using the training set. Then, for validation, we ran the validation-set latent activity through the fitted readout to predict the corresponding neural responses, and measured the normalized correlation coefficient between the readout predictions and the neural responses (see Performance metrics). The above fitting and validation process was repeated 5 times, once for each distinct validation set. This fivefold cross-validation process was applied for 9 different log-spaced values of <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (ranging from 10<sup>-6.5</sup> to 10<sup>-2.5</sup>). We then fitted the readout model on all the cross-validation data using the <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> value that maximized the mean normalized correlation coefficient across all validation folds. The final performance was calculated on the test set, averaged over all neurons. For the unsupervised models (PredNet, slowness, sparseness, and temporal prediction), we reported the stack that performed the best. For a fairer comparison, we also allowed for the possibility of model units being tuned to different spatial sizes than the neurons. Thus, for all models, we applied the above process at three spatial re-scalings of the model input (0.66×, 1× and 1.5×) and reported the performance for each model at its optimal scale.</p></sec><sec id="s4-12"><title>Performance metrics</title><sec id="s4-12-1"><title>Normalized correlation coefficient</title><p>We used the normalized correlation coefficient to quantify all model fits (<xref ref-type="bibr" rid="bib36">Hsu et al., 2004</xref>; <xref ref-type="bibr" rid="bib102">Schoppe et al., 2016</xref>). The normalized correlation coefficient CC<sub>norm</sub> quantifies a model’s performance as the Pearson correlation coefficient between the neural response and the model’s prediction, divided by the maximum achievable correlation coefficient given neural noise. This measure is related to signal power explained (<xref ref-type="bibr" rid="bib97">Sahani and Linden, 2003</xref>; <xref ref-type="bibr" rid="bib106">Shimazaki and Shinomoto, 2007</xref>). A CC<sub>norm</sub> of zero indicates no correlation between the model and the neural response, while a CC<sub>norm</sub> of one indicates they are as correlated as possible given neural noise.</p></sec><sec id="s4-12-2"><title>Statistical tests</title><p>Statistical comparison of the different models was performed using a bootstrapping method (<xref ref-type="bibr" rid="bib139">Zhuang et al., 2021</xref>). To calculate the probability, p, of model A performing better/worse statistically than model B, we sampled prediction scores over the neurons (paired, with replacement) for each model and calculated their respective means. We did this 10,000 times, counting the number of times model A had a larger mean than model B. Finally, this count was divided by 10,000 to obtain the probability, p, of model A having a larger mean than model B, with 1 - p being the probability of model B having a larger mean than model A.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing, Formal analysis</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Methodology, Writing – review and editing, Formal analysis</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision, Funding acquisition, Writing – review and editing, Project administration</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Visualization, Methodology, Writing – review and editing, Formal analysis</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-52599-transrepform1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All custom code used in this study was implemented in Python. The code for the models and analyses shown in Figures 1-8 and associated sections can be found at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/ox-ang/hierarchical_temporal_prediction/src/master/">https://bitbucket.org/ox-ang/hierarchical_temporal_prediction/src/master/</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib111">Singer et al., 2023a</xref>). The V1 neural response data (<xref ref-type="bibr" rid="bib89">Ringach et al., 2002</xref>) used for comparison with the temporal prediction model in Figure 6 came from <ext-link ext-link-type="uri" xlink:href="http://ringachlab.net/">http://ringachlab.net/</ext-link> (&quot;Data &amp; Code&quot;, &quot;Orientation tuning in Macaque V1&quot;). The V1 image response data used to test the models included in Figure 9 were downloaded with permission from <ext-link ext-link-type="uri" xlink:href="https://github.com/sacadena/Cadena2019PlosCB">https://github.com/sacadena/Cadena2019PlosCB</ext-link> (<xref ref-type="bibr" rid="bib13">Cadena et al., 2019</xref>). The V1 movie response data used to test these models were collected in the Laboratory of Dario Ringach at UCLA and downloaded from <ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/vc/pvc-1">https://crcns.org/data-sets/vc/pvc-1</ext-link> (<xref ref-type="bibr" rid="bib73">Nauhaus and Ringach, 2007</xref>; <xref ref-type="bibr" rid="bib90">Ringach and Nauhaus, 2009</xref>). The code for the models and analyses shown in Figure 9 and the associated section can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/webstorms/StackTP">https://github.com/webstorms/StackTP</ext-link> (copy archived at <xref ref-type="bibr" rid="bib112">Singer et al., 2023b</xref>) and <ext-link ext-link-type="uri" xlink:href="https://github.com/webstorms/NeuralPred">https://github.com/webstorms/NeuralPred</ext-link> (copy archived at <xref ref-type="bibr" rid="bib113">Singer et al., 2023c</xref>). The movies used for training the models in Figure 9 are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/dataset/Natural_movies/24265498">https://figshare.com/articles/dataset/Natural_movies/24265498</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Hierarchical temporal prediction captures motion processing along the visual pathway. Ox-ang/hierarchical_temporal_prediction</data-title><source>Bitbucket</source><pub-id pub-id-type="accession" xlink:href="https://bitbucket.org/ox-ang/hierarchical_temporal_prediction/src/master/">ox-ang/hierarchical_temporal_prediction/src/master/</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Hierarchical temporal prediction captures motion processing along the visual pathway. Webstorms/StackTP</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/webstorms/StackTP">StackTP</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset3"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Hierarchical temporal prediction captures motion processing along the visual pathway. Webstorms/NeuralPred</data-title><source>GitHub</source><pub-id pub-id-type="accession" xlink:href="https://github.com/webstorms/NeuralPred">NeuralPred</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset4"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Hierarchical temporal prediction captures motion processing along the visual pathway. Figshare ID natural.zip</data-title><source>figshare</source><pub-id pub-id-type="accession" xlink:href="https://figshare.com/articles/dataset/Natural_movies/24265498">24265498</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Yosef Singer was supported by the University of Oxford Clarendon Fund, the Oppenheimer Memorial Trust and the Goodger and Schorstein Research Scholarship in Medical Sciences. Luke Taylor was funded by the University of Oxford Clarendon Fund. Andrew King, Ben Willmore and Nicol Harper were supported by the Wellcome Trust (WT108369/Z/2015/Z).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>2</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1364/josaa.2.000284</pub-id><pub-id pub-id-type="pmid">3973762</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Rajendran</surname><given-names>VG</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Omission responses in local field potentials in rat auditory cortex</article-title><source>BMC Biology</source><volume>21</volume><elocation-id>130</elocation-id><pub-id pub-id-type="doi">10.1186/s12915-023-01592-4</pub-id><pub-id pub-id-type="pmid">37254137</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Summation and inhibition in the frog’s retina</article-title><source>The Journal of Physiology</source><volume>119</volume><fpage>69</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1953.sp004829</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible principles underlying the transformations of sensory messages</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group><source>Sensory Communication</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>217</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bartunov</surname><given-names>S</given-names></name><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Marris</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Assessing the scalability of biologically-motivated deep learning algorithms and architectures</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1807.04587">https://doi.org/10.48550/arXiv.1807.04587</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The “independent components” of natural scenes are edge filters</article-title><source>Vision Research</source><volume>37</volume><fpage>3327</fpage><lpage>3338</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00121-1</pub-id><pub-id pub-id-type="pmid">9425547</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Lamblin</surname><given-names>P</given-names></name><name><surname>Popovici</surname><given-names>D</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Greedy layer-wise training of deep networks</article-title><source>Advances in Neural Information Processing Systems</source><volume>19</volume><fpage>153</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.7551/mitpress/7503.001.0001</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Deep learning of representations for unsupervised and transfer learning</article-title><source>JMLR Work Conf Proc</source><volume>27</volume><fpage>17</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-39593-2</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Representation learning: A review and new perspectives</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>35</volume><fpage>1798</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Wiskott</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Slow feature analysis yields a rich repertoire of complex cell properties</article-title><source>Journal of Vision</source><volume>5</volume><fpage>579</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1167/5.6.9</pub-id><pub-id pub-id-type="pmid">16097870</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Turner</surname><given-names>RE</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A structured model of video reproduces primary visual cortical organisation</article-title><source>PLOS Comput Biol</source><volume>5</volume><elocation-id>e1000495</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000495</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Predictability, complexity, and learning</article-title><source>Neural Computation</source><volume>13</volume><fpage>2409</fpage><lpage>2463</lpage><pub-id pub-id-type="doi">10.1162/089976601753195969</pub-id><pub-id pub-id-type="pmid">11674845</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Learning intermediate-level representations of form and motion from natural movies</article-title><source>Neural Computation</source><volume>24</volume><fpage>827</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00247</pub-id><pub-id pub-id-type="pmid">22168556</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Canziani</surname><given-names>A</given-names></name><name><surname>Culurciello</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>CortexNet: A generic network family for robust visual temporal representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1706.02735">https://doi.org/10.48550/arXiv.1706.02735</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalk</surname><given-names>M</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Toward a unified theory of efficient, predictive, and sparse coding</article-title><source>PNAS</source><volume>115</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1073/pnas.1711114115</pub-id><pub-id pub-id-type="pmid">29259111</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Paiton</surname><given-names>DM</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The sparse manifold transform</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1806.08887">https://doi.org/10.48550/arXiv.1806.08887</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id><pub-id pub-id-type="pmid">11405422</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cloherty</surname><given-names>SL</given-names></name><name><surname>Ibbotson</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Contrast-dependent phase sensitivity in V1 but not V2 of macaque visual cortex</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>434</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1152/jn.00539.2014</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conwell</surname><given-names>C</given-names></name><name><surname>Mayo</surname><given-names>D</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Katz</surname><given-names>B</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Barbu</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural regression, representational similarity, model zoology &amp; neural taskonomy at scale in rodent visual cortex</article-title><source>Neuroscience</source><volume>1</volume><fpage>5590</fpage><lpage>5607</lpage><pub-id pub-id-type="doi">10.1101/2021.06.18.448431</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creutzig</surname><given-names>F</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Predictive coding and the slowness principle: an information-theoretic approach</article-title><source>Neural Computation</source><volume>20</volume><fpage>1026</fpage><lpage>1041</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.01-07-455</pub-id><pub-id pub-id-type="pmid">18085988</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crist</surname><given-names>RE</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Learning to see: experience and attention in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>519</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1038/87470</pub-id><pub-id pub-id-type="pmid">11319561</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Ohzawa</surname><given-names>I</given-names></name><name><surname>Freeman</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Spatiotemporal organization of simple-cell receptive fields in the cat’s striate cortex. I. General characteristics and postnatal development</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>1091</fpage><lpage>1117</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.4.1091</pub-id><pub-id pub-id-type="pmid">8492151</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Jager</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>Delta modulation, a method of PCM Transmission using the 1-Unit Code</article-title><source>Philips Res Reports</source><volume>7</volume><fpage>442</fpage><lpage>466</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname><given-names>RL</given-names></name><name><surname>Yund</surname><given-names>EW</given-names></name><name><surname>Hepler</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The orientation and direction selectivity of cells in macaque visual cortex</article-title><source>Vision Research</source><volume>22</volume><fpage>531</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(82)90112-2</pub-id><pub-id pub-id-type="pmid">7112953</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>V</given-names></name><name><surname>Visin</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Guide to Convolution Arithmetic for Deep Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1603.07285">https://doi.org/10.48550/arXiv.1603.07285</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fabre-Thorpe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The characteristics and limits of rapid visual categorization</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>243</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00243</pub-id><pub-id pub-id-type="pmid">22007180</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>A</given-names></name><name><surname>Mahringer</surname><given-names>D</given-names></name><name><surname>Oyibo</surname><given-names>HK</given-names></name><name><surname>Petersen</surname><given-names>AV</given-names></name><name><surname>Leinweber</surname><given-names>M</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Experience-dependent spatial expectations in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1658</fpage><lpage>1664</lpage><pub-id pub-id-type="doi">10.1038/nn.4385</pub-id><pub-id pub-id-type="pmid">27618309</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Földiák</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Learning invariance from transformation sequences</article-title><source>Neural Computation</source><volume>3</volume><fpage>194</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1162/neco.1991.3.2.194</pub-id><pub-id pub-id-type="pmid">31167302</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gatys</surname><given-names>L</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Texture synthesis using Convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems 28</conf-name><fpage>262</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.265</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiger</surname><given-names>A</given-names></name><name><surname>Lenz</surname><given-names>P</given-names></name><name><surname>Stiller</surname><given-names>C</given-names></name><name><surname>Urtasun</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Vision meets robotics: The KITTI dataset</article-title><source>The International Journal of Robotics Research</source><volume>32</volume><fpage>1231</fpage><lpage>1237</lpage><pub-id pub-id-type="doi">10.1177/0278364913491297</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Crist</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The neural basis of perceptual learning</article-title><source>Neuron</source><volume>31</volume><fpage>681</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00424-x</pub-id><pub-id pub-id-type="pmid">11567610</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gizzi</surname><given-names>MS</given-names></name><name><surname>Katz</surname><given-names>E</given-names></name><name><surname>Schumer</surname><given-names>RA</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Selectivity for orientation and direction of motion of single neurons in cat striate and extrastriate visual cortex</article-title><source>Journal of Neurophysiology</source><volume>63</volume><fpage>1529</fpage><lpage>1543</lpage><pub-id pub-id-type="doi">10.1152/jn.1990.63.6.1529</pub-id><pub-id pub-id-type="pmid">2358891</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hawkins</surname><given-names>J</given-names></name><name><surname>Blakeslee</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>On Intelligence: How a New Understanding of the Brain Will Lead to the Creation of Truly Intelligent Machines.</source><publisher-name>Times Books</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Osindero</surname><given-names>S</given-names></name><name><surname>Teh</surname><given-names>YW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A fast learning algorithm for deep belief nets</article-title><source>Neural Computation</source><volume>18</volume><fpage>1527</fpage><lpage>1554</lpage><pub-id pub-id-type="doi">10.1162/neco.2006.18.7.1527</pub-id><pub-id pub-id-type="pmid">16764513</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Quantifying variability in neural responses and its application for the validation of model predictions</article-title><source>Network</source><volume>15</volume><fpage>91</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1088/0954-898X-15-2-002</pub-id><pub-id pub-id-type="pmid">15214701</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Ma</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Fang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Han</surname><given-names>C</given-names></name><name><surname>Fang</surname><given-names>C</given-names></name><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Yan</surname><given-names>K</given-names></name><name><surname>Lu</surname><given-names>HD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual motion processing in Macaque V2</article-title><source>Cell Reports</source><volume>25</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.09.014</pub-id><pub-id pub-id-type="pmid">30282025</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Rao</surname><given-names>RPN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predictive coding</article-title><source>Wiley Interdisciplinary Reviews. Cognitive Science</source><volume>2</volume><fpage>580</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1002/wcs.142</pub-id><pub-id pub-id-type="pmid">26302308</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><volume>148</volume><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id><pub-id pub-id-type="pmid">14403679</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>Receptive fields and functional architecture in two nonstriate visual areas (18 and 19) of the cat</article-title><source>Journal of Neurophysiology</source><volume>28</volume><fpage>229</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1152/jn.1965.28.2.229</pub-id><pub-id pub-id-type="pmid">14283058</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Receptive fields and functional architecture of monkey striate cortex</article-title><source>The Journal of Physiology</source><volume>195</volume><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id><pub-id pub-id-type="pmid">4966457</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberman</surname><given-names>AD</given-names></name><name><surname>Feller</surname><given-names>MB</given-names></name><name><surname>Chapman</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms underlying development of visual maps and receptive fields</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>479</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125533</pub-id><pub-id pub-id-type="pmid">18558864</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurri</surname><given-names>J</given-names></name><name><surname>Hyvärinen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Simple-cell-like receptive fields maximize temporal coherence in natural video</article-title><source>Neural Computation</source><volume>15</volume><fpage>663</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1162/089976603321192121</pub-id><pub-id pub-id-type="pmid">12620162</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Hoyer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Emergence of phase- and shift-invariant features by decomposition of natural images into independent feature subspaces</article-title><source>Neural Computation</source><volume>12</volume><fpage>1705</fpage><lpage>1720</lpage><pub-id pub-id-type="doi">10.1162/089976600300015312</pub-id><pub-id pub-id-type="pmid">10935923</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Hoyer</surname><given-names>PO</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images</article-title><source>Vision Research</source><volume>41</volume><fpage>2413</fpage><lpage>2423</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00114-6</pub-id><pub-id pub-id-type="pmid">11459597</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Hurri</surname><given-names>J</given-names></name><name><surname>Väyrynen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bubbles: a unifying framework for low-level statistical properties of natural image sequences</article-title><source>Journal of the Optical Society of America A</source><volume>20</volume><fpage>1237</fpage><lpage>1252</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.20.001237</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title><conf-name>Proceedings of the 32nd International Conference on Machine Learning (ICML ’15)</conf-name><fpage>448</fpage><lpage>456</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><volume>58</volume><fpage>1233</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1152/jn.1987.58.6.1233</pub-id><pub-id pub-id-type="pmid">3437332</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name><name><surname>Bucy</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>New results in linear filtering and prediction theory</article-title><source>Journal of Basic Engineering</source><volume>83</volume><fpage>95</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1115/1.3658902</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karklin</surname><given-names>Y</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Emergence of complex cell properties by learning to generalize in natural scenes</article-title><source>Nature</source><volume>457</volume><fpage>83</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1038/nature07481</pub-id><pub-id pub-id-type="pmid">19020501</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Einhäuser</surname><given-names>W</given-names></name><name><surname>Dümmer</surname><given-names>O</given-names></name><name><surname>König</surname><given-names>P</given-names></name><name><surname>Körding</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title>Extracting slow subspaces from natural videos leads to complex cells. In</chapter-title><person-group person-group-type="editor"><name><surname>Dorffner</surname><given-names>G</given-names></name><name><surname>Bischof</surname><given-names>H</given-names></name><name><surname>Hornik</surname><given-names>K</given-names></name></person-group><source>ICANN 2001. Lecture Notes in Computer Science</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin, Heidelberg</publisher-loc><fpage>1075</fpage><lpage>1080</lpage><pub-id pub-id-type="doi">10.1007/3-540-44668-0_149</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Freeman</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Direction selectivity of neurons in the visual cortex is non-linear and lamina-dependent</article-title><source>The European Journal of Neuroscience</source><volume>43</volume><fpage>1389</fpage><lpage>1399</lpage><pub-id pub-id-type="doi">10.1111/ejn.13223</pub-id><pub-id pub-id-type="pmid">26929101</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiorpes</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual development in primates: Neural mechanisms and critical periods</article-title><source>Developmental Neurobiology</source><volume>75</volume><fpage>1080</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1002/dneu.22276</pub-id><pub-id pub-id-type="pmid">25649764</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Einhäuser</surname><given-names>W</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>How are complex cell properties adapted to the statistics of natural stimuli?</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>206</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1152/jn.00149.2003</pub-id><pub-id pub-id-type="pmid">12904330</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuffler</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="1953">1953</year><article-title>Discharge patterns and functional organization of mammalian retina</article-title><source>Journal of Neurophysiology</source><volume>16</volume><fpage>37</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1152/jn.1953.16.1.37</pub-id><pub-id pub-id-type="pmid">13035466</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Grosse</surname><given-names>R</given-names></name><name><surname>Ranganath</surname><given-names>R</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (ICML 2009)</article-title><conf-name>Proceedings of the 26th International Conference on Machine Learning</conf-name><fpage>609</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1145/1553374.1553453</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lempel</surname><given-names>AA</given-names></name><name><surname>Nielsen</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ferrets as a model for higher-level visual motion processing</article-title><source>Current Biology</source><volume>29</volume><fpage>179</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.017</pub-id><pub-id pub-id-type="pmid">30595516</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lies</surname><given-names>JP</given-names></name><name><surname>Häfner</surname><given-names>RM</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Slowness and sparseness have diverging effects on complex cell learning</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003468</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003468</pub-id><pub-id pub-id-type="pmid">24603197</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Deny</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A unified theory of early visual representations from retina to cortex through anatomically constrained deep CNNs</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/511535</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Livezey</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Gabor_Fit</data-title><version designator="swh:1:rev:5337ebacdf44dd7709152e7730bc5c29495c5329">swh:1:rev:5337ebacdf44dd7709152e7730bc5c29495c5329</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:1de99320e329ce13dc1446ecf0ed6244714e19d8;origin=https://github.com/JesseLivezey/gabor_fit;visit=swh:1:snp:b3fa2972fc53c3308f2970cd5761dd753352dcc7;anchor=swh:1:rev:5337ebacdf44dd7709152e7730bc5c29495c5329">https://archive.softwareheritage.org/swh:1:dir:1de99320e329ce13dc1446ecf0ed6244714e19d8;origin=https://github.com/JesseLivezey/gabor_fit;visit=swh:1:snp:b3fa2972fc53c3308f2970cd5761dd753352dcc7;anchor=swh:1:rev:5337ebacdf44dd7709152e7730bc5c29495c5329</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lotter</surname><given-names>W</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep predictive coding networks for video prediction and unsupervised learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1605.08104">https://doi.org/10.48550/arXiv.1605.08104</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lotter</surname><given-names>W</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A neural network trained for prediction mimics diverse features of biological neurons and perception</article-title><source>Nature Machine Intelligence</source><volume>2</volume><fpage>210</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-0170-9</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matteucci</surname><given-names>G</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unsupervised experience with temporal continuity of the visual environment is causally involved in the development of V1 complex cells</article-title><source>Science Advances</source><volume>6</volume><elocation-id>eaba3742</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aba3742</pub-id><pub-id pub-id-type="pmid">32523998</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>M</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The neural code of the retina</article-title><source>Neuron</source><volume>22</volume><fpage>435</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80700-x</pub-id><pub-id pub-id-type="pmid">10197525</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mineault</surname><given-names>PJ</given-names></name><name><surname>Bakhtiari</surname><given-names>S</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Your head is there to move you around: Goal-driven models of the primate dorsal pathway</article-title><source>Neuroscience</source><volume>1</volume><fpage>28757</fpage><lpage>28771</lpage><pub-id pub-id-type="doi">10.1101/2021.07.09.451701</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Thompson</surname><given-names>ID</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1978">1978a</year><article-title>Receptive field organization of complex cells in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><volume>283</volume><fpage>79</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1978.sp012489</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Thompson</surname><given-names>ID</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1978">1978b</year><article-title>Spatial summation in the receptive fields of simple cells in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><volume>283</volume><fpage>53</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1978.sp012488</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Gizzi</surname><given-names>MS</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1985">1985</year><chapter-title>The analysis of moving visual patterns in</chapter-title><person-group person-group-type="editor"><name><surname>Chagas</surname><given-names>C</given-names></name><name><surname>Gattass</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>C</given-names></name></person-group><source>Pattern Recognition Mechanisms</source><publisher-loc>Rome</publisher-loc><publisher-name>Vatican Press</publisher-name><fpage>117</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1007/978-3-662-09224-8</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Visual response properties of striate cortical neurons projecting to area MT in macaque monkeys</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>7733</fpage><lpage>7741</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-23-07733.1996</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Precise alignment of micromachined electrode arrays with V1 functional maps</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>3781</fpage><lpage>3789</lpage><pub-id pub-id-type="doi">10.1152/jn.00120.2007</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Kong</surname><given-names>NCL</given-names></name><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1011506</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011506</pub-id><pub-id pub-id-type="pmid">37782673</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nowak</surname><given-names>LG</given-names></name><name><surname>Munk</surname><given-names>MHJ</given-names></name><name><surname>Girard</surname><given-names>P</given-names></name><name><surname>Bullier</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Visual latencies in areas V1 and V2 of the macaque monkey</article-title><source>Visual Neuroscience</source><volume>12</volume><fpage>371</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1017/s095252380000804x</pub-id><pub-id pub-id-type="pmid">7786857</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nurminen</surname><given-names>L</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Bijanzadeh</surname><given-names>M</given-names></name><name><surname>Federer</surname><given-names>F</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Top-down feedback controls spatial summation and response amplitude in primate visual cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2281</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04500-5</pub-id><pub-id pub-id-type="pmid">29892057</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Deny</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The emergence of multiple retinal cell types through efficient coding of natural movies</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/458737</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Lewis</surname><given-names>R</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Action-conditional video prediction using deep networks in Atari games</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1507.08750</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title><source>Vision Research</source><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00169-7</pub-id><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Sparse coding of time-varying natural images</article-title><source>Journal of Vision</source><volume>2</volume><elocation-id>130</elocation-id><pub-id pub-id-type="doi">10.1167/2.7.130</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>Wyatte</surname><given-names>D</given-names></name><name><surname>Rohrlich</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning through time in the thalamocortical loops</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1407.3432</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osindero</surname><given-names>S</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Topographic product models applied to natural scene statistics</article-title><source>Neural Computation</source><volume>18</volume><fpage>381</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1162/089976606775093936</pub-id><pub-id pub-id-type="pmid">16378519</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Berezovskii</surname><given-names>VK</given-names></name><name><surname>Born</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic properties of neurons in cortical area MT in alert and anaesthetized macaque monkeys</article-title><source>Nature</source><volume>414</volume><fpage>905</fpage><lpage>908</lpage><pub-id pub-id-type="doi">10.1038/414905a</pub-id><pub-id pub-id-type="pmid">11780062</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palagina</surname><given-names>G</given-names></name><name><surname>Meyer</surname><given-names>JF</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Complex visual motion representation in mouse area V1</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>164</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0997-16.2017</pub-id><pub-id pub-id-type="pmid">28053039</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Palm</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Prediction as a Candidate for Learning Deep Hierarchical Models of Data.</source><publisher-loc>Denmark</publisher-loc><publisher-name>DTU Informatic-Msc.-2012</publisher-name></element-citation></ref><ref id="bib87"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Raina</surname><given-names>R</given-names></name><name><surname>Battle</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Packer</surname><given-names>B</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Self-taught learning: transfer learning from unlabeled data</article-title><conf-name>Proceedings of the 24th International Conference on Machine Learning</conf-name><fpage>759</fpage><lpage>766</lpage></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname><given-names>DL</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Orientation selectivity in macaque V1: diversity and laminar dependence</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5639</fpage><lpage>5651</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-13-05639.2002</pub-id><pub-id pub-id-type="pmid">12097515</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ringach</surname><given-names>D</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Single- and Multi-Unit Recordings from Monkey Primary Visual Cortex</source><publisher-name>CRCNS</publisher-name><pub-id pub-id-type="doi">10.6080/K0WD3XH6</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rochefort</surname><given-names>NL</given-names></name><name><surname>Narushima</surname><given-names>M</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Marandi</surname><given-names>N</given-names></name><name><surname>Hill</surname><given-names>DN</given-names></name><name><surname>Konnerth</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Development of direction selectivity in mouse cortical neurons</article-title><source>Neuron</source><volume>71</volume><fpage>425</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.013</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodman</surname><given-names>HR</given-names></name><name><surname>Albright</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Single-unit analysis of pattern-motion selective properties in the middle temporal visual area (MT)</article-title><source>Experimental Brain Research</source><volume>75</volume><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1007/BF00248530</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>J</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The representation of prediction error in auditory cortex</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005058</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005058</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Imagenet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Spatiotemporal elements of macaque V1 receptive fields</article-title><source>Neuron</source><volume>46</volume><fpage>945</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.021</pub-id><pub-id pub-id-type="pmid">15953422</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>How MT cells analyze the motion of visual patterns</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1421</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1038/nn1786</pub-id><pub-id pub-id-type="pmid">17041595</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How linear are auditory cortical responses</article-title><source>Adv Neural Information Proc Systems</source><volume>15</volume><fpage>109</fpage><lpage>116</lpage></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salisbury</surname><given-names>JM</given-names></name><name><surname>Palmer</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimal prediction in the retina and natural motion statistics</article-title><source>Journal of Statistical Physics</source><volume>162</volume><fpage>1309</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1007/s10955-015-1439-y</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scannell</surname><given-names>JW</given-names></name><name><surname>Sengpiel</surname><given-names>F</given-names></name><name><surname>Tovée</surname><given-names>MJ</given-names></name><name><surname>Benson</surname><given-names>PJ</given-names></name><name><surname>Blakemore</surname><given-names>C</given-names></name><name><surname>Young</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Visual motion processing in the anterior ectosylvian sulcus of the cat</article-title><source>Journal of Neurophysiology</source><volume>76</volume><fpage>895</fpage><lpage>907</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.76.2.895</pub-id><pub-id pub-id-type="pmid">8871207</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>P</given-names></name><name><surname>Finlay</surname><given-names>B</given-names></name><name><surname>Volman</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Quantitative studies of single-cell properties in monkey striate cortex. I. Spatiotemporal organization of receptive fields</article-title><source>J Neurophysiol</source><volume>39</volume><fpage>1288</fpage><lpage>1319</lpage><pub-id pub-id-type="doi">10.1152/jn.1976.39.6.1288</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname><given-names>B</given-names></name><name><surname>Tan</surname><given-names>AYY</given-names></name><name><surname>Corey</surname><given-names>J</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Emergence of orientation selectivity in the mammalian visual pathway</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>10616</fpage><lpage>10624</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0404-13.2013</pub-id><pub-id pub-id-type="pmid">23804085</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoppe</surname><given-names>O</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Measuring the performance of neural models</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2016.00010</pub-id><pub-id pub-id-type="pmid">26903851</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brain-score: Which artificial neural network for object recognition is most brain-like?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-triggered neural characterization</article-title><source>Journal of Vision</source><volume>6</volume><fpage>484</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1167/6.4.13</pub-id><pub-id pub-id-type="pmid">16889482</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapley</surname><given-names>R</given-names></name><name><surname>Perry</surname><given-names>VH</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Cat and monkey retinal ganglion cells and their visual functional roles</article-title><source>Trends in Neurosciences</source><volume>9</volume><fpage>229</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(86)90064-0</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimazaki</surname><given-names>H</given-names></name><name><surname>Shinomoto</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A method for selecting the bin size of A time histogram</article-title><source>Neural Computation</source><volume>19</volume><fpage>1503</fpage><lpage>1527</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.19.6.1503</pub-id><pub-id pub-id-type="pmid">17444758</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A model of neuronal responses in visual area MT</article-title><source>Vision Research</source><volume>38</volume><fpage>743</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00183-1</pub-id><pub-id pub-id-type="pmid">9604103</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>E</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Schwartz</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Characterization of neural responses with stochastic stimuli In</source><person-group person-group-type="editor"><name><surname>Gazzaniga</surname><given-names>M</given-names></name></person-group><chapter-title>The Cognitive Neurosciences, III</chapter-title><publisher-name>MIT Press</publisher-name><fpage>327</fpage><lpage>338</lpage></element-citation></ref><ref id="bib109"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1409.1556">https://doi.org/10.48550/arXiv.1409.1556</ext-link></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Teramoto</surname><given-names>Y</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sensory cortex is optimized for prediction of future input</article-title><source>eLife</source><volume>7</volume><elocation-id>e31557</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31557</pub-id><pub-id pub-id-type="pmid">29911971</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023a</year><data-title>Hierarchical_Temporal_Prediction</data-title><version designator="swh:1:rev:62a08ba1dcba502b4c086d8406843b0621df79d3">swh:1:rev:62a08ba1dcba502b4c086d8406843b0621df79d3</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3ccf20dd0dce9513c6a35b0e43b21895594d50c7;origin=https://bitbucket.org/ox-ang/hierarchical_temporal_prediction/src/master/;visit=swh:1:snp:8d52e04fe9097075dbef6a6d430022256b2b4686;anchor=swh:1:rev:62a08ba1dcba502b4c086d8406843b0621df79d3">https://archive.softwareheritage.org/swh:1:dir:3ccf20dd0dce9513c6a35b0e43b21895594d50c7;origin=https://bitbucket.org/ox-ang/hierarchical_temporal_prediction/src/master/;visit=swh:1:snp:8d52e04fe9097075dbef6a6d430022256b2b4686;anchor=swh:1:rev:62a08ba1dcba502b4c086d8406843b0621df79d3</ext-link></element-citation></ref><ref id="bib112"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023b</year><data-title>Stacktp</data-title><version designator="swh:1:rev:1ecfdc38caeb36cd3630354693a9453c4446f64d">swh:1:rev:1ecfdc38caeb36cd3630354693a9453c4446f64d</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:496903ae78c499372b24d8b7469fec420020467a;origin=https://github.com/webstorms/StackTP;visit=swh:1:snp:9a77d5dd75221c1fbb86edd5e9e4b39ce2e03569;anchor=swh:1:rev:1ecfdc38caeb36cd3630354693a9453c4446f64d">https://archive.softwareheritage.org/swh:1:dir:496903ae78c499372b24d8b7469fec420020467a;origin=https://github.com/webstorms/StackTP;visit=swh:1:snp:9a77d5dd75221c1fbb86edd5e9e4b39ce2e03569;anchor=swh:1:rev:1ecfdc38caeb36cd3630354693a9453c4446f64d</ext-link></element-citation></ref><ref id="bib113"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2023">2023c</year><data-title>Neuralpred</data-title><version designator="swh:1:rev:1484b1ae509bf58a2cc2f711e525fd1d225b9b79">swh:1:rev:1484b1ae509bf58a2cc2f711e525fd1d225b9b79</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:33bd8bcf0c2bbdccb4a8b979d73e6d655c3cdf62;origin=https://github.com/webstorms/NeuralPred;visit=swh:1:snp:6339994a7950c54e7ba2ea5c36f12d918cd3535e;anchor=swh:1:rev:1484b1ae509bf58a2cc2f711e525fd1d225b9b79">https://archive.softwareheritage.org/swh:1:dir:33bd8bcf0c2bbdccb4a8b979d73e6d655c3cdf62;origin=https://github.com/webstorms/NeuralPred;visit=swh:1:snp:6339994a7950c54e7ba2ea5c36f12d918cd3535e;anchor=swh:1:rev:1484b1ae509bf58a2cc2f711e525fd1d225b9b79</ext-link></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skottun</surname><given-names>BC</given-names></name><name><surname>De Valois</surname><given-names>RL</given-names></name><name><surname>Grosof</surname><given-names>DH</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Albrecht</surname><given-names>DG</given-names></name><name><surname>Bonds</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Classifying simple and complex cells on the basis of response modulation</article-title><source>Vision Research</source><volume>31</volume><fpage>1079</fpage><lpage>1086</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(91)90033-2</pub-id><pub-id pub-id-type="pmid">1909826</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Dynamics of motion signaling by neurons in macaque area MT</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>220</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1038/nn1382</pub-id><pub-id pub-id-type="pmid">15657600</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Softky</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Unsupervised pixel-prediction</article-title><source>Adv Neural Information Proc Systems</source><volume>8</volume><fpage>809</fpage><lpage>815</lpage></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spratling</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A review of predictive coding algorithms</article-title><source>Brain and Cognition</source><volume>112</volume><fpage>92</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2015.11.003</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Dubs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Predictive coding: a fresh view of inhibition in the retina</article-title><source>Proceedings of the Royal Society of London. Series B</source><volume>216</volume><fpage>427</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1098/rspb.1982.0085</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Mansimov</surname><given-names>E</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unsupervised learning of video representations using LSTMs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1502.04681">https://doi.org/10.48550/arXiv.1502.04681</ext-link></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Diverse deep neural networks all predict human inferior temporal cortex well, after training and fitting</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2044</fpage><lpage>2064</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01755</pub-id><pub-id pub-id-type="pmid">34272948</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>S</given-names></name><name><surname>Ribot</surname><given-names>J</given-names></name><name><surname>Imamura</surname><given-names>K</given-names></name><name><surname>Tani</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Orientation-restricted continuous visual exposure induces marked reorganization of orientation maps in early life</article-title><source>NeuroImage</source><volume>30</volume><fpage>462</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.09.056</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>L</given-names></name><name><surname>Nitschke</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Improving deep learning with generic data augmentation</article-title><conf-name>2018 IEEE Symposium Series on Computational Intelligence (SSCI</conf-name><conf-loc>Bangalore, India</conf-loc><fpage>1542</fpage><lpage>1547</lpage><pub-id pub-id-type="doi">10.1109/SSCI.2018.8628742</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>Ruderman</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B</source><volume>265</volume><fpage>2315</fpage><lpage>2320</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0577</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>van der Schaaf</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B</source><volume>265</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0303</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vondrick</surname><given-names>C</given-names></name><name><surname>Pirsiavash</surname><given-names>H</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Generating videos with scene dynamics</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1609.02612">https://doi.org/10.48550/arXiv.1609.02612</ext-link></element-citation></ref><ref id="bib127"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vondrick</surname><given-names>C</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generating the future with adversarial transformers</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><fpage>2992</fpage><lpage>3000</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.319</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>HX</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Properties of pattern and component direction-selective cells in area MT of the macaque</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2705</fpage><lpage>2720</lpage><pub-id pub-id-type="doi">10.1152/jn.00639.2014</pub-id><pub-id pub-id-type="pmid">26561603</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Long</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>PredRNN++: Towards a Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1804.06300</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>PS</given-names></name><name><surname>Long</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2103.09504</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weghenkel</surname><given-names>B</given-names></name><name><surname>Wiskott</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Slowness as a proxy for temporal predictability: An empirical comparison</article-title><source>Neural Computation</source><volume>30</volume><fpage>1151</fpage><lpage>1179</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_01070</pub-id><pub-id pub-id-type="pmid">29566353</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theories of error back-propagation in the brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>235</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.12.005</pub-id><pub-id pub-id-type="pmid">30704969</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Wu</surname><given-names>MCK</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The berkeley wavelet transform: a biologically inspired orthogonal wavelet transform</article-title><source>Neural Computation</source><volume>20</volume><fpage>1537</fpage><lpage>1564</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.05-07-513</pub-id><pub-id pub-id-type="pmid">18194102</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiskott</surname><given-names>L</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Slow feature analysis: unsupervised learning of invariances</article-title><source>Neural Computation</source><volume>14</volume><fpage>715</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1162/089976602317318938</pub-id><pub-id pub-id-type="pmid">11936959</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeki</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Functional organization of a visual area in the posterior bank of the superior temporal sulcus of the rhesus monkey</article-title><source>The Journal of Physiology</source><volume>236</volume><fpage>549</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1974.sp010452</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Improving deep neural networks using softplus units</article-title><conf-name>2015 International Joint Conference on Neural Networks (IJCNN</conf-name><conf-loc>Killarney, Ireland</conf-loc><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2015.7280459</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Yan</surname><given-names>S</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised neural network models of the ventral visual stream</article-title><source>PNAS</source><volume>118</volume><elocation-id>2014196118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2014196118</pub-id><pub-id pub-id-type="pmid">33431673</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.52599.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>This valuable work shows similarities between a multilayer, convolutional neural network trained to predict its next input and physiological features of visual processing in the brain. These solid results build on the authors' previous work and compare the match to real visual processing obtained by a hierarchical predictive network to that obtained by several other popular artificial neural networks. This work will be of interest to systems neuroscientists as well as computer scientists looking to make connections between normative theories of neural organization and training objectives in machine learning.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.52599.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Yamins</surname><given-names>Dan</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Hierarchical temporal prediction captures motion processing along the visual pathway&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Dan Yamins (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>This paper expands on a predictive-coding-like unsupervised learning procedure and applies it to natural videos, training a multilayer, convolutional neural network with the objective of predicting the next input. The resulting network is qualitatively compared to several known neurophysiological features of the (dorsal) visual pathway, focusing on V1, but also comparing model results to receptive field properties of the LGN and area MT. The authors find center surround, simple, and complex cell behavior like that found in the retina, LGN, and V1. They also find units sensitive to drifting plaids. Matches between the predictive performance of this network are better than for a basic autoencoder and also better than several other models that ablate aspects of the proposed encoding rule. The paper hypothesizes that prediction might be a fundamental principle of self-organization in the visual system.</p><p>Essential revisions:</p><p>This paper advances a reasonable hypothesis about a very important goal: developing an unsupervised neural network model of the visual system. However, the current draft is not ready for publication because empirical validation of the model and comparison to equally reasonable alternatives are weak. The following revisions were deemed essential by the reviewing team:</p><p>1) Key controls are missing: a comparison between the results of the present work and other obvious alternative hypotheses should be included. Here are five comparisons that should be made, in order of increasing power:</p><p>a) A sparse autoencoder. A stacked autoencoder is tested, but it's not said whether this model was regularized with an activity sparsity penalty on the bottleneck or not. If not, this should be done.</p><p>b) A model based on temporal continuity / slowness rather than predictability. It's likely that pretty much the same spatial tuning properties would emerge, but that subtle differences in the temporal dynamics would be detectable. This would be particularly interesting in the light of a recent paper by Weghenkel and Wiskott [1] (2018, Neural Computation), which has shown no advantage of predictability over slowness on a large set of real world data sets.</p><p>c) A Gabor-wavelet-based model of V1. A good example of that is the Berkeley Wavelet Transform (BTW) in Willmore et al. [2]. Obviously this might not have the temporal processing facets of the authors' model, but the main comparisons made to data in Figure 6 don't require this. A very basic question is: how much better (or worse?) is the authors' model as a model of V1, as compared to the hard-coded (non-learned) &quot;standard&quot; model of V1 that the BWT represents?</p><p>d) The PredNet [3] and PredRNN [4] and PredRNN++ [5] models. These are predictive coding models similar to the one the authors propose. The authors of course do cite [3], but do not compare to its results. They definitely should. Merely noting that Prednet &quot;has not been demonstrated [to] capture the phenomena that we describe …. &quot; does not mean that PredNet *wouldn't* capture these phenomena. Being fairly familiar with PredNet , we imagine that it could indeed capture these features. Showing that it does not, and that the current proposal is thus a better fit to the data, is a burden that is clearly on the authors in the case. The code for prednet is publicly released and could easily be downloaded and run by the authors (trained on their training set). We also suggest strongly that the authors look at PredRNN as well, which may be substantially better than PredNet.</p><p>e) The early layers of a supervised deep neural network. This is an important baseline -- how much better (if at all) are these authors' models at matching V1 data than the supervised network, which is (as the authors correctly point out) obviously trained in a deeply unbiologial way? Work such as that of Cadena et al. [6] clearly shows that the supervised network actually gives very reasonable results in V1 -- in fact, it's the state-of-the-art model of V1 in the literature, at least to our knowledge. How much better is their unsupervised network than this model at matching V1? If it's not better, then how big is the gap, and why would it exist? (If a supposedly much more biologically-correct model isn't substantially better than the unbiological one at matching biology data, has it really contributed a major advance?)</p><p>One thing that will naturally come up in making a comparison like this is that the authors have chosen a particular imageset (moving animals) for training. Perhaps it will arise that their model is not trained in a sufficiently general way to compare favorably with a model trained on a large dataset like ImageNet. (Of course, this is still a fair comparison since neither their model nor the deepnet model would have been trained on the set used for testing, e.g. oriented gratings, or some other set of natural or naturalistic stimuli.) To claim that a major advance in modeling V1 has been made, something along these lines needs to be included. Perhaps the authors could use large video datasets like Kinetics [7] or Moments in Time [8] or the Chicago Motion Database (https://cmd.rcc.uchicago.edu) as a replacement for their animals dataset.</p><p>2) The comparisons to neural data are weak and qualitative. Improving that w.r.t. V1 *or* MT would be a major and sufficient revision for publication. The network learns mostly center surround, simple and complex cell behavior, which is conventionally assigned to retina/LGN/V1, but other features of V1 and higher-order visual neurons are not observed. No end- or side-inhibition has been observed, and no object-sensitive units. Only a very small fraction of units sensitive to drifting plaids were found in the 4th stack/layer. Given that, billing the model as a candidate for explaining the whole visual system seems much overstated. The authors could capitalize more on the dynamics of the receptive fields in V1 or MT, which was an interesting result not so often obtained by other models, but that has been investigated much less and was not compared to experimental data (apart from motion sensitivity). In whichever way possible, more detailed comparisons to V1 or MT data are needed. Our suggestions are:</p><p>a-i) For V1, the comparisons in Figure 6 are fine, and represent a good first step at comparing models to the data in a gross way. However, it would be great if a slightly more quantitative approach was taken -- e.g. measuring model-similarity in some quantified way, especially to compare between the author's preferred model and the controls suggested in 1) above.</p><p>a-ii) Also for V1: A much stronger comparison would be to do something as in Cadena et al. [6]. Specifically, Cadena et al. build a neuron-by-neuron regression model from their model to real V1 neurons, on a large set of real-world and naturalistic images. That work shows that, on this type of high-resolution comparison, there is something substantially better as a model of V1 than the standard hand-coded gabor BWT -- namely, the early intermediate layer of a categorization-trained deepnet. The state of the field has now moved to a point where models are being separated not by coarse measures like what is shown in Figure 6, but rather these much more detailed, real-world-stimulus-based metrics. We think the authors need to address comparisons at this level of resolution, or else it's really hard to know whether their model has made any substantive advance. It's not clear whether the data from Cadena et al. is readily available (though we suspect that Andreas Tolias, the data generator for that paper, would provide it for this purpose if asked). However, there is (or at least used to be) publicly available data from the Neural Prediction Challenge -- definitely worth getting this or similar data from Jack Gallant at Berkeley. (Or any other source that would allow for a much more direct model-to-neuron prediction assessment across naturalistic stimuli.)</p><p>b) For MT: the comparison here is quite thin. What the authors have done seems to barely support the claim that their hierarchical model &quot;can capture how tuning properties change across multiple levels of the visual system&quot;. More needs to be done here. Several papers have shown such things, mostly (as the authors note) based on supervised models. E.g [9-11] show comparisons of various intermediate layers of a NN to V1, V4, PIT, AIT areas in the ventral visual pathway. An unsupervised model that did the equivalent of this would be a significant advance. To make a claim like what the authors are saying in this draft, there needs to be some equally strong data comparison, but with MT data. Shinji Nishimoto and Jack Gallant have collected data that would be useful for this comparison, but it's not clear whether it would be easy to get access to that or similar MT data.</p><p>Comparing to coding in the ventral stream might be an alternative if MT data are not available. V4 and IT data is easily available from Jim DiCarlo (e.g. the data for [9-10]). The authors could definitely check out how well their higher model layers regressed those data and see if they could sustain a claim about matching &quot;multiple levels of the visual system&quot;. (But perhaps it would be an unfair comparison? Do the authors think their model would have any power for describing the ventral pathway?)</p><p>If these more detailed comparisons cannot be made, the claims about matching &quot;multiple levels of the visual system&quot; must be removed or significantly modified.</p><p>[1] Weghenkel, Björn, and Laurenz Wiskott. &quot;Slowness as a proxy for temporal predictability: An empirical comparison.&quot; Neural computation 30, no. 5 (2018): 1151-1179.</p><p>[2] Willmore, Ben, Ryan J. Prenger, Michael C-K. Wu, and Jack L. Gallant. &quot;The berkeley wavelet transform: a biologically inspired orthogonal wavelet transform.&quot; Neural computation 20, no. 6 (2008): 1537-1564.</p><p>[3] Lotter, William, Gabriel Kreiman, and David Cox. &quot;Deep predictive coding networks for video prediction and unsupervised learning.&quot; arXiv preprint arXiv:1605.08104 (2016).</p><p>[4] Wang, Yunbo, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and S. Yu Philip. &quot;Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms.&quot; In Advances in Neural Information Processing Systems, pp. 879-888. 2017.</p><p>[5] Wang, Yunbo, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. &quot;Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning.&quot; arXiv preprint arXiv:1804.06300 (2018).</p><p>[6] Cadena, Santiago A., George H. Denfield, Edgar Y. Walker, Leon A. Gatys, Andreas S. Tolias, Matthias Bethge, and Alexander S. Ecker. &quot;Deep convolutional models improve predictions of macaque V1 responses to natural images.&quot; PLoS computational biology 15, no. 4 (2019): e1006897.</p><p>[7] https://deepmind.com/research/open-source/kinetics</p><p>[8] http://moments.csail.mit.edu/</p><p>[9] Yamins, Daniel LK, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, and James J. DiCarlo. &quot;Performance-optimized hierarchical models predict neural responses in higher visual cortex.&quot; Proceedings of the National Academy of Sciences 111, no. 23 (2014): 8619-8624.</p><p>[10] Nayebi, Aran, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli, David Sussillo, James J. DiCarlo, and Daniel L. Yamins. &quot;Task-Driven convolutional recurrent models of the visual system.&quot; In Advances in Neural Information Processing Systems, pp. 5290-5301. 2018.</p><p>[11] Khaligh-Razavi, Seyed-Mahdi, and Nikolaus Kriegeskorte. &quot;Deep supervised, but not unsupervised, models may explain IT cortical representation.&quot; PLoS computational biology 10, no. 11 (2014): e1003915.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.52599.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>This paper advances a reasonable hypothesis about a very important goal: developing an unsupervised neural network model of the visual system. However, the current draft is not ready for publication because empirical validation of the model and comparison to equally reasonable alternatives are weak. The following revisions were deemed essential by the reviewing team:</p><p>1) Key controls are missing: a comparison between the results of the present work and other obvious alternative hypotheses should be included. Here are five comparisons that should be made, in order of increasing power:</p></disp-quote><p>The original purpose of this research advance was to demonstrate that the unsupervised temporal prediction model described in our original <italic>eLife</italic> paper (Singer et al., 2018) as an explanation for the receptive field properties of neurons in primary visual (and auditory) cortex, could be extended to a hierarchical form that reproduced receptive field properties at different levels of the visual pathway. This was not intended to be a comprehensive quantitative comparison of different models of the visual pathway, which we think would go beyond the requirements of a research advance.</p><p>However, we certainly see the merits of the reviewers’ suggestions, and we have now made many of the requested changes. This has been a very substantial undertaking because it required the development of two new hierarchical models based on different principles (sparsity and slowness), adaptation of previously published models, and the building of the model testing framework.</p><p>As set out in the following, our new results demonstrate that our temporal prediction model is a leading contender among other principles for explaining the organization of the visual pathway. Attempting to identify a single winner would be misleading, because of variation between the models in their size, aims and scope, as well as implementational factors such as variation in performance over different training sets and the possibility that hyperparameters may not be perfectly optimized. Accounting fully for all of these factors is well beyond the scope of this study.</p><p>With this in mind, we have implemented the majority of the models that were requested and compared them quantitatively to the hierarchical temporal prediction model. We have focused on what we view as the most important and challenging comparison, the prediction of neural responses to natural stimuli.</p><p>As the additions and edits that we have made to the Results, Discussion and Methods are extensive and relevant to both Essential Revisions 1 and 2a, we will first respond to each point in 1 and 2a, and then post the edits in one large block after the response to point 2a.</p><disp-quote content-type="editor-comment"><p>a) A sparse autoencoder. A stacked autoencoder is tested, but it's not said whether this model was regularized with an activity sparsity penalty on the bottleneck or not. If not, this should be done.</p></disp-quote><p>We have now implemented a stacked autoencoder with activity sparsity. To facilitate comparison, we have given this the same structure as our temporal prediction model, except that we changed the objective function of each stack to that of an autoencoder with sparse activity.</p><disp-quote content-type="editor-comment"><p>b) A model based on temporal continuity / slowness rather than predictability. It's likely that pretty much the same spatial tuning properties would emerge, but that subtle differences in the temporal dynamics would be detectable. This would be particularly interesting in the light of a recent paper by Weghenkel and Wiskott [1] (2018, Neural Computation), which has shown no advantage of predictability over slowness on a large set of real world data sets.</p></disp-quote><p>We have implemented a version of this principle as a stacked network, whose activity is constrained to vary slowly over time and to be decorrelated over units. To facilitate comparison, we have given this the same structure as our temporal prediction model, except that we changed the objective function of each stack to that of finding slow features.</p><p>As an aside, there are various differences between the predictability models used in Weghenhal and Wiskott (2018) and our temporal prediction model. Notably, the models in Weghenkel and Wiskott find those features whose activation is predictable over time (predictable features), whereas temporal prediction finds those features that are predictive of future input (predictive features). While related, these are not the same. For our edits to the text on this point, please see the Discussion.</p><disp-quote content-type="editor-comment"><p>c) A Gabor-wavelet-based model of V1. A good example of that is the Berkeley Wavelet Transform (BTW) in Willmore et al. [2]. Obviously this might not have the temporal processing facets of the authors' model, but the main comparisons made to data in Figure 6 don't require this. A very basic question is: how much better (or worse?) is the authors' model as a model of V1, as compared to the hard-coded (non-learned) &quot;standard&quot; model of V1 that the BWT represents?</p></disp-quote><p>We adapted the Berkeley Wavelet Transform for this comparison.</p><disp-quote content-type="editor-comment"><p>d) The PredNet [3] and PredRNN [4] and PredRNN++ [5] models. These are predictive coding models similar to the one the authors propose. The authors of course do cite [3], but do not compare to its results. They definitely should. Merely noting that Prednet &quot;has not been demonstrated [to] capture the phenomena that we describe …. &quot; does not mean that PredNet *wouldn't* capture these phenomena. Being fairly familiar with PredNet , we imagine that it could indeed capture these features. Showing that it does not, and that the current proposal is thus a better fit to the data, is a burden that is clearly on the authors in the case. The code for prednet is publicly released and could easily be downloaded and run by the authors (trained on their training set). We also suggest strongly that the authors look at PredRNN as well, which may be substantially better than PredNet.</p></disp-quote><p>We have adapted PredNet for this comparison. One distinct advantage that PredNet (and PredRNN) have over all the other models (including the temporal prediction model) is that they are recurrent, an important property shared with the brain. Arguably, this is therefore not a fair comparison. We are currently developing a recurrent version of our hierarchical temporal prediction model, and we certainly plan to make quantitative comparisons with other recurrent models. For now, we have included PredNet as a model for comparison with our existing model, but not PredRNN or PredRNN++, as we feel that a future publication would be more appropriate for a fair comparison between recurrent models.</p><disp-quote content-type="editor-comment"><p>e) The early layers of a supervised deep neural network. This is an important baseline -- how much better (if at all) are these authors' models at matching V1 data than the supervised network, which is (as the authors correctly point out) obviously trained in a deeply unbiologial way? Work such as that of Cadena et al. [6] clearly shows that the supervised network actually gives very reasonable results in V1 -- in fact, it's the state-of-the-art model of V1 in the literature, at least to our knowledge. How much better is their unsupervised network than this model at matching V1? If it's not better, then how big is the gap, and why would it exist? (If a supposedly much more biologically-correct model isn't substantially better than the unbiological one at matching biology data, has it really contributed a major advance?)</p></disp-quote><p>We have adapted the visual geometry group (VGG) model for this comparison. Please note, however, that this is a supervised model, which was fitted to human behavioral data (labeled images), rather than a normative model in the strict sense (where the model is trained according to a statistical principle, without fitting to behavioral or neural data). Having these behavioral data provides quantitative measurements of the outputs of neural systems that can be used to constrain the model, which is not available to the other models that are being compared here, such as temporal prediction.</p><disp-quote content-type="editor-comment"><p>One thing that will naturally come up in making a comparison like this is that the authors have chosen a particular imageset (moving animals) for training. Perhaps it will arise that their model is not trained in a sufficiently general way to compare favorably with a model trained on a large dataset like ImageNet. (Of course, this is still a fair comparison since neither their model nor the deepnet model would have been trained on the set used for testing, e.g. oriented gratings, or some other set of natural or naturalistic stimuli.) To claim that a major advance in modeling V1 has been made, something along these lines needs to be included. Perhaps the authors could use large video datasets like Kinetics [7] or Moments in Time [8] or the Chicago Motion Database (https://cmd.rcc.uchicago.edu) as a replacement for their animals dataset.</p></disp-quote><p>Our dataset consisted of videos of different animals in motion in diverse natural environments, which is a relatively rich dataset and close to the kind of visual stimuli that would often occur in the wild. However, we agree with the reviewers that the diversity of the dataset could be increased still further, most notably with other kinds of moving forms and with forward motion. To this end, for our new figure (Figure 9), we have now trained the temporal prediction model on a new dataset of even more diverse stimuli. We recorded this new dataset ourselves and it includes not just animals in motion, but other forms of motion such as rolling balls or trees moving in the wind, as well as diverse camera dynamics, such as forward motion for visual flow, panning, and stillness.</p><disp-quote content-type="editor-comment"><p>2) The comparisons to neural data are weak and qualitative. Improving that w.r.t. V1 *or* MT would be a major and sufficient revision for publication. The network learns mostly center surround, simple and complex cell behavior, which is conventionally assigned to retina/LGN/V1, but other features of V1 and higher-order visual neurons are not observed. No end- or side-inhibition has been observed, and no object-sensitive units. Only a very small fraction of units sensitive to drifting plaids were found in the 4th stack/layer. Given that, billing the model as a candidate for explaining the whole visual system seems much overstated. The authors could capitalize more on the dynamics of the receptive fields in V1 or MT, which was an interesting result not so often obtained by other models, but that has been investigated much less and was not compared to experimental data (apart from motion sensitivity). In whichever way possible, more detailed comparisons to V1 or MT data are needed. Our suggestions are:</p><p>a-i) For V1, the comparisons in Figure 6 are fine, and represent a good first step at comparing models to the data in a gross way. However, it would be great if a slightly more quantitative approach was taken -- e.g. measuring model-similarity in some quantified way, especially to compare between the author's preferred model and the controls suggested in 1) above.</p><p>a-ii) Also for V1: A much stronger comparison would be to do something as in Cadena et al. [6]. Specifically, Cadena et al. build a neuron-by-neuron regression model from their model to real V1 neurons, on a large set of real-world and naturalistic images. That work shows that, on this type of high-resolution comparison, there is something substantially better as a model of V1 than the standard hand-coded gabor BWT -- namely, the early intermediate layer of a categorization-trained deepnet. The state of the field has now moved to a point where models are being separated not by coarse measures like what is shown in Figure 6, but rather these much more detailed, real-world-stimulus-based metrics. We think the authors need to address comparisons at this level of resolution, or else it's really hard to know whether their model has made any substantive advance. It's not clear whether the data from Cadena et al. is readily available (though we suspect that Andreas Tolias, the data generator for that paper, would provide it for this purpose if asked). However, there is (or at least used to be) publicly available data from the Neural Prediction Challenge -- definitely worth getting this or similar data from Jack Gallant at Berkeley. (Or any other source that would allow for a much more direct model-to-neuron prediction assessment across naturalistic stimuli.)</p></disp-quote><p>We agree and have opted to focus on the stronger comparison (as set out in reviewer comment a-ii). We have added the following sections to the manuscript, which address the points made above in response to points 1 and 2a.</p><p>We have added the following new section and Figure to the Results (pages 22-24):</p><p>“Predicting neural responses to natural stimuli</p><p>A standard method to assess a model’s capacity to explain neural responses is to measure how well it can predict neural responses to natural stimuli. We did this for our model for two datasets. The first comprised single-unit neural responses recorded from awake macaque V1 to images of natural scenes (Cadena et al., 2019). The second, particularly relevant to our model, consisted of multiunit responses from anesthetized macaque V1 to movies of natural scenes (Nahaus and Ringach, 2007; Ringach and Nahaus, 2009). Estimating neural responses to such dynamic natural stimuli has received less attention in models of visual processing.</p><p>[…]</p><p>On predicting the neural responses to images (Figure 9A), relative to the temporal prediction model, the BWT and autoencoder models performed slightly worse (bootstrap, n = 166, p &lt; 10<sup>-4</sup> and p &lt; 10<sup>-4</sup>, respectively, see Methods) and the VGG, PredNet and slowness models performed slightly better (bootstrap, n = 166, p &lt; 10<sup>-4</sup>, p &lt; 10<sup>-4</sup> and p = 5.3×10<sup>-3</sup>, respectively). However, for the dynamic visual stimuli, the results were quite different (Figure 9B). The BWT, PredNet, autoencoder, and slowness models all performed worse than the temporal prediction model (bootstrap, n = 23, p = 1.6×10<sup>-2</sup>, p = 2×10<sup>-4</sup>, p = 2.5×10<sup>-3</sup> and p &lt; 10<sup>-4</sup>, respectively), with only the VGG model being not significantly different (bootstrap, n = 23, n.s.). Hence, the temporal prediction model, an unsupervised model trained on unlabeled data with no explicit constraints imposed by either neural or behavioral data, rivals a supervised model (VGG) trained using a labeled behavioral dataset in predicting neural responses in V1 to dynamic stimuli. Thus, the temporal prediction model is a reasonable contender as an unsupervised model of V1 responses to natural stimuli, particularly dynamic natural stimuli.”</p><p>We have also added the following section to the Discussion (pages to 33-34):</p><p>“Quantitative comparison to different models</p><p>Because the visual system has evolved to process natural stimuli, an important consideration in whether a computational model captures visual processing is whether it can predict neural responses to such stimuli. To this end, we estimated how well the hierarchical temporal prediction model can predict responses in awake macaque V1 to movies and images of natural scenes. Of particular interest are responses to movies, as our model is focused on the neural processing of dynamic stimuli. We compared the temporal prediction model to other models of visual system organization. Two of these models have identical structure to the temporal prediction model, but implement different underlying principles; one model estimates the current rather than future input alongside a sparse activity constraint (sparse coding principle) and the other finds slowly-varying features (slowness principle). We also made comparisons with three other key models of visual processing, the PredNet model, which is based on the principle of predictive coding, a wavelet model (BWT), inspired by the classic Gabor model, and a deep supervised model trained on image recognition (VGG).</p><p>[…]</p><p>We found that the performance of the hierarchical temporal prediction model at predicting the responses of V1 neurons to images was not significantly different from leading models such as the VGG model. Importantly, for the V1 responses to movies, the temporal prediction model outperformed all the other models, except for the VGG model, which it performed as well as. However, the VGG model has the advantage of labeled data, and was also trained on far more data than our model, whereas the temporal prediction model is unsupervised. Our findings therefore indicate that temporal prediction remains a leading contender as an underlying normative unsupervised principle governing the neural processing of natural visual scenes.”</p><p>Furthermore, we have added the following section to the Methods (pages 58-67):</p><p>“Quantitative model comparison for neural response prediction</p><p>We compared how well the temporal prediction model and five other models could predict neural responses. The models that we compared to the temporal prediction model were the following: the visual geometry group (VGG) model – a spatial deep convolutional model trained in a supervised manner on labeled data for image recognition, which has been shown to predict well neural responses in V1 (Cadena et al., 2019); the Berkeley wavelet transform model – a descriptive spatial model of V1, and three unsupervised spatiotemporal models – a leading predictive coding model (PredNet), a slowness model and an autoencoder model with sparse activity. The models were compared for their capacity to predict neural responses to images and movies of natural scenes.</p><p>[…]</p><p>Statistical tests</p><p>Statistical comparison of the different models was performed using a bootstrapping method (Zhuang et al., 2021). To calculate the probability <italic>p</italic> of model A performing better/worse statistically than model B, we sampled prediction scores over the neurons (paired, with replacement) for each model and calculated their respective means. We did this 10,000 times, counting the number of times model A had a larger mean than model B. Finally, this count was divided by 10,000 to obtain the probability <italic>p</italic> of model A having a larger mean than model B, with 1-p being the probability of model B having a larger mean than model A.”</p><disp-quote content-type="editor-comment"><p>b) For MT: the comparison here is quite thin. What the authors have done seems to barely support the claim that their hierarchical model &quot;can capture how tuning properties change across multiple levels of the visual system&quot;. More needs to be done here. Several papers have shown such things, mostly (as the authors note) based on supervised models. E.g [9-11] show comparisons of various intermediate layers of a NN to V1, V4, PIT, AIT areas in the ventral visual pathway. An unsupervised model that did the equivalent of this would be a significant advance. To make a claim like what the authors are saying in this draft, there needs to be some equally strong data comparison, but with MT data. Shinji Nishimoto and Jack Gallant have collected data that would be useful for this comparison, but it's not clear whether it would be easy to get access to that or similar MT data.</p><p>Comparing to coding in the ventral stream might be an alternative if MT data are not available. V4 and IT data is easily available from Jim DiCarlo (e.g. the data for [9-10]). The authors could definitely check out how well their higher model layers regressed those data and see if they could sustain a claim about matching &quot;multiple levels of the visual system&quot;. (But perhaps it would be an unfair comparison? Do the authors think their model would have any power for describing the ventral pathway?)</p><p>If these more detailed comparisons cannot be made, the claims about matching &quot;multiple levels of the visual system&quot; must be removed or significantly modified.</p></disp-quote><p>We agree that the number of pattern-motion selective neurons in our model is small, but we still think it is interesting that they are observed. We are now developing a recurrent version of our hierarchical temporal prediction model for which early results are showing a much greater number of pattern-motion selective neurons. This represents a major extension of our existing model and is therefore more appropriate for a future publication. Given that the current paper is being considered as a “research advance” that builds on the findings of our original 2018 <italic>eLife</italic> paper, we have opted to focus more on the progression from retina/LGN to the granular layer of V1 to the superficial layers of V1, and weakened our previous statements about higher areas. Our paper is now more focused on how the model reproduces properties of the retina and granular and supragranular V1. We have made the following changes throughout the manuscript.</p><p>In the Abstract we changed “multiple” to the substantially more circumspect “at least two”:</p><p>“Here we show that hierarchical application of temporal prediction can capture how tuning properties change across at least two levels of the visual system.”</p><p>In the Introduction we changed “multiple” to “several” and “visual motion pathway” to “visual pathway” (line 74):</p><p>“The capacity of this model to explain spatiotemporal tuning properties at several levels of the visual pathway using iterated application of a single process, suggests that optimization for temporal prediction may be a fundamental principle of sensory neural processing.”</p><p>In the Results we added the following sentences with regard to Figure 7 (pages 16-17):</p><p>“A clear progression from non-orientation-selective, to simple-cell like, to complex-cell like is seen in the units as one progresses up the stacks of the model (Figure 7). This bears some resemblance to the progression from the retina and LGN, which has few orientation-selective neurons, at least in cats and monkeys (Barlow, 1953; Kuffler, 1953; Shapley and Perry, 1986), to the geniculorecipient granular layer of V1, which tend to show more simple cells, to the superficial layers of V1 where more complex cells have been found (Ringach 2002, Cloherty and Ibbotson 2015). This is also consistent with the substantial increase in the proportion of complex cells from V1 to extrastriate visual areas, such as V2 (Cloherty and Ibbotson 2015).”</p><p>We also substantially weakened claims in the Conclusions (page 34-35):</p><p>“Previous studies using principles related to temporal prediction have produced non-hierarchical models of retina or primary cortex alone and demonstrated retina-like RFs (Ocko et al., 2018), simple-cell like RFs (Chalk et al., 2018; Singer et al., 2018), or RFs resembling those found in primary auditory cortex (Singer et al., 2018). However, any general principle of neural representation should be extendable to a hierarchical form and not tailored to the region it is attempting to explain. Here we show that temporal prediction can indeed be made hierarchical and so reproduce the major motion-tuning properties that emerge along the visual pathway from retina and LGN to V1 granular layers to V1 superficial layers, and potentially some properties of the extrastriate dorsal cortical pathway. This model captures not only linear tuning features, such as those seen in center-surround retinal neurons and direction-selective simple cells, but also nonlinear features seen in complex cells, with some indication that it may also reproduce properties of the pattern-sensitive neurons that are found in area MT. The capacity of our hierarchical temporal prediction model to account for many tuning features at several levels of the visual system suggests that the same framework may well explain many more features of the brain than we have so far investigated. Iterated application of temporal prediction, as performed by our model, could be used to make predictions about the tuning properties of neurons in brain pathways that are much less well understood than those of the visual system. Our results suggest that, by learning behaviorally-useful features from dynamic unlabeled data, temporal prediction may represent a fundamental coding principle in the brain.”</p></body></sub-article></article>