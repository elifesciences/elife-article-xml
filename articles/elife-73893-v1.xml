<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">73893</article-id><article-id pub-id-type="doi">10.7554/eLife.73893</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>CompoundRay, an open-source tool for high-speed and high-fidelity rendering of compound eyes</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-253429"><name><surname>Millward</surname><given-names>Blayze</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9025-1484</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-255165"><name><surname>Maddock</surname><given-names>Steve</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3179-0263</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-166496"><name><surname>Mangan</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Computer Science</institution>, <institution>University of Sheffield</institution>, <addr-line><named-content content-type="city">Sheffield</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-15498"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewing editor</role><aff><institution>University of Cambridge</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>b.f.millward@sheffield.ac.uk</email> (BM);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>13</day><month>10</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e73893</elocation-id><history><date date-type="received"><day>14</day><month>09</month><year>2021</year></date><date date-type="accepted"><day>12</day><month>10</month><year>2022</year></date></history><permissions><copyright-statement>Â© 2022, Millward et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Millward et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-73893-v1.pdf"/><abstract><p>Revealing the functioning of compound eyes is of interest to biologists and engineers alike who wish to understand how visually complex behaviours (e.g. detection, tracking, navigation) arise in nature, and to abstract concepts to develop novel artificial sensory systems. A key investigative method is to replicate the sensory apparatus using artificial systems, allowing for investigation of the visual information that drives animal behaviour when exposed to environmental cues. To date, 'Compound Eye Models' (CEMs) have largely explored features such field of view and angular resolution, but the role of shape and overall structure have been largely overlooked due to modelling complexity. Modern real-time raytracing technologies are enabling the construction of a new generation of computationally fast, high-fidelity CEMs. This work introduces a new open-source CEM software (CompoundRay) that is capable of accurately rendering the visual perspective of bees (6,000 individual ommatidia arranged on two realistic eye surfaces) at over 3,000 frames per second. We show how the speed and accuracy facilitated by this software can be used to investigate pressing research questions (e.g. how low resolutions compound eyes can localise small objects) using modern methods (e.g. ML information exploration).</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/P006094/1</award-id><principal-award-recipient><name><surname>Millward</surname><given-names>Blayze</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/S030964/1</award-id><principal-award-recipient><name><surname>Mangan</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The manuscript is a computational study, with all modelling code and data accessible on GitHub at https://github.com/ManganLab/eye-rendererUse of the natural environment was kindly provided by Dr. JoeWoodgate, Queen Mary University of London and is subject to upcoming publication. As such, instead included in the CompoundRay repository is a stand-in natural 3D terrain model. As all models are used for demonstrative purpose, this stand-in model offers little difference to the natural model used, bar it's subjectively lower-quality aesthetics.</p></sec><supplementary-material><ext-link xlink:href="elife-73893-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>