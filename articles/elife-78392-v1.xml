<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">78392</article-id><article-id pub-id-type="doi">10.7554/eLife.78392</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-274454"><name><surname>Fox</surname><given-names>Kai J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-272295"><name><surname>Birman</surname><given-names>Daniel</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3748-6289</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-73545"><name><surname>Gardner</surname><given-names>Justin L</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution>, <institution>Stanford University</institution>, <addr-line><named-content content-type="city">Stanford</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution content-type="dept">Department of Biological Structure</institution>, <institution>University of Washington</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-4966"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing editor</role><aff><institution>University of California, San Diego</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>kaifox@stanford.edu</email> (KF);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>dbirman@uw.edu</email> (DB);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>15</day><month>05</month><year>2023</year></pub-date><volume>12</volume><elocation-id>e78392</elocation-id><history><date date-type="received"><day>05</day><month>03</month><year>2022</year></date><date date-type="accepted"><day>12</day><month>05</month><year>2023</year></date></history><permissions><copyright-statement>Â© 2023, Fox et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Fox et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-78392-v1.pdf"/><abstract><p>Attention allows us to focus sensory processing on behaviorally relevant aspects of the visual world. One potential mechanism of attention is a change in the gain of sensory responses. However, changing gain at early stages could have multiple downstream consequences for visual processing. Which, if any, of these effects can account for the benefits of attention for detection and discrimination? Using a model of primate visual cortex we document how a Gaussian-shaped gain modulation results in changes to spatial tuning properties. Forcing the model to use only these changes failed to produce any benefit in task performance. Instead, we found that gain alone was both necessary and sufficient to explain category detection and discrimination during attention. Our results show how gain can give rise to changes in receptive fields which are not necessary for enhancing task performance.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Washington Ressearch Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Birman</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001818</institution-id><institution>Research to Prevent Blindness</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gardner</surname><given-names>Justin L</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>Lions Club International</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gardner</surname><given-names>Justin L</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Hellman Fellows Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Gardner</surname><given-names>Justin L</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>T32EY07031</award-id><principal-award-recipient><name><surname>Birman</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Procedures were approved in advance by the Stanford Institutional Review Board on human participants research and all observers gave prior written informed consent before participating (Protocol IRB-32120).</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The images and composite grids used in this study as well as the code necessary to replicate our analyses are available in the Open Science Framework with the identifier 10.17605/OSF.IO/AGHQK.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Fox KJ</collab><collab>Birman D</collab><collab>Gardner JL</collab></person-group><year iso-8601-date="2020">2020</year><source>Gain, not changes in spatial receptive field properties, improves task performance in a neural network attention model</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/aghqk/">https://osf.io/aghqk/</ext-link><comment>OSF AGHQK</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-78392-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>