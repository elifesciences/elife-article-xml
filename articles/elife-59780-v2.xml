<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">59780</article-id><article-id pub-id-type="doi">10.7554/eLife.59780</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>On the objectivity, reliability, and validity of deep learning enabled bioimage analyses</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-126496"><name><surname>Segebarth</surname><given-names>Dennis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3806-9324</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-126497"><name><surname>Griebel</surname><given-names>Matthias</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1959-0242</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-126499"><name><surname>Stein</surname><given-names>Nikolai</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196053"><name><surname>von Collenberg</surname><given-names>Cora R</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196054"><name><surname>Martin</surname><given-names>Corinna</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-126502"><name><surname>Fiedler</surname><given-names>Dominik</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-126503"><name><surname>Comeras</surname><given-names>Lucas B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2445-3605</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196055"><name><surname>Sah</surname><given-names>Anupam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8298-6501</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196057"><name><surname>Schoeffler</surname><given-names>Victoria</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196058"><name><surname>Lüffe</surname><given-names>Teresa</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-126498"><name><surname>Dürr</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-126505"><name><surname>Gupta</surname><given-names>Rohini</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196059"><name><surname>Sasi</surname><given-names>Manju</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-196056"><name><surname>Lillesaar</surname><given-names>Christina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5166-2851</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-126507"><name><surname>Lange</surname><given-names>Maren D</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-109750"><name><surname>Tasan</surname><given-names>Ramon O</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-57990"><name><surname>Singewald</surname><given-names>Nicolas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0166-3370</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund13"/><xref ref-type="fn" rid="con17"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-117750"><name><surname>Pape</surname><given-names>Hans-Christian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6874-8224</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con18"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-126508"><name><surname>Flath</surname><given-names>Christoph M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1761-9833</contrib-id><email>christoph.flath@uni-wuerzburg.de</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con19"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-116471"><name><surname>Blum</surname><given-names>Robert</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5270-3854</contrib-id><email>Blum_R@ukw.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con20"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Institute of Clinical Neurobiology, University Hospital Würzburg</institution><addr-line><named-content content-type="city">Würzburg</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Department of Business and Economics, University of Würzburg</institution><addr-line><named-content content-type="city">Würzburg</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Institute of Physiology I, Westfälische Wilhlems-Universität</institution><addr-line><named-content content-type="city">Münster</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Department of Pharmacology, Medical University of Innsbruck</institution><addr-line><named-content content-type="city">Innsbruck</named-content></addr-line><country>Austria</country></aff><aff id="aff5"><label>5</label><institution>Department of Pharmacology and Toxicology, Institute of Pharmacy and Center for Molecular Biosciences Innsbruck, University of Innsbruck</institution><addr-line><named-content content-type="city">Innsbruck</named-content></addr-line><country>Austria</country></aff><aff id="aff6"><label>6</label><institution>Department of Child and Adolescent Psychiatry, Center of Mental Health, University Hospital Würzburg</institution><addr-line><named-content content-type="city">Würzburg</named-content></addr-line><country>Germany</country></aff><aff id="aff7"><label>7</label><institution>Comprehensive Anxiety Center</institution><addr-line><named-content content-type="city">Würzburg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Fraser</surname><given-names>Scott E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Southern California</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bronner</surname><given-names>Marianne E</given-names></name><role>Senior Editor</role><aff><institution>California Institute of Technology</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>19</day><month>10</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e59780</elocation-id><history><date date-type="received" iso-8601-date="2020-06-15"><day>15</day><month>06</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-10-16"><day>16</day><month>10</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Segebarth et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Segebarth et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-59780-v2.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="commentary" xlink:href="10.7554/eLife.64384"/><abstract><p>Bioimage analysis of fluorescent labels is widely used in the life sciences. Recent advances in deep learning (DL) allow automating time-consuming manual image analysis processes based on annotated training data. However, manual annotation of fluorescent features with a low signal-to-noise ratio is somewhat subjective. Training DL models on subjective annotations may be instable or yield biased models. In turn, these models may be unable to reliably detect biological effects. An analysis pipeline integrating data annotation, ground truth estimation, and model training can mitigate this risk. To evaluate this integrated process, we compared different DL-based analysis approaches. With data from two model organisms (mice, zebrafish) and five laboratories, we show that ground truth estimation from multiple human annotators helps to establish objectivity in fluorescent feature annotations. Furthermore, ensembles of multiple models trained on the estimated ground truth establish reliability and validity. Our research provides guidelines for reproducible DL-based bioimage analyses.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Research in biology generates many image datasets, mostly from microscopy. These images have to be analyzed, and much of this analysis relies on a human expert looking at the images and manually annotating features. Image datasets are often large, and human annotation can be subjective, so automating image analysis is highly desirable. This is where machine learning algorithms, such as deep learning, have proven to be useful. In order for deep learning algorithms to work first they have to be ‘trained’. Deep learning algorithms are trained by being given a training dataset that has been annotated by human experts. The algorithms extract the relevant features to look out for from this training dataset and can then look for these features in other image data.</p><p>However, it is also worth noting that because these models try to mimic the annotation behavior presented to them during training as well as possible, they can sometimes also mimic an expert’s subjectivity when annotating data. Segebarth, Griebel et al. asked whether this was the case, whether it had an impact on the outcome of the image data analysis, and whether it was possible to avoid this problem when using deep learning for imaging dataset analysis.</p><p>For this research, Segebarth, Griebel et al. used microscopy images of mouse brain sections, where a protein called cFOS had been labeled with a fluorescent tag. This protein typically controls the rate at which DNA information is copied into RNA, leading to the production of proteins. Its activity can be influenced experimentally by testing the behaviors of mice. Thus, this experimental manipulation can be used to evaluate the results of deep learning-based image analyses.</p><p>First, the fluorescent images were interpreted manually by a group of human experts. Then, their results were used to train a large variety of deep learning models. Models were trained either on the results of an individual expert or on the results pooled from all experts to come up with a consensus model, a deep learning model that learned from the personal annotation preferences of all experts. This made it possible to test whether training a model on multiple experts reduces the risk of subjectivity. As the training of deep learning models is random, Segebarth, Griebel et al. also tested whether combining the predictions from multiple models in a so-called model ensemble improves the consistency of the analyses. For evaluation, the annotations of the deep learning models were compared to those of the human experts, to ensure that the results were not influenced by the subjective behavior of one person. The results of all bioimage annotations were finally compared to the experimental results from analyzing the mice’s behaviors in order to check whether the models were able to find the behavioral effect on cFOS.</p><p>Segebarth, Griebel et al. concluded that combining the expert knowledge of multiple experts reduces the subjectivity of bioimage annotation by deep learning algorithms. Combining such consensus information in a group of deep learning models improves the quality of bioimage analysis, so that the results are reliable, transparent and less subjective.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>bioimage informatics</kwd><kwd>deep learning</kwd><kwd>reproducibility</kwd><kwd>objectivity</kwd><kwd>validity</kwd><kwd>fluorescence microscopy</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>ID 44541416 - TRR58 A10</award-id><principal-award-recipient><name><surname>Blum</surname><given-names>Robert</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>ID 44541416 - TRR58 A03</award-id><principal-award-recipient><name><surname>Pape</surname><given-names>Hans-Christian</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>ID 44541416 - TRR58 B08</award-id><principal-award-recipient><name><surname>Lange</surname><given-names>Maren D</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Graduate School of Life Sciences Wuerzburg</institution></institution-wrap></funding-source><award-id>fellowship</award-id><principal-award-recipient><name><surname>Gupta</surname><given-names>Rohini</given-names></name><name><surname>Sasi</surname><given-names>Manju</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002428</institution-id><institution>Austrian Science Fund</institution></institution-wrap></funding-source><award-id>P29952 &amp; P25851</award-id><principal-award-recipient><name><surname>Tasan</surname><given-names>Ramon O</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002428</institution-id><institution>Austrian Science Fund</institution></institution-wrap></funding-source><award-id>I 3875</award-id><principal-award-recipient><name><surname>Singewald</surname><given-names>Nicolas</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002428</institution-id><institution>Austrian Science Fund</institution></institution-wrap></funding-source><award-id>DKW-1206</award-id><principal-award-recipient><name><surname>Singewald</surname><given-names>Nicolas</given-names></name></principal-award-recipient></award-group><award-group id="fund13"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002428</institution-id><institution>Austrian Science Fund</institution></institution-wrap></funding-source><award-id>SFB F4410</award-id><principal-award-recipient><name><surname>Singewald</surname><given-names>Nicolas</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Interdisziplinaeres Zentrum fuer Klinische Zusammenarbeit Wuerzburg</institution></institution-wrap></funding-source><award-id>N-320</award-id><principal-award-recipient><name><surname>Lillesaar</surname><given-names>Christina</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>ID 424778381 A02</award-id><principal-award-recipient><name><surname>Blum</surname><given-names>Robert</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A comparison of different bioimage analysis pipelines reveals how deep learning can be used for automatized and reliable analysis of fluorescent features in biological datasets.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Modern microscopy methods enable researchers to capture images that describe cellular and molecular features in biological samples at an unprecedented scale. One of the most frequently used imaging methods is fluorescent labeling of biological macromolecules, both in vitro and in vivo. In order to test a biological hypothesis, fluorescent features have to be interpreted and analyzed quantitatively, a process known as bioimage analysis (<xref ref-type="bibr" rid="bib48">Meijering et al., 2016</xref>). However, fluorescence does not provide clear signal-to-noise borders, forcing human experts to utilize individual heuristic criteria, such as morphology, size, or signal intensity to classify fluorescent signals as background, or to, often manually, annotate them as a region of interest (ROI). This cognitive decision process depends on the graphical perception capabilities of the individual annotator (<xref ref-type="bibr" rid="bib10">Cleveland and McGill, 1985</xref>). Constant technological advances in fluorescence microscopy facilitate the automatized acquisition of large image datasets, even at high resolution and with high throughput (<xref ref-type="bibr" rid="bib43">Li et al., 2010</xref>; <xref ref-type="bibr" rid="bib46">McDole et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Osten and Margrie, 2013</xref>). The ever increasing workload associated with image feature annotation therefore calls for computer-aided automated bioimage analysis. However, attempts to replace human experts and to automate the annotation process using traditional image thresholding techniques (e.g. histogram shape-, entropy-, or clustering-based methods [<xref ref-type="bibr" rid="bib64">Sezgin and Sankur, 2004</xref>]) frequently lack flexibility, as they rely on a high signal-to-noise ratio in the images or require computational expertise for user-based adaptation to individual datasets (<xref ref-type="bibr" rid="bib73">von Chamier et al., 2019</xref>). In recent years, deep learning (DL) and in particular deep convolutional neural networks have shown remarkable capacities in image recognition tasks, opening new possibilities to perform automatized image analysis. DL-based approaches have emerged as an alternative to conventional feature annotation or segmentation methods (<xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>) and are even capable of performing complex tasks such as artificial labeling of plain bright-field images (<xref ref-type="bibr" rid="bib73">von Chamier et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Christiansen et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Ounkomol et al., 2018</xref>). The main difference between conventional and DL algorithms is that conventional algorithms follow predefined rules (hard-coded), while DL algorithms are flexible to learn the respective task on base of a training dataset (<xref ref-type="bibr" rid="bib41">LeCun et al., 2015</xref>). Yet, deployment of DL approaches necessitates both computational expertise and suitable computing resources. These requirements frequently prevent non-AI experts from applying DL to routine image analysis tasks. Initial efforts have already been made to break down these barriers, for instance, by integration into prevalent bioimaging tools such as <italic>ImageJ</italic> (<xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>) and <italic>CellProfiler</italic> (<xref ref-type="bibr" rid="bib47">McQuin et al., 2018</xref>), or using cloud-based approaches (<xref ref-type="bibr" rid="bib26">Haberl et al., 2018</xref>). To harness the potentials of these DL-based methods, they require integration into the bioimage analysis pipeline. We argue that such an integration into the scientific process ultimately necessitates DL-based approaches to meet the same standards as any method in an empirical study. We can derive these standards from the general quality criteria of qualitative and quantitative research: objectivity, reliability, and validity (<xref ref-type="bibr" rid="bib20">Frambach et al., 2013</xref>).</p><p><italic>Objectivity</italic> refers to the neutrality of evidence, with the aim to reduce personal preferences, emotions, or simply limitations introduced by the context in which manual feature annotation is performed (<xref ref-type="bibr" rid="bib20">Frambach et al., 2013</xref>). Manual annotation of fluorescent features has long been known to be subjective, especially in the case of weak signal-to-noise thresholds (<xref ref-type="bibr" rid="bib61">Schmitz et al., 1999</xref>; <xref ref-type="bibr" rid="bib11">Collier et al., 2003</xref>; <xref ref-type="bibr" rid="bib51">Niedworok et al., 2016</xref>). Notably, there is no objective ground truth reference in the particular case of fluorescent label segmentation, causing a critical problem for training and evaluation of DL algorithms. As multiple studies have pointed out that annotations of low quality can cause DL algorithms to either fail to train or to reproduce inconsistent annotations on new data (<xref ref-type="bibr" rid="bib73">von Chamier et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>), this is a crucial obstacle for applying DL to bioimage analysis processes.</p><p><italic>Reliability</italic> is concerned with the consistency of evidence (<xref ref-type="bibr" rid="bib20">Frambach et al., 2013</xref>). To allow an unambiguous understanding of this concept, we further distinguish between repeatability and reproducibility. Repeatability or test-retest reliability is defined as 'closeness of the agreement between the results of successive measurements of the same measure and carried out under the same conditions' (<xref ref-type="bibr" rid="bib69">Taylor and Kuyatt, 1994</xref>, 14), which is guaranteed for any deterministic DL model. Reproducibility, on the other hand, is specified as 'closeness of the agreement between the results of measurements of the same measure and carried out under changed conditions' (<xref ref-type="bibr" rid="bib69">Taylor and Kuyatt, 1994</xref>, 14), for example, different observer, or different apparatus. This is a critical point, since the output of different DL models trained on the same training dataset can vary significantly. This is caused by the stochastic training procedure (e.g. random initialization, random sampling and data augmentation [<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>]), the choice of model parameters (e.g. model architecture, weights, activation functions), and the choice of hyperparameters (e.g. learning rate, mini-batch size, training epochs). Consequently, the reproducibility of DL models merits careful investigation.</p><p>Finally, <italic>validity</italic> relates to the truth value of evidence, that is, whether we in fact measured what we intended to. Moreover, validity implies reliability - but not vice versa (<xref ref-type="bibr" rid="bib20">Frambach et al., 2013</xref>). On a basis of a given ground truth, validity is typically measured using appropriate similarity measures such as F1 score for detection and Intersection over Union (IoU) for segmentation purposes (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>). In addition, the DL community has established widely accepted standards for training models. These comprise, among other things, techniques to avoid overfitting (regularization techniques and cross-validation), tuning hyperparameters, and selecting appropriate metrics for model evaluation. However, these standards do not apply for the training and evaluation of a DL model in the absence of a ground truth, like in the case of fluorescent features.</p><p>Taken together and with regard to the discussion about a reproducibility crisis in the fields of biology, medicine and artificial intelligence (<xref ref-type="bibr" rid="bib66">Siebert et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Baker, 2016</xref>; <xref ref-type="bibr" rid="bib31">Ioannidis, 2016</xref>; <xref ref-type="bibr" rid="bib30">Hutson, 2018</xref>; <xref ref-type="bibr" rid="bib15">Fanelli, 2018</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2019</xref>), these limitations indicate that DL could aggravate this crisis by adding even more unknowns and uncertainties to bioimage analyses.</p><p>However, the present study asks whether DL, if instantiated in an appropriate manner, also holds the potential to instead enhance the objectivity, reproducibility and validity of bioimage analysis. To tackle this conundrum, we investigated different DL-based strategies on five fluorescence image datasets. We show that training of DL models on the pooled input of multiple human experts utilizing ground truth estimation (consensus models) increases objectivity of fluorescent feature segmentation. Furthermore, we demonstrate that ensembles of consensus models are even capable of enhancing the reliability and validity of bioimage analysis of ambiguous image data, such as fluorescence features in histological tissue sections.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In order to evaluate the impact of DL on bioimage analysis results, we instantiated three exemplary DL-based strategies (<xref ref-type="fig" rid="fig1">Figure 1</xref>; strategies color-coded in gray, blue, and orange) and investigate them in terms of objectivity, reliability, and validity of fluorescent feature annotation. The first strategy, <italic>expert models</italic> (gray), reflects mere automation of the annotation process of fluorescent features in microscopy images. Here, manual annotations of a single human expert are used to train an individual (and hence expert-specific) DL model with a U-Net (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>) architecture. U-Net and its variants have emerged as the de facto standard for biomedical image segmentation purposes (<xref ref-type="bibr" rid="bib47">McQuin et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>). The second strategy, consensus <italic>models</italic> (blue), addresses the objectivity of signal annotations. Contrary to the first strategy, simultaneous truth and performance level estimation (STAPLE) (<xref ref-type="bibr" rid="bib74">Warfield et al., 2004</xref>) is used to estimate a ground truth and create consensus annotations. The estimated ground truth (est. GT) annotation reflects the pooled input of multiple human experts and is therefore thought to be less affected by a potential subjective bias of a single expert. We then train a single U-Net model to create a consensus model. The third strategy, consensus <italic>ensembles</italic> (orange), seeks to ensure reliability and eventually validity. Going beyond the second strategy, we train multiple consensus U-Net models to create a consensus ensemble. Such model ensembles are known to be more robust to noise (<xref ref-type="bibr" rid="bib12">Dietterich, 2000</xref>). Hence, we hypothesize that the consensus ensembles mitigate the randomness in the training process. Moreover, deep ensembles are supposed to yield high-quality predictive uncertainty estimates (<xref ref-type="bibr" rid="bib38">Lakshminarayanan et al., 2017</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic illustration of bioimage analysis strategies and corresponding hypotheses.</title><p>Four bioimage analysis strategies are depicted. Manual (white) refers to manual, heuristic fluorescent feature annotation by a human expert. The three DL-based strategies for automatized fluorescent feature annotation are based on expert models (gray), consensus models (blue) and consensus ensembles (orange). For all DL-based strategies, a representative subset of microscopy images is annotated by human experts. Here, we depict labels of cFOS-positive nuclei and the corresponding annotations (pink). These annotations are used in either individual training datasets (gray: expert models) or pooled in a single training dataset by means of ground truth estimation from the expert annotations (blue: consensus models, orange: consensus ensembles). Next, deep learning models are trained on the training dataset and evaluated on a holdout validation dataset. Subsequently, the predictions of individual models (gray and blue) or model ensembles (orange) are used to compute binary segmentation masks for the entire bioimage dataset. Based on these fluorescent feature segmentations, quantification and statistical analyses are performed. The expert model strategy enables the automation of a manual analysis. To mitigate the bias from subjective feature annotations in the expert model strategy, we introduce the consensus model strategy. Finally, the consensus ensembles alleviate the random effects in the training procedure and seek to ensure reliability and eventually, validity.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>U-Net architecture.</title><p>The U-Net architecture was adapted from <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>. The input and output shapes denote (minibatch size x height x width x channels), where the mini-batch size is not specified. If applicable, the names of the layers describe the containing operations (e.g.: <italic>Conv2D_BN_leakyReLU</italic> represents a 2D convolution followed by a batch normalization layer and a leakyReLu activation function). All convolutional layers were instantiated with a kernel size of 3 × 3, a stride of one and, no padding, except for the last convolution (1 × 1 kernel). The leaky ReLU has a leakage factor of 0.1 and the max-pooling operation a stride of two. The up-convolution (Conv2DTranspose) has a kernel size of 2 × 2 and strides of two.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Illustration of bioimage dataset Lab-Wue1.</title><p>(<bold>A</bold>) A total of 319 images showing cFOS immunoreactivity in the dorsal hippocampus of mice was split up in 105 images of the Dentate gyrus, 106 images of CA3 and 108 images of CA1. To create a balanced training dataset, four images of each experimental condition were randomly selected (H, C-, C+) from each hippocampal subregion (DG, CA3, CA1; 4 × 3 × 3 = 36 images). (<bold>B</bold>) Five expert neuroscientists (experts 1–5) manually annotated cFOS-positive nuclei in the selected 36 images of the training dataset and in nine additional images (test dataset). The test images represented one image per region and condition (3 × 3). Annotation was performed independently and on different computers and screens. The training dataset was used to train either expert specific models (only annotations of a single expert were used) or consensus models (est. GT annotations computed from the annotations of all five experts were used). Using <italic>k-fold cross-validation</italic> during the training, we were able to test the model performance and to ultimately select only those models that reached human level performance. The final evaluation of all models was then performed on the additional nine images of the test dataset. For bioimage analyses, we used the remaining 274 images and the nine test images. (<bold>C</bold>) On average, each consensus ensemble annotated ∼10,000 cFOS-positive feature within the NeuN-positive areas in all 283 images used for bioimage analysis, which is equivalent to ∼35 features per image.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig1-figsupp2-v2.tif"/></fig></fig-group><p>For each of the three strategies, we complete the bioimage analysis by performing quantification and hypothesis testing on a typical fluorescent microscopy image dataset (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). These images describe changes in fluorescence signal abundance of a protein called cFOS in brain sections of mice. cFOS is an activity-dependent transcription factor and its expression in the brain can be modified experimentally by behavioral testing of the animals (<xref ref-type="bibr" rid="bib22">Gallo et al., 2018</xref>). The low signal-to-noise ratio of this label, its broad usage in neurobiology and the well-established correlation of its abundance with behavioral paradigms render it an ideal bioimage dataset to test our hypotheses (<xref ref-type="bibr" rid="bib65">Shuvaev et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Gallo et al., 2018</xref>).</p><sec id="s2-1"><title>Consensus ensembles yield the best results for validity and reproducibility metrics</title><p>The primary goal in bioimage analysis is to rigorously test a biological hypothesis. To leverage the potentials of DL models within this procedure, we need to trust our model – by establishing objectivity, reliability, and validity. Pertaining to the case of fluorescent labels, validity (measuring what is intended to be measured) requires objectivity to know what exactly we intend to measure in the absence of a ground truth. Similarly, reliability in terms of repeatability and reproducibility is a prerequisite for a valid and trustworthy model. Starting from the expert model strategy, we seek to establish objectivity (consensus models) and, successively, reliability and validity in the consensus ensemble strategy. In the following analysis, we first turn toward a comprehensive evaluation of the objectivity and its relation to validity before moving on to the concept of reliability.</p><p>To assess the three different strategies, a training dataset of 36 images and a test set of nine microscopy images (1024 × 1024 px, 1.61 px/µm, on average ∼35 nuclei per image, see also <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) showing cFOS immunoreactivity were manually annotated by five independent experts (experts 1–5). In absence of a rigorously objective ground truth, we used STAPLE (<xref ref-type="bibr" rid="bib74">Warfield et al., 2004</xref>) to compute an estimated ground truth (est. GT) based on all expert annotations for each image. First, we trained a set of DL models on the 36 training images and corresponding annotations, either made by an individual human expert or as reflected in the est. GT (see Materials and methods for the data set and detailed training, evaluation and model selection strategy). Then, we used our test set to evaluate the segmentation (Mean IoU) and detection (F1 score) performance of human experts and all trained models by means of similarity analysis on the level of individual images.</p><p>For the pairwise comparison of annotations (segmentation masks), we calculated the intersection over union (IoU) for all overlapping pairs of ROIs between two segmentation masks (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; see 7.9.1 Segmentation and detection). Following <xref ref-type="bibr" rid="bib45">Maška et al., 2014</xref>, we consider two ROIs with an IoU of at least 0.5 as matching and calculated the F1 score <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> as the harmonic mean of precision and recall (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; see 7.9.1 Segmentation and detection). As bioimaging studies predominantly use measures related to counting ROIs in their analyses, we also focused on the feature detection performance (<inline-formula><mml:math id="inf2"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula>). The color coding (gray, blue, orange) introduced in <xref ref-type="fig" rid="fig2">Figure 2C</xref> refers to the different strategies depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref> and applies to all figures, if not indicated otherwise.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Similarity analysis of fluorescent feature annotations by manual or DL-based strategies.</title><p>(<bold>A</bold>) Representative example of IoU <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>M</mml:mi><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula> calculations on a field of view (FOV) in a bioimage. Image raw data show the labeling of cFOS in a maximum intensity projection image of the CA1 region in the hippocampus (brightness and contrast enhanced). The similarity of estimated ground truth (est. GT) annotations (green), derived from the annotations of five expert neuroscientists, are compared to those of one human expert, an expert model, a consensus model, and a consensus ensemble (magenta, respectively). IoU results of two ROIs are shown in detail for each comparison (magnification of cyan box). Scale bar: 100 µm. (<bold>B</bold>) F1 score <inline-formula><mml:math id="inf4"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> calculations on the same FOV as shown in (<bold>A</bold>). The est. GT annotations (green; 53 ROIs) are compared to those of a consensus ensemble (magenta; 48 ROIs). IoU-based matching of ROIs at an IoU-threshold of <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> is depicted in three magnified subregions of the image (cyan boxes 1-3). Scale bar: 100 µm. (<bold>C–H</bold>) All comparisons are performed exclusively on a separate image test set which was withheld from model training and validation. (<bold>C</bold>) Color coding refers to the individual strategies, as introduced in <xref ref-type="fig" rid="fig1">Figure 1</xref>: white: manual approach, gray: expert models, blue: consensus models, orange: consensus ensembles. (<bold>D</bold>) <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> between individual manual expert annotations and their overall reliability of agreement given as the mean of Fleiss‘ <inline-formula><mml:math id="inf7"><mml:mi>κ</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) <inline-formula><mml:math id="inf8"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> between annotations predicted by individual models and the annotations of the respective expert (or est. GT), whose annotations were used for training. N<sub>models per expert</sub> = 4. (<bold>F</bold>) <inline-formula><mml:math id="inf9"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> between manual expert annotations, the respective expert models, consensus models, and consensus ensembles compared to the est. GT as reference. A horizontal line denotes human expert average. N<sub>models</sub> = 4, N<sub>ensembles</sub> = 4. (<bold>G</bold>) Means of <inline-formula><mml:math id="inf10"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of the individual DL-based strategies and of the human expert average compared to the est. GT plotted for different IoU matching thresholds <italic>t</italic>. A dashed line indicates the default threshold <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. N<sub>models</sub> = 4, N<sub>ensembles</sub> = 4. (<bold>H</bold>) Annotation reliability of the individual strategies assessed as the similarities between annotations within the respective strategy. We calculated <inline-formula><mml:math id="inf12"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> and Fleiss‘ <inline-formula><mml:math id="inf14"><mml:mi>κ</mml:mi></mml:math></inline-formula>. N<sub>experts</sub> = 5, N<sub>models</sub> = 4, N<sub>ensembles</sub> = 4.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Extended subjectivity analysis.</title><p>The subjectivity analysis depicts the relationship between the relative intensity difference of a florescent feature (ROI) to the background and the annotation count of human experts. A visual interpretation indicates that the annotation probability of a ROI is positively correlated with its relative relative intensity. The relative intensity difference is calculated as <inline-formula><mml:math id="inf15"><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:math></inline-formula>, where <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean signal intensity of the ROI and <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the mean signal intensity of its nearby outer area. We considered matching ROIs at an IoU threshold of <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. The expert in the title of the respective plot was used to create the region proposals of the ROIs, that is, the annotations served as origin for the other pairwise comparisons. (<bold>A</bold>) Legend of color codes: blue depicts that a ROI was only annotated by one or more human experts; yellow depicts the ROIs that were present in the estimated ground truth; green shows the ROIs that are only present in an exemplary consensus ensemble; pink depicts ROIs that are present in both estimated ground truth and consensus ensemble (<bold>B–I</bold>) All calculations are performed on the test set (n = 9 images) which was withheld from model training and validation. (<bold>B–F</bold>) The individual expert analysis shows the effects of different heuristic evaluation criteria. (<bold>G</bold>) The analysis of the est. GT annotations reveals the limitations of the ground truth estimation algorithm, which is based on the human annotations. An expert count of zero can result from merging different ROIs. (<bold>H</bold>) The analysis of a representative consensus ensemble shows that human annotators may have missed several ROIs (green) even with a large relative difference to the background. (<bold>I</bold>) Cumulative summary of B-F.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Ensemble size and reliability.</title><p>To determine an appropriate size for the consensus <italic>ensembles</italic>, we analyzed the homogeneity of the results through a similarity analysis. Therefore, we calculated the <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> at an IoU matching threshold of <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> for each ensemble size <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the holdout test set (n = 9 images). Stratified on the cross validation splits, we randomly sampled the ensembles from a collection of trained consensus <italic>models</italic>. We repeated this procedure five times to mitigate the random effect of the ensemble composition (N<sub>ensembles</sub> = 5 for each <italic>i</italic>). The blue box (<inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) depicts the variability between different consensus <italic>models</italic>. The orange box (<inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) shows the variability of the finally chosen size for the consensus <italic>ensembles</italic>, as no substantial reduction in variation can be observed for larger <italic>i</italic>. In addition, <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> corresponds to the number of cross validation splits (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>), meaning that the ensembles have seen the entire training set. The black line denotes the standard deviation of <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula>, which is scaled at the right y-Axis. The dashed black line denotes the best fitting function of type <inline-formula><mml:math id="inf27"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>a</mml:mi><mml:msqrt><mml:mi>x</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0.096</mml:mn></mml:mrow></mml:math></inline-formula> for the standard deviation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Extended similarity analysis: F1 score.</title><p>The heatmap shows the mean of <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> at a matching IoU-threshold of <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> for the image feature annotations of the indicated experts. Segmentation masks of the five human experts (N<sub>expert</sub> = 1 per expert), the estimated ground-truth (Nest. GT = 1), the respective expert models, the consensus models, and the consensus ensembles (N<sub>models</sub> = 4 per model or ensemble) are compared. The diagonal values show the inter-model reliability (no data available for the human experts who only annotated the images once). The consensus ensembles show the highest reliability (0.94) and perform on par with human experts compared to the est. GT (0.77). Both expert 1 and the corresponding expert 1 models show overall low similarities to other experts and expert models, while sharing a high similarity to each other (0.73).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Extended similarity analysis: mean IoU.</title><p>The heatmap shows the mean of <inline-formula><mml:math id="inf31"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula> for the image feature annotations of the indicated experts. Segmentation masks of the five human experts (N<sub>expert</sub> = 1 per expert), the estimated ground-truth (Nest. GT = 1), the respective expert models, the consensus models, and the consensus ensembles (N<sub>models</sub> = 4 per model or ensemble) are compared. The diagonal values show the inter-model reliability (no data available for the human experts who only annotated the images once). Again, consensus ensembles show highest reliability (0.91). Est. GT annotations are directly derived from manual expert annotations, which renders this comparison favorable.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig2-figsupp4-v2.tif"/></fig></fig-group><p>To better grasp the difficulties in annotating cFOS-positive nuclei as fluorescent features in these images, we first compared manual expert annotations (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The analysis revealed substantial differences between the annotations of the different experts and shows varying inter-rater agreement (<xref ref-type="bibr" rid="bib61">Schmitz et al., 1999</xref>; <xref ref-type="bibr" rid="bib11">Collier et al., 2003</xref>; <xref ref-type="bibr" rid="bib51">Niedworok et al., 2016</xref>). The level of inter-rater variability was inversely correlated with the relative signal intensities (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; <xref ref-type="bibr" rid="bib51">Niedworok et al., 2016</xref>).</p><p>By comparing the annotations of the expert models (gray) to the annotations of the respective expert (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), we observed a higher <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> median compared to the inter-rater agreement (<xref ref-type="fig" rid="fig2">Figure 2D</xref>) in the majority of cases. Furthermore, comparing the similarity analysis results of human experts with those of their respective expert-specific models revealed that they are closely related (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, and <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). As pointed out by <xref ref-type="bibr" rid="bib73">von Chamier et al., 2019</xref>, this indicates that our expert models are able to learn and reproduce the annotation behavior of the individual experts. This becomes particularly evident in the annotations of the DL models trained on expert 1 (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>, and <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p><p>Overall, the expert models yield a lower similarity to the est. GT compared to the consensus models (blue) or consensus ensembles (orange). Notably, both consensus models and consensus ensembles perform on par with human experts. Hereby, the consensus ensembles outperform all other strategies, even at varying IoU thresholds (<xref ref-type="fig" rid="fig2">Figure 2F</xref> and <xref ref-type="fig" rid="fig2">Figure 2G</xref>).</p><p>In order to test for reliability of our analysis, we measured the repeatability and reproducibility of fluorescent feature annotation of our DL strategies. We assumed that the repeatability is assured for all our strategies due to the deterministic nature of our DL models (unchanged conditions imply unchanged model weights). Hence, our evaluation was focused on the reproducibility, meaning the impact of the stochastic training process on the output. Inter-expert and inter-model comparisons within each strategy unveiled a better performance of the consensus ensembles strategy concerning both detection (<inline-formula><mml:math id="inf33"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula>) and segmentation (<inline-formula><mml:math id="inf34"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula>) of the fluorescent features (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). Calculating the Fleiss’ kappa value (<xref ref-type="bibr" rid="bib19">Fleiss and Cohen, 1973</xref>) revealed that consensus ensemble annotations show a high reliability of agreement (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). Following the Fleiss’ kappa interpretation from <xref ref-type="bibr" rid="bib40">Landis and Koch, 1977</xref>, the results for the consensus ensembles indicate a substantial or almost perfect agreement. In contrast, the Fleiss’ kappa values for human experts refer to a fair agreement while the results for the alternative DL strategies lead to a moderate agreement (<xref ref-type="fig" rid="fig2">Figure 2H</xref>).</p><p>In summary, the similarity analysis of the three strategies shows that training of DL models solely on the input of a single human expert imposes a high risk of incorporating an intrinsic bias and therefore resembles, as hypothesized, a mere automation of manual image annotation. Both consensus models and consensus ensembles perform on par with human experts regarding the similarity to the est. GT, but the consensus ensembles yield by far the best results regarding their reproducibility. We conclude that, in terms of similarity metrics, only the consensus ensemble strategy meet the bioimaging standards for objectivity, reliability, and validity.</p></sec><sec id="s2-2"><title>Consensus ensembles yield reliable bioimage analysis results</title><p>Similarity analysis is inevitable to assess the quality of a model’s output, that is, the predicted segmentations (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>). However, the primary goal of bioimage analysis is the unbiased quantification of distinct image features that correlate with experimental conditions. So far, it has remained unclear whether objectivity, reliability, and validity for bioimage analysis can be inferred directly from similarity metrics.</p><p>In order to systematically address this question, we used our image dataset to quantify the abundance of cFOS in brain sections of mice after Pavlovian contextual fear conditioning. It is well established in the neuroscientific literature that rodents show changes in the distribution and abundance of cFOS in a specific brain region, namely the hippocampus, after processing information about places and contexts (<xref ref-type="bibr" rid="bib34">Keiser et al., 2017</xref>; <xref ref-type="bibr" rid="bib5">Campeau et al., 1997</xref>; <xref ref-type="bibr" rid="bib29">Huff et al., 2006</xref>; <xref ref-type="bibr" rid="bib57">Ramamoorthi et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Tayler et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Murawski et al., 2012</xref>; <xref ref-type="bibr" rid="bib25">Guzowski et al., 2001</xref>). Consequently, our experimental dataset offered us a second line of evidence, the objective analysis of mouse behavior, in addition to the changes of fluorescent features to validate the bioimage analyses results of our DL-based strategies.</p><p>Our dataset comprised three experimental groups (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). In one group, mice were directly taken from their homecage as naive learning controls (H). In the second group, mice were re-exposed to a previously explored training context as context controls (C-). Mice in the third group underwent Pavlovian fear conditioning and were also re-exposed to the training context (C+) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). These three groups of mice showed different behavioral responses. For instance, fear (threat; <xref ref-type="bibr" rid="bib42">LeDoux, 2014</xref>) conditioned mice (C+) showed increased freezing behavior after fear acquisition and showed strong freezing responses when re-exposed to the training context 24 hr later (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). After behavioral testing, brain sections of the different groups of mice were prepared and labeled for the neuronal activity-related protein cFOS by indirect immunofluorescence. Sections were also labeled with the neuronal marker NeuN (Fox3), allowing the anatomical identification of hippocampal subregions of interest. Images were acquired as confocal microscopy image stacks (x,y-z) and maximum intensity projections were used for subsequent bioimage analysis (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Overall, we quantified the number of cFOS-positive nuclei and their mean signal intensity in five regions of the dorsal hippocampus (DG as a whole, suprapyramidal DG, infrapyramidal DG, CA3, and CA1), and tested for significant differences between the three experimental groups (<xref ref-type="fig" rid="fig3">Figure 3B–D</xref>). To extend this analysis beyond hypothesis testing at a certain significance level, we calculated the effect size (<inline-formula><mml:math id="inf35"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) for each of these 30 pairwise comparisons.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Application of different DL-based strategies for fluorescent feature annotation.</title><p>The figure introduces how three DL-based strategies are applied for annotation of a representative fluorescent label, here cFOS, in a representative image data set. Raw image data show behavior-related changes in the abundance and distribution of the protein cFOS in the dorsal hippocampus, a brain center for encoding of context-dependent memory. (<bold>A</bold>) Three experimental groups were investigated: Mice kept in their homecage (H), mice that were trained to a context, but did not experience an electric foot shock (C-) and mice exposed to five foot shocks in the training context (C+). 24 hr after the initial training (TR), mice were re-exposed to the training context for memory retrieval (RET). Memory retrieval induces changes in cFOS levels. (<bold>B–D</bold>) Brightness and contrast enhanced maximum intensity projections showing cFOS fluorescent labels of the three experimental groups (H, C-, C+) with representative annotations of a consensus ensemble, for each hippocampal subregion. The annotations are used to quantify the number of cFOS-positive nuclei for each image (#) per mm<sup>2</sup> and their mean signal intensity (mean int., in bit-values) within the corresponding image region of interest, here the neuronal layers in the hippocampus (outlined in cyan). In B: granule cell layer (supra- and infrapyramidal blade), dotted line: suprapyramidal blade, solid line: infrapyramidal blade. In C: pyramidal cell layer of CA3; in D: pyramidal cell layer in CA1. Scale bars: 200 µm. (<bold>E</bold>) Analyses of cFOS-positive nuclei per mm<sup>2</sup>, representatively shown for stratum pyramidale of CA1. Corresponding effect sizes are given as <inline-formula><mml:math id="inf36"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for each pairwise comparison. Two quantification results are shown for each strategy and were selected to represent the lowest (model 1 or ensemble 1) and highest (model 2 or ensemble 2) effect sizes (increase in cFOS) reported within each annotation strategy. Total analyses performed: N<sub>expert models</sub> = 20, N<sub>consensus models</sub> = 36, N<sub>consensus ensembles</sub> = 9. Number of analyzed mice (N) and images (n) per experimental condition: N<sub>H</sub> = 7, N<sub>C-</sub> = 7, N<sub>C+</sub> = 6; n<sub>H</sub> = 36, n<sub>C-</sub> = 32, n<sub>C+</sub> = 28. ***p&lt;0.001 with Mann-Whitney-U test. Statistical data are available in <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Source files for analyses of cFOS-positive nuclei in CA1.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-59780-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Behavioral analysis <italic>Lab-Wue1</italic>.</title><p>(<bold>A</bold>) Fear acquisition was observed in conditioned mice (C+), while unconditioned controls (C-) did not show freezing behavior during initial context exposure (TR). In the memory retrieval session (RET), conditioned mice showed strong freezing behavior, while unconditioned mice did not freeze in response to the training context (<italic>X</italic><sup>2</sup>(3) = 20.894, p&lt;0.001, N<sub>TR C-</sub> = 7, N<sub>TR C+</sub> = 6, N<sub>RET C-</sub> = 7, N<sub>RET C+</sub> = 6, Kruskal-Wallis ANOVA followed by pairwise Mann-Whitney tests with Bonferroni correction, *p&lt;0.05). (<bold>B</bold>) Distance traveled in the training context is reduced in fear conditioned mice (F(3, 22) = 19.484, p&lt;0.001, N<sub>TR C-</sub> = 7, N<sub>TR C+</sub> = 6, N<sub>RET C-</sub> = 7, N<sub>RET C+</sub> = 6, one-way ANOVA followed by pairwise t-tests with Bonferroni correction, **p&lt;0.01, ***p&lt;0.001). Source data is available as <xref ref-type="supplementary-material" rid="fig3s1sdata1">Figure 3—figure supplement 1—source data 1</xref>.</p><p><supplementary-material id="fig3s1sdata1"><label>Figure 3—figure supplement 1—source data 1.</label><caption><title>Source files for behavioral analysis in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-59780-fig3-figsupp1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig3-figsupp1-v2.tif"/></fig></fig-group><p>We illustrate our metrics with the detailed quantification of cFOS-positive nuclei in the stratum pyramidale of CA1 as a representative example and show two analyses for each DL strategy (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). These two examples represent those two models of each strategy that yielded the lowest and the highest effect sizes, respectively (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). Despite a general consensus of all models and ensembles on a context-dependent increase in the number of cFOS-positive nuclei, these quantifications already indicate that the variability of effect sizes decreases from expert models to consensus models and is lowest for consensus ensembles (<xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p>The analysis in <xref ref-type="fig" rid="fig4">Figure 4</xref> allows us to further explore the impact of the different DL strategies on the bioimage analysis results for each hippocampal subregion. Here, we display a high-level comparison of the effect sizes and corresponding significance levels of 20 independently trained expert models (4 per expert), 36 consensus models, and 9 consensus ensembles (each derived from four consensus models). In contrast to the detailed illustration of selected models in <xref ref-type="fig" rid="fig3">Figure 3E</xref>, <xref ref-type="fig" rid="fig4">Figure 4A</xref>, for instance, summarizes the results for all analyses of the stratum pyramidale of CA1. As indicated before, all models and ensembles show a highly significant context-dependent increase in the number of cFOS-positive nuclei, but also a notable variation in effect sizes for both expert and consensus models. Moreover, we identify a significant context-dependent increase in the mean signal intensity of cFOS-positive nuclei for all consensus models and ensembles. The expert models, by contrast, yield a high variation in effect sizes at different significance levels. Interestingly, all four expert models trained on the annotations of expert 1 (and two other expert models only in the case of H vs. C+) did not yield a significant increase, indicating that expert 1’s annotation behavior was incorporated into the expert-1-specific models and that this also affects the bioimage analysis results (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Consensus ensembles significantly increase reliability of bioimage analysis results.</title><p>(<bold>A–E</bold>) Single data points represent the calculated effect sizes for each pairwise comparison of all individual bioimage analyses for each DL-based strategy (gray: expert models, blue: consensus models, orange: consensus ensembles) in indicated hippocampal subregions. Three horizontal lines separate four significance intervals (n.s.: not significant, *: 0.05 ≥ p&gt;0.01, **: 0.01 ≥ p&gt;0.001, ***: p ≤ 0.001 after Bonferroni correction for multiple comparisons). The quantity of analyses of each strategy that report the respective statistical result of the indicated pairwise comparison (effect, x-axis) at a level of p ≤ 0.05 are given below each pairwise comparison in the corresponding color coding. In total, we performed all analyses with: N<sub>expert models</sub> = 20, N<sub>consensus models</sub> = 36, N<sub>consensus ensembles</sub> = 9. Number of analyzed mice (N) for all analyzed subregions: N<sub>H</sub> = 7, N<sub>C-</sub> = 7, N<sub>C+</sub> = 6. Numbers of analyzed images (n) are given for each analyzed subregion. Source files including source data and statistical data are available in <xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>. (<bold>A</bold>) Analyses of cFOS-positive nuclei in stratum pyramidale of CA1. n<sub>H</sub> = 36, n<sub>C</sub>- = 32, n<sub>C+</sub> = 28. (<bold>B</bold>) Analyses of cFOS-positive nuclei in stratum pyramidale of CA3. n<sub>H</sub> = 35, n<sub>C</sub>- = 31, n<sub>C+</sub> = 28. (<bold>C</bold>) Analyses of cFOS-positive nuclei in the granule cell layer of the whole DG. n<sub>H</sub> = 35, n<sub>C-</sub> = 31, n<sub>C+</sub> = 27. (<bold>D</bold>) Analyses of cFOS-positive nuclei in the granule cell layer of the suprapyramidal blade of the DG. n<sub>H</sub> = 35, n<sub>C-</sub> = 31, n<sub>C+</sub> = 27. (<bold>E</bold>) Analyses of cFOS-positive nuclei in the granule cell layer of the infrapyramidal blade of the DG. n<sub>H</sub> = 35, n<sub>C-</sub> = 31, n<sub>C+</sub> = 27. (<bold>F</bold>) Reliability of bioimage analysis results are assessed as <italic>variation per effect</italic> (left side) and <italic>variation per model</italic> (right side). For the <italic>variation per effect</italic>, single data points represent the standard deviation of reported effect sizes (<inline-formula><mml:math id="inf37"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>), calculated within each DL-based strategy for each of the 30 pairwise comparisons. Consensus ensembles show significantly lower standard (std.) deviations of <inline-formula><mml:math id="inf38"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> per pairwise comparison compared to alternative strategies (<italic>X</italic><sup>2</sup>(2) = 26.472, p&lt;0.001, N<sub>effects</sub> = 30, Kruskal-Wallis ANOVA followed by pairwise Mann-Whitney tests with Bonferroni correction, *p&lt;0.05, ***p&lt;0.001). For the <italic>variation per model</italic>, the standard deviation of centered <inline-formula><mml:math id="inf39"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> across all pairwise comparisons was calculated for each individual model and ensemble (y-axis). In addition, the number of deviations from the congruent majority vote (at p ≤ 0.05 after Bonferroni correction for multiple comparisons) were determined for each individual model and ensemble across all pairwise comparisons (x-axis). Visualizing the interaction of both measures for each model or model ensemble individually reveals that consensus ensembles show the highest reliability of all three DL-based strategies. The statistical data for the for variation per effect is available in <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Source files for the analysis of cFOS positive nuclei in the hippocampal subregions.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-59780-fig4-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>Statistical data for the variation per effect.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-59780-fig4-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig4-v2.tif"/></fig><p>The meta analysis discloses a context-dependent increase of cFOS in almost all analyzed hippocampal regions (<xref ref-type="fig" rid="fig4">Figure 4A–D</xref>), except for the infrapyramidal blade of the dentate gyrus (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Notably, the majority votes of all three strategies at a significance level of p ≤ 0.05 (after Bonferroni correction for multiple comparisons) are identical for each pairwise comparison (<xref ref-type="fig" rid="fig4">Figure 4A–E</xref>). However, the results can vary between individual models or ensembles (<xref ref-type="fig" rid="fig4">Figure 4A–E</xref>).</p><p>In order to assess the reliability of bioimage analysis results of the individual strategies, we further examined the variation per effect and variation per model in <xref ref-type="fig" rid="fig4">Figure 4F</xref>. For the variation per effect, we calculated the standard deviation of reported effect sizes within each strategy for every pairwise comparison (effect). This confirmed the visual impression from <xref ref-type="fig" rid="fig4">Figure 4A–E</xref> as the consensus ensembles yield a significantly lower standard deviation compared to both alternative strategies (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). To illustrate the variation per model, we show the interaction between the number of biological effects that the corresponding model (or ensemble) reported differently compared to the congruent majority votes versus the standard deviation of its centered effect sizes across all 30 analyzed effects. This analysis shows that no expert model detected all biological effects in the microscopy images that were defined by the majority votes of all models. This is in stark contrast to the consistency of effect interpretation across the consensus ensembles (<xref ref-type="fig" rid="fig4">Figure 4F</xref>).</p><p>Consequently, we conclude that the consensus ensemble strategy is best suited to satisfy the bioimaging standards for objectivity, reliability, and validity.</p></sec><sec id="s2-3"><title>Applicability of consensus ensemble strategy for the bioimage analysis of external data sets</title><p>Bioimage analysis of fluorescent labels comes with a huge variability in terms of investigated model organisms, analyzed fluorescent features and applied image acquisition techniques (<xref ref-type="bibr" rid="bib48">Meijering et al., 2016</xref>). In order to assess our consensus ensemble strategy across these varying parameters, we tested it on four external datasets that were created in a fully independent manner and according to individual protocols (<italic>Lab-Mue, Lab-Inns1, Lab-Inns2</italic>, and <italic>Lab-Wue2</italic>; see Materials and methods and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). Due to limited dataset sizes, the lab-specific training datasets consisted of just five microscopy images each and the corresponding est. GT based on the annotations from multiple experts. In the biomedical research field, the limited availability of training data is a common problem when training DL algorithms. For this reason, extensive data augmentation and regularization techniques, as well as transfer learning strategies are widely used to cope with small datasets (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Christiansen et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>). Transfer learning is a technique that enables DL models to reuse the image feature representations learned on another source, such as a task (e.g. image segmentation) or a domain (e.g. the fluorescent feature, here cFOS-positive nuclei). This is particularly advantageous when applied to a task or domain where limited training data is available (<xref ref-type="bibr" rid="bib77">Yosinski et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Oquab et al., 2014</xref>). Moreover, transfer learning might be used to reduce observer variability and to increase feature annotation objectivity (<xref ref-type="bibr" rid="bib3">Bayramoglu and Heikkilä, 2016</xref>). There are typically two ways to implement transfer learning for DL models, either by fine-tuning or by freezing features (i.e. model weights) (<xref ref-type="bibr" rid="bib77">Yosinski et al., 2014</xref>). The latter approach, if applied to the same task (e.g. image segmentation), does not require any further model training. These <italic>out-of-the-box</italic> models reduce time and hardware requirements and may further increase objectivity of image analysis, by altogether excluding the need for any additional manual input.</p><p>Consequently, we hypothesized that transfer learning from pretrained model ensembles would substantially reduce the training efforts (<xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>) and might even increase objectivity of bioimage analysis. To test this, we followed three different initialization variants of the consensus ensemble strategy (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). In addition to starting the training of DL models with randomly initialized weights (<xref ref-type="fig" rid="fig5">Figure 5A</xref> - <italic>from scratch</italic>), we reused the consensus ensemble weights from the previous evaluation (<italic>Lab-Wue1</italic>) by means of fine-tuning (<xref ref-type="fig" rid="fig5">Figure 5A</xref> - <italic>fine-tuned</italic>) and freezing of all model layers (<xref ref-type="fig" rid="fig5">Figure 5A</xref> - <italic>frozen</italic>). Although no training of the <italic>frozen</italic> model is required, we tested and evaluated the performance of <italic>frozen</italic> models to ensure their validity. After performing the similarity analysis, we compared the full bioimage analyses, including quantification and hypothesis testing, of the different initialization variants. Finally, to establish a notion of external validity, we also compared these results with the manually and independently performed bioimage analysis of a lab-specific expert (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Consensus ensembles for DL-based feature annotation in external bioimage data sets.</title><p>(<bold>A</bold>) Schematic overview depicting three initialization variants for creating consensus ensembles on new datasets. Data annotation by multiple human experts and subsequent ground truth estimation are required for all three initialization variants. In the <italic>from scratch</italic> variant, a U-Net model with random initialized weights is trained on pairs of microscopy images and estimated ground truth annotations. This variant was used to create consensus ensembles for the initial <italic>Lab-Wue1</italic> dataset. Alternatively, the same training dataset can be used to adapt a U-Net model with pretrained weights by means of transfer-learning (<italic>fine-tuned</italic>). In both variants, models are evaluated and selected on base of a validation set after model training. In a third variant, U-Net models with pretrained weights can be evaluated directly on a validation dataset, without further training (<italic>frozen</italic>). In all three variants, consensus ensembles of the respective models are then used for bioimage analysis. (<bold>B</bold>) Overall reliability of bioimage analysis results of each variant assessed as variation per effect. In all three strategies, consensus ensembles (orange) showed lower standard deviations than consensus models (blue). The <italic>frozen</italic> results need to be considered with caution as they are based on models that did not meet the selection criterion (see <xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref>). N<sub>pairwise comparisons</sub> = 6; N<sub>consensus models</sub> = 15, and N<sub>consensus ensembles</sub> = 3 for each variant. (<bold>C–E</bold>) Detailed comparison of the two external datasets with highest (<italic>Lab-Mue</italic>) and lowest (<italic>Lab-Wue2</italic>) similarity to <italic>Lab-Wue1</italic>. (<bold>C</bold>) Representative microscopy images. Orange: representative annotations of a lab-specific <italic>from scratch</italic> consensus ensemble. PVT: para-ventricular nucleus of thalamus, eRet: early retrieval, lRet: late retrieval, HB: hindbrain, wt: wildtype, kd: <italic>gad1b</italic> knock-down. Scale bars: <italic>Lab-Mue</italic> 100 µm and <italic>Lab-Wue2</italic> 6 µm. (<bold>D</bold>) Mean <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of <italic>from scratch</italic> (solid line) and <italic>fine-tuned</italic> (dashed line) consensus models on the validation dataset over the course of training (iterations). Mean <inline-formula><mml:math id="inf41"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of <italic>frozen</italic> consensus models are indicated with arrows. Box plots show the <inline-formula><mml:math id="inf42"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> among the annotations of human experts as reference and the mean <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of selected consensus models. Two dotted horizontal lines mark the whisker ends of the <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> among the human expert annotations. (<bold>E</bold>) Effect sizes of all individual bioimage analyses (black: manual experts, blue: consensus models, orange: consensus ensembles). Three horizontal lines separate the significance intervals (n.s.: not significant, *: 0.05≥ p&gt;0.01, **0.01≥ p&gt;0.001, ***p ≤ 0.001 with Mann-Whitney-U tests). <italic>Lab-Mue</italic>: N<sub>consensus ensembles</sub> = 3 for all initialization variants; N<sub>from scratch/fine-tuned consensus models</sub> = 12 (for each ensemble, 4/5 trained models per ensemble met the selection criterion), N<sub>frozen consensus models</sub> = 12 (for each ensemble, 4/4 models per ensemble did not meet the selection criterion). N<sub>eRet</sub> = 4, N<sub>lRet</sub> = 4; n<sub>eRet</sub> = 12, n<sub>lRet</sub> = 11. <italic>Lab-Wue2</italic>: N<sub>consensus ensembles</sub> = 3 for each initialization variant; N<sub>from scratch/fine-tuned consensus models</sub> = 15 (for each ensemble, 5/5 trained models per ensemble met the selection criterion), N<sub>frozen consensus models</sub> = 12 (for each ensemble, 4/4 models per ensemble did not meet the selection criterion). N<sub>wt</sub> = 5, N<sub>kd</sub> = 4, n<sub>wt</sub> = 20, n<sub>kd</sub> = 15. Source files of all statistical analyses (including <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) are available in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>. Information on all bioimage datasets (e.g. the number of images, image resolution, imaging techniques, etc.) are available in <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>. Source files on model performance and selection are available in (<xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref>).</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Statistical data for <italic>Lab-Mue</italic>, <italic>Lab-Wue2</italic>, <italic>Lab-Inns1</italic>, and <italic>Lab-Inns2</italic>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-59780-fig5-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Characteristics of all five bioimage datasets.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-59780-fig5-data2-v2.docx"/></supplementary-material></p><p><supplementary-material id="fig5sdata3"><label>Figure 5—source data 3.</label><caption><title>Model performance with selection criterion for <italic>Lab-Mue</italic>, <italic>Lab-Wue2</italic>, <italic>Lab-Inns1</italic>, and <italic>Lab-Inns2</italic>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-59780-fig5-data3-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Performance of consensus ensembles on feature annotation in image dataset Lab-Inns01.</title><p>(<bold>A</bold>) Representative microscopy images. Orange: representative annotations of a lab-specific <italic>from scratch</italic> consensus ensemble. BLA: basolateral amygdala, Ctrl: control, Ext: extinction. Scale bar: 80 µm. (<bold>B</bold>) Mean <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of <italic>from scratch</italic> (solid line) and <italic>fine-tuned</italic> (dashed line) consensus models on the validation dataset over the course of training (iterations). Mean <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of <italic>frozen</italic> consensus models are indicated with an arrow. Box plots show the <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> among the annotations of human experts as reference and the mean <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of selected consensus models. Two dotted horizontal lines mark the whisker ends of the <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> among the human expert annotations. (<bold>C</bold>) Effect sizes of all individual bioimage analyses (black: manual experts, blue: consensus models, orange: consensus ensembles). Three horizontal lines separate four selected significance intervals (n.s.: not significant, *: 0.05 ≥ p&gt;0.01, **: 0.01 ≥ p&gt;0.001, ***: p ≤ 0.001). The analyses were performed with: N<sub>consensus ensembles</sub> = 3 for each initialization variant; N<sub>from scratch consensus models</sub> = 6 (for each ensemble, 2/5 trained models per ensemble met the selection criterion), N<sub>fine-tuned consensus models</sub> = 8 (for ensemble 1 and 2, 3/5 trained models per ensemble met the selection criterion; for ensemble 3, 2/5 trained models met the selection criterion), N<sub>frozen consensus models</sub> = 12 (for each ensemble, 4/4 models per ensemble did not meet the selection criterion). Thus, the <italic>frozen</italic> consensus model and ensemble results need to be considered with caution. N<sub>Ctrl</sub> = 5, N<sub>Ext</sub> = 5; n<sub>Ctrl</sub> = 10, n<sub>Ext</sub> = 9. Statistical data can be found in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>. Source files on model performance and selection are available in <xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Performance of consensus ensembles on fluorescent feature annotation in image dataset Lab-Inns02.</title><p>(<bold>A</bold>) Representative microscopy images. Orange: representative annotations of a lab-specific <italic>from scratch</italic> consensus ensemble. Resp: responders, nResp: non-responders. Scale bar: 40 µm. (<bold>B</bold>) Mean <inline-formula><mml:math id="inf50"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of <italic>from scratch</italic> (solid line) and <italic>fine-tuned</italic> (dashed line) consensus models on the validation dataset over the course of training (iterations). Mean <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of <italic>frozen</italic> consensus models are indicated with an arrow. Box plots show the <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> among the annotations of human experts as reference and the mean <inline-formula><mml:math id="inf53"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> of selected consensus models. Two dotted horizontal lines mark the whisker ends of the <inline-formula><mml:math id="inf54"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> among the human expert annotations. (<bold>C–E</bold>) Effect sizes of all individual bioimage analyses (black: manual experts, blue: consensus models, orange: consensus ensembles). Three horizontal lines separate four selected significance intervals (n.s.: not significant, *: 0.05 ≥ p&gt;0.01, **: 0.01 ≥ p&gt;0.001, ***: p ≤ 0.001 after Bonferroni correction for multiple comparisons). The analyses were performed with: N<sub>consensus ensembles</sub> = 3 for each initialization variant; N<sub>from scratch consensus models</sub> = 15 (for each ensemble, 5/5 trained models per ensemble met the selection criterion), N<sub>fine-tuned consensus models</sub> = 15, (for each ensemble, 5/5 trained models per ensemble met the selection criterion), N<sub>frozen consensus models</sub> = 12 (for each ensemble, 4/4 models per ensemble did not meet the selection criterion). Thus, the <italic>frozen</italic> consensus model and ensemble results need to be considered with caution. N<sub>saline</sub> = 6, N<sub>L-DOPA/MS-275 Resp</sub> = 6, N<sub>L-DOPA/MS-275 nResp</sub> = 3; n<sub>saline</sub> = 10, n<sub>L-DOPA/MS-275 Resp</sub> = 10, n<sub>L-DOPA/MS-275 nResp</sub> = 5. Statistical data can be found in <xref ref-type="supplementary-material" rid="fig5sdata1">Figure 5—source data 1</xref>. Source files on model performance and selection are available in (<xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Expert similarity across all datasets: F1 scores.</title><p>The heatmaps show the mean of <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> at a matching IoU-threshold of <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> for the image feature annotations of the indicated experts. The estimated ground truth (est. GT) was always calculated on all available expert annotations. The expert number refers to a unique human annotator (e.g. expert 1 is the same person across the datasets in A-D). The similarity scores were calculated on n = 5 images for A,B,C, and E and n = 9 images (test set) for D. The similarity between the same experts varies across the datasets (<bold>A–D</bold>), indicating that the heuristic bias of the annotators depends on the underlying data. However, expert 1 consistently yields the lowest similarity scores A, C, and D. The overall performance between one group of experts remains within a similar range for different datasets (<bold>A–D</bold>) and is comparable for a second group of experts on a different image dataset (<bold>E</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Expert similarity across all datasets: mean IoU.</title><p>The heatmaps show the mean of <inline-formula><mml:math id="inf57"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula> for the image feature annotations of the indicated experts. The estimated ground truth (est. GT) was always calculated on all available expert annotations. The expert number refers to a unique human annotator (e.g. expert 1 is the same person across the datasets in <bold>A-D</bold>). The similarity scores were calculated on n = 5 images for A, B, C, and E and n = 9 images (test set) for D. The similarity between the same experts varies across the datasets (<bold>A–D</bold>), indicating that the heuristic bias of the annotators depends on the underlying data. However, the overall performance between the experts remains within a similar range.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig5-figsupp4-v2.tif"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 5.</label><caption><title>Reliability of the consensus approaches across different datasets.</title><p>To asses the reliability and reproducibility of the <italic>consensus models</italic> and <italic>consensus ensembles</italic> strategies, we illustrate the within group <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> at a matching IoU-threshold of <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and the within group <inline-formula><mml:math id="inf60"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula> across different datasets. Color coding refers to the different DL strategies: blue to <italic>consensus models</italic>; orange to <italic>consensus ensembles</italic>. All images used for training were excluded from the analysis. The differences between the datasets highlight the difficulties in establishing a unified approach for automated fluorescent label annotation. (<bold>A</bold>) The <italic>Lab-Mue</italic> analysis comprises n = 24 images and the following models: N<sub>consensus models</sub> = 12 and N<sub>consensus ensembles</sub> = 3 for all initialization variants. (<bold>B</bold>) The <italic>Lab-Inns1</italic> analysis comprises n = 20 images and the following models: N<sub>from scratch consensus models</sub> = 6, N<sub>fine-tuned consensus models</sub> = 8, N<sub>frozen consensus models</sub> = 12, and N<sub>consensus ensembles</sub> = 3 for all initialization variants. (<bold>C</bold>) The <italic>Lab-Inns2</italic> analysis comprises n = 25 images and the following models: N<sub>from scratch consensus models</sub> = 15, N<sub>fine-tuned consensus models</sub> = 15, N<sub>frozen consensus models</sub> = 12, and N<sub>consensus ensembles</sub> = 3 for all initialization variants. (<bold>D</bold>) The <italic>Lab-Wue2</italic> analysis comprises n = 25 images and the following models: N<sub>from scratch consensus models</sub> = 15, N<sub>fine-tuned consensus models</sub> = 15, N<sub>frozen consensus models</sub> = 12, and N<sub>consensus ensembles</sub> = 3 for each initialization variant.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-59780-fig5-figsupp5-v2.tif"/></fig></fig-group><sec id="s2-3-1"><title>Dataset characteristics</title><p>The first dataset (<italic>Lab-Mue</italic>) represents very similar image parameters compared to our original <italic>Lab-Wue1</italic> dataset (<xref ref-type="fig" rid="fig5">Figure 5C</xref> - <italic>Lab-Mue</italic> and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). Mice experienced restraint stress and subsequent Pavlovian fear conditioning (cue-conditioning, tone-footshock association) and the number of cFOS-positive cells in the paraventricular thalamus (PVT) was compared between early (eRet) and late (lRET) phases of fear memory retrieval. In the context of transfer learning, this dataset originates from a very similar domain and requires the same task (image segmentation). Another two external datasets are focused on the quantification of cFOS abundance (similar domain), albeit showing less similarity in image parameters to our initial dataset (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). In <italic>Lab-Inns1</italic>, mice underwent Pavlovian fear conditioning and extinction in the same context. The image dataset of <italic>Lab-Inns2</italic> shows cFOS immunoreactivity in the infralimbic cortex (IL) following fear renewal, meaning return of extinguished fear in a context different from the extinction training context. Since heterogeneity in this behavioral response was observed, mice were classified as responders (Resp) or non-responders (nResp), based on freezing responses (see Materials and methods). The image dataset of <italic>Lab-Wue2</italic> shows the least similarity of image parameters to the dataset of <italic>Lab-Wue1</italic>. This dataset represents another commonly used model organism in neurobiology, the zebrafish. Here, cell bodies of specific neurons (GABAergic neurons) instead of nuclei were fluorescently labeled (<xref ref-type="fig" rid="fig5">Figure 5C</xref> - <italic>Lab-Wue2</italic> and <xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). Hence, this dataset originates from a different domain but was acquired using the same technique.</p></sec><sec id="s2-3-2"><title>Similarity analysis</title><p>As only limited training data was available, we executed the similarity analysis for all external datasets by means of a <italic>k-fold cross-validation</italic>. We observed that the inter-rater variability differed between laboratories and different experts but remained comparable as previously for <italic>Lab-Wue1</italic> (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>, and <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>.) Both <italic>from scratch</italic> and <italic>fine-tuned</italic> initiation variants resulted in individual consensus models that reached human expert level performance (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). However, models adapted from pretrained weights yielded a higher validity in terms of similarity to the estimated ground truth. They either exceeded the maximal <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> reached by <italic>from scratch</italic> models (<xref ref-type="fig" rid="fig5">Figure 5D</xref> - <italic>Lab-Mue</italic>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) or reached them after less training iterations (<xref ref-type="fig" rid="fig5">Figure 5D</xref> - <italic>Lab-Wue2</italic>). As expected, the performance of <italic>frozen Lab-Wue1</italic>-specific consensus models was highly dependent on the image similarity between the original and the new dataset. Consequently, the <italic>out-of-the-box</italic> segmentation performance of the <italic>frozen Lab-Wue1</italic> models was very poor on dissimilar images (<xref ref-type="fig" rid="fig5">Figure 5D</xref> - <italic>Lab-Wue2</italic>), but we found it to be on par with human experts and adapted models on images that are highly similar to the original dataset (<xref ref-type="fig" rid="fig5">Figure 5D</xref> - <italic>Lab-Mue</italic> - very similar domain and the same task).</p></sec><sec id="s2-3-3"><title>Bioimage analysis results</title><p>To further strengthen the validity of our workflow, we compared all DL-based bioimage analyses to the manual analysis of a human expert from the individual laboratory (<xref ref-type="fig" rid="fig5">Figure 5E</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, and <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Bioimage analyses results of external datasets.</title><p>Data are based either on manual analysis or on annotations by a consensus ensemble. The results are given for the individual consensus ensemble initialization variants (<italic>from scratch, fine-tuned</italic>). p-Values of <italic>Lab-Inns2</italic> are corrected for multiple comparisons using Bonferroni correction. <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>: mean group 1, <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>: mean group 2, U: U-statistic, eRet: early retrieval, lRet: late retrieval, Ctrl: control, Ext: extinction, Sal: saline, Res: L-DOPA/MS-275 responder, nRes: L-DOPA/MS-275 non-responder, wt: wildtype, kd: <italic>gad1b</italic> knock-down.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Lab</th><th>Groups</th><th>Initialization variant</th><th><inline-formula><mml:math id="inf64"><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></th><th><inline-formula><mml:math id="inf65"><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></th><th>U</th><th>Significance level (p)</th><th><inline-formula><mml:math id="inf66"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula></th></tr></thead><tbody><tr><td>Mue</td><td>eRet ∼ lRet</td><td>Manual</td><td>1.00</td><td>1.65</td><td>19.0</td><td>** (0.002)</td><td>0.39</td></tr><tr><td/><td/><td>From scratch</td><td>1.00</td><td>1.70</td><td>25.0</td><td>** (0.007)</td><td>0.31</td></tr><tr><td/><td/><td>Fine-tuned</td><td>1.00</td><td>1.68</td><td>24.0</td><td>** (0.006)</td><td>0.32</td></tr><tr><td>Inns1</td><td>Ctrl ∼ Ext</td><td>Manual</td><td>1.00</td><td>3.92</td><td>10.0</td><td>** (0.005)</td><td>0.43</td></tr><tr><td/><td/><td>From scratch</td><td>1.00</td><td>2.26</td><td>13.0</td><td>* (0.010)</td><td>0.35</td></tr><tr><td/><td/><td>Fine-tuned</td><td>1.00</td><td>1.85</td><td>14.0</td><td>* (0.013)</td><td>0.33</td></tr><tr><td>Inns2</td><td>Sal ∼ Resp</td><td>Manual</td><td>1.00</td><td>1.83</td><td>5.0</td><td>** (0.002)</td><td>0.59</td></tr><tr><td/><td/><td>From scratch</td><td>1.00</td><td>1.96</td><td>0.0</td><td>*** (&lt;0.001)</td><td>0.71</td></tr><tr><td/><td/><td>Fine-tuned</td><td>1.00</td><td>2.07</td><td>0.0</td><td>*** (&lt;0.001)</td><td>0.71</td></tr><tr><td/><td>Sal ∼ nResp</td><td>Manual</td><td>1.00</td><td>1.05</td><td>27.0</td><td>n.s. (1.000)</td><td>0.00</td></tr><tr><td/><td/><td>From scratch</td><td>1.00</td><td>1.63</td><td>8.0</td><td>n.s. (0.130)</td><td>0.29</td></tr><tr><td/><td/><td>Fine-tuned</td><td>1.00</td><td>1.42</td><td>12.0</td><td>n.s. (0.377)</td><td>0.16</td></tr><tr><td/><td>Res ∼ nRes</td><td>Manual</td><td>1.83</td><td>1.05</td><td>42.0</td><td>n.s. (0.130)</td><td>0.29</td></tr><tr><td/><td/><td>From scratch</td><td>1.96</td><td>1.63</td><td>41.0</td><td>n.s. (0.173)</td><td>0.26</td></tr><tr><td/><td/><td>Fine-tuned</td><td>2.07</td><td>1.42</td><td>42.0</td><td>n.s. (0.130)</td><td>0.29</td></tr><tr><td>Wue2</td><td>wt ∼ kd</td><td>Manual</td><td>1.00</td><td>0.28</td><td>227.5</td><td>* (0.010)</td><td>0.19</td></tr><tr><td/><td/><td>From scratch</td><td>1.00</td><td>0.45</td><td>220.0</td><td>* (0.021)</td><td>0.16</td></tr><tr><td/><td/><td>Fine-tuned</td><td>1.00</td><td>0.37</td><td>216.0</td><td>* (0.029)</td><td>0.14</td></tr></tbody></table></table-wrap><p>For <italic>Lab-Mue</italic>, the bioimage analyses of all DL-based approaches, including the <italic>frozen</italic> consensus models and ensembles pretrained on <italic>Lab-Wue1</italic>, revealed a significantly higher number of cFOS-positive cells in the PVT of mice 24 hr after fear conditioning (lRET), which was confirmed by the manual expert analysis (<xref ref-type="fig" rid="fig5">Figure 5E</xref> - <italic>Lab-Mue</italic>, <xref ref-type="table" rid="table1">Table 1</xref>). Yet again, the formation of model ensembles increased the reproducibility of results by yielding less or almost no variation in the effect sizes (<xref ref-type="fig" rid="fig5">Figure 5E</xref> - <italic>Lab-Mue</italic>).</p><p>The manual expert analysis of the <italic>Lab-Inns1</italic> dataset revealed a significantly higher number of cFOS-positive nuclei in the basolateral amygdala (BLA) after extinction of a previously learned fear, which was also reliably detected by all consensus ensembles, regardless of initiation variant (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). However, this significant difference was only present in the analyses of most individual consensus models, both <italic>from scratch</italic> and <italic>fine-tuned</italic> (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Again, this could be attributed to a higher variability between the effect sizes of individual models, compared to a higher homogeneity among ensembles (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>For <italic>Lab-Inns2</italic>, the manual expert analysis as well as all DL-based approaches that were adapted to the <italic>Lab-Inns2</italic> dataset show increased numbers of cFOS-positive cells in the infralimbic cortex of L-DOPA/MS-275 responders (Resp) compared to control (Sal) mice (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). However, in L-DOPA/MS-275 non-responders (nResp), we did not observe a significant increase of cFOS-positive nuclei (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). Furthermore, the high effect sizes of the comparison between L-DOPA/MS-275 responders and non-responders further indicate that the differences observed in the behavioral responses of Resp and nResp mice were also reflected in the abundance of cFOS in the infralimbic cortex (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, <xref ref-type="table" rid="table1">Table 1</xref>).</p><p>Manual expert analysis of the fourth external dataset revealed a significantly lower amount of GABA-positive somata in <italic>gad1b</italic> knock-down zebrafish, compared to wildtypes (<xref ref-type="fig" rid="fig5">Figure 5E</xref> - <italic>Lab-Wue2</italic>, <xref ref-type="table" rid="table1">Table 1</xref>). Again, this effect was reliably detected by all deep-learning-based approaches that included training on the <italic>Lab-Wue2</italic>-specific training dataset and the effect sizes of ensembles showed less variability (<xref ref-type="fig" rid="fig5">Figure 5E</xref> - <italic>Lab-Wue2</italic>). Despite its poor segmentation performance and hence, poor validity, this effect was also present in the bioimage analysis of the <italic>frozen</italic> consensus models and ensembles pretrained on <italic>Lab-Wue1</italic> (<xref ref-type="fig" rid="fig5">Figure 5E</xref> - <italic>Lab-Wue2</italic>).</p><p>As with our initial dataset, we assessed reliability by calculating the variation per effect as the standard deviation of the reported effect sizes within each group and pooled these results across all external datasets. Consistent with the higher reliability of <italic>from scratch</italic> and <italic>fine-tuned</italic> ensemble annotations (<xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>), this analysis shows that the formation of model ensembles reduced the variation per effect in both variants, compared to the respective individual models (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The <italic>frozen</italic> models and ensembles exhibit a similar effect, but need to be considered with caution as they are based on models that did not meet the selection criterion (reliably performing on par with human experts; see 7.10.4 - Training, evaluation and model selection for a detailed explanation).</p><p>In summary, we assessed the reproducibility of our consensus ensemble strategy by using four external datasets. These datasets were acquired with different image acquisition techniques, investigate two common model organisms, and analyze the two main cellular compartments (nuclei and somata) at varying resolutions (<xref ref-type="supplementary-material" rid="fig5sdata2">Figure 5—source data 2</xref>). In-line with the results obtained on our initial dataset, we observed an increased reproducibility for the consensus ensembles compared to individual consensus models after training on all four external datasets (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>Moreover, our data also suggests that pretrained consensus models can even be deployed <italic>out-of-the-box</italic>, but only when carefully validated. Thus, sharing pretrained model weights across different laboratories reduces lab-specific biases within the bioimage analysis and may further increase objectivity and validity.</p><p>Ultimately, we conclude that our proposed ensemble consensus workflow is reproducible for different datasets and laboratories and increases objectivity, reliability, and validity of DL-based bioimage analyses.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present study contributes to bridging the gap between ‘methods’ and ‘biology’ oriented studies in image feature analysis (<xref ref-type="bibr" rid="bib48">Meijering et al., 2016</xref>). We explored the potentials and limitations of DL models utilizing the general quality criteria for quantitative research: objectivity, reliability, and validity. Thereby, we put forward an effective but easily implementable strategy that aims to establish reproducible, DL-based bioimage analysis within the life science community.</p><p>The number of DL-based tools for bioimage annotations and their accessibility for non-AI specialists is gradually increasing (<xref ref-type="bibr" rid="bib47">McQuin et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Haberl et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>). DL models can hold advantages over conventional algorithms (<xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>) and have the potential to be commonly used for bioimage analysis tasks throughout the life sciences. Usually, the performance of new bioimage analysis tools or methods is assessed by means of similarity measures to a certain ground truth (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib47">McQuin et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Haberl et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>). However, this is rarely sufficient to establish trust in the use of DL models for bioimage analysis, as the vast amount of parameters and flexibility to adapt DL models to virtually any task renders them prone to internalize unintended, but subjective human biases (<xref ref-type="bibr" rid="bib73">von Chamier et al., 2019</xref>). This is particularly true in the case of fluorescent feature analysis in bioimage datasets, as an objective ground truth is not available. In conjunction with the stochastic training process, this is a very critical point, because it holds the potential for intended or unintended tampering similar to p-hacking (<xref ref-type="bibr" rid="bib28">Head et al., 2015</xref>), for example by training DL models until non-significant results become significant.</p><p>To investigate the effects of DL-based strategies on the bioimage analysis of fluorescent features, we acquired a typical bioimage dataset (<italic>Lab-Wue1</italic>) and five experts manually annotated corresponding ROIs (here cFOS-positive nuclei) in a representative subset of images. Then, we tested three DL-based strategies for automatized feature segmentation. DL models were either trained on the manual annotations of a single expert (expert models) or on the input of multiple experts pooled by ground truth estimation (consensus models). In addition, we formed ensembles of consensus models (consensus ensembles).</p><sec id="s3-1"><title>Similarity analysis of fluorescent feature annotation</title><p>In accordance with previous studies, similarity analyses revealed a substantial level of inter-rater variability in the heuristic annotations of the single experts (<xref ref-type="bibr" rid="bib61">Schmitz et al., 1999</xref>; <xref ref-type="bibr" rid="bib11">Collier et al., 2003</xref>; <xref ref-type="bibr" rid="bib51">Niedworok et al., 2016</xref>). Furthermore, we confirmed the concerns already put forward by others (<xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib73">von Chamier et al., 2019</xref>) that training of DL models solely on the input of a single human expert imposes a high risk of incorporating an individual human bias into the trained models. We therefore conclude that models trained on single expert annotations resemble an automation of manual image annotation, but cannot remove subjective biases from bioimage analyses. Importantly, only consensus ensembles led to a coincident significant increase also in the reliability and validity of fluorescent feature annotations. Our analyses also show that annotations of multiple experts are imperative for two reasons: first, they mitigate or even eliminate the bias of expert-specific annotations and, second, are essential for the assessment of the model performance.</p></sec><sec id="s3-2"><title>Reproducibility and validity of bioimage analyses</title><p>Our bioimage dataset from <italic>Lab-Wue1</italic> enabled us to look at the impact of different DL-based strategies on the results of bioimage analyses. This revealed a striking model-to-model variability as the main factor impairing the reproducibility of DL-based bioimage analyses. Convincingly, the majority votes for each effect were identical for all three strategies. However, the variance within the reported effect sizes differed significantly for each strategy. This entailed, for example, that no expert model was in full agreement with the congruent majority votes. On the contrary, consensus ensembles detected all effects with significantly higher reliability. Thus, our data indicates that bioimage analysis performed with a consensus ensemble significantly reduces the risk of obtaining irreproducible results.</p></sec><sec id="s3-3"><title>Evaluation of consensus ensembles on external datasets</title><p>We then tested our consensus ensemble approach and three initialization variants on four external datasets with limited training data and varying similarities in terms of image parameters to our original dataset (<italic>Lab-Wue1</italic>). In line with previous studies on transfer learning, we demonstrate that the adaptation of models from pretrained weights to new, yet similar data requires less training iterations, compared to the training of models <italic>from scratch</italic> (<xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>). We extend these analyses and show that the reliability of <italic>fine-tuned</italic> ensembles was at least equivalent to <italic>from scratch</italic> ensembles, if not higher. Furthermore, we also provide initial evidence that pretrained ensembles can be used even without any adaptation, if task similarity is sufficiently high. Our data suggest that this component in the analysis pipeline could further increase the objectivity of bioimage analyses.</p></sec><sec id="s3-4"><title>Potentials of open-source pretrained consensus ensemble libraries</title><p>Sharing model weights from validated models in open-source libraries, similarly to <italic>TensorFlow Hub</italic> (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/hub">https://www.tensorflow.org/hub</ext-link>) or <italic>PyTorch Hub</italic> (<ext-link ext-link-type="uri" xlink:href="https://pytorch.org/hub/">https://pytorch.org/hub/</ext-link>), offers a great opportunity to provide annotation experience across labs in an open science community. In this study, for instance, we used the nuclear label of cFOS, an activity-dependent transcription factor, as fluorescent feature of interest. This label is in its signature indistinguishable from a variety of other fluorescent labels, like those of transcription factors (CREB, phospho-CREB, Pax6, NeuroG2, or Brain3a), cell division markers (phospho-histone H3), apotposis markers (Caspase-3), and multiple others. Similarly to the pretrained and shared models of <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>, we surmise that the learned feature representations (i.e. model weights) of our cFOS consensus ensembles may serve as a good initialization for models that aim at performing nucleosomatic fluorescent label segmentation in brain slices.</p><p>In line with the results of the <italic>Kaggle Data Science Bowl 2018</italic> (<xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>), however, our findings indicate that a model adapted to a specific data set usually outperforms a general model trained on different datasets from different domains. To use and share frozen <italic>out-of-the-box</italic> models across the science community, we therefore need to create a well-documented library that contains validated model weights for each specific task and domain (e.g. for each organism, marker type, image resolution, etc.). In conjunction with data repositories, this would also allow retrospective data analysis of prior studies.</p><p>In summary, open-source model libraries may contribute to a better reproducibility of scientific experiments (<xref ref-type="bibr" rid="bib15">Fanelli, 2018</xref>) by improving the objectivity in bioimage analyses, by offering openness to analysis criteria, and by sharing pretrained models for (re-)evaluation.</p></sec><sec id="s3-5"><title>Limitations</title><p>This paper describes a blueprint for the evaluation of DL models in biomedical imaging. Therefore, some of our methodological decisions were shaped by standardization considerations concerning the future deployment in bioimage analysis pipelines.</p><p>The project was triggered by segmentation tasks for fluorescent labels (cFOS) in the cell nucleus. These are rather simple features, and we could readily annotate data from different labs, which facilitated the evaluation. However, this limits the generalizability to more complex image segmentation tasks, where training data annotation is slow and tedious. In particular, human perceptive capabilities for richer graphical features, such as area, volume, or density, is much worse than for regular, linear image features (<xref ref-type="bibr" rid="bib10">Cleveland and McGill, 1985</xref>; <xref ref-type="bibr" rid="bib17">Feldman-Stewart et al., 2000</xref>). A case in point is the annotation of images showing ramified neurons or astrocytes. Such tasks would cause an enormous workload rendering complete human annotation virtually impossible. In this respect, we concur with prior research asserting that DL models based on human annotations will not be an option in these settings (<xref ref-type="bibr" rid="bib13">Driscoll et al., 2019</xref>).</p><p>The characteristics of our examined strategies are based on best practices in the field of DL and derived from extant literature (<xref ref-type="bibr" rid="bib48">Meijering et al., 2016</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>). The focus on the U-Net model architecture (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>) is a direct consequence of this standardization idea. Yet, it is also an important limitation of our study. Unlike more conventional studies that introduce a new method and provide a comprehensive performance comparison to the state of the art, we rely on U-net as the widely studied de facto standard for biomedical image segmentation purposes (<xref ref-type="bibr" rid="bib47">McQuin et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref>). Similarly, we chose to use STAPLE (<xref ref-type="bibr" rid="bib74">Warfield et al., 2004</xref>) as the benchmark procedure for ground truth estimation. Thereby, we forwent considering alternatives and variants (<xref ref-type="bibr" rid="bib39">Lampert et al., 2016</xref>). In addition, we tried different ways to incorporate the single expert annotations into one DL model. For instance, we followed the approach of <xref ref-type="bibr" rid="bib24">Guan et al., 2018</xref> by modeling individual experts in a multi-head DL model instead of pooling them in the first place. However, we decided to discard the approach as our tests did not improve the results but increased complexity.</p></sec><sec id="s3-6"><title>Accessibility of our workflow and pretrained consensus ensembles</title><p>To enable other researchers to easily access, to interact with, and to reproduce our results and to share our trained models, we provide an open-source <italic>Python</italic> library that is easily accessible for both local installation or cloud-based deployment.</p><p>With <italic>Jupyter Notebooks</italic> becoming the computational notebook of choice for data scientists (<xref ref-type="bibr" rid="bib56">Perkel, 2018</xref>), we also implemented a training pipeline for non-AI experts in a <italic>Jupyter Notebook</italic> optimized for Google Colab, providing free access to the required computational resources (e.g., GPUs and TPUs). In summary, we recommend to use the annotations of multiple human experts to train and evaluate DL consensus model ensembles. In such a way, DL can be used to increase the objectivity, reliability, and validity of bioimage analyses and pave the way for higher reproducibility in science.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th>Reagent type (species) <break/>or resource</th><th>Designation</th><th>Source or reference</th><th>Identifiers</th><th>Additional <break/>information</th></tr></thead><tbody><tr><td>Genetic reagent (<italic>Mus musculus</italic>, male)</td><td>C57BL/6J</td><td>Charles River</td><td>Cat# CRL:027; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/IMSR_CRL:27">IMSR_CRL:27</ext-link></td><td>Lab-Mue; Lab-Inns1</td></tr><tr><td>Genetic reagent (<italic>Mus musculus</italic>, male)</td><td>C57BL/6J</td><td>Jackson Laboratory</td><td>Cat# JAX:000664; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/IMSR_JAX:000664">IMSR_JAX:000664</ext-link></td><td>Lab-Wue1</td></tr><tr><td>Genetic reagent (<italic>Mus musculus</italic>, male)</td><td>129S1/SvlmJ (S1)</td><td>Charles River</td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/MGI:5658424">MGI:5658424</ext-link></td><td>Lab-Inns2</td></tr><tr><td>Genetic reagent (<italic>Danio rerio</italic>)</td><td>AB/AB</td><td>European Zebrafish Resource Center</td><td/><td>Lab-Wue2</td></tr><tr><td>Antibody</td><td>Anti-cFOS (rabbit polyclonal)</td><td>Santa Cruz</td><td>Cat# sc-52; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_2106783">AB_2106783</ext-link></td><td>Lab-Mue (1:500); Lab-Inns2 (1:1,000)</td></tr><tr><td>Antibody</td><td>Anti-cFOS (rabbit polyclonal)</td><td>Millipore</td><td>Cat# PC38; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_2106755">AB_2106755</ext-link></td><td>Lab-Inns1 (1:20,000)</td></tr><tr><td>Antibody</td><td>anti-cFOS (rabbit polyclonal)</td><td>Synaptic Systems</td><td>Cat# 226003; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_2231974">AB_2231974</ext-link></td><td>Lab-Wue1 (1:10,000)</td></tr><tr><td>Antibody</td><td>Anti-GABA (rabbit polyclonal)</td><td>Sigma-Aldrich</td><td>Cat#A2025; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_477652">AB_477652</ext-link></td><td>Lab-Wue2 (1:400)</td></tr><tr><td>Antibody</td><td>Anti-NeuN (guinea-pig polyclonal)</td><td>Synaptic Systems</td><td>Cat# 266004; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_2619988">AB_2619988</ext-link></td><td>Lab-Wue1 (1:400)</td></tr><tr><td>Antibody</td><td>Anti-Parvalbumin (mouse monoclonal)</td><td>Sigma-Aldrich</td><td>Cat# P3088; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_477329">AB_477329</ext-link></td><td>Lab-Inns1 (1:2,500)</td></tr><tr><td>Antibody</td><td>Anti-Parvalbumin (mouse monoclonal)</td><td>Swant</td><td>Cat# PV235; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/AB_10000343">AB_10000343</ext-link></td><td>Lab-Wue1 (1:5,000)</td></tr><tr><td>Software, algorithm</td><td>ImageJ</td><td>Fiji <ext-link ext-link-type="uri" xlink:href="http://www.fiji.sc/">www.fiji.sc/</ext-link></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002285">SCR_002285</ext-link></td><td>Lab-Mue; Lab-Inns2; Lab-Wue1; Lab-Wue2</td></tr><tr><td>Software, algorithm</td><td>Improvision Openlab software</td><td>Perkin Elmer <break/><ext-link ext-link-type="uri" xlink:href="http://www.perkinelmer.com/pages/020/cellularimaging/products/openlab.xhtml">www.perkinelmer.com/</ext-link> <break/><ext-link ext-link-type="uri" xlink:href="http://www.perkinelmer.com/pages/020/cellularimaging/products/openlab.xhtml">pages/020/cellularimaging/</ext-link> <break/><ext-link ext-link-type="uri" xlink:href="http://www.perkinelmer.com/pages/020/cellularimaging/products/openlab.xhtml">products/openlab.xhtml</ext-link></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_012158">SCR_012158</ext-link></td><td>Lab-Inns1, Version 5.5.0</td></tr><tr><td>Software, algorithm</td><td>GraphPad Prism software</td><td>GraphPad Prism <break/><ext-link ext-link-type="uri" xlink:href="http://www.graphpad.com/scientific-software/prism/">www.graphpad.com/</ext-link> <break/><ext-link ext-link-type="uri" xlink:href="http://www.graphpad.com/scientific-software/prism/">scientific-software/prism/</ext-link></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_015807">SCR_015807</ext-link></td><td>Lab-Inns1, Version 7.0</td></tr><tr><td>Software, algorithm</td><td>CellSens Dimension Desktop software</td><td>Olympus <break/><ext-link ext-link-type="uri" xlink:href="http://www.olympus-lifescience.com/en/software/cellsens/">www.olympus-lifescience.com/</ext-link> <break/><ext-link ext-link-type="uri" xlink:href="http://www.olympus-lifescience.com/en/software/cellsens/">en/software/cellsens/</ext-link></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_016238">SCR_016238</ext-link></td><td>Lab-Inns2, Version 1.9</td></tr><tr><td>Software, algorithm</td><td>Fluoview FV10-ASW</td><td>Olympus <break/><ext-link ext-link-type="uri" xlink:href="http://www.photonics.com/Product.aspx?PRID=47380">www.photonics.com/</ext-link> <break/><ext-link ext-link-type="uri" xlink:href="http://www.photonics.com/Product.aspx?PRID=47380">Product.aspx?PRID=47380</ext-link></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014215">SCR_014215</ext-link></td><td>Lab-Wue1</td></tr><tr><td>Software, algorithm</td><td>Tensorflow</td><td><ext-link ext-link-type="uri" xlink:href="http://www.tensorflow.org">www.tensorflow.org</ext-link>, <xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_016345">SCR_016345</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>Keras</td><td><ext-link ext-link-type="uri" xlink:href="http://www.keras.io">www.keras.io</ext-link>, <xref ref-type="bibr" rid="bib8">Chollet, 2015</xref></td><td/><td/></tr><tr><td>Software, algorithm</td><td>Imagej</td><td><ext-link ext-link-type="uri" xlink:href="http://www.imagej.net/">www.imagej.net/</ext-link>, <xref ref-type="bibr" rid="bib60">Rueden et al., 2017</xref></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_003070">SCR_003070</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>SciPy</td><td><ext-link ext-link-type="uri" xlink:href="http://www.scipy.org">www.scipy.org</ext-link>, <xref ref-type="bibr" rid="bib33">Jones et al., 2001</xref></td><td>RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008058">SCR_008058</ext-link></td><td/></tr><tr><td>Software, algorithm</td><td>scikit-learn</td><td><ext-link ext-link-type="uri" xlink:href="http://www.scikit-learn.org/">www.scikit-learn.org/</ext-link>, <xref ref-type="bibr" rid="bib55">Pedregosa et al., 2011</xref></td><td/><td/></tr><tr><td>Software, algorithm</td><td>scikit-image</td><td><ext-link ext-link-type="uri" xlink:href="http://www.scikit-image.org/">www.scikit-image.org/</ext-link>, <xref ref-type="bibr" rid="bib72">van der Walt et al., 2014</xref></td><td/><td/></tr><tr><td>Software, algorithm</td><td>Pingouin</td><td><ext-link ext-link-type="uri" xlink:href="https://pingouin-stats.org/">https://pingouin-stats.org/</ext-link>, <xref ref-type="bibr" rid="bib70">Vallat, 2018</xref></td><td/><td/></tr><tr><td>Software, algorithm</td><td>simpleITK</td><td><ext-link ext-link-type="uri" xlink:href="http://www.simpleitk.org/">www.simpleitk.org/</ext-link>, <xref ref-type="bibr" rid="bib44">Lowekamp et al., 2013</xref></td><td/><td/></tr></tbody></table></table-wrap><p>Data sets regarding animal behavior, immunofluorescence analysis and image acquisition were performed in five independent laboratories using lab-specific protocols. Experiments were not planned together to ensure the individual character of the datasets. We refer to the lab-specific protocols as follows:</p><list list-type="bullet"><list-item><p><italic>Lab-Mue</italic>: Institute of Physiology I, University of Münster, Germany</p></list-item><list-item><p><italic>Lab-Inns1</italic>: Department of Pharmacology, Medical University of Innsbruck, Austria</p></list-item><list-item><p><italic>Lab-Inns2</italic>: Department of Pharmacology and Toxicology, Institute of Pharmacy and Center for Molecular Biosciences Innsbruck, University of Innsbruck</p></list-item><list-item><p><italic>Lab-Wue1</italic>: Institute of Clinical Neurobiology, University Hospital, Würzburg, Germany</p></list-item><list-item><p><italic>Lab-Wue2</italic>: Department of Child and Adolescent Psychiatry, Center of Mental Health, University Hospital of Würzburg, Würzburg, Germany</p></list-item></list><sec id="s4-1"><title>Contacts for reagent and resource sharing</title><p>Further information and requests for resources and reagents should be directed to and will be fulfilled by the lead contact, Robert Blum (Blum_R@ukw.de). Requests regarding the machine learning model and infrastructure should be directed to Christoph M. Flath (christoph.flath@uni-wuerzburg.de).</p></sec><sec id="s4-2"><title>Experimental models</title><sec id="s4-2-1"><title>Mice</title><sec id="s4-2-1-1"><title>Lab-Mue</title><p>Male C57Bl/6J mice (Charles River, Sulzfeld, Germany) were kept on a 12hr-light-dark cycle and had access to food and water ad libitum. No more than five and no less than two mice were kept in a cage. Experimental animals of an age of 9–10 weeks were single housed for 1 week before the experiments started. All animal experiments were carried out in accordance with European regulations on animal experimentation and protocols were approved by the local authorities (Landesamt für Natur, Umwelt und Verbraucherschutz Nordrhein-Westfalen).</p></sec><sec id="s4-2-1-2"><title>Lab-Inns1</title><p>Experiments were performed in adult, male C57Bl/6NCrl mice (Charles River, Sulzfeld, Germany) at least 10–12 weeks old, during the light phase of the light/dark cycle. They were bred in the Department of Pharmacology at the Medical University Innsbruck, Austria in Sealsafe IVC cages (1284L Eurostandard Type II L: 365 × 207×140 mm, floor area cm<sup>2</sup> 530, Tecniplast Deutschland GmbH, HohenpeiÃŸenberg, Germany). Mice were housed in groups of three to five animals under standard laboratory conditions (12 hr/12 hr light/dark cycle, lights on: 07:00, food and water ad libitum). All procedures involving animals and animal care were conducted in accordance with international laws and policies (Directive 2010/63/EU of the European parliament and of the council of 22 September 2010 on the protection of animals used for scientific purposes; Guide for the Care and Use of Laboratory Animals, U.S. National Research Council, 2011) and were approved by the Austrian Ministry of Science. All efforts were taken to minimize the number of animals used and their suffering.</p></sec><sec id="s4-2-1-3"><title>Lab-Inns2</title><p>Male 3-month-old 129S1/SvImJ (S1) mice (Charles River, Sulzfeld, Germany) were housed (four per cage) in a temperature- (22 ± 2°C) and humidity- (50–60%) controlled vivarium with food and water ad libitum under a 12 hr light/dark cycle. All mice were healthy and pathogen-free, with no obvious behavioral phenotypes. The Austrian Animal Experimentation Ethics Board (Bundesministerium für Wissenschaft Forschung und Wirtschaft, Kommission für Tierversuchsangelegenheiten) approved all experimental procedures.</p></sec><sec id="s4-2-1-4"><title>Lab-Wue1</title><p>All experiments were in accordance with the Guidelines set by the European Union and approved by our institutional Animal Care, the Utilization Committee and the Regierung von Unterfranken, Würzburg, Germany (License number: 55.2–2531.01-95/13). C57BL/6J wildtype mice were bred in the animal facility of the Institute of Clinical Neurobiology, University Hospital of Würzburg, Germany. Mice were housed in groups of three to five animals under standard laboratory conditions (12 hr/12 hr light/dark cycle, food and water ad libitum). All mice were healthy and pathogen-free, with no obvious behavioral phenotypes. Mice were quarterly tested according to the Harlan 51M profile (Harlan Laboratories, Netherlands). Yearly pathogen-screening was performed according to the Harlan 52M profile. All behavioral experiments were performed with male mice at an age of 8–12 weeks during the subjective day-phase of the animals and were randomly allocated to experimental groups.</p></sec></sec><sec id="s4-2-2"><title>Zebrafish</title><sec id="s4-2-2-1"><title>Lab-Wue2</title><p>Zebrafish (<italic>Danio rerio</italic>) embryos of the AB/AB strain (European Zebrafish Resource Center, Karlsruhe, Germany) were cultivated at 28°C with a 14/10 hr light/dark cycle in Danieau's solution containing 0.2 mM phenylthiocarbamide to prevent pigmentation. The embryos were staged according to <xref ref-type="bibr" rid="bib35">Kimmel et al., 1995</xref>. To knock-down expression of <italic>gad1b</italic>, fertilized eggs were injected with 500 µM of a <italic>gad1b</italic> splice blocking morpholino targeting the exon 8/intron 8 boundary of <italic>gad1b</italic> (Ensembl, GRCz11). Morpholino sequence: 5'<named-content content-type="sequence">tttgtgatcagtttaccaggtgaga3</named-content>' (Gene Tools). The efficiency of the morpholino was tested by reverse transcription PCR on DNase I treated total RNA collected from 24 hr post-fertilization and 5 days post-fertilization embryos. Sanger sequencing showed that the morpholino causes a partial inclusion of intron 8, which generates a premature stop codon.</p></sec></sec></sec><sec id="s4-3"><title>Mouse behavior</title><sec id="s4-3-1"><title>Restraint stress and Pavlovian fear conditioning</title><sec id="s4-3-1-1"><title>Lab-Mue</title><p>Animals were randomly assigned to four groups considering the following conditions; stress vs. control and early retrieval vs. late retrieval. Mice experienced restraint stress and a Pavlovian fear-conditioning paradigm as described earlier <xref ref-type="bibr" rid="bib6">Chauveau et al., 2012</xref>. In brief, on day one, animals of the stress group underwent restraint stress for 2 hr by using a perforated standard 50 ml falcon tube, allowing ventilation, but restricting movement. Animals of the control group remained in their homecages. On day 10, animals were adapted through two presentations of six CS<sup>−</sup> (2.5 kHz tone, 85 dB, stimulus duration 10 s, inter-stimulus interval 20 s; inter-trial interval 6 hr). On the next day, fear conditioning was performed in two sessions of three randomly presented CS<sup>+</sup> (10 kHz tone, 85 dB, stimulus duration 10 s, randomized inter-stimulus interval 10–30 s; inter-session interval 6 hr), each of which was co-terminated with an unconditioned stimulus (scrambled foot shock of 0.4 mA, duration 1 s). Animals of the early retrieval group underwent a retrieval phase on the same day (day 11), 1 hr after the last conditioning session, whereas animals of the late retrieval group underwent the retrieval phase on the next day (day 12), 24 hr after the conditioning session. For fear memory retrieval, mice were transferred to a new context. After an initial habituation phase of 2 min, mice were exposed to 4 CS<sup>−</sup> and 40 s later to 4 CS<sup>+</sup> presentations (stimulus duration 10 s, inter-stimulus interval 20 s) without receiving foot shocks. Afterwards, mice remained in this context for another 2 min before being returned to their homecages.</p></sec></sec><sec id="s4-3-2"><title>Fear conditioning and extinction</title><sec id="s4-3-2-1"><title>Lab-Inns1</title><p>Mice were single housed and stored in the experimental rooms in cages covered by filter tops with food and water ad libitum 3 days before behavioral testing. Fear acquisition and fear extinction were performed in a fear conditioning arena consisting of a transparent acrylic rodent conditioning chamber with a metal grid floor (Ugo Basile, Italy). Illumination was 80 lux and the chambers were cleaned with 70% ethanol. On acquisition day, following a habituation period of 120 s, mice were fear conditioned to the context by delivery of 5 foot-shocks (unconditioned stimulus, US, 0.5 mA for 2 s) with a random inter-trial interval of 70–100 s. After the test, mice remained in the test apparatus for an additional 120 s and were then returned to their homecage. On the next day, fear extinction training was performed. For this, mice were placed into the same arena as during acquisition and left undisturbed for 25 min. Freezing behavior was recorded and quantified by a pixel-based analysis software in one min bins (AnyMaze, Stoelting, USA). 90 min after the end of the extinction training, the mice were killed and the brains were processed for immunohistochemistry. Mice for homecage condition were kept in the experimental rooms for the same time period.</p></sec><sec id="s4-3-2-2"><title>Lab-Inns2</title><p>Cued fear conditioning, extinction and extinction retrieval was carried out as previously described <xref ref-type="bibr" rid="bib76">Whittle et al., 2016</xref>. Context dependence of fear extinction memories was assessed using a fear renewal tests in a novel context <xref ref-type="bibr" rid="bib50">Murphy et al., 2017</xref>. Fear conditioning and control of stimulus presentation occurred in a TSE operant system (TSE, Bad Homburg, Germany). Mice were conditioned in a 25 × 25 × 35 cm chamber with transparent walls and a metal-rod floor, cleaned with water, and illuminated to 300 lux ('context A'). The mice were allowed to acclimatize for 120 s before receiving three pairings of a 30 s, 75 dB 10 kHz sine tone conditioned stimulus (CS) and a 2 s scrambled-foot-shock unconditioned stimulus (US; 0.6 mA), with a 120 s inter-pairing interval. After the final pairing, mice were given a 120 s stimulus-free consolidation period before they were returned to the homecage. Fear extinction training was performed in 'context B', a 25 × 25 × 35 cm cage with a solid gray floor and black walls, cleaned with a 100% ethanol and illuminated to 10 lux with a red lamp. After a 120 s acclimation period, the mice were subjected to 16x CS-alone trials, separated by 5 s inter-CS intervals. Extinction retrieval was conducted in 'context B' by repeating the conditions used in extinction training procedure but presenting only two CS trials. Fear renewal in a novel context was quantified 11 days following the extinction-retrieval test in a novel context ('context C'), a round plexiglas cylinder of 20 cm in diameter, and a height of 35 cm. The cylinder was covered on the outside with red diamond-printed white paper with an uneven pale ceramic tiled floor, illuminated to 5 lux with white light. After the mice were acclimated for 120 s, they were given two CS-alone trials, with a 5 s inter-CS interval. A trained observer blind to the animals' grouping measured freezing, defined as showing no visible movement except that required for respiration, as an index of fear <xref ref-type="bibr" rid="bib23">Gruene et al., 2015</xref>. The observer manually scored freezing based on video recordings throughout the CS and determined the duration of freezing within the CS per total time of the CS in percent. Freezing during all phases was averaged over two CS presentations and presented in eight trial blocks during extinction training and a one trail block each for extinction retrieval and fear renewal. Freezing during fear conditioning was quantified and presented as single CS. In the treatment group, mice which displayed freezing levels two times above than the standard deviation of the mean were classified as non-responders.</p></sec></sec><sec id="s4-3-3"><title>Contextual fear conditioning</title><sec id="s4-3-3-1"><title>Lab-Wue1</title><p>Male animals, initially kept as siblings in groups, were put to a new cage and kept in single-housing conditions with visual, olfactory and auditory contact in a ventilated cabinet (Scantainer, Scanbur). To habituate the mice to the male experimenter and the experimental rooms, mice were handled twice a day for at least 2 consecutive days prior to behavioral analysis. Mice were put to three different groups: (1) the homecage group, (2) the context control group that experienced the training context, but did not receive any electric foot shock, and (3) the context-conditioned group, which received electric foot shocks in the training context. Contextual fear (threat) conditioning was performed in a square conditioning arena with a metal grid floor (Multi conditioning setup, 256060 series, TSE, Bad Homburg, Germany). Before each experiment, the arena was cleaned with 70% ethanol. Mice were transported in their homecage to the experimental rooms and were put into the conditioning arena. After an initial habituation phase of 60 s, fear acquisition was induced by five electric foot shocks (unconditioned stimulus, US; 1 s, 0.7 mA) with an inter-stimulus interval of 60 s. After the foot shock presentation, mice remained in the training context for 30 s before being returned to their homecages in their housing cabinet. For fear memory retrieval, 24 hr after the training session, the mice were re-exposed to the conditioning arena for 360 s, without any US presentation. Mice were again put back to their homecage for 90 min, before mice were anesthetized and prepared for immunohistological analysis. Mouse behavior was videotaped. The MCS FCS-SQ MED software (TSE, Bad Homburg, Germany) was used to automatically track mice behavior and to quantify the freezing behavior during all sessions. Freezing was defined as a period of time of at least 2 s showing absence of visible movement except that required for respiration (<xref ref-type="bibr" rid="bib16">Fanselow, 1980</xref>). The percentage time spent freezing was calculated by dividing the amount of time spent in the training chamber.</p></sec></sec></sec><sec id="s4-4"><title>Drug treatment</title><p>MS-275 (Entinostat, Selleck Chemicals, Vienna, Austria; 10 mg kg<sup>−1</sup> dissolved in saline +25% dimethylsulfoxide vehicle) was administered immediately (&lt;1 min) following an extinction training session and L-DOPA (Sigma-Aldrich, Vienna, Austria; 20 mg kg<sup>−1</sup> dissolved in saline) was administered 1 hr before an extinction training session. All drugs were administered intraperitoneally in a volume of 10 ml kg<sup>−1</sup> body weight. Control animals received saline. Mice were randomly selected to be administered either vehicle or pharmacological compound (<xref ref-type="bibr" rid="bib76">Whittle et al., 2016</xref>).</p></sec><sec id="s4-5"><title>Immunohistochemistry and microscopy</title><sec id="s4-5-1"><title>Lab-Mue</title><p>Mice were anesthetized via inhalation anesthesia (isoflurane, 5% in O<sub>2</sub>; CP Pharma, Germany) and perfused with phosphate-buffered saline (PBS) and then 4% paraformaldehyde (PFA; Roti-Histofix 4%, Carl Roth). Brains were isolated and post-fixed overnight in 4% PFA, treated with 30% sucrose/PBS solution for 48 hr, and then stored at 4°C until sectioning. Coronal sections (40 µm thick) were prepared on a freezing microtome (Leica, Wetzlar, Germany) and stored in PBS until use. Immunostaining was performed on free-floating sections. Sections were washed 3 × 10 min with PBS and then incubated in blocking solution (10% goat serum, 3% BSA, 0.3% Triton X100 in PBS) for 1 hr. After blocking, sections were treated at 4°C overnight with a primary antibody (rabbit anti-cFOS, 1:500, Santa Cruz Biotechnology, California, USA) diluted in blocking solution. On the next day, sections were washed 3 × 10 min with PBS and incubated for 1 hr at room temperature with the secondary antibody (goat anti-rabbit Alexa Fluor 488, 1:1000; Invitrogen, Germany) diluted in blocking solution. The incubation was followed by three 5 min washing steps in PBS. Sections were then mounted on SuperFrostPlus slides (Menzel, Braunschweig, Germany) and embedded with Vectashield Mounting Medium (Vector Laboratories, Burlingame, California) + 4’,6-diamidino- 2-phenylindole (DAPI). Fluorescence labeling was visualized and photographed using a laser-scanning confocal microscope (Nikon eC1 plus) with a 16x water objective at a step size of 1.5 µm, covering the whole section. Identical exposure settings were used for images that show the same region in the brains. The experimenter was blinded to the treatment conditions.</p></sec><sec id="s4-5-2"><title>Lab-Inns1</title><p>Ninety minutes after extinction training, mice were injected intraperitoneally with thiopental (150 mg/kg, i.p., Sandoz, Austria) for deep anesthesia. Transcardial perfusion, 3 min with PBS at room temperature followed by 10 min of 4% PFA at 4°C, was performed by a peristaltic pump at a flow rate of 9 ml/min (Ismatec, IPC, Cole-Parmer GmbH, Wertheim, Germany). Subsequently, brains were removed and postfixed in 4% PFA for 90 min at 4°C, cryoprotected for 48 hr in 20% sucrose at 4°C and then snap frozen in isopentane (2-methylbutane, Merck GmbH, Austria) for 3 min at −60°C. Brains were transferred to pre-cooled open tubes and stored at −70°C until further use. For immunohistochemistry, coronal 40 µm sections were cut by a cryostat from rostral to caudal, collected in Tris-buffered saline (TBS) + 0.1% sodium azide. Sections from Bregma −1.22 mm (<xref ref-type="bibr" rid="bib21">Franklin and Paxinos, 2008</xref>) were incubated for 30 min in TBS-Triton (0.4%), for 90 min in 10% normal goat/horse serum and overnight with the first primary antibody (diluted in 10% serum containing 0.1% sodium azide). Rabbit anti-cFOS (Millipore, PC-38, 1:20,000) and mouse anti-Parvalbumin (Sigma-Aldrich, P3088, 1:2500) were used as primary antibodies. After washing with TBS-buffer 3 × 5 min, secondary antibodies (goat anti-rabbit, Vector Laboratories inc, PI-1000, 1:1000 and biotinylated horse anti-mouse, Vector Laboratories inc, PK-4002, 1:200) were added to the sections for 150 min. Then, sections were incubated in the dark for 8 min in TSA-fluorescein (in-house, 1:100) staining solution (50 mM PBS and 0.02% H<sub>2</sub>O<sub>2</sub>). Sections were rinsed 3 × 5 min in TBS buffer and then incubated for 100 min in a solution of streptavidin Dylight 649 (Vector laboratories, SA5649, 1:100) in TBS buffer. Fluorescently stained sections were mounted on slides using gelatin and cover-slipped with glycerol-DABCO anti-fading mounting medium. Photomicrographs were taken on a fluorescent microscope (Zeiss Axio Imager M1) equipped with a halogen light source, respective filter sets and a monochrome camera (Hamamatsu ORCA ER C4742-80-12AG). Images of the basolateral amygdala (BLA) were taken with an EC Plan-Neofluar 10x/0.3 objective. All images were acquired using the same exposure time and software settings and the experimenter was blinded to the treatment conditions (homecage vs extinction).</p></sec><sec id="s4-5-3"><title>Lab-Inns2</title><p>Mice were killed 2 hr after the start of the fear renewal session using an overdose of sodium pentobarbital (200 mg/kg) and transcardially perfused with 40 ml of 0.9% saline followed by 40 ml of 4% paraformaldehyde in 0.1 M phosphate buffer, pH 7.4. Brains were then removed and post fixed at 4°C for 2 hr in 4% paraformaldehyde in phosphate buffer. Brains were sectioned at the coronal plane with a thickness of 40 µm on a vibratome (VT1000S, Leica). Free-floating sections were incubated for 30 min in blocking solution using 1% BSA in 50 mM Tris buffer (pH 7.4) with 0.1% Triton-X100 and incubated overnight at 4°C with a rabbit antibody against cFOS (1:1000; sc-52, Szabo-Scandic, Vienna, Austria). The sections were then washed (3 × 15 min in 1% BSA in Tris buffer containing 0.1% Triton-X100) and incubated for 2.5 hr with a secondary CY2-conjugated donkey anti rabbit IgG (1:500, #82371, Jackson ImmunoResearch). The sections were then washed (3 × 15 min in 50 mM Tris buffer), mounted on microscope slides and air-dried. Slides were embedded in ProLong Gold anti-fade reagent containing DAPI (P36935, Life Technologies). Immunofluorescence was assessed using a fluorescent microscope (Olympus BX51 microscope, Olympus XM10 video camera, CellSens Dimension 1.5 software, Olympus). Immunolabeled sections were visualized using a 20x oil-objective (UPlanSApo, Olympus) at 488 nm excitation.</p></sec><sec id="s4-5-4"><title>Lab-Wue1</title><p>To analyze anti-cFOS immunoreactivity after retrieval of a contextual memory, mice were anesthetized 90 min after the end of the retrieval session (C+). Mice that spent the same time in the conditioning arena without presentation of the US served as context controls (C-). Single-housed mice that were never exposed to the conditioning arena served as naïve learning controls (homecage; H). A rodent anesthesia setup (Harvard Apparatus) was used to quickly anesthetize the mice with the volatile narcotic isoflurane (airflow 0.4 L/min, 4% isoflurane, Iso-Vet, Chanelle) for one minute. Then a mixture of ketamine (120 mg/kg; Ketavet, Pfizer) and xylazine (16 mg/kg; cp-Pharma, Xylavet, Burgdorf, Germany) was injected (12 µl/g bodyweight, intraperitoneal) to provide sedation and analgesia. Then anesthetized mice were transcardially perfused (gravity perfusion) with 0.4% heparin (Ratiopharm) in phosphate-buffered saline (PBS), followed by fixation with 4% paraformaldehyde in PBS. Brains were dissected and post-fixed in 4% paraformaldehyde for two hours at 4°C. The tissue was embedded in 6% agarose and coronal sections (40 µm) were cut using a vibratome (Leica VT1200). A total of 30 sections starting from Bregma −1.22 mm (<xref ref-type="bibr" rid="bib21">Franklin and Paxinos, 2008</xref>) were considered as dorsal hippocampus. Immunofluorescent labeling was performed in 24-well plates with up to three sections per well under constant shaking. Slices were first incubated in 100 mM glycine, buffered at pH 7.4 with 2 M Tris-base for 1 hr at room temperature. Slices were transferred in blocking solution consisting of 10% normal horse serum, 0.3% Triton X100, 0.1% Tween 20 in PBS for 1 hr at room temperature. Primary antibodies were applied in blocking solution for 48 hr at 4°C. The following primary antibodies were used at indicated dilutions: mouse anti-Parvalbumin, SWANT, PV235, 1:5,000; guinea-pig anti-NeuN, SynapticSystems, 266004, 1:400; rabbit anti-cFOS, SynapticSystems, 226003, 1:10,000 (lot# 226003/7). Secondary antibodies were used for 1.5 hr at room temperature at a concentration of 0.5 µg / ml in blocking solution. The following antibodies were used: goat anti-mouse Alexa-488 conjugated (Life sciences, Thermo), donkey anti-rabbit Cy3 conjugated (Jackson ImmunoResearch), and donkey anti-guinea-pig Cy5 conjugated (Jackson ImmunoResearch). Sections were embedded in Aqua-Poly/Mount (Polysciences). Confocal image acquisition was performed with an Olympus IX81 microscope combined with an Olympus FV1000 confocal laser scanning microscope, a FVD10 SPD spectral detector and diode lasers of 473, 559, and 635 nm. Image acquisition was performed using an Olympus UPlan SAPO 20x/0.75 objective. Images with 1024 pixel to monitor 636 µm<sup>2</sup> were taken as 12 bit z-stacks with a step-size of 1.5 µm, covering the whole section. Images of <italic>dentate gyrus</italic> (DG), <italic>Cornu ammonis 1</italic> (CA1) and CA3 were taken in each hemisphere of three sections of the dorsal hippocampus to achieve a maximum of six images (n) per region for each animal (N). During image acquisition, the experimenter was blinded to the treatment condition (C+ versus C- versus H).</p></sec><sec id="s4-5-5"><title>Lab-Wue2</title><p>For immunohistochemistry of whole-mount specimens, the embryos were fixed at 30 hr post-fertilization in 4% PFA at 4°C over night. The specimens were subsequently washed 3 × 10 min in PBS with 0.1% Tween-20 (PBST) and then once for 5 min in 150 mM Tris-HCl buffer (pH 9.0). The solution was exchanged for fresh Tris-HCl buffer, and the embryos were incubated for 15 min at 70°C, cooled down to room temperature and then washed 2 × 5 min in PBST. To further increase permeability, the embryos were first rinsed quickly two times in ice-cold dH<sub>2</sub>O and then incubated with pre-cooled acetone for 20 min at −20°C. The acetone was quickly washed off with dH<sub>2</sub>O, and then with PBS containing 0.8% Triton X100 (PBSTX) for 2 × 5 min. The specimens were incubated at room temperature for 1 hr in blocking buffer (PBSTX with 10% normal sheep serum and 2% bovine serum albumin) and subsequently with the primary antibody (rabbit-anti-GABA, Sigma-Aldrich A2052, diluted 1:400 in blocking buffer) at 4°C for 3 days with gentle shaking. After extensive washes in PBSTX, the embryos were incubated with the secondary antibody (goat-anti-rabbit AlexaFluor488, Invitrogen, Thermo Fisher Scientific, diluted 1:1000 in blocking buffer) at 4°C for 2 days with gentle shaking. Finally, the embryos were washed extensively in PBST, transferred and stored in 80% glycerol in PBST at 4°C until imaging. Before microscopy, the yolk was removed and the embryos were mounted in 80% glycerol to be imaged from the dorsal side. Confocal image acquisition was performed using a Nikon eclipse C1 laser scanning microscope with a Plan Apo VC 20x/0.75 DIC N2 objective and a Coherent Saphire 488 nm laser. All specimens were imaged using NIS Elements software (Nikon) with the same acquisition settings. Images with 2048x2048 pixels were taken as 12 bit z-stacks with a step-size of 2.5 µm, covering the whole region, including the dorsal-ventral dimension, of the hindbrain that contains GABA immunoreactive cells.</p></sec></sec><sec id="s4-6"><title>Image processing and manual analysis</title><sec id="s4-6-1"><title>Lab-Mue</title><p>Images were adjusted in brightness and contrast using ImageJ. One expert from Lab-Mue manually segmented the paraventricular thalamus (PVT) and quantified cFOS-positive cells in the PVT for bioimage analysis. For the training and validation of DL models, cFOS-positive ROIs in five additional images were manually segmented by four experts from <italic>Lab-Wue1</italic> (expert 1 and experts 3–5) using ImageJ. All experts were blinded to another and the treatment condition.</p></sec><sec id="s4-6-2"><title>Lab-Inns1</title><p>Number of cFOS-positive neurons was obtained from two basolateral amygdalae (BLA) per animal of five homecage mice and five mice subjected to contextual fear extinction. PV staining was used to identify the localization and extension of the BLA and the borders were manually drawn by a neuroscientist of Lab-Inns1 using the free shape tool of the Improvision Openlab software (PerkinElmer). Boundaries were projected to the respective cFOS-immunoreactive image and cFOS-positive nuclei were counted manually inside that area by the expert of Lab-Inns1. For the training and validation of DL models, cFOS-positive ROIs in five additional images were manually segmented by four experts from <italic>Lab-Wue1</italic> (expert 1 and experts 3–5) using ImageJ. All experts were blinded to another and the treatment condition.</p></sec><sec id="s4-6-3"><title>Lab-Inns2</title><p>The anatomical localization of cells within the infralimbic cortex was aided by using illustrations in a stereotaxic atlas (<xref ref-type="bibr" rid="bib21">Franklin and Paxinos, 2008</xref>), published anatomical studies (<xref ref-type="bibr" rid="bib71">Van De Werd et al., 2010</xref>) and former studies in S1 mice (<xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Whittle et al., 2010</xref>). All analyses were done in a comparable area under similar optical and light conditions. For manual analysis, an expert of Lab-Inns2 viewed the digitized images on a computer screen using CellSens Dimension 1.5 software (Olympus Corporation, Tokyo, Japan) and evaluated cFOS-positive nuclei within the infralimbic cortex, the brain region of the interest. For the training and validation of DL models, cFOS-positive ROIs in five additional images were manually segmented by four experts from <italic>Lab-Wue1</italic> (expert 1 and experts 3–5) using ImageJ. All experts were blinded to another and the treatment condition.</p></sec><sec id="s4-6-4"><title>Lab-Wue1</title><p>For image preprocessing, 12-bit gray-scale confocal image z-stacks were projected (maximum intensity) and converted to 8-bit. Five expert neuroscientists from <italic>Lab-Wue1</italic> manually segmented cFOS-positive nuclei and Parvalbumin-positive somata as regions of interest (ROIs) in a total of 45 images (36 training and nine test images). The NeuN immunoreactive granule cell layer of the dentate gyrus and the pyramidal cell layer in CA1 and CA3 were annotated as NeuN-positive ROIs. The NeuN-positive areas used for the quantifications of cFOS-positive cells were identical for all analyses and segmented manually by one human expert. All experts were blinded to another and the treatment condition.</p></sec><sec id="s4-6-5"><title>Lab-Wue2</title><p>The confocal z-stacks with 12-bit gray-scale images were imported into ImageJ (<xref ref-type="bibr" rid="bib62">Schneider et al., 2012</xref>). An expert of <italic>Lab-Wue2</italic> manually counted GABA-positive somata in every 4th section of each confocal z-stack, covering the entire hindbrain region housing GABAergic neurons (N<sub>untreated controls</sub> = 5, N<sub>morphants</sub> = 4). For the training and validation of DL models, GABA-positive ROIs in five additional images were manually segmented by three experts from <italic>Lab-Wue2</italic> (experts 6–8) using ImageJ. All experts were blinded to another and the treatment condition.</p></sec></sec><sec id="s4-7"><title>Ground truth estimation</title><p>In absence of an objective ground truth, we derived a probabilistic estimate of the ground truth by running the expectation-maximization algorithm for simultaneous truth and performance level estimation (STAPLE, <xref ref-type="bibr" rid="bib74">Warfield et al., 2004</xref>). The STAPLE algorithm iteratively estimates the ground truth segmentation (est. GT) based on the expert segmentation maps. During each algorithm iteration two steps are performed:</p><p><bold>Estimation step:</bold> The ground truth segmentation’s conditional probability is estimated based on the expert decisions and previous performance parameter estimates.</p><p><bold>Maximization step:</bold> The performance parameters (sensitivity and specificity) for each expert segmentation are estimated by maximizing the conditional expectation.</p><p>Iterations are repeated until convergence is reached. We implemented the algorithm using the simplified interface to the Insight Toolkit (SimpleITK 1.2.4, <xref ref-type="bibr" rid="bib44">Lowekamp et al., 2013</xref>).</p></sec><sec id="s4-8"><title>Evaluation metrics</title><p>All evaluation metrics were calculated using Python (version 3.7.3), SciPy (version 1.4.1), and scikit-image (version 0.16.2). We provide the source code in <italic>Jupyter Notebooks</italic> (see 7.13 Data and software availability).</p><sec id="s4-8-1"><title>Segmentation and detection</title><p>Following <xref ref-type="bibr" rid="bib4">Caicedo et al., 2019</xref> we based our evaluation on identifying segmentation and detection similarities on object-level (ROI-level). In a segmentation mask, we define an object as a set of pixels that were horizontally, vertically, and diagonally connected (8-connectivity). We only considered ROIs at a biologically justifiable size, depending on the data set characteristics. We approximated the minimum size based on the smallest area that was annotated by a human expert (<italic>Lab-Mue</italic>: 30px, <italic>Lab-Inns1</italic>: 16px, <italic>Lab-Inns2</italic>: 60px, <italic>Lab-Wue1</italic>: 30px, <italic>Lab-Wue2</italic>: 112px).</p><p>To compare the segmentation similarity between a source and a target segmentation mask, we first computed the intersection-over-union (IoU) score for all pairs of objects. The IoU, also known as Jaccard similarity, of two sets of pixels <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the size of the intersection divided by the size of the union:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>IoU</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∩</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∪</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Second, we used the pairwise IoUs to match the objects of each mask. We solved the assignment problem by maximizing the sum of IoUs by means of the Hungarian method (<xref ref-type="bibr" rid="bib37">Kuhn, 1955</xref>). This ensures an optimal matching of objects in the case of ambiguity, that is, overlap of one source object with one or more targets object. We reported the segmentation similarity of two segmentation masks by calculating the arithmetic mean of <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>M</mml:mi><mml:mtext>IoU</mml:mtext></mml:msub></mml:math></inline-formula> over all matching objects:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>IoU</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>M</mml:mi><mml:mtext>IoU</mml:mtext><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an assigned match and <italic>N</italic> denotes number of matching objects. By this definition, the <italic>Mean IoU</italic> only serves as a measure for the segmentation similarity of matching objects and neglects objects that do not overlap at all.</p><p>To address this issue, we additionally calculated measures to account for the detection similarity. Therefore, we define a pair of objects with an IoU is above a threshold <italic>t</italic> as correctly detected (true positive - <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>). Objects that match with an IoU at or below <italic>t</italic> or have no match at all are considered to be false negative (<inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>) for the source mask and false positive (<inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>) for the target mask. This allows us to calculate the Precision <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>M</mml:mi><mml:mtext>Precision</mml:mtext></mml:msub></mml:math></inline-formula>, Recall <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>M</mml:mi><mml:mtext>Recall</mml:mtext></mml:msub></mml:math></inline-formula>, and F1 score <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> as the harmonic mean of <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>M</mml:mi><mml:mtext>Precision</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>M</mml:mi><mml:mtext>Recall</mml:mtext></mml:msub></mml:math></inline-formula>:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>Precision</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>Recall</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mtext>F1 score</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mtext>Precision</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mtext>Recall</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mtext>Precision</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mtext>Recall</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as a fixed IoU threshold. If not indicated differently, we used <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> in our calculations.</p></sec><sec id="s4-8-2"><title>Inter-rater reliability</title><p>To quantify the reliability of agreement between different annotators we calculated Fleiss‘ <inline-formula><mml:math id="inf81"><mml:mi>κ</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib19">Fleiss and Cohen, 1973</xref>). In contrast to the previously introduced metrics, Fleiss‘ <inline-formula><mml:math id="inf82"><mml:mi>κ</mml:mi></mml:math></inline-formula> accounts for the agreement that would be expected by chance. For a collection of segmentation masks of the same image, each object (ROI) <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is assigned to a class <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <italic>N</italic> denotes the total number of unique objects (ROIs) and <italic>K</italic> the number of categories (<inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for binary segmentation). Then, <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the number of annotators who assigned object <italic>i</italic> to class <italic>j</italic>. We again leveraged the IoU metric to match the ROIs from different segmentation masks above a given threshold <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Following <xref ref-type="bibr" rid="bib19">Fleiss and Cohen, 1973</xref>, we define the proportion of all assignments for each class:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>d</italic> denotes the count of the annotators. We define the extent to which the annotators agree on the i-th object as<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Subsequently, we define the mean of the <inline-formula><mml:math id="inf88"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>e</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, Fleiss‘ <inline-formula><mml:math id="inf89"><mml:mi>κ</mml:mi></mml:math></inline-formula> at a given threshold <italic>t</italic> is defined as<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf90"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the degree of agreement attainable above chance and <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>P</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> the actually achieved agreement in excess of chance. To allow a better estimate of the chance we randomly added region proposals of class <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (background). If not indicated differently, we use <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> in our calculations.</p></sec></sec><sec id="s4-9"><title>Deep learning approach</title><p>The deep learning pipeline was implemented in Python (version 3.7.3), Tensorflow (version 1.14.0), Keras (version 2.2.4), scikit-image (version 0.16.2), and scikit-learn (version 0.21.2). We provide the source code in <italic>Jupyter Notebooks</italic> (see 7.13 Data and software availability).</p><sec id="s4-9-1"><title>Network architecture</title><p>We instantiated all DL models with a U-Net architecture (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>), a fully convolutional neural network for semantic segmentation. The key principle of a U-Net is that one computational path stays at the original scale, preserving the spatial information for the output, while the other computational path learns the specific features necessary for classification by applying convolutional filters and thus condensing information (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>). We adopted the model hyperparameters (e.g. hidden layers, activation functions, weight initialization) from <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref> as these are extensively tested and evaluated on different biomedical data sets. The layers of the U-Net architecture are logically grouped into an encoder and a decoder (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Following <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref> the VGG-like encoder consists of five convolutional modules. Each module comprises two convolution layers with no padding, each followed by a leaky ReLU with a leakage factor of 0.1 and a max-pooling operation with a stride of two. The last module, however, does not contain the max-pooling layer and constitutes the origin of the decoder. The decoder consists of four (up-) convolutional modules. Each of these modules comprises of a transposed convolution layer (also called up- or deconvolution), a concatenate layer for the corresponding cropped encoder feature map, and two convolution layers. Again, each layer is followed by a leaky ReLU with a leakage factor of 0.1. The final layer consists of a 1 × 1 convolution with a softmax activation function. The resulting (pseudo-) probabilities allow a comparison to the target segmentation mask using cross-entropy on pixel level. Unless indicated differently, we used a kernel size of 3 × 3. To allow faster convergence during training, we included batch normalization layers (<xref ref-type="bibr" rid="bib32">Ioffe and Szegedy, 2015</xref>) after all (up-) convolutions below the first level. By this, an unnormalized path from the input features to the output is remaining to account for absolute input values, for example, the brightness of fluorescent labels.</p></sec><sec id="s4-9-2"><title>Weighted soft-max cross-entropy loss</title><p>Fluorescent microscopy images typically exhibit more background than fluorescent features of interest. To control the impacts of the resulting class imbalance, we implemented a pixel-weighted softmax cross-entropy loss. Thus, we compute the loss from the raw score (logits) of the last 1 × 1 convolution without applying the softmax. As proposed by <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref> we define the weighted cross entropy loss for an input image <italic>I</italic> as<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>wce</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>x</italic> is a pixel in image domain <inline-formula><mml:math id="inf94"><mml:mi mathvariant="normal">Ω</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the pixel-wise weight map, <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> the target segmentation mask, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> the predicted score for class <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <italic>K</italic> the number of classes (<inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for binary classification). Consequently, <inline-formula><mml:math id="inf100"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted score for the target class <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at position <italic>x</italic>.</p><p>Similar to <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref> we compose the weight map <italic>w</italic> from two different weight maps <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The former allows to mitigate the class imbalances by decreasing the weight of background pixels by the factor <inline-formula><mml:math id="inf104"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mtext>bal</mml:mtext></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We add a smoothly decreasing Gaussian function at the edges of the foreground objects accordingly and define<disp-formula id="equ12"><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="center center" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mtext>bal</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>bal </mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the distance to the closest foreground object and <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>bal </mml:mtext></mml:msub></mml:math></inline-formula> the standard deviation of the Gaussian function.</p><p>By definition, semantic segmentation performs a pixel-wise classification and is unaware of different object instances (ROIs). Following <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>, we the force learning of the different instances by increasing the weight of the separating ridges. We estimate the width of a ridge by adding <italic>d</italic><sub>1</sub> (distance to nearest ROI) and <italic>d</italic><sub>2</sub> (distance to second nearest ROI) at each pixel. We define<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>σ</mml:mi><mml:mi>sep</mml:mi></mml:msub></mml:math></inline-formula> defines the standard deviation of the decreasing Gaussian function. The final weight map is given by <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>w</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mtext>bal</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>sep</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> allows to control the focus on instance separation. We used the following parameter set in our experiments: <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf111"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mtext>bal</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mtext>sep</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-9-3"><title>Tile sampling and augmentation</title><p>Given limited training data availability, we leveraged effective data augmentation techniques for biomedical images as proposed by <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>. These comprise transformations and elastic deformations by means of a random deformation field. To become invariant to the input sizes (image shapes), we leveraged the overlap tile strategy introduced by <xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>. Thus, images of any size can be processed. Both data augmentation and overlap tile strategy were adopted from a Tensorflow implementation of <xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>. We used an input tile shape of 540 × 540 × 1 (height x width x channels) and a corresponding output tile shape of 356 × 356 × 1 for all our experiments.</p></sec><sec id="s4-9-4"><title>Training, evaluation, and model selection</title><p>We trained, evaluated and selected all deep learning models for our different strategies – <italic>expert models</italic>, <italic>consensus models</italic>, <italic>consenus ensembles</italic> – following the same steps:</p><list list-type="order"><list-item><p>Determining an appropriate learning rate using the <italic>learning rate finder</italic> (<xref ref-type="bibr" rid="bib67">Smith, 2018</xref>)</p></list-item><list-item><p>Splitting the data into train and validation set (random stratified sampling)</p></list-item><list-item><p>Training the model on the train set according to the <italic>fit-one-cycle</italic> policy of <xref ref-type="bibr" rid="bib67">Smith, 2018</xref></p></list-item><list-item><p>Selecting the model with the highest <inline-formula><mml:math id="inf114"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> median on the validation set (post-hoc evaluation).</p></list-item></list><p>We used the annotations from individual experts to train the <italic>expert models</italic> and the consensus annotations (est. GT) for the <italic>consensus models</italic> and <italic>consensus ensembles</italic>. The post-hoc evaluation on the validation set was performed using the saved model weights (checkpoints) from each epoch. For the similarity analysis, we converted the model output (pixel-wise softmax score) to a segmentation mask by assigning each pixel to the class with the highest softmax score. For the <italic>consensus ensemble</italic> approach, we repeated the steps above according to the principle of a <italic>k-fold cross-validation</italic>. We ensembled the resulting <italic>k</italic> models by averaging the softmax predictions.</p><p>Our initial experimental results have indicated that an adequately trained DL-model performs on par with a human expert. However, insufficient training data may impair the model performance. As there were only five annotated training images for the external laboratories (<italic>Lab-Mue, Lab-Inns1, Lab-Inns2</italic>, and <italic>Lab-Wue2</italic>), we additionally defined a model selection criterion to establish trust in our consensus ensemble approaches: A selected consensus model must at least match the performance of the ‘worst’ human expert for each validation image (measured as the <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>M</mml:mi><mml:mtext>F1 score</mml:mtext></mml:msub></mml:math></inline-formula> to the estimated ground truth). This selection criterion serves as a lower bound for individual model performance. All consensus models trained for <italic>Lab-Wue1</italic> met this criterion. For the other laboratories, we have included the model selection results in <xref ref-type="supplementary-material" rid="fig5sdata3">Figure 5—source data 3</xref>. In those cases where the criterion discarded models, the issue was typically due to a validation image being very different from the training data for a given train-validation split. This issue was often resolved when pretrained model weights were used. For the <italic>frozen</italic> approach (see 7.10.5 Transfer learning) the models never met the selection criterion. Yet, we decided to retain these models to facilitate a comparison among the different approaches. We also indicated that these models and ensembles should be considered with caution and did not use them for further biological analyses.</p><p>We trained all models on a NVIDIA GeForce GTX 1080 TI with 11 GB GDDR5X RAM using the Adam optimizer (<xref ref-type="bibr" rid="bib36">Kingma and Ba, 2014</xref>) and a mini-batch size of four. If not indicated differently, the initial weights were drawn from a truncated normal distribution (<xref ref-type="bibr" rid="bib27">He et al., 2015</xref>). We chose the appropriate maximum learning rates according to the learning rate finder (step two). For <italic>Lab-Wue1</italic> we selected a maximum learning rate of 4e-4 and a minimum learning rate of 4e-5 over a training cycle length of 972 iterations within <inline-formula><mml:math id="inf116"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> validation splits. For <italic>Lab-Mue, Lab-Inns1, Lab-Inns2</italic> and <italic>Lab-Wue2</italic> we chose a maximum learning rate of 1e-4 and a minimum learning rate of 1e-5 over a training cycle length of 972 iterations within <inline-formula><mml:math id="inf117"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> validation splits.</p></sec><sec id="s4-9-5"><title>7.10.5 Transfer learning</title><p>To implement transfer learning we adapted the training procedure from above. For the <italic>fine-tuning</italic> approach, we initialized the weights from the <italic>consensus models</italic> of <italic>Lab-Wue1</italic> and performed all steps for model training, evaluation and selection. For the <italic>frozen</italic> approach, we also initialized the weights from the <italic>consensus models</italic> of <italic>Lab-Wue1</italic> but skipped steps two (finding a learning rate) and three (model training). Hence, we did not adjust the model weights to the new training data. Hardware and training hyperparameters remained unchanged.</p></sec></sec><sec id="s4-10"><title>Quantification of fluorescent features</title><p>Fluorescent features were analyzed on base of the binary segmentation masks derived from the output of DL models or model ensembles, or counted manually by lab-specific experts. In order to compare the number of fluorescent features across images, we normalized in each image the number of annotated fluorescent features to the area of the analyzed region (e.g. the number of cFOS-positive features per NeuN-positive area for <italic>Lab-Wue1</italic>). For one set of experiment, we pooled this data for each condition (e.g. H, C- and C+ for <italic>Lab-Wue1</italic>) and the analyzed brain region (e.g. whole DG, infrapyramidal DG, suprapyramidal DG, CA3, or CA1 for <italic>Lab-Wue1</italic>). To compare different sets of experiments with each other, we normalized all relative fluorescent feature counts to the mean value of the respective control group (e.g. H for <italic>Lab-Wue1</italic>).</p><p>The mean signal intensity for each image was calculated as the mean signal intensity of all ROIs annotated within the analyzed NeuN-positive region (only performed for <italic>Lab-Wue1</italic>). Subsequent pooling steps were identical as described above for the count of fluorescent features.</p></sec><sec id="s4-11"><title>Statistical analysis</title><p>All statistical analyses were performed using Python (version 3.7.3), SciPy (version 1.4.1), and Pingouin (version 0.3.4). We provide all source datasets and source codes in <italic>Jupyter Notebooks</italic> (see 7.13 Data and software availability). In box plots, the area of the box represents the interquartile range (IQR, 1st to 3rd quartile) and whiskers extend to the maximal or minimal values, but no longer than 1.5 × IQR.</p><sec id="s4-11-1"><title>7.12.1 Statistical analysis of fluorescent feature quantifications</title><p>All DL-based quantifications of fluorescent features were tested for significant outliers (Grubb’s test). If an image was detected as significant outlier in several DL-based quantification results, it was visually inspected by an expert and excluded from the analysis if abnormalities (e.g. clusters of fluorescent particles or folding of the tissue) were detected. Throughout all bioimage analyses, N represents the number of investigated animals and n the number of analyzed images. Normality (Shapiro-Wilk) and homogeneity of variance (Levene's) were tested for all DL-based quantification results. For comparison of multiple quantifications of the same image dataset, non-parametric statistical tests were applied to all bioimage analyses. This ensured comparability of the results. To compare two experimental conditions (<italic>Lab-Mue, Lab-Inns1,</italic> and <italic>Lab-Wue2</italic>), Mann-Whitney-U tests were used. In case of three experimental conditions (<italic>Lab-Wue1</italic> and <italic>Lab-Inns2</italic>), Kruskal-Wallis-ANOVA followed by Mann-Whitney-U tests with Bonferroni correction for multiple comparisons was applied.</p></sec><sec id="s4-11-2"><title>7.12.2 Effect size calculation</title><p>Effect sizes (<inline-formula><mml:math id="inf118"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) were calculated for each pairwise comparison. First, the Z-statistic (<italic>Z</italic>) was calculated from the U-statistic (<italic>U</italic>) of the Mann-Whitney-U test as:<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>U</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>12</mml:mn></mml:mfrac></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>n</italic><sub>1</sub> and <italic>n</italic><sub>2</sub> are the numbers of analyzed images of the two compared groups, group 1 and group 2, respectively. Following <xref ref-type="bibr" rid="bib59">Rosenthal and DiMatteo, 2002</xref>, <inline-formula><mml:math id="inf119"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> was calculated as:<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Furthermore, the three critical values of <inline-formula><mml:math id="inf120"><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> that mark the borders between the four significance levels (e.g. for p = 0.05, p = 0.01, and p = 0.001 for a pairwise comparison without Bonferroni correction for multiple comparisons) were calculated from the chi-square distribution.</p></sec><sec id="s4-11-3"><title>All other statistical analyses</title><p>Data was tested for normal distribution (Shapiro-Wilk) and homoscedasticity (Levene's) and parametric or non-parametric tests were used accordingly, as reported in the figure legends (parametric: one-way ANOVA, followed by T-tests (or Welchs T-test for unequal sample sized) with Bonferroni correction for multiple comparisons; non-parametric: Kruskal-Wallis ANOVA, followed by Mann-Whitney tests with Bonferroni correction for multiple comparisons).</p></sec></sec><sec id="s4-12"><title>Data and software availability</title><sec id="s4-12-1"><title>Data</title><p>We provide the full bioimage datasets of <italic>Lab-Wue1</italic> and <italic>Lab-Mue</italic>, including microscopy images, segmentation masks of all DL models and ensembles, and annotations of analyzed regions of interest. For all five bioimage datasets, we provide the quantification results of the bioimage analyses for all models and ensembles as source data. Likewise, the results of the behavioral analysis of <italic>Lab-Wue1</italic> are available as source data. As indicated in the respective figure legends, we also provide all statistical data in full detail as source data files. Furthermore, we provide all training and test datasets that were created in the course of this study. These include all microscopy images with the corresponding manual expert annotations and estimated ground-truth annotations. As part of our proposed pipeline, we share one trained and validated consensus ensembles for each bioimage dataset within our open-source model library. All data and code can be accessed at our Dryad repository (<ext-link ext-link-type="uri" xlink:href="http://www.doi.org/10.5061/dryad.4b8gtht9d">www.doi.org/10.5061/dryad.4b8gtht9d</ext-link>). The source code is also available in our GitHub repository (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/matjesg/bioimage_analysis">www.github.com/matjesg/bioimage_analysis</ext-link>; <xref ref-type="bibr" rid="bib63">Segebarth, 2020</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3667709e0895395bf0cc1ed2a35be46c62b9af05;origin=https://github.com/matjesg/bioimage_analysis;visit=swh:1:snp:2639ba530951fd190f0f29f904e7f6395fe7b9af;anchor=swh:1:rev:eafeb5f8e1312ab29416144df0212761ddf4cfc4/">swh:1:rev:eafeb5f8e1312ab29416144df0212761ddf4cfc4</ext-link>).</p></sec><sec id="s4-12-2"><title>Software</title><p>We provide all source code within python modules and Jupyter Notebooks in our Dryad (<ext-link ext-link-type="uri" xlink:href="http://www.doi.org/10.5061/dryad.4b8gtht9d">www.doi.org/10.5061/dryad.4b8gtht9d</ext-link>) and in our GitHub repository (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/matjesg/bioimage_analysis">www.github.com/matjesg/bioimage_analysis</ext-link>). This includes the code for the bioimage analyses, all statistical analyses, and our proposed pipeline to create, use, and share consensus ensembles for fluorescent feature annotations.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank all members of our collaborative research center (SFB-TRR58) for all the fruitful discussions and the continuous support over the course of this project. We thank Toni Greif for critically reviewing the mathematical content. We thank Thorsten Falk for providing us with the code for data preprocessing and augmentation. We thank Friederike Griebel for her valuable advice on the design of our central figures.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Validation, Visualization, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con4"><p>Data curation, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Data curation, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Data curation, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Data curation, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Data curation, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Data curation, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con10"><p>Data curation, Validation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con11"><p>Software, Writing - review and editing</p></fn><fn fn-type="con" id="con12"><p>Data curation, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con13"><p>Data curation, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con14"><p>Resources, Data curation, Funding acquisition, Validation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con15"><p>Conceptualization, Resources, Data curation, Funding acquisition, Validation, Investigation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con16"><p>Resources, Funding acquisition, Validation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con17"><p>Resources, Funding acquisition, Validation, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con18"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con19"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con20"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: All effort was taken to minimize the number of animals used and their suffering. Lab-Wue1: All experiments with C57BL/6J wildtype mice were in accordance with the Guidelines set by the European Union and approved by our institutional Animal Care, the Utilization Committee and the Regierung von Unterfranken, Würzburg, Germany (License number: 55.2-2531.01-95/13). C57BL/6J wildtype mice were bred in the animal facility of the Institute of Clinical Neurobiology, University Hospital of Würzburg, Germany. Lab Mue: All animal experiments with male C57Bl/6J mice (Charles River, Sulzfeld, Germany) were carried out in accordance with European regulations on animal experimentation and protocols were approved by the local authorities (Landesamt für Natur, Umwelt und Verbraucherschutz Nordrhein-Westfalen). Lab-Inns1: Experiments were performed in adult, male C57Bl/6NCrl mice (Charles River, Sulzfeld, Germany). They were bred in the Department of Pharmacology at the Medical University Innsbruck, Austria. All procedures involving animals and animal care were conducted in accordance with international laws and policies (Directive 2010/63/EU of the European parliament and of the council of 22 September 2010 on the protection of animals used for scientific purposes; Guide for the Care and Use of Laboratory Animals, U.S. National Research Council, 2011) and were approved by the Austrian Ministry of Science. Lab-Inns2: Male 129S1/SvImJ (S1) mice (Charles River, Sulzfeld, Germany) were used for experimentation. The Austrian Animal Experimentation Ethics Board (Bundesministerium für Wissenschaft Forschung und Wirtschaft, Kommission für Tierversuchsangelegenheiten) approved all experimental procedures.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-59780-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The official repository of our study &quot;On the objectivity, reliability, and validity of deep learning enabled bioimage analyses&quot; can be found at Dryad (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.4b8gtht9d">https://doi.org/10.5061/dryad.4b8gtht9d</ext-link>). In addition, we also provide all code in our GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/matjesg/bioimage_analysis">https://github.com/matjesg/bioimage_analysis</ext-link>). A copy is archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:eafeb5f8e1312ab29416144df0212761ddf4cfc4/">https://archive.softwareheritage.org/swh:1:rev:eafeb5f8e1312ab29416144df0212761ddf4cfc4/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Segebarth</surname><given-names>D</given-names></name><name><surname>Griebel</surname><given-names>M</given-names></name><name><surname>Stein</surname><given-names>N</given-names></name><name><surname>Martin</surname><given-names>C</given-names></name><name><surname>Fiedler</surname><given-names>D</given-names></name><name><surname>Comeras</surname><given-names>LB</given-names></name><name><surname>Sah</surname><given-names>A</given-names></name><name><surname>Schoeffler</surname><given-names>V</given-names></name><name><surname>Lüffe</surname><given-names>T</given-names></name><name><surname>Dürr</surname><given-names>A</given-names></name><name><surname>Gupta</surname><given-names>R</given-names></name><name><surname>Sasi</surname><given-names>M</given-names></name><name><surname>Lillesaar</surname><given-names>C</given-names></name><name><surname>Lange</surname><given-names>MD</given-names></name><name><surname>Tasan</surname><given-names>RO</given-names></name><name><surname>Singewald</surname><given-names>N</given-names></name><name><surname>Pape</surname><given-names>HC</given-names></name><name><surname>Flath</surname><given-names>CM</given-names></name><name><surname>Blum</surname><given-names>R</given-names></name><collab>von Collenberg CR</collab></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: On the objectivity, reliability, and validity of deep learning enabled bioimage analyses</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.4b8gtht9d</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname> <given-names>M</given-names></name><name><surname>Agarwal</surname> <given-names>A</given-names></name><name><surname>Barham</surname> <given-names>P</given-names></name><name><surname>Brevdo</surname> <given-names>E</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Citro</surname> <given-names>C</given-names></name><name><surname>Corrado</surname> <given-names>GS</given-names></name><name><surname>Davis</surname> <given-names>A</given-names></name><name><surname>Dean</surname> <given-names>J</given-names></name><name><surname>Devin</surname> <given-names>M</given-names></name><name><surname>Ghemawat</surname> <given-names>S</given-names></name><name><surname>Goodfellow</surname> <given-names>I</given-names></name><name><surname>Harp</surname> <given-names>A</given-names></name><name><surname>Irving</surname> <given-names>G</given-names></name><name><surname>Isard</surname> <given-names>M</given-names></name><name><surname>Jia</surname> <given-names>Y</given-names></name><name><surname>Jozefowicz</surname> <given-names>R</given-names></name><name><surname>Kaiser</surname> <given-names>L</given-names></name><name><surname>Kudlur</surname> <given-names>M</given-names></name><name><surname>Levenberg</surname> <given-names>J</given-names></name><name><surname>Mane</surname> <given-names>D</given-names></name><name><surname>Monga</surname> <given-names>R</given-names></name><name><surname>Moore</surname> <given-names>S</given-names></name><name><surname>Murray</surname> <given-names>D</given-names></name><name><surname>Olah</surname> <given-names>C</given-names></name><name><surname>Schuster</surname> <given-names>M</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Steiner</surname> <given-names>B</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Talwar</surname> <given-names>K</given-names></name><name><surname>Tucker</surname> <given-names>P</given-names></name><name><surname>Vanhoucke</surname> <given-names>V</given-names></name><name><surname>Vasudevan</surname> <given-names>V</given-names></name><name><surname>Viegas</surname> <given-names>F</given-names></name><name><surname>Vinyals</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>P</given-names></name><name><surname>Wattenberg</surname> <given-names>M</given-names></name><name><surname>Wicke</surname> <given-names>M</given-names></name><name><surname>Yu</surname> <given-names>Y</given-names></name><name><surname>Zheng</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorflow: large-scale machine learning on heterogeneous distributed systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reproducibility crisis?</article-title><source>Nature</source><volume>533</volume><fpage>353</fpage><lpage>366</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bayramoglu</surname> <given-names>N</given-names></name><name><surname>Heikkilä</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Transfer learning for cell nuclei classification in histopathology images</article-title><conf-name>European Conference on Computer Vision</conf-name><fpage>532</fpage><lpage>539</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caicedo</surname> <given-names>JC</given-names></name><name><surname>Goodman</surname> <given-names>A</given-names></name><name><surname>Karhohs</surname> <given-names>KW</given-names></name><name><surname>Cimini</surname> <given-names>BA</given-names></name><name><surname>Ackerman</surname> <given-names>J</given-names></name><name><surname>Haghighi</surname> <given-names>M</given-names></name><name><surname>Heng</surname> <given-names>C</given-names></name><name><surname>Becker</surname> <given-names>T</given-names></name><name><surname>Doan</surname> <given-names>M</given-names></name><name><surname>McQuin</surname> <given-names>C</given-names></name><name><surname>Rohban</surname> <given-names>M</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>Carpenter</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Nucleus segmentation across imaging experiments: the 2018 data science bowl</article-title><source>Nature Methods</source><volume>16</volume><fpage>1247</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0612-7</pub-id><pub-id pub-id-type="pmid">31636459</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campeau</surname> <given-names>S</given-names></name><name><surname>Falls</surname> <given-names>WA</given-names></name><name><surname>Cullinan</surname> <given-names>WE</given-names></name><name><surname>Helmreich</surname> <given-names>DL</given-names></name><name><surname>Davis</surname> <given-names>M</given-names></name><name><surname>Watson</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Elicitation and reduction of fear: behavioural and neuroendocrine indices and brain induction of the immediate-early gene c-fos</article-title><source>Neuroscience</source><volume>78</volume><fpage>1087</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1016/S0306-4522(96)00632-X</pub-id><pub-id pub-id-type="pmid">9174076</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chauveau</surname> <given-names>F</given-names></name><name><surname>Lange</surname> <given-names>MD</given-names></name><name><surname>Jüngling</surname> <given-names>K</given-names></name><name><surname>Lesting</surname> <given-names>J</given-names></name><name><surname>Seidenbecher</surname> <given-names>T</given-names></name><name><surname>Pape</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prevention of stress-impaired fear extinction through neuropeptide s action in the lateral amygdala</article-title><source>Neuropsychopharmacology</source><volume>37</volume><fpage>1588</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1038/npp.2012.3</pub-id><pub-id pub-id-type="pmid">22298122</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Dallmeier-Tiessen</surname> <given-names>S</given-names></name><name><surname>Dasler</surname> <given-names>R</given-names></name><name><surname>Feger</surname> <given-names>S</given-names></name><name><surname>Fokianos</surname> <given-names>P</given-names></name><name><surname>Gonzalez</surname> <given-names>JB</given-names></name><name><surname>Hirvonsalo</surname> <given-names>H</given-names></name><name><surname>Kousidis</surname> <given-names>D</given-names></name><name><surname>Lavasa</surname> <given-names>A</given-names></name><name><surname>Mele</surname> <given-names>S</given-names></name><name><surname>Rodriguez</surname> <given-names>DR</given-names></name><name><surname>Šimko</surname> <given-names>T</given-names></name><name><surname>Smith</surname> <given-names>T</given-names></name><name><surname>Trisovic</surname> <given-names>A</given-names></name><name><surname>Trzcinska</surname> <given-names>A</given-names></name><name><surname>Tsanaktsidis</surname> <given-names>I</given-names></name><name><surname>Zimmermann</surname> <given-names>M</given-names></name><name><surname>Cranmer</surname> <given-names>K</given-names></name><name><surname>Heinrich</surname> <given-names>L</given-names></name><name><surname>Watts</surname> <given-names>G</given-names></name><name><surname>Hildreth</surname> <given-names>M</given-names></name><name><surname>Lloret Iglesias</surname> <given-names>L</given-names></name><name><surname>Lassila-Perini</surname> <given-names>K</given-names></name><name><surname>Neubert</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Open is not enough</article-title><source>Nature Physics</source><volume>15</volume><fpage>113</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1038/s41567-018-0342-2</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Chollet</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Keras</article-title><ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link><date-in-citation iso-8601-date="2020-11-19">November 19, 2020</date-in-citation></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christiansen</surname> <given-names>EM</given-names></name><name><surname>Yang</surname> <given-names>SJ</given-names></name><name><surname>Ando</surname> <given-names>DM</given-names></name><name><surname>Javaherian</surname> <given-names>A</given-names></name><name><surname>Skibinski</surname> <given-names>G</given-names></name><name><surname>Lipnick</surname> <given-names>S</given-names></name><name><surname>Mount</surname> <given-names>E</given-names></name><name><surname>O'Neil</surname> <given-names>A</given-names></name><name><surname>Shah</surname> <given-names>K</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Goyal</surname> <given-names>P</given-names></name><name><surname>Fedus</surname> <given-names>W</given-names></name><name><surname>Poplin</surname> <given-names>R</given-names></name><name><surname>Esteva</surname> <given-names>A</given-names></name><name><surname>Berndl</surname> <given-names>M</given-names></name><name><surname>Rubin</surname> <given-names>LL</given-names></name><name><surname>Nelson</surname> <given-names>P</given-names></name><name><surname>Finkbeiner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>In Silico labeling: predicting fluorescent labels in unlabeled images</article-title><source>Cell</source><volume>173</volume><fpage>792</fpage><lpage>803</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.03.040</pub-id><pub-id pub-id-type="pmid">29656897</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cleveland</surname> <given-names>WS</given-names></name><name><surname>McGill</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Graphical perception and graphical methods for analyzing scientific data</article-title><source>Science</source><volume>229</volume><fpage>828</fpage><lpage>833</lpage><pub-id pub-id-type="doi">10.1126/science.229.4716.828</pub-id><pub-id pub-id-type="pmid">17777913</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collier</surname> <given-names>DC</given-names></name><name><surname>Burnett</surname> <given-names>SS</given-names></name><name><surname>Amin</surname> <given-names>M</given-names></name><name><surname>Bilton</surname> <given-names>S</given-names></name><name><surname>Brooks</surname> <given-names>C</given-names></name><name><surname>Ryan</surname> <given-names>A</given-names></name><name><surname>Roniger</surname> <given-names>D</given-names></name><name><surname>Tran</surname> <given-names>D</given-names></name><name><surname>Starkschall</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Assessment of consistency in contouring of normal-tissue anatomic structures</article-title><source>Journal of Applied Clinical Medical Physics</source><volume>4</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1120/jacmp.v4i1.2538</pub-id><pub-id pub-id-type="pmid">12540815</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dietterich</surname> <given-names>TG</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Ensemble methods in machine learning</article-title><conf-name> International Workshop on Multiple Classifier Systems</conf-name><fpage>1</fpage><lpage>15</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname> <given-names>MK</given-names></name><name><surname>Welf</surname> <given-names>ES</given-names></name><name><surname>Jamieson</surname> <given-names>AR</given-names></name><name><surname>Dean</surname> <given-names>KM</given-names></name><name><surname>Isogai</surname> <given-names>T</given-names></name><name><surname>Fiolka</surname> <given-names>R</given-names></name><name><surname>Danuser</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Robust and automated detection of subcellular morphological motifs in 3D microscopy images</article-title><source>Nature Methods</source><volume>16</volume><fpage>1037</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0539-z</pub-id><pub-id pub-id-type="pmid">31501548</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname> <given-names>T</given-names></name><name><surname>Mai</surname> <given-names>D</given-names></name><name><surname>Bensch</surname> <given-names>R</given-names></name><name><surname>Çiçek</surname> <given-names>Ö</given-names></name><name><surname>Abdulkadir</surname> <given-names>A</given-names></name><name><surname>Marrakchi</surname> <given-names>Y</given-names></name><name><surname>Böhm</surname> <given-names>A</given-names></name><name><surname>Deubner</surname> <given-names>J</given-names></name><name><surname>Jäckel</surname> <given-names>Z</given-names></name><name><surname>Seiwald</surname> <given-names>K</given-names></name><name><surname>Dovzhenko</surname> <given-names>A</given-names></name><name><surname>Tietz</surname> <given-names>O</given-names></name><name><surname>Dal Bosco</surname> <given-names>C</given-names></name><name><surname>Walsh</surname> <given-names>S</given-names></name><name><surname>Saltukoglu</surname> <given-names>D</given-names></name><name><surname>Tay</surname> <given-names>TL</given-names></name><name><surname>Prinz</surname> <given-names>M</given-names></name><name><surname>Palme</surname> <given-names>K</given-names></name><name><surname>Simons</surname> <given-names>M</given-names></name><name><surname>Diester</surname> <given-names>I</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name><name><surname>Ronneberger</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title><source>Nature Methods</source><volume>16</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id><pub-id pub-id-type="pmid">30559429</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Opinion: is science really facing a reproducibility crisis, and do we need it to?</article-title><source>PNAS</source><volume>115</volume><fpage>2628</fpage><lpage>2631</lpage><pub-id pub-id-type="doi">10.1073/pnas.1708272114</pub-id><pub-id pub-id-type="pmid">29531051</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanselow</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Conditioned and unconditional components of post-shock freezing</article-title><source>The Pavlovian Journal of Biological Science</source><volume>15</volume><fpage>177</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1007/BF03001163</pub-id><pub-id pub-id-type="pmid">7208128</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman-Stewart</surname> <given-names>D</given-names></name><name><surname>Kocovski</surname> <given-names>N</given-names></name><name><surname>McConnell</surname> <given-names>BA</given-names></name><name><surname>Brundage</surname> <given-names>MD</given-names></name><name><surname>Mackillop</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Perception of quantitative information for treatment decisions</article-title><source>Medical Decision Making</source><volume>20</volume><fpage>228</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1177/0272989X0002000208</pub-id><pub-id pub-id-type="pmid">10772360</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzgerald</surname> <given-names>PJ</given-names></name><name><surname>Whittle</surname> <given-names>N</given-names></name><name><surname>Flynn</surname> <given-names>SM</given-names></name><name><surname>Graybeal</surname> <given-names>C</given-names></name><name><surname>Pinard</surname> <given-names>CR</given-names></name><name><surname>Gunduz-Cinar</surname> <given-names>O</given-names></name><name><surname>Kravitz</surname> <given-names>AV</given-names></name><name><surname>Singewald</surname> <given-names>N</given-names></name><name><surname>Holmes</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Prefrontal single-unit firing associated with deficient extinction in mice</article-title><source>Neurobiology of Learning and Memory</source><volume>113</volume><fpage>69</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2013.11.002</pub-id><pub-id pub-id-type="pmid">24231425</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleiss</surname> <given-names>JL</given-names></name><name><surname>Cohen</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability</article-title><source>Educational and Psychological Measurement</source><volume>33</volume><fpage>613</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1177/001316447303300309</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frambach</surname> <given-names>JM</given-names></name><name><surname>van der Vleuten</surname> <given-names>CP</given-names></name><name><surname>Durning</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Am last page: quality criteria in qualitative and quantitative research</article-title><source>Academic Medicine : Journal of the Association of American Medical Colleges</source><volume>88</volume><elocation-id>552</elocation-id><pub-id pub-id-type="doi">10.1097/ACM.0b013e31828abf7f</pub-id><pub-id pub-id-type="pmid">23531762</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Franklin</surname> <given-names>KBJ</given-names></name><name><surname>Paxinos</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>The Mouse Brain in Stereotaxic Coordinates</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallo</surname> <given-names>FT</given-names></name><name><surname>Katche</surname> <given-names>C</given-names></name><name><surname>Morici</surname> <given-names>JF</given-names></name><name><surname>Medina</surname> <given-names>JH</given-names></name><name><surname>Weisstaub</surname> <given-names>NV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Immediate early genes, memory and psychiatric disorders: focus on c-Fos, Egr1 and arc</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>12</volume><elocation-id>79</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00079</pub-id><pub-id pub-id-type="pmid">29755331</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruene</surname> <given-names>TM</given-names></name><name><surname>Flick</surname> <given-names>K</given-names></name><name><surname>Stefano</surname> <given-names>A</given-names></name><name><surname>Shea</surname> <given-names>SD</given-names></name><name><surname>Shansky</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sexually divergent expression of active and passive conditioned fear responses in rats</article-title><source>eLife</source><volume>4</volume><elocation-id>e11352</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11352</pub-id><pub-id pub-id-type="pmid">26568307</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Guan</surname> <given-names>MY</given-names></name><name><surname>Gulshan</surname> <given-names>V</given-names></name><name><surname>Dai</surname> <given-names>AM</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Who said what: modeling individual labelers improves classification</article-title><conf-name>Thirty-Second AAAI Conference on Artificial Intelligence</conf-name><fpage>3109</fpage><lpage>3118</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzowski</surname> <given-names>JF</given-names></name><name><surname>Setlow</surname> <given-names>B</given-names></name><name><surname>Wagner</surname> <given-names>EK</given-names></name><name><surname>McGaugh</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Experience-dependent gene expression in the rat Hippocampus after spatial learning: a comparison of the immediate-early genes arc, c-<italic>fos</italic>, and <italic>zif268</italic></article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>5089</fpage><lpage>5098</lpage><pub-id pub-id-type="pmid">11438584</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haberl</surname> <given-names>MG</given-names></name><name><surname>Churas</surname> <given-names>C</given-names></name><name><surname>Tindall</surname> <given-names>L</given-names></name><name><surname>Boassa</surname> <given-names>D</given-names></name><name><surname>Phan</surname> <given-names>S</given-names></name><name><surname>Bushong</surname> <given-names>EA</given-names></name><name><surname>Madany</surname> <given-names>M</given-names></name><name><surname>Akay</surname> <given-names>R</given-names></name><name><surname>Deerinck</surname> <given-names>TJ</given-names></name><name><surname>Peltier</surname> <given-names>ST</given-names></name><name><surname>Ellisman</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CDeep3M-Plug-and-Play cloud-based deep learning for image segmentation</article-title><source>Nature Methods</source><volume>15</volume><fpage>677</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0106-z</pub-id><pub-id pub-id-type="pmid">30171236</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delving deep into rectifiers: surpassing human-level performance on imagenet classification</article-title><conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name><fpage>1026</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Head</surname> <given-names>ML</given-names></name><name><surname>Holman</surname> <given-names>L</given-names></name><name><surname>Lanfear</surname> <given-names>R</given-names></name><name><surname>Kahn</surname> <given-names>AT</given-names></name><name><surname>Jennions</surname> <given-names>MD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The extent and consequences of p-hacking in science</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002106</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002106</pub-id><pub-id pub-id-type="pmid">25768323</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huff</surname> <given-names>NC</given-names></name><name><surname>Frank</surname> <given-names>M</given-names></name><name><surname>Wright-Hardesty</surname> <given-names>K</given-names></name><name><surname>Sprunger</surname> <given-names>D</given-names></name><name><surname>Matus-Amat</surname> <given-names>P</given-names></name><name><surname>Higgins</surname> <given-names>E</given-names></name><name><surname>Rudy</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Amygdala regulation of immediate-early gene expression in the Hippocampus induced by contextual fear conditioning</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>1616</fpage><lpage>1623</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4964-05.2006</pub-id><pub-id pub-id-type="pmid">16452685</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hutson</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Artificial intelligence faces reproducibility crisis</article-title><conf-name>American Association for the Advancement of Science</conf-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why most clinical research is not useful</article-title><source>PLOS Medicine</source><volume>13</volume><elocation-id>e1002049</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1002049</pub-id><pub-id pub-id-type="pmid">27328301</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ioffe</surname> <given-names>S</given-names></name><name><surname>Szegedy</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Oliphant</surname> <given-names>T</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>SciPy: Open Source Scientific Tools for Python</source><publisher-name>Technical report</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keiser</surname> <given-names>AA</given-names></name><name><surname>Turnbull</surname> <given-names>LM</given-names></name><name><surname>Darian</surname> <given-names>MA</given-names></name><name><surname>Feldman</surname> <given-names>DE</given-names></name><name><surname>Song</surname> <given-names>I</given-names></name><name><surname>Tronson</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sex differences in context fear generalization and recruitment of Hippocampus and amygdala during retrieval</article-title><source>Neuropsychopharmacology</source><volume>42</volume><fpage>397</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1038/npp.2016.174</pub-id><pub-id pub-id-type="pmid">27577601</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmel</surname> <given-names>CB</given-names></name><name><surname>Ballard</surname> <given-names>WW</given-names></name><name><surname>Kimmel</surname> <given-names>SR</given-names></name><name><surname>Ullmann</surname> <given-names>B</given-names></name><name><surname>Schilling</surname> <given-names>TF</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Stages of embryonic development of the zebrafish</article-title><source>Developmental Dynamics</source><volume>203</volume><fpage>253</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1002/aja.1002030302</pub-id><pub-id pub-id-type="pmid">8589427</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname> <given-names>HW</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>The hungarian method for the assignment problem</article-title><source>Naval Research Logistics Quarterly</source><volume>2</volume><fpage>83</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1002/nav.3800020109</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lakshminarayanan</surname> <given-names>B</given-names></name><name><surname>Pritzel</surname> <given-names>A</given-names></name><name><surname>Blundell</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Simple and scalable predictive uncertainty estimation using deep ensembles</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>6402</fpage><lpage>6413</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lampert</surname> <given-names>TA</given-names></name><name><surname>Stumpf</surname> <given-names>A</given-names></name><name><surname>Gancarski</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An empirical study into annotator agreement, ground truth estimation, and algorithm evaluation</article-title><source>IEEE Transactions on Image Processing</source><volume>25</volume><fpage>2557</fpage><lpage>2572</lpage><pub-id pub-id-type="doi">10.1109/TIP.2016.2544703</pub-id><pub-id pub-id-type="pmid">27019487</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landis</surname> <given-names>JR</given-names></name><name><surname>Koch</surname> <given-names>GG</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>The measurement of observer agreement for categorical data</article-title><source>Biometrics</source><volume>33</volume><fpage>159</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.2307/2529310</pub-id><pub-id pub-id-type="pmid">843571</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeDoux</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Coming to terms with fear</article-title><source>PNAS</source><volume>111</volume><fpage>2871</fpage><lpage>2878</lpage><pub-id pub-id-type="doi">10.1073/pnas.1400335111</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>A</given-names></name><name><surname>Gong</surname> <given-names>H</given-names></name><name><surname>Zhang</surname> <given-names>B</given-names></name><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>Yan</surname> <given-names>C</given-names></name><name><surname>Wu</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>Q</given-names></name><name><surname>Zeng</surname> <given-names>S</given-names></name><name><surname>Luo</surname> <given-names>Q</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Micro-optical sectioning tomography to obtain a high-resolution atlas of the mouse brain</article-title><source>Science</source><volume>330</volume><fpage>1404</fpage><lpage>1408</lpage><pub-id pub-id-type="doi">10.1126/science.1191776</pub-id><pub-id pub-id-type="pmid">21051596</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowekamp</surname> <given-names>BC</given-names></name><name><surname>Chen</surname> <given-names>DT</given-names></name><name><surname>Ibáñez</surname> <given-names>L</given-names></name><name><surname>Blezek</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The design of SimpleITK</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>45</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00045</pub-id><pub-id pub-id-type="pmid">24416015</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maška</surname> <given-names>M</given-names></name><name><surname>Ulman</surname> <given-names>V</given-names></name><name><surname>Svoboda</surname> <given-names>D</given-names></name><name><surname>Matula</surname> <given-names>P</given-names></name><name><surname>Matula</surname> <given-names>P</given-names></name><name><surname>Ederra</surname> <given-names>C</given-names></name><name><surname>Urbiola</surname> <given-names>A</given-names></name><name><surname>España</surname> <given-names>T</given-names></name><name><surname>Venkatesan</surname> <given-names>S</given-names></name><name><surname>Balak</surname> <given-names>DM</given-names></name><name><surname>Karas</surname> <given-names>P</given-names></name><name><surname>Bolcková</surname> <given-names>T</given-names></name><name><surname>Streitová</surname> <given-names>M</given-names></name><name><surname>Carthel</surname> <given-names>C</given-names></name><name><surname>Coraluppi</surname> <given-names>S</given-names></name><name><surname>Harder</surname> <given-names>N</given-names></name><name><surname>Rohr</surname> <given-names>K</given-names></name><name><surname>Magnusson</surname> <given-names>KE</given-names></name><name><surname>Jaldén</surname> <given-names>J</given-names></name><name><surname>Blau</surname> <given-names>HM</given-names></name><name><surname>Dzyubachyk</surname> <given-names>O</given-names></name><name><surname>Křížek</surname> <given-names>P</given-names></name><name><surname>Hagen</surname> <given-names>GM</given-names></name><name><surname>Pastor-Escuredo</surname> <given-names>D</given-names></name><name><surname>Jimenez-Carretero</surname> <given-names>D</given-names></name><name><surname>Ledesma-Carbayo</surname> <given-names>MJ</given-names></name><name><surname>Muñoz-Barrutia</surname> <given-names>A</given-names></name><name><surname>Meijering</surname> <given-names>E</given-names></name><name><surname>Kozubek</surname> <given-names>M</given-names></name><name><surname>Ortiz-de-Solorzano</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A benchmark for comparison of cell tracking algorithms</article-title><source>Bioinformatics</source><volume>30</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu080</pub-id><pub-id pub-id-type="pmid">24526711</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDole</surname> <given-names>K</given-names></name><name><surname>Guignard</surname> <given-names>L</given-names></name><name><surname>Amat</surname> <given-names>F</given-names></name><name><surname>Berger</surname> <given-names>A</given-names></name><name><surname>Malandain</surname> <given-names>G</given-names></name><name><surname>Royer</surname> <given-names>LA</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Branson</surname> <given-names>K</given-names></name><name><surname>Keller</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>In toto imaging and reconstruction of Post-Implantation mouse development at the Single-Cell level</article-title><source>Cell</source><volume>175</volume><fpage>859</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.09.031</pub-id><pub-id pub-id-type="pmid">30318151</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McQuin</surname> <given-names>C</given-names></name><name><surname>Goodman</surname> <given-names>A</given-names></name><name><surname>Chernyshev</surname> <given-names>V</given-names></name><name><surname>Kamentsky</surname> <given-names>L</given-names></name><name><surname>Cimini</surname> <given-names>BA</given-names></name><name><surname>Karhohs</surname> <given-names>KW</given-names></name><name><surname>Doan</surname> <given-names>M</given-names></name><name><surname>Ding</surname> <given-names>L</given-names></name><name><surname>Rafelski</surname> <given-names>SM</given-names></name><name><surname>Thirstrup</surname> <given-names>D</given-names></name><name><surname>Wiegraebe</surname> <given-names>W</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>Becker</surname> <given-names>T</given-names></name><name><surname>Caicedo</surname> <given-names>JC</given-names></name><name><surname>Carpenter</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CellProfiler 3.0: next-generation image processing for biology</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2005970</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2005970</pub-id><pub-id pub-id-type="pmid">29969450</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijering</surname> <given-names>E</given-names></name><name><surname>Carpenter</surname> <given-names>AE</given-names></name><name><surname>Peng</surname> <given-names>H</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name><name><surname>Olivo-Marin</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Imagining the future of bioimage analysis</article-title><source>Nature Biotechnology</source><volume>34</volume><fpage>1250</fpage><lpage>1255</lpage><pub-id pub-id-type="doi">10.1038/nbt.3722</pub-id><pub-id pub-id-type="pmid">27926723</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murawski</surname> <given-names>NJ</given-names></name><name><surname>Klintsova</surname> <given-names>AY</given-names></name><name><surname>Stanton</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neonatal alcohol exposure and the Hippocampus in developing male rats: effects on behaviorally induced CA1 c-Fos expression, CA1 pyramidal cell number, and contextual fear conditioning</article-title><source>Neuroscience</source><volume>206</volume><fpage>89</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2012.01.006</pub-id><pub-id pub-id-type="pmid">22285885</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>CP</given-names></name><name><surname>Li</surname> <given-names>X</given-names></name><name><surname>Maurer</surname> <given-names>V</given-names></name><name><surname>Oberhauser</surname> <given-names>M</given-names></name><name><surname>Gstir</surname> <given-names>R</given-names></name><name><surname>Wearick-Silva</surname> <given-names>LE</given-names></name><name><surname>Viola</surname> <given-names>TW</given-names></name><name><surname>Schafferer</surname> <given-names>S</given-names></name><name><surname>Grassi-Oliveira</surname> <given-names>R</given-names></name><name><surname>Whittle</surname> <given-names>N</given-names></name><name><surname>Hüttenhofer</surname> <given-names>A</given-names></name><name><surname>Bredy</surname> <given-names>TW</given-names></name><name><surname>Singewald</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MicroRNA-Mediated rescue of fear extinction memory by miR-144-3p in Extinction-Impaired mice</article-title><source>Biological Psychiatry</source><volume>81</volume><fpage>979</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2016.12.021</pub-id><pub-id pub-id-type="pmid">28104225</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niedworok</surname> <given-names>CJ</given-names></name><name><surname>Brown</surname> <given-names>AP</given-names></name><name><surname>Jorge Cardoso</surname> <given-names>M</given-names></name><name><surname>Osten</surname> <given-names>P</given-names></name><name><surname>Ourselin</surname> <given-names>S</given-names></name><name><surname>Modat</surname> <given-names>M</given-names></name><name><surname>Margrie</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>aMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11879</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11879</pub-id><pub-id pub-id-type="pmid">27384127</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Oquab</surname> <given-names>M</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name><name><surname>Laptev</surname> <given-names>I</given-names></name><name><surname>Sivic</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning and transferring mid-level image representations using convolutional neural networks</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1717</fpage><lpage>1724</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2014.222</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osten</surname> <given-names>P</given-names></name><name><surname>Margrie</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mapping brain circuitry with a light microscope</article-title><source>Nature Methods</source><volume>10</volume><fpage>515</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2477</pub-id><pub-id pub-id-type="pmid">23722211</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ounkomol</surname> <given-names>C</given-names></name><name><surname>Seshamani</surname> <given-names>S</given-names></name><name><surname>Maleckar</surname> <given-names>MM</given-names></name><name><surname>Collman</surname> <given-names>F</given-names></name><name><surname>Johnson</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy</article-title><source>Nature Methods</source><volume>15</volume><fpage>917</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0111-2</pub-id><pub-id pub-id-type="pmid">30224672</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Michel</surname> <given-names>V</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Grisel</surname> <given-names>O</given-names></name><name><surname>Blondel</surname> <given-names>M</given-names></name><name><surname>Prettenhofer</surname> <given-names>P</given-names></name><name><surname>Weiss</surname> <given-names>R</given-names></name><name><surname>Dubourg</surname> <given-names>V</given-names></name><name><surname>Vanderplas</surname> <given-names>J</given-names></name><name><surname>Passos</surname> <given-names>A</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Brucher</surname> <given-names>M</given-names></name><name><surname>Perrot</surname> <given-names>M</given-names></name><name><surname>Duchesnay</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in Python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perkel</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Why Jupyter is data scientists' computational notebook of choice</article-title><source>Nature</source><volume>563</volume><fpage>145</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1038/d41586-018-07196-1</pub-id><pub-id pub-id-type="pmid">30375502</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramamoorthi</surname> <given-names>K</given-names></name><name><surname>Fropf</surname> <given-names>R</given-names></name><name><surname>Belfort</surname> <given-names>GM</given-names></name><name><surname>Fitzmaurice</surname> <given-names>HL</given-names></name><name><surname>McKinney</surname> <given-names>RM</given-names></name><name><surname>Neve</surname> <given-names>RL</given-names></name><name><surname>Otto</surname> <given-names>T</given-names></name><name><surname>Lin</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Npas4 regulates a transcriptional program in CA3 required for contextual memory formation</article-title><source>Science</source><volume>334</volume><fpage>1669</fpage><lpage>1675</lpage><pub-id pub-id-type="doi">10.1126/science.1208049</pub-id><pub-id pub-id-type="pmid">22194569</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-net: convolutional networks for biomedical image segmentation</article-title><conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosenthal</surname> <given-names>R</given-names></name><name><surname>DiMatteo</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Stevens’ Handbook of Experimental Psychology</source><publisher-name>Wiley</publisher-name><pub-id pub-id-type="doi">10.1002/9781119170174</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rueden</surname> <given-names>CT</given-names></name><name><surname>Schindelin</surname> <given-names>J</given-names></name><name><surname>Hiner</surname> <given-names>MC</given-names></name><name><surname>DeZonia</surname> <given-names>BE</given-names></name><name><surname>Walter</surname> <given-names>AE</given-names></name><name><surname>Arena</surname> <given-names>ET</given-names></name><name><surname>Eliceiri</surname> <given-names>KW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>ImageJ2: imagej for the next generation of scientific image data</article-title><source>BMC Bioinformatics</source><volume>18</volume><elocation-id>529</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-017-1934-z</pub-id><pub-id pub-id-type="pmid">29187165</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitz</surname> <given-names>C</given-names></name><name><surname>Korr</surname> <given-names>H</given-names></name><name><surname>Heinsen</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Design-based counting techniques: the real problems</article-title><source>Trends in Neurosciences</source><volume>22</volume><elocation-id>345</elocation-id><pub-id pub-id-type="doi">10.1016/S0166-2236(99)01418-6</pub-id><pub-id pub-id-type="pmid">10407447</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname> <given-names>CA</given-names></name><name><surname>Rasband</surname> <given-names>WS</given-names></name><name><surname>Eliceiri</surname> <given-names>KW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>NIH image to ImageJ: 25 years of image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>671</fpage><lpage>675</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2089</pub-id><pub-id pub-id-type="pmid">22930834</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Segebarth</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>bioimage_analysis</data-title><source>GitHub</source><version designator="eafeb5f">eafeb5f</version><ext-link ext-link-type="uri" xlink:href="https://github.com/matjesg/bioimage_analysis">https://github.com/matjesg/bioimage_analysis</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sezgin</surname> <given-names>M</given-names></name><name><surname>Sankur</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Survey over image thresholding techniques and quantitative performance evaluation</article-title><source>Journal of Electronic Imaging</source><volume>13</volume><fpage>146</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1117/1.1631315</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuvaev</surname> <given-names>SA</given-names></name><name><surname>Lazutkin</surname> <given-names>AA</given-names></name><name><surname>Kedrov</surname> <given-names>AV</given-names></name><name><surname>Anokhin</surname> <given-names>KV</given-names></name><name><surname>Enikolopov</surname> <given-names>GN</given-names></name><name><surname>Koulakov</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DALMATIAN: an algorithm for automatic cell detection and counting in 3D</article-title><source>Frontiers in Neuroanatomy</source><volume>11</volume><elocation-id>117</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2017.00117</pub-id><pub-id pub-id-type="pmid">29311849</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siebert</surname> <given-names>S</given-names></name><name><surname>Machesky</surname> <given-names>LM</given-names></name><name><surname>Insall</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Overflow in science and its implications for trust</article-title><source>eLife</source><volume>4</volume><elocation-id>e10825</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10825</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>LN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A disciplined approach to neural network hyper-parameters: part 1–learning rate, batch size, momentum, and weight decay</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.09820">https://arxiv.org/abs/1803.09820</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tayler</surname> <given-names>KK</given-names></name><name><surname>Tanaka</surname> <given-names>KZ</given-names></name><name><surname>Reijmers</surname> <given-names>LG</given-names></name><name><surname>Wiltgen</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reactivation of neural ensembles during the retrieval of recent and remote memory</article-title><source>Current Biology</source><volume>23</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.11.019</pub-id><pub-id pub-id-type="pmid">23246402</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>BN</given-names></name><name><surname>Kuyatt</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Guidelines for Evaluating and Expressing the Uncertainty of Nist Measurement Results</source><publisher-name>Technical report, US Department of Commerce, Technology Administration, National Institute of Standards and Technology</publisher-name></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallat</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pingouin: statistics in python</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>1026</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01026</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van De Werd</surname> <given-names>HJ</given-names></name><name><surname>Rajkowska</surname> <given-names>G</given-names></name><name><surname>Evers</surname> <given-names>P</given-names></name><name><surname>Uylings</surname> <given-names>HB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cytoarchitectonic and chemoarchitectonic characterization of the prefrontal cortical Areas in the mouse</article-title><source>Brain Structure and Function</source><volume>214</volume><fpage>339</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1007/s00429-010-0247-z</pub-id><pub-id pub-id-type="pmid">20221886</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname> <given-names>S</given-names></name><name><surname>Schönberger</surname> <given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name><name><surname>Boulogne</surname> <given-names>F</given-names></name><name><surname>Warner</surname> <given-names>JD</given-names></name><name><surname>Yager</surname> <given-names>N</given-names></name><name><surname>Gouillart</surname> <given-names>E</given-names></name><name><surname>Yu</surname> <given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Chamier</surname> <given-names>L</given-names></name><name><surname>Laine</surname> <given-names>RF</given-names></name><name><surname>Henriques</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Artificial intelligence for microscopy: what you should know</article-title><source>Biochemical Society Transactions</source><volume>47</volume><fpage>1029</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1042/BST20180391</pub-id><pub-id pub-id-type="pmid">31366471</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warfield</surname> <given-names>SK</given-names></name><name><surname>Zou</surname> <given-names>KH</given-names></name><name><surname>Wells</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><volume>23</volume><fpage>903</fpage><lpage>921</lpage><pub-id pub-id-type="doi">10.1109/TMI.2004.828354</pub-id><pub-id pub-id-type="pmid">15250643</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittle</surname> <given-names>N</given-names></name><name><surname>Hauschild</surname> <given-names>M</given-names></name><name><surname>Lubec</surname> <given-names>G</given-names></name><name><surname>Holmes</surname> <given-names>A</given-names></name><name><surname>Singewald</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Rescue of impaired fear extinction and normalization of cortico-amygdala circuit dysfunction in a genetic mouse model by dietary zinc restriction</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>13586</fpage><lpage>13596</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0849-10.2010</pub-id><pub-id pub-id-type="pmid">20943900</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittle</surname> <given-names>N</given-names></name><name><surname>Maurer</surname> <given-names>V</given-names></name><name><surname>Murphy</surname> <given-names>C</given-names></name><name><surname>Rainer</surname> <given-names>J</given-names></name><name><surname>Bindreither</surname> <given-names>D</given-names></name><name><surname>Hauschild</surname> <given-names>M</given-names></name><name><surname>Scharinger</surname> <given-names>A</given-names></name><name><surname>Oberhauser</surname> <given-names>M</given-names></name><name><surname>Keil</surname> <given-names>T</given-names></name><name><surname>Brehm</surname> <given-names>C</given-names></name><name><surname>Valovka</surname> <given-names>T</given-names></name><name><surname>Striessnig</surname> <given-names>J</given-names></name><name><surname>Singewald</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Enhancing dopaminergic signaling and histone acetylation promotes long-term rescue of deficient fear extinction</article-title><source>Translational Psychiatry</source><volume>6</volume><elocation-id>e974</elocation-id><pub-id pub-id-type="doi">10.1038/tp.2016.231</pub-id><pub-id pub-id-type="pmid">27922638</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yosinski</surname> <given-names>J</given-names></name><name><surname>Clune</surname> <given-names>J</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Lipson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How transferable are features in deep neural networks?</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>3320</fpage><lpage>3328</lpage></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59780.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Fraser</surname><given-names>Scott E</given-names></name><role>Reviewing Editor</role><aff><institution>University of Southern California</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Ruderman</surname><given-names>Dan</given-names> </name><role>Reviewer</role><aff><institution>USC</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Danuser</surname><given-names>Gaudenz</given-names> </name><role>Reviewer</role><aff><institution>University of Texas Southwestern Medical Center</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>There are three things about this contribution that make it certain to impact the field in a positive fashion. The clear presentation style, the balanced discussion and that careful benchmarking should establish a more objective style of evaluation. Given the speed with which this field is moving and the need for better standards, this should become a well-cited paper.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;DeepFLaSH, a deep learning pipeline for segmentation of fluorescent labels in microscopy images&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>The reviewers and I appreciate the strengths of DeepFLaSH, and the challenges of the important problem you are attacking. We all feel that research employing fluorescence imaging will be dramatically enhanced as optimized machine learning techniques are refined that are tolerant of the particular challenges you outline. However, the current manuscript falls short of documenting the advance represented by DeepFLaSH.</p><p>We feel that any study reporting an advance must fully embrace two key demonstrations:</p><p>1) A complete and balanced comparison of the speed, ease-of-use and accuracy of the new approach (in this case, DeepFLaSH) with a full spectrum of the other tools currently available. The reviewers believe that the comparisons performed in this manuscript are not sufficient in depth or breadth. Comparisons to techniques that would not be employed by a knowledgeable user because they are known to fail (eg. a simple thresholding) only serves to weaken the confidence of the reader in the nature of the advance.</p><p>2) A use of the new approach (in this case, DeepFLaSH) to analyze a biological system and extract a publishable advance.</p><p>A manuscript expanded in this fashion, to perform a relevant bank of tests and to present a novel biological insight, would be high-impact, and would justify publication in <italic>eLife</italic> or other first-tier journals.</p><p><italic>Reviewer #1:</italic></p><p>The paper presents DeepFlaSH, a tool to apply U-Net networks to microscopy images. Although the presented experimental results are interesting, I think that the manuscript has several significant issues that have to be addressed before it can be considered for publication in <italic>eLife</italic>. Most importantly, the focus of the manuscript is unclear, and the presented tool does not seem to be significantly different from existing tools. More detailed comments are given below:</p><p>– It is unclear what the focus of the manuscript is. The title and manuscript type (Tools and Resources) suggest that the focus is on the introduction of the DeepFlaSH tool, but the manuscript itself is much more focused on the specific application of studying contextual fear conditioning. There is little description of the tool, how it can be used, and how it relates to existing tools and resources. Instead, a detailed description of the experiments and their results is given. Presenting these experimental results could be an interesting paper, since the results show that U-Net networks can be successfully applied for fluorescence labeling in real-world experiments, but in that case the title, manuscript type, and focus of the paper should be adjusted to reflect this.</p><p>– Related, since the current focus of the paper is the DeepFlaSH tool, I would expect a relatively mature software tool that is significantly different from existing tools/resources, for example by providing an integrated tool for manual labeling, network training, and application of trained networks to new images. In the current form, however, the DeepFlaSH tool is not much more than a single example Python script that calls a slightly modified U-Net network implemented with Keras. This is not significantly different from code packages that are released with many machine learning papers, and, in fact, there are already many existing packages that provide U-Net-like network implementations (in Keras and other frameworks) with accompanying example scripts. Therefore, I don't really know whether the presented tool is novel enough for a paper in <italic>eLife</italic>, and, in any case, I don't think the authors can claim that the tool is a “unique deep learning pipeline”.</p><p>– The use of Google Colab to offer a cloud-based solution that users can run the code in is useful, but also introduces several potential issues. First, the service could be changed, stopped, and/or monetized at any time by Google, which could limit the DeepFlaSH tool itself. Second, resources of Google Colab servers have to be shared between users, and at a given moment, required resources such as a GPU could be either unavailable or only available in a limited way (several reports can be found online about this). Finally, to be able to use DeepFlaSH in Google Colab, the user has to upload their data to a Google server, which users might not feel comfortable with and/or are unable to do because of privacy concerns. Because of these issues, I am not sure whether the tool can be presented as a “cloud-based virtual notebook with free access to high computing power”, unless a cloud-based solution is made available that is not as dependent on a specific company.</p><p>– Related, the authors state that the tool can be run on a local machine, but the code itself doesn't really support this at the moment (although it is technically possible). No installation instructions are given, no list of dependencies are given, and the code itself currently depends on Google Colab functionality and is not available as an easily installable and callable Python module.</p><p><italic>Reviewer #2:</italic></p><p>In this manuscript Segebarth et al. describe the application of a variant of the U-Net deep learning network to segmentation of cells in histological brain slices. They show segmentation in 2D on two markers, one localizing with nuclei and one showing the activity of a particular signal in the soma. They use standard tricks of data augmentation by image transformation to increase the pool of training data, although it is not documented quantitatively how necessary this is.</p><p>Overall, there is nothing severely wrong with this work, but there is also nothing new or exciting. The authors mention that the segmentation pipeline called DeepFlaSH is accessible through a cloud-based notebook, which offers simple operation. I did not test this. It may be possible that DeepFlaSH is particularly simple to use, and thus a brief application note may be justified. Although, such note would have to better acknowledge the numerous existing software tools addressing particle / nucleus detection, which are integrated in software platforms that are widely accessed by the community and probably are, I quote the authors, “hassle-free” in their use as well. I am not sure whether <italic>eLife</italic> is in the business of documenting random software packages. I am sure that DeepFlasSH serves the authors lab very well, and the tool could be advertised in the methods sections of papers describing the application in a particular project, even as an accessible open-source. It will then be picked up by the community working on similar data. But, DeepFlaSH does not hold up for a standalone paper on histological image segmentation. As a regular author and reviewer I have always seen <italic>eLife</italic> as a journal publishing scientific discoveries or innovative methods that have the promise to lead to such discoveries. I am afraid, the present manuscript does not meet that standard.</p><p>From an image analysis perspective, I would actually challenge the authors with the claim that any filter, potentially for the soma segmentation a multi-scale filter, will do an equal or better segmentation job than DeepFlaSH. The authors anecdotally compare the Deep Learning result to a trivial thresholding method. This is inappropriate. No one with a bit of understanding of signal processing expects a simple threshold to work on the data presented in Figure 1. But, there is an extended literature on particle detection that relies on linear or non-linear amplification of bona fide particle signals followed by, in the more advanced approaches, statistics-based foreground-background segregation. The advantage of such an approach is that the segmentation is unsupervised and controlled by an interpretable confidence level for signal-to-background separation. In my view the presented segmentation problem does not require Deep Learning and I would suspect that conventional particle detectors will actually do better, as they explicitly include priors of the punctuate nature of the signal.</p><p>From the perspective of the Deep Learning (DL) field, even the DL subfield focused on applications in microscopy, there is nothing new to be found in this paper. The authors make a point about integration of training sets from multiple trainers to increase robustness. In my view this is a standard measure in supervised machine learning. Also, there is no systematic performance analysis (e.g. what are the break points with respect to abundance or SNR of the training data) or interpretation of what that network learns and how variation, e.g. simplification, of the network affects the segmentation.</p><p><italic>Reviewer #3:</italic></p><p>This paper presents a method for the automatic segmentation of fluorescence microscopy images from brain specimens. The method is trained by and validated against ground truth image segmentations of five experts. A cloud-based implementation of the segmentation pipeline is available for public use.</p><p>The method (&quot;DeepFLaSH&quot;) is based on the U-Net deep convolutional neural network architecture, but with two alterations that purportedly reduce the amount of training data required: batch normalization and depthwise separable convolution. The network performs on par with experts, after having been trained on just 36 1024x1024 images. The method appears to be solid and certainly of utility to others.</p><p>The paper falls short in one key area. This is in explicitly comparing DeepFLaSH's performance to a traditional U-Net. For readers to benefit fully from this work, they should better understand its method's improvement over techniques they would otherwise use. I would suggest:</p><p>1) The authors train a U-Net on the same data to demonstrate the performance difference.</p><p>2) Stating the number of network parameters that are determined through training, both for DeepFLaSH and the corresponding U-Net.</p><p>3) Show how the network performance depends on the size of the training set, from just a few images up to the full 36. Has cross-validation performance saturated?</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;On the objectivity, reliability, and validity of deep learning enabled bioimage analyses&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Marianne Bronner as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Dan Ruderman (Reviewer #1); Gaudenz Danuser (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. We all feel that this manuscript will be ready for acceptance after some minor revisions, not requiring re-review.</p><p>I agree with all three reviewers that this manuscript is a significant step forward. Reviewer 3 summarized the manuscript and its strengths nicely: &quot;This is a very high quality evaluation of procedures that should be put in place to set up objective, reliable, and valid Deep Learning models for fluorescence image segmentation. I commend the authors for teaming up multiple labs in order to build consensus “ground truth” models. They have done an impressive amount of work in documenting the performance of the various strategies for generating training data sets. The paper is written very well. Despite the dryness of the subject and the at times dense lines of argumentation, one can get through the material in a reasonable amount of time, and then go deep where necessary. This paper can become the reference for every investigator starting a quantitative imaging study with deep learning.&quot;</p><p>The reviewers and I would like to offer some suggested changes to the presentation, without requiring the addition of more experimental work or analyses:</p><p>1) Figures and Tables.</p><p>A) Both reviewer 2 and I found Figure 1 a bit confusing and I found at least three different ways that it could be misunderstood. The problem is that the workflow icons in the individual vertical boxes appear to lie up with the horizontal bars, which I do not believe is your intent. Please revise Figure 1 so that your intent is more clear. I suggest that changing the scale and spacing of the icons might solve it, but putting the workflow icons below the horizontal bars would make the figure only slightly larger and make your intent much more clear.</p><p>B) Please use similar scales and formats for figures that readers are likely to compare, and might misunderstand. For example Figure 2—figure supplement 3 and Figure 2—figure supplement 4 are slightly different sizes, and use different scales (what the different shades of gray represent), so I found myself puzzled. Figure 5—figure supplement 3 and Figure 5—figure supplement 4 use different scales as well.</p><p>C) Table 1, and elsewhere in the text needs attention to the number of significant figures that are displayed. Table 1 shows p-values to 5 or 4 significant figures for non-significant differences. It shows p-values to 3 significant figures for some of the significant differences. If the authors reflect on this, I believe they will agree that there is no way that the data set is sufficiently large for this number of significant figures. Furthermore, the large number of figures distract the reader from the message I believe the authors are intending to convey.</p><p>2) Language and definitions.</p><p>A) The reviewers point out that this very strong paper is not made stronger by claims that seem beyond beyond the data. These include the claim that the DL approach surpasses the experts.</p><p>B) There are terms that should be clarified or defined; in most of these cases, it might be wise to find language that is less likely to be misunderstood. These include: &quot;invalid&quot;, &quot;bad&quot;, &quot;biased&quot;, &quot;irreproducible&quot;, &quot;appear to alter the results&quot;, &quot;DL models are proven to…&quot; etc. Note that all of these should be easy to resolve, but could confuse, anger and/or frustrate a reader, which will not optimize the acceptance of the important lessons of this work.</p><p><italic>Reviewer #1:</italic></p><p>This work is high quality, employs real-world data sets from multiple sources, and addresses questions of broad interest. I recommend publication in <italic>eLife</italic>.</p><p><italic>Reviewer #2:</italic></p><p>In general, I really enjoyed reading this manuscript, and I think the new focus resulted in a much clearer paper compared with the previous version. As a researcher in machine learning myself, the manuscript includes several findings that I will use in my future research, especially the comparison between the various training approaches. As such, I think that the paper will be a valuable contribution to the field, and I recommend publication after my comments below (and those of other reviewers) have been addressed. Note that my expertise is in developing machine learning algorithms, and I don't have enough background in the specific biology application to feel comfortable with commenting on the validity of the biology.</p><p>1) My main concern with respect to the method is the exclusion of what the authors call “invalid” methods in the ensemble approaches. By construction, this exclusion biases the results towards better metrics. Therefore, it is not entirely clear whether the better performance of the ensemble approaches is caused by the ensemble itself, or rather simply by the fact that multiple networks were trained and “bad” networks were thrown away. This question can be answered in several ways: for example, it would already be informative to know how many networks were rejected in this way (if it is a small number compared with the total number of networks, it is not likely the improvement is due to the rejection). Even better would be to also apply the same rejection strategy to the expert model approach and see whether that improves these by the same amount (even though you would have access to the estimated GT in pure expert model applications).</p><p>2) My second main concern is with regards to practical application of the consensus ensemble approach. In many applications, it is very time consuming to acquire manually annotated images due to the required expertise for manual annotation and the time it takes to annotate. Therefore, getting enough manual annotations to obtain accurate consensus models might be prohibitively difficult in practice. A solution for this might be to use multiple experts, but have each expert annotate a different set of images. This would drastically reduce the required manual annotation time compared with obtaining a full consensus model, but you would still have information from multiple experts, which might improve results. For the current manuscript, it would be very informative to include this approach in the results. One way of doing this is to use the data that the authors already have, assign each of the 36 training images to one of the 5 experts, and then during training only use the manual annotation of the assigned expert for that image.</p><p>3) To me Figure 1 is quite hard to read. I do get what the authors mean, but the fact that the icons in the “vertical blocks” (e.g. “data annotation” and “automated annotation”) align with the rows makes it seem that each icon in the block actually belongs to a certain row. A solution would be to rearrange the icons inside each block somewhat (e.g. by making them smaller) so that they don't line up anymore with the rows.</p><p>4) It would be interesting to investigate what the accuracy is of an approach in which only the GT of a single expert is used, but multiple networks are trained in an ensemble. In other words, a combination between the expert models and consensus ensembles, but without using the estimated GT. This would indicate whether the improvement of the consensus ensembles is not purely due to the ensemble itself. I don't expect that this will actually achieve very accurate results, so probably a short paragraph or an added sentence or two will be enough to describe these additional results in the paper.</p><p><italic>Reviewer #3:</italic></p><p>This is a very high quality evaluation of procedures that should be put in place to set up objective, reliable, and valid Deep Learning models for fluorescence image segmentation. I commend the authors for teaming up multiple labs in order to build consensus “ground truth” models. They have done an impressive amount of work in documenting the performance of the various strategies for generating training data sets. The paper is written very well. Despite the dryness of the subject and the at times dense lines of argumentation, one can get through the material in a reasonable amount of time, and then go deep where necessary. This paper can become the reference for every investigator starting a quantitative imaging study with deep learning. I really have nothing substantial to comment.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.59780.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>The reviewers and I appreciate the strengths of DeepFLaSH, and the challenges of the important problem you are attacking. We all feel that research employing fluorescence imaging will be dramatically enhanced as optimized machine learning techniques are refined that are tolerant of the particular challenges you outline. However, the current manuscript falls short of documenting the advance represented by DeepFLaSH.</p><p>We feel that any study reporting an advance must fully embrace two key demonstrations:</p><p>1) A complete and balanced comparison of the speed, ease-of-use and accuracy of the new approach (in this case, DeepFLaSH) with a full spectrum of the other tools currently available. The reviewers believe that the comparisons performed in this manuscript are not sufficient in depth or breadth. Comparisons to techniques that would not be employed by a knowledgeable user because they are known to fail (eg. a simple thresholding) only serves to weaken the confidence of the reader in the nature of the advance.</p><p>2) A use of the new approach (in this case, DeepFLaSH) to analyze a biological system and extract a publishable advance.</p><p>A manuscript expanded in this fashion, to perform a relevant bank of tests and to present a novel biological insight, would be high-impact, and would justify publication in eLife or other first-tier journals.</p></disp-quote><p>The thorough reviews have made it very clear to us that we did a subpar job of conveying the main intention behind the manuscript.</p><p>Our primary goal was not to establish a new method in the sense of a Deep Learning (DL) algorithm, but rather to demonstrate that a light-weight, end-to-end integration of DL methods can reliably and reproducibly verify the presence of biological effects in laboratory data – both on the inter-individual level (heterogeneous coding in lab) as well as on the inter-lab level (a network trained in lab A can with minimal re-training be used to analyze data from lab B).</p><p>Our work ultimately addresses a central concern put forward by Falk et al. (2019, Nature Methods: U-Net: deep learning for cell counting, detection, and morphometry): “U-Net learns from the provided examples. If the examples are not representative of the actual task, or if the manual annotation in these examples is low quality and inconsistent, U-Net will either fail to train or will reproduce inconsistent annotations on new data.” We suggest the following corollary of this statement: Local instantiations of deep learning models (e.g., training in a certain lab) can at most speed up the local analysis pipeline while retaining the intrinsic bias of human coders. Yet, a common integrated workflow instantiated by a light-weight tool (we called this DeepFlaSH) can additionally ensure objectivity, reliability, reproducibility and transparency through shared neural network weights.</p><p>To confirm this idea experimentally, we created typical image raw data and we used the biological model of cFOS changes after behavioral training of mice. There is no ground truth in these images showing fluorescent labels of cFOS; there is no ultimate parameter allowing signal definition. One cannot decide, whether “manual annotation in these examples is low quality and inconsistent” or not. Our experimental strategy gave us a second parameter; the behavior of the mice. Easy said: the analysis of the mouse behavior gave us a coincident parameter in order to decide whether the U-Net failed to create consistent or inconsistent data. Our main result is that the speed and flexibility of a light-weight DL workflow (such as DeepFlaSH) can be used for a higher reproducibility, reliability, objectivity and transparency in image analysis.</p><p>Over the last year, we moved forward and fully overhauled the entire study. We followed our initial idea, namely to investigate how DL can contribute to more objectivity and reproducibility of bioimage analyses. Considering these developments, we think that there is no benefit of associating this study with the prior submission.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>The reviewers and I would like to offer some suggested changes to the presentation, without requiring the addition of more experimental work or analyses:</p><p>1) Figures and Tables.</p><p>A) Both reviewer 2 and I found Figure 1 a bit confusing and I found at least three different ways that it could be misunderstood. The problem is that the workflow icons in the individual vertical boxes appear to lie up with the horizontal bars, which I do not believe is your intent. Please revise Figure 1 so that your intent is more clear. I suggest that changing the scale and spacing of the icons might solve it, but putting the workflow icons below the horizontal bars would make the figure only slightly larger and make you intent much more clear.</p></disp-quote><p>We appreciate this input. Having spent a lot of time on Figure 1 in the initial submission process we were so deep into it that we did not realize that the ordering of icons, process steps and approaches may be confusing. Looking at the figure again after 3 months, the issues you raised become directly evident. We modified the figure along your suggestions and feel that it is much easier to understand now.</p><disp-quote content-type="editor-comment"><p>B) Please use similar scales and formats for figures that readers are likely to compare, and might misunderstand. For example Figure 2—figure supplement 3 and Figure 2—figure supplement 4 are slightly different sizes, and use different scales (what the different shades of gray represent), so I found myself puzzled. Figure 5—figure supplement 3 and Figure 5—figure supplement 4 use different scales as well.</p></disp-quote><p>In our initial version we tried to exploit the whole greyscale space to visually distinguish the heatmap entries, using different scales for each metric (f1 score/iou). We agree that the resulting scaling differences lead to confusion when comparing across figures. We revised the figures and aligned the scales for Figure 2—figure supplement 3/ Figure 2—figure supplement 4 and Figure 5—figure supplement 3/ Figure 5—figure supplement 4.</p><disp-quote content-type="editor-comment"><p>C) Table 1, and elsewhere in the text needs attention to the number of significant figures that are displayed. Table 1 shows p-values to 5 or 4 significant figures for non-significant differences. It shows p-values to 3 significant figures for some of the significant differences. If the authors reflect on this, I believe they will agree that there is no way that the data set is sufficiently large for this number of significant figures. Furthermore, the large number of figures distract the reader from the message I believe the authors are intending to convey.</p></disp-quote><p>We fully agree that the limited size of these data sets also restricts the validity of the calculated p-values to less post decimal positions. We therefore reduced the number of presented post decimal positions for the calculated p-values in Table 1 to 3 and denote p-values that are smaller than 0.001 as &lt; 0.001.</p><disp-quote content-type="editor-comment"><p>2) Language and definitions.</p><p>A) The reviewers point out that this very strong paper is not made stronger by claims that seem beyond beyond the data. These include the claim that the DL approach surpasses the experts.</p><p>B) There are terms that should be clarified or defined; in most of these cases, it might be wise to find language that is less likely to be misunderstood. These include: &quot;invalid&quot;, &quot;bad&quot;, &quot;biased&quot;, &quot;irreproducible&quot;, &quot;appear to alter the results&quot;, &quot;DL models are proven to…&quot; etc. Note that all of these should be easy to resolve, but could confuse, anger and/or frustrate a reader, which will not optimize the acceptance of the important lessons of this work.</p></disp-quote><p>We appreciate these pointers to our usage of ambiguous terms. In the revised document we carefully rephrased the corresponding passages. We replaced the corresponding terms or replaced the sentences with more careful statements. Overstatements without solid statistical foundation such as “constantly outperform/surpass” were deleted.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This work is high quality, employs real-world data sets from multiple sources, and addresses questions of broad interest. I recommend publication in eLife.</p><p>Reviewer #2:</p><p>In general, I really enjoyed reading this manuscript, and I think the new focus resulted in a much clearer paper compared with the previous version. As a researcher in machine learning myself, the manuscript includes several findings that I will use in my future research, especially the comparison between the various training approaches. As such, I think that the paper will be a valuable contribution to the field, and I recommend publication after my comments below (and those of other reviewers) have been addressed. Note that my expertise is in developing machine learning algorithms, and I don't have enough background in the specific biology application to feel comfortable with commenting on the validity of the biology.</p><p>1) My main concern with respect to the method is the exclusion of what the authors call “invalid” methods in the ensemble approaches. By construction, this exclusion biases the results towards better metrics. Therefore, it is not entirely clear whether the better performance of the ensemble approaches is caused by the ensemble itself, or rather simply by the fact that multiple networks were trained and “bad” networks were thrown away. This question can be answered in several ways: for example, it would already be informative to know how many networks were rejected in this way (if it is a small number compared with the total number of networks, it is not likely the improvement is due to the rejection). Even better would be to also apply the same rejection strategy to the expert model approach and see whether that improves these by the same amount (even though you would have access to the estimated GT in pure expert model applications).</p></disp-quote><p>Thank you for this valuable feedback! We agree that our initial manuscript did not sufficiently justify the model selection process and the impacts on model performance. Following your suggestion, we have indicated the number of discarded models in the corresponding figure legends (Figures 5, Figure 5—figure supplement 1, Figure 5—figure supplement 2) and included the model selection results in the corresponding figure source data. Moreover, we have revised the description of the model selection process (as stated in our response to the Editor).</p><disp-quote content-type="editor-comment"><p>2) My second main concern is with regards to practical application of the consensus ensemble approach. In many applications, it is very time consuming to acquire manually annotated images due to the required expertise for manual annotation and the time it takes to annotate. Therefore, getting enough manual annotations to obtain accurate consensus models might be prohibitively difficult in practice. A solution for this might be to use multiple experts, but have each expert annotate a different set of images. This would drastically reduce the required manual annotation time compared with obtaining a full consensus model, but you would still have information from multiple experts, which might improve results. For the current manuscript, it would be very informative to include this approach in the results. One way of doing this is to use the data that the authors already have, assign each of the 36 training images to one of the 5 experts, and then during training only use the manual annotation of the assigned expert for that image.</p></disp-quote><p>Indeed, the acquisition of annotated training data was one of the greatest challenges for our study, and it will most likely continue to be the bottleneck for future DL based studies. For this study, we avoided splitting the training images because our aim was to control the DL strategies exactly on the same images to get information about the variability between experts and expert models. During our experiments, we have already tried different approaches for model training, e.g., we trained models using all expert segmentation instead of the estimated ground truth. This approach led to an unstable training behavior (heavy oscillations of the loss function from iteration to iteration). We would argue that your suggested approach could lead to the same training instability (of course, depending on the data and the differences between expert annotations). However, we agree that there is still potential to reduce the annotation effort that should be evaluated in future studies.</p><disp-quote content-type="editor-comment"><p>3) To me Figure 1 is quite hard to read. I do get what the authors mean, but the fact that the icons in the “vertical blocks” (e.g. “data annotation” and “automated annotation”) align with the rows makes it seem that each icon in the block actually belongs to a certain row. A solution would be to rearrange the icons inside each block somewhat (e.g. by making them smaller) so that they don't line up anymore with the rows.</p></disp-quote><p>We modified the figure and feel that it is much easier to understand now.</p><disp-quote content-type="editor-comment"><p>4) It would be interesting to investigate what the accuracy is of an approach in which only the GT of a single expert is used, but multiple network are trained in an ensemble. In other words, a combination between the expert models and consensus ensembles, but without using the estimated GT. This would indicate whether the improvement of the consensus ensembles is not purely due to the ensemble itself. I don't expect that this will actually achieve very accurate results, so probably a short paragraph or an added sentence or two will be enough to describe these additional results in the paper.</p></disp-quote><p>As stated above, we trained models using all expert segmentation instead of the estimated ground truth in our initial experiments. This has led to an unstable training behavior. We think this was caused by the considerable differences in expert annotations. This approach, however, might work with less ambiguous data.</p></body></sub-article></article>