<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">64978</article-id><article-id pub-id-type="doi">10.7554/eLife.64978</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Strategically managing learning during perceptual decision making</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-215629"><name><surname>Masís</surname><given-names>Javier</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9643-8677</contrib-id><email>jmasis@princeton.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-215630"><name><surname>Chapman</surname><given-names>Travis</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-302065"><name><surname>Rhee</surname><given-names>Juliana Y</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa2">‡</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-215632"><name><surname>Cox</surname><given-names>David D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa3">§</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-302066"><name><surname>Saxe</surname><given-names>Andrew M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9831-8812</contrib-id><email>a.saxe@ucl.ac.uk</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="pa4">#</xref><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Department of Molecular and Cellular Biology, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Center for Brain Science, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Experimental Psychology, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>École normale supérieure, PSL University, INSERM</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Princeton Neuroscience Institute, Princeton University, Princeton, United States</p></fn><fn fn-type="present-address" id="pa2"><label>‡</label><p>The Rockefeller University, New York, United States</p></fn><fn fn-type="present-address" id="pa3"><label>§</label><p>MIT-IBM Watson AI Lab, Cambridge, United States</p></fn><fn fn-type="present-address" id="pa4"><label>#</label><p>Gatsby Unit &amp; Sainsbury Wellcome Centre, University College London, London, United Kingdom</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e64978</elocation-id><history><date date-type="received" iso-8601-date="2020-11-18"><day>18</day><month>11</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2023-01-15"><day>15</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-09-02"><day>02</day><month>09</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.09.01.259911"/></event></pub-history><permissions><copyright-statement>© 2023, Masís et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Masís et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-64978-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-64978-figures-v1.pdf"/><abstract><p>Making optimal decisions in the face of noise requires balancing short-term speed and accuracy. But a theory of optimality should account for the fact that short-term speed can influence long-term accuracy through learning. Here, we demonstrate that long-term learning is an important dynamical dimension of the speed-accuracy trade-off. We study learning trajectories in rats and formally characterize these dynamics in a theory expressed as both a recurrent neural network and an analytical extension of the drift-diffusion model that learns over time. The model reveals that choosing suboptimal response times to learn faster sacrifices immediate reward, but can lead to greater total reward. We empirically verify predictions of the theory, including a relationship between stimulus exposure and learning speed, and a modulation of reaction time by future learning prospects. We find that rats’ strategies approximately maximize total reward over the full learning epoch, suggesting cognitive control over the learning process.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>learning</kwd><kwd>decision making</kwd><kwd>neural networks</kwd><kwd>behavior</kwd><kwd>cognitive control</kwd><kwd>inter-temporal choice</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>Intelligence Advanced Research Projects Activity</institution></institution-wrap></funding-source><award-id>D16PC00002</award-id><principal-award-recipient><name><surname>Cox</surname><given-names>David D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001341</institution-id><institution>Richard and Susan Smith Family Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Cox</surname><given-names>David D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007229</institution-id><institution>Harvard University</institution></institution-wrap></funding-source><award-id>Harvard Brain Science Initiative</award-id><principal-award-recipient><name><surname>Masís</surname><given-names>Javier</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100006734</institution-id><institution>Princeton University</institution></institution-wrap></funding-source><award-id>Presidential Postdoctoral Research Fellowship</award-id><principal-award-recipient><name><surname>Masís</surname><given-names>Javier</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship (216386/Z/19/Z)</award-id><principal-award-recipient><name><surname>Saxe</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship (216386/Z/19/Z)</award-id><principal-award-recipient><name><surname>Saxe</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Swartz Foundation</institution></institution-wrap></funding-source><award-id>Swartz Postdoctoral Fellowship in Theoretical Neuroscience</award-id><principal-award-recipient><name><surname>Saxe</surname><given-names>Andrew M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>During perceptual decision making, maximizing total reward in the long term requires trading reward in the short term for a faster improvement in perceptual representations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Optimal behavior in decision making is frequently defined as maximization of reward over time (<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>), and this requires balancing the speed and accuracy of one’s choices (<xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>). For example, imagine you are given a multiple-choice quiz on an esoteric topic with which you are familiar, such as behavioral neuroscience or cognitive psychology, and rewarded for every correct answer. In balancing speed and accuracy, you should spend some time on each question to ensure you get it right. Now imagine that you are given a different quiz on an esoteric topic with which you are not familiar, such as low Reynolds number hydrodynamics or underwater basket weaving. In balancing speed and accuracy, you should now guess on as many questions as you can as quickly as you can in order to maximize reward. The ideal balance of speed and accuracy differs considerably in the cases of high and low competence. However, there is an important additional dynamical aspect to consider: competence can change as a function of experience through learning. For instance, taking the hydrodynamics quiz enough times, you might start to get the hang of it, by going slow enough that you can remember questions and their associated answers, rather than guessing as quickly as you can. Given these almost opposing normative strategies for high and low competence, how does one effectively move from low competence to high competence? In other words, how does an agent strategically manage decision making in light of learning?</p><p>In this study, we formalize this problem in the context of a two-choice perceptual decision making task in rodents and simulated agents. Perceptual decisions, in particular two-choice decisions, allow us to leverage one of the most prolific decision making models, the drift-diffusion model (DDM) (<xref ref-type="bibr" rid="bib83">Ratcliff, 1978</xref>) (and the considerable analytical dissections of it <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>), and one of the most prolific paradigms captured by it, the speed-accuracy trade-off (SAT), as a measurement of optimal behavior (i.e. maximization of reward per unit time) (<xref ref-type="bibr" rid="bib122">Woodworth, 1899</xref>, <xref ref-type="bibr" rid="bib41">Henmon, 1911</xref>, <xref ref-type="bibr" rid="bib28">Garrett, 1922</xref>, <xref ref-type="bibr" rid="bib78">Pew, 1969</xref>, <xref ref-type="bibr" rid="bib75">Pachella, 1974</xref>, <xref ref-type="bibr" rid="bib118">Wickelgren, 1977</xref>, <xref ref-type="bibr" rid="bib96">Ruthruff, 1996</xref>, <xref ref-type="bibr" rid="bib85">Ratcliff and Rouder, 1998</xref>, <xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>, <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib10">Bogacz et al., 2010</xref>; <xref ref-type="bibr" rid="bib39">Heitz and Schall, 2012</xref>; <xref ref-type="bibr" rid="bib40">Heitz, 2014</xref>, <xref ref-type="bibr" rid="bib82">Rahnev and Denison, 2018</xref>).</p><p>Studies of the SAT have focused on how the brain may solve it (<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>, <xref ref-type="bibr" rid="bib92">Roitman and Shadlen, 2002</xref>), what the optimal solution is (<xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>), and whether agents can indeed manage it (<xref ref-type="bibr" rid="bib102">Simen et al., 2006</xref>, <xref ref-type="bibr" rid="bib3">Balci et al., 2011a</xref>; <xref ref-type="bibr" rid="bib103">Simen et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Bogacz et al., 2010</xref>, <xref ref-type="bibr" rid="bib22">Drugowitsch et al., 2014</xref>, <xref ref-type="bibr" rid="bib23">Drugowitsch et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Manohar et al., 2015</xref>). Though most work in this area has taken place in humans and non-human primates, several studies have established the presence of a SAT in rodents (<xref ref-type="bibr" rid="bib111">Uchida and Mainen, 2003</xref>, <xref ref-type="bibr" rid="bib1">Abraham et al., 2004</xref>; <xref ref-type="bibr" rid="bib91">Rinberg et al., 2006</xref>; <xref ref-type="bibr" rid="bib88">Reinagel, 2013a</xref>; <xref ref-type="bibr" rid="bib89">Reinagel, 2013b</xref>; <xref ref-type="bibr" rid="bib51">Kurylo et al., 2020</xref>). The broad conclusion of much of this literature is that after extensive training, many subjects come close to optimal performance (<xref ref-type="bibr" rid="bib103">Simen et al., 2009</xref><xref ref-type="bibr" rid="bib10">Bogacz et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>; <xref ref-type="bibr" rid="bib123">Zacksenhouse et al., 2010</xref>, <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>, <xref ref-type="bibr" rid="bib104">Starns and Ratcliff, 2010</xref>, <xref ref-type="bibr" rid="bib42">Holmes and Cohen, 2014</xref>, <xref ref-type="bibr" rid="bib22">Drugowitsch et al., 2014</xref>, <xref ref-type="bibr" rid="bib23">Drugowitsch et al., 2015</xref>). When faced with deviations from optimality, several hypotheses have been proposed, including error avoidance, poor internal estimates of time, and a minimization of the cognitive cost associated with an optimal strategy (<xref ref-type="bibr" rid="bib63">Maddox and Bohil, 1998</xref>, <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>, <xref ref-type="bibr" rid="bib123">Zacksenhouse et al., 2010</xref>).</p><p>Past studies have shown how agents behave after reaching steady-state performance (<xref ref-type="bibr" rid="bib103">Simen et al., 2009</xref>, <xref ref-type="bibr" rid="bib104">Starns and Ratcliff, 2010</xref>, <xref ref-type="bibr" rid="bib10">Bogacz et al., 2010</xref><xref ref-type="bibr" rid="bib123">Zacksenhouse et al., 2010</xref>, <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>, <xref ref-type="bibr" rid="bib3">Balci et al., 2011a</xref>; <xref ref-type="bibr" rid="bib104">Starns and Ratcliff, 2010</xref>, <xref ref-type="bibr" rid="bib22">Drugowitsch et al., 2014</xref>, <xref ref-type="bibr" rid="bib23">Drugowitsch et al., 2015</xref>), but relatively less attention has been paid to how agents learn to approach near-optimal behavior (but see <xref ref-type="bibr" rid="bib54">Law and Gold, 2009</xref>, <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>, <xref ref-type="bibr" rid="bib24">Drugowitsch et al., 2019</xref>). While maximizing instantaneous reward rate is a sensible goal when the task is fully mastered, it is less clear that this objective is appropriate during learning.</p><p>Here, we set out to understand how agents manage the SAT during learning by studying the learning trajectory of rats and simulated agents in a free-response two-alternative forced-choice visual object recognition task (<xref ref-type="bibr" rid="bib125">Zoccolan et al., 2009</xref>). Rats near-optimally maximized instantaneous reward rate (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) at the end of learning but chose response times that were too slow to be <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal early in learning. To understand the rats’ learning trajectory, we examined learning trajectories in a recurrent neural network (RNN) trained on the same task. We derive a reduction of this RNN to a learning drift-diffusion model (LDDM) with time-varying parameters that describes the network’s average learning dynamics. Mathematical analysis of this model reveals a dilemma: at the beginning of learning when error rates are high, <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> is maximized by fast responses (<xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>). However, fast responses mean minimal stimulus exposure, little opportunity for perceptual processing, and consequently slow learning. Because of this learning speed/<inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (LS/<inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) trade-off, slow responses early in learning can yield greater total reward over engagement with the task, suggesting a normative basis for the rats’ behavior. We then experimentally tested and confirmed several model predictions by evaluating whether response time and learning speed are causally related, and whether rats choose their response times so as to take advantage of learning opportunities. Our results suggest that rats exhibit cognitive control of the learning process, adapting their behavior to approximately accrue maximal total reward across the entire learning trajectory, and indicate that a policy that prioritizes learning in perceptual tasks may be advantageous from a total reward perspective.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Trained rats solve the SAT</title><p>We trained <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats on a visual object recognition two-alternative forced-choice task (see Methods) (<xref ref-type="bibr" rid="bib125">Zoccolan et al., 2009</xref>). The rats began a trial by licking the central of three capacitive lick ports, at which time a static visual object that varied in size and rotation from one of two categories appeared on a screen. After evaluating the stimulus, the rats licked the right or left lick port. When correct, they received a water reward, and when incorrect, a timeout period (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Because this was a free-response task, rats were also able to initiate a trial and not make a response, but these ignored trials made up a small fraction of all trials and were not considered during our analysis (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Trained rats solve the speed-accuracy trade-off.</title><p>(<bold>a</bold>) Rat initiates trial by licking center port, one of two visual stimuli appears on the screen, rat chooses correct left/right response port for that stimulus and receives a water reward. (<bold>b</bold>) Speed-accuracy space: a decision making agent’s <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and mean normalized <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> (a normalization of <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> based on the average timing between one trial and the next, see Methods). Assuming a simple drift-diffusion process, agents that maximize <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (<italic>see</italic> Methods) must lie on an optimal performance curve (OPC, black trace) (<xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>). Points on the OPC relate error rate to mean normalized decision time, where the normalization takes account of task timing parameters (e.g. average response-to-stimulus interval). For a given SNR, an agent’s performance must lie on a performance frontier swept out by the set of possible threshold-to-drift ratios and their corresponding error rates and mean normalized decision times. The intersection point between the performance frontier and the OPC is the error rate and mean normalized decision time combination that maximizes <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> for that SNR. Any other point along the performance frontier, whether above or below the OPC, will achieve a suboptimal. <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> Overall, <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> increases toward the bottom left with maximal instantaneous reward rate at error rate = 0.0 and mean normalized decision time = 0.0. (<bold>c</bold>) Mean performance across 10 sessions for trained rats (<inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula>) at asymptotic performance plotted in speed-accuracy space. Each cross is a different rat. Color indicates fraction of maximum instantaneous reward rate (<inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) as determined by each rat’s performance frontier. Errors are bootstrapped SEMs. (<bold>d</bold>) Violin plots depicting fraction of maximum, <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> a quantification of distance to the OPC, for same rats and same sessions as c. Fraction of maximum <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> is a comparison of an agent’s current <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> with its optimal <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> given its inferred SNR. Approximately 15 of 26 (∼60%) of rats attain greater than 99% fraction maximum <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for their individual inferred SNRs. * denotes p &lt; 0.05 one-tailed Wilcoxon signed-rank test for mean &gt;0.99.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Task schematic for error trials.</title><p>Error trial: rat chooses incorrect left/right response port and incurs a timeout period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Fraction of ignored trials during learning.</title><p>(<bold>a</bold>) Schematic of an ignore trial: rat does not choose a left/right response port and receives no feedback. (<bold>b</bold>) Fraction of trials ignored (ignored trials/(correct + incorrect + ignored trials)) during learning for animals encountering the task for the first time (stimulus pair 1). (<bold>c</bold>) Fraction of trials ignored for animals learning stimulus pair 2 after training on stimulus pair 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Drift-diffusion model data fits.</title><p>(<bold>a</bold>) The accuracy and reaction time data from 26 trained rats was fit to a simple drift-diffusion model using the hierarchical Bayesian estimation of the drift-diffusion model (HDDM) package (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>). (<bold>b</bold>) Estimated posterior distributions of parameter values across all animals.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Estimating <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>T</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula>.</title><p>(<bold>a</bold>) Linear and quadratic extrapolations to accuracy as a function of reaction time. The <italic>T</italic><sub>0</sub> estimate is when each extrapolation intersects chance accuracy (0.5). (<bold>b</bold>) Mean accuracy for trials with reaction times 350–375 ms for <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats. (<bold>c</bold>) Minimum motor time estimated by looking at first peak of time between licks to/from center port for <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math></inline-formula> rats. (<bold>d</bold>) Cartoon of stimulus onset latency across visual areas from <xref ref-type="bibr" rid="bib113">Vermaercke et al., 2014</xref> to estimate minimum visual processing time. (<bold>e</bold>) Diagram of <italic>T</italic><sub>0</sub> estimates, with an upper limit (minimum reaction time) and lower limit (minimum motor time + minimum visual processing time). (<bold>f</bold>) Mean learning trajectory for <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats with various <italic>t</italic><sub>0</sub> estimates. (<bold>g</bold>) Subjects (<inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula>) in speed-accuracy space with various <italic>T</italic><sub>0</sub> estimates.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Analysis of voluntary intertrial intervals (ITIs).</title><p>(a) Histogram of voluntary ITIs (time in addition to mandatory experimentally determined <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) for <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats across 10 sessions for previous correct (blue) and previous error (red) ITIs. Voluntary ITIs are spaced every 500 ms because of violations to the ‘cannot lick’ period. <italic>Inset</italic>: proportion of voluntary ITIs below 500, 1000, and 2000 ms boundaries. (b) Median voluntary ITIs up too 500, 1000, and 2000 ms boundaries. (c) Overlay of voluntary ITIs spaced 500 ms apart after previous correct trials. (d) Overlay of voluntary ITIs spaced 500 ms apart after previous error trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp5-v1.tif"/></fig><fig id="fig1s6" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 6.</label><caption><title>Mandatory post-error (<inline-formula><mml:math id="inf29"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and post-correct (<inline-formula><mml:math id="inf30"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>o</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) response-to-stimulus interval times.</title><p>(<bold>a</bold>) Diagram of intertrial interval (ITI) after previous error trial. All times (punishment stimulus, enforced intertrial interval, cannot lick reward ports, and pre-stimulus time) were verified based on timestamps on experimental file logs. After the punishment stimulus and enforced intertrial interval, there is a 300 ms period where rats cannot lick the reward ports. If violated, 500 ms are added to the intertrial interval followed by another 300 ms ‘cannot lick’ period. In addition to this restriction, rats may take as much voluntary time between trials as they wish. Any violation of the ‘cannot lick’ period is counted as voluntary time, and only the minimum mandatory time of 3136 ms is counted for <inline-formula><mml:math id="inf31"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. (<bold>b</bold>) Diagram of ITI after previous correct trial. All times (dispense water reward, collect water reward, enforced intertrial interval, cannot lick reward ports, pre-stimulus time) were verified based on timestamps on experimental file logs. The same ‘cannot lick’ period is present as in a. All times (dispense water reward, collect water reward, enforced intertrial interval, cannot lick reward ports, pre-stimulus time) were verified based on timestamps on experimental file logs. Any violation of the ‘cannot lick’ period is counted as voluntary time, and only the minimum mandatory time of 6370 ms is counted for <inline-formula><mml:math id="inf32"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp6-v1.tif"/></fig><fig id="fig1s7" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 7.</label><caption><title>Reward rate sensitivity to <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>T</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub></mml:math></inline-formula> and voluntary intertrial interval (ITI).</title><p>(<bold>a</bold>) Fraction of maximum instantaneous reward rate across <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats over 10 sessions at asymptotic performance over possible voluntary ITI values of 0–1000 ms and over the minimum and maximum estimated <italic>T</italic><sub>0</sub> values. (<bold>b</bold>) Fraction of maximum instantaneous reward rate across <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats over 10 sessions at asymptotic performance over possible <italic>T</italic><sub>0</sub> values from 160 to 350 ms (min to max estimated <italic>T</italic><sub>0</sub> values) and over the median voluntary ITIs with 500, 1000, and 2000 ms boundaries. (<bold>c</bold>) Fraction of maximum instantaneous reward rate across <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats as a function of normalized training time during learning period and possible voluntary ITIs from 0 to 2000 ms calculated with the <italic>T</italic><sub>0</sub> minimum of 160 ms. The gray curves represent a weighted average over previous correct/error median voluntary ITIs over normalized training time. Contours with different fractions of maximum instantaneous reward rate in pink. (<bold>d</bold>) Same as in c but calculated with <italic>T</italic><sub>0</sub> maximum of 350 ms.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig1-figsupp7-v1.tif"/></fig></fig-group><p>We examined the relationship between error rate (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and reaction time (<inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) during asymptotic performance using the DDM (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). In the DDM, perceptual information is integrated through time until the level of evidence for one alternative reaches a threshold. The SAT is controlled by the subject’s choice of threshold, and is solved when a subject’s performance lies on an optimal performance curve (OPC; <xref ref-type="fig" rid="fig1">Figure 1b</xref>; <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>). The OPC defines the mean normalized decision time (<inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> combination for which an agent will collect maximal <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (see Methods). At any given time, an agent will have some perceptual sensitivity (signal-to-noise ratio [SNR]) which reflects how much information about the stimulus arrives per unit time. Given this SNR, an agent’s position in speed-accuracy space (the space relating <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) is constrained to lie on a performance frontier traced out by different thresholds (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Using a low threshold yields fast but error-prone responses, while using a high threshold yields slow but accurate responses. An agent only maximizes <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> when it chooses the <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> combination on its performance frontier that intersects the OPC. After learning the task to criterion, over half the subjects collected over 99% of their total possible reward, based on inferred SNRs assuming a DDM (<xref ref-type="fig" rid="fig1">Figure 1c and d</xref>).</p><p>Calculating mean normalized <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for comparison with the OPC requires knowing two quantities, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and the average non-decision time per error trial <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The average non-decision time <inline-formula><mml:math id="inf50"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> contains the motor and initial perceptual processing components of <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, denoted <italic>T</italic><sub>0</sub>; and the post-response timeout on error trials <inline-formula><mml:math id="inf52"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Mean normalized <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is then the ratio <inline-formula><mml:math id="inf54"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In order to determine each subject’s <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, we estimated <italic>T</italic><sub>0</sub> through a variety of methods, opting for a biological estimate (measured lickport latency response times and published visual processing latencies; <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). To ensure that our results did not depend on our choice of <italic>T</italic><sub>0</sub>, we ran a sensitivity analysis on a wide range of possible values of <italic>T</italic><sub>0</sub> (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4f</xref>). We then had to determine <inline-formula><mml:math id="inf56"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which can contain mandatory and voluntary intertrial intervals. We found that the rats generally kept voluntary intertrial intervals to a minimum, and we interpreted longer intervals as effectively ‘exiting’ the DDM framework (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). As such, we defined <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to only contain mandatory intertrial intervals (see Methods, <xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>). To ensure that our results did not depend on either choice, we ran a sensitivity analysis on the combined effects of <italic>T</italic><sub>0</sub> and a <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> containing voluntary intertrial intervals on RR (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7</xref>). A full discussion of how these parameters were determined is included in the Methods.</p><p>Across a population, a uniform stimulus difficulty will reveal different SNRs because the internal perceptual processing ability in every subject will be different. Thus, although we did not explicitly vary stimulus difficulty (<xref ref-type="bibr" rid="bib103">Simen et al., 2009</xref>, <xref ref-type="bibr" rid="bib10">Bogacz et al., 2010</xref>; <xref ref-type="bibr" rid="bib123">Zacksenhouse et al., 2010</xref><xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>), as a population, animals clustered along the OPC across a range of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), supporting the assertion that well-trained rats achieve a near maximal <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> in this perceptual task. We note that subjects did not span the entire range of possible <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and that the differences in optimal <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> dictated by the OPC for the <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> we did observe are not large. It remains unclear whether our subjects would be optimal over a wider range of task parameters. Notwithstanding, previous work with a similar task found that rats did increase <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in response to increased penalty times, indicating a sensitivity to these parameters (<xref ref-type="bibr" rid="bib88">Reinagel, 2013a</xref>). Thus, for our perceptual task and its parameters, trained rats approximately solve the SAT.</p></sec><sec id="s2-2"><title>Rats do not maximize instantaneous reward rate during learning</title><p>Knowing that rats harvested reward near-optimally after learning, we next asked whether rats harvested instantaneous reward near-optimally during learning as well. If rats optimized <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> throughout learning, their trajectories in speed-accuracy space should always track the OPC.</p><p>During learning, a representative individual (<inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) started with long <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that decreased as accuracy increased across training time (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Transforming this trajectory to speed-accuracy space revealed that throughout learning the individual did not follow the OPC (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Early in learning, the individual started with a much higher <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> than optimal, but as learning progressed it approached the OPC. The maximum <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> opportunity cost is the fraction of maximum possible <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> relinquished for a choice of threshold (and average <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) (see Methods). We found that this individual gave up over 20% of possible <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> at the beginning of learning but harvested reward near-optimally at asymptotic performance (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). These trends held when the learning trajectories of <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> individuals were averaged (<xref ref-type="fig" rid="fig2">Figure 2d–f</xref>). To ensure that our particular training regime (which involved changes in stimulus size and rotation) was not responsible for these trends, we trained a separate cohort (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>) with a simplified regime that did not involve any changes to the stimuli and we did not observe any meaningful differences (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, see Methods). These results show that rats do not greedily maximize <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> throughout learning and lead to the question: if rats maximize <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> at the end of learning, what principle governs their strategy at the beginning of learning?</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Rats do not greedily maximize instantaneous reward rate during learning.</title><p>(<bold>a</bold>) Reaction time (blue) and error rate (pink) for an example subject (rat AL14) across 23 sessions. (<bold>b</bold>) Learning trajectory of individual subject (rat AL14) in speed-accuracy space. Color map indicates training time. Optimal performance curve (OPC) in blue. (<bold>c</bold>) Maximum <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> opportunity cost (see Methods) for individual subject (rat AL14). (<bold>d</bold>) Mean reaction time (blue) and error rate (pink) for <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats during learning. Sessions across subjects were transformed into normalized sessions, averaged and binned to show learning across 10 bins. Normalized training time allows averaging across subjects with different learning rates (see Methods). (<bold>e</bold>) Learning trajectory of <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats in speed-accuracy space. Color map and OPC as in a. (<bold>f</bold>) Maximum <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> opportunity cost of rats in b throughout learning. Errors reflect within-subject session SEMs for a and b and across-subject session SEMs for d, e, and f.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Comparison of training regimes.</title><p>(<bold>a</bold>) ‘Canonical only’: rats trained to asymptotic performance with only front-view image of each of the two stimuli. ‘Size and rotation’: rats first shown front-view image of stimuli. After reaching criterion (<inline-formula><mml:math id="inf81"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula>), size staircased. Following criterion, rotation staircased. Upon criterion, stimuli randomly drawn across size and rotation. (<bold>b</bold>) Learning trajectory in speed-accuracy space over normalized training time for rats trained with the ‘size and rotation’ (left panel) and the ‘canonical only’ training regimes (right panel). (<bold>c</bold>) Average location in speed-accuracy space for 10 sessions after asymptotic performance for individual rats in both training regimes, as in b. (<bold>d</bold>) Mean accuracy over learning (left panel) and for 5 sessions after asymptotic performance (right panel) for rats trained with the ‘size and rotation’ (<inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula>) and the ‘canonical only’ (n=8) training regimes. (<bold>e</bold>) Mean reaction time. (<bold>f</bold>) Mean fraction max <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. (<bold>g</bold>) Mean total trials per session. (<bold>h</bold>) Mean voluntary intertrial interval up to 500 ms after error trials. (<bold>i</bold>) Mean fraction ignored trials. All errors are SEM. Significance in right panels of d–i determined by Wilcoxon rank-sum test with p&lt;0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig2-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Learning DDM</title><p>To theoretically understand the effect of different learning strategies, we developed a simple linear RNN formalism for our task. This framework enables investigation of how long-term perceptual learning across many trials is influenced by the choice of decision time on individual trials (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We first describe this neural network formalism, before showing how it can be analytically reduced to a classic DDM with time-dependent parameters that evolve over the course of learning.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Recurrent neural network and learning drift diffusion model (DDM).</title><p>(<bold>a</bold>) Roll out in time of recurrent neural network (RNN) for one trial. (<bold>b</bold>) The decision variable for the recurrent neural network (dark gray), and other trajectories of the equivalent DDM for different diffusion noise samples (light gray). (<bold>c, d, e</bold>) Changes in <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> over a long period of task engagement in the RNN (light gray, pixel simulation individual traces; black, pixel simulation mean; pink, Gaussian simulation mean) compared to the theoretical predictions from the learning DDM (blue). (<bold>f</bold>) Visualization of traces in c and d in speed-accuracy space along with the optimal performance curve (OPC) in green. The threshold policy was set to be <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive for c–f.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Analytical reduction of linear drift-diffusion model (LDDM) matches error-corrective learning neural network dynamics during learning.</title><p>(<bold>a</bold>) The recurrent linear neural network can be analytically reduced. In the reduction, the decision variable draws an observation from one of two randomly chosen Gaussian ‘stimuli’. The observations are scaled by a perceptual weight. After the addition of some irreducible noise, the value of the decision variable at previous time step is added to the current time step. A trial ends once the decision variable hits a predetermined threshold. The dynamics of the perceptual weight capture the mean effect of gradient descent learning in the recurrent linear neural network. (<bold>b</bold>) Weight <italic>w</italic> of neural network across task engagement time for multiple simulations of the network (gray), the mean of the simulations (black), and the analytical reduction of the network (blue). (<bold>c</bold>) Same as in b but for the threshold <italic>z</italic>. (<bold>d</bold>) Same as in b but for the error rate. (<bold>e</bold>) Same as in b but for the decision time. (<bold>f</bold>) Same as in b but for the instantaneous reward rate (correct trials per second). (<bold>g</bold>) Learning trajectory in speed-accuracy space for simulations, simulation mean, and analytical reduction (theory). Optimal performance curve (OPC) is shown in red.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig3-figsupp1-v1.tif"/></fig></fig-group><sec id="s2-3-1"><title>Linear RNN</title><p>Our model takes the form of a simple RNN, depicted unrolled through time in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. The network receives noisy sensory input over time during a trial, amplifies this evidence through weighted synaptic connections, and integrates the result until a threshold is reached. After making a decision and receiving feedback, the synaptic connections are updated a small amount according to an error-corrective gradient descent learning rule. Therefore, there are two key timescales in the model: first, the fast activity dynamics during a single trial, which produces a single decision with a certain reaction time; and second, the slow weight dynamics due to learning across many trials. In the following, we denote time within trial as the variable <inline-formula><mml:math id="inf88"><mml:mi>t</mml:mi></mml:math></inline-formula>, and the trial number as <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>. We now describe the dynamics on each timescale in greater detail.</p><p>Within a trial, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> dimensional inputs <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> arrive at discrete times <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a small time step parameter. In our experimental task, <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> might represent the activity of LGN neurons in response to a given visual stimulus. Because of eye motion and noise in the transduction from light intensity to visual activity, the response of individual neurons will only probabilistically relate to the correct answer at any given instant. In our simulations, we take <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be the pixel values of the exact images presented to the animals, but transformed at each time point by small rotations (±20°) and translations (±25% of the image width and height), as depicted in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. This input variability over time makes temporal integration valuable even in this visual classification task. To perform this integration, each input <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is filtered through perceptual weights <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and added to a read-out node (decision variable) <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> along with i.i.d. integrator noise <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This integrator noise models internal neural noise. The evolution of the decision variable is given by the simple linear recurrence<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>until the decision variable hits a threshold <inline-formula><mml:math id="inf100"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that is constant on each trial. Here, the RNN already performs an integration through time (a choice motivated by prior experiments in rodents <xref ref-type="bibr" rid="bib12">Brunton et al., 2013</xref>), and improvements in performance come from adjusting the input-to-integrator weights <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to better extract task-relevant sensory information.</p><p>Across trials, the perceptual weights <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are updated to improve performance. In principle this could be accomplished with many possible learning mechanisms such as reinforcement learning (<xref ref-type="bibr" rid="bib54">Law and Gold, 2009</xref>) or Bayesian inference (<xref ref-type="bibr" rid="bib24">Drugowitsch et al., 2019</xref>). Here, we investigate gradient-based optimization of an objective function, as commonly used in deep learning approaches (<xref ref-type="bibr" rid="bib90">Richards et al., 2019</xref>, <xref ref-type="bibr" rid="bib97">Saxe et al., 2021</xref>). In particular, we consider using gradient descent on the hinge loss, corresponding to standard practice in deep learning. The hinge loss is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf103"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the correct output sign for the trial. Then the weights are updated by gradient descent on this loss,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf104"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a small learning rate. The hinge loss is a proxy for accuracy, and so this weight update implements a learning scheme based on error feedback. In essence, perceptual weights are updated after error trials to improve the likelihood of answering correctly in the future.</p><p>To summarize the key parameters of the RNN, the model requires specifying the input distribution <inline-formula><mml:math id="inf105"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the initial perceptual weights <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the integrator noise variance <inline-formula><mml:math id="inf107"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, the gradient descent learning rate <inline-formula><mml:math id="inf108"><mml:mi>λ</mml:mi></mml:math></inline-formula>, and the decision threshold <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> used on each trial. With these parameters specified, the model can be simulated to make predictions for how behavior will evolve over training, as shown in <xref ref-type="fig" rid="fig3">Figure 3c–f</xref>, gray and black traces.</p></sec><sec id="s2-3-2"><title>Reduction to LDDM</title><p>While the behavior of the RNN model obtained in simulations can be compared to data, deep network models remain challenging to understand (<xref ref-type="bibr" rid="bib97">Saxe et al., 2021</xref>). We therefore sought to mathematically analyze this setting to derive a simple theory of the average learning dynamics that highlights key trade-offs.</p><p>We start by noting that the input to the decision variable <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at each time step is a weighted sum of many random variables, which by the law of large numbers will be approximately Gaussian. We therefore develop a reduction of this model based on an effective Gaussian scalar input distribution. At each time step the input pathway receives a Gaussian input <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>y</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf112"><mml:mi>A</mml:mi></mml:math></inline-formula> parametrizes the signal related to <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the input noise variance <inline-formula><mml:math id="inf114"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> parametrizes irreducible noise in input channels that cannot be rejected. This input is multiplied by a scalar weight <inline-formula><mml:math id="inf115"><mml:mi>u</mml:mi></mml:math></inline-formula>, added to output noise <inline-formula><mml:math id="inf116"><mml:mi>η</mml:mi></mml:math></inline-formula> of variance <inline-formula><mml:math id="inf117"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and sent into the integrating node <inline-formula><mml:math id="inf118"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>,<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where we emphasize that <inline-formula><mml:math id="inf119"><mml:mi>u</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are now both scalar. We may then perform gradient descent on the hinge loss, yielding the update <inline-formula><mml:math id="inf121"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. As expected from the law of large numbers, for the right choice of input signal and parameters <inline-formula><mml:math id="inf122"><mml:mi>A</mml:mi></mml:math></inline-formula> and <italic>c</italic><sub><italic>i</italic></sub>, simulations of this effective Gaussian model closely match the full simulation from pixels, as shown in <xref ref-type="fig" rid="fig3">Figure 3c–f</xref>, pink trace.</p><p>Next, to relate these dynamics to the well-studied DDM framework, we examine behavior when the time step is small (<inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) to obtain a continuous time formulation. In the continuum limit, these discrete within-trial dynamics of the network yield decision variables with identical distributions to a drift-diffusion process with an effective SNR <inline-formula><mml:math id="inf124"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> and normalized threshold <inline-formula><mml:math id="inf125"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>yielding the mean error rate (<inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) and decision time (<inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>)<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Finally, we assume that the learning rate is small (<inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), such that weights change little on any given trial and the gradient dynamics are driven by the mean update,<disp-formula id="equ9">,<label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>⟨</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the average with respect to the distribution of outputs obtained with perceptual weights <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and threshold <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These average dynamics depend in a complex way on the current performance of the network. We compute these average dynamics analytically (see Methods), yielding the continuous time change in effective SNR in the DDM that is equivalent to gradient descent learning in the underlying neural network model. In particular, gradient descent in the RNN is equivalent to the following SNR dynamics in the DDM:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:mfrac></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, time <inline-formula><mml:math id="inf132"><mml:mi>t</mml:mi></mml:math></inline-formula> measures seconds of task engagement (i.e. it measures time passing within a trial as well as intertrial time and any penalty delays after error trials), and <inline-formula><mml:math id="inf133"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the average non-decision task engagement time per trial (where <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the average non-decision task engagement times after correct and error trials). The SNR dynamics depend on five parameters: the time constant <inline-formula><mml:math id="inf136"><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> related to the learning rate, the initial SNR <inline-formula><mml:math id="inf137"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the asymptotic achievable SNR after learning <inline-formula><mml:math id="inf138"><mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, the integration-noise to input-noise variance ratio <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>c</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the choice of threshold <inline-formula><mml:math id="inf140"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over training. We note that the dependence of the dynamics on the choice of threshold <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is implicit in <inline-formula><mml:math id="inf142"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>. The dynamics of this LDDM closely tracks simulated trajectories of the full network from pixels (<xref ref-type="fig" rid="fig3">Figure 3c–f</xref> blue trace, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>; see Methods).</p><p>Remarkably, this reduction shows that the high-dimensional dynamics of the RNN receiving stochastic pixel input and performing gradient descent on the weights (<xref ref-type="fig" rid="fig3">Figure 3</xref>, gray trace) can be described by a DDM with a single deterministic scalar variable – the effective SNR – that changes over time (<xref ref-type="fig" rid="fig3">Figure 3</xref>, blue trace). Notably, without the mapping to the original RNN, it is not possible to understand what effect error-corrective gradient descent learning would have at the level of the DDM, or how the learning process is influenced by choice of decision times. In particular, the change in SNR that arises from gradient descent on the underlying RNN weights (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) is not equivalent to that arising from gradient descent on the SNR parameter in the DDM directly because gradient descent is not parametrization invariant.</p></sec><sec id="s2-3-3"><title>Learning speed trades off with instantaneous reward rate</title><p>The LDDM reveals that learning dynamics depend on the choice of threshold <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on each trial over learning, because threshold impacts both error rate and decision time, which appear in the SNR dynamics of <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>. We next sought to qualitatively understand this relationship. A key prediction of the LDDM is a tension between learning speed and <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, the LS/<inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> trade-off. This tension is clearest early in learning when <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are near 50%. Then the rate of change in SNR is<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the proportionality constant does not depend on <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> (see derivation, Methods). Hence learning speed increases with increasing <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. By contrast, when accuracy is 50% the <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> decreases with increasing <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>When encountering a new task, therefore, agents face a dilemma: they can either harvest a large <inline-formula><mml:math id="inf152"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> or they can learn quickly.</p></sec><sec id="s2-3-4"><title>Learning dynamics depend on threshold policies</title><p>Just as the standard DDM instantiates different decision making strategies as different choices of threshold (for instance aimed at maximizing <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, accuracy, or robustness) (<xref ref-type="bibr" rid="bib42">Holmes and Cohen, 2014</xref>; <xref ref-type="bibr" rid="bib123">Zacksenhouse et al., 2010</xref>), the LDDM instantiates different learning strategies through the choice of threshold trajectory over learning. Threshold affects <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, and through these, the learning dynamics in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>. To consider a range of strategies, we developed four potential threshold policies.</p><p>Constant threshold. This policy implements a fixed constant threshold <inline-formula><mml:math id="inf156"><mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. It serves as a control for behavior that would arise without the ability to modulate decision threshold. Constant thresholds across difficulties have been found to be used as part of near-optimal and presumably cognitively cheaper strategies in humans (<xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>). This policy introduces the parameter <italic>z</italic><sub>0</sub>.</p><p><italic>iRR</italic>-greedy. This policy sets the threshold to the value that maximizes instantaneous reward on each trial, <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>g</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, such that behavior always lies on the OPC. This instantiates a ‘myopic’ strategy that does not consider how threshold can impact long-term learning. This policy is similar to a previously proposed neural network model of rapid threshold adjustment based on reward rate (<xref ref-type="bibr" rid="bib102">Simen et al., 2006</xref>). The policy introduces no parameters.</p><p><italic>iRR</italic>-sensitive. This policy implements a threshold <inline-formula><mml:math id="inf158"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that decays with time constant <inline-formula><mml:math id="inf159"><mml:mi>γ</mml:mi></mml:math></inline-formula> from an initial value <inline-formula><mml:math id="inf160"><mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> toward the <inline-formula><mml:math id="inf161"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal threshold,<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Notably, as the SNR changes due to learning, the target threshold also changes through time. Asymptotically, this policy converges to greedy <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal behavior; however, by starting with a high initial threshold, it can undergo a transient period where responses are slower or faster than <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal, potentially influencing learning. It instantiates a heuristic strategy in which behavior differs from <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal behavior early in learning. This policy introduces two parameters, <italic>z</italic><sub>0</sub> and <inline-formula><mml:math id="inf165"><mml:mi>γ</mml:mi></mml:math></inline-formula>.</p><p>Global optimal. This policy selects the threshold <inline-formula><mml:math id="inf166"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that maximizes total cumulative reward at some known predetermined end to the task <inline-formula><mml:math id="inf167"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>,<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo lspace="0" rspace="0" movablelimits="true">argmax</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We approximately compute this threshold function using automatic differentiation (see Methods). This policy serves as a normative oracle to which behavior may be compared. We note that this optimal policy considers the full time course of learning and is aware of all task parameters such as the duration of total task engagement <inline-formula><mml:math id="inf168"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, asymptotically achievable SNR <inline-formula><mml:math id="inf169"><mml:msup><mml:mi>A</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, etc. In practice these parameters cannot be known before experiencing the task, and so this policy is not an implementable strategy but a normative reference point. The policy introduces no parameters.</p><p>In designing this model, we kept components as simple as possible to highlight key qualitative trade-offs between learning speed and decision strategy. Because of its simplicity, like the standard DDM, it is not meant to quantitatively describe all aspects of behavior. We instead use it to investigate qualitative features of decision making strategy, and expect that these features would be preserved in other related models of perceptual decision making (<xref ref-type="bibr" rid="bib112">Usher and McClelland, 2001</xref><xref ref-type="bibr" rid="bib66">Mazurek et al., 2003</xref>, <xref ref-type="bibr" rid="bib32">Gold and Shadlen, 2007</xref>, <xref ref-type="bibr" rid="bib37">Heekeren et al., 2004</xref><xref ref-type="bibr" rid="bib38">Heekeren et al., 2008</xref><xref ref-type="bibr" rid="bib62">Ma et al., 2006</xref><xref ref-type="bibr" rid="bib11">Brown and Heathcote, 2008</xref>, <xref ref-type="bibr" rid="bib87">Ratcliff and McKoon, 2008</xref>, <xref ref-type="bibr" rid="bib5">Beck et al., 2008</xref>, <xref ref-type="bibr" rid="bib92">Roitman and Shadlen, 2002</xref>; <xref ref-type="bibr" rid="bib81">Purcell et al., 2010</xref><xref ref-type="bibr" rid="bib6">Bejjanki et al., 2011</xref>; <xref ref-type="bibr" rid="bib21">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Fard et al., 2017</xref>).</p></sec></sec><sec id="s2-4"><title>Model reveals that prioritizing learning can maximize total reward</title><p>In order to qualitatively understand how these models behave through time, we visualized their learning dynamics. To approximately place the LDDM task parameters in a similar space to the rats, we performed maximum likelihood fitting using automatic differentiation through the discretized reduction dynamics (see Methods). The four policies we considered clustered into two groups, distinguished by their behavior early in learning. A ‘greedy’ group, which contained just the <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy policy, remained always on the OPC (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), and had fast initial response times (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), a long initial period at high error (<xref ref-type="fig" rid="fig4">Figure 4c</xref>), and high initial <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). By contrast, a ‘non-greedy’ group, which contained the <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive, constant threshold, and global optimal policies, started far above the OPC (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), and had slow initial response times (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), rapid improvements in ER (<xref ref-type="fig" rid="fig4">Figure 4c</xref>), and low <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4d</xref>). Notably, while members of the non-greedy group started off with lower <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, they rapidly surpassed the slow learning group (<xref ref-type="fig" rid="fig4">Figure 4d</xref>) and ultimately accrued more total reward (<xref ref-type="fig" rid="fig4">Figure 4e</xref>). Overall, these results show that threshold strategy strongly impacts learning dynamics due to the learning speed/<inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> trade-off (<xref ref-type="fig" rid="fig4">Figure 4f</xref>), and that prioritizing learning speed can achieve higher cumulative reward than prioritizing instantaneous reward rate.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model reveals rat learning dynamics lead to higher instantaneous reward rate and long-term rewards than greedily maximizing instantaneous reward rate.</title><p>(<bold>a</bold>) Model learning trajectories in speed-accuracy space plotted against the optimal performance curve (OPC) (black). (<bold>b</bold>) Decision time through learning for the four different threshold policies in a. (<bold>c</bold>) Error rate throughout learning for the four different threshold policies in a. (<bold>d</bold>) Instantaneous reward rate as a function of task engagement time for the full learning trajectory and a zoom-in on the beginning of learning (<italic>inset</italic>). (<bold>e</bold>) Cumulative reward as a function of task engagement time for the full learning trajectory and a zoom-in on the beginning of learning (<italic>inset</italic>). Threshold policies: <inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy (green), constant threshold (blue), <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive (orange), and global optimal (red). (<bold>f</bold>) In the speed-accuracy trade-off (<italic>left</italic>), <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (blue) decreases with increasing initial mean <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (green) at high error rates (∼0.5) also decreases with increasing initial mean <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. Thus, at high <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, an agent solves the speed-accuracy trade-off by choosing fast <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that result in higher <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and maximize <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. In the learning speed/ <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> trade-off (<italic>right</italic>), initial learning speed (<inline-formula><mml:math id="inf186"><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, pink) increases with increasing initial mean <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, whereas <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (green) follows the opposite trend. Thus, an agent must trade <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> in order to access higher learning speeds. Plots generated using linear drift-diffusion model (LDDM).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Allowing both drift rate and threshold to vary with learning provides the best drift-diffusion model (DDM) fits.</title><p>(<bold>a</bold>) Deviance information criterion (DIC) for different hierarchical DDM (HDDM) fits to learning during stimulus pair 1 (lower value indicates a better fit). The models were fit to the first 1000 and last 1000 trials for every animal using the HDDM framework (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>). Different parameters were allowed to vary with learning phase while the rest were fixed across learning phase. We fit three simple DDMs, one model that only allowed drift rate variability to vary with learning, three DDMs that included a fixed drift rate variability across learning phase (‘include drift variability’), and three DDMs where drift rate variability varied with learning in addition to different combinations of drift rate and threshold. The best models were those that allowed both drift rate and threshold to vary with learning. Including drift rate variability and allowing it to also vary with learning phase did not improve the model fits. Parameters for these model fits are included in the subsequent figures. (<bold>b</bold>) Same as a but for stimulus pair 2. The models were fit to the last 500 trials of baseline sessions with stimulus pair 1, and the first 500 trials and last 500 trials of stimulus pair 2, with each 500 trial batch serving as a learning phase. As with stimulus pair 1, the best models were those that allowed both drift rate and threshold to vary with learning, and drift rate variability did not appear to allow a better model fit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Simple drift-diffusion model (DDM) fits indicate threshold decreases and drift rate increases during learning.</title><p>(<bold>a</bold>) The learning data from stimulus pair 1 (<bold>a, b, c</bold>) and 2 (<bold>d, e, f</bold>) were fit with a simple DDM using the hierarchical DDM (HDDM) framework (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>) as described in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. The HDDM reports posterior probability estimates for its parameters. The posterior for mean parameters across subjects is on the left of every panel, and the mean of the posterior for every individual fit is on the right of every panel. (<bold>a</bold>) While holding threshold constant, drift increased with learning. (<bold>b</bold>) While holding drift rate constant, threshold decreased with learning. (<bold>c</bold>) When allowing both drift rate and threshold to vary with learning, drift rate increased and threshold decreased with learning. (<bold>d</bold>) For stimulus pair 2, while holding threshold constant, drift increased with learning, matching its value during baseline sessions. (<bold>e</bold>) While holding drift rate constant, threshold decreased with learning, matching its value during baseline sessions. (<bold>f</bold>) When allowing both drift rate and threshold to vary with learning, drift rate increased and threshold decreased with learning, matching their values during baseline sessions. p-Values for mean estimates were calculated by taking the difference of the posteriors and counting the proportion of differences that was, depending on directionality 0. p-Values for individual estimates were calculated by taking a Wilcoxon rank-sum test across pairs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Simple drift-diffusion model (DDM) + fixed drift rate variability fits indicate threshold decreases and drift rate increases during learning.</title><p>(<bold>a</bold>) The learning data from stimulus pair 1 (<bold>a, b, c</bold>) and 2 (<bold>d, e, f</bold>) were fit with a simple DDM + fixed drift rate variability using the hierarchical DDM (HDDM) framework (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>) as described in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. (<bold>a</bold>) While holding threshold constant, drift increased with learning. (<bold>b</bold>) While holding drift rate constant, threshold decreased with learning. (<bold>c</bold>) When allowing both drift rate and threshold to vary with learning, drift rate increased and threshold decreased with learning. Drift rate variability estimates were close to 0. (<bold>d</bold>) For stimulus pair 2, while holding threshold constant, drift increased with learning, matching its value during baseline sessions. (<bold>e</bold>) While holding drift rate constant, threshold decreased with learning, matching its value during baseline sessions. (<bold>f</bold>) When allowing both drift rate and threshold to vary with learning, drift rate increased and threshold decreased with learning, matching their values during baseline sessions. p-Values were calculated as in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Simple drift-diffusion model (DDM) + variable drift rate variability fits indicate threshold decreases and drift rate increases during learning.</title><p>(<bold>a</bold>) The learning data from stimulus pair 1 (<bold>a, b, c</bold>) and 2 (<bold>d, e, f</bold>) were fit with a simple DDM + variable drift rate variability using the hierarchical DDM (HDDM) framework (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>) as described in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. (<bold>a</bold>) While holding threshold constant, drift and drift rate variability increased with learning. (<bold>b</bold>) While holding drift rate constant, threshold and drift rate variability decreased with learning. (<bold>c</bold>) When allowing both drift rate and threshold to vary with learning, drift rate and drift rate variability increased and threshold decreased with learning. (<bold>d</bold>) For stimulus pair 2, while holding threshold constant, drift increased with learning, matching its value during baseline sessions, while drift rate variability trended toward decreasing with stimulus pair 2. (<bold>e</bold>) While holding drift rate constant, threshold decreased with learning, matching its value during baseline sessions, and drift rate variability decreased. (<bold>f</bold>) When allowing both drift rate and threshold to vary with learning, drift rate increased and threshold decreased with learning, matching their values during baseline sessions. Drift rate variability trended toward decreasing with stimulus pair 2. p-Values were calculated as in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Model reveals rat learning dynamics resemble optimal trajectory without relinquishing initial rewards.</title><p>(<bold>a</bold>) Fraction of instantaneous reward rate with respect to the <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy policy for all model threshold policies during learning. The instantaneous reward rates of all policies were normalized by the <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy policy’s instantaneous reward rate through task engagement time. (<bold>b</bold>) Same as a but for the full trajectory of the simulation. (c) Fraction of instantaneous reward rate with respect to the global optimal policy for all model threshold policies during learning. The instantaneous reward rates of all policies were normalized by the greedy policy’s instantaneous reward rate through task engagement time. (<bold>d</bold>) Same as c but for the full trajectory of the simulation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig4-figsupp5-v1.tif"/></fig></fig-group><p>We further analyzed the differences between the three strategies in the non-greedy group. The global optimal policy selects extremely slow initial <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to maximize the initial speed of learning. By contrast, the <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive and constant threshold policies start with moderately slow responses. Nevertheless, we found that these simple strategies accrued 99% of the total reward of the global optimal strategy (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). Hence these more moderate policies, which do not require oracle knowledge of future task parameters, derive most of the benefit in terms of total reward and may reflect a reasonable approach when the duration of task engagement is unknown.</p><p>Considering the rats’ trajectories in light of these strategies, their slow responses early in learning stand in stark contrast to the fast responses of the <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy policy (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, <xref ref-type="fig" rid="fig4">Figure 4a</xref>). Equally, their responses were faster than the extremely slow initial <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of the global optimal model. Both the <inline-formula><mml:math id="inf196"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive and constant threshold models qualitatively matched the rats’ learning trajectory. However, the best DDM parameter fits of the rats’ behavior allowed their thresholds to decrease throughout learning, failing to support the constant threshold model (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s4">4</xref>). Subsequent experiments (Figure 6) provide further evidence against a simple constant threshold strategy. Consistent with substantial improvements in perceptual sensitivity through learning, DDM fits to the rats also showed an increase in drift rate throughout learning (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s4">4</xref>). Similar increases in drift rate have been observed as a universal feature of learning throughout numerous studies fitting learning data with the DDM (<xref ref-type="bibr" rid="bib86">Ratcliff et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Dutilh et al., 2009</xref><xref ref-type="bibr" rid="bib77">Petrov et al., 2011</xref><xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>, <xref ref-type="bibr" rid="bib60">Liu and Watanabe, 2012</xref>, <xref ref-type="bibr" rid="bib124">Zhang and Rowe, 2014</xref>). These qualitative comparisons suggest that rats adopt a ‘non-greedy’ strategy that trades initial rewards to prioritize learning in order to harvest a higher <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> sooner and accrue more total reward over the course of learning.</p></sec><sec id="s2-5"><title>Learning speed scales with reaction time</title><p>To test the central prediction of the LDDM that learning (change in SNR) scales with mean <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, we designed an <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction experiment and studied the effects of the restriction on learning in the rats. Previously trained rats (<inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>) were randomly divided into two groups in which they would have to learn a new stimulus pair while responding above or below their individual mean <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (‘slow’ and ‘fast’) for the previously trained stimulus pair (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Before introducing the new stimuli, we carried out practice sessions with the new timing restrictions to reduce potential effects related to a lack of familiarity with the new regime. After the restriction, <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were significantly different between the two groups (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). In the model, we simulated an <inline-formula><mml:math id="inf203"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction by setting two different <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5c</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Longer reaction times lead to faster learning and higher instantaneous reward rates.</title><p>(<bold>a</bold>) Schematic of experiment and hypothesized results. Previously trained animals were randomly divided into two groups: could only respond above (blue, <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>) or below (black, <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) their individual mean reaction times for the previously trained stimulus and the new stimulus. Subjects responding above their individual mean reaction times were predicted to learn faster, reach a higher instantaneous reward rate sooner and accumulate more total reward. (<bold>b</bold>) Mean and individual reaction times before and after the reaction time restriction in rats. The mean reaction time for subjects randomly chosen to respond above their individual mean reaction times (blue, <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>) was not significantly different to those randomly chosen to respond below their individual means (black, <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) before the restriction (Wilcoxon rank-sum test p &gt; 0.05), but were significant after the restriction (Wilcoxon rank-sum test p &lt; 0.05). Errors represent 95% confidence intervals. (<bold>c</bold>) In the model a long (blue) and a short (black) target decision time were set through a control feedback loop on the threshold, <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with parameter <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>d</bold>) Mean accuracy ±95% confidence interval across sessions for rats required to respond above (blue, <inline-formula><mml:math id="inf211"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>) or below (black, <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) their individual mean reaction times for a previously trained stimulus. Both groups had initial accuracy below chance because rats assume a response mapping based on an internal assessment of similarity of new stimuli to previously trained stimuli. To counteract this tendency and ensure learning, we chose the response mapping for new stimuli that contradicted the rats’ mapping assumption, having the effect of below-chance accuracy at first. * denotes p &lt; 0.05 in two-sample independent <inline-formula><mml:math id="inf213"><mml:mi>t</mml:mi></mml:math></inline-formula>-test. <italic>Inset</italic>: accuracy change (slope of linear fit to accuracy across sessions to both groups, units: fraction per session). * denotes p &lt; 0.05 in a Wilcoxon rank-sum test. (<bold>e</bold>) Mean inferred signal-to-noise ratio (SNR), (<bold>f</bold>) mean, <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and (<bold>g</bold>) mean cumulative reward across task engagement time for new stimulus pair for animals in each group. (<bold>h</bold>) Accuracy, (<bold>i</bold>) SNR, (<bold>j</bold>) <inline-formula><mml:math id="inf215"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, and (<bold>k</bold>) cumulative reward across task engagement time for long (blue) and short (black) target decision times in the linear drift-diffusion model (LDDM).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig5-v1.tif"/></fig><p>We found no difference in initial mean session accuracy between the two groups, followed by significantly higher accuracy in the slow group in subsequent sessions (<xref ref-type="fig" rid="fig5">Figure 5d</xref>). The slope of accuracy across sessions was significantly higher in the slow group (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, inset). Importantly, the fast group had a positive slope and an accuracy above chance by the last session of the experiment, indicating this group learned (<xref ref-type="fig" rid="fig5">Figure 5d</xref>).</p><p>Because of the SAT in the DDM, however, accuracy could be higher in the slow group even with no difference in perceptual sensitivity (SNR) or learning speed simply because on average they view the stimulus for longer during a trial, reflecting a higher threshold. To see if underlying perceptual sensitivity increased faster in the slow group, we computed the rats’ inferred SNR throughout learning (see Methods, <xref ref-type="disp-formula" rid="equ26">Equation 24</xref>), which takes account of the relationship between <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. The SNR of the slow group increased faster (<xref ref-type="fig" rid="fig5">Figure 5e</xref>), consistent with a learning speed that scales with <inline-formula><mml:math id="inf218"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>We found that the slow group had a lower initial <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, but that this <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> exceeded that of the fast group halfway through the experiment (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). Similarly, the slow group trended toward a higher cumulative reward by the end of the experiment (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). The LDDM qualitatively replicates all of our behavioral findings (<xref ref-type="fig" rid="fig5">Figure 5h–k</xref>). These results demonstrate the potential total reward benefit of faster learning, which in this case was a product of enforced slower <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Our experiments and simulations demonstrate that longer <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> lead to faster learning and higher reward for our task setting both in vivo and in silico. Moreover, they are consistent with the hypothesis that rats choose high initial <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in order to prioritize learning and achieve higher <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and cumulative rewards during the task.</p></sec><sec id="s2-6"><title>Rats choose reaction time based on learning prospects</title><p>The previous experiments suggest that rats trade initial rewards for faster learning. Nonetheless, it is unclear how much control rats exert over their <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. A control-free heuristic approach, such as adopting a fixed high threshold (our constant threshold policy), might incidentally appear near optimal for our particular task parameters, but might not be responsive to changed task conditions. If an agent is controlling the reward investment it makes in the service of learning, then it should only make that investment if it is possible to learn.</p><p>To test whether the rats’ <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> modulations were sensitive to learnability, we conducted a new experiment in which we divided rats into a group that encountered new learnable visible stimuli (<inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula>, sessions = 13), and another that encountered unlearnable transparent or near-transparent stimuli (<inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>, sessions = 11) (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). From the perspective of the LDDM, both groups start with approximately zero SNR, however only the group with the visible stimuli can improve that SNR. Because the rats do not know the learnability of new stimuli, we initialize the LDDM with a high threshold to model the belief that any new stimuli may be learnable. If the rats choose their <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> based on how much it is possible to learn, then: (1) rats encountering new stimuli that they can learn will increase their <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to learn quickly and increase future <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. (2) Rats encountering new stimuli that they cannot learn might first increase their <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to learn that there is nothing to learn, but (3) will subsequently decrease <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to maximize <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Rats choose reaction time based on stimulus learnability.</title><p>(<bold>a</bold>) Schematic of experiment: rats trained on stimulus pair 1 were presented with new visible stimulus pair 2 or transparent (alpha = 0, 0.1) stimuli. If rats change their reaction times based on stimulus learnability, they should increase their reaction times for the new visible stimuli to increase learning and future <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and decrease their reaction time to increase <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> for the transparent stimuli. (<bold>b</bold>) Learning across normalized sessions in speed-accuracy space for new visible stimuli (<inline-formula><mml:math id="inf237"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula>, crosses) and transparent stimuli (<inline-formula><mml:math id="inf238"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>, squares). Color map indicates time relative to start and end of the experiment. (<bold>c</bold>) <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive threshold model runs with ‘visible’ (crosses) and ‘transparent’ (squares) stimuli (modeled as containing some signal, and no signal) plotted in speed-accuracy space. The crosses are illustrative and do not reflect any uncertainty. Color map indicates time relative to start and end of simulation. (<bold>d</bold>) Mean change in reaction time across sessions for visible stimuli or transparent stimuli compared to previously known stimuli. Positive change means an increase relative to previous average. <italic>Inset</italic>: first and second half of first session for transparent stimuli. * denotes <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> in permutation test. (<bold>e</bold>) Correlation between initial individual mean change in reaction time (quantity in d) and change in signal-to-noise ratio (SNR) (learning speed: slope of linear fit to SNR per session) for first three sessions with new visible stimuli. <italic>R</italic><sup>2</sup> and <inline-formula><mml:math id="inf241"><mml:mi>p</mml:mi></mml:math></inline-formula> from linear regression in d. Error bars reflect standard error of the mean in b and d. (<bold>f</bold>) Decision time across time engagement time for visible and transparent stimuli runs in model simulation. (<bold>g</bold>) Instantaneous change in SNR (<inline-formula><mml:math id="inf242"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) as a function of initial reaction time (decision time + non-decision time <italic>T</italic><sub>0</sub>) in model simulation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Reaction time analysis of transparent stimuli experiment.</title><p>(<bold>a</bold>) During transparent stimuli, the reaction time (<inline-formula><mml:math id="inf243"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) minimum was relaxed to 0 ms to fully measure a possible shift in <inline-formula><mml:math id="inf244"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> behavior. To be able to ascertain whether transparent stimuli led to a significant change in <inline-formula><mml:math id="inf245"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, the <inline-formula><mml:math id="inf246"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> histogram of transparent stimuli (early [first two sessions]: purple, late [last two sessions]: yellow) sessions was compared to control sessions with visible stimuli (gray) with no <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> minimum. Medians indicated with dashed lines. Kolmogorv-Smirnov two-sample tests over distributions found significant differences (<inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). (<bold>b</bold>) Vincentized <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for transparent and control visible stimuli sessions with no minimum reaction time showed the early transparent sessions were slower than the control sessions, and the late sessions were faster across quantiles.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Vincentized reaction time distributions throughout learning.</title><p>(<bold>a</bold>) Vincentized reaction time distributions for <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> subjects learning stimulus pair 1 (first 3 sessions, purple; last 10 asymptotic sessions, yellow). (<bold>b</bold>) Vincentized reaction time distributions for <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula> subjects learning stimulus pair 2 (first 2 sessions, purple; last 2 sessions, yellow). (<bold>b</bold>) Vincentized reaction time distributions for <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> subjects learning transparent stimuli (first 2 sessions, purple; last 2 sessions, yellow).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig6-figsupp2-v1.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Simple drift-diffusion model (DDM) + variable drift rate variability fits for transparent stimuli.</title><p>(<bold>a</bold>) The learning data from transparent stimuli were fit with a simple DDM + variable drift rate variability using the hierarchical DDM (HDDM) framework (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>). Three learning phases were included: the last 500 trials with control visible stimuli, and the first 500 and the last trials with transparent stimuli. (<bold>a</bold>) We allowed drift rate, threshold, drift rate variability, and <italic>T</italic><sub>0</sub> to vary with learning phase. Drift rate decreased with transparent stimuli, remaining constant throughout. Threshold monotonically decreased with transparent stimuli. Drift rate variability appeared to decrease and stay constant with transparent stimuli, albeit at a value near 0. <italic>T</italic><sub>0</sub> appeared to decrease with transparent stimuli. p-Values were calculated as in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig6-figsupp3-v1.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 4.</label><caption><title>Analysis of stimulus-independent strategies for transparent stimuli.</title><p>In order to measure the extent of stimulus-independent strategies for transparent stimuli, we fit baseline sessions with visible stimuli and sessions with the transparent stimuli with PsyTrack, a flexible generalized linear model (GLM) package for measuring the weights of different inferred psychophysical variables (<xref ref-type="bibr" rid="bib94">Roy et al., 2021</xref>). We fit our data with a model that included bias, win-stay/lose-switch (previous trial outcome), perseverance (previous trial choice), and the actual stimulus as potential explanatory variables for left/right choice behavior. (<bold>a</bold>) Measurement of bias across <inline-formula><mml:math id="inf253"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> animals. Generally, bias and bias variability increased with transparent stimuli compared to visible stimuli. Although not uniform, animals tended to become more biased to the side that they were already biased during visible stimuli. (<bold>b</bold>) During visible stimuli, the stimulus had strong non-zero weights, indicating it influenced choice behavior. Stimulus has positive weights for some animals and negative for others because stimuli mappings were counterbalanced across animals. Win-stay/lose-switch and perseverance weights varied across animals during visible stimuli. Generally, these variables increased weights and variability during transparent stimuli, while the stimulus collapsed to a weight of 0 (as expected, given it was transparent). The weight of the bias variable was omitted for visual clarity as the actual bias was reported in a.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig6-figsupp4-v1.tif"/></fig><fig id="fig6s5" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 5.</label><caption><title>Post-error slowing during rat learning dynamics.</title><p>(<bold>a</bold>) Individual (gray) and mean (black) post-error slowing across first 15 sessions for <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> animals. Post-error slowing was calculated by taking the difference between <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on trials with previous correct trials and previous error trials. A positive difference indicates post-error slowing. (<bold>b</bold>) Individual mean (gray) and population mean (black) post-error slowing for first 2 sessions of learning and last 2 sessions of learning for <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> animals. A Wilcoxon signed-rank test found no significant difference in post-error slowing between the first 2 and last 2 sessions for every animal (p = 0.585). (<bold>c</bold>) Same as in a for <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula> rats, with the addition of 4 baseline sessions with stimulus pair 1 plus the 13 sessions while subjects were learning stimulus pair 2. (<bold>d</bold>) Same as in b but comparing the last 2 baseline sessions with stimulus pair 1 and the first 2 sessions learning stimulus pair 2. A Wilcoxon signed-rank test found no significant difference in post-error slowing when the animals started learning stimulus pair 2 (p = 0.255).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig6-figsupp5-v1.tif"/></fig></fig-group><p>We found that the rats with the visible stimuli qualitatively replicated the same trajectory in speed-accuracy space that we found when rats were trained for the first time (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, <xref ref-type="fig" rid="fig6">Figure 6b</xref>). Indeed, the best DDM fits were those that allowed both threshold and drift rate to vary with learning, as was the case with the first stimuli the rats encountered, and in line with the LDDM (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s4">4</xref>). Because these previously trained rats had already mastered the task mechanics, this result rules out non-stimulus-related learning effects as the sole explanation for long <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at the beginning of learning and supports our hypothesis that the slowdown in <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> was attributable to the rats trying to learn the new stimuli efficiently. We calculated the mean change in <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> (mean Δ<italic>RT</italic>) of new stimuli versus known stimuli. The visible stimuli group had a significant slowdown in <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> lasting many sessions that returned to baseline by the end of the experiment (<xref ref-type="fig" rid="fig6">Figure 6d</xref>, black trace).</p><p>Rats with the transparent stimuli also approached the OPC by decreasing their <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> across sessions to better maximize <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). After a brief initial increase in <inline-formula><mml:math id="inf264"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> in the first half of the first session (<xref ref-type="fig" rid="fig6">Figure 6d</xref>, <italic>inset</italic>), <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> rapidly decreased (<xref ref-type="fig" rid="fig6">Figure 6d</xref>, gray trace). Notably, <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> fell below the baseline <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, indicating a strategy of responding quickly, which approaches <inline-formula><mml:math id="inf268"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal behavior for this zero SNR task. Additionally, we considered the rats’ entire <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> distributions to investigate the effect of learnability beyond <inline-formula><mml:math id="inf270"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> means. We found that while the <inline-formula><mml:math id="inf271"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> distributions changed similarly from the beginning to end of learning for the learnable stimuli (stimulus pair 1 and 2), they differed for the unlearnable (transparent) stimuli, indicating an effect of learnability on the entire <inline-formula><mml:math id="inf272"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> distributions (<xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). Hence, rodents are capable of modulating their strategy depending on their learning prospects.</p><p>Although there is no informative signal in this task with transparent stimuli, the rats could still be using stimulus-independent signals, such as choice history or feedback, to drive heuristic strategies. Indeed, DDM fits indicated a non-zero drift rate even in the absence of informative stimuli (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>). To investigate whether the rats implemented stimulus-independent heuristic strategies in addition to random choice, we measured left/right bias and quantified the weights of bias, perseverance (choose the same port as the previous trial), and win-stay/lose-switch (choose the port that was correct on the previous trial) (<xref ref-type="bibr" rid="bib94">Roy et al., 2021</xref>). In general, bias seemed to increase with transparent stimuli in the direction that each individual was already biased during visible stimuli. Perseverance and win-stay/lose-switch also seemed to increase and fluctuate more during transparent stimuli, suggesting a greater reliance on these heuristics now that the stimulus was uninformative (<xref ref-type="fig" rid="fig6s4">Figure 6—figure supplement 4</xref>). Engaging these heuristics may be a way that the rats expedited their choices in order to maximize <inline-formula><mml:math id="inf273"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> while still ‘monitoring’ the task for any potentially informative changes or patterns. Despite the fact that the animals’ still engaged these non-optimal heuristics, the lack of learnability in the transparent stimuli still led to a change in strategy that was distinct from that with learnable stimuli.</p><p>Importantly, this learnability experiment argues against other simple strategies accounting for the changes in <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. If rats respond more slowly after error trials, a phenomenon known as post-error slowing (PES), they might exhibit slower <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> early in learning when errors are frequent (<xref ref-type="bibr" rid="bib73">Notebaert et al., 2009</xref>). Indeed, we found a slight mean post-error slowing effect of about 50 ms that was on average constant throughout learning, though it was highly variable across individuals (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5</xref>). However, rats viewing transparent stimuli had <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> constrained to 50%, yet their <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> systematically decreased (<xref ref-type="fig" rid="fig6">Figure 6b</xref>), such that post-error slowing alone cannot account for their strategy. Similarly, choosing <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> as a simple function of time since encountering a task would not explain the difference in <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> trajectories between visible and transparent stimuli (<xref ref-type="fig" rid="fig6">Figure 6d</xref>).</p><p>A simulation of this experiment with the <inline-formula><mml:math id="inf280"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive threshold LDDM qualitatively replicated the rats’ behavior (<xref ref-type="fig" rid="fig6">Figure 6c, f and g</xref>). Rodent behavior is thus consistent with a threshold policy that starts with a relatively long <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> upon encountering a new task, and then decays toward the <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal <inline-formula><mml:math id="inf283"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. All other threshold strategies we considered fail to account for the totality of the results. The <inline-formula><mml:math id="inf284"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy strategy – as before – stays pinned to the OPC and speeds up upon encountering the novel stimuli rather than slowing down. The constant threshold strategy fails to predict the speed-up in <inline-formula><mml:math id="inf285"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for the transparent stimuli if we assume constant diffusion noise. This is because when the perceptual signal is small, mean <inline-formula><mml:math id="inf286"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> can be shown to be the squared ratio of threshold to diffusion noise (see Methods). It is thus also possible to explain the speed-up with a constant threshold and increasing diffusion noise. With either interpretation, however, it is clear that a policy where the ratio of threshold to diffusion noise is constant is not compatible with the results. Finally, the global optimal strategy (which has oracle knowledge of the prospects for learning in each task) behaves like the <inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-greedy policy from the start on the transparent stimuli as there is nothing to learn.</p><p>Our <inline-formula><mml:math id="inf288"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction experiment showed that higher initial <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> led to faster learning, a higher <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and more cumulative reward. Consistent with these findings, there was a correlation between initial mean Δ<italic>RT</italic> and initial Δ<italic>SNR</italic> across subjects viewing the visible stimuli, indicating the more an animal slowed down, the faster it learned (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). We further tested these results in the voluntary setting by tracking <inline-formula><mml:math id="inf291"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and cumulative reward for the rats in the learnable stimuli setting with the largest (blue, <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) and smallest (black, <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) ‘self-imposed’ change in <inline-formula><mml:math id="inf294"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). The rats with the largest change started with a lower but ended with a higher mean iRR, and collected more cumulative reward (<xref ref-type="fig" rid="fig7">Figure 7b and c</xref>). Thus, in the voluntary setting there is a clear relationship between <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, learning speed, and its total reward benefits.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Rats that slowed down reaction times the most reached a higher instantaneous reward rate sooner and collected more reward.</title><p>(a) Schematic showing segregation of top 25% of subjects (<inline-formula><mml:math id="inf296"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) with the largest initial Δ<italic>RTs</italic> for the new visible stimuli and the bottom 25% of subjects (<inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>) with the smallest initial Δ<italic>RTs</italic>. Initial Δ<italic>RTs</italic> were calculated as an average of the first two sessions for all subjects. (<bold>b</bold>) Mean <inline-formula><mml:math id="inf298"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> for subjects with largest and smallest mean changes in reaction time across task engagement time. (<bold>c</bold>) Mean cumulative reward over task engagement time for subjects as in b.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-64978-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Summary and limitations</title><p>Our theoretical and empirical results identify a trade-off between the need to learn rapidly and the need to accrue immediate reward in a perceptual decision making task. We find that rats adapt their decision strategy to improve learning speed and approximately maximize total reward, effectively navigating this trade-off over the total period of task engagement. In our experiments, rats responded slowly upon encountering novel stimuli, but only when there was a visual stimulus to learn from. This result indicates that they chose to respond more slowly in order to learn quickly, and only made the investment when learning was possible. This behavior requires foregoing both a cognitively easier strategy – fast random choice – and relinquishing a higher immediately available reward for several sessions spanning multiple days. By imposing different response times in groups of animals, we empirically verified our theoretical prediction that slow responses lead to faster learning and greater total reward in our task. These findings collectively show that rats exhibit cognitive control of the learning process, that is, the ability to engage in goal-directed behavior that would otherwise conflict with default or more immediately rewarding responses (<xref ref-type="bibr" rid="bib20">Dixon et al., 2012</xref>, <xref ref-type="bibr" rid="bib99">Shenhav et al., 2013</xref>; <xref ref-type="bibr" rid="bib100">Shenhav et al., 2017</xref>, <xref ref-type="bibr" rid="bib15">Cohen et al., 1990</xref>; <xref ref-type="bibr" rid="bib16">Cohen and Egner, 2017</xref>).</p><p>Our high-throughput behavioral study with a controlled training protocol permits examination of the entire trajectory of learning, revealing hallmarks of non-greedy decision making. Nonetheless, it is accompanied by several experimental limitations. Our estimation of SNR improvements during learning relies on the DDM. Importantly, while this approach has been widely used in prior work (<xref ref-type="bibr" rid="bib12">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib86">Ratcliff et al., 2006</xref>; <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>; <xref ref-type="bibr" rid="bib24">Drugowitsch et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Petrov et al., 2011</xref> ), our conclusions are predicated on this model’s approximate validity for our task. Future work could address this issue by using a paradigm in which learners with different response deadlines are tested at the same fixed response deadline, equalizing the impact of stimulus exposure at test. This model-free paradigm is not trivial in rodents, because response deadlines cannot be rapidly instructed. Our study also focuses on one visual perceptual task. Further work should verify our findings with other perceptual tasks across difficulties, modalities, and organisms.</p><p>To understand possible learning trajectories, we introduced a theoretical framework based on an RNN, and from this derived an LDDM. The LDDM extends the canonical drift-diffusion framework to incorporate long-term perceptual learning, and formalizes a trade-off between learning speed and instantaneous reward. However, it remains approximate and limited in several ways. The LDDM builds off the simplest form of a DDM, while various extensions and related models have been proposed to better fit behavioral data, including urgency signals (<xref ref-type="bibr" rid="bib19">Ditterich, 2006</xref>; <xref ref-type="bibr" rid="bib14">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib18">Deneve, 2012</xref>; <xref ref-type="bibr" rid="bib35">Hanks et al., 2011</xref>; <xref ref-type="bibr" rid="bib21">Drugowitsch et al., 2012</xref>), history-dependent effects (<xref ref-type="bibr" rid="bib13">Busse et al., 2011</xref>; <xref ref-type="bibr" rid="bib98">Scott et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Akrami et al., 2018</xref>; <xref ref-type="bibr" rid="bib74">Odoemene et al., 2018</xref>; <xref ref-type="bibr" rid="bib79">Pinto et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Lak et al., 2018</xref>; <xref ref-type="bibr" rid="bib67">Mendonça et al., 2018</xref>), imperfect sensory integration (<xref ref-type="bibr" rid="bib12">Brunton et al., 2013</xref>), confidence (<xref ref-type="bibr" rid="bib46">Kepecs et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Lak et al., 2014</xref>; <xref ref-type="bibr" rid="bib24">Drugowitsch et al., 2019</xref>), and multi-alternative choices (<xref ref-type="bibr" rid="bib49">Krajbich and Rangel, 2011</xref>, <xref ref-type="bibr" rid="bib108">Tajima et al., 2019</xref>). Prior work in the DDM framework has investigated learning dynamics with a Bayesian update and constant thresholds across trials (<xref ref-type="bibr" rid="bib24">Drugowitsch et al., 2019</xref>). Our framework uses simpler error-corrective learning rules, and focuses on how the decision threshold policy over many trials influences long-term learning dynamics and total reward. Future work could combine these approaches to understand how Bayesian updating on each trial would change long-term learning dynamics, and potentially, the optimality of different threshold strategies.</p><p>More broadly, it remains unclear whether the drift-diffusion framework in fact underlies perceptual decision making, with a variety of other proposals providing differing accounts (<xref ref-type="bibr" rid="bib32">Gold and Shadlen, 2007</xref>, <xref ref-type="bibr" rid="bib126">Zoltowski et al., 2019</xref>; <xref ref-type="bibr" rid="bib105">Stine et al., 2020</xref>). We speculate that the qualitative learning speed/instantaneous reward rate trade-off that we formally derive in the LDDM would also arise in other models of within-trial decision making dynamics. In addition, on a long timescale over many trials, the LDDM improves performance through error-corrective learning. Future work could investigate learning dynamics under other proposed learning algorithms such as feedback alignment (<xref ref-type="bibr" rid="bib59">Lillicrap et al., 2016</xref>), node perturbation (<xref ref-type="bibr" rid="bib120">Williams, 1992</xref>), or reinforcement learning (<xref ref-type="bibr" rid="bib54">Law and Gold, 2009</xref>). Additionally, the LDDM does not currently include a meta-learning component with which the agent can dynamically gauge the learnability of the task explicitly in order to set its decision threshold. Instead, the LDDM assumes a ‘learnability prior’ implemented as a high initial threshold condition for every new task. This limitation could be solved with a Bayesian observer that predicts learnability based on experience and controls the threshold accordingly. One potential avenue in this direction would be the implementation of the <italic>learned value of control</italic> theory, which provides a mechanism through which an agent can compare stimulus features to those it has encountered in the past in order to determine control allocation (<xref ref-type="bibr" rid="bib58">Lieder et al., 2018</xref>). Moreover, the link between the LDDM and cognitive control is implicit: we interpret the choice of threshold in the DDM as a control process (a higher threshold than is optimal reflects control because it requires foregoing present reward in the service of future reward). Future modeling work should make the choice of control explicit, taking into account the inherent cost of control (<xref ref-type="bibr" rid="bib99">Shenhav et al., 2013</xref>), and then using that choice to determine the decision threshold. Doing so would allow control to not only reflect the choice of threshold, as we have done, but also as a gain term on the drift rate (<xref ref-type="bibr" rid="bib56">Leng et al., 2021</xref>), which may more completely capture control’s role in two-choice decisions.</p></sec><sec id="s3-2"><title>Explore/exploit trade-off</title><p>Conceptually, the learning speed/instantaneous reward rate trade-off is related to the explore/exploit trade-off common in reinforcement learning, but differs in level of analysis. As traditionally framed in reinforcement learning, an agent has the option of maximizing reward based on its current information (exploitation), or of reaching a potentially larger future reward by expanding its current information (exploration). When framed this way, learning is an act of exploration. However, as framed in our study, learning is a systematic, directed strategy (or ‘action’), that is, exploitation, employed in order to maximize total future reward. The reconciliation between these seemingly contradictory accounts occurs at the meta-level: when an agent is aware that learning is the optimal strategy to maximize total future discounted reward, it is exploiting a strategy that trades learning speed for instantaneous reward rate. However, when that agent is not yet aware whether it can learn, then it must explore this question (i.e. meta-learn) before deciding whether it should exploit an explicit learning strategy (‘exploitation of exploration’) that will also come at the cost of instantaneous reward. Although explained sequentially, these two mechanisms can occur in parallel (i.e. an agent constantly probing its learning prospects). One intriguing finding is that state-of-the-art deep reinforcement learning agents, which succeed in navigating the traditional explore/exploit dilemma on complicated tasks like Atari games (<xref ref-type="bibr" rid="bib69">Mnih et al., 2016</xref>), nevertheless fail to learn perceptual decisions like those considered here (<xref ref-type="bibr" rid="bib55">Leibo et al., 2018</xref>). This may be because exploration and exploitation can mean different things depending on the level of analysis, and efficiently learning a perceptual task may require the ‘exploitation of exploration’. Our findings may thus offer routes for improving these artificial systems.</p></sec><sec id="s3-3"><title>Cognitive control</title><p>In order to navigate the learning speed/instantaneous reward rate trade-off, our findings suggest that rats deploy cognitive control of the learning process. Two main features of cognitive control govern its use: it is limited (<xref ref-type="bibr" rid="bib100">Shenhav et al., 2017</xref>), and it is costly (<xref ref-type="bibr" rid="bib50">Krebs et al., 2010</xref>; <xref ref-type="bibr" rid="bib76">Padmala and Pessoa, 2011</xref>, <xref ref-type="bibr" rid="bib47">Kool et al., 2010</xref>, <xref ref-type="bibr" rid="bib20">Dixon et al., 2012</xref>; <xref ref-type="bibr" rid="bib115">Westbrook et al., 2013</xref>; <xref ref-type="bibr" rid="bib48">Kool and Botvinick, 2018</xref>; <xref ref-type="bibr" rid="bib116">Westbrook et al., 2019</xref>). If control is costly, then its application needs to be justified by the benefits of its application. The <italic>expected value of control</italic> (EVC) theory posits that control is allocated in proportion to the EVC (<xref ref-type="bibr" rid="bib99">Shenhav et al., 2013</xref>). Previous work demonstrated that rats are capable of the economic reasoning required for optimal control allocation (<xref ref-type="bibr" rid="bib71">Niyogi et al., 2014a</xref>; <xref ref-type="bibr" rid="bib72">Niyogi et al., 2014b</xref>; <xref ref-type="bibr" rid="bib107">Sweis et al., 2018</xref>). We demonstrated that rats incur a substantial initial instantaneous reward rate opportunity cost to learn the task more quickly, foregoing a cognitively less demanding fast random strategy that would yield higher initial rewards. Rather than optimizing instantaneous reward rate, which has been the focus of prior theories (<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>, <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>; <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>), our analysis suggests that rats approximately optimize total reward over task engagement. Relinquishing initial reward to learn faster, a cognitively costly strategy, is justified by a larger total reward over task engagement. This pattern of behavior matches theoretical predictions of the value of learning based on a recent expansion of the EVC theory (<xref ref-type="bibr" rid="bib65">Masís et al., 2021</xref>).</p><p>Assessing the expected value of learning in a new task requires knowing how much can be learned, how quickly one can learn, and for how long the task will be performed (<xref ref-type="bibr" rid="bib65">Masís et al., 2021</xref>). None of these quantities is directly observable upon first encountering a new task, leading to the question of how rodents know to slow down in one task but not another. Importantly, rats only traded reward for information when learning was possible, a result in line with data demonstrating that humans are more likely to trade reward for information during long experimental time horizons, when learning is more likely (<xref ref-type="bibr" rid="bib121">Wilson et al., 2014</xref>). Monkeys also reduce their reliance on expected value during decision making in order to explore strategically when it is deemed beneficial (<xref ref-type="bibr" rid="bib43">Jahn et al., 2022</xref>). Moreover, previous work has highlighted the explicit opportunity cost of longer deliberation times (<xref ref-type="bibr" rid="bib21">Drugowitsch et al., 2012</xref>), a trade-off that will differ during learning and at asymptotic performance, as we demonstrate here. One possibility is that rats estimate learnability and task duration through meta-learning processes that learn to estimate the value of learning through experience with many tasks (<xref ref-type="bibr" rid="bib27">Finn et al., 2017</xref>; <xref ref-type="bibr" rid="bib114">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Metcalfe, 2009</xref>). The amount of control allocated to learning the current task could be proportional to its estimated value, determined based on similarity to previous learning situations and their reward outcomes and control costs (<xref ref-type="bibr" rid="bib58">Lieder et al., 2018</xref>). Some of this bias for new information, termed curiosity, could be partly endogenous, serving as a useful heuristic for organisms outside of the lab, where rewards are sparse and action spaces are broad (<xref ref-type="bibr" rid="bib33">Gottlieb and Oudeyer, 2018</xref>). Previous observations of suboptimal decision times in humans analogous to those we observed in rats might reflect incomplete learning, or subjects who think they still have more to learn (<xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>; <xref ref-type="bibr" rid="bib10">Bogacz et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Cohen et al., 1990</xref>). Future work could test further predictions emerging from a control-based theory of learning. An agent should assess both the predicted duration of task engagement and the predicted difficulty of learning in order to determine the optimal decision making strategy early in learning, and this can be tested by, for instance, manipulating the time horizon and difficulty of the task. From a control-based perspective, the expected reward from a task is also relevant to control allocation. Indeed, recent work in humans shows that externally motivating learners with the prospect of a test at the end of a task led to a much higher allocation of time on the harder-to-learn items compared to the case when learners were not warned of a test (<xref ref-type="bibr" rid="bib109">Ten et al., 2020</xref>).</p><p>The trend of a decrease in response time and an increase in accuracy through practice – which we observed in our rats – has been widely observed for decades in the skill acquisition literature, and is known as the <italic>Law of Practice</italic> (<xref ref-type="bibr" rid="bib110">Thorndike, 1913</xref>, <xref ref-type="bibr" rid="bib70">Newell and Rosenbloom, 1981</xref>, <xref ref-type="bibr" rid="bib61">Logan, 1992</xref>, <xref ref-type="bibr" rid="bib36">Heathcote et al., 2000</xref>). Accounts of the Law of Practice have posited a cognitive control-mediated transition from shared/controlled to separate/automatic representations of skills with practice (<xref ref-type="bibr" rid="bib80">Posner and Snyder, 1975</xref>, <xref ref-type="bibr" rid="bib101">Shiffrin and Schneider, 1977</xref>, <xref ref-type="bibr" rid="bib15">Cohen et al., 1990</xref>). On this view, control mechanisms are a limited, slow resource that impose unwanted processing delays. Our results suggest an alternative non-mutually exclusive reward-based account for why we may so ubiquitously observe the Law of Practice. Slow responses early in learning may be the goal of cognitive control, as they allow for faster learning, and faster learning leads to higher total reward. When faced with the ever-changing tasks furnished by naturalistic environments, it is the speed of learning which may exert the strongest impact on total reward.</p></sec><sec id="s3-4"><title>Bounded optimality</title><p>More broadly, the optimization of behavior, not in a vacuum, but in the context of one’s constraints – intrinsic and environmentally determined – underlies several general theories of cognition, including theories that explain the allocation of cognitive control (<xref ref-type="bibr" rid="bib99">Shenhav et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Lieder et al., 2018</xref>), the selection of decision heuristics (<xref ref-type="bibr" rid="bib30">Gigerenzer, 2008</xref>), and the rationale of seemingly irrational economic choices (<xref ref-type="bibr" rid="bib45">Kahneman and Tversky, 1979</xref>, <xref ref-type="bibr" rid="bib44">Juechems et al., 2021</xref>). These theories are instances of bounded optimality – a prominent theoretical framework of biological and artificial cognition stating that an agent is optimal when it maximizes reward per unit time within the limitations of its computational architecture (<xref ref-type="bibr" rid="bib95">Russell and Subramanian, 1994</xref>, <xref ref-type="bibr" rid="bib57">Lewis et al., 2014</xref>; <xref ref-type="bibr" rid="bib29">Gershman et al., 2015</xref>; <xref ref-type="bibr" rid="bib34">Griffiths et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Bhui et al., 2021</xref>; <xref ref-type="bibr" rid="bib106">Summerfield and Parpart, 2021</xref>).</p><p>Instances of this framework typically assume that cognitive constraints remain fixed and, more so, that agents do not take alterations of these constraints into account when choosing what to do. There exists, however, a novel theoretical avenue within this framework. An agent can optimize its behavior not only through maximization of reward <italic>within</italic> constraints, but also through the <italic>minimization of those constraints themselves</italic>. If an agent can change itself to minimize its constraints by, for example, improving its perceptual representations through learning, the future reward prospects of doing so should be considered in its current choices, even if it is at the expense of current reward. Intelligent agents, like humans, can and do change themselves through learning in order to improve future reward prospects. Our study formalizes this phenomenon in the context of two-choice perceptual decisions, but much work remains to be done in other contexts, modalities, and organisms.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Behavioral training</title><sec id="s4-1-1"><title>Subjects</title><p>All care and experimental manipulation of animals were reviewed and approved by the Harvard Institutional Animal Care and Use Committee (IACUC), protocol 27–22. We trained animals on a high-throughput visual object recognition task that has been previously described (<xref ref-type="bibr" rid="bib125">Zoccolan et al., 2009</xref>). A total of 44 female Long-Evans rats were used for this study, with 38 included in analyses. Twenty-eight rats (AK1–12 and AL1–16) initiated training on stimulus pair 1, and 26 completed it (AK8 and AL12 failed to learn). Another 8 animals (AM1–8) were trained on stimulus pair 1 but were not included in the initial analysis focusing on asymptotic performance and learning (<xref ref-type="fig" rid="fig1">Figure 1d and e</xref>; <xref ref-type="fig" rid="fig2">Figure 2</xref>) because they were trained after the analyses had been completed. Subjects AM5–8, although trained, did not participate in other behavioral experiments so do not appear in this study. Sixteen animals (AL1–8, AL13–16, and AM1–8) participated in learning stimulus pair 2 (‘new visible stimuli’; canonical-only training regime) while 10 animals (AK1–3, 5–7, 9–12) initially participated in viewing transparent (alpha = 0; AK1, 3, 6, 7, 11) or near-transparent stimuli (alpha = 0.1; AK2, 5, 9, 10, 12), with the subjects sorted randomly into each group. The transparent and near-transparent groups were aggregated but two animals from the near-transparent group were excluded for performing above chance (AK5 and AK12) as this experiment focused on the effects of stimuli that could not be learned. The same 16 animals used for stimulus pair 2 were used for learning stimulus pair 3 under two different reaction time restrictions in which the subjects were sorted randomly. One rat (AL1) was excluded from the outset for not having learned stimulus pair 2. Two additional rats (AL4 and AL7) were excluded for not completing enough trials during practice sessions with the new reaction time restrictions. A final rat (AM1) was excluded because she failed to learn the task. The 12 remaining rats were grouped into seven subjects required to respond above (AL3, AL8, AL13, AL15, AL16, AM3, AM4) and five subjects required to respond below their individual average reaction times (AL2, AL5, AL6, AL14, AM2). Finally, eight rats (AN1–8) were trained on a simplified training regime (‘canonical only’) used as a control for the typical ‘size and rotation’ training object recognition regime (described below). <xref ref-type="table" rid="table1">Table 1</xref> summarizes individual subject participation across behavioral experiments.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Individual animal participation across behavioral experiments.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Animal</th><th align="left" valign="bottom">Sex</th><th align="left" valign="bottom">Stimulus pair 1</th><th align="left" valign="bottom">Stimulus pair 2</th><th align="left" valign="bottom">Transparent stimuli</th><th align="left" valign="bottom">Stimulus pair 3</th></tr></thead><tbody><tr><td align="left" valign="bottom">AK1</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK2</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK3</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.0</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK4</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK5</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.1 (excluded)<xref ref-type="table-fn" rid="table1fn3"><sup>‡</sup></xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK6</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK7</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK8</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation (excluded)<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK9</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK10</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.1</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK11</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.0</td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AK12</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Alpha = 0.1 (excluded)<xref ref-type="table-fn" rid="table1fn3"><sup>‡</sup></xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AL1</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="char" char="." valign="bottom">(Excluded)<xref ref-type="table-fn" rid="table1fn4"><sup>§</sup></xref></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AL2</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom">Below</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AL3</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom">Above</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AL4</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom">Below (excluded) <xref ref-type="table-fn" rid="table1fn5"><sup>¶</sup></xref></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AL5</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom">Below</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">AL6</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Below</td></tr><tr><td align="left" valign="bottom">AL7</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Below (excluded)<xref ref-type="table-fn" rid="table1fn5"><sup>¶</sup></xref></td></tr><tr><td align="left" valign="bottom">AL8</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Above</td></tr><tr><td align="left" valign="bottom">AL9</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation</td></tr><tr><td align="left" valign="bottom">AL10</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation</td></tr><tr><td align="left" valign="bottom">AL11</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation</td></tr><tr><td align="left" valign="bottom">AL12</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation (excluded)<xref ref-type="table-fn" rid="table1fn1">*</xref></td></tr><tr><td align="left" valign="bottom">AL13</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Above</td></tr><tr><td align="left" valign="bottom">AL14</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Below</td></tr><tr><td align="left" valign="bottom">AL15</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Above</td></tr><tr><td align="left" valign="bottom">AL16</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Above</td></tr><tr><td align="left" valign="bottom">AM1</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Below (excluded)<xref ref-type="table-fn" rid="table1fn6">**</xref></td></tr><tr><td align="left" valign="bottom">AM2</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Below</td></tr><tr><td align="left" valign="bottom">AM3</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Above</td></tr><tr><td align="left" valign="bottom">AM4</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="2">Above</td></tr><tr><td align="left" valign="bottom">AM5</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td></tr><tr><td align="left" valign="bottom">AM6</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td></tr><tr><td align="left" valign="bottom">AM7</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td></tr><tr><td align="left" valign="bottom">AM8</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Size and rotation<xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td></tr><tr><td align="left" valign="bottom">AN1</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom" colspan="4">Canonical only</td></tr><tr><td align="left" valign="bottom">AN2</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical ony</td><td align="left" valign="bottom" colspan="3"/></tr><tr><td align="left" valign="bottom">AN3</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="3"/></tr><tr><td align="left" valign="bottom">AN4</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="3"/></tr><tr><td align="left" valign="bottom">AN5</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="3"/></tr><tr><td align="left" valign="bottom">AN6</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="3"/></tr><tr><td align="left" valign="bottom">AN7</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="3"/></tr><tr><td align="left" valign="bottom">AN8</td><td align="left" valign="bottom">F</td><td align="left" valign="bottom">Canonical only</td><td align="left" valign="bottom" colspan="3"/></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p>Failed to learn task.</p></fn><fn id="table1fn2"><label>†</label><p>Not included in initial learning experiment.</p></fn><fn id="table1fn3"><label>‡</label><p>Above chance for near-transparent stimuli.</p></fn><fn id="table1fn4"><label>§</label><p>Failed to learn previous stimuli.</p></fn><fn id="table1fn5"><label>¶</label><p>Not enough practice trials with reaction time restrictions.</p></fn><fn id="table1fn6"><label>**</label><p>Failed to learn stimuli with reaction time restrictions.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4-1-2"><title>Behavioral training boxes</title><p>Rats were trained in high-throughput behavioral training rigs, each made up of four vertically stacked behavioral training boxes. In order to enter the behavioral training boxes, the animals were first individually transferred from their home cages to temporary plastic housing cages that would slip into the behavioral training boxes and snap into place. Each plastic cage had a porthole in front where the animals could stick out their head. In front of the animal in the behavior boxes were three easily accessible stainless steel lickports electrically coupled to capacitive sensors, and a computer monitor (Dell P190S, Round Rock, TX, USA; Samsung 943-BT, Seoul, South Korea) at approximately 40° visual angle from the rats’ location. The three sensors were arranged in a straight horizontal line approximately a centimeter apart and at mouth-height for the rats. The two side ports (L/R) were connected to syringe pumps (New Era Pump Systems, Inc NE-500, Farmingdale, NY, USA) that would automatically dispense water upon a correct trial. The center port was connected to a syringe that was used to manually dispense water during the initial phases of training (see below). Each behavior box was equipped with a computer (Apple Macmini 6,1 running OsX 10.9.5 [13F34] or Macmini 7.1 running OSX El Capitan 10.11.13, Cupertino, CA, USA) running MWorks, an open source software for running real-time behavioral experiments (MWorks 0.5.dev [d7c9069] or 0.6 [c186e7], The MWorks Project <ext-link ext-link-type="uri" xlink:href="https://mworks.github.io/">https://mworks.github.io/</ext-link>). The capacitive sensors (Phidget Touch Sensor P/N 1129_1, Calgary, Alberta, Canada) were controlled by a microcontroller (Phidget Interface Kit 8/8/8P/N 1018_2) that was connected via USB to the computer. The syringe pumps were connected to the computer via an RS232 adapter (Startech RS-232/422/485 Serial over IP Ethernet Device Server, Lockbourne, OH, USA). To allow the experimenter visual access to the rats’ behavior, each box was, in addition, illuminated with red LEDs, not visible to the rats.</p></sec><sec id="s4-1-3"><title>Habituation</title><p>Long-Evans rats (Charles River Laboratories, Wilmington, MA, USA) of about 250 g were allowed to acclimate to the laboratory environment upon arrival for about a week. After acclimation, they were habituated to humans for 1 or 2 days. The habituation procedure involved petting and transfer of the rats from their cage to the experimenter’s lap until the animals were comfortable with the handling. Once habituated to handling, the rats were introduced to the training environment. To allow the animals to get used to the training plastic cages, the feedback sounds generated by the behavior rigs, and to become comfortable in the behavior training room, they were transferred to the temporary plastic cages used in our high-throughput behavioral training rigs and kept in the training room for the duration of a training session undergone by a set of trained animals. This procedure was repeated after water deprivation, and during the training session undergone by the trained animals, the new animals were taught to poke their head out of a porthole available in each plastic cage to receive a water reward from a handheld syringe connected to a lickport identical to the ones in the behavior training boxes in the training rigs. Once the animals reliably stuck their head out of the porthole (1 or 2 days) and accessed water from the syringe, they were moved into the behavior boxes.</p></sec><sec id="s4-1-4"><title>Early shaping</title><p>On their first day in the behavior boxes, rats were individually tutored as follows: Water reward was manually dispensed from the center lickport which is normally used to initiate a trial. When the animal licked the center lickport, a trial began. After a 500 ms tone period, one of two visual objects (stimulus pair 1) appeared on the screen (large front view, degree of visual angle 40°) chosen pseudo-randomly (three randomly consecutive presentations of one stimulus resulted in a subsequent presentation of the other stimulus). This appearance was followed by a 350 ms minimum reaction time that was instituted to promote visual processing of the stimuli. If the animal licked one of the side (L/R) lickports during this time, then the trial was aborted, there would be a minimum intertrial time (1300 ms), and the process would begin again.</p><p>At the time of stimulus presentation, a free water reward was dispensed from the correct side (L/R) lickport. If the animals licked the correct side lickport within the allotted amount of time (3500 ms) then an additional reward was automatically dispensed from that port. This portion of training was meant to begin teaching the animals the task mechanics, that is to first lick the center port, and then one of the two side ports.</p><p>After the rats were sufficiently engaged with the lickports and began self-initiating trials by licking the center lickport (usually 1 to several days, determined by experimenter) no more water was dispensed manually through the center lickport, but the free water rewards from the side lickports were still given. Once the rats were self-initiating enough trials without manual rewards from the center lickport (&gt;200 per session), the free reward condition was stopped, and only correct responses were rewarded.</p></sec><sec id="s4-1-5"><title>Training</title><p>Data collection for this study began once the rats had demonstrated proficiency of the task mechanics (as described above). The training curriculum followed was similar to that by <xref ref-type="bibr" rid="bib125">Zoccolan et al., 2009</xref>. Rats performed the task for about 2 hr daily. Initially, the rats were only presented with large front views (40° visual angle, 0° of rotation) of the two stimuli (stimulus pair 1). Once the rats reached a performance level of ≥70% with these views, the stimuli decreased in size to 15° visual angle in a staircased fashion with steps of 2.5° visual angle. Once the rats reached 15° visual angle, rotations of the stimuli to the left or right were staircased in steps of 5° at a constant size of 30° visual angle. Once the rats reached ±60° of rotation, they were considered to have completed training and were presented with random transformations of the stimuli at different sizes (15°–40° visual angle, step = 15°; 0° of rotation) or different rotations (-60° to +60° of rotation, step = 15°; 30° visual angle). After this point, 10 additional training sessions were collected to allow the animals’ performance to stabilize with this expanded stimulus set.</p><p>During training, there was a bias correction that tracked the animals’ tendency to be biased to one side. If biased, stimuli mapped to the unbiased side were presented for a maximum of three consecutive trials. For example, if the bias correction detected an animal was biased to the right, the left-mapped stimulus would appear three trials at a time in a non-random fashion and the animals’ performance would drop from 50% to 25%, reducing the advantageousness of a biased strategy dramatically. If the animals continued to exhibit bias after one or two sessions of bias correction, then the limit was pushed to five consecutive trials. Once the bias disappeared, stimulus presentation resumed in a pseudo-random fashion.</p><p>The left/right mapping of the stimuli to lickports was counterbalanced across animals, ruling out any effects related left/right stimulus-independent biases, or left/right-independent stimulus bias across animals.</p></sec><sec id="s4-1-6"><title>Training regime comparison</title><p>Although object recognition is supposed to be a fairly automatic process (<xref ref-type="bibr" rid="bib17">Cox, 2014</xref>), it is possible that the 14 possible presentations of each stimulus of stimulus pair 1 (6 sizes at constant rotation, and 8 rotations at constant size) varied in difficulty. To rule out any possible difficulty effects during training and at asymptotic performance, We trained <inline-formula><mml:math id="inf299"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> different rats to asymptotic performance on the task but only on large, front views of the visual objects (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). We compared the learning and asymptotic performance of the ‘size and rotation’ cohort and the ‘canonical only’ cohort across a wide range of behavioral measures. During learning, animals in both regimes followed similar learning trajectories in speed-accuracy space (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>), and clustered around the OPC at asymptotic performance (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c</xref>). Comparisons of accuracy, reaction time, and fraction maximum instantaneous reward rate trajectories during learning and at averages asymptotic performance revealed no detectable differences (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1d—f</xref>). Total trials per session, and voluntary intertrial intervals after error trials did show slightly varied trajectories during learning, though there were no differences in their means after learning (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1g, h</xref>). The difference in total trials per session could be unrelated to the difference in training regimes. The difference in voluntary intertrial intervals, however, could be related to the introduction of different sizes and rotations: a sudden spike in this metric is seen about halfway through normalized sessions and decays over time. If this is the case, it is a curious result that rats choose to display their purported ‘surprise’ in between trials, and not during trials, as we found no difference in the reaction time trajectories. Both training regimes had overlapping fraction trials ignored metrics during learning, with a sharp decrease after the start, and a small significant difference in their number at asymptotic performance (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1i</xref>). We point out the fact that we do not consider voluntary intertrial intervals nor ignored trials in our analysis, so the differences between the regimes do not affect our conclusions. Overall, these results suggest that there is not a measurable or relevant difficulty effect based on our training regime with a variety of stimulus presentations.</p></sec><sec id="s4-1-7"><title>Stimulus learnability experiment</title><p>Transparent stimuli. In order to assess how animals behaved in a scenario with non-existent learning potential, a subset of already well-trained animals were presented with transparent (<inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, alpha = 0) or near-transparent (<inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, alpha = 0.1) versions of the familiar stimulus pair 1 for a duration of 11 sessions. Before these sessions, 4 sessions with stimulus pair 1 at full opacity (alpha = 1) were conducted to ensure animals could perform the task adequately before the manipulation. We predicted that the near-transparent condition would segregate animals into two groups, those that could perform the task and those that could not, based on each individual’s perceptual ability. The animals in the near-transparent condition that remained around chance performance (<inline-formula><mml:math id="inf302"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, rat AK2, AK9, and AK10) were grouped with the animals from the transparent condition, while those that performed well above chance (<inline-formula><mml:math id="inf303"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, rat AK5 and AK12) were excluded.</p><p>Reaction times were predicted to decrease during the course of the experiment, so to measure the change most effectively, the minimum reaction time requirement of 350 ms was removed. However, removing the requirement could lead to reduced reaction times regardless of the presented stimuli. To be able to measure whether the transparent stimuli led to a significant difference in reaction times compared to visible stimuli, we ran sessions with visible stimuli with no reaction time requirement for the same animals and compared these reaction times with those from the transparent condition. We found that the aggregate reaction time distributions were significantly different (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a</xref>). A comparison of vincentized reaction times revealed that there was a significant difference in the fastest reaction time decile (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1b</xref>), confirming that reaction times decreased significantly during presentation of transparent stimuli.</p><p>New visible stimuli. In order to assess how animals behaved in a scenario with high learning potential, a subset (<inline-formula><mml:math id="inf304"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math></inline-formula>) of already well-trained animals on stimulus pair 1 were presented with a never before seen stimulus pair (stimulus pair 2) for a duration of 13 sessions. Before these sessions, 5 sessions with the familiar stimulus pair 1 were recorded immediately preceding the stimulus pair 2 sessions in order to compare performance and reaction time after the manipulation for every animal. Previous pilot experiments showed that the animals immediately assigned a left/right mapping to the new stimuli based on presumed similarity to previously trained stimulus pair, so in order to enforce learning, the left/right mapping contrary to that predicted by the animals in the pilot tests was chosen. Because of this, animals typically began with an accuracy below 50%, as they first had to undergo reversal learning for their initial mapping assumptions. Because the goal of this experiment was to measure effects during learning and not demonstrate invariant object recognition, the new stimuli were presented in large front views only (visual angle = 40°, rotation = 0°).</p></sec></sec><sec id="s4-2"><title>Behavioral data analysis</title><sec id="s4-2-1"><title>Software</title><p>Behavioral psychophysical data was recorded using the open-source MWorks 0.5.1 and 0.6 software (<ext-link ext-link-type="uri" xlink:href="https://mworks.github.io/downloads/">https://mworks.github.io/downloads/</ext-link>). The data were analyzed using Python 2.7 with the pymworks extension. We employed the hierarchical estimation of the DDM in python (HDDM) package for DDM fits (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>). To measure stimulus-independent psychophysical strategies such as bias and perseverance, we employed PsyTrack, a generalized linear model package for fitting dynamic psychophysical models to behavioral data (<xref ref-type="bibr" rid="bib94">Roy et al., 2021</xref>) in conjunction with Python 3.8.</p></sec><sec id="s4-2-2"><title>DDM fit</title><p>In order to verify that our behavioral data could be modeled as a drift-diffusion process, the data were fit with an HDDM (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>), permitting subsequent analysis (such as comparison to the OPC) based on the assumption of a drift-diffusion process. To verify that a DDM was appropriate for our data, we fit a simple DDM to 10 asymptotic sessions after learning stimulus pair 1 for <inline-formula><mml:math id="inf305"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> subjects (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). In order to assess parameter changes across learning, we fit DDMs to the stimulus pair 1 experiment and the stimulus pair 2 experiment where the learning epochs were treated as conditions in each experiment. This allowed us to hold some parameters constant while conditioning others on learning. We fit both simple DDMs and DDMs with drift rate variability to the two experiments, allowing drift rate, threshold, and drift rate variability to vary with learning epoch. In particular, we fit three broad types of models: (1) simple DDMs (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), (2) DDMs + fixed drift rate variability (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>), and (3) DDMs + drift rate variability that varied freely with learning epoch (<xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). For each of the types of models we held drift constant, threshold constant, or allowed both to vary with learning. The best fits, as determined by the deviance information criterion (DIC), came from models where we allowed both drift and threshold to vary with learning; the addition of drift rate variability did not appear to improve model fits (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). For both learning experiments, drift rates increased and thresholds decreased by the end of learning, in agreement with previous findings (<xref ref-type="bibr" rid="bib19">Ditterich, 2006</xref>; <xref ref-type="bibr" rid="bib86">Ratcliff et al., 2006</xref>; <xref ref-type="bibr" rid="bib25">Dutilh et al., 2009</xref>; <xref ref-type="bibr" rid="bib4">Balci et al., 2011b</xref>; <xref ref-type="bibr" rid="bib60">Liu and Watanabe, 2012</xref>; <xref ref-type="bibr" rid="bib124">Zhang and Rowe, 2014</xref>). In addition, for the transparent stimuli experiment we fit a DDM that allowed drift rate, threshold, drift rate variability, and <italic>T</italic><sub>0</sub> to vary with learning phase in order to observe the changes in drift rate and threshold (<xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>).</p></sec><sec id="s4-2-3"><title>PsyTrack fit</title><p>In order to estimate stimulus-independent psychophysical strategies in the transparent stimulus experiment (<xref ref-type="fig" rid="fig6s4">Figure 6—figure supplement 4</xref>), we used PsyTrack to fit a generalized linear model to our behavioral data (<xref ref-type="bibr" rid="bib94">Roy et al., 2021</xref>). The model assigns weights to user-determined input variables to explain the output variable. The output variable consists of a vector for left/right choices on every trial for an individual subject, where left = 0 and right = 1 (or 1 and 2). PsyTrack automatically calculates weights on bias, with a positive weight indicating a rightward bias, and a negative weight indicating a leftward bias. We fit the model by providing it with three explanatory input variables: stimulus, perseverance, and win-stay/lose-switch. For the input variables, the left/right coding differed from that for the output variable as per the model documentation (left = -1, right = +1). The stimulus variable indicated the stimulus that appeared on that trial (stimulus A = -1, stimulus B = +1). However, the left/right mapping of these stimuli was counterbalanced across subjects, so depending on the mapping, subjects could have a strong positive or negative weight, both indicating the stimuli explained choices. The perseverance variable indicated the left/right location of the subject’s choice on the previous trial. The win-stay/lose-switch variable indicated the location of the correct choice on the previous trial. For both perseverance and win-stay/lose-switch, positive weights indicate the predicted presence of perseverance and win-stay/lose-switch rather than left/right information.</p></sec><sec id="s4-2-4"><title>Behavioral metrics</title><p>Error rate (<inline-formula><mml:math id="inf306"><mml:mrow><mml:mi>E</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) was calculated by dividing the number of error trials by the number of total trials (error + correct) within a given window of trials in a full behavioral training session. Accuracy was calculated as <inline-formula><mml:math id="inf307"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="equ15"><label>(13)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(14)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:mtext>accuracy</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Reaction time (<inline-formula><mml:math id="inf308"><mml:mrow><mml:mi>R</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) for one trial was measured by subtracting the time of the first lick on a response lickport from the stimulus onset time on the computer monitor. Mean <inline-formula><mml:math id="inf309"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> was calculated by averaging reaction times across trials within a given window of trials or the trials in a full behavioral training session.<disp-formula id="equ17"><label>(15)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Vincentized reaction time is one method to report aggregate reaction time data meant to preserve individual distribution shape and be less sensitive to outliers in the group distribution (<xref ref-type="bibr" rid="bib84">Ratcliff, 1979</xref>, <xref ref-type="bibr" rid="bib8">Blokland, 1998</xref>), although some scientists have argued parametric fitting (with an ex-Gaussian distribution, for example) and parameter averaging across subjects outperforms Vincentizing as sample size increases (<xref ref-type="bibr" rid="bib93">Rouder and Speckman, 2004</xref>; <xref ref-type="bibr" rid="bib117">Whelan, 2008</xref>). Each subject’s reaction time distribution is divided into quantiles (e.g. deciles; similar to percentile, but between 0 and 1), and then the quantiles across subjects are averaged.</p><p>Decision time (<inline-formula><mml:math id="inf310"><mml:mrow><mml:mi>D</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) for one trial was measured by subtracting the non-decision time <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Estimating <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) from <inline-formula><mml:math id="inf313"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. Mean <inline-formula><mml:math id="inf314"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> was calculated by subtracting <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from the mean <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> across trials within a given window of trials or the trials in a full behavioral training session.<disp-formula id="equ18">.<label>(16)</label><mml:math id="m18"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Post-error and correct non-decision task engagement times (<inline-formula><mml:math id="inf317"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf318"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>o</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) are defined (for notational simplicity) as the sum of non-decision time <italic>T</italic><sub>0</sub> and the experimentally determined response-to-stimulus times after error <inline-formula><mml:math id="inf319"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and correct trials <inline-formula><mml:math id="inf320"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Please see Determining <inline-formula><mml:math id="inf321"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf322"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo mathvariant="normal" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>o</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for how we determined these experimental variables.<disp-formula id="equ19"><label>(17)</label><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ20">.<label>(18)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Mean normalized decision time (<inline-formula><mml:math id="inf323"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo mathvariant="normal">/</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) was measured by dividing mean <inline-formula><mml:math id="inf324"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> by <inline-formula><mml:math id="inf325"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the sum of the non-decision time <italic>T</italic><sub>0</sub> and <inline-formula><mml:math id="inf326"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the mean non-decision task engagement time in an error trial (see <inline-formula><mml:math id="inf327"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf328"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p><p>Mean difference in mean reaction time (<inline-formula><mml:math id="inf329"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>R</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) was calculated by subtracting the mean reaction time of a number of baseline sessions from the mean reaction time of an experimental session. A positive difference indicates an increase over baseline mean reaction time. The mean of the two immediately preceding sessions with stimulus pair 1 were subtracted from the mean reaction time of every session with stimulus pair 2 or transparent stimuli for every animal individually (<xref ref-type="fig" rid="fig6">Figure 6d and e</xref>). These differences were then averaged to get a mean difference in mean reaction time <inline-formula><mml:math id="inf330"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Mean instantaneous reward rate (<inline-formula><mml:math id="inf331"><mml:mrow><mml:mi>i</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>R</mml:mi><mml:mo class="ltx_font_smallcaps" mathvariant="normal">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) (regularly referred to as just reward rate, <inline-formula><mml:math id="inf332"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) is defined as mean accuracy per mean time per trial (<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>):<disp-formula id="equ21"><label>(19)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We define the average non-decision task engagement time per trial,<disp-formula id="equ22"><label>(20)</label><mml:math id="m22"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The mean instantaneous reward rate is then (see Equation A26 in <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>)<disp-formula id="equ23">.<label>(21)</label><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>N.B. Because our study hinges on the important difference between present, future and cumulative rewards and tracks changes in <inline-formula><mml:math id="inf333"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf334"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> over learning, we write what is traditionally referred to as reward rate <inline-formula><mml:math id="inf335"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> as instantaneous reward rate <inline-formula><mml:math id="inf336"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> to emphasize these differences. Because <inline-formula><mml:math id="inf337"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf338"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> can change throughout learning, reward rate as traditionally defined only captures an ‘instant’ in a learning trajectory.</p><p>Mean total correct trials is a model-free measure of the reward attained by the animals within a given window of trials. Every correct response yields an identical water reward, hence, reward can be counted by counting correct responses across trials. For one subject <italic>a</italic>∈ [1, 2, 3,…, <italic>k</italic>], total correct trials at trial <italic>n</italic> are the sum of correct trials up to trial <italic>n</italic>:<disp-formula id="equ24">,<label>(22)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf339"><mml:msubsup><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup></mml:math></inline-formula> is an element in a vector <inline-formula><mml:math id="inf340"><mml:msup><mml:mi>o</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:math></inline-formula> containing the outcomes of those trials <inline-formula><mml:math id="inf341"><mml:mrow><mml:msup><mml:mi>o</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>o</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo rspace="7.5pt">,</mml:mo><mml:msubsup><mml:mi>o</mml:mi><mml:mn>2</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo rspace="7.5pt">,</mml:mo><mml:msubsup><mml:mi>o</mml:mi><mml:mn>3</mml:mn><mml:mi>a</mml:mi></mml:msubsup><mml:mo rspace="7.5pt">,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo rspace="7.5pt">,</mml:mo><mml:msubsup><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For correct and error responses  <inline-formula><mml:math id="inf342"><mml:msubsup><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msubsup></mml:math></inline-formula> = 1 and 0 respectively (e.g.<inline-formula><mml:math id="inf343"><mml:msubsup><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msubsup></mml:math></inline-formula> = [0, 0, 1, 1, 0, …, 1]).</p><p>Mean total correct trials up to trial <italic>n</italic> is calculated by taking the average of total correct trials across all animals <italic>k</italic> up to trial <italic>n</italic>.<disp-formula id="equ25">.<label>(23)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Mean cumulative reward is a measure of the reward attained by the animals within a given window of trials. To calculate this quantity, a moving average of <inline-formula><mml:math id="inf344"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and accuracy for a given window size are first calculated for every animal individually. To avoid averaging artifacts, only values a full window length from the beginning are considered. Given these moving averages, <inline-formula><mml:math id="inf345"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> is then calculated for every animal and subsequently averaged across animals to get a moving average of mean reward rate. To calculate the mean cumulative reward, a numerical integral over a particular task time, such as <italic>task engagement time</italic> (see Measuring task time), is then calculated using the composite trapezoidal rule.</p><p>SNR is a measure of an agent’s perceptual ability in a discrimination task. Given an animal’s particular <inline-formula><mml:math id="inf346"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf347"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, we use an equation to infer its SNR <inline-formula><mml:math id="inf348"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> deduced from standard DDM equations to infer its SNR (Equation 56 in <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>):<disp-formula id="equ26">.<label>(24)</label><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The SNR equation defines a U-shaped curve that increases as <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> move away from 0.5. For cases early in learning where <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were below 0.5 because of potential initial biases, we assumed the inferred SNR was negative (meaning the animals had to unlearn the biases in order to learn, and thus had a monotonically increasing SNR during learning).</p><p>SNR performance frontier is a measure of an agent’s possible error rate and reaction time combinations based on their current perceptual ability. Because of the SAT, not all combinations of <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf352"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> are possible. Instead, performance is bounded by an agent’s SNR <inline-formula><mml:math id="inf353"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> at any point in time, and their particular (<inline-formula><mml:math id="inf354"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf355"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) combination will depend on their choice of threshold.</p><p>Given a fixed <inline-formula><mml:math id="inf356"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (as in the case of our experiment), this bound exists in the form of a performance frontier – the combination of all resultant <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and mean normalized <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> possible given a fixed SNR <inline-formula><mml:math id="inf359"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> and all possible thresholds <inline-formula><mml:math id="inf360"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>.</p><p>We can use <inline-formula><mml:math id="inf361"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>infer</mml:mtext></mml:msub></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ26">Equation 24</xref>) to calculate its performance frontier for a range of thresholds <inline-formula><mml:math id="inf362"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>∈</mml:mo></mml:math></inline-formula> [0, <inline-formula><mml:math id="inf363"><mml:mi mathvariant="normal">∞</mml:mi></mml:math></inline-formula>) with standard equations from the DDM:<disp-formula id="equ27"><label>(25)</label><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ28">.<label>(26)</label><mml:math id="m28"><mml:mrow><mml:mi>D</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For every performance frontier there will be one unique (<inline-formula><mml:math id="inf364"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>infer</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf365"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>infer</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula>) combination for which reward rate will be greatest, and it will lie on the OPC.</p><p>Fraction maximum instantaneous reward rate is a measure of distance to the OPC, that is, optimal performance. Given an animal’s <inline-formula><mml:math id="inf366"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf367"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, we inferred their SNR and calculated their performance frontier as described above. We then divided the animal’s reward rate by the maximum reward rate on their performance frontier, corresponding to the point on the OPC they could have attained given their inferred SNR <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ29">. <label>(27)</label><mml:math id="m29"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>fraction max</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mtext>infer</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Maximum instantaneous reward rate opportunity cost, like fraction maximum instantaneous reward rate, is also measure of distance to the OPC, that is, optimal performance, but it emphasizes the reward rate fraction given up by the subject given its current <inline-formula><mml:math id="inf369"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> combination along its SNR performance frontier. It is simply:<disp-formula id="equ30">.<label>(28)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mtext>opportunity cost</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mtext>fraction max</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Mean post-error slowing is a metric to account for the potential policy of learning by slowing down after error trials. In order to quantify the amount of post-error slowing in a particular subject, the subject’s reaction times in a session are segregated into correct trials following an error, and correct trials following a correct choice, and separately averaged. The difference between these indicates the degree of post-error slowing present in that subject during that session.<disp-formula id="equ31">.<label>(29)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:mtext>post-error slowing</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mtext>post-error correct trials</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mtext>post-correct correct trials</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The mean post-error slowing for one session is thus the mean of this quantity across all subjects <italic>k</italic>.<disp-formula id="equ32">.<label>(30)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mi>S</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>k</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Left/right bias measures the extent to which a subject is biased to the left or right lickport regardless of the stimulus presented. For every individual, left or right choices for every trial are coded as a binary vector (left = 0, right = 1). The correct response side is also coded as a binary vector. Bias is calculated by taking the difference of these vectors. A Gaussian filter is then applied to smooth the bias vector over time, with negative numbers reflecting a left bias and positive numbers reflecting a right bias.</p></sec><sec id="s4-2-5"><title>Computing error</title><p>Within-subject session errors (e.g. <xref ref-type="fig" rid="fig1">Figure 1d</xref>) for accuracy and reaction times were calculated by bootstrapping trial outcomes and reaction times for each session. We calculated a bootstrapped standard error of the mean by taking the standard deviation of the distribution of means from the bootstrapped samples. A 95% confidence interval can be calculated from the distribution of means as well.</p><p>Across-subject session errors (e.g. <xref ref-type="fig" rid="fig6">Figure 6d</xref>) were computed by calculating the standard error of the mean of individual animal session means.</p><p>Across-subject sliding window errors (e.g. <xref ref-type="fig" rid="fig6">Figure 6b</xref>; <xref ref-type="fig" rid="fig7">Figure 7b</xref>) were calculated by averaging trials over a sliding window (e.g. 200 trials) for each animal first, then taking the standard error of the mean of each step across animals. Alternatively, the average could be taken across a quantile (e.g. first decile, second decile, etc.), and then the standard error of the mean of each quantile across animals was computed.</p></sec><sec id="s4-2-6"><title>Measuring task time</title><p>Trials are the smallest unit of behavioral measure in the task and are defined by one stimulus presentation accompanied by one outcome (correct, error) and one reaction time.</p><p>Sessions are composed of as many trials as an animal chooses to complete within a set window of wall clock time, typically around 2 hr once daily. An error rate (fraction of error trials over total trials for the session) and a mean reaction time can be calculated for a session.</p><p>Normalized sessions are a group of sessions (e.g. 1, 2, 3, …, 10) where a particular session’s normalized index corresponds to its index divided by the total number of sessions in the group (e.g. 0.1, 0.2, 0.3, …, 1.0). Because animals may take different numbers of sessions to learn to criterion, a normalized index for sessions allows better comparison of psychophysical measurements throughout learning.</p><p>Stimulus viewing time measures the time that the animals are viewing the stimulus, defined as the sum of all reaction times up to trial <italic>n</italic> as:<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>stimulus viewing time</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ34">.<label>(32)</label><mml:math id="m34"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>task engagement time</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow><mml:mtext> </mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The sum of reaction times up to trial <italic>n</italic> plus the sum of  <inline-formula><mml:math id="inf371"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 3136 ms and  <inline-formula><mml:math id="inf372"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> = 6370 ms, the mandatory post-error and post-correct response-to-stimulus intervals, proportional to the number of error and correct trials (<italic>n</italic>=<italic>n</italic><sub><italic>corr</italic></sub> + <italic>n<sub>nerr</sub></italic>).</p></sec><sec id="s4-2-7"><title>Statistical analyses</title><p><xref ref-type="fig" rid="fig1">Figure 1d</xref>: We wished to test whether the mean fraction maximum reward rate of our subjects over the 10 sessions after having completed training were significantly different from optimal performance. A Shapiro-Wilk test failed to reject (p&lt;0.05) a null hypothesis for normality for 18/26 subjects, with the following p-values (from left to right): (0.8162, 0.1580, 0.3746, 0.6985, 0.0025, 0.0467, 0.0040, 0.6522, 0.0109, 0.1625, 1.8178e-05, 0.0901, 0.7606, 0.0295, 0.0009, 0.2483, 0.5627, 0.0050, 0.4464, 0.6839, 0.5953, 0.0140, 0.1820, 0.1747, 0.6385, 0.2304). Thus, we conducted a one-sided Wilcoxon signed-rank test on our sample against 0.99, testing for the evidence that each subject’s mean fraction max reward rate was greater than 99% of the maximum (p&lt;0.05), and obtained the following p-values (from left to right): (0.0025, 0.0025, 0.0025, 0.1013, 0.2223, 0.0063, 0.0047, 0.0025, 0.0025, 0.0025, 0.0025, 0.0571, 0.6768, 0.0047, 0.7125, 0.0372, 0.8794, 0.4797, 0.7125, 0.8987, 0.0372, 0.0109, 0.9975, 0.9766, 0.9917, 0.9975).</p><p><xref ref-type="fig" rid="fig5">Figure 5b</xref>: We wished to test the difference in mean <inline-formula><mml:math id="inf373"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> between two randomly chosen groups of animals before and after an <inline-formula><mml:math id="inf374"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction to assess the effectiveness of the restriction. A Shapiro-Wilk test did not support an assumption of normality for the ‘below’ group in either condition resulting in the following (W statistic, p-value) for the pre-<inline-formula><mml:math id="inf375"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction ‘above’ and ‘below’ groups and post-RT restriction ‘above’ and ‘below’ groups: (0.9073, 0.3777), (0.6806, 0.0059), (0.8976, 0.3168), (0.6583, 0.0033). Hence, we conducted a Wilcoxon rank-sum test for the pre- and post-<inline-formula><mml:math id="inf376"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction groups and found the pre-<inline-formula><mml:math id="inf377"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction group was not significant (p=0.570) while the post-<inline-formula><mml:math id="inf378"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> restriction group was (p=0.007), indicating the two groups were not significantly different before the RT restriction, but became significantly different after the restriction.</p><p><xref ref-type="fig" rid="fig5">Figure 5d</xref>: We wished to test the difference in accuracy between the ‘above’ and ‘below’ groups for every session of stimulus pair 3. A Shapiro-Wilk test failed to reject the assumption of normality (p&lt;0.05) for any session from either condition (except session 4, ‘above’, which could be expected given there were 16 tests), with the following (W statistic, p-value) for [session: ‘above’, ‘below’] by session: [1: (0.9340, 0.6240),(0.8959, 0.3068)], [2: (0.9381, 0.6522), (0.8460, 0.1130)], [3: (0.9631, 0.8291), (0.9058, 0.3676)], [4: (0.7608, 0.0374), (0.9728, 0.9177)], [5: (0.8921, 0.3680), (0.9779, 0.9486)], [6: (0.7813, 0.0565), (0.9702, 0.9002)], [7: (0.8942, 0.3786), (0.9711, 0.9062)], [8: (0.7848, 0.0605), (0.9611, 0.8280)].</p><p>A Levene test failed to reject the assumption of equal variances for every pair of sessions except the first (statistic, p-value): (6.3263, 0.0306), (2.2780, 0.1621), (1.2221, 0.2948), (0.8570, 0.3764), (2.7979, 0.1253), (0.7364, 0.4109), (0.0871, 0.7739), (0.0088, 0.9269).</p><p>Hence, we performed a two-sample independent <italic>t</italic>-test for every session with the following p-values: (0.4014, 0.04064, 0.0057, 0.0038, 0.0011, 0.0038, 0.0006, 6.3658e-05).</p><p>We also wished to test the difference between the slopes of linear fits to the accuracy curves for both conditions. A Shapiro-Wilk test failed to reject the assumption of normality (p&lt;0.05) for either condition, with the following (W statistic, p-value) for ‘above’ and ‘below’: (0.8964, 0.3095), (0.8794, 0.3065). A Levene test failed to reject the assumption of equal variances (p&lt;0.05) for each condition (statistic, p-value): (0.2141, 0.6535). Hence, we performed a two-sample independent <italic>t</italic>-test and found a significant difference (p=0.0027).</p><p><xref ref-type="fig" rid="fig6">Figure 6d</xref>: We wished to test whether the animals had significantly changed their session mean <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with respect to their individual previous baseline <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (paired samples). To do this, we conducted a permutation test for every session with the new visible stimuli (stimulus pair 2) or the transparent stimuli. For 1000 repetitions, we randomly assigned labels to the experimental or baseline <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and then averaged the paired differences. The p-value for a particular session was the fraction of instances where the average permutation difference was more extreme than the actual experimental difference. For sessions with stimulus pair 2, the p-values from the permutation test were: (0.0034, 0.0069, 0.0165, 0.0071, 0.0291, 0.0347, 0.06, 0.0946, 0.3948, 0.244, 0.244, 0.4497, 0.3437). For sessions with transparent stimuli (plus rats AK2, AK9, and AK10 from the near-transparent stimuli), the p-values from the permutation were (0.0859375, 0.44921875, 0.15625, 0.03125, 0.02734375, 0.015625, 0.26953125, 0.02734375, 0.03125, 0.01953125, 0.0546875). To investigate whether the animals’ significantly slowed down their mean <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> compared to baseline during the first session of transparent stimuli, we divided <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the first session in half and ran a permutation test on each half with the following p-values: (0.0390625, 0.2890625).</p><p><xref ref-type="fig" rid="fig6">Figure 6e</xref>: In order to test the correlation between the initial change in <inline-formula><mml:math id="inf384"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and the initial change in SNR for stimulus pair 2, we ran a standard linear regression on the average per subject for each of these variables for the first three sessions of stimulus pair 2 with  <inline-formula><mml:math id="inf385"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> = 0.38 and p-value = 0.01.</p><p><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1d—i</xref>: Statistical significance of differences in means between the two training regimes for a variety of psychophysical measures was determined by a Wilcoxon rank-sum test with p&lt;0.05. The p-values were: (<bold>d</bold>) accuracy: 0.21, (<bold>e</bold>) reaction time: 0.81, (<bold>f</bold>) fraction max <inline-formula><mml:math id="inf386"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>: 0.22, (<bold>g</bold>) total trial number: 0.46, (<bold>h</bold>): voluntary intertrial interval after error: 0.75, (<bold>i</bold>) fraction trials ignored: 0.03.</p><p><xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>: Statistical significance for mean parameters was calculated by taking the difference between the posterior distributions and using the proportion of the difference distribution that overlapped with 0 as the p-value. For individuals’ parameters, p-values were determined via a Wilcoxon signed-rank test. The p-values were: (<bold>a</bold>) drift mean:&lt;1e-4, drift individuals:&lt;1e-4; (<bold>b</bold>) threshold mean: 0.0012, threshold individuals: 0.0008; (<bold>c</bold>) drift mean: &lt;1e-4, drift individuals: &lt;1e-4, threshold mean: 0.0298, threshold individuals: 0.0585; (<bold>d</bold>) drift mean: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.0378, start versus after learn: &lt;1e-4), drift individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.0703, start versus after learn: 0.0004); (<bold>e</bold>) threshold mean: (baseline versus start learn: 0.1100, baseline versus after learn: 0.3546, start versus after learn: 0.1904), threshold individuals: (baseline versus start learn: 0.0045, baseline versus after learn: 0.4380, start versus after learn: 0.1627); (<bold>f</bold>) drift mean: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.0616, start versus after learn: &lt;1e-4), drift individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.1089, start versus after learn: 0.0004), threshold mean: (baseline versus start learn: 0.2546, baseline versus after learn: 0.4614, start versus after learn: 0.2816), threshold individuals: (baseline versus start learn: 0.0200, baseline versus after learn: 0.7174, start versus after learn: 0.1089).</p><p><xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>: Statistical significance was determined as for <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. The p-values were: (<bold>a</bold>) drift mean: &lt;1e-4, drift individuals: &lt;1e-4; (<bold>b</bold>) threshold mean: 0.0036, threshold individuals: 0.0013; (<bold>c</bold>) drift mean: &lt;1e-4, drift individuals: &lt;1e-4, threshold mean: 0.0314, threshold individuals: 0.0585; (<bold>d</bold>) drift mean: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.0428, start versus after learn: &lt;1e-4), drift individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.0703, start versus after learn: 0.0004); (<bold>e</bold>) threshold mean: (baseline versus start learn: 0.0866, baseline versus after learn: 0.4192, start versus after learn: 0.1252), threshold individuals: (baseline versus start learn: 0.0038, baseline versus after learn: 0.5349, start versus after learn: 0.1089); (<bold>f</bold>) drift mean: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.0618, start versus after learn: &lt;1e-4), drift individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.0980, start versus after learn: 0.0004), threshold mean: (baseline versus start learn: 0.2436, baseline versus after learn: 0.4596, start versus after learn: 0.2860), threshold individuals: (baseline versus start learn: 0.0200, baseline versus after learn: 0.7174, start versus after learn: 0.1089).</p><p><xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>: Statistical significance was determined as for <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. The p-values were: (<bold>a</bold>) drift mean: &lt;1e-4, drift individuals: &lt;1e-4, drift variability: &lt;1e-4; (<bold>b</bold>) threshold mean: 0.0036, threshold individuals:&lt; 1e-4, drift variability: &lt;1e-4; (<bold>c</bold>) drift mean: &lt;1e-4, drift individuals: &lt;1e-4, threshold mean: 0.0696, threshold individuals: 0.1587, drift variability: &lt;1e-4; (<bold>d</bold>) drift mean: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.0422, start versus after learn: &lt;1e-4), drift individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.0557, start versus after learn: 0.0004), drift variability: (baseline versus start learn: 0.7940, baseline versus after learn: 0.8104, start versus after learn: 0.4564); (<bold>e</bold>) threshold mean: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.4188, start versus after learn: 0.0002), threshold individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.5014, start versus after learn: 0.0004), drift variability: (baseline versus start learn: &lt;1e-4, baseline versus after learn: 0.4442, start versus after learn: &lt;1e-4); (<bold>f</bold>) drift mean: (baseline versus start learn:&lt;1e-4, baseline versus after learn: 0.0596, start versus after learn: &lt;1e-4), drift individuals: (baseline versus start learn: 0.0004, baseline versus after learn: 0.0980, start versus after learn: 0.0004), threshold mean: (baseline versus start learn: 0.2474, baseline versus after learn: 0.4702, start versus after learn: 0.2652), threshold individuals: (baseline versus start learn: 0.0200, baseline versus after learn: 0.7174, start versus after learn: 0.1089), drift variability: (baseline versus start learn: 0.2392, baseline versus after learn: 0.5294, start versus after learn: 0.2132).</p><p><xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1a, b</xref>: We tested for a difference in the aggregate reaction time distributions of a transparent stimuli condition in the first two and last two sessions (<inline-formula><mml:math id="inf387"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> subjects), and a no minimum reaction time condition with known stimuli (<inline-formula><mml:math id="inf388"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> subjects) via a two-sample Kolmogorov-Smirnov test and found a p-value of &lt;1e-4 for both comparisons.</p><p><xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>: Statistical significance was determined as for <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. The p-values were: (<bold>a</bold>) drift mean: (visible versus start transparent: 0.0002, visible versus end transparent: &lt;1e-4, start versus end transparent: 0.3180), drift individuals: (visible versus start transparent: 0.0117, visible versus end transparent: 0.0117, start versus end transparent: 0.5754), threshold mean: (visible versus start transparent: 0.1362, visible versus end transparent: 0.0094, start versus end transparent: 0.0658), threshold individuals: (visible versus start transparent: 0.0499, visible versus end transparent: 0.0173, start versus end transparent: 0.0173), drift variability: (visible versus start transparent: 0.1494, visible versus end transparent: 0.1614, start versus end transparent: 0.5032), <italic>T</italic><sub>0</sub> mean: (visible versus start transparent: 0.0068, visible versus end transparent: 0.0194, start versus end transparent: 0.6106), <italic>T</italic><sub>0</sub> individuals: (visible versus start transparent: 0.0117, visible versus end transparent: 0.0117, start versus end transparent: 0.0929).</p><p><xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5b, d</xref>: We tested for a difference in mean post-error slowing between the first two sessions and last two sessions of training for each animal for stimulus pair 1 (<bold>b</bold>) or the last two sessions of stimulus pair 1 and the first two sessions of stimulus pair 2 (<bold>d</bold>) via a Wilcoxon-signed rank test. The p-values were (<bold>b</bold>) 0.585 and (<bold>d</bold>) 0.255.</p></sec><sec id="s4-2-8"><title>Evaluation of optimality</title><p>Under the assumptions of a simple drift-diffusion process, the OPC defines a set of optimal threshold-to-drift ratios with corresponding decision times and error rates for which an agent maximizes instantaneous reward rate (<xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>). Decision times are scaled by the particular task timing as mean normalized decision time: <inline-formula><mml:math id="inf389"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The OPC is parameter free and can thus be used to compare performance across tasks, conditions, and individuals. An optimal agent will lie on different points on the OPC depending on differences in task timing (<inline-formula><mml:math id="inf390"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and stimulus difficulty (SNR). Assuming constant task timing, the SNR will determine different positions along the OPC for an optimal agent. For <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf392"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the OPC is defined as:<disp-formula id="equ35"><label>(33)</label><mml:math id="m35"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and exists in speed-accuracy space, defined by <inline-formula><mml:math id="inf393"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and ER. Given an estimate of <inline-formula><mml:math id="inf394"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the <inline-formula><mml:math id="inf395"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf396"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for any given animal can be compared to the optimal values defined by the OPC in speed-accuracy space.</p><p>Moreover, because <inline-formula><mml:math id="inf397"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> should decrease with learning, learning trajectories for different subjects and models can also be compared to the OPC and to each other in speed-accuracy space.</p><p>Mean normalized decision time depends only on <inline-formula><mml:math id="inf398"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>For completeness, we include a derivation showing that the appropriate normalized decision time for the OPC depends only on <inline-formula><mml:math id="inf399"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, not <inline-formula><mml:math id="inf400"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. According to <xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>, average reward rate is defined as:<disp-formula id="equ36">.<label>(34)</label><mml:math id="m36"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>average accuracy</mml:mtext></mml:mrow><mml:mrow><mml:mtext>average time per trial</mml:mtext></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We can write the average reward rate as (see A26 from <xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>),<disp-formula id="equ37"><label>(35)</label><mml:math id="m37"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Optimal behavior is defined as maximizing reward rate with respect to the thresholds in the DDM. We thus rewrite <inline-formula><mml:math id="inf401"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf402"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> in terms of average threshold and average SNR,<disp-formula id="equ38"><label>(36)</label><mml:math id="m38"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ39"><label>(37)</label><mml:math id="m39"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Next to find the extremum, we take the derivative of <inline-formula><mml:math id="inf403"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> with respect to the threshold and set it to zero,<disp-formula id="equ40"><label>(38)</label><mml:math id="m40"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>R</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ41"><label>(39)</label><mml:math id="m41"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ42">,<label>(40)</label><mml:math id="m42"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where in the final step we have rewritten <inline-formula><mml:math id="inf404"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf405"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> in terms of <inline-formula><mml:math id="inf406"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf407"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Rearranging to place <inline-formula><mml:math id="inf408"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> on the left-hand side reveals an OPC where decision time is normalized by the post-error non-decision time <inline-formula><mml:math id="inf409"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ43">.<label>(41)</label><mml:math id="m43"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Notably, the post-correct non-decision time <inline-formula><mml:math id="inf410"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is not part of the normalization. Intuitively, this is because post-correct delays are an unavoidable part of accruing reward and therefore do not influence the optimal policy.</p></sec><sec id="s4-2-9"><title>Estimating <italic>T</italic><sub>0</sub></title><p><italic>T</italic><sub>0</sub> is defined as the non-decision time component of a reaction time, comprising motor and perceptual processing time (<xref ref-type="bibr" rid="bib42">Holmes and Cohen, 2014</xref>). It can be estimated by fitting a DDM to the psychophysical data. Because of the experimentally imposed minimum reaction time meant to ensure visual processing of the stimuli, however, our reaction time distributions were truncated at 350 ms, meaning a DDM fit estimate of <italic>T</italic><sub>0</sub> is likely to be an overestimate. To address this issue, we set out to determine possible boundaries for <italic>T</italic><sub>0</sub> and estimated it in a few ways, all of which did indeed fall between those boundaries (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4e</xref>).</p><p>We found that after training, in the interval between 350 and 375 ms, nearly all of our animals had accuracy measurements above chance (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4b</xref>), meaning that the minimum reaction time of 350 ms served as an upper bound to possible <italic>T</italic><sub>0</sub> values.</p><p>To determine a lower bound, we obtained measurements for the two components comprising <italic>T</italic><sub>0</sub>: motor and initial perceptual processing times. To measure the minimum motor time required to complete a trial, we analyzed licking times across the different lickports. The latency from the last lick in the central port to the first lick in one of the two side ports peaked at around 80 ms (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4c</xref>). In addition, the latency from one lick to the next lick at the same port at any of the lickports was also around 80 ms (data not shown). Because the latencies in lick times between lickports (requires movement of the head) and within the same lockport (does not require movement of the head) were about equal we concluded that the minimum motor time was determined by the limit on lick frequency, and not on a movement of the head redirecting the animal from the central port to one of the side ports. To measure the initial perceptual processing times, we looked to published latencies of visual stimuli traveling to higher visual areas in the rat. Published latencies reaching area TO (predicted to be after V1, LM, and LI in the putative ventral stream in the rat) were around 80 ms (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4d</xref>; <xref ref-type="bibr" rid="bib113">Vermaercke et al., 2014</xref>). Based on these measurements, we estimated a <italic>T</italic><sub>0</sub> lower bound of approximately 160 ms.</p><p>One worry is that our lower bound could potentially be too low, as it is only estimated indirectly. Recent work on the SAT in a low-level visual discrimination tasks in rats found that accuracy was highest at a reaction time of 218 ms (<xref ref-type="bibr" rid="bib51">Kurylo et al., 2020</xref>). However, accuracy was still above chance for reaction times binned between 130 and 180 ms. In this task, reaction time was measured when an infrared beam was broken, which means we can assume there was no motor processing time. This leaves decision time, and initial perceptual processing time (part of <italic>T</italic><sub>0</sub>) within the 130–180 ms duration. The complexity of solving a high-level visual task like ours and a low-level one will result in substantial differences in decision time, but should not in principle affect non-decision time. Considering a latency estimate of 80 ms based on physiological evidence (<xref ref-type="bibr" rid="bib113">Vermaercke et al., 2014</xref>) can account for the initial perceptual processing component of <italic>T</italic><sub>0</sub> and gives an estimate <italic>T</italic><sub>0</sub>=80 ms for this study.</p><p>Because a reaction time around <italic>T</italic><sub>0</sub> should not allow for any decision time, accuracy should be around 50%. To estimate <italic>T</italic><sub>0</sub> based on this observation, we extrapolated the time at which accuracy would drop to 50% after plotting accuracy as a function of reaction time (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4a</xref>) and found values of 165 and 225 ms for linear and quadratic extrapolations respectively. Finally, we fit our behavioral data with an HDDM (<xref ref-type="bibr" rid="bib119">Wiecki et al., 2013</xref>) and found a <italic>T</italic><sub>0</sub> estimate of 295 ±4 ms (despite there being no data below 350 ms). To address this issue, we fit a DDM to a small number of behavioral sessions we conducted with animals trained on the minimum reaction time of 350 ms but where that constraint was eliminated and found a <italic>T</italic><sub>0</sub> estimate of 265 ± 120 (SD) ms. We stress that because the animals were trained with a minimum reaction time, they likely would have required extensive training without that constraint to fully make use of the time below the minimum reaction time, thus this estimate is likely to also be an overestimate. We do note however that the estimate is lower than the estimate with an enforced minimum reaction time and has a much higher standard deviation (spanning our lower and upper bound estimates).</p><p>Despite the range of possible <italic>T</italic><sub>0</sub> values, we find that our qualitative findings (in terms of learning trajectory and near-optimality after learning) do not change (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4f, g</xref>), and proceed with a <italic>T</italic><sub>0</sub>=160 ms for the main text.</p></sec></sec><sec id="s4-3"><title>Determining <inline-formula><mml:math id="inf411"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf412"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></title><p>The experimental protocol imposes a mandatory post-error and post-correct response-to-stimulus time (<inline-formula><mml:math id="inf413"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf414"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively). However, these times may not be accurate because of delays in the software communicating with different components such as the syringe pumps, and other delays such as screen refresh rates. We thus determined the actual mandatory post-error and post-correct response-to-stimulus times by measuring them based on timestamps on experimental file logs and found that <inline-formula><mml:math id="inf415"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3136</mml:mn></mml:mrow></mml:math></inline-formula> ms, and <inline-formula><mml:math id="inf416"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>6370</mml:mn></mml:mrow></mml:math></inline-formula> ms (<xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>).</p><sec id="s4-3-1"><title>Voluntary intertrial interval</title><p>We assume that the animals optimize reward rate based on task engagement time, that is, the sum of reaction times and all mandatory task delays, but not including any extra voluntary intertrial intervals. Therefore, our measures of non-decision task engagement time <inline-formula><mml:math id="inf417"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf418"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> do not include voluntary intertrial intervals. In essence this amounts to the assumption that animals exit the task between trials, potentially pursuing other goals, and do not count this voluntary interval when measuring their within-task reward rate.</p><p>We conducted a detailed analysis of the voluntary intertrial intervals after both correct and error trials (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). To prevent a new trial from initiating while the animals were licking one of the side lickports, the task included a 300 ms interval at the end of a trial where an extra 500 ms were added if the animal licked one of the side lickports (<xref ref-type="fig" rid="fig1s6">Figure 1—figure supplement 6</xref>). There was no stimulus (visual or auditory) to indicate the presence of this task feature so the animals were not expected to learn it. It was clear that the animals did not learn this task feature as most voluntary intertrial intervals are clustered in 500 ms intervals and decay after each boundary (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5a</xref>). Aligning the voluntary intertrial distributions every 500 ms reveals substantial overlap (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5c, d</xref>), indicating similar urgency in every 500 ms interval, with an added amount of variance the farther the interval from zero. Moreover, measuring the median voluntary intertrial interval from 0 to 500, 0–1000, and 0–2000 ms showed very similar values (47, 67, 108 ms after error trials, <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5b</xref>). The median was higher after correct trials (55, 134, 512 ms, <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5b</xref>) because the animals were collecting reward from the side lickports and much more likely to trigger the extra 500 ms penalty times.</p></sec><sec id="s4-3-2"><title>Reward rate sensitivity to <italic>T</italic><sub>0</sub> and voluntary intertrial interval</title><p>To ensure that our results did not depend on our chosen estimate for <italic>T</italic><sub>0</sub> and our choice to ignore voluntary intertrial intervals when computing metrics like <inline-formula><mml:math id="inf419"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and reward rate, we computed fraction maximum instantaneous reward rate as a function of <italic>T</italic><sub>0</sub> and voluntary intertrial interval. We conducted this analysis across <inline-formula><mml:math id="inf420"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> rats at asymptotic performance (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7a, b</xref>), and during the learning period (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7c, d</xref>). During asymptotic performance, sweeping <italic>T</italic><sub>0</sub> from our estimated minimum to our maximum possible values generated negligible changes in reward rate across a much larger range of possible voluntary intertrial intervals than we observed (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7a</xref>). Reward rate was more sensitive to voluntary intertrial intervals, but did not drop below 90% of the possible maximum when considering a median voluntary intertrial interval up to 2000 ms (the median when allowing up to a 2000 ms window after a trial, after which agents are considered to have ‘exited the task’) (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7b</xref>). During learning, we found similar results, with possible voluntary intertrial interval values have a larger effect on reward rate than <italic>T</italic><sub>0</sub>, however even with the most extreme combination of a maximum <italic>T</italic><sub>0</sub>=350 ms, and the median voluntary intertrial interval up to 2000 ms (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7d</xref>, light gray trace), fraction maximum reward rate was at most 10–15% away from the least extreme combination of <italic>T</italic><sub>0</sub>=160 ms and voluntary intertrial interval = 0 (<xref ref-type="fig" rid="fig1s7">Figure 1—figure supplement 7c</xref>, horizontal line along the bottom of the heat map) for most of the learning period. These results confirm that our qualitative findings do not depend on our estimated values of <italic>T</italic><sub>0</sub> and choice to ignore voluntary intertrial intervals.</p></sec><sec id="s4-3-3"><title>Ignore trials</title><p>Because of the free-response nature of the task, animals were permitted to ignore trials after having initiated them (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Although the fraction of ignored trials did seem to be higher at the beginning of learning for the first set of stimuli the animals learned (stimulus pair 1; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>), this effect did not repeat for the second set (stimulus pair 2, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b</xref>). This suggests that the cause for ignoring the trials during learning was not stimulus-based but rather related to learning the task for the first time. Overall, the mean fraction of ignored trials remained consistently low across stimulus sets and ignore trials were excluded from our analyses.</p></sec><sec id="s4-3-4"><title>Post-error slowing</title><p>In order to verify whether the increase in reaction time we saw at the beginning of learning relative to the end of learning was not solely attributable to a post-error slowing policy, we quantified the amount of post-error slowing during learning for both stimulus pair 1 and stimulus pair 2. For stimulus pair 1, we found that there was a consistent but slight amount of average post-error slowing (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5a</xref>). This amount was not significantly different at the start and end of learning (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5b</xref>).</p><p>We re-did this analysis for stimulus pair 2 and found similar results: animals had a consistent, modest amount of post-error slowing but it did not change across sessions during learning (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5c</xref>). We tested for a significant difference in post-error slowing between the last two sessions of stimulus pair 1 and the first two sessions of the completely new stimulus pair 2 and found none (<xref ref-type="fig" rid="fig6s5">Figure 6—figure supplement 5d</xref>) even though there was a large immediate change in error rate. In fact, there was a trend toward a decrease in post-error slowing (and toward post-correct slowing) in the first few sessions of stimulus pair 2. This is consistent with the hypothesis that post-error slowing is an instance of a more general policy of orienting toward infrequent events (<xref ref-type="bibr" rid="bib73">Notebaert et al., 2009</xref>). As correct trials became more infrequent than error trials when stimulus pair 2 was presented, we observed a trend toward post-correct slowing, as predicted by this interpretation.</p><p>Our subjects exhibit a modest, consistent amount of post-error slowing, which could at least partially explain the reaction time differences we see throughout learning. An experiment with transparent stimuli where error rate was constant but reaction times dropped, however, strongly contradicts the account that the rats implement a simple strategy like post-error slowing to modulate their reaction times during learning.</p></sec></sec><sec id="s4-4"><title>RNN model and LDDM reduction</title><p>We consider a recurrent network receiving noisy visual inputs over time. In particular, we imagine that an input layer projects through weighted connections to a single recurrently connected read-out node, and that the weights must be tuned to extract relevant signals in the input. The read-out node activity is compared to a modifiable threshold which governs when a decision terminates. This network model can then be trained via error-corrective gradient descent learning or some other procedure. In the following we derive the average dynamics of learning.</p><p>To reduce this network to a DDM with time-dependent SNR, we first note that due to the law of large numbers, activity increments of the read-out node will be Gaussian provided that the distribution of input stimuli has bounded moments. We can thus model the input-to-readout pathway at each time step as a Gaussian input <inline-formula><mml:math id="inf421"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> flowing through a scalar weight <inline-formula><mml:math id="inf422"><mml:mi>u</mml:mi></mml:math></inline-formula>, with noise of variance <inline-formula><mml:math id="inf423"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> added before the signal is sent into an integrating network. Taking the continuum limit, this yields a drift-diffusion process with effective drift rate <inline-formula><mml:math id="inf424"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and noise variance <inline-formula><mml:math id="inf425"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf426"><mml:mi>A</mml:mi></mml:math></inline-formula> parameterizes the perceptual signal, <inline-formula><mml:math id="inf427"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the input noise variance (noise in input channels that cannot be rejected), and <inline-formula><mml:math id="inf428"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the output noise variance (internal noise in output circuitry). The resulting decision variable <inline-formula><mml:math id="inf429"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> at time <inline-formula><mml:math id="inf430"><mml:mi>T</mml:mi></mml:math></inline-formula> is Gaussian distributed as <inline-formula><mml:math id="inf431"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf432"><mml:mi>y</mml:mi></mml:math></inline-formula> is the correct binary choice. A decision is made when <inline-formula><mml:math id="inf433"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> hits a threshold of <inline-formula><mml:math id="inf434"><mml:mrow><mml:mo>±</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>.</p><sec id="s4-4-1"><title>Within-trial drift-diffusion dynamics</title><p>On every trial, therefore, the subject’s behavior is described by a drift-diffusion process, for which the average reward rate as a function of signal to noise and threshold parameters is known (<xref ref-type="bibr" rid="bib9">Bogacz et al., 2006</xref>). The accuracy and decision time of this scheme is determined by two quantities. First, the SNR<disp-formula id="equ44"><label>(42)</label><mml:math id="m44"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and second, the threshold-to-drift ratio <inline-formula><mml:math id="inf435"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>/</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p><p>We can rewrite the SNR as<disp-formula id="equ45"><label>(43)</label><mml:math id="m45"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>From this it is clear that, when learning has managed to amplify the input signals such that <inline-formula><mml:math id="inf436"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, the asymptotic SNR is simply <inline-formula><mml:math id="inf437"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. Further, rearranging to<disp-formula id="equ46"><label>(44)</label><mml:math id="m46"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>shows that there are in fact just two parameters: the asymptotic achievable SNR <inline-formula><mml:math id="inf438"><mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> and the output-to-input noise variance ratio <inline-formula><mml:math id="inf439"><mml:mrow><mml:mi>c</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ47"><label>(45)</label><mml:math id="m47"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The mean error rate (<inline-formula><mml:math id="inf440"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>), mean decision time (<inline-formula><mml:math id="inf441"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>), and mean reward rate (<inline-formula><mml:math id="inf442"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>) are therefore<disp-formula id="equ48"><label>(46)</label><mml:math id="m48"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ49"><label>(47)</label><mml:math id="m49"><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ50">,<label>(48)</label><mml:math id="m50"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have suppressed the dependence of <inline-formula><mml:math id="inf443"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf444"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> on time for clarity. Here, <inline-formula><mml:math id="inf445"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the average non-decision task engagement time.</p><p>The term <inline-formula><mml:math id="inf446"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is a measure of the total evidence accrued on average, and is equal to<disp-formula id="equ51"><label>(49)</label><mml:math id="m51"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ52"><label>(50)</label><mml:math id="m52"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, for a fixed threshold <inline-formula><mml:math id="inf447"><mml:mi>z</mml:mi></mml:math></inline-formula>, the denominator shows the trade-off for increasing perceptual sensitivity: small <inline-formula><mml:math id="inf448"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> causes errors due to output noise, while large <inline-formula><mml:math id="inf449"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> causes errors due to overly fast integration for the specified threshold level.</p></sec><sec id="s4-4-2"><title>Across-trial error-corrective learning dynamics</title><p>To model learning, we consider that animals adjust perceptual sensitivities <inline-formula><mml:math id="inf450"><mml:mi>u</mml:mi></mml:math></inline-formula> over time in service of minimizing an objective function. In this section we derive the average learning dynamics when the objective is to minimize the error rate. The LDDM can be conceptualized as an ‘outer-loop’ that modifies the SNR of a standard DDM ‘inner-loop’ described in the preceding subsection. If perceptual learning is slow, there is a strong separation of timescales between these two loops. On the timescale of a single trial, the agent’s SNR is approximately constant and evidence accumulation follows a standard DDM, whereas on the timescale of many trials, the specific outcome on any one trial has only a small effect on the network weights <inline-formula><mml:math id="inf451"><mml:mi>w</mml:mi></mml:math></inline-formula>, such that the learning-induced changes are driven by the <italic>mean</italic> <inline-formula><mml:math id="inf452"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf453"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>To derive the mean effect of error-corrective learning updates, we suppose that on each trial the network uses gradient descent on the hinge loss to update its parameters, corresponding to standard practice for supervised neural networks. The hinge loss is<disp-formula id="equ53"><label>(51)</label><mml:math id="m53"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>yielding the gradient descent update.<disp-formula id="equ54">,<label>(52)</label><mml:math id="m54"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">←</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf454"><mml:mi>λ</mml:mi></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="inf455"><mml:mi>r</mml:mi></mml:math></inline-formula> is the trial number.</p><p>When the learning rate is small (<inline-formula><mml:math id="inf456"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), each trial changes the weights minimally and the overall update is approximately given by the average continuous time dynamics<disp-formula id="equ55"><label>(53)</label><mml:math id="m55"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ56"><label>(54)</label><mml:math id="m56"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>correct</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ57"><label>(55)</label><mml:math id="m57"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ58">,<label>(56)</label><mml:math id="m58"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo>⋅</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes an average over the correct answer <inline-formula><mml:math id="inf458"><mml:mi>y</mml:mi></mml:math></inline-formula>, the inputs and the output noise. The first step follows from iterated expectation. The second step follows from the fact that the probability of an error is simply the error rate <inline-formula><mml:math id="inf459"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, and for correct trials, the derivative of the hinge loss is zero. Next,<disp-formula id="equ59"><label>(57)</label><mml:math id="m59"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>u</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle><mml:mspace linebreak="newline"/></mml:mrow></mml:math></disp-formula><disp-formula id="equ60">,<label>(58)</label><mml:math id="m60"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf460"><mml:mi>T</mml:mi></mml:math></inline-formula> is the time step at which <inline-formula><mml:math id="inf461"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> crosses the decision threshold <inline-formula><mml:math id="inf462"><mml:mrow><mml:mo>±</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>. Returning to <xref ref-type="disp-formula" rid="equ58">Equation (56)</xref>,<disp-formula id="equ61"><label>(59)</label><mml:math id="m61"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>λ</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Hence, the magnitude of the update depends on the typical total sensory evidence given that an error is made. To calculate this, let <inline-formula><mml:math id="inf463"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> be the total sensory evidence up to time <inline-formula><mml:math id="inf464"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf465"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> be the total decision noise up to <inline-formula><mml:math id="inf466"><mml:mi>t</mml:mi></mml:math></inline-formula>. These are independent and normally distributed as<disp-formula id="equ62"><label>(60)</label><mml:math id="m62"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ63"><label>(61)</label><mml:math id="m63"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Therefore, we have<disp-formula id="equ64"><label>(62)</label><mml:math id="m64"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ65"><label>(63)</label><mml:math id="m65"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ66"><label>(64)</label><mml:math id="m66"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">|</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>These variables are jointly Gaussian. Letting <inline-formula><mml:math id="inf467"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf468"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the means <inline-formula><mml:math id="inf469"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, variances <inline-formula><mml:math id="inf470"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and covariance <inline-formula><mml:math id="inf471"><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="inf472"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> given the hitting time <inline-formula><mml:math id="inf473"><mml:mi>T</mml:mi></mml:math></inline-formula> are<disp-formula id="equ67"><label>(65)</label><mml:math id="m67"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ68"><label>(66)</label><mml:math id="m68"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ69"><label>(67)</label><mml:math id="m69"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ70"><label>(68)</label><mml:math id="m70"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ71"><label>(69)</label><mml:math id="m71"><mml:mrow><mml:mrow><mml:mtext>Cov</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>y</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>η</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ72"><label>(70)</label><mml:math id="m72"><mml:mrow><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ73"><label>(71)</label><mml:math id="m73"><mml:mrow><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The conditional expectation is therefore<disp-formula id="equ74"><label>(72)</label><mml:math id="m74"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mtext>Cov</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ75"><label>(73)</label><mml:math id="m75"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ76">,<label>(74)</label><mml:math id="m76"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have used the fact that <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>T</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mtext>error</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, because in the DDM the mean decision time is the same for correct and error trials. Inserting <xref ref-type="disp-formula" rid="equ76">Equation (74)</xref> into <xref ref-type="disp-formula" rid="equ61">Equation (59)</xref> yields<disp-formula id="equ77"><label>(75)</label><mml:math id="m77"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mfrac><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>c</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ78"><label>(76)</label><mml:math id="m78"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Finally, we switch the units of the time variable from trials to seconds using the relation <inline-formula><mml:math id="inf475"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, yielding the dynamics<disp-formula id="equ79"><label>(77)</label><mml:math id="m79"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi>z</mml:mi><mml:mi>u</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The above equation describes the dynamics of <inline-formula><mml:math id="inf476"><mml:mi>u</mml:mi></mml:math></inline-formula> under gradient descent learning. We note that here, the dependence of the dynamics on threshold trajectory is contained implicitly in the <inline-formula><mml:math id="inf477"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf478"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf479"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> terms.</p><p>To obtain equivalent dynamics for the SNR <inline-formula><mml:math id="inf480"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>, we have<disp-formula id="equ80"><label>(78)</label><mml:math id="m80"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>u</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ81"><label>(79)</label><mml:math id="m81"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>u</mml:mi><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Rearranging the definition of <inline-formula><mml:math id="inf481"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> yields<disp-formula id="equ82"><label>(80)</label><mml:math id="m82"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Inserting (<xref ref-type="disp-formula" rid="equ82">Equation 80</xref>) into (<xref ref-type="disp-formula" rid="equ81">Equation 79</xref>) and simplifying, we have<disp-formula id="equ83"><label>(81)</label><mml:math id="m83"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:mfrac></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mi>u</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ84"><label>(82)</label><mml:math id="m84"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:mfrac></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mi>z</mml:mi><mml:mi>u</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ85"><label>(83)</label><mml:math id="m85"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>A</mml:mi><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:mfrac></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, in the second step we have used the fact that <inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and <xref ref-type="disp-formula" rid="equ82">Equation (80)</xref>. Finally, absorbing the drift rate <inline-formula><mml:math id="inf483"><mml:mi>A</mml:mi></mml:math></inline-formula> into the time constant <inline-formula><mml:math id="inf484"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, we have the dynamics<disp-formula id="equ86"><label>(84)</label><mml:math id="m86"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:mfrac></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This equation reveals that the LDDM has four scalar parameters: the asymptotic SNR <inline-formula><mml:math id="inf485"><mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, the output-to-input-noise variance ratio <inline-formula><mml:math id="inf486"><mml:mi>c</mml:mi></mml:math></inline-formula>, the initial SNR at time zero <inline-formula><mml:math id="inf487"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and the combined drift rate/learning rate time constant <inline-formula><mml:math id="inf488"><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula>. In addition, it requires the choice of threshold trajectory <inline-formula><mml:math id="inf489"><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To reveal the basic learning speed/instantaneous reward rate trade-off in this model, we investigate the limit where <inline-formula><mml:math id="inf490"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> is small but finite (low signal-to-noise) and the threshold is small, such that the error rate is near <inline-formula><mml:math id="inf491"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Then the final term in <xref ref-type="disp-formula" rid="equ86">Equation (84)</xref> goes to zero, giving<disp-formula id="equ87"><label>(85)</label><mml:math id="m87"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>τ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>≈</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:mfrac></mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ88"><label>(86)</label><mml:math id="m88"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>∝</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>such that learning speed is increasing in <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. By contrast the instantaneous reward rate when <inline-formula><mml:math id="inf493"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is<disp-formula id="equ89"> <label>(87)</label><mml:math id="m89"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which is a decreasing function of <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>We note that when the perceptual signal is small, <inline-formula><mml:math id="inf495"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is determined by the ratio of threshold to diffusion noise. Starting with <xref ref-type="disp-formula" rid="equ49">Equation 47</xref>, we rewrite it in terms of threshold, perceptual signal, and noise:<disp-formula id="equ90">.<label>(88)</label><mml:math id="m90"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>If we explore the limit in which perceptual signal is small, and following L’Hôpital’s rule:<disp-formula id="equ91"><label>(89)</label><mml:math id="m91"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac><mml:mi>z</mml:mi><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>z</mml:mi><mml:msup><mml:mtext>sech</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Leaving:<disp-formula id="equ92">.<label>(90)</label><mml:math id="m92"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, a change in <inline-formula><mml:math id="inf496"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> when perceptual signal is low could be caused by either a changing threshold with fixed diffusion noise, a constant threshold with varying diffusion noise, or a combination thereof, without the immediate ability to tell these apart. In these cases, however, we note that the ratio of threshold to diffusion noise cannot stay constant if <inline-formula><mml:math id="inf497"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> changes.</p></sec><sec id="s4-4-3"><title>Threshold policies</title><p>We evaluate several simple threshold policies.<disp-formula id="equ93"><label>(91)</label><mml:math id="m93"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>A</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><list list-type="simple"><list-item><p>The <italic>iRR</italic>-greedy policy sets <inline-formula><mml:math id="inf498"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, the instantaneous reward rate optimal policy at all times.</p></list-item><list-item><p>The constant threshold policy sets <inline-formula><mml:math id="inf499"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula> to a fixed constant throughout learning.</p></list-item><list-item><p>The <italic>iRR</italic>-sensitive policy implements a threshold <inline-formula><mml:math id="inf500"><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that decays with time constant <inline-formula><mml:math id="inf501"><mml:mi>γ</mml:mi></mml:math></inline-formula> from an initial value <inline-formula><mml:math id="inf502"><mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> toward the <inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-optimal threshold,</p></list-item></list><p>where <inline-formula><mml:math id="inf504"><mml:mi>γ</mml:mi></mml:math></inline-formula> controls the rate of convergence.</p><p>Finally, the global optimal policy optimizes the entire function <inline-formula><mml:math id="inf505"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to maximize total cumulative reward during exposure to the task. To compute the optimal threshold trajectory, we discretize the reduction dynamics in <xref ref-type="disp-formula" rid="equ79">Equation 77</xref> and perform gradient ascent on <inline-formula><mml:math id="inf506"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using automatic differentiation in the PyTorch python package. While this procedure is not guaranteed to find the global optimum (due to potential nonconvexity of the optimization problem), in practice we found highly reliable results from a range of initial conditions and believe that the identified threshold trajectory is near the global optimum.</p></sec><sec id="s4-4-4"><title>Parameter fitting</title><p>The LDDM has several parameters governing its performance, including the asymptotic optimal SNR, the output/input noise variance ratio, the learning rate, and parameters controlling threshold policies where applicable. To fit these, we discretized the reduction dynamics and performed gradient ascent on the log likelihood of the observed data under the LDDM, again using automatic differentiation in the PyTorch python package. Because our model is highly simplified, our goal was only to place the parameters in a reasonable regime rather than obtain quantitative fits. We note that our fitting procedure could become stuck in local minima, and that a range of other parameter settings might also be consistent with the data. The best fitting parameters we obtained and used in all model results were <inline-formula><mml:math id="inf507"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9542</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3216</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We used a discretization time step of <inline-formula><mml:math id="inf508"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>160</mml:mn></mml:mrow></mml:math></inline-formula>. For the constant threshold and <inline-formula><mml:math id="inf509"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive policies, the best fitting initial threshold was <inline-formula><mml:math id="inf510"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>. For the <inline-formula><mml:math id="inf511"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>-sensitive policy, the best fitting decay rate was <inline-formula><mml:math id="inf512"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.00011891</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Project administration, Software, Validation, Writing – original draft, Writing – review and editing, Conceived the work</p></fn><fn fn-type="con" id="con2"><p>Software, Methodology, Aided JM in establishing initial operant training procedures and behavioral analysis</p></fn><fn fn-type="con" id="con3"><p>Resources, Methodology, Designed the behavioral response rigs</p></fn><fn fn-type="con" id="con4"><p>Resources, Supervision, Funding acquisition, Methodology, Provided input to experimental design and acquired funding for the project</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Methodology, Project administration, Software, Supervision, Visualization, Writing – original draft, Writing – review and editing, Conceived the work</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All care and experimental manipulation of animals were reviewed and approved by the Harvard Institutional Animal Care and Use Committee (IACUC), protocol 27-22.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-64978-transrepform1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and code are freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmasis/strategiclearning_and_lddm">https://github.com/jmasis/strategiclearning_and_lddm</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:65fcd14000382b67a7f562224704793e64e860b6;origin=https://github.com/jmasis/strategiclearning_and_lddm;visit=swh:1:snp:c5846309d41380bf208630d30bb7441228fe3906;anchor=swh:1:rev:26aa21d1e830657896325b1a26e9b84f5e3be93d">swh:1:rev:26aa21d1e830657896325b1a26e9b84f5e3be93d</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Chris Baldassano, Christopher Summerfield, Rahul Bhui, Grigori Guitchounts, Laura Bustamante, Sebastian Musslick, and Jonathan D Cohen for useful discussions. We thank Joshua Breedon for summer assistance in developing faster animal training procedures. We thank Ed Soucy and the NeuroTechnology Core for help with improvements to the behavioral response rigs. This work was supported by the Richard A and Susan F Smith Family Foundation and IARPA contract # D16PC00002. JM was supported by the Harvard Brain Science Initiative (HBI) and the Department of Molecular and Cellular Biology at Harvard, and a Presidential Postdoctoral Research Fellowship at Princeton. AMS was supported by a Swartz Postdoctoral Fellowship in Theoretical Neuroscience and a Sir Henry Dale Fellowship from the Wellcome Trust and Royal Society (Grant Number 216386/Z/19/Z).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>NM</given-names></name><name><surname>Spors</surname><given-names>H</given-names></name><name><surname>Carleton</surname><given-names>A</given-names></name><name><surname>Margrie</surname><given-names>TW</given-names></name><name><surname>Kuner</surname><given-names>T</given-names></name><name><surname>Schaefer</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Maintaining accuracy at the expense of speed: stimulus similarity defines odor discrimination time in mice</article-title><source>Neuron</source><volume>44</volume><fpage>865</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2004.11.017</pub-id><pub-id pub-id-type="pmid">15572116</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Posterior parietal cortex represents sensory history and mediates its effects on behaviour</article-title><source>Nature</source><volume>554</volume><fpage>368</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nature25510</pub-id><pub-id pub-id-type="pmid">29414944</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balci</surname><given-names>F</given-names></name><name><surname>Freestone</surname><given-names>D</given-names></name><name><surname>Simen</surname><given-names>P</given-names></name><name><surname>Desouza</surname><given-names>L</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Optimal temporal risk assessment</article-title><source>Frontiers in Integrative Neuroscience</source><volume>5</volume><elocation-id>56</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2011.00056</pub-id><pub-id pub-id-type="pmid">21991250</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balci</surname><given-names>F</given-names></name><name><surname>Simen</surname><given-names>P</given-names></name><name><surname>Niyogi</surname><given-names>R</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Hughes</surname><given-names>JA</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Acquisition of decision making criteria: reward rate ultimately beats accuracy</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>73</volume><fpage>640</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.3758/s13414-010-0049-7</pub-id><pub-id pub-id-type="pmid">21264716</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hanks</surname><given-names>T</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Roitman</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Probabilistic population codes for bayesian decision making</article-title><source>Neuron</source><volume>60</volume><fpage>1142</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.021</pub-id><pub-id pub-id-type="pmid">19109917</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Lu</surname><given-names>ZL</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perceptual learning as improved probabilistic inference in early sensory areas</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>642</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1038/nn.2796</pub-id><pub-id pub-id-type="pmid">21460833</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhui</surname><given-names>R</given-names></name><name><surname>Lai</surname><given-names>L</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Resource-rational decision making</article-title><source>Current Opinion in Behavioral Sciences</source><volume>41</volume><fpage>15</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.02.015</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blokland</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Reaction time responding in rats</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>22</volume><fpage>847</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1016/s0149-7634(98)00013-x</pub-id><pub-id pub-id-type="pmid">9809315</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>E</given-names></name><name><surname>Moehlis</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Hu</surname><given-names>PT</given-names></name><name><surname>Holmes</surname><given-names>PJ</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Do humans produce the speed-accuracy trade-off that maximizes reward rate?</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>63</volume><fpage>863</fpage><lpage>891</lpage><pub-id pub-id-type="doi">10.1080/17470210903091643</pub-id><pub-id pub-id-type="pmid">19746300</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The simplest complete model of choice response time: linear ballistic accumulation</article-title><source>Cognitive Psychology</source><volume>57</volume><fpage>153</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2007.12.002</pub-id><pub-id pub-id-type="pmid">18243170</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title><source>Science</source><volume>340</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1126/science.1233912</pub-id><pub-id pub-id-type="pmid">23559254</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Ayaz</surname><given-names>A</given-names></name><name><surname>Dhruv</surname><given-names>NT</given-names></name><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Schölvinck</surname><given-names>ML</given-names></name><name><surname>Zaharia</surname><given-names>AD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The detection of visual contrast in the behaving mouse</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>11351</fpage><lpage>11361</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6689-10.2011</pub-id><pub-id pub-id-type="pmid">21813694</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Puskas</surname><given-names>GA</given-names></name><name><surname>El-Murr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decisions in changing conditions: the urgency-gating model</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>11560</fpage><lpage>11571</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1844-09.2009</pub-id><pub-id pub-id-type="pmid">19759303</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Dunbar</surname><given-names>K</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>On the control of automatic processes: a parallel distributed processing account of the stroop effect</article-title><source>Psychological Review</source><volume>97</volume><fpage>332</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.97.3.332</pub-id><pub-id pub-id-type="pmid">2200075</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>The Wiley Handbook of Cognitive Control. Chapter Cognitive Control: Core Constructs and Current Considerations</source><publisher-name>John Wiley &amp; Sons, Ltd</publisher-name><pub-id pub-id-type="doi">10.1002/9781118920497</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Do we understand high-level vision?</article-title><source>Current Opinion in Neurobiology</source><volume>25</volume><fpage>187</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.01.016</pub-id><pub-id pub-id-type="pmid">24552691</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Making decisions with unknown sensory reliability</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00075</pub-id><pub-id pub-id-type="pmid">22679418</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditterich</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Evidence for time-variant decision making</article-title><source>The European Journal of Neuroscience</source><volume>24</volume><fpage>3628</fpage><lpage>3641</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.05221.x</pub-id><pub-id pub-id-type="pmid">17229111</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dixon</surname><given-names>ML</given-names></name><name><surname>Christoff</surname><given-names>K</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The decision to engage cognitive control is driven by expected reward-value: neural and behavioral evidence</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e51637</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0051637</pub-id><pub-id pub-id-type="pmid">23284730</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cost of accumulating evidence in perceptual decision making</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3612</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4010-11.2012</pub-id><pub-id pub-id-type="pmid">22423085</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Klier</surname><given-names>EM</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal multisensory decision-making in a reaction-time task</article-title><source>eLife</source><volume>3</volume><elocation-id>e03005</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03005</pub-id><pub-id pub-id-type="pmid">24929965</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tuning the speed-accuracy trade-off to maximize reward rate in multisensory decision-making</article-title><source>eLife</source><volume>4</volume><elocation-id>e06678</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06678</pub-id><pub-id pub-id-type="pmid">26090907</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Mendonça</surname><given-names>AG</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning optimal decisions with confidence</article-title><source>PNAS</source><volume>116</volume><fpage>24872</fpage><lpage>24880</lpage><pub-id pub-id-type="doi">10.1073/pnas.1906787116</pub-id><pub-id pub-id-type="pmid">31732671</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dutilh</surname><given-names>G</given-names></name><name><surname>Vandekerckhove</surname><given-names>J</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A diffusion model decomposition of the practice effect</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>1026</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.3758/16.6.1026</pub-id><pub-id pub-id-type="pmid">19966251</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fard</surname><given-names>PR</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Warkentin</surname><given-names>A</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Bitzer</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A bayesian reformulation of the extended drift-diffusion model in perceptual decision making</article-title><source>Frontiers in Computational Neuroscience</source><volume>11</volume><elocation-id>29</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2017.00029</pub-id><pub-id pub-id-type="pmid">28553219</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>C</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Levine</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Model-agnostic meta-learning for fast adaptation of deep networks</article-title><conf-name>International Conference on Machine Learning (ICML</conf-name><fpage>1126</fpage><lpage>1135</lpage></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Garrett</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="1922">1922</year><source>A Study of the Relation of Accuracy to Speed</source><publisher-name>Columbia University</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Horvitz</surname><given-names>EJ</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational rationality: a converging paradigm for intelligence in brains, minds, and machines</article-title><source>Science</source><volume>349</volume><fpage>273</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1126/science.aac6076</pub-id><pub-id pub-id-type="pmid">26185246</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gigerenzer</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Why heuristics work</article-title><source>Perspectives on Psychological Science</source><volume>3</volume><fpage>20</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1111/j.1745-6916.2008.00058.x</pub-id><pub-id pub-id-type="pmid">26158666</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Banburismus and the brain: decoding the relationship between sensory stimuli, decisions, and reward</article-title><source>Neuron</source><volume>36</volume><fpage>299</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00971-6</pub-id><pub-id pub-id-type="pmid">12383783</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gottlieb</surname><given-names>J</given-names></name><name><surname>Oudeyer</surname><given-names>PY</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Towards a neuroscience of active sampling and curiosity</article-title><source>Nature Reviews. Neuroscience</source><volume>19</volume><fpage>758</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0078-0</pub-id><pub-id pub-id-type="pmid">30397322</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rational use of cognitive resources: levels of analysis between the computational and the algorithmic</article-title><source>Topics in Cognitive Science</source><volume>7</volume><fpage>217</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1111/tops.12142</pub-id><pub-id pub-id-type="pmid">25898807</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Mazurek</surname><given-names>ME</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Hopp</surname><given-names>E</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Elapsed decision time affects the weighting of prior probability in a perceptual decision task</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>6339</fpage><lpage>6352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5613-10.2011</pub-id><pub-id pub-id-type="pmid">21525274</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heathcote</surname><given-names>A</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Mewhort</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The power law repealed: the case for an exponential law of practice</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>7</volume><fpage>185</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.3758/bf03212979</pub-id><pub-id pub-id-type="pmid">10909131</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Marrett</surname><given-names>S</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A general mechanism for perceptual decision-making in the human brain</article-title><source>Nature</source><volume>431</volume><fpage>859</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/nature02966</pub-id><pub-id pub-id-type="pmid">15483614</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Marrett</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The neural systems that mediate human perceptual decision making</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>467</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1038/nrn2374</pub-id><pub-id pub-id-type="pmid">18464792</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heitz</surname><given-names>RP</given-names></name><name><surname>Schall</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural mechanisms of speed-accuracy tradeoff</article-title><source>Neuron</source><volume>76</volume><fpage>616</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.030</pub-id><pub-id pub-id-type="pmid">23141072</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heitz</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The speed-accuracy tradeoff: history, physiology, methodology, and behavior</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>150</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00150</pub-id><pub-id pub-id-type="pmid">24966810</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henmon</surname><given-names>VAC</given-names></name></person-group><year iso-8601-date="1911">1911</year><article-title>The relation of the time of a judgment to its accuracy</article-title><source>Psychological Review</source><volume>18</volume><fpage>186</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1037/h0074579</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimality and some of its discontents: successes and shortcomings of existing models for binary decisions</article-title><source>Topics in Cognitive Science</source><volume>6</volume><fpage>258</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1111/tops.12084</pub-id><pub-id pub-id-type="pmid">24648411</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jahn</surname><given-names>CI</given-names></name><name><surname>Grohn</surname><given-names>J</given-names></name><name><surname>Cuell</surname><given-names>S</given-names></name><name><surname>Emberton</surname><given-names>A</given-names></name><name><surname>Bouret</surname><given-names>S</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Strategic Exploration in the Macaque’s Prefrontal Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.11.491468</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Balaguer</surname><given-names>J</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Optimal utility and probability functions for agents with finite computational precision</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2002232118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2002232118</pub-id><pub-id pub-id-type="pmid">33380453</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name><name><surname>Tversky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Prospect theory: an analysis of decision under risk</article-title><source>Econometrica</source><volume>47</volume><elocation-id>263</elocation-id><pub-id pub-id-type="doi">10.2307/1914185</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Zariwala</surname><given-names>HA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural correlates, computation and behavioural impact of decision confidence</article-title><source>Nature</source><volume>455</volume><fpage>227</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nature07200</pub-id><pub-id pub-id-type="pmid">18690210</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Rosen</surname><given-names>ZB</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decision making and the avoidance of cognitive demand</article-title><source>Journal of Experimental Psychology. General</source><volume>139</volume><fpage>665</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1037/a0020198</pub-id><pub-id pub-id-type="pmid">20853993</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mental labour</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>899</fpage><lpage>908</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0401-9</pub-id><pub-id pub-id-type="pmid">30988433</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</article-title><source>PNAS</source><volume>108</volume><fpage>13852</fpage><lpage>13857</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101328108</pub-id><pub-id pub-id-type="pmid">21808009</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krebs</surname><given-names>RM</given-names></name><name><surname>Boehler</surname><given-names>CN</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The influence of reward associations on conflict processing in the stroop task</article-title><source>Cognition</source><volume>117</volume><fpage>341</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2010.08.018</pub-id><pub-id pub-id-type="pmid">20864094</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurylo</surname><given-names>D</given-names></name><name><surname>Lin</surname><given-names>C</given-names></name><name><surname>Ergun</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Visual discrimination accuracy across reaction time in rats</article-title><source>Animal Behavior and Cognition</source><volume>7</volume><fpage>23</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.26451/abc.07.01.03.2020</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Costa</surname><given-names>GM</given-names></name><name><surname>Romberg</surname><given-names>E</given-names></name><name><surname>Koulakov</surname><given-names>AA</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal cortex is required for optimal waiting based on decision confidence</article-title><source>Neuron</source><volume>84</volume><fpage>190</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.039</pub-id><pub-id pub-id-type="pmid">25242219</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Moss</surname><given-names>M</given-names></name><name><surname>Gurnani</surname><given-names>H</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dopaminergic and Frontal Signals for Decisions Guided by Sensory Evidence and Reward Value</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/411413</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Law</surname><given-names>CT</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reinforcement learning can account for associative and perceptual learning on a visual-decision task</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>655</fpage><lpage>663</lpage><pub-id pub-id-type="doi">10.1038/nn.2304</pub-id><pub-id pub-id-type="pmid">19377473</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>d’Autume</surname><given-names>C</given-names></name><name><surname>Zoran</surname><given-names>D</given-names></name><name><surname>Amos</surname><given-names>D</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>K</given-names></name><name><surname>García Castañedo</surname><given-names>A</given-names></name><name><surname>Sanchez</surname><given-names>M</given-names></name><name><surname>Green</surname><given-names>S</given-names></name><name><surname>Gruslys</surname><given-names>A</given-names></name><name><surname>Legg</surname><given-names>S</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.08116">https://arxiv.org/abs/1801.08116</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leng</surname><given-names>X</given-names></name><name><surname>Yee</surname><given-names>D</given-names></name><name><surname>Ritz</surname><given-names>H</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dissociable influences of reward and punishment on adaptive cognitive control</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009737</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009737</pub-id><pub-id pub-id-type="pmid">34962931</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>RL</given-names></name><name><surname>Howes</surname><given-names>A</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Computational rationality: linking mechanism and behavior through bounded utility maximization</article-title><source>Topics in Cognitive Science</source><volume>6</volume><fpage>279</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1111/tops.12086</pub-id><pub-id pub-id-type="pmid">24648415</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rational metareasoning and the plasticity of cognitive control</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006043</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006043</pub-id><pub-id pub-id-type="pmid">29694347</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Cownden</surname><given-names>D</given-names></name><name><surname>Tweed</surname><given-names>DB</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>CC</given-names></name><name><surname>Watanabe</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Accounting for speed-accuracy tradeoff in perceptual learning</article-title><source>Vision Research</source><volume>61</volume><fpage>107</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.09.007</pub-id><pub-id pub-id-type="pmid">21958757</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logan</surname><given-names>GD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Shapes of reaction-time distributions and shapes of learning curves: a test of the instance theory of automaticity</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>18</volume><fpage>883</fpage><lpage>914</lpage><pub-id pub-id-type="doi">10.1037//0278-7393.18.5.883</pub-id><pub-id pub-id-type="pmid">1402715</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddox</surname><given-names>WT</given-names></name><name><surname>Bohil</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Base-rate and payoff effects in multidimensional perceptual categorization</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>24</volume><fpage>1459</fpage><lpage>1482</lpage><pub-id pub-id-type="doi">10.1037//0278-7393.24.6.1459</pub-id><pub-id pub-id-type="pmid">9835061</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manohar</surname><given-names>SG</given-names></name><name><surname>Chong</surname><given-names>TTJ</given-names></name><name><surname>Apps</surname><given-names>MAJ</given-names></name><name><surname>Batla</surname><given-names>A</given-names></name><name><surname>Stamelou</surname><given-names>M</given-names></name><name><surname>Jarman</surname><given-names>PR</given-names></name><name><surname>Bhatia</surname><given-names>KP</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reward pays the cost of noise reduction in motor and cognitive control</article-title><source>Current Biology</source><volume>25</volume><fpage>1707</fpage><lpage>1716</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.05.038</pub-id><pub-id pub-id-type="pmid">26096975</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Masís</surname><given-names>JA</given-names></name><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The value of learning and cognitive control allocation</article-title><conf-name>Proceedings of the Annual Meeting of the Cognitive Science Society</conf-name></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazurek</surname><given-names>ME</given-names></name><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Ditterich</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A role for neural integrators in perceptual decision making</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1257</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg097</pub-id><pub-id pub-id-type="pmid">14576217</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mendonça</surname><given-names>AG</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Inês Vicente</surname><given-names>M</given-names></name><name><surname>DeWitt</surname><given-names>E</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Impact of Learning on Perceptual Decisions and Its Implication for Speed-Accuracy Tradeoffs</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/501858</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metcalfe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Metacognitive judgments and control of study</article-title><source>Current Directions in Psychological Science</source><volume>18</volume><fpage>159</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8721.2009.01628.x</pub-id><pub-id pub-id-type="pmid">19750138</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Badia</surname><given-names>AP</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Harley</surname><given-names>T</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Asynchronous methods for deep reinforcement learning</article-title><conf-name>International conference on machine learning</conf-name><fpage>1928</fpage><lpage>1937</lpage></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>A</given-names></name><name><surname>Rosenbloom</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1981">1981</year><source>Cognitive Skills and Their Acquisition. Chapter Mechanisms of Skill Acquisition and the Law of Practice</source><publisher-name>Erlbaum</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niyogi</surname><given-names>RK</given-names></name><name><surname>Breton</surname><given-names>YA</given-names></name><name><surname>Solomon</surname><given-names>RB</given-names></name><name><surname>Conover</surname><given-names>K</given-names></name><name><surname>Shizgal</surname><given-names>P</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Optimal indolence: a normative microscopic approach to work and leisure</article-title><source>Journal of the Royal Society, Interface</source><volume>11</volume><elocation-id>20130969</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2013.0969</pub-id><pub-id pub-id-type="pmid">24284898</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niyogi</surname><given-names>RK</given-names></name><name><surname>Shizgal</surname><given-names>P</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Some work and some play: microscopic and macroscopic approaches to labor and leisure</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003894</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003894</pub-id><pub-id pub-id-type="pmid">25474151</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Notebaert</surname><given-names>W</given-names></name><name><surname>Houtman</surname><given-names>F</given-names></name><name><surname>Opstal</surname><given-names>FV</given-names></name><name><surname>Gevers</surname><given-names>W</given-names></name><name><surname>Fias</surname><given-names>W</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Post-error slowing: an orienting account</article-title><source>Cognition</source><volume>111</volume><fpage>275</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2009.02.002</pub-id><pub-id pub-id-type="pmid">19285310</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odoemene</surname><given-names>O</given-names></name><name><surname>Pisupati</surname><given-names>S</given-names></name><name><surname>Nguyen</surname><given-names>H</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual evidence accumulation guides decision-making in unrestrained mice</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>10143</fpage><lpage>10155</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3478-17.2018</pub-id><pub-id pub-id-type="pmid">30322902</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pachella</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="1974">1974</year><source>Human Information Processing: Tutorials in Performance and Cognition. Chapter The Interpretation of Reaction Time in Information Processing Research</source><publisher-name>Erlbaum</publisher-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padmala</surname><given-names>S</given-names></name><name><surname>Pessoa</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reward reduces conflict by enhancing attentional control and biasing visual cortical processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>3419</fpage><lpage>3432</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00011</pub-id><pub-id pub-id-type="pmid">21452938</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrov</surname><given-names>AA</given-names></name><name><surname>Van Horn</surname><given-names>NM</given-names></name><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dissociable perceptual-learning mechanisms revealed by diffusion-model analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>18</volume><fpage>490</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.3758/s13423-011-0079-8</pub-id><pub-id pub-id-type="pmid">21394547</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pew</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>The speed-accuracy operating characteristic</article-title><source>Acta Psychologica</source><volume>30</volume><fpage>16</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(69)90035-3</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>L</given-names></name><name><surname>Koay</surname><given-names>SA</given-names></name><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Yoon</surname><given-names>AM</given-names></name><name><surname>Deverett</surname><given-names>B</given-names></name><name><surname>Thiberge</surname><given-names>SY</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An accumulation-of-evidence task using visual pulses for mice navigating in virtual reality</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>12</volume><elocation-id>36</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00036</pub-id><pub-id pub-id-type="pmid">29559900</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name><name><surname>Snyder</surname><given-names>CRR</given-names></name></person-group><year iso-8601-date="1975">1975</year><source>Information Processing and Cognition: The Loyola Symposium. Chapter Attention and Cognitive Control</source><publisher-name>Erlbaum</publisher-name></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purcell</surname><given-names>BA</given-names></name><name><surname>Heitz</surname><given-names>RP</given-names></name><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Schall</surname><given-names>JD</given-names></name><name><surname>Logan</surname><given-names>GD</given-names></name><name><surname>Palmeri</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neurally constrained modeling of perceptual decision making</article-title><source>Psychological Review</source><volume>117</volume><fpage>1113</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1037/a0020311</pub-id><pub-id pub-id-type="pmid">20822291</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnev</surname><given-names>D</given-names></name><name><surname>Denison</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Suboptimality in perceptual decision making</article-title><source>The Behavioral and Brain Sciences</source><volume>41</volume><elocation-id>e223</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X18000936</pub-id><pub-id pub-id-type="pmid">29485020</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Group reaction time distributions and an analysis of distribution statistics</article-title><source>Psychological Bulletin</source><volume>86</volume><fpage>446</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.86.3.446</pub-id><pub-id pub-id-type="pmid">451109</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for two-choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Thapar</surname><given-names>A</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Aging, practice, and perceptual tasks: a diffusion model analysis</article-title><source>Psychology and Aging</source><volume>21</volume><fpage>353</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1037/0882-7974.21.2.353</pub-id><pub-id pub-id-type="pmid">16768580</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinagel</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Speed and accuracy of visual image discrimination by rats</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>200</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00200</pub-id><pub-id pub-id-type="pmid">24385954</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinagel</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>Speed and accuracy of visual motion discrimination by rats</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e68505</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0068505</pub-id><pub-id pub-id-type="pmid">23840856</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rinberg</surname><given-names>D</given-names></name><name><surname>Koulakov</surname><given-names>A</given-names></name><name><surname>Gelperin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Speed-Accuracy tradeoff in olfaction</article-title><source>Neuron</source><volume>51</volume><fpage>351</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.07.013</pub-id><pub-id pub-id-type="pmid">16880129</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>9475</fpage><lpage>9489</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-21-09475.2002</pub-id><pub-id pub-id-type="pmid">12417672</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname><given-names>JN</given-names></name><name><surname>Speckman</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>An evaluation of the vincentizing method of forming group-level response time distributions</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>11</volume><fpage>419</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.3758/bf03196589</pub-id><pub-id pub-id-type="pmid">15376789</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Bak</surname><given-names>JH</given-names></name><name><surname>Laboratory</surname><given-names>TIB</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extracting the dynamics of behavior in sensory decision-making experiments</article-title><source>Neuron</source><volume>109</volume><fpage>597</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.004</pub-id><pub-id pub-id-type="pmid">33412101</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>SJ</given-names></name><name><surname>Subramanian</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Provably bounded-optimal agents</article-title><source>Journal of Artificial Intelligence Research</source><volume>2</volume><fpage>575</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1613/jair.133</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruthruff</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A test of the deadline model for speed-accuracy tradeoffs</article-title><source>Perception &amp; Psychophysics</source><volume>58</volume><fpage>56</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.3758/BF03205475</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>If deep learning is the answer, what is the question?</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-00395-8</pub-id><pub-id pub-id-type="pmid">33199854</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Constantinople</surname><given-names>CM</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sources of noise during accumulation of evidence in unrestrained and voluntarily head-restrained rats</article-title><source>eLife</source><volume>4</volume><elocation-id>e11308</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11308</pub-id><pub-id pub-id-type="pmid">26673896</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The expected value of control: an integrative theory of anterior cingulate cortex function</article-title><source>Neuron</source><volume>79</volume><fpage>217</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.007</pub-id><pub-id pub-id-type="pmid">23889930</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Toward a rational and mechanistic account of mental effort</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>99</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031526</pub-id><pub-id pub-id-type="pmid">28375769</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiffrin</surname><given-names>RM</given-names></name><name><surname>Schneider</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Controlled and automatic human information processing: II. perceptual learning, automatic attending and a general theory</article-title><source>Psychological Review</source><volume>84</volume><fpage>127</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.84.2.127</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simen</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Rapid decision threshold modulation by reward rate in a neural network</article-title><source>Neural Networks</source><volume>19</volume><fpage>1013</fpage><lpage>1026</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2006.05.038</pub-id><pub-id pub-id-type="pmid">16987636</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simen</surname><given-names>P</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name><name><surname>Buck</surname><given-names>C</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reward rate optimization in two-alternative decision making: empirical tests of theoretical predictions</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>35</volume><fpage>1865</fpage><lpage>1897</lpage><pub-id pub-id-type="doi">10.1037/a0016926</pub-id><pub-id pub-id-type="pmid">19968441</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starns</surname><given-names>JJ</given-names></name><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The effects of aging on the speed-accuracy compromise: boundary optimality in the diffusion model</article-title><source>Psychology and Aging</source><volume>25</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1037/a0018022</pub-id><pub-id pub-id-type="pmid">20545422</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stine</surname><given-names>GM</given-names></name><name><surname>Zylberberg</surname><given-names>A</given-names></name><name><surname>Ditterich</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Differentiating between integration and non-integration strategies in perceptual decision making</article-title><source>eLife</source><volume>9</volume><elocation-id>e55365</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55365</pub-id><pub-id pub-id-type="pmid">32338595</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Parpart</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Normative Principles for Decision-Making in Natural Environments</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://psyarxiv.com/s2wvz/">https://psyarxiv.com/s2wvz/</ext-link></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweis</surname><given-names>BM</given-names></name><name><surname>Abram</surname><given-names>SV</given-names></name><name><surname>Schmidt</surname><given-names>BJ</given-names></name><name><surname>Seeland</surname><given-names>KD</given-names></name><name><surname>MacDonald</surname><given-names>AW</given-names></name><name><surname>Thomas</surname><given-names>MJ</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sensitivity to “ sunk costs ” in mice, rats, and humans</article-title><source>Science</source><volume>361</volume><fpage>178</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1126/science.aar8644</pub-id><pub-id pub-id-type="pmid">30002252</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Patel</surname><given-names>N</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimal policy for multi-alternative decisions</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1503</fpage><lpage>1511</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0453-9</pub-id><pub-id pub-id-type="pmid">31384015</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ten</surname><given-names>A</given-names></name><name><surname>Kaushik</surname><given-names>P</given-names></name><name><surname>Oudeyer</surname><given-names>PY</given-names></name><name><surname>Gottlieb</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Humans monitor learning progress in curiosity-driven exploration</article-title><source>Natural Communication</source><volume>12</volume><elocation-id>5972</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26196-w</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thorndike</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1913">1913</year><source>Educational Psychology, Vol 2: The Psychology of Learning</source><publisher-name>Teachers College</publisher-name><pub-id pub-id-type="doi">10.1037/13051-000</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Speed and accuracy of olfactory discrimination in the rat</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>1224</fpage><lpage>1229</lpage><pub-id pub-id-type="doi">10.1038/nn1142</pub-id><pub-id pub-id-type="pmid">14566341</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title><source>Psychological Review</source><volume>108</volume><fpage>550</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.108.3.550</pub-id><pub-id pub-id-type="pmid">11488378</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Gerich</surname><given-names>FJ</given-names></name><name><surname>Ytebrouck</surname><given-names>E</given-names></name><name><surname>Arckens</surname><given-names>L</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Van den Bergh</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional specialization in rat occipital and temporal visual cortex</article-title><source>Journal of Neurophysiology</source><volume>112</volume><fpage>1963</fpage><lpage>1983</lpage><pub-id pub-id-type="doi">10.1152/jn.00737.2013</pub-id><pub-id pub-id-type="pmid">24990566</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Tirumala</surname><given-names>D</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>860</fpage><lpage>868</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0147-8</pub-id><pub-id pub-id-type="pmid">29760527</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westbrook</surname><given-names>A</given-names></name><name><surname>Kester</surname><given-names>D</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>What is the subjective cost of cognitive effort? load, trait, and aging effects revealed by economic preference</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e68210</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0068210</pub-id><pub-id pub-id-type="pmid">23894295</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westbrook</surname><given-names>A</given-names></name><name><surname>Lamichhane</surname><given-names>B</given-names></name><name><surname>Braver</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The subjective value of cognitive effort is encoded by a domain-general valuation network</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>3934</fpage><lpage>3947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3071-18.2019</pub-id><pub-id pub-id-type="pmid">30850512</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whelan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Effective analysis of reaction time data</article-title><source>The Psychological Record</source><volume>58</volume><fpage>475</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1007/BF03395630</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickelgren</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Speed-accuracy tradeoff and information processing dynamics</article-title><source>Acta Psychologica</source><volume>41</volume><fpage>67</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/0001-6918(77)90012-9</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname><given-names>TV</given-names></name><name><surname>Sofer</surname><given-names>I</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>HDDM: hierarchical Bayesian estimation of the drift-diffusion model in python</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><pub-id pub-id-type="pmid">23935581</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>229</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/BF00992696</pub-id><pub-id pub-id-type="pmid">1490149</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Geana</surname><given-names>A</given-names></name><name><surname>White</surname><given-names>JM</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Humans use directed and random exploration to solve the explore-exploit dilemma</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>2074</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1037/a0038199</pub-id><pub-id pub-id-type="pmid">25347535</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodworth</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1899">1899</year><article-title>Accuracy of voluntary movement</article-title><source>The Psychological Review</source><volume>3</volume><fpage>i</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1037/h0092992</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacksenhouse</surname><given-names>M</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Robust versus optimal strategies for two-alternative forced choice tasks</article-title><source>Journal of Mathematical Psychology</source><volume>54</volume><fpage>230</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2009.12.004</pub-id><pub-id pub-id-type="pmid">23180885</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Rowe</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dissociable mechanisms of speed-accuracy tradeoff during visual perceptual learning are revealed by a hierarchical drift-diffusion model</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00069</pub-id><pub-id pub-id-type="pmid">24782701</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Oertelt</surname><given-names>N</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A rodent model for the study of invariant visual object recognition</article-title><source>PNAS</source><volume>106</volume><fpage>8748</fpage><lpage>8753</lpage><pub-id pub-id-type="doi">10.1073/pnas.0811583106</pub-id><pub-id pub-id-type="pmid">19429704</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoltowski</surname><given-names>DM</given-names></name><name><surname>Latimer</surname><given-names>KW</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discrete stepping and nonlinear Ramping dynamics underlie spiking responses of lip neurons during decision-making</article-title><source>Neuron</source><volume>102</volume><fpage>1249</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.031</pub-id><pub-id pub-id-type="pmid">31130330</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64978.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>École normale supérieure, PSL University, INSERM</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><p>This manuscript provides a fresh view on the fundamental trade-off between the speed and accuracy of perceptual decision-making. Using computational modeling, the authors establish the important finding that adopting a momentary suboptimal trade-off for maximizing reward rate at the beginning of learning can yield better decisions and larger rewards at later stages. This novel prediction is tested in rodent experiments. The experiments and their detailed analysis provide compelling evidence for the authors' theoretical predictions.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64978.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>École normale supérieure, PSL University, INSERM</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Tsetsos</surname><given-names>Konstantinos</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Strategically managing learning during perceptual decision making&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by Valentin Wyart as the Reviewing Editor and Michael Frank as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Konstantinos Tsetsos (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Classical descriptions of the speed-accuracy trade-off during perceptual decision-making assume that agents balance decision speed and accuracy given a fixed level of perceptual sensitivity. These descriptions ignore how agents learn to process the incoming sensory information for the purpose of decision-making. This manuscript develops a theory for how this perceptual learning ought to occur, and tests predictions from the theory using rodent experiments. This theory of perceptual learning leads to a new way of understanding suboptimal slow decisions at the early stages of learning. The manuscript is theoretically and technically sound. Additionally, the experiments are ingeniously designed and rigorously analysed, and their results provide empirical support for the theory proposed by the authors. There are however additional analyses that should be performed to validate the authors' specific claims regarding the strategic adaptation of perceptual sensitivity throughout task execution. Furthermore, the manuscript could be improved for clarity.</p><p>A current weakness of the manuscript concerns the preference for a strategic adaptation of perceptual sensitivity throughout learning (iRR-sensitive policy) over a simpler gradual increase in perceptual sensitivity (constant-threshold policy). Indeed, the strategies provide qualitatively similar predictions (see, e.g., Figure 3). Validating the iRR-sensitive policy over the constant-threshold policy is critical to the overall conclusions of the work (namely, that rats are strategically adapting their decision times to promote learning). However, it is currently unclear how the constant-threshold policy (in which rats do not control the speed of their decision but benefit from a gradual improvement in perceptual sensitivity) has been conclusively ruled out.</p><p>To rule out the constant-threshold policy, the authors argue first that drift-diffusion model (DDM) fits to the data show both drift rate and decision boundary changes throughout learning (Figure S5). However, it is not clear that a concurrent change in drift rate and decision boundary is the most parsimonious explanation of the data. The authors should establish that indeed both parameters change via comparing different versions of the DDM where a subset of the parameters are allowed to vary while others remain fixed throughout learning. Additionally, it would be interesting to know whether the conclusions remain the same when a more 'complete' version of the DDM is used (including drift-rate variability as a free parameter).</p><p>The second argument in support of the iRR-sensitive policy comes from experiment 2, in which the authors convincingly show that the improvement in perceptual sensitivity (or SNR) scales with decision times. It is indeed important to show that longer viewing leads to larger SNR improvements. However, it is currently unclear how this observation rules out the constant-threshold policy. Unless additional analyses are performed to show that the constant-threshold policy does not make this prediction, this observation appears necessary but not sufficient to validate the iRR-sensitive policy.</p><p>The third argument in support of the iRR-sensitive policy comes from experiment 3, in which a first group of rats performed a 'learnable' perceptual experiment while a second group performed an experiment with 'unlearnable' (transparent) stimuli. Indeed, this second group showed a reduction in reaction times as the experiment progressed, which (a) is reward-rate optimal, and (b) can be understood as a strategic change of decision boundary, since the SNR in this experiment is theoretically zero. However, it is unclear how rodents behave in this 'unlearnable' context. Presumably, during the first two sessions, the rats may be trying to figure out what the task is (e.g., waiting to see if there are visible stimuli in a subset of trials). In the third session, the rats speed up but it is not clear if they keep speeding up later in the experiment. Are reaction times significantly decreasing beyond the third session? Finally, it is not obvious that the effective SNR in this experiment is zero. In this 'unlearnable' experiment, rats may use some non-sensory information (e.g., choice history information such as their preceding response and whether they got rewarded) as input to their drift rate.</p><p>Another weakness comes from the use of recurrent neural networks (RNNs) to model that accumulation of decision-relevant evidence. Indeed, these networks are tuned such that they become equivalent to DDMs. Framed in this way, the connection between RNNs and DDMs appears somewhat trivial, such that the introduction of RNNs does not add anything to the manuscript, and might even be confusing to some readers. The authors should either reframe the specific role of RNNs for supporting their key findings, or possibly remove them if they do not provide unique insights beyond classical drift-diffusion modeling.</p><p>The detailed description of the models is currently hidden in the Methods section, even though it is essential for understanding their learning dynamics. In particular, the authors assume two sources of noise in the model: one on the input, and one on the accumulator. Learning is achieved by re-scaling the input by an 'input weight'. Increasing the input weight boosts the input signal compared to the accumulation noise, such that the latter can be effectively suppressed by making the input weight large enough. By contrast, the input noise cannot be suppressed by such re-scaling, such that it is this noise that ultimately limits asymptotic performance, and determines the asymptotic SNR. This important constraint is currently not clear after reading the manuscript. The authors should reframe the manuscript to highlight and discuss the model variables that affect the SNR, including the input weight. This would clarify what the authors mean by 'learning' in their theory. The authors initialize the model with a low input weight to reflect that the agent has not yet learned how to interpret the sensory information for the purpose of decision-making. Thus, the input weight is not something that would have a direct mechanistic implementation in the brain. Instead, it is an abstract quantity that describes how well a decision-maker can turn sensory information into a perceptual decision. Once this interpretation of input weight is stated clearly in the manuscript (which it is not in the current version), starting the task with a low input weight makes sense.</p><p>Finally, a few choices made by the authors are critical for their findings, but are not sufficiently described in the main text. First, the observed reaction time (RT) is composed of a non-decision time (T0) and a decision time (DT). Experiments allow to measure RT, but the theory makes predictions about DT, which requires inferring T0. The magnitude of T0 impacts the results, but how it is inferred is currently buried in the Methods section. Second, the rodents sometimes choose to not immediately initiate a new trial (the &quot;voluntary inter-trial interval&quot;). The authors assume that rodents ignore this interval when maximizing reward rate, and find near-optimal reward rates under this assumption. Importantly, including this voluntary inter-trial interval makes reward rates drop significantly. However, these details are again buried in the Methods section, and are not mentioned nor discussed in the main text.</p><p>– The main conclusions hinge upon concurrent changes in both drift rate and decision boundary throughout learning. This change is assessed via fitting HDDM models. It is important that the authors fit and compare the following 3 DDM variants: (a) drift rate changes throughout learning but decision boundary remains fixed, (b) decision boundary changes throughout learning but drift rate remains fixed, (c) both drift rate and decision boundary change throughout learning.</p><p>– The HDDM fitting procedure is not fully described. In particular, it is not clear what parameters, asides the drift rate, the decision boundary and the non-decision time, varied. For instance, was the drift rate variability a free parameter? It is important to report more precisely the details of the model fits and, if simple variants of the DDM were used, to fit the data using more complex DDMs that include drift rate variability as a free parameter. Additionally, please report in Figure S5 all parameter estimates during learning and not just the drift rate and the decision boundary. Details around Equation (40) should also be in the main text. Furthermore, except for the internal noise, the setup looks very similar to that of Drugowitsch et al. (2019), and the relationship should be discussed somewhere in the main text.</p><p>– Currently the evolution of the mean RT during learning is examined. Plotting the change in the RT distribution (averaged/vincentised across participants) can be more informative about changes in the strategy being used than plotting the mean RT alone.</p><p>– The results of experiment 3 are interesting, but at the same time the behaviour in the transparent group requires more scrutiny. How do rats behave in this condition? It appears that random choice in combination with heuristic strategies (e.g. win-stay lose-switch) are viable possibilities. The argument that the SNR is intrinsically zero in this task. However, the signal in this experiment may contain non-zero, irrelevant information (such as choice history or feedback information). Plotting the RT distributions as a function of learning could provide insight in this regard, because boundary changes and SNR changes manifest differently in the shapes of RT distributions.</p><p>– The use of RNNs is not sufficiently supported beyond classical drift-diffusion modeling. The authors should either reframe the specific role of RNNs for supporting their key findings, or possibly remove them if they do not provide unique insights beyond classical drift-diffusion modeling.</p><p>– The authors should clarify early on in the manuscript what &quot;learning&quot; means in their model and theory, and why it makes sense to start with a low input weight (after describing the meaning of the input weight). The authors should also explain what T0 and D_RSI are, how they are determined (and the choices made to determine them), and how these parameters impact the results (also related to Figure S13). In particular the relationship between RT and DT is already required to understand Figure 1c, and many plots thereafter.</p><p>– Box 1 provides some details of the model, but leaves out others – e.g., the different sources of noise in the model. From Box 1 alone, it is unclear how the asymptotic SNR or the iRR-sensitive threshold are computed. It is indeed nice that it is possible to derive Equation (3), but the equation itself is not particularly informative for the exposition of the main findings, and so it could be moved to the Methods section.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.64978.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>1) Classical descriptions of the speed-accuracy trade-off during perceptual decision-making assume that agents balance decision speed and accuracy given a fixed level of perceptual sensitivity. These descriptions ignore how agents learn to process the incoming sensory information for the purpose of decision-making. This manuscript develops a theory for how this perceptual learning ought to occur, and tests predictions from the theory using rodent experiments. This theory of perceptual learning leads to a new way of understanding suboptimal slow decisions at the early stages of learning. The manuscript is theoretically and technically sound. Additionally, the experiments are ingeniously designed and rigorously analysed, and their results provide empirical support for the theory proposed by the authors. There are however additional analyses that should be performed to validate the authors' specific claims regarding the strategic adaptation of perceptual sensitivity throughout task execution. Furthermore, the manuscript could be improved for clarity.</p><p>A current weakness of the manuscript concerns the preference for a strategic adaptation of perceptual sensitivity throughout learning (iRR-sensitive policy) over a simpler gradual increase in perceptual sensitivity (constant-threshold policy). Indeed, the strategies provide qualitatively similar predictions (see, e.g., Figure 3). Validating the iRR-sensitive policy over the constant-threshold policy is critical to the overall conclusions of the work (namely, that rats are strategically adapting their decision times to promote learning). However, it is currently unclear how the constant-threshold policy (in which rats do not control the speed of their decision but benefit from a gradual improvement in perceptual sensitivity) has been conclusively ruled out.</p><p>To rule out the constant-threshold policy, the authors argue first that drift-diffusion model (DDM) fits to the data show both drift rate and decision boundary changes throughout learning (Figure S5). However, it is not clear that a concurrent change in drift rate and decision boundary is the most parsimonious explanation of the data. The authors should establish that indeed both parameters change via comparing different versions of the DDM where a subset of the parameters are allowed to vary while others remain fixed throughout learning. Additionally, it would be interesting to know whether the conclusions remain the same when a more 'complete' version of the DDM is used (including drift-rate variability as a free parameter).</p></disp-quote><p>We have fit the data with several different versions of the DDM where a subset of the parameters are allowed to vary with learning while others remain fixed, including versions with drift rate variability, as requested by the reviewers (Figure 4—figure supplements 9-12). We found that the best model fits were those where both drift rate and threshold were allowed to vary with learning, and the presence of drift rate variability did not seem to noticeably improve model fits (Figure 4—figure supplement 9).</p><disp-quote content-type="editor-comment"><p>The second argument in support of the iRR-sensitive policy comes from experiment 2, in which the authors convincingly show that the improvement in perceptual sensitivity (or SNR) scales with decision times. It is indeed important to show that longer viewing leads to larger SNR improvements. However, it is currently unclear how this observation rules out the constant-threshold policy. Unless additional analyses are performed to show that the constant-threshold policy does not make this prediction, this observation appears necessary but not sufficient to validate the iRR-sensitive policy.</p></disp-quote><p>The reviewers are correct that this experiment does not rule out the constant-threshold policy, and therefore in the text we did not make this claim. The purpose of this experiment was solely to verify that longer viewing times lead to larger SNR improvements (a key prediction of our learning framework regardless of threshold policy).</p><disp-quote content-type="editor-comment"><p>The third argument in support of the iRR-sensitive policy comes from experiment 3, in which a first group of rats performed a 'learnable' perceptual experiment while a second group performed an experiment with 'unlearnable' (transparent) stimuli. Indeed, this second group showed a reduction in reaction times as the experiment progressed, which (a) is reward-rate optimal, and (b) can be understood as a strategic change of decision boundary, since the SNR in this experiment is theoretically zero. However, it is unclear how rodents behave in this 'unlearnable' context. Presumably, during the first two sessions, the rats may be trying to figure out what the task is (e.g., waiting to see if there are visible stimuli in a subset of trials). In the third session, the rats speed up but it is not clear if they keep speeding up later in the experiment. Are reaction times significantly decreasing beyond the third session? Finally, it is not obvious that the effective SNR in this experiment is zero. In this 'unlearnable' experiment, rats may use some non-sensory information (e.g., choice history information such as their preceding response and whether they got rewarded) as input to their drift rate.</p></disp-quote><p>The reviewers make an astute observation that the effective SNR for the group experiencing the transparent stimuli may not be zero due to stimulus-independent information, such as choice and reward history. To test for these strategies, we fit a generalized linear model to trial-bytrial choices for every subject and found that overall there was an increase in the weights associated with these strategies (perseverance and win-stay/lose-switch) (Figure 6—figure supplement 4). HDDM fits for this experiment also indicated that there appeared to be a drift rate greater than 0, indicating a non-zero SNR, perhaps due to these stimulus-independent strategies (Figure 6—figure supplement 3). However, the HDDM fits still found a decrease in threshold, as predicted by our model, and did not find an increase in drift rate (Figure 6—figure supplement 3). These threshold and drift rate trajectories differ from the ones found in the learnable stimuli, indicating that although the rats could be implementing stimulus-independent “monitoring” strategies, their choice of threshold still indicated a reduced belief in stimulus learnability. Overall, we find that the presence of these strategies does not invalidate our conclusions. We also make a terminological distinction: the SNR may refer to the signal in the task that is predictive of the correct answer. On this definition, because the correct answer is random in this experiment, the SNR is zero regardless of any information employed by the animal, as no strategy will attain better than 50% accuracy. Said another way, in the DDM, if drift rate is drift toward the correct answer, then this can only be zero. However, in the way used by the reviewer, SNR could correspond to a more mechanistic account based on the DDM and reflect signals supplied to the integrator even if uncorrelated with the rewarded side on a trial. These new analyses focus on this second definition, but we believe our point about speeding up responses because of a lack of task-relevant information can still be appreciated using the first definition.</p><disp-quote content-type="editor-comment"><p>Another weakness comes from the use of recurrent neural networks (RNNs) to model that accumulation of decision-relevant evidence. Indeed, these networks are tuned such that they become equivalent to DDMs. Framed in this way, the connection between RNNs and DDMs appears somewhat trivial, such that the introduction of RNNs does not add anything to the manuscript, and might even be confusing to some readers. The authors should either reframe the specific role of RNNs for supporting their key findings, or possibly remove them if they do not provide unique insights beyond classical drift-diffusion modeling.</p></disp-quote><p>We thank the reviewer for pointing to this important lack of clarity in the text. The RNN is essential to the analysis, because it is only from the RNN that the learning dynamics for the DDM parameters can be derived. The derivation of the differential equation on the SNR (Equation 8) is a central result of our work, and relies on the RNN interpretation. Starting with the RNN, we derive the effect of doing standard gradient based learning of the underlying weight parameters. We then back out the SNR dynamics in a DDM that exactly correspond to these gradient based updates. With only the DDM, it is not clear what differential equation should govern changes in SNR due to learning. Therefore, the main result that learning speed scales with viewing time is derived from the RNN interpretation and cannot be seen just from a DDM framework. Said another way, while the RNN and DDM are trivially the same for the within-trial dynamics where parameters are constant, by itself the DDM makes no predictions for across-trial learning dynamics, where parameters change as a function of mean performance. For this, the RNN interpretation is required. We note that gradient descent is not invariant to parametrization and so performing gradient descent directly on the SNR variable of a DDM is not equivalent to the SNR dynamics we have derived based on gradient descent on the weights in the underlying RNN. We have expanded the model explanation in the main text considerably, and included the following sentences highlighting the necessity of an RNN for our reduction to a DDM:</p><p>“Remarkably, this reduction shows that the high-dimensional dynamics of the RNN receiving stochastic pixel input and performing gradient descent on the weights (Figure 3, grey trace) can be described by a drift diffusion model with a single deterministic scalar variable--the effective SNR--that changes over time (Figure 3, blue trace). Notably, without the mapping to the original recurrent neural network, it is not possible to understand what effect error-corrective gradient descent learning would have at the level of the DDM, or how the learning process is influenced by choice of decision times. In particular, the change in SNR that arises from gradient descent on the underlying RNN weights (Equation10) is not equivalent to that arising from gradient descent on the SNR parameter in the DDM directly because gradient descent is not parametrization invariant.”</p><disp-quote content-type="editor-comment"><p>The detailed description of the models is currently hidden in the Methods section, even though it is essential for understanding their learning dynamics. In particular, the authors assume two sources of noise in the model: one on the input, and one on the accumulator. Learning is achieved by re-scaling the input by an 'input weight'. Increasing the input weight boosts the input signal compared to the accumulation noise, such that the latter can be effectively suppressed by making the input weight large enough. By contrast, the input noise cannot be suppressed by such re-scaling, such that it is this noise that ultimately limits asymptotic performance, and determines the asymptotic SNR. This important constraint is currently not clear after reading the manuscript. The authors should reframe the manuscript to highlight and discuss the model variables that affect the SNR, including the input weight. This would clarify what the authors mean by 'learning' in their theory. The authors initialize the model with a low input weight to reflect that the agent has not yet learned how to interpret the sensory information for the purpose of decision-making. Thus, the input weight is not something that would have a direct mechanistic implementation in the brain. Instead, it is an abstract quantity that describes how well a decision-maker can turn sensory information into a perceptual decision. Once this interpretation of input weight is stated clearly in the manuscript (which it is not in the current version), starting the task with a low input weight makes sense.</p></disp-quote><p>We have modified the manuscript to explain these components of the model in the main text. In particular, we include sections explaining what “learning” means in the LDDM. First, we now explicitly describe and simulate a recurrent neural network from pixels, which provides a better account of the meaning of the input weights–taken literally, these could be synaptic weights from a population of neurons representing the visual input, to a population of neurons that integrate this signal, and gestures toward a mechanistic implementation in the brain. As the reviewer notes, though, these could in principle also be more abstract, and the model may describe functional connectivity implemented in more complex circuits.</p><p>“Within a trial, <italic>N</italic> dimensional inputs <italic>s(t)</italic> <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> arrive at discrete times <italic>t</italic> = 1<italic>dt,</italic> 2<italic>dt,</italic>∙∙∙ where <italic>dt</italic> is a small time step parameter. In our experimental task, <italic>s(t)</italic> might represent the activity of LGN neurons in response to a given visual stimulus. Because of eye motion and noise in the transduction from light intensity to visual activity, the response of individual neurons will only probabilistically relate to the correct answer at any given instant. In our simulations, we take <italic>s(t)</italic> to be the pixel values of the exact images presented to the animals, but transformed at each time point by small rotations (<italic>±20deg</italic>) and translations (<italic>±25deg</italic> of the image width and height), as depicted in Figure 3a. This input variability over time makes temporal integration valuable even in this visual classification task. To perform this integration, each input <italic>s(t)</italic> is filtered through perceptual weights <italic>s(t)</italic> <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and added to a read-out node (decision variable) <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> along with i.i.d. <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>dt</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This integrator noise models internal neural noise. The evolution of the decision variable is given by the simple linear recurrence <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mtext>dt</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mtext>ial</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> until the decision variable hits a threshold <italic>±z(trial)</italic> that is constant on each trial. Here the RNN already performs an integration through time (a choice motivated by prior experiments in rodents [37]), and improvements in performance come from adjusting the input-to-integrator weights <italic>w(trial)</italic> to better extract task relevant sensory information.”</p><p>In the second section, we describe the reduction of this RNN to the LDDM. The input weight in the reduction summarizes the functional effect of many individual weights in the RNN. In this sense, exactly as the reviewer says, the LDDM input weight is a more abstract quantity. However, it summarizes the behavior of the more mechanistically plausible weights of the RNN. We show that simulations of the reduction match those of a real RNN trained from pixels, verifying our methods.</p><p>“We start by noting that the input to the decision variable <inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at each time step is a weighted sum of many random variables, which by the law of large numbers will be approximately Gaussian. […] The dynamics of this “learning DDM” (LDDM) closely tracks simulated trajectories of the full network from pixels (Figure 3c-f blue trace, Figure 3—figure supplement 1; see Methods).”</p><disp-quote content-type="editor-comment"><p>Finally, a few choices made by the authors are critical for their findings, but are not sufficiently described in the main text. First, the observed reaction time (RT) is composed of a non-decision time (T0) and a decision time (DT). Experiments allow to measure RT, but the theory makes predictions about DT, which requires inferring T0. The magnitude of T0 impacts the results, but how it is inferred is currently buried in the Methods section. Second, the rodents sometimes choose to not immediately initiate a new trial (the &quot;voluntary inter-trial interval&quot;). The authors assume that rodents ignore this interval when maximizing reward rate, and find near-optimal reward rates under this assumption. Importantly, including this voluntary inter-trial interval makes reward rates drop significantly. However, these details are again buried in the Methods section, and are not mentioned nor discussed in the main text.</p></disp-quote><p>We added the following paragraph near the beginning of the Results section describing these parameters:</p><p>“Calculating mean normalized DT for comparison with the OPC requires knowing two quantities, DT and the average non-decision time per error trial <italic>D<sub>err</sub></italic>. The average non-decision time <italic>D<sub>err</sub> = T<sub>0</sub></italic> +<inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>err</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> contains the motor and initial perceptual processing components of RT, denoted T0; and the post response timeout on error trials <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>err</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Mean normalized DT is then the ratio DT/Derr. In order to determine each subject’s <italic>DT</italic>, we estimated T0 through a variety of methods, opting for a biological estimate (measured lickport latency response times and published visual processing latencies; Fig. 1–figure supplement 4). To ensure that our results did not depend on our choice of <italic>T<sub>0</sub></italic>, we ran a sensitivity analysis on a wide range of possible values of <italic>T<sub>0</sub></italic> (Fig. 1–figure supplement 4f). We then had to determine <inline-formula><mml:math id="sa2m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>err</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which can contain mandatory and voluntary intertrial intervals. We found that the rats generally kept voluntary intertrial intervals to a minimum, and we interpreted longer intervals as effectively “exiting” the DDM framework (Fig. 1–figure supplement 5). As such, we defined <inline-formula><mml:math id="sa2m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>err</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to only contain mandatory intertrial intervals (see Methods, Fig. 1–figure supplement 6). To ensure that our results did not depend on either choice, we ran a sensitivity analysis on the combined effects of <italic>T<sub>0</sub></italic> and a <inline-formula><mml:math id="sa2m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>¨</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mtext>err</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> containing voluntary intertrial intervals on RR (Fig. 1–figure supplement 7). A full discussion of how these parameters were determined is included in the Methods.”</p><disp-quote content-type="editor-comment"><p>– The main conclusions hinge upon concurrent changes in both drift rate and decision boundary throughout learning. This change is assessed via fitting HDDM models. It is important that the authors fit and compare the following 3 DDM variants: (a) drift rate changes throughout learning but decision boundary remains fixed, (b) decision boundary changes throughout learning but drift rate remains fixed, (c) both drift rate and decision boundary change throughout learning.</p></disp-quote><p>We fit the 3 DDM variants suggested by the reviewers and found that models that allowed both drift and threshold to vary with learning provided the best fits. We discuss these model variants in the point below.</p><p>“Indeed, the best DDM model fits were those that allowed both threshold and drift rate to vary with learning, as was the case with the first stimuli the rats encountered, and in line with the LDDM model (Figure 4—figure supplement 1, Figure 4—figure supplement 2, Figure 4—figure supplement 3, Figure 4—figure supplement 4).”</p><disp-quote content-type="editor-comment"><p>– The HDDM fitting procedure is not fully described. In particular, it is not clear what parameters, asides the drift rate, the decision boundary and the non-decision time, varied. For instance, was the drift rate variability a free parameter? It is important to report more precisely the details of the model fits and, if simple variants of the DDM were used, to fit the data using more complex DDMs that include drift rate variability as a free parameter. Additionally, please report in Figure S5 all parameter estimates during learning and not just the drift rate and the decision boundary. Details around Equation (40) should also be in the main text. Furthermore, except for the internal noise, the setup looks very similar to that of Drugowitsch et al. (2019), and the relationship should be discussed somewhere in the main text.</p></disp-quote><p>In our original submission, we used the HDDM model to first fit a simple</p><p>DDM to 10 sessions of asymptotic behavior after learning stimulus pair 1 (n = 26) as a sanity check that a simple DDM was adequate (Figure 1—figure supplement 3). (We kept this figure unchanged).</p><p>To assess parameter changes during learning, in our original submission we fit separate, simple DDMs to each of the learning epochs of stimulus pair 1, and stimulus pair 2. As these were simple DDMs, drift rate variability was not a free parameter. For stimulus pair 1, the two learning epochs were the (1) first and (2) last 1000 trials for every subject. For stimulus pair 2, the three learning epochs were (1) 500 trials from a baseline session of stimulus pair 1 prior to introduction of stimulus pair 2, the (2) first 500 trials and the (3) last 500 trials of stimulus pair 2. Fewer trials were used because the stimulus pair 2 experiment had fewer sessions and trials overall. In order to succinctly address the modeling requests from the paper reviews, we modified the fitting procedure (described below) and replaced Figure S5 with new Figure 4—figure supplements 1-4, where we report the values of all parameters in every model fit.</p><p>The qualitative results were the same across all model fit variations: drift rate increased with learning and threshold decreased, even with the addition of drift rate variability. We report DICs (Figure 4—figure supplement 1) for each fit and arrive at two conclusions. First, models where both drift and threshold varied with learning fit the best, as opposed to models where drift and/or threshold were held constant. Second, more complex models involving drift rate variability did not provide a better fit over a simple DDM, and those that performed the best were those where drift and threshold were already allowed to vary with learning. This means that drift rate variability did not seem to provide a substantial benefit in explaining the data. Moreover, the direction in which drift rate variability changed with learning (increasing or decreasing with learning epoch) seemed to depend on which parameters also varied with learning (drift and/or threshold) leading to differing accounts. However, we cannot rule out that drift rate variability may indeed play a role in learning strategy. We leave the interpretability of such an account to our reviewers and our readers.</p><p>We have provided a more complete description of the revised HDDM fitting procedure in the Methods section:</p><p>“In order to verify that our behavioral data could be modeled as a drift-diffusion process, the data were fit with a hierarchical drift-diffusion model [121], permitting subsequent analysis (such as comparison to the optimal performance curve) based on the assumption of a drift-diffusion process. To verify that a DDM was appropriate for our data, we fit a simple DDM to 10 asymptotic sessions after learning stimulus pair 1 for <italic>n</italic> = 26 subjects (Fig. 1–figure supplement 3). In order to assess parameter changes across learning, we fit DDMs to the stimulus pair 1 experiment and the stimulus pair 2 experiment where the learning epochs were treated as conditions in each experiment. This allowed us to hold some parameters constant while conditioning others on learning. We fit both simple DDMs and DDMs with drift rate variability to the two experiments, allowing drift rate, threshold and drift rate variability to vary with learning epoch. In particular, we fit three broad types of models: (1) simple DDMs (Fig. 4–figure supplement 2), (2) DDMs + fixed drift rate variability (Fig. 4–figure supplement 3), and (3) DDMs + drift rate variability that varied freely with learning epoch (Fig. 4–figure supplement 4). For each of the types of models we held drift constant, threshold constant, or allowed both to vary with learning. The best fits, as determined by the deviance information criterion (DIC), came from models where we allowed both drift and threshold to vary with learning; the addition of drift rate variability did not appear to improve model fits (Fig. 4–figure supplement 1). For both learning experiments, drift rates increased and thresholds decreased by the end of learning, in agreement with previous findings [19, 53–57]. In addition, for the transparent stimuli experiment we fit a DDM model that allowed drift rate, threshold, drift rate variability and <italic>T<sub>0</sub></italic> to vary with learning phase in order to observe the changes in drift rate and threshold 6–figure supplement 3.”</p><p>We included details around Equation 40 in the main text (Within-trial DriftDiffusion Dynamics subsection).</p><p>We now discuss Drugowitsch et al., 2019 at greater length in the discussion. In short, they treat learning in a Bayesian formulation, using primarily simulation-based methods, under the assumption that the diffusion boundaries (thresholds) are constant. By accounting for uncertainty in the weights, their Bayesian learning algorithm is more powerful than ours. The principal difference with our work, however, is the assumption of constant thresholds throughout learning. Our core interest is in how modification of response times (through changes in the decision threshold) impacts long term learning. Our simpler gradient descent learning model (still widely used throughout deep learning) permits analytical computation of average trajectories and updates that reveal the basic tradeoff between viewing time and learning speed in this setting. We believe future work could profitably combine Bayesian learning with threshold trajectories.</p><p>“Prior work in the DDM framework has investigated learning dynamics with a Bayesian update and constant thresholds across trials [35]. Our framework uses simpler error corrective learning rules, and focuses on how the decision threshold policy over many trials influences long-term learning dynamics and total reward. Future work could combine these approaches to understand how Bayesian updating on each trial would change long-term learning dynamics, and potentially, the optimality of different threshold strategies.”</p><disp-quote content-type="editor-comment"><p>– Currently the evolution of the mean RT during learning is examined. Plotting the change in the RT distribution (averaged/vincentised across participants) can be more informative about changes in the strategy being used than plotting the mean RT alone.</p></disp-quote><p>We have provided the change in correct RT distributions across learning in Figure 6—figure supplement 2. The change in RT distributions for stimulus pair 1 and 2 is qualitatively similar (slower RTs get faster, and faster RTs remain about the same) suggesting a similar mechanism (increase in drift accompanied by a decrease in threshold, which is indeed what we see in our DDM fits).</p><disp-quote content-type="editor-comment"><p>– The results of experiment 3 are interesting, but at the same time the behaviour in the transparent group requires more scrutiny. How do rats behave in this condition? It appears that random choice in combination with heuristic strategies (e.g. win-stay lose-switch) are viable possibilities. The argument that the SNR is intrinsically zero in this task. However, the signal in this experiment may contain non-zero, irrelevant information (such as choice history or feedback information). Plotting the RT distributions as a function of learning could provide insight in this regard, because boundary changes and SNR changes manifest differently in the shapes of RT distributions.</p></disp-quote><p>We have provided the change in vincentized correct RT distributions as a function of learning for the transparent stimuli experiment in Figure 6—figure supplement 2. The change in the RT distribution for transparent stimuli differs from that of stimulus pair 1 and 2 (slow and fast RTs all get faster), suggesting a different mechanism (decrease in threshold with a constant drift rate, as supported by our DDM fits in Figure 6—figure supplement 3). We added the following sentence to the text noting this: “Additionally, we considered the rats' entire <italic>RT</italic> distributions to investigate the effect of learnability beyond <italic>RT</italic> means. We found that while the <italic>RT</italic> distributions changed similarly from the beginning to end of learning for the learnable stimuli (stimulus pair 1 and 2), they differed for the unlearnable (transparent) stimuli, indicating an effect of learnability on the entire <italic>RT</italic> distributions (Figure 6—figure supplement 2). Hence rodents are capable of modulating their strategy depending on their learning prospects.”</p><p>The reviewers raise the point that the rats may be using non-stimulus information in order to sustain an SNR &gt; 0. In fact, HDDM fits of this experiment (Figure 6—figure supplement 3) reveal that although drift rate does decrease with the transparent stimuli, it is still above 0. That being said, we observe a systematic decrease in threshold throughout learning with the transparent stimuli, in agreement with our model. This monotonic decrease in threshold is in contrast to the experiment with new visible stimuli (stimulus pair 2), where we also first observe a decrease in drift, but that decrease in drift is accompanied with an increase in threshold (Figure 4—figure supplements 2-4). After learning, threshold decays back to its baseline state, and drift also increases to match its baseline state (Figure 4—figure supplement 2-4).</p><p>To investigate whether the rats implemented stimulus-independent heuristic strategies in addition to random choice, we measured left/right bias and quantified the weights of bias, perseverance (choose the same port as previous trial) and win-stay/lose-switch (choose the port that was correct on the previous trial) in addition to the stimulus presented using a dynamic generalized linear model (PsyTrack: Roy et al., 2021; Figure 6—figure supplement 4). In general, bias seemed to increase with transparent stimuli in the direction that each individual was already biased during visible stimuli. The weights for perseverance and winstay/lose-switch also seemed to increase and fluctuate more during transparent stimuli, suggesting a greater reliance on these heuristics now that the stimulus was uninformative (stimulus weights dropped to 0, whereas they were strongly positive or negative during visible stimuli depending on each individual’s left/right stimulus mapping).</p><p>These results support the reviewers’ suggestion that the rats may indeed be relying more on stimulus-independent heuristic strategies. Although the rats may be sustaining a non-zero drift rate through stimulus-independent information, this might amount to a “monitoring” of the task in case any changes or reliably informative patterns do arise, rather than a strategy to improve perceptual learning as we argue is the case with the new visible stimuli.</p><p>We note that we still observe a monotonic decrease in threshold in our DDM fits, and a non-zero drift rate due to these heuristics does not contravene our conclusions.</p><p>Regarding heuristic strategies, we have added the following paragraph to the text:</p><p>“Although there is no informative signal in this task with transparent stimuli, the rats could still be using stimulus-independent signals, such as choice history or feedback, to drive heuristic strategies. Indeed, DDM fits indicated a non-zero drift rate even in the absence of informative stimuli (Figure 6—figure supplement 3 ). To investigate whether the rats implemented stimulus-independent heuristic strategies in addition to random choice, we measured left/right bias and quantified the weights of bias, perseverance (choose the same port as the previous trial) and win-stay/lose-switch (choose the port that was correct on the previous trial) [58]. In general, bias seemed to increase with transparent stimuli in the direction that each individual was already biased during visible stimuli. Perseverance and win-stay/lose-switch also seemed to increase and fluctuate more during transparent stimuli, suggesting a greater reliance on these heuristics now that the stimulus was uninformative (Figure 6—figure supplement 4). Engaging these heuristics may be a way that the rats expedited their choices in order to maximize <italic>iRR</italic> while still ``monitoring&quot; the task for any potentially informative changes or patterns. Despite the fact that the animals' still engaged these non-optimal heuristics, the lack of learnability in the transparent stimuli still led to a change in strategy that was distinct from that with learnable stimuli.”</p><disp-quote content-type="editor-comment"><p>– The use of RNNs is not sufficiently supported beyond classical drift-diffusion modeling. The authors should either reframe the specific role of RNNs for supporting their key findings, or possibly remove them if they do not provide unique insights beyond classical drift-diffusion modeling.</p></disp-quote><p>Please see the response above to point #1.</p><disp-quote content-type="editor-comment"><p>– The authors should clarify early on in the manuscript what &quot;learning&quot; means in their model and theory, and why it makes sense to start with a low input weight (after describing the meaning of the input weight). The authors should also explain what T0 and D_RSI are, how they are determined (and the choices made to determine them), and how these parameters impact the results (also related to Figure S13). In particular the relationship between RT and DT is already required to understand Figure 1c, and many plots thereafter.</p></disp-quote><p>We added a better explanation of what “learning” means in the LDDM. We also explained T0 in more detail early in the Results section. We also simplified our timing terms to remove confusion around D_RSI.</p><p>Please see the full response to this comment above point #1.</p><disp-quote content-type="editor-comment"><p>– Box 1 provides some details of the model, but leaves out others – e.g., the different sources of noise in the model. From Box 1 alone, it is unclear how the asymptotic SNR or the iRR-sensitive threshold are computed. It is indeed nice that it is possible to derive Equation (3), but the equation itself is not particularly informative for the exposition of the main findings, and so it could be moved to the Methods section.</p></disp-quote><p>We replaced Box 1 with a more complete explanation of the model in the main text (Linear Drift-Diffusion Model (LDDM) section), including information on the asymptotic SNR and the threshold policies. We believe that including Equation 3 in the main text is important to demonstrate that the dynamics of learning in the RNN can be expressed as these specific dynamics for SNR in DDM terms, making it readily applicable to other settings. It is in our view a central result, not a method.</p></body></sub-article></article>