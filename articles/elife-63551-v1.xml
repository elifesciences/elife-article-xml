<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63551</article-id><article-id pub-id-type="doi">10.7554/eLife.63551</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Temporo-parietal cortex involved in modeling one’s own and others’ attention</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-140502"><name><surname>Guterstam</surname><given-names>Arvid</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3694-1318</contrib-id><email>arvidg@princeton.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-209760"><name><surname>Bio</surname><given-names>Branden J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-209761"><name><surname>Wilterson</surname><given-names>Andrew I</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-31478"><name><surname>Graziano</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, Princeton University, Department of Psychology</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Clinical Neuroscience, Karolinska Institutet</institution><addr-line><named-content content-type="city">Stockholm</named-content></addr-line><country>Sweden</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>02</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e63551</elocation-id><history><date date-type="received" iso-8601-date="2020-09-28"><day>28</day><month>09</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-02-04"><day>04</day><month>02</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Guterstam et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Guterstam et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63551-v1.pdf"/><abstract><p>In a traditional view, in social cognition, attention is equated with gaze and people track other people’s attention by tracking their gaze. Here, we used fMRI to test whether the brain represents attention in a richer manner. People read stories describing an agent (either oneself or someone else) directing attention to an object in one of two ways: either internally directed (endogenous) or externally induced (exogenous). We used multivoxel pattern analysis to examine how brain areas within the theory-of-mind network encoded attention type and agent type. Brain activity patterns in the left temporo-parietal junction (TPJ) showed significant decoding of information about endogenous versus exogenous attention. The left TPJ, left superior temporal sulcus (STS), precuneus, and medial prefrontal cortex (MPFC) significantly decoded agent type (self versus other). These findings show that the brain constructs a rich model of one’s own and others’ attentional state, possibly aiding theory of mind.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>attention</kwd><kwd>theory of mind</kwd><kwd>fMRI</kwd><kwd>self</kwd><kwd>awareness</kwd><kwd>TPJ</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001388</institution-id><institution>Wenner-Gren Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Guterstam</surname><given-names>Arvid</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003792</institution-id><institution>Swedish Brain Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Guterstam</surname><given-names>Arvid</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001727</institution-id><institution>Sweden-America Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Guterstam</surname><given-names>Arvid</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009389</institution-id><institution>Promobilia Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Guterstam</surname><given-names>Arvid</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008141</institution-id><institution>Princeton Institute for International and Regional Studies</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Guterstam</surname><given-names>Arvid</given-names></name><name><surname>Bio</surname><given-names>Branden J</given-names></name><name><surname>Wilterson</surname><given-names>Andrew I</given-names></name><name><surname>Graziano</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Functional brain scans of human participants show that the brain encodes other people's attention in enough richness to distinguish whether that attention was directed exogenously (stimulus-driven) or endogenously (internally driven).</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Reconstructing someone else’s attentional state is of central importance in theory of mind (<xref ref-type="bibr" rid="bib3">Baron-Cohen, 1997</xref>; <xref ref-type="bibr" rid="bib7">Calder et al., 2002</xref>; <xref ref-type="bibr" rid="bib20">Graziano, 2013</xref>). By identifying the object of someone else’s attention, and having some intuitive understanding of the complex dynamics and consequences of attention, one can reconstruct at least some of the other person’s likely thoughts, intentions, and emotions, and make predictions about that person’s behavior. Almost all work on how people reconstruct the attention of others has focused on gaze direction. For example, the human eye has a high contrast between pupil and sclera, possibly an adaptation for better gaze tracking (<xref ref-type="bibr" rid="bib37">Kobayashi and Kohshima, 1997</xref>). The superior temporal sulcus in monkeys and humans may contain specialized neural circuitry for processing gaze direction (<xref ref-type="bibr" rid="bib33">Hoffman and Haxby, 2000</xref>; <xref ref-type="bibr" rid="bib44">Marquardt et al., 2017</xref>; <xref ref-type="bibr" rid="bib52">Perrett et al., 1985</xref>; <xref ref-type="bibr" rid="bib56">Puce et al., 1998</xref>; <xref ref-type="bibr" rid="bib68">Wicker et al., 1998</xref>). Seeing a face gaze at an object automatically draws one’s own attention to the object (<xref ref-type="bibr" rid="bib15">Friesen and Kingstone, 1998</xref>; <xref ref-type="bibr" rid="bib16">Frischen et al., 2007</xref>). These and other findings show the importance of reconstructing gaze direction in social cognition.</p><p>To be adaptive in aiding theory of mind, however, a model of attention should be far more than a vector indicating gaze direction. We previously suggested that the human brain constructs a rich, dynamic, and predictive model of other people’s attention (<xref ref-type="bibr" rid="bib21">Graziano, 2019</xref>; <xref ref-type="bibr" rid="bib20">Graziano, 2013</xref>; <xref ref-type="bibr" rid="bib22">Graziano and Kastner, 2011</xref>). The model should contain information about different types of attention, about the rapidity or sluggishness with which attention tends to move from item to item, about how external factors such as salience and clutter are likely to affect a person’s attention, and about how attention profoundly affects thought, memory, and behavior. In the proposal, that deeper model is constrained by incoming information, including gaze direction. However, other cues can also constrain the model. People rely on the other person’s body posture, on cues in the surrounding environment, on speech, and on social context. For example, blind people must be able to build models of other people’s attention without seeing the other person’s eyes. Likewise, during a phone conversation, we cannot see the other person and yet we intuitively understand whether that person is attending to what we have said or is distracted by her own words or by a salient event on her end of the line.</p><p>Several recent experiments provide evidence for an automatically constructed model of the attention of others that may go beyond merely registering gaze direction (<xref ref-type="bibr" rid="bib25">Guterstam et al., 2019</xref>; <xref ref-type="bibr" rid="bib27">Guterstam and Graziano, 2020a</xref>; <xref ref-type="bibr" rid="bib36">Kelly et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Pesquita et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Randall and Guterstam, 2020</xref>; <xref ref-type="bibr" rid="bib66">Vernet et al., 2019</xref>). For example, <xref ref-type="bibr" rid="bib53">Pesquita et al., 2016</xref> found that when participants watch an actor in a video attending to an object, the participants implicitly distinguish between whether the actor’s attention was drawn to the object exogenously (bottom-up or stimulus-driven attention), or whether the actor endogenously shifted attention to the object (top-down or internally driven attention). Exogenous and endogenous attention are the two principal ways in which selective attention moves between objects. They are emphasized in distinct cortical networks (the ventral and dorsal attention networks), and they influence the behavior of agents in profoundly different manners (<xref ref-type="bibr" rid="bib8">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib55">Posner, 1980</xref>; <xref ref-type="bibr" rid="bib63">Shulman et al., 2010</xref>). The ability to distinguish between someone else’s exogenous and endogenous attention is therefore one example of how people may construct a rich, dynamic, and useful model of other people’s attention beyond merely encoding gaze direction or identifying the object of attention.</p><p>Inspired by the vignette-style tasks widely used in studies on theory of mind (<xref ref-type="bibr" rid="bib12">Fletcher et al., 1995</xref>; <xref ref-type="bibr" rid="bib18">Gallagher et al., 2000</xref>; <xref ref-type="bibr" rid="bib29">Happé, 1994</xref>; <xref ref-type="bibr" rid="bib59">Saxe and Kanwisher, 2003</xref>; <xref ref-type="bibr" rid="bib67">Vogeley et al., 2001</xref>), in the present study, we used functional magnetic resonance imaging (fMRI) and multi-voxel pattern analysis (MVPA) to study brain activity in participants while they read brief stories about people’s attention (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Some of the stories implied that attention was being attracted exogenously (‘Kevin walks into his closet and notices the bright red tie…') and some stories implied that attention was being directed endogenously (‘Kevin walks into his closet and looks for the bright red tie…'). We also included analogous stories written in the first person, casting the subject of the experiment as the agent (‘You walk into your closet and notice the bright red tie…'). The study therefore used a 2 × 2 design (exogenous versus endogenous attention X self agent versus other agent). Finally, we included a fifth, control condition, consisting of nonsocial stories in which the agent was replaced by an inanimate object, that, like attention, has a source and a target, such as a camera or a light source (‘In a closet, a light shines on a red tie…').</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Methods.</title><p>(<bold>A</bold>) Schematic timeline of the fMRI design. In each trial, subjects were presented with a short story for 10 s, describing a scene in which an agent attended to an object in the environment. A probe statement was then shown for 4 s, relating to either the story’s spatial context or object property, to which the subjects responded either true or false by button press. (<bold>B</bold>) The agent in the story was either the subject him-/herself (self) or another person (other), and directed attention to the object endogenously (internally driven attention) or exogenously (stimulus-driven attention), yielding a 2 × 2 factorial design of attention type × agent. We created 80 unique stories in four different versions, one for each condition. We made minimal changes to the wordings to keep the story versions as semantically similar as possible. Green highlighting indicates wording specifying agent, yellow highlighting indicates wording specifying attention type (colors not part of actual visual stimuli). For each story, each subject saw only one of the four versions (balanced across subjects). We also included a nonsocial control condition (20 unique stories based on a subset of the 80 social stories) in which the agent was replaced by a non-human object.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig1-v1.tif"/></fig><p>We made four predictions. Our first, central prediction was inspired by the <xref ref-type="bibr" rid="bib53">Pesquita et al., 2016</xref> study described above. We hypothesized that participants would encode the type of attention in the story (exogenous versus endogenous), and that this encoding would be evident in some subset of the areas classically involved in theory of mind. Previous experiments on theory of mind typically recruited a network of cortical areas including the temporoparietal junction (TPJ), the superior temporal sulcus (STS), the medial prefrontal cortex (MPFC), and the precuneus (<xref ref-type="bibr" rid="bib18">Gallagher et al., 2000</xref>; <xref ref-type="bibr" rid="bib59">Saxe and Kanwisher, 2003</xref>; <xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>; <xref ref-type="bibr" rid="bib67">Vogeley et al., 2001</xref>). This first prediction, that the social cognition network will encode the exogenous-versus-endogenous distinction, represents the main, novel contribution of this study. Previous studies have used MVPA to decode various aspects of other people’s mental states from activity in social brain areas, such as their beliefs (<xref ref-type="bibr" rid="bib40">Koster-Hale et al., 2017</xref>), intentions (<xref ref-type="bibr" rid="bib38">Koster-Hale et al., 2013</xref>), and perceptual source (<xref ref-type="bibr" rid="bib39">Koster-Hale et al., 2014</xref>). To the best of our knowledge, this investigation is the first to test whether activity in social brain areas can decode other people’s attentional states.</p><p>Our second prediction was that participants would encode information about the agent in the story (self versus other), and that this encoding would again be evident in some subset of the areas classically involved in theory of mind. Self-versus-other encoding has been examined in previous studies, and found to be reflected in the theory-of-mind network (e.g. <xref ref-type="bibr" rid="bib49">Northoff et al., 2006</xref>; <xref ref-type="bibr" rid="bib50">Ochsner et al., 2004</xref>; <xref ref-type="bibr" rid="bib51">Passingham et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Qin and Northoff, 2011</xref>; <xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>). This second prediction represents a test of whether our present paradigm, using subtle wording differences between similar sentences, can produce results consistent with previous findings.</p><p>Third, we predicted that participants would encode information associated with the interaction between the two factors. We predicted that at least some subset of the areas in the theory-of-mind network may encode the type of attention (exogenous versus endogenous) to a different extent in self-related stories as compared to other-related stories.</p><p>Fourth and finally, we tested for brain regions that encoded the distinction between social stories (with human agents) and nonsocial stories (with only non-agent objects). We predicted that this social-versus-nonsocial encoding would again be evident in the same network of brain regions noted above, that are known to be involved in theory of mind. This final analysis served as a control to check on the validity of the story stimuli and confirm that they engaged social cognition as expected.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral results</title><p>Participants answered a simple true-or-false probe question after each story (e.g. ‘Emma is on a bus’), to ensure alertness throughout the experiment. Although the MRI results depended on the time interval during the reading of the story and not during the reading or answering of the probe question, the behavioral response to the probe question may give some indication of whether the story conditions were well balanced. If one type of story was more difficult to process, or caused the subjects to think more deeply about the character in the story, that difference may be reflected in a different accuracy or latency in responding to the subsequent probe question. However, no significant differences were observed in accuracy (F<sub>3,93</sub>=0.15, p=0.930, ANOVA) or latency (F<sub>3,93</sub>=1.66, p=0.181, ANOVA) among the four social story conditions (<xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Behavioral results.</title><p>Mean accuracy and latency for the probe question that was presented after each story, for each of the five experimental conditions. The mean accuracy and latency across all social story conditions are also reported.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Story type</th><th>Self-endo</th><th>Self-exo</th><th>Other-endo</th><th>Other-exo</th><th>Non-social</th><th>Social (all)</th></tr></thead><tbody><tr><td rowspan="2">Accuracy (%)</td><td>Mean</td><td>94.2</td><td>93.4</td><td>93.1</td><td>93.4</td><td>90.0</td><td>93.6</td></tr><tr><td>SEM</td><td>1.9</td><td>1.8</td><td>1.6</td><td>1.9</td><td>2.2</td><td>1.4</td></tr><tr><td rowspan="2">Latency (ms)</td><td>Mean</td><td>1613</td><td>1601</td><td>1667</td><td>1653</td><td>1957</td><td>1634</td></tr><tr><td>SEM</td><td>54</td><td>56</td><td>46</td><td>56</td><td>63</td><td>49</td></tr></tbody></table></table-wrap><p>When participants responded to the probe questions after non-social, control stories, versus when they responded to the probe questions after social stories, no significant difference in accuracy was found (t<sub>31</sub> = −1.83, p=0.077), although as expected, a slightly longer latency was observed after non-social stories than after social stories (t<sub>31</sub> = 9.18, p&lt;0.001; see <xref ref-type="table" rid="table1">Table 1</xref>). The reason for the latency difference is almost certainly because the probe statements in the non-social condition were on average two words longer than those in the social conditions (eight words versus six words), because the character in the probe statements (&quot;You …&quot; or &quot;Emma …&quot;) was replaced by an object that required more words to describe (e.g. &quot;The surveillance camera …&quot;). One might therefore expect that it took participants a little longer to read the non-social probe statements compared to the social ones. We suggest that this subtle difference in latency during the post-story question period is unlikely to have affected the comparison of MRI activity between social and non-social conditions, since the relevant MRI activity was evoked by the time period during the reading of the story, not during the reading and answering of the questions.</p></sec><sec id="s2-2"><title>Prediction 1</title><p>We hypothesized that participants would encode the attentional state of the agents in the stories in enough detail to distinguish between endogenous and exogenous attention, even though the difference between the story types was extremely subtle – only a few words that very slightly altered the semantic meaning of the sentences. We made the strong prediction that decoding would be found within the set of brain areas typically included in the theory-of-mind cortical network. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows six ROIs within the theory-of-mind network, based on a meta-analysis of previous theory-of-mind studies (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>). <xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the results (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for more details). Decoding accuracy for endogenous versus exogenous stories was significantly above chance for the left TPJ, and the significance of the left TPJ decoding survived a multiple comparison correction for the six ROIs (mean decoding accuracy 52.9%, 95% CI 50.7–55.2, p<sub>uncorrected</sub> = 0.0046, p<sub>FDR-corrected</sub> = 0.0276). For the sake of a thorough evaluation, because different researchers have defined slightly different locations for the TPJ, we replicated the finding of a significant decoding in the left TPJ using three additional, previously reported theory-of-mind ROIs in the left TPJ (<xref ref-type="bibr" rid="bib43">Mar, 2011</xref>; <xref ref-type="bibr" rid="bib47">Molenberghs et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Schurz et al., 2014</xref>), suggesting that the effect is robust (<xref ref-type="fig" rid="fig3">Figure 3B</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). The results therefore show that activity in the left TPJ allowed for significant decoding of the attentional state – exogenous versus endogenous – of agents in a story.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Regions of interest (ROIs).</title><p>Six ROIs were defined based on peaks reported in an activation likelihood estimation meta-analysis of 16 fMRI studies involving theory-of-mind reasoning (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>). The ROIs consisted of 10-mm-radius spheres centered on peaks in the bilateral temporoparietal junction (TPJ) and superior temporal sulcus (STS), and two midline structures: the precuneus and medial prefrontal cortex (MPFC). Here, the TPJ and STS ROIs on the left side are shown.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig2-v1.tif"/></fig><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Decoding attention type, agent, and the interaction between them, in six brain areas.</title><p>For definition of ROIs, see <xref ref-type="fig" rid="fig2">Figure 2</xref>. Each point shows mean decoding accuracy. Error bars show SEM. Red horizontal line indicates chance level decoding. Significance indicated by * (p&lt;0.05) and ** (p&lt;0.01), based on permutation testing (all significant p values also survived FDR correction for multiple comparisons across all six ROIs [all significant corrected ps &lt;0.05]). (<bold>A</bold>) The ability of a classifier, trained on BOLD activity patterns within each ROI, to decode endogenous (endo) versus exogenous (exo) attention. (<bold>B</bold>) To test the robustness of the endo-versus-exo decoding in the left TPJ, we replicated the results in three ROIs derived from theory-of-mind neuroimaging meta-analyses (<xref ref-type="bibr" rid="bib43">Mar, 2011</xref>; <xref ref-type="bibr" rid="bib47">Molenberghs et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Schurz et al., 2014</xref>) other than the one used for the main analysis (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>) (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for details). (<bold>C</bold>) The cluster shown had the highest decoding accuracy in the whole-brain, searchlight analysis, for the endo-versus-exo comparison. No clusters in this analysis survived brain-wide correction for multiple comparisons. We here report clusters surviving the conventional uncorrected voxelwise threshold p&lt;0.001, for purely descriptive purposes. See <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for details. (<bold>D</bold>) Decoding accuracy for agent (self versus other). (<bold>E</bold>) Decoding accuracy for the interaction between type of attention and agent. (<bold>F</bold>) The ability of a classifier, trained to discriminate attention type in self stories, to decode attention type in other stories, and vice versa (i.e. two-way cross-classification), based on activity patterns in the left TPJ ROI.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Decoding attention type, agent, and the interaction between the two, within the six ROIs.</title><p>Violin plots showing mean (black horizontal line) and median (white circle) decoding accuracy (%), 95% confidence interval around the mean based on bootstrap distribution (black vertical line), a kernel density estimation (shaded area), and individual data points for each of the six ROIs. The horizontal red line indicates chance decoding. *p&lt;0.05, **p&lt;0.01.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Replicating attention type decoding results in additional TPJ ROIs.</title><p>To test the robustness of the endogenous-versus-exogenous decoding in the left TPJ, we replicated the results in three ROIs derived from theory-of-mind neuroimaging meta-analyses (<xref ref-type="bibr" rid="bib43">Mar, 2011</xref>; <xref ref-type="bibr" rid="bib47">Molenberghs et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Schurz et al., 2014</xref>) other than the one used for the main analysis (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>). The ROIs were defined as 10-mm-radius spheres around the peak left TPJ coordinates reported in <xref ref-type="bibr" rid="bib43">Mar, 2011</xref> (MNI: −53,–56, 22), <xref ref-type="bibr" rid="bib62">Schurz et al., 2014</xref> (MNI: −55,–59, 20), and <xref ref-type="bibr" rid="bib47">Molenberghs et al., 2016</xref> (MNI: −52,–58, 22). The results showed significant decoding in the Mar ROI (mean decoding accuracy 53.4%, 95% CI 51.0–55.1, p=0.0023), the Schurz ROI (mean decoding accuracy 52.7%, 95% CI 50.8–54.4, p=0.0092), and in the Molenberghs ROI (mean decoding accuracy 53.4%, 95% CI 51.3–55.2, p=0.0020). Violin plots show mean (black horizontal line) and median (white circle) decoding accuracy (%), 95% confidence interval around the mean based on bootstrap distribution (black vertical line), a kernel density estimation (shaded area), and individual data points for the two ROIs. The red horizontal line indicates chance decoding. **p&lt;0.01.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Decoding attention type at the whole-brain level.</title><p>The cluster shown had the highest decoding accuracy in the whole-brain, searchlight analysis, for the endogenous-versus-exogenous comparison. See <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for numerical details. Top: projected onto a 3D canonical brain surface. Bottom: projected onto sections of the average structural scan generated from the 32 subjects for anatomical localization. For display purposes, the statistical threshold for the activation maps was set to p&lt;0.001, uncorrected.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Decoding agent type at the whole-brain level.</title><p>The cluster shown had the highest decoding accuracy in the whole-brain, searchlight analysis, for the self-versus-other comparison. See <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> for numerical details. Top: projected onto a 3D canonical brain surface. Bottom: projected onto a parasagittal section of the average structural scan generated from the 32 subjects for anatomical localization. For display purposes, the statistical threshold for the activation maps was set to p&lt;0.001, uncorrected.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Decoding attention-by-agent interaction at the whole-brain level.</title><p>The cluster shown had the highest decoding accuracy difference in the whole-brain, searchlight analysis, for the (endogenous-versus-exogenous)<sub>SELF</sub> versus (endogenous-versus-exogenous)<sub>OTHER</sub> comparison. See <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref> for numerical details. Top: projected onto a 3D canonical brain surface. Bottom: projected onto a parasagittal section of the average structural scan generated from the 32 subjects for anatomical localization. For display purposes, the statistical threshold for the activation maps was set to p&lt;0.001, uncorrected.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp5-v1.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>Decoding attention type in the dorsal attention network.</title><p>The TPJ is part of the ventral attention network, which has been implicated in exogenous attention. Could it be the case that participants simulate attention orienting when reading the stories, leading to the above-chance endogenous-versus-exogenous decoding in the TPJ? If this is the case, one should expect above-chance decoding also in areas of the dorsal attention network, which has been implicated in endogenous attention. To test this hypothesis, we replicated the endogenous-versus-exogenous analysis in four areas (bilaterally) typically considered to constitute the dorsal attention network: the frontal eye fields (FEF), the anterior and posterior intraparietal sulcus (aIPS and pIPS), and the middle temporal complex (MT+). The ROIs were defined as 10-mm-radius spheres around the peak coordinates (reported in <xref ref-type="bibr" rid="bib14">Fox et al., 2006</xref>): left FEF (MNI: −25,–16, 61), right FEF (MNI: 27,–14, 58), left aIPS (MNI: −42,–44, 46), right aIPS (MNI: 34,–51, 49), left pIPS (MNI: −22,–72, 51), right pIPS (MNI: 19,–72, 57), left MT+ (MNI: −47,–68, −14), and right MT+ (MNI: 51,–63, −14). The results showed that none of the dorsal attention network ROIs decoded the attention type in the stories better than chance (all ps &gt; 0.05, based on permutation testing with 10,000 iterations, uncorrected for multiple comparisons). Violin plots show mean (black horizontal line) and median (white circle) decoding accuracy (%), 95% confidence interval around the mean based on bootstrap distribution (black vertical line), a kernel density estimation (shaded area), and individual data points. The red horizontal line indicates chance decoding.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp6-v1.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Decoding of attention type generalizes across self-stories and other-stories.</title><p>Violin plot showing the mean (black horizontal line) and median (white circle) decoding accuracy (%), 95% confidence interval around the mean based on bootstrap distribution (black vertical line), a kernel density estimation (shaded area), and individual data points for a two-way cross-classification analysis based on activity patterns in the left TPJ ROI. The red horizontal line indicates chance decoding. *p&lt;0.05.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp7-v1.tif"/></fig><fig id="fig3s8" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 8.</label><caption><title>Eye tracking results.</title><p>Subjects tended to fixate in a similar spatial pattern across the screen, and engage in similar saccade dynamics, regardless of the type of story presented. No significant ability to decode the story type based on the pattern of eye movement was obtained. The red rectangle in the heat maps in panels A-E indicates the area of the screen within which the story text appeared.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp8-v1.tif"/></fig><fig id="fig3s9" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 9.</label><caption><title>Univariate fMRI results within ROIs.</title><p>The difference in average activity across all voxels within each ROI, for the endogenous-versus-exogenous, self-versus-other, and social-versus-nonsocial contrasts, are shown. Violin plot showing the mean (colored horizontal line) and median (white circle) contrast estimate, interquartile range (black vertical line), a kernel density estimation (shaded area), and individual data points. None of the contrasts were statistically significant after controlling for multiple comparisons across all six ROIs (all FDR-corrected ps &gt;0.21).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig3-figsupp9-v1.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Decoding attention type, agent, and the interaction between the two, within the six ROIs.</title><p>For definition of ROIs, see <xref ref-type="fig" rid="fig2">Figure 2</xref>. Mean decoding accuracy (%), 95% confidence interval (based on bootstrap distribution), and p value (based on permutation testing) are shown for each of the six ROIs. Results shown for decoding endogenous (endo) versus exogenous (exo) attention type, self versus other agent type, and the interaction between the two variables. * indicates significant p values that survived correction for multiple comparisons across all six ROIs (FDR-corrected p&lt;0.05).</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top"/><th>L TPJ</th><th>R TPJ</th><th>L STS</th><th>R STS</th><th>MPFC</th><th>Precuneus</th></tr></thead><tbody><tr><td rowspan="3">Endo vs. Exo</td><td>Mean accuracy</td><td>52.9%</td><td>51.4%</td><td>50.4%</td><td>48.0%</td><td>49.5%</td><td>50.2%</td></tr><tr><td> 95% CI</td><td>50.7–55.2</td><td>49.1–53.9</td><td>47.8–52.8</td><td>45.9–50.1</td><td>47.6–51.4</td><td>48.5–51.8</td></tr><tr><td> P value</td><td>0.0046*</td><td>0.1148</td><td>0.3518</td><td>0.9547</td><td>0.6439</td><td>0.4428</td></tr><tr><td rowspan="3">Self vs. Other</td><td>Mean accuracy</td><td>53.0%</td><td>51.0%</td><td>52.3%</td><td>51.3%</td><td>52.6%</td><td>52.7%</td></tr><tr><td> 95% CI</td><td>50.1–55.6</td><td>48.5–53.4</td><td>50.6–54.1</td><td>48.9–54.1</td><td>50.5–55.0</td><td>50.4–55.0</td></tr><tr><td> P value</td><td>0.0053*</td><td>0.1974</td><td>0.0204*</td><td>0.1241</td><td>0.0105*</td><td>0.0099*</td></tr><tr><td rowspan="3">(Self vs. Other) × (Endo vs. Exo)</td><td>Mean accuracy diff</td><td>1.6%</td><td>1.5%</td><td>2.0%</td><td>−3.0%</td><td>2.5%</td><td>0.6%</td></tr><tr><td> 95% CI</td><td>−2.7–6.3</td><td>−3.6–6.5</td><td>−2.7–6.3</td><td>−7.4–1.1</td><td>−2.3–6.7</td><td>−5.5–5.5</td></tr><tr><td> P value</td><td>0.2430</td><td>0.2639</td><td>0.1967</td><td>0.8944</td><td>0.1414</td><td>0.3900</td></tr></tbody></table></table-wrap><p>We then used a searchlight analysis over the whole brain to test whether any further areas may have significantly decoded the endogenous-versus-exogenous distinction. It should be noted that an exploratory searchlight analysis, compared to ROI analyses based on strong predictions, is much more statistically conservative because of the brain-wide correction for multiple comparisons. Its usefulness is that it may reveal any cluster of very strong decoding that was missed by the more sensitive analysis restricted to the ROIs. We found no brain-wide significant clusters of decoding for the endogenous versus exogenous distinction. However, four clusters survived the uncorrected p&lt;0.001 threshold, and are reported in a purely descriptive manner in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. The brain-wide searchlight peak was located in the left posterior STS (decoding accuracy 53.7%, t = 4.21, p&lt;0.001 uncorrected; <xref ref-type="fig" rid="fig3">Figure 3C</xref>), at a distance of 20 mm from the center of the left TPJ ROI, and coincided with the posterior TPJ (TPJp) subregion as defined by <xref ref-type="bibr" rid="bib6">Bzdok et al., 2013</xref> and <xref ref-type="bibr" rid="bib45">Mars et al., 2012</xref>.</p><p>To control for potential univariate effects that could drive classifier performance, we explored the <italic>endogenous &gt; exogenous</italic> and <italic>exogenous &gt; endogenous</italic> univariate contrasts, which did not reveal any significant activity within the ROIs (<xref ref-type="fig" rid="fig3s9">Figure 3—figure supplement 9</xref>) or anywhere else in the brain, not even at the uncorrected threshold p&lt;0.001 (<xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). These findings are compatible with previous studies (e.g. <xref ref-type="bibr" rid="bib30">Hassabis et al., 2009</xref>) that have demonstrated the superiority of pattern-sensitive multivariate analyses compared with conventional univariate approaches for detecting differences in activity between conditions with highly similar macroscopic characteristics.</p><p>In addition to these planned analyses, we explored the endogenous-versus-exogenous decoding within dorsal attention network regions. This analysis was motivated by an alternative hypothesis: people might simulate the act of attention orienting when reading the exogenous and endogenous stories, and thus activate the corresponding ventral (exogenous) attention network, to which the TPJ belongs, and dorsal (endogenous) attention network in a ‘mirror-neuron-like’ fashion (see Discussion for details). However, this control analysis revealed no significant decoding in any of the dorsal attention network ROIs (<xref ref-type="fig" rid="fig3s6">Figure 3—figure supplement 6</xref>).</p></sec><sec id="s2-3"><title>Prediction 2</title><p>We hypothesized that participants would process the distinction between the two types of agent in the stories (self versus other). We made the strong prediction that decoding would be found within the same set of ROIs in the theory-of-mind cortical network. <xref ref-type="fig" rid="fig3">Figure 3D</xref> shows the results (see <xref ref-type="table" rid="table2">Table 2</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for more details). Decoding accuracy for self versus other stories was significantly above chance, and survived a multiple comparisons correction, for the left TPJ (mean decoding accuracy 53.0%, 95% CI 50.1 to –55.6, p<sub>uncorrected</sub> = 0.0053, p<sub>FDR-corrected</sub> = 0.0210), left STS (mean decoding accuracy 52.3%, 95% CI 50.6–54.1, p<sub>uncorrected</sub> = 0.0204, p<sub>FDR-corrected</sub> = 0.0306), MPFC (mean decoding accuracy 52.6%, 95% CI 50.5–55.0, p<sub>uncorrected</sub> = 0.0105, p<sub>FDR-corrected</sub> = 0.0210), and precuneus (mean decoding accuracy 52.7%, 95% CI 50.4–55.0, p<sub>uncorrected</sub> = 0.0099, p<sub>FDR-corrected</sub> = 0.0210). These results confirm that the present paradigm, using stories that are subtly different from each other, can obtain social cognitive results that are consistent with previous findings.</p></sec><sec id="s2-4"><title>Prediction 3</title><p>We hypothesized that areas in the theory-of-mind network would not only encode the distinction between endogenous and exogenous attention, but do so to a significantly different extent in self-related stories than in other-related stories. However, the results showed no significant interaction in any of the ROIs (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, <xref ref-type="table" rid="table2">Table 2</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Thus, we found no support for prediction 3.</p><p>An alternative hypothesis is that attention type is encoded similarly in self and others. In a <italic>post hoc</italic> analysis, we focused on the left TPJ which was the only ROI that showed significant attention type decoding (<italic>Prediction 1</italic>), and tested for overlap in attention encoding in self and others using a two-way cross-classification analysis (see <xref ref-type="fig" rid="fig3">Figure 3F</xref> and <xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7</xref>). In this analysis, one classifier was trained to discriminate endogenous versus exogenous self-stories and tested on other-stories, and another classifier was trained to discriminate endogenous versus exogenous other-stories and tested on self-stories, from which an average cross-classification decoding accuracy for the left TPJ was obtained. Endogenous-versus-exogenous decoding significantly generalized across self-stories and other-stories (mean decoding accuracy 51.9%, 95% CI 49.9–54.1, p=0.0393), suggesting at least some degree of overlap in the encoding of attention in others and in oneself.</p></sec><sec id="s2-5"><title>Prediction 4</title><p>Finally, we asked whether the activity in the theory-of-mind network would distinguish between social stories and nonsocial stories. This final analysis served as a control to check the validity of the story stimuli and confirm that they engaged social cognition as expected. We expected a signal of much greater magnitude in this analysis than in the analyses described above. The reason is that, as noted above, the types of social stories differed from each other by only a few words, and were nearly identical in semantic content; thus any brain signal reflecting those differences is expected to be subtle. The distinction between social and nonsocial stories, however, was much greater semantically, and therefore the evidence of decoding in the brain is expected to be of greater magnitude.</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows the results (see <xref ref-type="table" rid="table3">Table 3</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for more details). The results are separated into six ROIs, and for each ROI, separated into four individual analyses, corresponding to each of the four main social conditions contrasted with the nonsocial control. Decoding accuracy was significantly greater than chance in almost all analyses across the six ROIs. The right STS showed the least consistent evidence of decoding. The TPJ bilaterally and the precuneus showed the most consistent evidence of decoding. These results show strong evidence of decoding of the social versus nonsocial stimuli in the known theory-of-mind, cortical network.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Decoding social versus nonsocial stories.</title><p>The ability of a classifier, trained on BOLD activity patterns within each of the six ROIs, to decode each of the four social story conditions (endogenous-self, exogenous-self, endogenous-other, and exogenous-other) versus the nonsocial control. Each bar shows mean decoding accuracy, error bars show SEM, red horizontal line shows chance level decoding. Significance indicated by * (p&lt;0.05), ** (p&lt;0.01), and *** (p&lt;0.001) based on permutation testing (all but one of the significant p values also survived FDR correction for multiple comparisons across all six ROIs; see <xref ref-type="table" rid="table3">Table 3</xref> for numerical details).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Decoding social versus nonsocial stories at the whole-brain level.</title><p>The clusters shown are areas of overlap between four whole-brain, searchlight analyses, for the social-versus-nonsocial comparison (endogenous-self versus nonsocial, exogenous-self versus nonsocial, endogenous-other versus nonsocial, exogenous-other versus nonsocial). See <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref> for numerical details. Projected onto a 3D canonical brain surface.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Decoding social versus nonsocial stories within the six ROIs.</title><p>Violin plots showing mean (black horizontal line) and median (white circle) decoding accuracy (%), 95% confidence interval around the mean based on bootstrap distribution (black vertical line), a kernel density estimation (shaded area), and individual data points for each of the six ROIs. The horizontal red line indicates chance decoding.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-fig4-figsupp2-v1.tif"/></fig></fig-group><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Decoding social versus nonsocial stories within the six ROIs.</title><p>For definition of ROIs, see <xref ref-type="fig" rid="fig2">Figure 2</xref>. Mean decoding accuracy (%), 95% confidence interval (based on bootstrap distribution), and p value (based on permutation testing) are shown for each of the six ROIs. Results shown for each of four social story conditions (endogenous-self, exogenous-self, endogenous-other, and exogenous-other) versus the nonsocial control. * indicates significant p values that survived correction for multiple comparisons across all six ROIs (FDR-corrected p&lt;0.05).</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top"/><th>L TPJ</th><th>R TPJ</th><th>L STS</th><th>R STS</th><th>MPFC</th><th>Precuneus</th></tr></thead><tbody><tr><td rowspan="3">Endo-Self vs. nonsocial</td><td>Mean accuracy</td><td>58.5%</td><td>56.7%</td><td>53.0%</td><td>50.9%</td><td>55.2%</td><td>54.6%</td></tr><tr><td> 95% CI</td><td>55.5–61.6</td><td>53.5–59.8</td><td>49.3–56.8</td><td>47.5–54.0</td><td>52.3–58.0</td><td>51.5–57.7</td></tr><tr><td> p value</td><td>0.0001*</td><td>0.0001*</td><td>0.0338*</td><td>0.2703</td><td>0.0026*</td><td>0.0027*</td></tr><tr><td rowspan="3">Exo-Self vs. nonsocial</td><td>Mean accuracy</td><td>60.7%</td><td>56.5%</td><td>56.3%</td><td>53.6%</td><td>54.5%</td><td>55.7%</td></tr><tr><td> 95% CI</td><td>57.0–64.4</td><td>53.7–58.8</td><td>53.5–59.0</td><td>50.5–56.6</td><td>51.7–57.1</td><td>51.9–59.4</td></tr><tr><td> p value</td><td>0.0001*</td><td>0.0002*</td><td>0.0001*</td><td>0.0179*</td><td>0.0044*</td><td>0.0001*</td></tr><tr><td rowspan="3">Endo-Other vs. nonsocial</td><td>Mean accuracy</td><td>58.4%</td><td>52.7%</td><td>53.5%</td><td>52.2%</td><td>53.4%</td><td>55.4%</td></tr><tr><td> 95% CI</td><td>55.9–60.9</td><td>49.6–55.9</td><td>51.1–55.9</td><td>48.8–55.0</td><td>49.5–57.7</td><td>52.6–58.2</td></tr><tr><td> p value</td><td>0.0001*</td><td>0.0497</td><td>0.0147*</td><td>0.0969</td><td>0.0219*</td><td>0.0014*</td></tr><tr><td rowspan="3">Exo-Other vs. nonsocial</td><td>Mean accuracy</td><td>56.7%</td><td>54.2%</td><td>53.1%</td><td>51.5%</td><td>53.4%</td><td>55.2%</td></tr><tr><td> 95% CI</td><td>53.8–59.8</td><td>51.1–57.3</td><td>50.4–55.9</td><td>49.1–54.0</td><td>49.8–56.7</td><td>52.0–58.5</td></tr><tr><td> p value</td><td>0.0002*</td><td>0.0065*</td><td>0.0319*</td><td>0.1901</td><td>0.0197*</td><td>0.0012*</td></tr></tbody></table></table-wrap></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study analyzed brain activity while people read stories about agents attending to objects in the environment. We examined whether specific brain areas could decode information about the type of attention referenced in the story (exogenous versus endogenous), and about the type of agent in the story (whether the agent was the subject reading the story or a different person). We hypothesized that if the brain constructs a model of attentional state that is used in social cognition, then areas of the brain known to be involved in social cognition should be able to distinguish between the two types of attention, exogenous and endogenous, represented in the stories. Our main analysis confirmed the hypothesis: the left TPJ showed significant decoding of information about endogenous versus exogenous attention. The finding is, arguably, remarkable, given that the semantic and wording difference between the two story types is extremely subtle.</p><p>These results support a new and growing body of evidence that the human brain constructs a model of attention to aid in theory of mind (<xref ref-type="bibr" rid="bib25">Guterstam et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Guterstam and Graziano, 2020b</xref>; <xref ref-type="bibr" rid="bib36">Kelly et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Pesquita et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Vernet et al., 2019</xref>). The model includes information about attention that is deeper and more complex than just gaze direction or an identification of the attended object. At least one aspect of attention incorporated into the model appears to be the manner in which attention moves to an object: endogenously (internally directed) or exogenously (externally induced). The processing of the model appears to engage the theory-of-mind cortical network. The left TPJ showed the strongest decoding result.</p><p>Why should the TPJ in particular have shown involvement in decoding someone else’s attention state, rather than areas in the STS that are known for responding to the gaze direction of others (<xref ref-type="bibr" rid="bib44">Marquardt et al., 2017</xref>)? As noted in the Introduction, we suggest that modeling someone else’s attention, and processing someone else’s gaze direction, are not the same. Gaze is only one of many cues that can be integrated to constrain a model of someone else’s attention. In <xref ref-type="bibr" rid="bib36">Kelly et al., 2014</xref>, in a task in which participants judged the attention state of a cartoon, the TPJ was not active in association with gaze direction, and also not active in association with facial expression; but both left and right TPJ were active in association with the integration of the two cues, gaze and expression, in order to judge the cartoon’s attentional state. The current finding of significant decoding in the left TPJ is consistent with that prior finding.</p><p>It is not clear why the left hemisphere showed stronger activity than the right in the present task. Social cognition tasks often activate the TPJ bilaterally, but typically engage the right TPJ more (<xref ref-type="bibr" rid="bib60">Saxe and Wexler, 2005</xref>). One speculation is that some aspect of the present task, perhaps explicitly instructing people that the task was a test of reading comprehension, caused an emphasis on linguistic processing, biasing the activity toward the left hemisphere. Other explanations for the left-hemisphere bias may also be possible.</p><p>One alternative interpretation of the present finding in the TPJ is that people simulated attention orienting when reading the stories, and activated the corresponding dorsal (endogenous) and ventral (exogenous) attention networks in a ‘mirror-neuron-like’ fashion. Under this hypothesis, the TPJ, as part of the ventral attention network, was activated when reading the exogenous stories, and the dorsal attention network (consisting of the frontal eye fields, intraparietal sulcus, and middle temporal complex) should be active in association with reading the endogenous stories (<xref ref-type="bibr" rid="bib8">Corbetta et al., 2008</xref>; <xref ref-type="bibr" rid="bib9">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib14">Fox et al., 2006</xref>). However, a control analysis showed that none of these dorsal attention network regions significantly decoded information concerning the attention type in the stories (<xref ref-type="fig" rid="fig3s6">Figure 3—figure supplement 6</xref>), which is incompatible with the attention-orienting simulation account of the TPJ result. Our findings are thus more consistent with the notion that the TPJ is involved in constructing a rich, perhaps implicit model of attention that may assist in social cognition (<xref ref-type="bibr" rid="bib22">Graziano and Kastner, 2011</xref>; <xref ref-type="bibr" rid="bib26">Guterstam et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Guterstam and Graziano, 2020a</xref>; <xref ref-type="bibr" rid="bib36">Kelly et al., 2014</xref>). We are not suggesting, however, that the possible involvement of the TPJ in modeling one’s own and others’ attention, and the involvement of the TPJ in the control of attention, is a coincidence. We suggested in previous work (<xref ref-type="bibr" rid="bib23">Graziano and Webb, 2015</xref>) that a rich model of attention may be of benefit in the control of attention.</p><p>A second, alternative interpretation of the present results is that the exogenous sentences might make the reader focus more on the object in the story (thus engaging less mentalizing), whereas the endogenous sentences might make the reader focus more on the character’s mental act of attending (thus engaging more mentalizing). In that interpretation, the TPJ shows a significant decoding result because it becomes more active in the endogenous story type, due to more mentalizing cognition. Although this possible explanation of the TPJ decoding results is difficult to exclude, we believe it is unlikely. First, if endogenous stories simply engaged more mentalizing than exogenous stories, and thus caused more activity in the TPJ, our univariate analysis should have found more activity in the TPJ to endogenous stories. It did not (see <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). We found no evidence that a simple difference in the amount of mentalizing between exogenous and endogenous stories affected the overall amount of activity in the TPJ or anywhere else in the brain. Although the pattern of activity in the left TPJ clearly contains information about the exogenous-endogenous distinction, that information is not in the form of a simple increase in activity during the endogenous stories. Second, it is not clear that subjects should mentalize more in the endogenous story type than in the exogenous story type. When reading that ‘Alice looks for the red ball,’ subjects might wonder why, and think about Alice’s mental state. When reading that ‘Alice notices the red ball,’ subjects might again wonder why the ball was of enough significance to be capturing her notice, or what mental reaction she experiences when noticing that apparently unanticipated object. Both story types invite mentalizing, alhough one focuses on an endogenous process of attention and the other on exogenous attention. Third, the probe task was designed to limit readers from speculating too much about the deeper meaning of the stories by asking only about literal details and never about the internal states of the characters. Thus, the subjects had an incentive to focus on the same physical aspects of the story in both the exogenous and endogenous conditions. Fourth, and finally, prior studies suggest that if an experimental and a control story both include human agents with potential thoughts, then subjects automatically mentalize in both story types, and the contrast between the story types will tend not to show a differential degree of activity in theory-of-mind cortical areas, especially in the TPJ (<xref ref-type="bibr" rid="bib59">Saxe and Kanwisher, 2003</xref>). It is likely that, in our case, subjects cannot help mentalizing about the characters in both story types. We acknowledge that any story stimuli are always so complex that a variety of unintended, subtle differences may affect the results, but for the reasons listed here, we argue that a difference in the overall amount of mentalizing during endogenous versus exogenous stories is unlikely to explain the present results in the TPJ. The results point to the left TPJ processing different information in the exogenous and endogenous story types, but not a difference in overall amount of activity.</p><p>In addition to the endogenous-versus-exogenous comparisons, we also analyzed brain areas involved in self-versus-other encoding. We found evidence of self-versus-other encoding in the left TPJ, left STS, MPFC, and precuneus. The MPFC and precuneus have been previously implicated in self processing (<xref ref-type="bibr" rid="bib49">Northoff et al., 2006</xref>; <xref ref-type="bibr" rid="bib50">Ochsner et al., 2004</xref>; <xref ref-type="bibr" rid="bib51">Passingham et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Qin and Northoff, 2011</xref>; <xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>), and the TPJ is consistently activated in fMRI studies involving self-recognition (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>) and first-person perspective taking (<xref ref-type="bibr" rid="bib34">Ionta et al., 2011</xref>). These results lend confidence to the present paradigm, showing that even the very subtle differences between our story stimuli were able to reveal cortical results consistent with previous studies.</p><p>Contrary to our prediction 3, we found no evidence for an interaction between attention type and agent type decoding in the theory-of-mind ROIs. (As noted in the <italic>Supplementary Information</italic>, during an exploratory searchlight analysis, we also found no evidence of an interaction effect in any other brain area.) Although it is possible that our paradigm was simply not sensitive enough to detect subtle interaction effects, these results suggest that the brain encodes information about attention type in a similar manner in the self and in others. In light of previous results showing that the attribution of sensory awareness to others and to oneself have a shared representation in the TPJ (<xref ref-type="bibr" rid="bib36">Kelly et al., 2014</xref>), we directly tested this notion in a <italic>post hoc</italic> cross-classification analysis focusing on the left TPJ, which was the only region showing significant decoding of attention type. We found that endogenous-versus-exogenous decoding significantly generalized across self-stories and other-stories (<xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7</xref>), suggesting that there is an overlap in brain mechanisms that participate in the encoding of attention in others and in encoding of attention in the self.</p><p>Finally, significant decoding of the social-versus-nonsocial distinction was obtained across most of the theory-of-mind ROIs. This finding confirmed the validity of the paradigm, and was expected based on previous experiments of the theory-of-mind network (<xref ref-type="bibr" rid="bib18">Gallagher et al., 2000</xref>; <xref ref-type="bibr" rid="bib59">Saxe and Kanwisher, 2003</xref>; <xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>; <xref ref-type="bibr" rid="bib67">Vogeley et al., 2001</xref>).</p><p>The use of a story-reading paradigm allowed us to systematically manipulate the kind of attention represented in the stimulus while keeping other experimental factors close to identical. The endogenous and exogenous story versions differed only with respect to a few key words specifying the type of attention, while the rest of the stories were semantically the same. The absence of any univariate effect within the ROIs or anywhere else in the brain, even at a liberal uncorrected threshold, confirm that the stimuli were well matched (<xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). To avoid cognitive bias or expectation effects, the probe task performed by the subjects concerned details about the spatial context or the objects in the stories, effectively distracting subjects from the description of attention. We also speculate that this design of the probe task minimized the theoretical risk of the reader focusing on slightly different aspects of the story in the endogenous and exogenous conditions (e.g. focusing more on the mental act of attending in endogenous stories, and more on the attention-grabbing object in the exogenous stories), because the task ‘forced’ the reader to focus on processing the spatial context and the object descriptions equally in both conditions. A post-scan questionnaire confirmed that none of the subjects came close to figuring out the purpose of the experiment (which they had been told was a ‘Reading Comprehension Experiment’). The finding of brain areas that significantly decoded the type of attention, despite the distinction between endogenous and exogenous attention being subtle and task-irrelevant, suggests that the human brain automatically, and possibly also implicitly (<xref ref-type="bibr" rid="bib53">Pesquita et al., 2016</xref>), constructs a model of an agents’ attention that specifies at least some dynamic aspects of how that attention is moving around the scene.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Thirty-two healthy human volunteers (12 females, 30 right-handed, aged 18–52, normal or corrected to normal vision) participated in the study. Subjects were recruited either from a paid subject pool, receiving 40 USD for participation, or from among Princeton undergraduate students, who received course credits as compensation. In the subject recruitment material, the experiment was described as a ‘Reading Comprehension Study.’ All subjects provided informed consent and all procedures were approved by the Princeton Institutional Review Board.</p></sec><sec id="s4-2"><title>Experimental setup</title><p>Before scanning, subjects were instructed and then shown three sample trials (which were not part of the stories presented in the subsequent experiment) on a laptop computer screen. All subjects gave the correct response to all three trials on the first try, indicating they had understood the instructions adequately. During scanning, the subjects laid comfortably in a supine position on the MRI bed. Through an angled mirror mounted on top of the head coil, they viewed a translucent screen approximately 80 cm from the eyes, on which visual stimuli were projected with a Hyperion MRI Digital Projection System (Psychology Software Tools, Sharpsburg, PA, USA) with a resolution of 1920 × 1080 pixels. A PC running MATLAB (MathWorks, Natick, MA, USA) and the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib5">Brainard, 1997</xref>) was used to present visual stimuli. A right hand 5-button response unit (Psychology Software Tools Celeritas, Sharpsburg, PA, USA) was strapped to the subjects’ right wrist. Subjects used the right index finger button to indicate a true response, and the right middle finger to indicate a false response during the probe phase of each trial.</p></sec><sec id="s4-3"><title>Experimental conditions and stimuli</title><p>Five experimental conditions were included. Subjects were presented with short stories (2–3 sentences, average word count = 24) describing a scene in which an agent, which was either the subject him-/herself (self) or another person (other), directed attention to something in the external world endogenously (e.g., ‘X is attentively looking for Y’) or exogenously (e.g., ‘X’s attention is captured by Y’). These four conditions made up a 2 × 2 factorial design: attention type (endogenous versus exogenous) X agent (self versus other). In addition, we included a control condition featuring stories in which the agent was substituted by a non-human object. In each trial, after a 9–11 s inter-trial interval, the story was presented for 10 s in easily readable, white text on a black background, at the center of the screen, after which a probe statement was shown for 4 s, to which the subjects responded either true or false by button press. See <xref ref-type="fig" rid="fig1">Figure 1</xref> for details, and <xref ref-type="supplementary-material" rid="supp6">Supplementary file 6</xref> for all stories.</p><p>Each subject ran 100 trials and thus saw 100 stories: 80 social stories and 20 non-social control stories. The 80 social stories were constructed as follows. We began with 80 unique short stories. For each story, four versions were constructed, one for each of the factorial conditions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To keep the story versions as semantically similar as possible, we made minimal changes to the wordings. To distinguish the self and other versions, we substituted the word ‘you’ with a name (e.g. ‘Karen’) and the word ‘your’ with ‘his’ or ‘her’. The names in the stories were selected from a list of the 100 most popular given names for male and female babies born during the years 1919–2018 in the United States, which is published by the Social Security Administration (<ext-link ext-link-type="uri" xlink:href="https://www.ssa.gov/oact/babynames/decades/century.html">https://www.ssa.gov/oact/babynames/decades/century.html</ext-link>). Half of the names were masculine, half feminine. To distinguish the endogenous and exogenous story versions, we used different wording for the part of the story where the agent (X) is related to the object (Y). In the endogenous versions, we used formulations such as: ‘X is trying to find Y,’ ‘X is trying to spot Y,’ or ‘X is looking attentively for Y.’ In the exogenous versions, we used formulations such as: ‘X’s eyes are drawn to Y,’ ‘X’s gaze is captured by Y,’ or ‘X’s attention is captured by Y.’ We matched the average number of words across all four conditions (24 words). The number of stories that included the words ‘attention’ or ‘attentively’ was balanced between the endogenous and exogenous categories (43 stories in each). Among the 80 stories, for each subject, 20 were randomly selected to be used in the endogenous-self version; 20 in the endogenous-other version; 20 in the exogenous-self version; 20 in the exogenous-other version. Thus, for the example story shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, each subject saw only one of the four versions. In this manner, each subject saw 80 social stories, 20 of each type, balanced for as many properties as possible other than the two factors that were manipulated.</p><p>Finally, we constructed 20 additional stories for the non-social control condition (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To keep the control stories as semantically similar as possible to the social stories, we based them on a subset of the 80 original stories. Crucially, the agent in the original story was substituted with a non-human object, such as a camera or a spotlight, that has a source and a target just as attention does. For instance, the original story, &quot;You are in a bike shop, and numerous bikes hang on one of the walls. You are attentively looking for that red Italian sports bike&quot;, was adapted to the non-social condition by substituting the agent with a spotlight: &quot;In a bike shop, on one of the walls, hangs numerous bikes. A bright spotlight is shining on a red Italian sports bike&quot;. The average number of words of the non-social stories (24 words, standard deviation = 3) was matched with the attention stories.</p><p>The purpose of the probe statement at the end of each trial was to ensure that subjects carefully read the stories. Each statement described one detail of the preceding story that could be either true or false. We restricted the probe statements to the spatial context of the story (place probe: e.g. ‘Emma is on a bus’) or the object being described (object probe: e.g. ‘The Van Gogh painting has sunflowers’) in order to avoid alerting subjects to the focus of the experiment on theory of mind and attention. Half of the probe statements were place probes and half object probes. Within both the place and the object probes, half were true and half were false. The probe was on screen for 4 s, during which subjects were required to indicate whether the statement was true or false by button press.</p><p>The experiment consisted of 10 runs of approximately 4 min each. In each run, the five conditions were repeated two times, yielding a total of 10 trials per run. The trial order was randomized, with the limitation that two consecutive trials could not belong to the same condition. Each run included 18 s of baseline before the onset of the first trial and 12 s of baseline after the offset of the last trial.</p></sec><sec id="s4-4"><title>Post-scan questionnaire</title><p>At the end of the scanning session, subjects were asked what they thought the purpose of the experiment was and what they thought it was testing.</p></sec><sec id="s4-5"><title>fMRI data acquisition</title><p>Functional imaging data were collected using a Siemens Prisma 3T scanner equipped with a 64-channel head coil. Gradient-echo T2*-weighted echo-planar images (EPI) with blood-oxygen dependent (BOLD) contrast were used as an index of brain activity (<xref ref-type="bibr" rid="bib42">Logothetis et al., 2001</xref>). Functional image volumes were composed of 54 near-axial slices with a thickness of 2.5 mm (with no interslice gap), which ensured that the entire brain excluding cerebellum was within the field-of-view in all subjects (54 × 78 matrix, 2.5 mm x 2.5 mm in-plane resolution, TE = 30 ms, flip angle = 80<bold>°</bold>). Simultaneous multi-slice (SMS) imaging was used (SMS factor = 2). One complete volume was collected every 2 s (TR = 2000 ms). A total of 1300 functional volumes were collected for each participant, divided into 10 runs (130 volumes per run). The first three volumes of each run were discarded to account for non-steady-state magnetization. A high-resolution structural image was acquired for each participant at the end of the experiment (3D MPRAGE sequence, voxel size = 1 mm isotropic, FOV = 256 mm, 176 slices, TR = 2300 ms, TE = 2.96 ms, TI = 1000 ms, flip angle = 9°, iPAT GRAPPA = 2). At the end of each scanning session, matching spin echo EPI pairs (anterior-to-posterior and posterior-to-anterior) were acquired for blip-up/blip-down field map correction.</p></sec><sec id="s4-6"><title>FMRI preprocessing</title><p>Results included in this manuscript come from preprocessing performed using FMRIPREP version 1.2.3 (<xref ref-type="bibr" rid="bib11">Esteban et al., 2019</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016216">SCR_016216</ext-link>), a Nipype (<xref ref-type="bibr" rid="bib19">Gorgolewski et al., 2011</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002502">SCR_002502</ext-link>) based tool. Each T1w (T1-weighted) volume was corrected for INU (intensity non-uniformity) using N4BiasFieldCorrection v2.1.0 (<xref ref-type="bibr" rid="bib64">Tustison et al., 2010</xref>) and skull-stripped using antsBrainExtraction.sh v2.1.0 (using the OASIS template). Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="bib13">Fonov et al., 2009</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_008796">SCR_008796</ext-link>) was performed through nonlinear registration with the antsRegistration tool of ANTs v2.1.0 (<xref ref-type="bibr" rid="bib2">Avants et al., 2008</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_004757">SCR_004757</ext-link>), using brain-extracted versions of both T1w volume and template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="bib69">Zhang et al., 2001</xref>) (FSL v5.0.9, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002823">SCR_002823</ext-link>).</p><p>Functional data was slice time corrected using 3dTshift from AFNI v16.2.07 (<xref ref-type="bibr" rid="bib10">Cox, 1996</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_005927">SCR_005927</ext-link>) and motion corrected using mcflirt (FSL v5.0.9) (<xref ref-type="bibr" rid="bib35">Jenkinson et al., 2002</xref>). This was followed by co-registration to the corresponding T1w using boundary-based registration (<xref ref-type="bibr" rid="bib24">Greve and Fischl, 2009</xref>) with six degrees of freedom, using flirt (FSL). Motion correcting transformations, BOLD-to-T1w transformation and T1w-to-template Montreal Neurological Institute (MNI) warp were concatenated and applied in a single step using antsApplyTransforms (ANTs v2.1.0) using Lanczos interpolation.</p><p>Many internal operations of FMRIPREP use Nilearn (<xref ref-type="bibr" rid="bib1">Abraham et al., 2014</xref>) (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001362">SCR_001362</ext-link>) principally within the BOLD-processing workflow. For more details of the pipeline see <ext-link ext-link-type="uri" xlink:href="https://fmriprep.readthedocs.io/en/latest/workflows.html">https://fmriprep.readthedocs.io/en/latest/workflows.html</ext-link>.</p><sec id="s4-6-1"><title>Testing prediction 1</title><p>The purpose of the first analysis was to determine whether the brain encoded information concerning the type of attention (endogenous or exogenous) present in the stories. For this analysis, we used MVPA, which tests whether patterns of brain activity can be used to decode the distinction between two conditions. It is a more sensitive analysis than the more common, simple subtraction methods. The reason for using this sensitive measure is that the difference between exogenous and endogenous trial types was extremely subtle. Both trial types engaged social cognition, and therefore might cancel each other out in a simple subtraction. The stimuli were nearly identical, differing only in a few words that indicated the type of attention used by the agent in the story. In addition, the type of attention featured in the story was irrelevant to the task performed by the subject. To accommodate the subtlety of the distinction between conditions, we designed the study to use MVPA. We hypothesized that with MVPA, brain activity would carry information about the endogenous versus exogenous distinction; and that decoding would be evident in regions of interest (ROIs) within the network of areas typically found to be involved in social cognition.</p><p>We defined our ROIs as spheres centered on the statistical peaks reported in an activation likelihood estimation (ALE) meta-analysis of 16 fMRI studies (including 291 subjects) involving theory-of-mind reasoning (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>), in accordance with generally accepted guidelines in ROI analysis (<xref ref-type="bibr" rid="bib54">Poldrack, 2007</xref>). The ROIs are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The peaks were located in six areas: the left TPJ (Montreal Neurological Institute [MNI]: −52,–56, 24), right TPJ (MNI: 55,–53, 24), left STS (MNI: −59,–26, −9), right STS (MNI: 59,–18, −17), MPFC (MNI: 1, 58, 19), and the precuneus (MNI: −3,–56, 37). The radius of the ROI spheres was 10 mm, corresponding to the approximate volume (4000 mm<sup>3</sup>) of the largest clusters (TPJ and MPFC) reported in <xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>. The same sphere radius was used for all ROIs.</p><p>The fMRI data from all participants were analyzed with the Statistical Parametric Mapping software (SPM12) (Wellcome Department of Cognitive Neurology, London, UK) (<xref ref-type="bibr" rid="bib17">Friston et al., 1994</xref>). We first used a conventional general linear model (GLM) to estimate regression (beta) coefficients for each individual trial (i.e. 100 regressors), focusing on the 10 s story presentation phase of each trial. One regressor of no interest modeled the 4 s probe statement phase across conditions. Each regressor was modeled with a boxcar function and convolved with the standard SPM12 hemodynamic response function. In addition, ten run-specific regressors controlling for baseline differences between runs, and six motion regressors, were included. The trialwise beta coefficients for the endogenous and exogenous conditions (i.e. 80 beta maps) were then submitted to subsequent multivariate analyses (<xref ref-type="bibr" rid="bib31">Haxby et al., 2001</xref>).</p><p>The MVPA was carried out using The Decoding Toolbox (TDT) version 3.999 (<xref ref-type="bibr" rid="bib32">Hebart et al., 2014</xref>) for SPM. For each subject and ROI, we used linear support vector machines (SVMs, with the fixed regularization parameter of C = 1) to compute decoding accuracies. To ensure independent training and testing data sets, we used leave-one-run-out cross-validation approach. For each fold, the betas across all training runs were normalized relative the mean and standard deviation, and the same Z-transformation was applied to the betas in the left-out test run (<xref ref-type="bibr" rid="bib46">Misaki et al., 2010</xref>). An SVM was then trained to discriminate activity patterns belonging to the endogenous or exogenous trials in nine runs, and then tested on the left-out run, repeated for all runs, resulting in a run-average decoding accuracy for each ROI and subject.</p><p>For statistical inference, the true group mean decoding accuracy was compared to a null distribution of group mean accuracies obtained from permutation testing. The same MVPA was repeated within each subject and ROI using permuted condition labels (10,000 iterations). A p value was computed as (1+the number of permuted group accuracy values &gt; true value)/(1+the total number of permutations). To control for multiple comparisons across the six ROIs, we used the false discovery rate (FDR) correction (<xref ref-type="bibr" rid="bib4">Benjamini and Hochberg, 1995</xref>). In addition, we also computed a bootstrap distribution around the true group mean accuracy by resampling individual-subject mean accuracies with replacement (10,000 iterations), from which a 95% confidence interval (CI) was derived (<xref ref-type="bibr" rid="bib48">Nakagawa and Cuthill, 2007</xref>). A corrected p value &lt; 0.05 in combination with a 95% CI that does not cross chance level were interpreted as a significant decoding effect at the group level (<xref ref-type="bibr" rid="bib48">Nakagawa and Cuthill, 2007</xref>).</p><p>In addition, as further exploratory statistics beyond the targeted hypotheses of this study, we used a whole-brain searchlight analysis (<xref ref-type="bibr" rid="bib41">Kriegeskorte et al., 2006</xref>) to test for possible areas of decoding outside the ROIs. This searchlight analysis is described in the <italic>Supplementary Information</italic> (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>–<xref ref-type="supplementary-material" rid="supp4">4</xref> and <xref ref-type="supplementary-material" rid="supp7">7</xref>, <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplements 3</xref>–<xref ref-type="fig" rid="fig3s5">5</xref>, and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p></sec><sec id="s4-6-2"><title>Testing prediction 2</title><p>The purpose of the second analysis was to determine whether the brain encoded information concerning the type of agent (self versus other) present in the stories. The analysis methods were the same as for testing hypothesis 1, except that for regressors of interest we used the self-related and other-related trials, collapsed across the type of attention (exogenous or endogenous). Just as for hypothesis 1, we tested the six defined ROIs within the theory-of-mind network.</p></sec><sec id="s4-6-3"><title>Testing prediction 3</title><p>The purpose of the third analysis was to test for an interaction between the two variables (endogenous versus exogenous, and self versus other). We used MVPA to test whether the decoding for the type of attention was significantly different between the self-related and the other-related stories. The analysis methods were similar to those used for testing hypotheses 1 and 2, except in the following ways. We computed two MVPA decoding results, the first for distinguishing endogenous-self from exogenous-self stories, the second for distinguishing endogenous-other from exogenous-other stories. We then computed the difference between the two decoding results ([endogenous-self versus exogenous-self] – [endogenous-other versus exogenous-other]) to create a decoding difference score. Just as for hypotheses 1 and 2, we tested the six defined ROIs within the theory-of-mind network (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>).</p><p>In a post hoc analysis, we tested for overlap in attention type encoding in self and others in the left TPJ ROI by using a two-way cross-classification analysis. One classifier was trained to discriminate endogenous versus exogenous self-stories and tested on other-stories (using a leave-one-run-out approach), and another classifier was trained to discriminate endogenous versus exogenous other-stories and tested on self-stories, from which an average cross-classification decoding accuracy was obtained. Overlap in attention type representations in self and other should be reflected in above-chance decoding.</p></sec><sec id="s4-6-4"><title>Testing prediction 4</title><p>The purpose of the fourth analysis was to confirm whether our story stimuli engaged social cognition and thereby recruited brain areas within the expected theory of mind network. The analysis was meant as an added control to check the validity of the paradigm. The analysis methods were similar to those used for testing hypotheses 1–3, except in the following ways. We computed four MVPA decoding results: endogenous-self versus nonsocial, endogenous-other versus nonsocial, exogenous-self versus nonsocial, and exogenous-other versus nonsocial. (Because using MVPA to compare two conditions requires equal numbers of trials in both conditions, it was not possible to use a single analysis to compare all 80 social trials to the 20 nonsocial trials.) Each analysis represents a separate, alternative way to assess the social-versus-nonsocial decoding. Just as for hypotheses 1–3, we tested the six defined ROIs within the theory-of-mind network (<xref ref-type="bibr" rid="bib65">van Veluw and Chance, 2014</xref>).</p></sec></sec><sec id="s4-7"><title>Eye tracking analysis</title><p>Eye movements were recorded via an MRI-compatible infrared eye tracker (SR Research EyeLink 1000 Plus), mounted just below the projector screen, sampling at 1000 Hz. Before each scanning session, a calibration routine on five screen locations was used and repeated until the maximum error for any point was less than 1°. The obtained eye position data was cleaned of artifacts related to blink events and smoothed using a 20 ms moving average. We then built an SVM decoding model analogue to the cross-validation approach used for the fMRI data, but here based purely on eye tracking data, to test whether eye movement dynamics alone were sufficient to decode the conditions of interest (endogenous versus exogenous, and self versus other). In keeping with a previous study (<xref ref-type="bibr" rid="bib61">Schneider et al., 2013</xref>), we organized the data in the following way. The part of the display within which the stimuli appeared was divided into an 8 × 4 grid of 32 equally sized squares. The grid covered the screen area within which the stories were presented (see red outline in <xref ref-type="fig" rid="fig3s8">Figure 3—figure supplement 8</xref>), and approximately corresponded to the locations of individual words (four lines, with eight words per line). For each trial, the proportion of time that the subject fixated within each square (32 features) and the saccades between those regions (32 × 32 = 1024 features) was calculated. These 1056 features, representing information about both where people were looking as well as saccade dynamics, were then averaged across repetitions for each of the four main conditions within each of the 10 runs, yielding one eye movement feature vector per condition per run (per subject). The feature vectors were submitted to an SVM classifier (C = 1). Using a leave-one-run-out approach, the SVM model was trained on endogenous versus exogenous story types, and then tested in the left-out run. At the group level, the decoding accuracies were tested against chance level using t-tests. A similar analysis was then performed on the contrast between self-related stories versus other-related stories. The results showed that endogenous-versus-exogenous and self-versus-other story types could not be decoded significantly better than chance using the pattern of eye movement. See <italic>Supplementary Information</italic> (<xref ref-type="supplementary-material" rid="supp7">Supplementary file 7</xref> and <xref ref-type="fig" rid="fig3s8">Figure 3—figure supplement 8</xref>) for the results of the eye-tracking analysis.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by the Princeton Neuroscience Institute Innovation Fund. Arvid Guterstam was supported by the Wenner-Gren Foundation, the Sweden-America Foundation, the Swedish Brain Foundation, and the Promobilia Foundation. The authors would like to thank Sam Nastase for valuable input regarding the multivoxel pattern analysis.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All subjects provided informed consent and all procedures were approved by the Princeton Institutional Review Board (IRB# 10740).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Decoding endogenous versus exogenous at the whole-brain level.</title><p>All clusters (≥10 voxels) of decoding activity passing the voxelwise threshold of p&lt;0.001 (none of the clusters survived p&lt;0.05 correction for multiple comparisons using the whole brain as search space). *The left posterior STS cluster also coincided with the posterior TPJ (TPJp) subregion as defined by <xref ref-type="bibr" rid="bib6">Bzdok et al., 2013</xref>; <xref ref-type="bibr" rid="bib45">Mars et al., 2012</xref>.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-supp1-v1.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Decoding agent type at the whole-brain level.</title><p>All clusters (≥10 voxels) decoding agent type (self versus other) activity at the threshold of p&lt;0.001 (uncorrected). Corrected p values represent cluster-level correction using the whole brain as search space.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-supp2-v1.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Decoding attention-by-agent interaction at the whole-brain level.</title><p>All clusters (≥10 voxels) in which endogenous-versus-exogenous decoding was better in self-related compared to other-related stories at p&lt;0.001 (uncorrected). (none of the clusters survived correction for multiple comparisons using the whole brain as search space).</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-supp3-v1.docx"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Decoding social versus nonsocial stories at the whole-brain level.</title><p>Clusters (≥10 voxels) decoding social versus nonsocial stories significantly better than chance. The listed clusters represent the overlap of significant clusters (p&lt;0.05, corrected using a cluster-defining uncorrected threshold of p&lt;0.001 and the entire brain as search space) across four separate whole-brain searchlight analyses: endogenous-self versus nonsocial, exogenous-self versus nonsocial, endogenous-other versus nonsocial, and exogenous-other versus nonsocial.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-supp4-v1.docx"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Univariate fMRI results.</title><p>All clusters (k ≥ 10) of voxels surviving a p&lt;0.001 (uncorrected) threshold are shown for the univariate contrasts corresponding to the main multivariate analyses (<xref ref-type="fig" rid="fig3">Figures 3</xref>–<xref ref-type="fig" rid="fig4">4</xref>). The preprocessed functional images were smoothed using a 6 mm full FWHM Gaussian kernel and subjected to conventional GLMs. We report the p values for clusters that survived a cluster-level familywise error rate correction for multiple comparisons, either using the whole brain or one of the ROI volumes as search space.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-supp5-v1.docx"/></supplementary-material><supplementary-material id="supp6"><label>Supplementary file 6.</label><caption><title>Story stimuli.</title><p>List of all of the stories and story versions (endogenous-self, exogenous-self, endogenous-other, and exogenous-other) presented to the participants during the experiment. Story #1–80 are the social stories, and #81–100 are the nonsocial control stories.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-63551-supp6-v1.xlsx"/></supplementary-material><supplementary-material id="supp7"><label>Supplementary file 7.</label><caption><title>Searchlight analysis and eye-tracking decoding methods text.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-supp7-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-63551-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data that support the findings of this study are available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/c3463d15bc78106a1b5c">https://figshare.com/s/c3463d15bc78106a1b5c</ext-link> (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.12273128">https://doi.org/10.6084/m9.figshare.12273128</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Guterstam</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Temporo-Parietal Cortex Involved in Modeling One's Own and Others' Attention</data-title><source>figshare</source><pub-id assigning-authority="figshare" pub-id-type="doi">10.6084/m9.figshare.12273128.v1</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname> <given-names>A</given-names></name><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Eickenberg</surname> <given-names>M</given-names></name><name><surname>Gervais</surname> <given-names>P</given-names></name><name><surname>Mueller</surname> <given-names>A</given-names></name><name><surname>Kossaifi</surname> <given-names>J</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname> <given-names>BB</given-names></name><name><surname>Epstein</surname> <given-names>CL</given-names></name><name><surname>Grossman</surname> <given-names>M</given-names></name><name><surname>Gee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baron-Cohen</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Mindblindness: An Essay on Autism and Theory of Mind</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.1016/S0166-2236(96)60013-7</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname> <given-names>Y</given-names></name><name><surname>Hochberg</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bzdok</surname> <given-names>D</given-names></name><name><surname>Langner</surname> <given-names>R</given-names></name><name><surname>Schilbach</surname> <given-names>L</given-names></name><name><surname>Jakobs</surname> <given-names>O</given-names></name><name><surname>Roski</surname> <given-names>C</given-names></name><name><surname>Caspers</surname> <given-names>S</given-names></name><name><surname>Laird</surname> <given-names>AR</given-names></name><name><surname>Fox</surname> <given-names>PT</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name><name><surname>Eickhoff</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Characterization of the temporo-parietal junction by combining data-driven parcellation, complementary connectivity analyses, and functional decoding</article-title><source>NeuroImage</source><volume>81</volume><fpage>381</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.046</pub-id><pub-id pub-id-type="pmid">23689016</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname> <given-names>AJ</given-names></name><name><surname>Lawrence</surname> <given-names>AD</given-names></name><name><surname>Keane</surname> <given-names>J</given-names></name><name><surname>Scott</surname> <given-names>SK</given-names></name><name><surname>Owen</surname> <given-names>AM</given-names></name><name><surname>Christoffels</surname> <given-names>I</given-names></name><name><surname>Young</surname> <given-names>AW</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Reading the mind from eye gaze</article-title><source>Neuropsychologia</source><volume>40</volume><fpage>1129</fpage><lpage>1138</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(02)00008-8</pub-id><pub-id pub-id-type="pmid">11931917</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Patel</surname> <given-names>G</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The reorienting system of the human brain: from environment to theory of mind</article-title><source>Neuron</source><volume>58</volume><fpage>306</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.017</pub-id><pub-id pub-id-type="pmid">18466742</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id><pub-id pub-id-type="pmid">11994752</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname> <given-names>O</given-names></name><name><surname>Markiewicz</surname> <given-names>CJ</given-names></name><name><surname>Blair</surname> <given-names>RW</given-names></name><name><surname>Moodie</surname> <given-names>CA</given-names></name><name><surname>Isik</surname> <given-names>AI</given-names></name><name><surname>Erramuzpe</surname> <given-names>A</given-names></name><name><surname>Kent</surname> <given-names>JD</given-names></name><name><surname>Goncalves</surname> <given-names>M</given-names></name><name><surname>DuPre</surname> <given-names>E</given-names></name><name><surname>Snyder</surname> <given-names>M</given-names></name><name><surname>Oya</surname> <given-names>H</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name><name><surname>Wright</surname> <given-names>J</given-names></name><name><surname>Durnez</surname> <given-names>J</given-names></name><name><surname>Poldrack</surname> <given-names>RA</given-names></name><name><surname>Gorgolewski</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname> <given-names>PC</given-names></name><name><surname>Happé</surname> <given-names>F</given-names></name><name><surname>Frith</surname> <given-names>U</given-names></name><name><surname>Baker</surname> <given-names>SC</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Other minds in the brain: a functional imaging study of &quot;theory of mind&quot; in story comprehension</article-title><source>Cognition</source><volume>57</volume><fpage>109</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(95)00692-R</pub-id><pub-id pub-id-type="pmid">8556839</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname> <given-names>VS</given-names></name><name><surname>Evans</surname> <given-names>AC</given-names></name><name><surname>McKinstry</surname> <given-names>RC</given-names></name><name><surname>Almli</surname> <given-names>CR</given-names></name><name><surname>Collins</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>MD</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Vincent</surname> <given-names>JL</given-names></name><name><surname>Raichle</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spontaneous neuronal activity distinguishes human dorsal and ventral attention systems</article-title><source>PNAS</source><volume>103</volume><fpage>10046</fpage><lpage>10051</lpage><pub-id pub-id-type="doi">10.1073/pnas.0604187103</pub-id><pub-id pub-id-type="pmid">16788060</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friesen</surname> <given-names>CK</given-names></name><name><surname>Kingstone</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The eyes have it! reflexive orienting is triggered by nonpredictive gaze</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>5</volume><fpage>490</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.3758/BF03208827</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frischen</surname> <given-names>A</given-names></name><name><surname>Bayliss</surname> <given-names>AP</given-names></name><name><surname>Tipper</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Gaze cueing of attention</article-title><source>Psychological Bulletin</source><volume>133</volume><fpage>694</fpage><lpage>724</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.133.4.694</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Holmes</surname> <given-names>AP</given-names></name><name><surname>Worsley</surname> <given-names>KJ</given-names></name><name><surname>Poline</surname> <given-names>J-P</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Frackowiak</surname> <given-names>RSJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title><source>Human Brain Mapping</source><volume>2</volume><fpage>189</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallagher</surname> <given-names>HL</given-names></name><name><surname>Happé</surname> <given-names>F</given-names></name><name><surname>Brunswick</surname> <given-names>N</given-names></name><name><surname>Fletcher</surname> <given-names>PC</given-names></name><name><surname>Frith</surname> <given-names>U</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Reading the mind in cartoons and stories: an fMRI study of 'theory of mind' in verbal and nonverbal tasks</article-title><source>Neuropsychologia</source><volume>38</volume><fpage>11</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(99)00053-6</pub-id><pub-id pub-id-type="pmid">10617288</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname> <given-names>K</given-names></name><name><surname>Burns</surname> <given-names>CD</given-names></name><name><surname>Madison</surname> <given-names>C</given-names></name><name><surname>Clark</surname> <given-names>D</given-names></name><name><surname>Halchenko</surname> <given-names>YO</given-names></name><name><surname>Waskom</surname> <given-names>ML</given-names></name><name><surname>Ghosh</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in Python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Consciousness and the Social Brain</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Attributing awareness to others: the attention schema theory and its relationship to behavioural prediction</article-title><source>Journal of Consciousness Studies : Controversies in Science &amp; the Humanities</source><volume>26</volume><fpage>17</fpage><lpage>37</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MS</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Human consciousness and its relationship to social neuroscience: a novel hypothesis</article-title><source>Cognitive Neuroscience</source><volume>2</volume><fpage>98</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1080/17588928.2011.565121</pub-id><pub-id pub-id-type="pmid">22121395</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname> <given-names>MS</given-names></name><name><surname>Webb</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The attention schema theory: a mechanistic account of subjective awareness</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>500</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00500</pub-id><pub-id pub-id-type="pmid">25954242</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname> <given-names>DN</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guterstam</surname> <given-names>A</given-names></name><name><surname>Kean</surname> <given-names>HH</given-names></name><name><surname>Webb</surname> <given-names>TW</given-names></name><name><surname>Kean</surname> <given-names>FS</given-names></name><name><surname>Graziano</surname> <given-names>MSA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Implicit model of other people's visual attention as an invisible, force-carrying beam projecting from the eyes</article-title><source>PNAS</source><volume>116</volume><fpage>328</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1073/pnas.1816581115</pub-id><pub-id pub-id-type="pmid">30559179</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guterstam</surname> <given-names>A</given-names></name><name><surname>Wilterson</surname> <given-names>AI</given-names></name><name><surname>Wachtell</surname> <given-names>D</given-names></name><name><surname>Graziano</surname> <given-names>MSA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Other people's gaze encoded as implied motion in the human brain</article-title><source>PNAS</source><volume>117</volume><fpage>13162</fpage><lpage>13167</lpage><pub-id pub-id-type="doi">10.1073/pnas.2003110117</pub-id><pub-id pub-id-type="pmid">32457153</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guterstam</surname> <given-names>A</given-names></name><name><surname>Graziano</surname> <given-names>MSA</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Visual motion assists in social cognition</article-title><source>PNAS</source><volume>117</volume><fpage>32165</fpage><lpage>32168</lpage><pub-id pub-id-type="doi">10.1073/pnas.2021325117</pub-id><pub-id pub-id-type="pmid">33257566</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guterstam</surname> <given-names>A</given-names></name><name><surname>Graziano</surname> <given-names>MSA</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Implied motion as a possible mechanism for encoding other people's attention</article-title><source>Progress in Neurobiology</source><volume>190</volume><elocation-id>101797</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101797</pub-id><pub-id pub-id-type="pmid">32217129</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Happé</surname> <given-names>FG</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>An advanced test of theory of mind: understanding of story characters' thoughts and feelings by able autistic, mentally handicapped, and normal children and adults</article-title><source>Journal of Autism and Developmental Disorders</source><volume>24</volume><fpage>129</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1007/BF02172093</pub-id><pub-id pub-id-type="pmid">8040158</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Chu</surname> <given-names>C</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name><name><surname>Weiskopf</surname> <given-names>N</given-names></name><name><surname>Molyneux</surname> <given-names>PD</given-names></name><name><surname>Maguire</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding neuronal ensembles in the human Hippocampus</article-title><source>Current Biology</source><volume>19</volume><fpage>546</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.02.033</pub-id><pub-id pub-id-type="pmid">19285400</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Gobbini</surname> <given-names>MI</given-names></name><name><surname>Furey</surname> <given-names>ML</given-names></name><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Schouten</surname> <given-names>JL</given-names></name><name><surname>Pietrini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The decoding toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>EA</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Distinct representations of eye gaze and identity in the distributed human neural system for face perception</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/71152</pub-id><pub-id pub-id-type="pmid">10607399</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ionta</surname> <given-names>S</given-names></name><name><surname>Heydrich</surname> <given-names>L</given-names></name><name><surname>Lenggenhager</surname> <given-names>B</given-names></name><name><surname>Mouthon</surname> <given-names>M</given-names></name><name><surname>Fornari</surname> <given-names>E</given-names></name><name><surname>Chapuis</surname> <given-names>D</given-names></name><name><surname>Gassert</surname> <given-names>R</given-names></name><name><surname>Blanke</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multisensory mechanisms in temporo-parietal cortex support self-location and first-person perspective</article-title><source>Neuron</source><volume>70</volume><fpage>363</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.009</pub-id><pub-id pub-id-type="pmid">21521620</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Bannister</surname> <given-names>P</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname> <given-names>YT</given-names></name><name><surname>Webb</surname> <given-names>TW</given-names></name><name><surname>Meier</surname> <given-names>JD</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Graziano</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Attributing awareness to oneself and to others</article-title><source>PNAS</source><volume>111</volume><fpage>5012</fpage><lpage>5017</lpage><pub-id pub-id-type="doi">10.1073/pnas.1401201111</pub-id><pub-id pub-id-type="pmid">24639542</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname> <given-names>H</given-names></name><name><surname>Kohshima</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Unique morphology of the human eye</article-title><source>Nature</source><volume>387</volume><fpage>767</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1038/42842</pub-id><pub-id pub-id-type="pmid">9194557</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koster-Hale</surname> <given-names>J</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name><name><surname>Dungan</surname> <given-names>J</given-names></name><name><surname>Young</surname> <given-names>LL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Decoding moral judgments from neural representations of intentions</article-title><source>PNAS</source><volume>110</volume><fpage>5648</fpage><lpage>5653</lpage><pub-id pub-id-type="doi">10.1073/pnas.1207992110</pub-id><pub-id pub-id-type="pmid">23479657</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koster-Hale</surname> <given-names>J</given-names></name><name><surname>Bedny</surname> <given-names>M</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thinking about seeing: perceptual sources of knowledge are encoded in the theory of mind brain regions of sighted and blind adults</article-title><source>Cognition</source><volume>133</volume><fpage>65</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2014.04.006</pub-id><pub-id pub-id-type="pmid">24960530</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koster-Hale</surname> <given-names>J</given-names></name><name><surname>Richardson</surname> <given-names>H</given-names></name><name><surname>Velez</surname> <given-names>N</given-names></name><name><surname>Asaba</surname> <given-names>M</given-names></name><name><surname>Young</surname> <given-names>L</given-names></name><name><surname>Saxe</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mentalizing regions represent distributed, continuous, and abstract dimensions of others' beliefs</article-title><source>NeuroImage</source><volume>161</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.026</pub-id><pub-id pub-id-type="pmid">28807871</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Information-based functional brain mapping</article-title><source>PNAS</source><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Pauls</surname> <given-names>J</given-names></name><name><surname>Augath</surname> <given-names>M</given-names></name><name><surname>Trinath</surname> <given-names>T</given-names></name><name><surname>Oeltermann</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title><source>Nature</source><volume>412</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/35084005</pub-id><pub-id pub-id-type="pmid">11449264</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mar</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neural bases of social cognition and story comprehension</article-title><source>Annual Review of Psychology</source><volume>62</volume><fpage>103</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-120709-145406</pub-id><pub-id pub-id-type="pmid">21126178</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marquardt</surname> <given-names>K</given-names></name><name><surname>Ramezanpour</surname> <given-names>H</given-names></name><name><surname>Dicke</surname> <given-names>PW</given-names></name><name><surname>Thier</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Following eye gaze activates a patch in the posterior temporal cortex that is not part of the human &quot;Face Patch&quot; System</article-title><source>Eneuro</source><volume>4</volume><elocation-id>ENEURO.0317-16.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0317-16.2017</pub-id><pub-id pub-id-type="pmid">28374010</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mars</surname> <given-names>RB</given-names></name><name><surname>Sallet</surname> <given-names>J</given-names></name><name><surname>Schüffelgen</surname> <given-names>U</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Toni</surname> <given-names>I</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Connectivity-based subdivisions of the human right &quot;temporoparietal junction area&quot;: evidence for different areas participating in different cortical networks</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>1894</fpage><lpage>1903</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr268</pub-id><pub-id pub-id-type="pmid">21955921</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Misaki</surname> <given-names>M</given-names></name><name><surname>Kim</surname> <given-names>Y</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title><source>NeuroImage</source><volume>53</volume><fpage>103</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.051</pub-id><pub-id pub-id-type="pmid">20580933</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molenberghs</surname> <given-names>P</given-names></name><name><surname>Johnson</surname> <given-names>H</given-names></name><name><surname>Henry</surname> <given-names>JD</given-names></name><name><surname>Mattingley</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Understanding the minds of others: a neuroimaging meta-analysis</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>65</volume><fpage>276</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.03.020</pub-id><pub-id pub-id-type="pmid">27073047</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakagawa</surname> <given-names>S</given-names></name><name><surname>Cuthill</surname> <given-names>IC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Effect size, confidence interval and statistical significance: a practical guide for biologists</article-title><source>Biological Reviews</source><volume>82</volume><fpage>591</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1111/j.1469-185X.2007.00027.x</pub-id><pub-id pub-id-type="pmid">17944619</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Northoff</surname> <given-names>G</given-names></name><name><surname>Heinzel</surname> <given-names>A</given-names></name><name><surname>de Greck</surname> <given-names>M</given-names></name><name><surname>Bermpohl</surname> <given-names>F</given-names></name><name><surname>Dobrowolny</surname> <given-names>H</given-names></name><name><surname>Panksepp</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Self-referential processing in our brain--a meta-analysis of imaging studies on the self</article-title><source>NeuroImage</source><volume>31</volume><fpage>440</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.12.002</pub-id><pub-id pub-id-type="pmid">16466680</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ochsner</surname> <given-names>KN</given-names></name><name><surname>Knierim</surname> <given-names>K</given-names></name><name><surname>Ludlow</surname> <given-names>DH</given-names></name><name><surname>Hanelin</surname> <given-names>J</given-names></name><name><surname>Ramachandran</surname> <given-names>T</given-names></name><name><surname>Glover</surname> <given-names>G</given-names></name><name><surname>Mackey</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Reflecting upon feelings: an fMRI study of neural systems supporting the attribution of emotion to self and other</article-title><source>Journal of Cognitive Neuroscience</source><volume>16</volume><fpage>1746</fpage><lpage>1772</lpage><pub-id pub-id-type="doi">10.1162/0898929042947829</pub-id><pub-id pub-id-type="pmid">15701226</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Passingham</surname> <given-names>RE</given-names></name><name><surname>Bengtsson</surname> <given-names>SL</given-names></name><name><surname>Lau</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Medial frontal cortex: from self-generated action to reflection on one's own performance</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>16</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.11.001</pub-id><pub-id pub-id-type="pmid">19969501</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname> <given-names>DI</given-names></name><name><surname>Smith</surname> <given-names>PA</given-names></name><name><surname>Potter</surname> <given-names>DD</given-names></name><name><surname>Mistlin</surname> <given-names>AJ</given-names></name><name><surname>Head</surname> <given-names>AS</given-names></name><name><surname>Milner</surname> <given-names>AD</given-names></name><name><surname>Jeeves</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Visual cells in the temporal cortex sensitive to face view and gaze direction</article-title><source>Proceedings of the Royal Society of London. Series B, Biological Sciences</source><volume>223</volume><fpage>293</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1098/rspb.1985.0003</pub-id><pub-id pub-id-type="pmid">2858100</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesquita</surname> <given-names>A</given-names></name><name><surname>Chapman</surname> <given-names>CS</given-names></name><name><surname>Enns</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Humans are sensitive to attention control when predicting others' actions</article-title><source>PNAS</source><volume>113</volume><fpage>8669</fpage><lpage>8674</lpage><pub-id pub-id-type="doi">10.1073/pnas.1601872113</pub-id><pub-id pub-id-type="pmid">27436897</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Region of interest analysis for fMRI</article-title><source>Social Cognitive and Affective Neuroscience</source><volume>2</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1093/scan/nsm006</pub-id><pub-id pub-id-type="pmid">18985121</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Orienting of attention</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>32</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1080/00335558008248231</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname> <given-names>A</given-names></name><name><surname>Allison</surname> <given-names>T</given-names></name><name><surname>Bentin</surname> <given-names>S</given-names></name><name><surname>Gore</surname> <given-names>JC</given-names></name><name><surname>McCarthy</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>2188</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-06-02188.1998</pub-id><pub-id pub-id-type="pmid">9482803</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname> <given-names>P</given-names></name><name><surname>Northoff</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How is our self related to midline regions and the default-mode network?</article-title><source>NeuroImage</source><volume>57</volume><fpage>1221</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.05.028</pub-id><pub-id pub-id-type="pmid">21609772</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Randall</surname> <given-names>W</given-names></name><name><surname>Guterstam</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A motion aftereffect from viewing other people’s gaze</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.08.373308</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname> <given-names>R</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>People thinking about thinking people the role of the temporo-parietal junction in &quot;theory of mind&quot;</article-title><source>NeuroImage</source><volume>19</volume><fpage>1835</fpage><lpage>1842</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00230-1</pub-id><pub-id pub-id-type="pmid">12948738</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname> <given-names>R</given-names></name><name><surname>Wexler</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Making sense of another mind: the role of the right temporo-parietal junction</article-title><source>Neuropsychologia</source><volume>43</volume><fpage>1391</fpage><lpage>1399</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.02.013</pub-id><pub-id pub-id-type="pmid">15936784</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schneider</surname> <given-names>B</given-names></name><name><surname>Pao</surname> <given-names>Y</given-names></name><name><surname>Pea</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predicting students’ Learning Outcomes Using Eye-Tracking Data</article-title><conf-name>Learn Anal Knowl Conference</conf-name></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurz</surname> <given-names>M</given-names></name><name><surname>Radua</surname> <given-names>J</given-names></name><name><surname>Aichhorn</surname> <given-names>M</given-names></name><name><surname>Richlan</surname> <given-names>F</given-names></name><name><surname>Perner</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fractionating theory of mind: a meta-analysis of functional brain imaging studies</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>42</volume><fpage>9</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.01.009</pub-id><pub-id pub-id-type="pmid">24486722</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>Pope</surname> <given-names>DL</given-names></name><name><surname>Astafiev</surname> <given-names>SV</given-names></name><name><surname>McAvoy</surname> <given-names>MP</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Right hemisphere dominance during spatial selective attention and target detection occurs outside the dorsal frontoparietal network</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3640</fpage><lpage>3651</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4085-09.2010</pub-id><pub-id pub-id-type="pmid">20219998</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname> <given-names>NJ</given-names></name><name><surname>Avants</surname> <given-names>BB</given-names></name><name><surname>Cook</surname> <given-names>PA</given-names></name><name><surname>Zheng</surname> <given-names>Y</given-names></name><name><surname>Egan</surname> <given-names>A</given-names></name><name><surname>Yushkevich</surname> <given-names>PA</given-names></name><name><surname>Gee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 Bias correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Veluw</surname> <given-names>SJ</given-names></name><name><surname>Chance</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Differentiating between self and others: an ALE meta-analysis of fMRI studies of self-recognition and theory of mind</article-title><source>Brain Imaging and Behavior</source><volume>8</volume><fpage>24</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1007/s11682-013-9266-8</pub-id><pub-id pub-id-type="pmid">24535033</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vernet</surname> <given-names>M</given-names></name><name><surname>Japee</surname> <given-names>S</given-names></name><name><surname>Lokey</surname> <given-names>S</given-names></name><name><surname>Ahmed</surname> <given-names>S</given-names></name><name><surname>Zachariou</surname> <given-names>V</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Endogenous visuospatial attention increases visual awareness independent of visual discrimination sensitivity</article-title><source>Neuropsychologia</source><volume>128</volume><fpage>297</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.08.015</pub-id><pub-id pub-id-type="pmid">28807647</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogeley</surname> <given-names>K</given-names></name><name><surname>Bussfeld</surname> <given-names>P</given-names></name><name><surname>Newen</surname> <given-names>A</given-names></name><name><surname>Herrmann</surname> <given-names>S</given-names></name><name><surname>Happé</surname> <given-names>F</given-names></name><name><surname>Falkai</surname> <given-names>P</given-names></name><name><surname>Maier</surname> <given-names>W</given-names></name><name><surname>Shah</surname> <given-names>NJ</given-names></name><name><surname>Fink</surname> <given-names>GR</given-names></name><name><surname>Zilles</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Mind reading: neural mechanisms of theory of mind and self-perspective</article-title><source>NeuroImage</source><volume>14</volume><fpage>170</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0789</pub-id><pub-id pub-id-type="pmid">11525326</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wicker</surname> <given-names>B</given-names></name><name><surname>Michel</surname> <given-names>F</given-names></name><name><surname>Henaff</surname> <given-names>MA</given-names></name><name><surname>Decety</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Brain regions involved in the perception of gaze: a PET study</article-title><source>NeuroImage</source><volume>8</volume><fpage>221</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0357</pub-id><pub-id pub-id-type="pmid">9740764</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain MR images through a hidden markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63551.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution>Radboud University</institution><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Atkinson</surname><given-names>Anthony</given-names> </name><role>Reviewer</role><aff><institution>Durham University</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper reports the novel finding that fMRI activity patterns in the left TPJ distinguish between stories in which an agent endogenously versus exogenously attends to an object. The experimental design is straightforward and elegant, using tightly-controlled comparisons. The findings suggest that brain regions implicated in theory-of-mind represent a model of another person's attentional state.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Temporo-Parietal Cortex Involved in Modeling One's Own and Others' Attention&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Anthony Atkinson (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Summary:</p><p>This fMRI study tested whether theory-of-mind (ToM) regions differentially represent information about endogenous and exogenous attention of the self and of another person. Participants read short passages that described themselves or another person deliberately attending to or having their attention drawn to an object. The main finding was that activity patterns in left TPJ allowed for decoding endogenous vs. exogenous attention (collapsed across self and other). This finding indicates a role for the left TPJ in modelling attentional states.</p><p>Revisions for this paper:</p><p>Motivation and interpretation:</p><p>1) Considering that the study decodes mental states (here: attentional states) in ToM areas, as described in stories, please review work similarly decoding mental states in these regions (e.g., work by Koster-Hale, Saxe). How is your study similar or different from previous work decoding mental states? Do results follow from these previous studies? I also missed a review of (and integration with) the literature on endogenous vs exogenous attention itself (e.g., ventral vs dorsal attention network).</p><p>2) It was not clear from the Introduction why you hypothesized that exo vs endo attention should be decodable in TPJ. Is this where eye gaze direction – which appeared to have motivated the study – can be decoded from? The study cited for this hypothesis (Kelly et al., 2014; reanalysed in Igelstrom et al., 2016) showed that univariate activity in TPJ reflected the difficulty of social attribution, which does not obviously lead to the current hypothesis.</p><p>3) Does the main finding reflect a model of others' attention or differences in mental state attribution: sentences describing endogenous attention focus the reader more on the mental act of attending and might also induce further mentalizing (e.g., the reader may wonder: &quot;why would he decide to look for that object?&quot;). In exogenous sentences, the focus of the sentence is instead more strongly on the attention-grabbing object (e.g., &quot;the bright red tie&quot;). Can this alternative interpretation be excluded?</p><p>4) Searchlight analysis for exogenous v. endogenous attention: Is the cluster centred at -59, -47, 5, labelled left posterior STS (TPJ), really TPJ? It is, after all, squarely in the temporal lobe and some distance (22mm) from the centre of the left TPJ ROI, the latter being in left angular gyrus (areas PGa and PFm). Do you have some independent justification for the labelling of the location of this cluster as TPJ? For example, perhaps it lies within the anterior TPJ (TPJa) subregion identified by Mars et al. (Cerebral Cortex 2012)? The posterior STS area from the searchlight analysis seems to fall into the &quot;gaze-following patch&quot; in the posterior STS as reported by Marquardt et al., 2017. Since this cortical area is not generally considered to be a part of the theory of mind network, the seeming involvement of this area potentially changes the interpretation of the results.</p><p>Additional analyses:</p><p>5) Please report and compare accuracy and RTs for the response to the probe statement. If either differs between conditions, then that becomes a potential confound for the between condition contrasts in the MVPA analyses, considering that the BOLD response to the story and probe events could not be separated.</p><p>6) The TPJ is introduced in the context of theory of mind and social cognition, but it has also been implicated in attentional orienting. Could it be the case that participants simulate such orienting when reading the stories, leading to the above-chance decoding in TPJ? If this is the case, one may similarly expect above-chance decoding in areas that have been implicated in endogenous attention. This should be tested by including dorsal attention network ROIs. Above-chance decoding in such attention regions may inform the interpretation of the TPJ results.</p><p>7) Please provide univariate activity estimates, both in the ROIs and in whole-brain contrasts. This may help to interpret the multivariate results.</p><p>Further support for main finding:</p><p>8) The reported decoding accuracy values are really quite small and close to 50% (chance), especially for the endogenous vs. exogenous and self vs. other contrasts, even those values that are statistically significant. Please report appropriate effect sizes and the confidence intervals around those effect sizes (for all your reported t-tests, not just those that are statistically significant). It would also be informative if you were to include in your graphs the individual subject data (e.g., mean decoding accuracy per subject for each ROI) and/or plots of the effect sizes and their distributions. For more on effect sizes, their CIs and associated plots, I point you to the following sources and the references therein:</p><p>https://thenewstatistics.com/itns</p><p>https://thenewstatistics.com/itns/2019/05/20/reply-to-lakens-the-correctly-used-p-value-needs-an-effect-size-and-ci/</p><p>https://www.estimationstats.com/</p><p>9) The authors computed the attentional state decoding separately for &quot;self&quot; and &quot;other&quot;. These decoding accuracies did not differ. Please also provide and test the accuracies of self and other separately; this would shed light on the reliability of the main result (which was collapsed across the two conditions) and might also indicate whether the decoding was more reliable (less variable) in self or other.</p><p>10) Replicate results in one or multiple additional ToM TPJ ROIs. Reviewers raised two suggestions: 1) Surface-based ROI: the TPJ is a highly anatomically variable cortical region that isn't well approximated by a single volume ROI (see Croxson et al., 2017). There are many surface-based ROIs now available that mitigate the effect of this variability, including ones from the authors' own lab. 2) Use a different meta-analysis: The used meta-analysis defines “theory of mind” solely in terms of false-belief tasks (van Veluw and Chance, 2014), which may not be appropriate for delineating the ROIs and their exact locations in your study. Different types of ToM task reliably activate different brain areas, as well as common ones (mPFC and bilateral TPJ: Molenberghs et al., Neuroscience and Biobehavioral Review 2016). Consider using the results of a different meta-analysis, e.g., one that identifies regions based on the conjunction of multiple types of theory-of-mind tasks (e.g., Mar, Annual Review of Psychology 2011; Molenberghs et al., 2016; Schurz et al., Neuroscience and Biobehavioral Reviews 2014).</p><p>11) In previous work (Kelly et al., 2014), the authors were interested in an overlap between attention in self and other. Here, this could be addressed in a cross-decoding analysis, training a classifier on exo vs endo in the &quot;self&quot; stories and testing this on the &quot;other&quot; stories. Above-chance classification in this analysis would strengthen the evidence for lTPJ involvement and would provide additional information that would help interpreting the TPJ findings.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Temporo-Parietal Cortex Involved in Modeling One's Own and Others' Attention&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, one of whom is a member of our Board of Reviewing Editor, and the evaluation has been overseen by Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Summary:</p><p>Both reviewers noted that many of their concerns were addressed. However, they each had one remaining concern that would need to be addressed.</p><p>Essential Revisions:</p><p>1) Please report and analyze the univariate activity in the ROIs</p><p>2) More fully report (in main Results section) and discuss the searchlight results.</p><p><italic>Reviewer #1:</italic></p><p>The authors have addressed many of my concerns. In particular, the additional TPJ ROIs and the demonstration of self-other cross-decoding increased my confidence in the main finding of attention state decoding in lTPJ.</p><p>One comment has not been sufficiently addressed, however: it would be highly relevant and informative to see the average univariate activity for each of the conditions in the 6 original ROIs, plotted in a graph. The endo&gt;exo contrast should be tested, correcting for the 6 ROIs, similar to how the decoding accuracies are tested (i.e., with the same p&lt;0.05 cut-off). This ROI-based univariate analysis is also interesting for other comparisons, e.g. to test whether the social conditions gave higher activity than the non-social condition, as would be expected based on previous work. Thus, please provide a full univariate analysis on average ROI activity.</p><p>There is a lot of interesting information in the Supplementary figures (e.g., searchlight, additional ROIs), which will not be visible when viewing or printing pdfs. You could consider moving some of this to the main text, or add panels to existing figures where you include some of this information.</p><p><italic>Reviewer #2:</italic></p><p>In this revised manuscript, the authors have greatly clarified some key points, most important of which was how well matched the behavior was across all of the conditions. While there is still a potential confound in the social vs. non-social contrast given the difference in reaction time, (a) the confound effects are likely negligible in size, (b) the end result matches the previous literature, and (c) the contrast was only a control analysis and doesn't greatly affect the main point of the paper. However, the handling of the left posterior STS result in the searchlight analysis remains problematic, both in the manuscript and in the author's response.</p><p>pSTS response and discussion: The main point of the manuscript is to examine whether known theory of mind areas contain information that differentiates between the endogenous vs. exogenous story conditions. The revised manuscript makes quite clear that the answer is yes, activity in the L TPJ differentiates between the two conditions, albeit with a relatively low decoding accuracy. The fact that multiple versions of the L TPJ ROI produces the same result is particularly reassuring. The spotlight analysis, however, finds that there is potentially more discriminatory information to be found in the posterior STS/middle temporal gyrus, and this is where the manuscript runs into problems.</p><p>In the revised manuscript, this result is simply not mentioned, though it clearly has implications for the interpretations of the result as mentioned in the previous round of reviews. In addition, in the response to the reviewers' comments, the authors give a very convoluted and confusing argument based on MNI coordinates that somehow this focus is somehow (a) part of the TPJ and (b) may be a part of the attention-related TPJa as opposed to their TPJp ROI. In the process of saying that the surface projections are misleading, they state that the focus as falling on the STG on the surface projection, when it is really on the MTG. In addition, they base their arguments on MNI coordinates, but MNI coordinates are notoriously inaccurate in comparing results from different studies, so the author's attempts to justify their conclusions using these coordinates falls flat. The authors own volume images very clearly show the focus as being in the STS and not on either the angular gyrus or supramarginal gyrus that Mars, Corbetta, and others have generally shown the TPJp and TPJa to fall on. And even if this focus was in the TPJa, as the authors seem to hint at, they do not discuss the implications of this result.</p><p>The authors' defensiveness around this point is frankly puzzling. The searchlight results do not seem to invalidate their main point, only potentially augment it. The fact that the posterior STS (and what seem to be the right FEF and iPCS areas) exhibit higher discriminability between the endo and exo conditions may not be shocking given that the theory of mind areas are likely involved in many operations in this task, whereas the pSTS and right hemisphere areas likely may only be involved in just the imagined attention/sensory processing aspects of the task. It is possible that these areas are &quot;reading out&quot; the differing attentional conditions from the L TPJ. Whatever the explanation may be, the authors seem to be trying to bury this result to shoehorn the results into fitting their a priori model, which is a disservice and misleading to the readers, and needs to be rectified before the manuscript can be published. My recommendation is to at least mention the searchlight results in the main Results section, then add a paragraph to the Discussion discussing the implications of these results. Better yet would be to quantify the discrimination accuracy within each of the foci uncovered in the spotlight analyses.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63551.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Revisions for this paper:</p><p>Motivation and interpretation:</p><p>1) Considering that the study decodes mental states (here: attentional states) in ToM areas, as described in stories, please review work similarly decoding mental states in these regions (e.g., work by Koster-Hale, Saxe). How is your study similar or different from previous work decoding mental states? Do results follow from these previous studies? I also missed a review of (and integration with) the literature on endogenous vs exogenous attention itself (e.g., ventral vs dorsal attention network).</p></disp-quote><p>We thank the reviewers for these relevant suggestions. The work in our lab has focused on how people construct models of their own and others’ attention, an area of modeling mental states that is relatively less well studied. We now cite and briefly review previous relevant work using MVPA to decode various aspects of other people’s mental state, such as their beliefs (Koster-Hale et al., 2017), intentions (Koster-Hale et al., 2013), and perceptual source (Koster-Hale et al., 2014), from activity patterns in the theory of mind network. We also clarify what we believe distinguishes the current study from earlier ones. From the fifth paragraph of the Introduction:</p><p>“This first prediction, that the social cognition network will encode the exogenous-versus-endogenous distinction, represents the main, novel contribution of this study. Previous studies have used MVPA to decode various aspects of other people’s mental states from activity in social brain areas, such as their beliefs (Koster-Hale et al., 2017), intentions (Koster-Hale et al., 2013), and perceptual source (Koster-Hale et al., 2014). To the best of our knowledge, this investigation is the first to test whether activity in social brain areas can decode other people’s attentional states.”</p><p>We also added a brief review of endogenous versus exogenous attention in the revised Introduction (paragraph three):</p><p>“For example, Pesquita et al. (2016) found that when participants watch an actor in a video attending to an object, the participants implicitly distinguish between whether the actor’s attention was drawn to the object exogenously (bottom-up, or stimulus-driven attention), or whether the actor endogenously shifted attention to the object (top-down, or internally driven attention). Exogenous and endogenous attention are the two principal ways in which selective attention moves between objects. They may be relatively emphasized in distinct cortical networks (the ventral and dorsal attention networks), and they influence the behavior of agents in profoundly different manners (Corbetta et al., 2008; Corbetta and Shulman, 2002; Posner, 1980; Shulman et al., 2010). The ability to distinguish between someone else’s exogenous and endogenous attention is therefore one example of how people may construct a rich, dynamic, and useful model of other people’s attention beyond merely encoding gaze direction or identifying the object of attention.”</p><disp-quote content-type="editor-comment"><p>2) It was not clear from the Introduction why you hypothesized that exo vs endo attention should be decodable in TPJ. Is this where eye gaze direction – which appeared to have motivated the study – can be decoded from? The study cited for this hypothesis (Kelly et al., 2014; reanalysed in Igelstrom et al., 2016) showed that univariate activity in TPJ reflected the difficulty of social attribution, which does not obviously lead to the current hypothesis.</p></disp-quote><p>We thank the reviewers for allowing us to clarify the rationale behind our central <italic>Prediction 1</italic>. First, we would like to stress that the wording of <italic>Prediction 1</italic> emphasized that we expected above-chance decoding in “some subset” of theory-of-mind areas, not only in the TPJ:</p><p>“We hypothesized that participants would encode the type of attention in the story (exogenous versus endogenous), and that this encoding would be evident in some subset of the areas classically involved in theory of mind. […] We therefore predicted that the exogenous-versus-endogenous distinction would be significantly encoded in some subset of these areas.”</p><p>In accordance with this prediction, we tested exogenous-versus-endogenous decoding in each of the six ROIs and corrected for multiple comparisons across all ROIs.</p><p>We also anticipated that, among the theory-of-mind areas, the TPJ might show the clearest evidence of attention type decoding, based on the results of Kelly et al. (2014). Kelly et al. (2014) is particularly relevant to our study because it identified areas showing greater activity when the social attribution of an attentional state to a cartoon face is more difficult, while controlling for factors such as the direction of gaze, emotional valence, and low-level visual features. An important underlying concept in our current work is that modeling someone else’s attention, and processing someone else’s gaze direction, are not the same. Gaze is only one cue to someone else’s attention. In Kelly et al. (2014), we found that the TPJ was not active in association with gaze, and also not active in association with facial expression; but was active in association with a task in which subjects integrated both gaze and expression in order to judge the attentional state of someone else. Considering that the task in Kelly et al. (2014) engaged the TPJ bilaterally, but none of the other theory-of-mind regions investigated here (i.e., STS, precuneus, and MPFC), we anticipated the TPJ might be more likely than the other theory-of-mind ROIs to be involved in encoding attention type. As discussed more in detail in our response to point #4 below, the TPJ region investigated here, which was defined based on studies of theory of mind reasoning, is spatially separated (by 2 cm) from the location of the gaze-following patch within the STS.</p><p>To address the reviewers’ concern, we removed the statement in the Introduction that predicts an effect specifically in the TPJ. Instead, in the Introduction, we confine <italic>Prediction 1</italic> to a more cautious statement, predicting that the exogenous-versus-endogenous distinction should be significantly encoded in at least some subset of the ROIs representing the ToM cortical network.</p><p>In the revised Discussion section, we added a paragraph (third paragraph) discussing the reasons why the TPJ may have been a special focus of activity here. In that paragraph, we describe more fully why Kelly et al. (2014) is consistent with this finding.</p><disp-quote content-type="editor-comment"><p>3) Does the main finding reflect a model of others' attention or differences in mental state attribution: sentences describing endogenous attention focus the reader more on the mental act of attending and might also induce further mentalizing (e.g., the reader may wonder: &quot;why would he decide to look for that object?&quot;). In exogenous sentences, the focus of the sentence is instead more strongly on the attention-grabbing object (e.g., &quot;the bright red tie&quot;). Can this alternative interpretation be excluded?</p></disp-quote><p>This is an important point that we thought deeply about when constructing the story stimuli. The short answer is that the alternative interpretation can’t be entirely excluded, but we believe it is unlikely. We now address it explicitly in the sixth paragraph of the Discussion section:</p><p>“A second, alternative interpretation of the present results is that the exogenous sentences might make the reader focus more on the object in the story (thus engaging less mentalizing), whereas the endogenous sentences might make the reader focus more on the character’s mental act of attending (thus engaging more mentalizing). […] The results point to the left TPJ processing different information in the exogenous and endogenous story types, but not a difference in overall amount of activity.”</p><disp-quote content-type="editor-comment"><p>4) Searchlight analysis for exogenous v. endogenous attention: Is the cluster centred at -59, -47, 5, labelled left posterior STS (TPJ), really TPJ? It is, after all, squarely in the temporal lobe and some distance (22mm) from the centre of the left TPJ ROI, the latter being in left angular gyrus (areas PGa and PFm). Do you have some independent justification for the labelling of the location of this cluster as TPJ? For example, perhaps it lies within the anterior TPJ (TPJa) subregion identified by Mars et al. (Cerebral Cortex 2012)? The posterior STS area from the searchlight analysis seems to fall into the &quot;gaze-following patch&quot; in the posterior STS as reported by Marquardt et al. 2017. Since this cortical area is not generally considered to be a part of the theory of mind network, the seeming involvement of this area potentially changes the interpretation of the results.</p></disp-quote><p>We very much appreciate the reviewers bringing this issue of neuroanatomy to our attention, and for directing us to the Mars et al. (2012) paper. First, we would like to emphasize that the anatomical localization of the Searchlight decoding clusters was based on the projection of the clusters onto sections of the average structural scan generated from the 32 subjects. The projection of volumetric results onto a 3D canonical brain surface is an approximate and imperfect process, and was here used for visualization purposes only. To better clarify the three-dimensional location of the left posterior STS cluster in the endogenous-vs-exogenous Searchlight analysis, we have now included all three sections in the bottom part of Figure 3—figure supplement 3</p><p>As can be seen in Figure 3—figure supplement 3, the cluster lies entirely in the posterior section of the STS (and not on the STG, which one might guess from looking at the brain surface projection). As the reviewers point out, the peak of the Searchlight cluster (MNI: -59, -47, 5) is anterior-inferior with respect to the center of our left TPJ ROI (MNI: -52, -56, 24). However, do they belong to the same or different TPJ subregions? According to the connectivity-based TPJ subdivision proposed by Mars et al. (2012), the border between the anterior (TPJa) and posterior TPJ (TPJp) within the posterior STS is somewhere between the MNI y-coordinates -40 and -48 (see Figure 3a in Mars et al.; unfortunately, a more precise definition in terms of MNI coordinates are not reported). Our TPJ ROI is clearly located in TPJp. The Searchlight cluster is likely located in TPJp but very close to the border of TPJa, given that the Searchlight cluster peak is 3 mm closer to the center of gravity for TPJp compared to TPJa (23 mm vs 26 mm). This anatomical labelling is in accordance with another functional TPJ parcellation (Bzdok et al. 2013), in which the TPJp within STS starts at the MNI y-coordinate -47 (see Figure 2 in Bzdok et al. 2013).</p><p>It should be noted that both the Mars et al. (2012) and Bzdok et al. (2013) TPJ characterizations are based on the right TPJ. However, the TPJ has a known hemispheric asymmetry regarding functional specialization (Seghier, 2013), neurological lesion effects (Corbetta et al., 2000), functional (Uddin et al., 2010) and anatomical (Caspers et al., 2011) connectivity, as well as cytoarchitectonic borders and gyral pattern (Caspers et al., 2006, 2008). The proposed TPJa and TPJp subdivisions may therefore not directly apply to the left TPJ, and should be interpreted with caution.</p><p>Finally, the endogenous-vs-exogenous decoding cluster does not overlap with the gaze-following patch (GFP) reported in Marquardt et al. 2017 (MNI: -55, -67, 6). The GFP is located 20 mm directly posterior to the decoding cluster (and inferior to our TPJ ROI).</p><p>We have updated the Supplementary file 7, Figure 3—figure supplement 3, and Supplementary file 1, which now briefly discuss the spatial discrepancy of the left TPJ ROI and the Searchlight decoding cluster. We have also added references to Mars et al. (2012) and Bzdok et al. (2013).</p><disp-quote content-type="editor-comment"><p>Additional analyses:</p><p>5) Please report and compare accuracy and RTs for the response to the probe statement. If either differs between conditions, then that becomes a potential confound for the between condition contrasts in the MVPA analyses, considering that the BOLD response to the story and probe events could not be separated.</p></disp-quote><p>We thank the reviewers for suggesting this relevant analysis. Among the four social conditions, <italic>self-endo</italic>, <italic>self-exo</italic>, <italic>other-endo</italic>, and <italic>other-exo</italic>, we did not expect any systematic differences in performance on the probe statement task, given the extremely subtle semantic differences across conditions. As predicted, neither the mean accuracies (94.2% vs 93.4% vs 93.1% vs 93.4%; F<sub>3,93</sub>=0.15, p=0.930, repeated-measures ANOVA) nor the mean RTs (1.61s vs 1.60s vs 1.67s vs 1.65s; F<sub>3,93</sub>=1.66, p=0.181, repeated-measures ANOVA) for the response to the probe statement differed significantly across the four conditions.</p><p>These behavioral results relating to the response to the probe statement are now included in the revised manuscript, in a new first section of the Results (<italic>Behavioral results</italic>). Note also that since the story period was 10 sec, and the subsequent probe event was 4 sec, it was possible to focus the analysis on the MRI activity evoked by the story, fairly well uncontaminated by the probe event.</p><disp-quote content-type="editor-comment"><p>6) The TPJ is introduced in the context of theory of mind and social cognition, but it has also been implicated in attentional orienting. Could it be the case that participants simulate such orienting when reading the stories, leading to the above-chance decoding in TPJ? If this is the case, one may similarly expect above-chance decoding in areas that have been implicated in endogenous attention. This should be tested by including dorsal attention network ROIs. Above-chance decoding in such attention regions may inform the interpretation of the TPJ results.</p></disp-quote><p>We thank the reviewers for this intriguing suggestion. We have performed the requested analysis and included it in the new submission. If we understand the reviewers’ suggestion correctly, they hypothesize that our subjects, when reading stories, may have simulated exogenous and endogenous attention reorienting by activating the corresponding ventral and dorsal attention networks in a “mirror-neuron-like” fashion. This proposal would explain our TPJ result, because this area is a key node in the ventral attention network and might thus be involved in simulating exogenous attention orienting. If this mechanism is the underlying cause of the TPJ result, one would also predict the involvement of the dorsal attention network when subjects read (and simulated) the endogenous stories.</p><p>To test this prediction, as requested, we repeated the endogenous-versus-exogenous decoding analysis in four areas typically considered to constitute the dorsal attention network: the frontal eye fields (FEF), the anterior and posterior intraparietal sulcus (aIPS and pIPS), and the middle temporal complex (MT+). The ROIs were defined as 10-mm-radius spheres around the peak coordinates reported in Fox et al. (2006). The results are shown in Figure 3—figure supplement 6. None of the dorsal attention network ROIs decoded the attention type better than chance (all ps &gt; 0.05, based on permutation testing with 10,000 iterations, uncorrected for multiple comparisons). These findings are thus incompatible with the proposed attention simulation account of the TPJ result.</p><p>This issue and these new results are now discussed in the fifth paragraph of the revised Discussion section, and the analysis results are also included in the supplementary material and shown in Figure 3—figure supplement 6.</p><disp-quote content-type="editor-comment"><p>7) Please provide univariate activity estimates, both in the ROIs and in whole-brain contrasts. This may help to interpret the multivariate results.</p></disp-quote><p>The results for the seven univariate contrasts corresponding to the main multivariate analyses are now included in the supplementary material (Supplementary file 5). We report univariate activity corrected for multiple comparisons both at the whole-brain level and within the search volume of each ROI. Notably, there were no significant clusters in any of the ROIs, or anywhere else in the brain, for the <italic>ENDOGENOUS &gt; EXOGENOUS</italic> and <italic>EXOGENOUS &gt; ENDGENOUS</italic> contrasts, not even at the uncorrected threshold p&lt;0.001. These findings are compatible with previous studies (e.g., Hassabis et al. 2009 Current Biology) that have demonstrated the superiority of pattern-sensitive multivariate analyses compared with conventional univariate approaches for detecting differences in activity between conditions with highly similar macroscopic characteristics. In the revised manuscript, we added a sentence in the tenth Discussion paragraph referring to the univariate results.</p><p>The SELF &gt; OTHER contrast also did not reveal any significant activity (even at the p&lt;0.001 uncorrected level). The OTHER &gt; SELF contrast revealed one single significant cluster, which was located in the left calcarine sulcus and survived whole-brain correction for multiple comparisons. We speculate that this activation reflects a main effect of the visual input of a name (e.g. “Emma”) in the OTHER condition versus the word “You” in the SELF. We observed no other activations at the p&lt;0.001 uncorrected level in the rest of the brain, and the left calcarine activation was located at a distance from the locations of our ROIs (the STS, TPJ, MPFC, and precuneus) and did not overlap with the decoding results.</p><p>The attention type X agent type interactions did not reveal any activity at the whole-brain or ROI levels that survived multiple comparisons.</p><p>The Social &gt; Non-Social contrast revealed more significant and widespread activity than the above contrasts, which was expected given the much greater semantic difference between the conditions. We found no significant activity within the ROIs, but four clusters survived correction at the whole-brain level: the left middle orbital gyrus, right SMG, left SFG, and the left insula.</p><disp-quote content-type="editor-comment"><p>Further support for main finding:</p><p>8) The reported decoding accuracy values are really quite small and close to 50% (chance), especially for the endogenous vs. exogenous and self vs. other contrasts, even those values that are statistically significant. Please report appropriate effect sizes and the confidence intervals around those effect sizes (for all your reported t-tests, not just those that are statistically significant). It would also be informative if you were to include in your graphs the individual subject data (e.g., mean decoding accuracy per subject for each ROI) and/or plots of the effect sizes and their distributions. For more on effect sizes, their CIs and associated plots, I point you to the following sources and the references therein:</p><p>https://thenewstatistics.com/itns</p><p>https://thenewstatistics.com/itns/2019/05/20/reply-to-lakens-the-correctly-used-p-value-needs-an-effect-size-and-ci/</p><p>https://www.estimationstats.com/</p></disp-quote><p>We thank the reviewers for this suggestion. We believe the decoding accuracy effects are not so small compared to other findings in the literature. This is especially so when considering that the effects noted by the reviewers are for the most subtle stimulus distinctions in our experiments. The effects are, of course, larger for the social-vs-nonsocial comparisons, because the distinction between the stimuli is much larger. In addition (as suggested by the reviewers in point 10 below), we have now replicated the crucial left TPJ decoding effect using three other anatomical definitions of that theory-of-mind ROI (see new Figure 3—figure supplement 2). All three anatomical definitions of the ROI show a highly significant effect (actually larger than the effect reported in our main analysis in the ROI based on van Veluw and Chance, 2014). Those replications of the finding may help convince the reviewers of the robustness of the result.</p><p>We also thank the reviewers for the suggestion to show more information about effect size. In Tables 1 and 2, we had already reported, for all analyses regardless of their significance, the effect size in terms of mean decoding accuracy (relative to chance), the 95% confidence interval around the mean (based on a bootstrap distribution), and the p value (based on permutation testing). To visualize the data more clearly, we have now included two additional supplementary figures featuring violin plots for all other ROI analyses (see Figure 3—figure supplement 1 and Figure 4—figure supplement 2). The violin plots show mean and median decoding accuracy, 95% confidence interval, a kernel density estimation, and individual data points for each decoding analysis and each ROI. (To avoid confusion and visual clutter, we left the figures in the main body of the paper as they were, and presented the new violin plots in the supplementary material.)</p><disp-quote content-type="editor-comment"><p>9) The authors computed the attentional state decoding separately for &quot;self&quot; and &quot;other&quot;. These decoding accuracies did not differ. Please also provide and test the accuracies of self and other separately; this would shed light on the reliability of the main result (which was collapsed across the two conditions) and might also indicate whether the decoding was more reliable (less variable) in self or other.</p></disp-quote><p>Thank you for this suggestion. These results are now presented in Figure 3—figure supplement 1 in the revised submission. These results revealed no significant activity in the left TPJ (or in any of the other ROIs), which is likely due to a lack of statistical power (since only half of the data set is used for these analyses). There is, however, an alternative analysis that gets at the same question but does not suffer from a reduction in statistical power, and which does show significant activity in the left TPJ. We included that new analysis in the revision. Please see our response to the related point #11 for a description of that new analysis.</p><disp-quote content-type="editor-comment"><p>10) Replicate results in one or multiple additional ToM TPJ ROIs. Reviewers raised two suggestions: 1) Surface-based ROI: the TPJ is a highly anatomically variable cortical region that isn't well approximated by a single volume ROI (see Croxson et al. 2017). There are many surface-based ROIs now available that mitigate the effect of this variability, including ones from the authors' own lab. 2) Use a different meta-analysis: The used meta-analysis defines “theory of mind” solely in terms of false-belief tasks (van Veluw and Chance, 2014), which may not be appropriate for delineating the ROIs and their exact locations in your study. Different types of ToM task reliably activate different brain areas, as well as common ones (mPFC and bilateral TPJ: Molenberghs et al., Neuroscience and Biobehavioral Review 2016). Consider using the results of a different meta-analysis, e.g., one that identifies regions based on the conjunction of multiple types of theory-of-mind tasks (e.g., Mar, Annual Review of Psychology 2011; Molenberghs et al., 2016; Schurz et al., Neuroscience and Biobehavioral Reviews 2014).</p></disp-quote><p>We welcome the reviewers’ suggestion of replicating the endogenous-versus-exogenous decoding results in different ToM TPJ ROIs. In line with their recommendation, we defined three new left TPJ ROIs based on the peak coordinates reported in Mar (2011), Schurz et al. (2014), and Molenberghs et al. (2016). The results showed significant decoding in the Mar ROI (mean decoding accuracy 53.4%, 95% CI 51.0 to 55.1, p=0.0023), the Schurz ROI (mean decoding accuracy 52.7%, 95% CI 50.8 to 54.4, p=0.0092), and in the Molenberghs ROI (mean decoding accuracy 53.4%, 95% CI 51.3 to 55.2, p=0.0020), suggesting that the attention type decoding in the left TPJ is robust. In the revised paper, these results are included in the supplementary material (see Figure 3—figure supplement 2, also pasted above in relation to major point 8).</p><p>The suggestion of re-analyzing the data set using surface-based techniques is interesting and definitely something we aim to pursue in future studies. However, we feel that mixing surface- and volume-based analysis methods, and defining ROIs using fundamentally different approaches, might risk over-complicating the paper. Furthermore, the surface-based TPJ subdivision described by Igelstrom et al. (2016) suffers the same draw-back as the van Veluw and Chance (2014) meta-analysis in that it only involves one type of theory-of-mind task (a false belief task). We therefore decided to use volume-based methods throughout.</p><disp-quote content-type="editor-comment"><p>11) In previous work (Kelly et al., 2014), the authors were interested in an overlap between attention in self and other. Here, this could be addressed in a cross-decoding analysis, training a classifier on exo vs endo in the &quot;self&quot; stories and testing this on the &quot;other&quot; stories. Above-chance classification in this analysis would strengthen the evidence for lTPJ involvement and would provide additional information that would help interpreting the TPJ findings.</p></disp-quote><p>We thank the reviewer for this excellent suggestion. Indeed, one of the goals of the study was to test for differences in the encoding of attention type in self versus others (<italic>Prediction 3</italic>). Attention type could be significantly decoded in the left TPJ when taking all data (“self” and “other” stories) into account (Figure 3A). However, we did not find significant decoding in the left TPJ when analyzing the “self” stories and “other” stories separately (see our response to point #9), which is likely due to a lack of statistical power because the data set was split in two. The absence of a self-vs-other difference in attention decoding could thus be due to either a lack of statistical power, or a true overlap between attention type encoding in self and other.</p><p>A direct way of testing for overlap in attention encoding in self and others in the left TPJ, while maintaining statistical power by using the entire data set, is to employ a two-way cross-classification analysis. In line with the reviewers’ suggestion, we have now included this analysis in the paper (Figure 3—figure supplement 7). In each subject, one classifier was trained to discriminate endogenous versus exogenous self-stories and tested on other-stories (using a leave-one-run out approach), and another classifier was trained to discriminate endogenous versus exogenous other-stories and tested on self-stories, from which an average cross-classification decoding accuracy for the left TPJ was obtained. The group-level result shows significant above-chance decoding (mean decoding accuracy 51.9%, 95% CI 49.9 to 54.1, p=0.0393, permutation testing with 10,000 iterations). This finding suggests that there is an overlap in brain mechanisms that participate in the encoding of attention in others and in encoding of attention in the self in the left TPJ.</p><p>The new results are now included in the Results section (under <italic>Prediction 3</italic>) and Figure 3—figure supplement 7 in the revised paper. We have also expanded the eighth Discussion paragraph, relating to the interaction between attention type and agent type, to address this point.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>1) Please report and analyze the univariate activity in the ROIs</p><p>2) More fully report (in main Results section) and discuss the searchlight results.</p></disp-quote><p>We thank the editor and two reviewers for their detailed and constructive comments. In the revised submission, we report the univariate activity averaged across all voxels within each ROI, and report and discuss the Searchlight results in the main text. In our view the paper has been greatly improved as a result of these revisions. Below we answer all comments in a point-by-point manner.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The authors have addressed many of my concerns. In particular, the additional TPJ ROIs and the demonstration of self-other cross-decoding increased my confidence in the main finding of attention state decoding in lTPJ.</p><p>One comment has not been sufficiently addressed, however: it would be highly relevant and informative to see the average univariate activity for each of the conditions in the 6 original ROIs, plotted in a graph. The endo&gt;exo contrast should be tested, correcting for the 6 ROIs, similar to how the decoding accuracies are tested (i.e., with the same p&lt;0.05 cut-off). This ROI-based univariate analysis is also interesting for other comparisons, e.g. to test whether the social conditions gave higher activity than the non-social condition, as would be expected based on previous work. Thus, please provide a full univariate analysis on average ROI activity.</p></disp-quote><p>We thank the reviewer for this suggestion. In the revised submission, we now include a supplementary figure showing the difference in average univariate activity across all voxels within each ROI, for the endo-vs-exo, self-vs-other, and social-vs-nonsocial contrasts. In line with the voxelwise univariate results reported in Supplementary file 5, none of these contrasts were statistically significant (all FDR-corrected ps &gt; 0.21).</p><disp-quote content-type="editor-comment"><p>There is a lot of interesting information in the Supplementary figures (e.g., searchlight, additional ROIs), which will not be visible when viewing or printing pdfs. You could consider moving some of this to the main text, or add panels to existing figures where you include some of this information.</p></disp-quote><p>We agree with the reviewer, and have incorporated some of the supplementary results into the main text in the revised submission. Specifically, we have added three panels to Figure 3, which now features the additional left TPJ ROI results, the endo-versus-exo Searchlight results, and the cross-classification results.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>In this revised manuscript, the authors have greatly clarified some key points, most important of which was how well matched the behavior was across all of the conditions. While there is still a potential confound in the social vs. non-social contrast given the difference in reaction time, (a) the confound effects are likely negligible in size, (b) the end result matches the previous literature, and (c) the contrast was only a control analysis and doesn't greatly affect the main point of the paper. However, the handling of the left posterior STS result in the searchlight analysis remains problematic, both in the manuscript and in the author's response.</p><p>pSTS response and discussion: The main point of the manuscript is to examine whether known theory of mind areas contain information that differentiates between the endogenous vs. exogenous story conditions. The revised manuscript makes quite clear that the answer is yes, activity in the L TPJ differentiates between the two conditions, albeit with a relatively low decoding accuracy. The fact that multiple versions of the L TPJ ROI produces the same result is particularly reassuring. The spotlight analysis, however, finds that there is potentially more discriminatory information to be found in the posterior STS/middle temporal gyrus, and this is where the manuscript runs into problems.</p><p>In the revised manuscript, this result is simply not mentioned, though it clearly has implications for the interpretations of the result as mentioned in the previous round of reviews. In addition, in the response to the reviewers' comments, the authors give a very convoluted and confusing argument based on MNI coordinates that somehow this focus is somehow (a) part of the TPJ and (b) may be a part of the attention-related TPJa as opposed to their TPJp ROI. In the process of saying that the surface projections are misleading, they state that the focus as falling on the STG on the surface projection, when it is really on the MTG. In addition, they base their arguments on MNI coordinates, but MNI coordinates are notoriously inaccurate in comparing results from different studies, so the author's attempts to justify their conclusions using these coordinates falls flat. The authors own volume images very clearly show the focus as being in the STS and not on either the angular gyrus or supramarginal gyrus that Mars, Corbetta, and others have generally shown the TPJp and TPJa to fall on. And even if this focus was in the TPJa, as the authors seem to hint at, they do not discuss the implications of this result.</p><p>The authors' defensiveness around this point is frankly puzzling. The searchlight results do not seem to invalidate their main point, only potentially augment it. The fact that the posterior STS (and what seem to be the right FEF and iPCS areas) exhibit higher discriminability between the endo and exo conditions may not be shocking given that the theory of mind areas are likely involved in many operations in this task, whereas the pSTS and right hemisphere areas likely may only be involved in just the imagined attention/sensory processing aspects of the task. It is possible that these areas are &quot;reading out&quot; the differing attentional conditions from the L TPJ. Whatever the explanation may be, the authors seem to be trying to bury this result to shoehorn the results into fitting their a priori model, which is a disservice and misleading to the readers, and needs to be rectified before the manuscript can be published. My recommendation is to at least mention the searchlight results in the main Results section, then add a paragraph to the Discussion discussing the implications of these results. Better yet would be to quantify the discrimination accuracy within each of the foci uncovered in the spotlight analyses.</p></disp-quote><p>We are sorry to hear that the reviewer interpreted our previous response as defensive, it was not our intention. And we apologize for erroneously referring to STG in our response (“they state that the focus as falling on the STG on the surface projection”), it should have said MTG, which is obvious from the surface projection in Figure 3—figure supplement 3. In the revised submission, we now include the searchlight results in the main Results section and in the main results Figure 3 (panel C).</p><p>First, we would like to take the opportunity put the searchlight result into a broader context. The searchlight analysis is fundamentally different from the ROI analysis. It is not targeted to specific brain areas on the basis of strong predictions. Instead, it is a whole-brain analysis that is much more statistically conservative because of the brain-wide correction for multiple comparisons. In general, one would not necessarily expect the searchlight analysis to align perfectly with the ROI analysis. Its usefulness is that it may reveal any cluster of very strong decoding that was missed by the more sensitive analysis restricted to the ROIs. However, the results of our searchlight analysis revealed no brain-wide significant clusters of decoding for the endogenous versus exogenous distinction. Thus, we cannot make any inferences about attention-type decoding based on the searchlight results. However, in a purely descriptive manner, we report the (four) decoding clusters that survived the conventional uncorrected statistical threshold p&lt;0.001. It is interesting to note that the brain-wide searchlight decoding peak happened to fall in the posterior STS just 20 mm from the center of our predefined left TPJ ROI, and that they belong to the same anatomical subregion of the TPJ (TPJp). In our view, this approximate, descriptive overlap lends at least some additional confidence to our significant left TPJ ROI decoding result. Nevertheless, since the searchlight result consists of an uncorrected decoding map, we have been very careful to not over-interpret the data. We suspect that the reviewer may have taken this caution on our part as defensiveness. Because the searchlight results for the endo-versus-exo distinction were inconclusive, interpretations such as the one offered by the reviewer (“The fact that the posterior STS [and what seem to be the right FEF and iPCS areas] exhibit higher discriminability between the endo and exo conditions may not be shocking given that the theory of mind areas are likely involved in many operations in this task, whereas the pSTS and right hemisphere areas likely may only be involved in just the imagined attention/sensory processing aspects of the task. It is possible that these areas are &quot;reading out&quot; the differing attentional conditions from the L TPJ.”) are in our view therefore highly speculative.</p><p>The goal with our previous response was to make an honest attempt at pinpointing the functional location the left pSTS searchlight cluster, which was prompted by the reviewer’s previous comment that questioned whether the cluster belong to the TPJ at all and speculated that it perhaps belongs to TPJa or coincides with the gaze-following patch reported in Marquardt et al. 2017. In contrast to the reviewer’s assertion that the TPJa and TPJp encompass only the angular gyrus and the SMG, the connectivity-based subdivision of TPJa and TPJp described by Bzdok et al. (2013) and Mars et al. (2012) both define the pSTS as part of the TPJ. According to this definition, our pSTS searchlight cluster clearly belongs to the TPJ and specifically to the posterior subregion TPJp. The reviewer appears to have the impression that we are arguing the cluster belongs to the TPJa (“the authors [argue that] this focus is somehow (a) part of the TPJ and (b) may be a part of the attention-related TPJa as opposed to their TPJp ROI.” and “And even if this focus was in the TPJa, as the authors seem to hint at […]”); however, we are not. Furthermore, the pSTS searchlight cluster does not coincide with the gaze-following patch reported in Marquardt et al. 2017.</p><p>To help clarify the relationship between the left TPJ ROI, in which we obtained significant decoding, and the left STS, in which we found a non-significant peak in the searchlight analysis, we performed a further analysis for the reviewer. In this new analysis, we used a more lax statistical threshold, to better examine whether there is any halo of activity hidden under the statistical threshold. Instead of using the arbitrary uncorrected threshold of p&lt;0.001, we used the uncorrected threshold of p&lt;0.01, in the searchlight analysis. <xref ref-type="fig" rid="respfig1">Author response image 1</xref> shows the result. Using this threshold, the left posterior STS cluster merged with clusters encompassing both the angular gyrus, the SMG and the MTG. The general area of activity is now more clearly in the TPJ, including the TPJ ROI and extending into the STS. It is important to keep in mind, however, that all of these methods – the method that shows a small peak in the STS, and the method that shows a large cluster encompassing the TPJ – should not be taken as definite, statistically strong findings. They are the result of exploratory searchlight analysis and, though interesting, should be taken cautiously.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63551-resp-fig1-v1.tif"/></fig><p>We certainly want to avoid the impression of “burying” the searchlight results. Although descriptive, they are interesting and compatible with our ROI results, and constitute valuable information not least for future studies. Therefore, we now include the endo-versus-exo searchlight results in the updated main results figure (Figure 3C) and report the results, including the decoding accuracy for the pSTS peak voxel (53.7%), in the main Results section under “Prediction 1”.</p></body></sub-article></article>