<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86327</article-id><article-id pub-id-type="doi">10.7554/eLife.86327</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Gaze patterns and brain activations in humans and marmosets in the Frith-Happé theory-of-mind animation task</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-287096"><name><surname>Dureux</surname><given-names>Audrey</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1687-8347</contrib-id><email>audrey.dureux@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-284050"><name><surname>Zanini</surname><given-names>Alessandro</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-304760"><name><surname>Selvanayagam</surname><given-names>Janahan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3708-8742</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-197999"><name><surname>Menon</surname><given-names>Ravi S</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-2445"><name><surname>Everling</surname><given-names>Stefan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9714-9757</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Centre for Functional and Metabolic Mapping, Robarts Research Institute, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02grkyz14</institution-id><institution>Department of Physiology and Pharmacology, University of Western Ontario</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Irish</surname><given-names>Muireann</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0384j8v12</institution-id><institution>University of Sydney</institution></institution-wrap><country>Australia</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>14</day><month>07</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e86327</elocation-id><history><date date-type="received" iso-8601-date="2023-01-20"><day>20</day><month>01</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2023-07-13"><day>13</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2023-01-18"><day>18</day><month>01</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.16.524238"/></event></pub-history><permissions><copyright-statement>© 2023, Dureux et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Dureux et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86327-v3.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86327-figures-v3.pdf"/><abstract><p>Theory of Mind (ToM) refers to the cognitive ability to attribute mental states to other individuals. This ability extends even to the attribution of mental states to animations featuring simple geometric shapes, such as the Frith-Happé animations in which two triangles move either purposelessly (Random condition), exhibit purely physical movement (Goal-directed condition), or move as if one triangle is reacting to the other triangle’s mental states (ToM condition). While this capacity in humans has been thoroughly established, research on nonhuman primates has yielded inconsistent results. This study explored how marmosets (<italic>Callithrix jacchus</italic>), a highly social primate species, process Frith-Happé animations by examining gaze patterns and brain activations of marmosets and humans as they observed these animations. We revealed that both marmosets and humans exhibited longer fixations on one of the triangles in ToM animations, compared to other conditions. However, we did not observe the same pattern of longer overall fixation duration on the ToM animations in marmosets as identified in humans. Furthermore, our findings reveal that both species activated extensive and comparable brain networks when viewing ToM versus Random animations, suggesting that marmosets differentiate between these scenarios similarly to humans. While marmosets did not mimic human overall fixation patterns, their gaze behavior and neural activations indicate a distinction between ToM and non-ToM scenarios. This study expands our understanding of nonhuman primate cognitive abilities, shedding light on potential similarities and differences in ToM processing between marmosets and humans.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>In our daily life, we often guess what other people are thinking or intending to do, based on their actions. This ability to ascribe thoughts, intentions or feelings to others is known as Theory of Mind.</p><p>While we often use our Theory of Mind to understand other humans and interpret social interactions, we can also apply our Theory of Mind to assign feelings and thoughts to animals and even inanimate objects. For example, people watching a movie where the characters are represented by simple shapes, such as triangles, can still see a story unfold, because they infer the triangles’ intentions based on what they see on the screen.</p><p>While it is clear that humans have a Theory of Mind, how the brain manages this capacity and whether other species have similar abilities remain open questions. Dureux et al. used animations showing abstract shapes engaging in social interactions and advanced brain imaging techniques to compare how humans and marmosets – a type of monkey that is very social and engages in shared childcare – interpret social cues. By comparing the eye movements and brain activity of marmosets to human responses, Dureux et al. wanted to uncover common strategies used by both species to understand social signals, and gain insight into how these strategies have evolved.</p><p>Dureux et al. found that, like humans, marmosets seem to perceive a difference between shapes interacting socially and moving randomly. Not only did their gaze linger longer on certain shapes in the social scenario, but their brain activity also mirrored that of humans viewing the same scenes. This suggests that, like humans, marmosets possess an inherent ability to interpret social scenarios, even when they are presented in an abstract form, providing a fresh perspective on primates’ abilities to interpret social cues.</p><p>The findings of Dureux et al. have broad implications for our understanding of human social behavior and could lead to the development of better communication strategies, especially for individuals social cognitive conditions, such as Autism Spectrum Disorder. However, further research will be needed to understand the neural processes underpinning the interpretation of social interactions. Dureux et al.’s research indicates that the marmoset monkey may be the ideal organism to perform this research on.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Theory of Mind</kwd><kwd>Frith-Happé animations</kwd><kwd>fMRI</kwd><kwd>eye tracking</kwd><kwd>marmoset monkeys</kwd><kwd><italic>Callithrix jacchus</italic></kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000024</institution-id><institution>Canadian Institutes of Health Research</institution></institution-wrap></funding-source><award-id>FRN 148365</award-id><principal-award-recipient><name><surname>Everling</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010785</institution-id><institution>Canada First Research Excellence Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Everling</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><award-id>Discovery grant</award-id><principal-award-recipient><name><surname>Everling</surname><given-names>Stefan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Shared traits in gaze patterns and brain activations between marmosets and humans during Theory of Mind animations reveal cross-species cognitive similarities.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Theory of Mind (ToM) refers to the capacity to ascribe mental states to other subjects (<xref ref-type="bibr" rid="bib10">Carruthers and Smith, 1996</xref>; <xref ref-type="bibr" rid="bib38">Premack and Woodruff, 1978</xref>). Various experimental approaches have been devised to investigate the cognitive processes involved in ToM, including tasks involving text (<xref ref-type="bibr" rid="bib25">Happé, 1994</xref>), non-verbal pictures (<xref ref-type="bibr" rid="bib40">Sarfati et al., 1997</xref>), false belief (<xref ref-type="bibr" rid="bib49">Wimmer and Perner, 1983</xref>), and silent animations featuring geometric shapes. The latter approach is based on Heider and Simmel’s observation that participants attribute intentional actions, human character traits, and even mental states to moving abstract shapes (<xref ref-type="bibr" rid="bib26">Heider and Simmel, 1944</xref>). Subsequent studies used these animations to test the ability to ascribe mental states in autistic children (<xref ref-type="bibr" rid="bib6">Bowler and Thommen, 2000</xref>; <xref ref-type="bibr" rid="bib31">Klin, 2000</xref>).</p><p>In the Frith-Happé animations, a large red triangle and a small blue triangle move around the screen (<xref ref-type="bibr" rid="bib1">Abell et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Castelli et al., 2002</xref>; <xref ref-type="bibr" rid="bib11">Castelli et al., 2000</xref>). In the Random condition, the two triangles do not interact and move purposelessly, in the Goal-Directed (GD) condition the triangles interact but in a purely physical manner (i.e. chase, dancing, fighting and leading) and in the ToM condition the two animated triangles move as if one triangle is reacting to the other’s mental state (i.e. coaxing, surprising, seducing and mocking). Functional imaging studies have demonstrated that the observation of ToM compared to Random animations activates brain regions typically associated with social cognition, including dorso-medial frontal, temporoparietal, inferior and superior temporal cortical regions (<xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Bliksted et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Castelli et al., 2000</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib23">Gobbini et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Vandewouw et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Weiss et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Wheatley et al., 2007</xref>).</p><p>Although the spontaneous attribution of mental states to moving shapes has been well established in humans, it remains uncertain whether other primate species share this capacity. There is some evidence suggesting that monkeys can attribute goals to agents with varying levels of similarity and familiarity to conspecifics, including human agents, monkey robots, moving geometric boxes, animated shapes, and simple moving dots (<xref ref-type="bibr" rid="bib3">Atsumi et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Atsumi and Nagasaka, 2015</xref>; <xref ref-type="bibr" rid="bib32">Krupenye and Hare, 2018</xref>; <xref ref-type="bibr" rid="bib33">Kupferberg et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Uller, 2004</xref>). However, the findings in this area are somewhat mixed, with some studies investigating the attribution of goals to inanimate moving objects yielding inconclusive results (<xref ref-type="bibr" rid="bib2">Atsumi and Nagasaka, 2015</xref>; <xref ref-type="bibr" rid="bib33">Kupferberg et al., 2013</xref>). Nonhuman primates' spontaneous attribution of mental states to Frith-Happé animations is even less certain. While human subjects exhibit longer eye fixations when viewing the ToM condition compared to the Random condition of the Frith-Happé animations (<xref ref-type="bibr" rid="bib30">Klein et al., 2009</xref>), a recent eye tracking study in macaque monkeys did not observe similar differences (<xref ref-type="bibr" rid="bib41">Schafroth et al., 2021</xref>). Similarly, a recent fMRI study conducted on macaques found no discernible differences in activations between ToM and random Frith-Happé animations (<xref ref-type="bibr" rid="bib39">Roumazeilles et al., 2021</xref>).</p><p>In this study, we investigated the behaviour and brain activations of New World common marmoset monkeys (<italic>Callithrix jacchus</italic>) while they viewed Frith-Happé animations. Living in closely-knit family groups, marmosets exhibit significant social parallels with humans, including prosocial behavior, imitation, and cooperative breeding. These characteristics establish them as a promising nonhuman primate model for investigating social cognition (<xref ref-type="bibr" rid="bib7">Burkart et al., 2009</xref>; <xref ref-type="bibr" rid="bib8">Burkart and Finkenwirth, 2015</xref>; <xref ref-type="bibr" rid="bib37">Miller et al., 2016</xref>). To directly compare humans and marmosets in their response to these animations, we employed high-speed video eye-tracking to record eye movements in eleven healthy humans and eleven marmoset monkeys. Additionally, we conducted ultra-high field fMRI scans on ten healthy humans at 7T and six common marmoset monkeys at 9.4T. These combined methods allowed us to examine the visual behavior and brain activations of both species while they observed the Frith-Happé animations.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To investigate whether marmoset monkeys, like humans, exhibit distinct processing patterns in response to the conditions in Frith-Happé animations (i.e. ToM, GD, and Random conditions), we compared gaze patterns and fMRI activations in both marmosets and human subjects as they watched shortened versions of the Frith-Happé animations (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task Design.</title><p>Two different conditions of video clips resulting in eight animations were used during the scanning (ToM and Random animations), and an additional condition with four animations was used for the eye-tracking (ToM, GD and Random animations). In the ToM animations, one triangle reacted to the other triangle’s mental state, whereas in the Random animations the same two triangles did not interact with each other. In the GD animations, the two triangles interact with simple intentions. Each animation video lasted 19.5 s and was separated by baseline blocks of 15 s where a central dot was displayed in the center of the screen. In the fMRI task, several runs were used with a Randomized order of the two conditions whereas in the eye-tracking task one run containing all the twelve animations once was used.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig1-v3.tif"/></fig><sec id="s2-1"><title>Gaze patterns for Frith-Happé’s ToM, GD and Random animations in humans and marmosets</title><p>We first investigated in both humans and marmosets whether fixation durations differed between the three conditions (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). By conducting mixed analyses of variance (ANOVA), with factors of species (Human vs Marmoset) and condition (ToM vs GD vs Random animation videos), we found a significant interaction between species and condition (<italic>F</italic><sub>(2,40)</sub>=13.9, p=&lt;0.001, <italic>η<sub>p</sub><sup>2</sup></italic>p2.410). Here we observed longer fixation durations for ToM animation videos (<italic>M</italic>=432.6ms) as compared to GD videos (M=279.9ms, p=0.008) and Random videos (<italic>M</italic>=308.2ms, p=0.01) for humans (p=0.029) but not for marmosets (233.7ms for ToM videos, 219.6ms for GD videos and 235.6ms for Random videos, ToM vs GD: p=0.90 and ToM vs Random: p=1).This finding confirms that humans fixate longer in the ToM condition (<xref ref-type="bibr" rid="bib30">Klein et al., 2009</xref>), whereas marmosets, like macaques (<xref ref-type="bibr" rid="bib41">Schafroth et al., 2021</xref>), do not show this effect.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Fixation duration (A) and proportion of time looking triangles (B) in Frith-Happé’s ToM, GD and Random animations in humans (left) and marmosets (right).</title><p>(<bold>A</bold>). Bar plot depicting the fixation duration in the screen as a function of each condition. (<bold>B</bold>). Bar plot representing the proportion of time the radial distance between the current gaze position and each triangle was within 4 visual degrees, as a function of each condition. Green represents results obtained for ToM animation videos, orange represents results for GD animation videos and blue represents results for Random animation videos. In each graph, the left panel shows the results for 11 humans and the right panel for 11 marmosets. Each colored bar represents the group mean and the vertical bars represent the standard error from the mean. The differences between conditions were tested using ANOVA: p&lt;0.05*, p&lt;0.01** and p&lt;0.001***.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig2-v3.tif"/></fig><p>To further analyze the gaze patterns of both humans and marmosets, we next measured the proportion of time subjects looked at each of the triangles in the videos (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We conducted mixed ANOVAs on the proportion of time the radial distance between the current gaze position and each triangle was within 4 visual degrees for each triangle separately.</p><p>Importantly, we observed a significant interaction between species and condition for the proportion of time spent looking at the large red triangle (<italic>F</italic><sub>(2,40)</sub>=9.83, p&lt;0.001, <italic>η<sub>p</sub><sup>2</sup></italic>p2.330). Specifically, both humans (<xref ref-type="fig" rid="fig2">Figure 2B</xref> left) and marmosets (<xref ref-type="fig" rid="fig2">Figure 2B</xref> right) spent a greater proportion of time looking at the red triangle in ToM compared to the GD and Random videos (for humans, ToM vs GD: <italic>Δ</italic>=.23, p&lt;0.001 and ToM vs Random: <italic>Δ</italic>=0.31, p&lt;0.001; for marmosets, ToM vs GD: <italic>Δ</italic>=0.13, p&lt;0.01 and ToM vs Random: <italic>Δ</italic>=0.13, p&lt;0.01). However, while humans also allocated a greater proportion of time to the red triangle in GD compared to Random animations (<italic>Δ</italic>=0.08, p=0.05), marmosets did not show any difference between these two conditions (<italic>Δ</italic>=0.0003, p=1).</p><p>For the small blue triangle, we also observed a significant interaction of species and condition (<italic>F</italic><sub>(2,40)</sub>=3.54, p=0.04, <italic>η<sub>p</sub><sup>2</sup></italic>p2.151) but no significant pairwise differences were observed following Bonferroni correction. Therefore, humans and marmosets spent the same proportion of time looking at the blue triangle in the three different types of videos (for humans, ToM vs GD: <italic>Δ</italic>=-0.02, p=1, ToM vs Random: <italic>Δ</italic>=0.04, p=1 and GD vs Random: <italic>Δ</italic>=0.07, p=0.23; for marmosets, ToM vs GD: <italic>Δ</italic>=-0.05, p=0.89, ToM vs Random: <italic>Δ</italic>=0.07, p=0.66 and GD vs Random: <italic>Δ</italic>=-0.02, p=1; <xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>These results highlight the variation in gaze patterns observed in both humans and marmosets when their focus is directed towards the large red triangle during the viewing of ToM, GD, and Random videos. Notably, humans show a gradient of proportion of time spent looking at the red triangle across the three conditions, with the smallest proportion in Random videos and the greatest proportion in ToM videos. In contrast, marmosets exhibit a different pattern, spending more time looking at the red triangle in ToM videos, but allocating the same proportion of time to look at the red triangle in both Random and GD videos. This finding suggests that while humans demonstrate distinct attentional preferences for the red triangle across the three conditions, marmosets exhibit a similar attentional focus on the red triangle in the Random and GD conditions, but their pattern differs in the ToM condition. This suggests that marmosets process the Random and GD conditions in a similar manner, but their processing of the ToM condition is distinct, indicating a differential response to stimuli representing social interactions.</p></sec><sec id="s2-2"><title>Functional brain activations while watching ToM and Random Frith-Happé’s animations in humans</title><p>Given that humans exhibited only minor differences, and marmosets showed no differences in eye movements between the Random and GD animations, coupled with task design constraints, we only used the Random and ToM animations for the fMRI studies in both humans and marmosets (see Materials and methods).</p><p>We first investigated ToM and Random animations processing in humans. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows group activation maps for ToM (A) and Random (B) conditions as well as the comparison between ToM and Random conditions (C) obtained for human participants.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Brain networks involved in processing of Frith-Happé’s ToM and Random animations in humans.</title><p>Group functional maps displayed on right fiducial (lateral and medial views) and left and right fiducial (dorsal and ventral views) of human cortical surfaces showing significant greater activations for ToM condition (<bold>A</bold>), Random condition (<bold>B</bold>) and the comparison between ToM and Random conditions (<bold>C</bold>). The white line delineates the regions based on the recent multi-modal cortical parcellation atlas (<xref ref-type="bibr" rid="bib22">Glasser et al., 2016</xref>). The maps depicted are obtained from 10 human subjects with an activation threshold corresponding to z-scores &gt;2.57 for regions with yellow/red scale or z-scores &lt;–2.57 for regions with purple/green scale (AFNI’s 3dttest++, cluster-forming threshold of p&lt;0.01 uncorrected and then FWE-corrected <italic>α</italic>=0.05 at cluster-level from 10000 Monte-Carlo simulations).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Brain networks involved in ToM animations processing in humans.</title><p>Group functional maps showing significant greater activations for the comparison between ToM animations and Random animations displayed on the left and right fiducial human cortical surfaces (lateral and medial views) as well as on coronal slices, to illustrate the activations in subcortical areas. The white line delineates the regions based on the recent multi-modal cortical parcellation atlas (<xref ref-type="bibr" rid="bib22">Glasser et al., 2016</xref>). (<bold>A</bold>) The map depicted is obtained from 10 human subjects with an activation threshold corresponding to z-scores&gt;2.57 (AFNI’s 3dttest++, cluster-forming threshold of p&lt;0.01 uncorrected and then FWE-corrected <italic>α</italic>=0.05 at cluster-level from 10000 Monte-Carlo simulations). The subcortical maps correspond to an activation threshold of z-scores&gt;3.29 (AFNI’s 3dttest++, threshold of p&lt;0.001 uncorrected). (<bold>B</bold>) The map depicted has been downloaded from <ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.image">https://identifiers.org/neurovault.image</ext-link>:3179 and is described in the study of <xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>. The brain areas reported have activation threshold corresponding to z-scores&gt;6, uncorrected.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig3-figsupp1-v3.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Image and temporal signal-to-noise-ration (SNR) calculated on fMRI data acquired at 7T with an AC-84 Mark II gradient coil, an in-house 8-channel transmit, and a 32-channel receive coil (see methods, main text).</title><p>(<bold>A</bold>). Image SNR maps from gradient-echo-images obtained from <xref ref-type="bibr" rid="bib20">Gilbert et al., 2021</xref>. (<bold>B</bold>) Temporal SNR (i.e. ratio of the mean signal to the standard deviation through the time course) for EPI BOLD images obtained from one run of one participant. The mean tSNR calculated within peripheral brain regions nearest the coil elements is 10% higher for right hemisphere than tSNR produced by left hemisphere.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig3-figsupp2-v3.tif"/></fig></fig-group><p>Both ToM (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) and Random (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) videos activated a large bilateral network. While the same larger areas were activated in both conditions, the specific voxels showing this activation within those areas were typically distinct. In some cases, both conditions activated the same voxels, but the degree of activation differed. This suggests a degree of both spatial and intensity variation in the activations for the two conditions within the same areas. The activated areas included visual areas (V1, V2, V3, V3CD, V3B, V4, V4T, V6A, V7, MT, MST), lateral occipital areas 1, 2, and 3 (LO1, LO2, LO3), temporal areas (FST, PH, PHT, TE2, posterior inferotemporal complex PIT and fusiform face complex FFC), temporo-parietal junction areas (TPOJ2 and TPOJ3), lateral posterior parietal areas also comprising the parietal operculum (supramarginal areas PF, PFt, angular areas PGp and PGi, superior temporal visual area STV, perisylvian language area PSL, medial intraparietal area MIP, ventral and dorsal lateral intraparietal areas LIPv and LIPd, anterior intraparietal area AIP, IPS1, IPS0, 7PC and 5 L), medial superior parietal areas (7am, PCV, 5 mv), secondary somatosensory cortex (S2), premotor areas (6, 55b, premotor eye field PEF, frontal eye field FEF), and frontal areas (8Av, 8 C, IFJp, IFIa).</p><p>The ToM condition (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) also showed bilateral activations in posterior superior temporal sulcus (STSdp), in temporo-parietal junction area TPOJ1, in ventral visual complex (VVC), in parahippocampal area 3 (PHA3), in lateral posterior parietal areas Pfop and PFcm, in lateral prefrontal areas 8 C, 8Av, 44 and 45, in inferior frontal areas IFSp, IFSa, and in frontal opercular area 5 (FOP5).</p><p>To identify brain areas that are more active during the observation of ToM compared to Random videos, we directly compared the two conditions (i.e., ToM animations &gt;Random animations contrast, <xref ref-type="fig" rid="fig3">Figure 3C</xref> and Figure 5A). This analysis reveals increased activations for the ToM condition compared to the Random condition in occipital, temporal, parietal and frontal areas. This includes notable differences in the bilateral visual areas V1, V2, V3, V3CD, V4, V4t, MT, MST, as well as the bilateral LO1, LO2 and LO3 regions. The increase extends into the lateral temporal lobe, as observed in the bilateral PH and FST areas and into the more inferior part of the temporal lobe in the bilateral TE2, FFC, and PIT areas. We also found greater activations in the bilateral temporo-parietal junction areas (TPOJ1, TPOJ2, TPOJ3), and along the right STS in STSdp and STSda areas. This extended to left and right parietal areas, especially in the inferior parietal lobule, including the right supramarginal and opercular supramarginal areas (PF, PFm, PFt, Pfop, and PFcm), left PFt, bilateral opercular areas PSL and STV, bilateral angular areas PGp and Pgi, right IPS1, and bilateral IP0. The activation also extended into the superior parietal lobule (right AIP). Moving anteriorly, we observed greater activations during ToM animations in the secondary somatosensory cortex, premotor areas (6 r and PEF), lateral prefrontal areas (8 C, 44, and 45), and the inferior (IFSa, IFSp, IFJa, and IFJp) as well as the opercular (FOP5) frontal areas in the right hemisphere. In contrast, the Random condition exhibited greater activations, than the ToM condition predominantly within the left and right visual areas (V1, V2, V3, V3A, V4) and in dorsolateral (10d and 10r bilateral), lateral (9m left) and medial frontal areas (d32 and a24 bilateral, p24 and s32 right).</p><p>At the subcortical level (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, left panel), we observed enhanced bilateral activations in the cerebellum and in certain areas of the thalamus (namely, the right ventroposterior thalamus or THA-VP, and the left and right dorsoanterior thalamus or THA-DA) under both ToM (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>, left panel) and Random conditions (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>, left panel) when compared to the baseline. Additionally, a small section of the right amygdala was engaged in the ToM condition. We noted more pronounced activations in the posterior lobe of the cerebellum, the right amygdala and thalamus (right THA-VP, right ventroanterior thalamus or THA-VA, and left and right dorsoposterior thalamus or THA-DP) for the ToM condition compared to the Random condition (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, left panel). No regions showed greater activations for Random condition compared to the ToM condition.</p><p>As we used shorter modified versions of the Frith-Happé animations (i.e. videos of 19.5 s instead of 40 s), we also validated our stimuli and our fMRI protocol by comparing the brain responses elicited by ToM animation videos – compared to Random animation videos – obtained in our group of 10 human subjects and those reported by the large group of humans (496) used in the social cognition task of the Human Connectome Project (HCP; <xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>), which also used shortened versions of the Frith-Happé animations.</p><p>This comparison is shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Overall, we observed similar distinct patterns of brain activations (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A and B</xref>), including a set of areas in occipital, temporal, parietal and frontal cortices, as described previously (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The main differences were stronger activations in the left hemisphere in the HCP dataset. Therefore, these results show that our stimuli and our protocol are appropriate to investigate mental state attribution to animated moving shapes.</p></sec><sec id="s2-3"><title>Functional brain activations while watching ToM and Random Frith-Happé’s animations in marmosets</title><p>Having identified the brain regions activated during the processing of ToM or Random videos in human subjects and validated our protocol, we proceeded to use the same stimuli in marmosets. <xref ref-type="fig" rid="fig4">Figure 4</xref> illustrates the brain network obtained for the ToM condition (A), Random condition (B), and the contrast between ToM and Random conditions (C) in six marmosets.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Brain networks involved in processing of Frith-Happé’s ToM and Random animations in marmosets.</title><p>Group functional maps showing significant greater activations for ToM condition (<bold>A</bold>), Random condition (<bold>B</bold>) and the comparison between ToM and Random conditions (<bold>C</bold>). Group map obtained from six marmosets displayed on lateral and medial views of the right fiducial marmoset cortical surfaces as well as dorsal and ventral views of left and right fiducial marmoset cortical surfaces. The white line delineates the regions based on the Paxinos parcellation of the NIH marmoset brain atlas (<xref ref-type="bibr" rid="bib34">Liu et al., 2018</xref>). The brain areas reported have activation threshold corresponding to z-scores &gt;2.57 (yellow/red scale) or z-scores &lt;–2.57 (purple/green scale) (AFNI’s 3dttest++, cluster-forming threshold of p&lt;0.01 uncorrected and then FWE-corrected <italic>α</italic>=0.05 at cluster-level from 10,000 Monte-Carlo simulations).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig4-v3.tif"/></fig><p>Both the ToM (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and Random (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) animations activated an extensive network involving a variety of areas in the occipito-temporal, parietal and frontal regions. As in human subjects, it should be noted that while both conditions elicited strong activation in some of the same larger areas, these activations might have either occurred in distinct voxels within those areas, or the same voxels were activated to varying degrees for both conditions. This suggests distinct yet overlapping patterns of neural processing for the ToM and Random conditions.</p><p>In the occipital and temporal cortex, the activations were located in the visual areas V1, V2, V3, V3A, V4, V4t, V5, V6, MST, the medial (19 M), and dorsointermediate parts (19DI) of area 19, ventral temporal area TH, enthorinal cortex, and lateral and inferior temporal areas TE3 and TEO. Activations were also observed in the posterior parietal cortex, specifically in bilateral regions surrounding the intraparietal sulcus (IPS), in areas LIP, MIP, PE, PG, PFG, PF, V6A, PEC, in the occipito-parietal transitional area (OPt) and in medial part of the parietal cortex (area PGM). More anteriorly, bilateral activations were present in areas 1/2, 3a, 3b of the somatosensory cortex, in primary motor area 4 parts a, b and c (area 4ab and 4c), in area 6 ventral part (6Va) of the premotor cortex and in frontal areas 45 and 8Av.</p><p>The ToM condition (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) also recruited bilateral activations in areas V5, TE2, FST, Pga-IPa, temporoparietal transitional area (TPt), around the IPS in AIP and VIP, in the internal part (S2I), parietal rostral part (S2PR) and ventral part (S2PV) of the secondary somatosensory cortex, in agranular insular cortex (AI), granular and dysgranular insular areas (GI and DI), retroinsular area (ReI) and orbital periallocortex (OPAI), as well as in premotor cortex in area 8 caudal part (8C), in area 6 dorsocaudal and dorsorostral parts (6DC, 6DR). Additionally, we also observed activations in posterior cingulate areas 23a, 23b, 29d, 30, 24d, and 24b.</p><p>Next, we examined the difference between ToM and Random animations (i.e. ToM condition &gt;Random condition contrast, <xref ref-type="fig" rid="fig4">Figure 4C</xref> and <xref ref-type="fig" rid="fig5">Figure 5B</xref>). We found enhanced bilateral activations for the ToM condition across a range of regions. These encompassed occipital areas V1, V2, V3, V3A, V4, V4t, V5, V6, 19DI, 19M, temporal areas TH, TE2, TE3, FST, MST, TPt, and parietal areas LIP, MIP, VIP, AIP, PE, PG, PFG, OPt, V6A, PEC. Moreover, these activations extended to the somatosensory cortex (areas 1/2, 3 a, 3b, S2I, S2PV), the primary motor cortex (areas 4ab and 4c), lateral frontal areas 6DC, 8C, 6Va, 8Av, 8Ad (left hemisphere), and insular areas (ReI, S2I, S2PV, DI, AI). Additional activations were observed in the OPAI area, medial frontal area 32 and posterior cingulate areas (23 a, 23b, 29d, 30). Contrarily, we did not find any regions exhibiting stronger activations for the Random condition compared to the ToM condition. This further emphasizes the distinctive neural recruitment and processing associated with ToM animations within the marmoset brain.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Brain network involved during processing of ToM compared to Random Frith-Happé’s animations in both humans (<bold>A</bold>) and marmosets (<bold>B</bold>).</title><p>Group functional maps showing significant greater activations for ToM animations compared to Random animations. (<bold>A</bold>) Group map obtained from 10 human subjects displayed on the left and right human cortical flat maps. The white line delineates the regions based on the recent multi-modal cortical parcellation atlas (<xref ref-type="bibr" rid="bib22">Glasser et al., 2016</xref>). (<bold>B</bold>) Group map obtained from 6 marmosets displayed on the left and right marmoset cortical flat maps. The white line delineates the regions based on the Paxinos parcellation of the NIH marmoset brain atlas (<xref ref-type="bibr" rid="bib34">Liu et al., 2018</xref>). The brain areas reported in A and B have activation threshold corresponding to z-scores &gt;2.57 (yellow/red scale) or z-scores &lt;–2.57 (purple/green scale) (AFNI’s 3dttest++, cluster-forming threshold of p&lt;0.01 uncorrected and then FWE-corrected <italic>α</italic>=0.05 at cluster-level from 10,000 Monte-Carlo simulations).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Subcortical activations during processing of Frith-Happé’s ToM and Random animations in humans (left) and marmosets (right).</title><p>Group subcortical functional maps showing significant greater activations for ToM condition (<bold>A</bold>), Random condition (<bold>B</bold>) and the comparison between ToM and Random conditions (<bold>C</bold>). Group maps displayed on coronal slices obtained from ten humans (left side) and 6 marmosets (right side). The brain areas reported have activation threshold corresponding to z-scores&gt;3.29 (AFNI’s 3dttest++, threshold of p&lt;0.001 uncorrected). <italic>CeB, cerebellum; THA-VP, ventroposterior thalamus; THA-DA, dorsoanterior thalamus; THA-VA, ventroanterior thalamus; Amyg, amygdala; Hipp, hippocampus; Pul, pulvinar; SC, superior colliculus; LGN, lateral geniculate nucleus</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86327-fig5-figsupp1-v3.tif"/></fig></fig-group><p>At the subcortical level (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>, right panel), the ToM condition showed involvement of several areas including the bilateral hippocampus, bilateral pulvinar (lateral, medial and inferior parts), bilateral amygdala, and left caudate. On the other hand, the Random condition recruited only the pulvinar (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>, right panel). Upon comparison of the ToM and Random conditions, the ToM animations showed stronger activations in the right superior colliculus (SC), right lateral geniculate nucleus (LGN), left caudate, left amygdala, left hippocampus and certain portions of the right and left pulvinar (lateral and inferior pulvinar; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, right panel).</p></sec><sec id="s2-4"><title>Comparison of functional brain activations in humans and marmosets</title><p>As described earlier, both humans (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and marmosets (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) displayed an extended network of activations across the occipital, temporal, parietal, and frontal cortices in response to ToM animations compared to Random animations. Overall, there were substantial similarities between the two species, with both exhibiting enhanced activations for ToM animations compared to Random animations within visual areas, inferior and superior temporal areas, the inferior parietal lobe, and AIP area encircling the IPS in the superior parietal lobe. We also found parallel activations in the somatosensory cortex, although the activation was more widespread in marmosets compared to humans, where it was confined to the secondary somatosensory cortex. Additional similarities were identified in the premotor cortex and certain regions of the lateral prefrontal cortex. Overall, left and right hemisphere activations demonstrated greater congruity in marmosets compared to humans. However, this might be attributed to our human head coil, which had a lower signal-to-noise ratio (SNR) in the left hemisphere (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Indeed, similar bilateral activations in humans have been observed in the human HCP dataset (<xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>).</p><p>Nevertheless, there were also discernible differences between the two species for ToM compared to Random animations, including stronger activations in medial frontal cortex, primary motor area and posterior cingulate cortex for marmosets, which were absent in our human sample. Moreover, different parts of the insular cortex were recruited in marmosets, whereas in humans, activations were limited to the parietal operculum and did not extend into the insula. At the subcortical level, although both humans and marmosets demonstrated activations in the amygdala, humans recruited the dorsal thalamus and the cerebellum, whereas marmosets displayed activations in the hippocampus, the SC and the LGN.</p><p>These results indicate that, while there were many shared brain activation patterns in both humans and marmosets during the processing of ToM animations compared to Random animations, several notable species-specific differences were also evident.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the present study, we investigated whether New-World common marmoset monkeys, like humans, process videos of animated abstract shapes differently when these shapes appear to be reacting to each other (ToM condition) compared to when they interact in a purely physical manner (GD condition) or when they move purposelessly (Random condition). To facilitate a direct comparative analysis between the two primate species, we measured their gaze patterns and brain activations as they viewed the widely-used Frith-Happé’s animations (<xref ref-type="bibr" rid="bib1">Abell et al., 2000</xref>; <xref ref-type="bibr" rid="bib11">Castelli et al., 2000</xref>). In these animations, the ToM condition is characterized by one triangle reacting to the other’s mental state, exemplifying behaviors like coaxing, surprising, seducing, and mocking. In the GD condition, the two triangles appear to engage purely physically without any implied mental attribution, depicting behaviors such as chasing, dancing, fighting, and leading. In the Random condition, the two triangles move independently, depicting motions akin to a game of billiards, drifting movements, a star pattern, or a tennis game. In all these animations, the physical interaction of the triangles does appear to follow the laws of physics in a reasonably predictable manner. This is probably most evident in the random ‘billiard’ condition in which the two triangles bounce off the walls. However, the ToM animations also follow Newton’s third law, for example when the small triangle is trying to get inside the box and bounces against it in the ‘seducing’ condition, or when the large triangle pushes the small triangle in the ‘coaxing’ condition.</p><p>In our first experiment, we examined the gaze patterns of marmosets and humans during the viewing of these video animations. <xref ref-type="bibr" rid="bib30">Klein et al., 2009</xref> reported differing fixation durations for these animations, where the longest fixations were observed for ToM animations, followed by GD animations and the shortest fixations for Random animations. They further reported that the intentionality score - derived from verbal descriptions of the animations - followed a similar pattern: highest for ToM, lowest for Random, and intermediate for GD animations. This validated the degree of mental state attribution according to the categories and established that animations provoking mentalizing (ToM condition) were associated with long fixations. This, in turn, supports the use of fixation durations as a nonverbal metric for mentalizing capacity (<xref ref-type="bibr" rid="bib30">Klein et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Meijering et al., 2012</xref>). Our results with human subjects, which demonstrated longer fixation durations for the ToM animations compared to the GD and Random animations, paralleled those of <xref ref-type="bibr" rid="bib30">Klein et al., 2009</xref>. However, unlike Klein et al.’s findings, we did not observe intermediate durations for GD animations in our study.</p><p>Interestingly, our marmoset data did not align with the human findings but instead resonated more with <xref ref-type="bibr" rid="bib41">Schafroth et al., 2021</xref>’s observations in macaque monkeys, which did not show significant differences in fixation durations across the three animation types.</p><p>Our study went a step further than previous research in humans (<xref ref-type="bibr" rid="bib30">Klein et al., 2009</xref>) and macaques (<xref ref-type="bibr" rid="bib41">Schafroth et al., 2021</xref>) by investigating the proportion of time that subjects devoted to looking at the two central figures in the animations: the large red triangle and the small blue triangle. Our results indicate that both humans and marmosets spent significantly more time looking at the large red triangle during ToM, compared to GD and Random animations. Humans also exhibited a preference for the red triangle in the GD over the Random condition, a differentiation not evident in marmosets. This result suggests that marmosets process these conditions similarly, indicating that, unlike humans, they do not seem to discern a marked difference between purposeless and goal-directed motions. However, they did show a distinctive gaze pattern in the ToM condition, pointing to their capacity to potentially perceive or react to animated sequences with complex mental interactions. Our findings revealed no significant differences in gaze patterns towards the smaller blue triangle across the three conditions in both humans and marmosets, potentially due to the perception of the large red triangle as a more salient or socially relevant figure in the interactions.</p><p>Together, the observed gaze patterns do not support the idea that marmosets increase their cognitive processing during ToM animations in the same way as humans. However, the findings point to a certain level of sophistication in the marmosets' perception of abstract ToM animations.</p><p>Thus, in our second experiment, we investigated the brain networks involved when viewing the ToM and Random Frith-Happé’s animations in humans and marmosets. Previous fMRI studies in humans identified a specific network associated with ToM processing in tasks such as stories, humorous cartoons, false-belief tasks, and social gambling. This network typically includes areas such as the medial frontal gyrus, posterior cingulate cortex, inferior parietal cortex, and temporoparietal junction (<xref ref-type="bibr" rid="bib18">Fletcher et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Gallagher et al., 2000</xref>). Nevertheless, these studies used complex stimuli and yielded heterogeneous results, with varied activations in regions such as the medial and lateral prefrontal cortex, inferior parietal lobule, occipital cortex, and insula across different studies (<xref ref-type="bibr" rid="bib9">Carrington and Bailey, 2009</xref>). This variability is likely due to the diverse experimental paradigms employed to study ToM. Studies employing Frith-Happé’s animations, which are less complex and more controlled, reported distinct patterns of brain activation when viewing ToM compared to Random animations, involving areas such as the dorso-medial prefrontal cortex, inferior parietal cortex, temporoparietal, inferior and superior temporal regions, and lateral superior occipital regions (<xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Bliksted et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Castelli et al., 2000</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib23">Gobbini et al., 2007</xref>; <xref ref-type="bibr" rid="bib45">Vandewouw et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Weiss et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Wheatley et al., 2007</xref>).</p><p>Our slightly adapted versions of the Frith-Happé animations led to a similar distinct pattern of brain activations, with an exception for the lack of activations in the dorsal part of the medial prefrontal cortex. This discrepancy could be attributable to various factors, including differences in task design, methodological aspects such as statistical power, or variations in participants characteristics. Otherwise, our results and those of the HCP data from <xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>, revealed stronger activations for ToM versus Random animations in areas in premotor, prefrontal, parietal, visual, inferior and superior temporal cortices including the STS and temporoparietal junction. Overall, the ToM network we identified, as well as that reported by <xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>, appear to be more extensive than those described in studies employing more complex experimental paradigms to study ToM. This aligns with the recent meta-analysis conducted by <xref ref-type="bibr" rid="bib42">Schurz et al., 2021</xref>, which demonstrated that the network activated by simpler, non-verbal stimuli like social animations differs from the traditional network, with involvement of both cognitive and affective networks (<xref ref-type="bibr" rid="bib42">Schurz et al., 2021</xref>).</p><p>As in humans, the comparison of responses to ToM and Random animations in marmosets revealed activations in occipito-temporal, parietal, and frontal regions. Specifically, activations in the TE areas in marmosets could be equivalent to those observed along the STS in humans (<xref ref-type="bibr" rid="bib50">Yovel and Freiwald, 2013</xref>). We also observed in both our human subjects and marmosets, activations in the inferior parietal cortex, previously reported in human literature (<xref ref-type="bibr" rid="bib9">Carrington and Bailey, 2009</xref>; <xref ref-type="bibr" rid="bib18">Fletcher et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Gallagher et al., 2000</xref>). We also found the similar activations in the superior parietal cortex in both our human and marmoset subjects, specifically in the area surrounding the IPS, but this have not been predominantly described in previous work (<xref ref-type="bibr" rid="bib9">Carrington and Bailey, 2009</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib18">Fletcher et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Gallagher et al., 2000</xref>; <xref ref-type="bibr" rid="bib23">Gobbini et al., 2007</xref>). However, there are also noteworthy differences between our results and those of our human data. Firstly, while we did not observe activations in the medial prefrontal cortex in humans, they were present in marmosets, aligning with previous human fMRI studies (<xref ref-type="bibr" rid="bib5">Bliksted et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">Castelli et al., 2000</xref>; <xref ref-type="bibr" rid="bib47">Weiss et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Wheatley et al., 2007</xref>). The marmoset network also included the posterior cingulate cortex and the insula, areas known to be involved in mentalizing and affective processing respectively in human ToM studies that employed more complex stimuli (<xref ref-type="bibr" rid="bib18">Fletcher et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Gallagher et al., 2000</xref>; <xref ref-type="bibr" rid="bib48">Wheatley et al., 2007</xref>). Finally, a prominent difference between humans and marmosets is the strong activation in the marmoset motor cortex for ToM animations, which was absent in humans, in addition to the differences observed at the subcortical level. Interestingly, we have also recently reported activations in marmoset primary motor cortex during the observation of social interactions (<xref ref-type="bibr" rid="bib14">Cléry et al., 2021</xref>), suggesting a potential role for the marmoset motor cortex in interaction observation. Regarding the distinct subcortical activations observed in humans and marmosets, it’s important to consider the specific social cognitive demands that might be unique to each species. The involvement of the dorsal thalamus, cerebellum, and a small portion of the amygdala in humans may reflect the complexities of information processing, social cognition, and emotional involvement required to interpret the ToM animations (e.g. <xref ref-type="bibr" rid="bib24">Halassa and Sherman, 2019</xref>; <xref ref-type="bibr" rid="bib28">Janak and Tye, 2015</xref>; <xref ref-type="bibr" rid="bib46">Van Overwalle et al., 2014</xref>). Conversely, the activation of the amygdala and hippocampus in marmosets could suggest a more emotion- and memory-based processing of the social stimuli (e.g. <xref ref-type="bibr" rid="bib17">Eichenbaum, 2017</xref>; <xref ref-type="bibr" rid="bib46">Van Overwalle et al., 2014</xref>). However, it’s critical to consider that these interpretations are speculative and would require further study for confirmation.</p><p>Together, these findings demonstrate that marmosets, while observing interacting animated shapes as opposed to randomly moving shapes, exhibit enhanced activation in several brain regions previously associated with ToM processing in humans.</p><p>Interestingly, our results differed from those obtained by <xref ref-type="bibr" rid="bib39">Roumazeilles et al., 2021</xref> in their fMRI study conducted in macaques using the same animations. Roumazeilles and colleagues reported no differences in activation between ToM and Random animations, suggesting that rhesus macaques may not respond to the social cues presented by the ToM Frith-Happé animations. This disparity between our marmoset findings and those of macaques raises intriguing questions about potential differences in the evolutionary development of ToM processing within non-human primates. Marmosets, as New World monkeys, are part of an evolutionary lineage that diverged earlier than the lineage of Old-World monkeys such as macaques. This difference in lineage might lead to distinct evolutionary trajectories in cognitive processing, which could include varying sensitivity to abstract social cues in animations.</p><p>In summary, our study reveals novel insights into how New World marmosets, akin to humans, differentially process abstract animations that depict complex social interactions and animations that display purely physical or random movements. Our findings, supported by both specific gaze behaviors (i.e. the proportion of time spent on the red triangle, despite the inconclusiveness of overall fixation) and distinct neural activation patterns, shed light on the marmosets' capacity to interpret social cues embedded in these animations.</p><p>The differences observed between humans, marmosets, and macaques underscore the diverse cognitive strategies that primate species have evolved to decipher social information. This diversity may be influenced by unique evolutionary pressures that arise from varying social structures and lifestyles. Like macaque monkeys, humans often live in large, hierarchically organized social groups where status influences access to resources. However, both humans and marmosets share a common trait: a high degree of cooperative care for offspring within the group, with individuals other than the biological parents participating in child-rearing. These distinctive social dynamics of marmosets and humans may have driven the development of unique social cognitive abilities. This could explain their enhanced sensitivity to abstract social cues in the Frith-Happé animations.</p><p>Nonetheless, it is crucial to emphasize that even though marmosets respond to the social cues in the Frith-Happé animations, this does not automatically imply that they possess mental-state attributions comparable to humans. As such, future research including a range of tasks, from sensory-affective components to more abstract and decoupled representations of others' mental states (<xref ref-type="bibr" rid="bib42">Schurz et al., 2021</xref>), will be fundamental in further unravelling the complexities of the evolution and functioning of the Theory of Mind across the primate lineage.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Common marmosets</title><p>All experimental procedures were in accordance with the Canadian Council of Animal Care policy and a protocol approved by the Animal Care Committee of the University of Western Ontario Council on Animal Care #2021–111.</p><p>Eleven adult marmosets (4 females, 32–57 months, mean age: 36.6 months) were subjects in this study. All animals were implanted for head-fixed experiments with either a fixation chamber (<xref ref-type="bibr" rid="bib29">Johnston et al., 2018</xref>) or a head post (<xref ref-type="bibr" rid="bib21">Gilbert et al., 2023</xref>) under anesthesia and aseptic conditions. Briefly, the animals were placed in a stereotactic frame (Narishige, model SR-6C-HT) while being maintained under gas anaesthesia with a mixture of O<sub>2</sub> and air (isoflurane 0.5–3%). After a midline incision of the skin along the skull, the skull surface was prepared by applying two coats of an adhesive resin (All-Bond Universal; Bisco, Schaumburg, IL) using a microbrush, air-dried, and cured with an ultraviolet dental curing light (King Dental). Then, the head post or fixation chamber was positioned on the skull and maintained in place using a resin composite (Core-Flo DC Lite; Bisco). Heart rate, oxygen saturation, and body temperature were continuously monitored during this procedure.</p><p>Six of these animals (four females - weight 315–442 g, age 30–34 months - and two males - weight 374–425 g, age 30 and 55 months) were implanted with an MRI-compatible machined PEEK head post (<xref ref-type="bibr" rid="bib21">Gilbert et al., 2023</xref>). Two weeks after the surgery, these marmosets were acclimatized to the head-fixation system in a mock MRI environment.</p></sec><sec id="s4-2"><title>Human participants</title><p>Eleven healthy humans (4 females, 25–42 years, mean age: 30.7 years) participated in the eye tracking experiment. Among these, five individuals, along with five additional subjects (4 females, 26–45 years), took part in the fMRI experiment. All subjects self-reported as right-handed, had normal or corrected-to-normal vision and had no history of neurological or psychiatric disorders. Importantly, all subjects confirmed they had not previously been exposed to the Frith-Happé animation videos used in our study. Subjects were informed about the experimental procedures and provided informed written consent. These studies were approved by the Ethics Committee of the University of Western Ontario.</p></sec><sec id="s4-3"><title>Stimuli</title><p>Eight animations featuring simple geometric shapes with distinct movement patterns were used (<xref ref-type="fig" rid="fig1">Figure 1</xref>). These animations, originally developed by Abell and colleagues (<xref ref-type="bibr" rid="bib1">Abell et al., 2000</xref>), presented two animated triangles - a large red triangle and a small blue triangle - moving within a framed white background. The original social animation task included three conditions: ToM, Goal-Directed (GD), and Random. In the ToM animations, one triangle displayed behaviors indicative of mental interactions by reacting to the mental state of the other triangle. The GD animations depicted simple interactions between the two triangles, while the Random animations showed the triangles moving and bouncing independently.</p><p>The ToM animations portrayed various scenarios, such as one triangle attempting to seduce (<xref ref-type="video" rid="video1">Video 1</xref>) or persuade the other mocking it behind its back (<xref ref-type="video" rid="video2">Video 2</xref>), surprising it by hiding behind a door (<xref ref-type="video" rid="video3">Video 3</xref>), or coaxing it out of an enclosure (<xref ref-type="video" rid="video4">Video 4</xref>). In the GD animations, the triangles could dance together (<xref ref-type="video" rid="video5">Video 5</xref>), fight together (<xref ref-type="video" rid="video6">Video 6</xref>), or one triangle could chase (<xref ref-type="video" rid="video7">Video 7</xref>) or lead the other (<xref ref-type="video" rid="video8">Video 8</xref>). The Random animations featured independent movements of the triangles, following patterns such as billiard (<xref ref-type="video" rid="video9">Video 9</xref>), drifting (<xref ref-type="video" rid="video10">Video 10</xref>), star (<xref ref-type="video" rid="video11">Video 11</xref>), or tennis (<xref ref-type="video" rid="video12">Video 12</xref>). Similar to the approach used in the HCP study (<xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>), we modified the original video clips and shortened each animation to 19.5 s using custom video-editing software (iMovie, Apple Incorporated, CA).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Theory of Mind (ToM)' Category, Frith-Happe Animations – Seducing Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Theory of Mind (ToM)' Category, Frith-Happe Animations – Mocking Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Theory of Mind (ToM)' Category, Frith-Happe Animations – Surprise Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Theory of Mind (ToM)' Category, Frith-Happe Animations – Coaxing Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video5.mp4" id="video5"><label>Video 5.</label><caption><title>Goal-Directed (GD)' Category, Frith-Happe Animations – Dancing Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video6.mp4" id="video6"><label>Video 6.</label><caption><title>Goal-Directed (GD)' Category, Frith-Happe Animations – Fighting Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video7.mp4" id="video7"><label>Video 7.</label><caption><title>Goal-Directed (GD)' Category, Frith-Happe Animations – Chase Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video8.mp4" id="video8"><label>Video 8.</label><caption><title>Goal-Directed (GD)' Category, Frith-Happe Animations – Leading Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video9.mp4" id="video9"><label>Video 9.</label><caption><title>Random' Category, Frith-Happé Animations – Billiard Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video10.mp4" id="video10"><label>Video 10.</label><caption><title>Random' Category, Frith-Happé Animations – Drifting Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video11.mp4" id="video11"><label>Video 11.</label><caption><title>Random' Category, Frith-Happé Animations – Star Simulation.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86327-video12.mp4" id="video12"><label>Video 12.</label><caption><title>Random' Category, Frith-Happé Animations – Tennis Simulation.</title></caption></media></sec><sec id="s4-4"><title>Eye tracking task and data acquisition</title><p>To investigate potential behavioral differences during the viewing of Frith-Happé animations, we presented all ToM, GD and Random video clips once each in a pseudorandomized manner to both marmoset and human subjects. The presentation of stimuli was controlled using Monkeylogic software (<xref ref-type="bibr" rid="bib27">Hwang et al., 2019</xref>). All stimuli were presented on a CRT monitor (ViewSonic Optiquest Q115, 76 Hz non-interlaced, 1600 x 1280 resolution). Eye position was digitally recorded at 1 kHz via video tracking of the left pupil (EyeLink 1000, SR Research, Ottawa, ON, Canada).</p><p>At the beginning of each session, horizontal and vertical eye positions of the left eye were calibrated by presenting a 1 degree dot at the display centre and at 6 degrees in each of the cardinal directions for 300–600ms. Monkeys were rewarded at the beginning and end of each session. Crucially, no rewards were provided during the calibration or while the videos were played.</p></sec><sec id="s4-5"><title>fMRI task</title><p>For the fMRI experiment, it was crucial for us to ensure that the subjects remained alert and focused throughout the entire scanning session, which becomes increasingly difficult with longer runs. There, we used only the ToM and Random conditions in our functional runs, as the GD condition is situated between these two extremes, depicting physical interaction among the triangles without suggesting any mental state attribution. The limitation to ToM and random conditions is consistent with the design of previous fMRI studies in humans and macaques that employed Frith-Happé animations (<xref ref-type="bibr" rid="bib23">Gobbini et al., 2007</xref>; <xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Bliksted et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Vandewouw et al., 2021</xref>; <xref ref-type="bibr" rid="bib47">Weiss et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib39">Roumazeilles et al., 2021</xref>).</p><p>Humans and marmosets were presented with ToM and Random video clips in a block design. Each run consisted of eight blocks of stimuli (19.5 s each) interleaved by baseline blocks (15 s each). ToM or Random animations were presented pseudorandomly, and each condition was repeated four times (<xref ref-type="fig" rid="fig1">Figure 1</xref>). For each run, the order of these conditions was randomized leading to 14 different stimulus sets, counterbalanced within and between subjects. In baseline blocks, a 0.36° circular black cue was displayed at the center of the screen against a gray background. We found previously that such a stimulus reduced the vestibulo-ocular reflex evoked by the strong magnetic field.</p></sec><sec id="s4-6"><title>fMRI experimental setup</title><p>During the scanning sessions, the marmosets sat in a sphinx position in a custom-designed plastic chair positioned within a horizontal magnet (see below). Their head was restrained using a head fixation system allowing to secure the surgically implanted head post to a clamping bar (<xref ref-type="bibr" rid="bib21">Gilbert et al., 2023</xref>). After the head was immobilized, the two halves of the coil housing were positioned on either side of the head. Inside the scanner, monkeys faced a translucent screen placed 119 cm from their eyes where visual stimuli were projected with an LCSD-projector (Model VLP-FE40, Sony Corporation, Tokyo, Japan) via a back-reflection on a first surface mirror. Visual stimuli were presented with the Keynote software (version 12.0, Apple Incorporated, CA) and were synchronized with MRI TTL pulses triggered by a Raspberry Pi (model 3B+, Raspberry Pi Foundation, Cambridge, UK) running via a custom-written Python program. No reward was provided to the monkeys during the scanning sessions. Animals were monitored using an MRI-compatible camera (Model 12M-I, MRC Systems GmbH). Horizontal and vertical eye movements were monitored at 60 Hz using a video eye tracker (ISCAN, Boston, Massachusetts). While we were able to obtain relatively stable eye movement recordings from a few runs per animal (min 1, max 5 runs per animal), the quality of the recordings was not sufficient for a thorough analysis. The large marmoset pupil represents a challenge for video eye tracking when the eyes are not fully open. Data from functional runs with more stable eye signals (n=15) show good compliance in the marmosets. The percentage of time spent in each run looking at the screen in the two experimental conditions (ToM, Random) and during the Baseline periods (fixation point in the center of the screen) was higher than 85% (88.2%, 88.6% and 93.4% respectively for ToM, Random and Baseline conditions). There was no significant differences between the ToM and Random condition (paired t-test, t<sub>(14)</sub>=-0.374, p=0.71), ruling out the possibility that any differences in fMRI activation between the ToM and Random condition were simply due to a different exposure to the videos.</p><p>Human subjects lay in a supine position and watched the stimuli presented via a rear projection system (Avotech SV-6011, Avotec Incorporated) through a surface mirror affixed to head coil. As for marmosets, visual stimuli were presented with the Keynote software (version 12.0, Apple Incorporated, CA) and were synchronized with MRI TTL pulses triggered by a Raspberry Pi (model 3B+, Raspberry Pi Foundation, Cambridge, UK) running via a custom-written python program.</p></sec><sec id="s4-7"><title>MRI data acquisition</title><p>Marmoset and human imaging were performed at the Center for Functional and Metabolic Mapping at the University of Western Ontario.</p><p>For marmoset subjects, fMRI data were acquired on a 9.4T 31 cm horizontal bore magnet (Varian) with a Bruker BioSpec Avance III HD console running software package Paravision-360 (Bruker BioSpin Corp), a custom-built high-performance 15 cm diameter gradient coil (maximum gradient strength: 1.5 mT/m/A), and an eight-channel receive coil. Preamplifiers were located behind the animals, and the receive coil was placed inside an in-house built quadrature birdcage coil (12 cm inner diameter) used for transmission. Functional images were acquired during 6 functional runs for each animal using gradient-echo based single-shot echo-planar images (EPI) sequence with the following parameters: TR = 1.5 s, TE = 15ms, flip angle = 40°, field of view = 64 × 48 mm, matrix size = 96 × 128, resolution of 0.5 mm3 isotropic, number of slices = 42 [axial], bandwidth = 400 kHz, GRAPPA acceleration factor: 2 (left-right). Another set of EPIs with an opposite phase-encoding direction (right-left) was collected for the EPI-distortion correction. A T2-weighted structural was also acquired for each animal during one of the sessions with the following parameters: TR = 7 s, TE = 52ms, field of view = 51.2 × 51.2 mm, resolution of 0.133x0.133 × 0.5 mm, number of slices = 45 [axial], bandwidth = 50 kHz, GRAPPA acceleration factor: 2.</p><p>For human subjects, fMRI data were acquired on a 7T 68 cm MRI scanner (Siemens Magnetom 7T MRI Plus) with an AC-84 Mark II gradient coil, an in-house 8-channel parallel transmit, and a 32-channel receive coil (<xref ref-type="bibr" rid="bib20">Gilbert et al., 2021</xref>). Functional images were acquired during 3 functional runs for each participant using Multi-Band EPI BOLD sequences with the following parameters: TR = 1.5 s, TE = 20ms, flip angle = 30°, field of view = 208 × 208 mm, matrix size = 104 × 104, resolution of 2 mm3 isotropic, number of slices = 62, GRAPPA acceleration factor: 3 (anterior-posterior), multi-band acceleration factor: 2. Field map images were also computed from the magnitude image and the two phase images. An MP2RAGE structural image was also acquired for each subject during the sessions with the following parameters: TR = 6 s, TE = 2.13ms, TI1 /TI2=800 / 2700ms, field of view = 240 × 240 mm, matrix size = 320 × 320, resolution of 0.75 mm3 isotropic, number of slices = 45, GRAPPA acceleration factor (anterior posterior): 3.</p></sec><sec id="s4-8"><title>MRI data preprocessing</title><p>Marmoset fMRI data were preprocessed using AFNI (<xref ref-type="bibr" rid="bib15">Cox, 1996</xref>) and FSL (<xref ref-type="bibr" rid="bib43">Smith et al., 2004</xref>) software packages. Raw MRI images were first converted to NIfTI format using dcm2nixx AFNI’s function and then reoriented to the sphinx position using fslswapdim and fslorient FSL’s functions. Functional images were despiked using 3Ddespike AFNI’s function and time shifted using 3dTshift AFNI’s function. Then, the images obtained were registered to the base volume (i.e., corresponding to the middle volume of each time series) with 3dvolreg AFNI’s function. The output motion parameters obtained from volume registration were later used as nuisance regressors. All fMRI images were spatially smoothed with a 1.5 mm half-maximum Gaussian kernel (FWHM) with 3dmerge AFNI’s function, followed by temporal filtering (0.01–0.1 Hz) using 3dBandpass AFNI’s function. The mean functional image was calculated for each run and linearly registered to the respective anatomical image of each animal using FMRIB’s linear registration tool (FLIRT).</p><p>The transformation matrix obtained after the registration was then used to transform the 4D time series data. The brain was manually skull-stripped from individual anatomical images using FSL eyes tool and the mask of each animal was applied to the functional images. Finally, the individual anatomical images were linearly registered to the NIH marmoset brain template (<xref ref-type="bibr" rid="bib34">Liu et al., 2018</xref>) using Advanced Normalization Tools (ANTs).</p><p>Human fMRI data were preprocessed using SPM12 (Wellcome Department of Cognitive Neurology). After converting raw images into NifTI format, functional images were realigned to correct for head movements and underwent slice timing correction. A field map correction was applied to the functional images from the magnitude and phase images with the specify toolbox implemented in SPM. Then, the anatomical and functional volumes corrected were coregistered with the MP2RAGE structural scan from each individual participant and normalized to the Montreal Neurological Institute (MNI) standard brain space. Anatomical images were segmented into white matter, gray matter, and CSF partitions and also normalized to the MNI space. The functional images were then spatially smoothed with a 6 mm FWHM isotropic Gaussian kernel. A high-pass filter (128 s) was also applied to the time series.</p></sec><sec id="s4-9"><title>Statistical analysis</title><sec id="s4-9-1"><title>Behavioral eye tracking data</title><p>To evaluate gaze patterns during observation of ToM and Random videos, we used mixed analyses of variance (ANOVA), with factors of species (Human vs Marmoset) and condition (ToM vs Random videos) on the overall fixation duration and on the proportion of time when the radial distance between the subject’s gaze position and each triangle was less than 4 degrees. Partial eta squared (<italic>η<sub>p</sub><sup>2</sup></italic>) was computed as a measure of effect size and <italic>post-hoc</italic> comparisons were Bonferroni corrected.</p></sec><sec id="s4-9-2"><title>fMRI data</title><p>For each run, a general linear regression model was defined: the task timing was convolved to the hemodynamic response (AFNI’s ‘BLOCK’ convolution for marmosets’ data and SPM12 hemodynamic response function for humans’ data) and a regressor was generated for each condition (AFNI’s 3dDeconvolve function for marmosets and SPM12 function for humans). The two conditions were entered into the same model, corresponding to the 19.5 s presentation of the stimuli, along with polynomial detrending regressors and the marmosets’ motions parameters or human’s head movement parameters estimated during realignment.</p><p>The resultant regression coefficient maps of marmosets were then registered to template space using the transformation matrices obtained with the registration of anatomical images on the template (see MRI data processing part above).</p><p>Finally, we obtained for each run in marmosets and humans, two T-value maps registered to the NIH marmoset brain atlas (<xref ref-type="bibr" rid="bib34">Liu et al., 2018</xref>) and to the MNI brain standard space, respectively.</p><p>These maps were then compared at the group level via paired t-tests using AFNI’s 3dttest ++function, resulting in Z-value maps. To protect against false positives and to control for multiple comparisons, we adopted a clustering method derived from 10000 Monte Carlo simulations to the resultant z-test maps using ClustSim option (<italic>α</italic>=0.05). This method corresponds to performing cluster-forming threshold of p&lt;0.01 uncorrected and then applying a family-wise error (FWE) correction of p&lt;0.05 at the cluster-level.</p><p>We used the Paxinos parcellation of the NIH marmoset brain atlas (<xref ref-type="bibr" rid="bib34">Liu et al., 2018</xref>) and the most recent multi-modal cortical parcellation atlas (<xref ref-type="bibr" rid="bib22">Glasser et al., 2016</xref>) to define anatomical locations of cortical and subcortical regions for both marmosets and humans respectively.</p><p>First, we identified brain regions involved in the processing of ToM and Random animations by contrasting each condition with a baseline (i.e. ToM condition &gt;baseline and Random condition &gt;baseline contrasts). This baseline brain activation recorded during the presentation of the circular black cue between video clips (i.e. baseline blocks of 15 s, see above), reflects 'resting state' activation. By comparing it to the brain activation during ToM and Random animations, we could specifically highlight the task-related activations and isolate brain regions engaged during each condition. Subsequently, we then determined the clusters that displayed significantly greater activation for the ToM animations compared to the Random animations (ToM condition &gt;Random condition contrast), and vice versa. The resultant Z-value maps were displayed on fiducial maps obtained from the Connectome Workbench (v1.5.0 [<xref ref-type="bibr" rid="bib35">Marcus et al., 2011</xref>]) using the NIH marmoset brain template (<xref ref-type="bibr" rid="bib34">Liu et al., 2018</xref>) for marmosets and the MNI Glasser brain template (<xref ref-type="bibr" rid="bib22">Glasser et al., 2016</xref>) for humans. Subcortical activations were displayed on coronal sections.</p><p>As we used shortened video clips (i.e. 19.5 s compared to the 40 s originally designed by <xref ref-type="bibr" rid="bib1">Abell et al., 2000</xref>), we validated our fMRI protocol by confirming that our shorter videos elicited similar responses to those previously observed in the HCP (<xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>), whichalso used modified versions of these animation videos. We compared our ToM vs Random Z-value map obtained in human subjects with those of the HCP (<xref ref-type="bibr" rid="bib4">Barch et al., 2013</xref>). To this end, we downloaded the Z-value map of activations for ToM animations compared to Random animations from 496 subjects from the Neurovalt site (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.image">https://identifiers.org/neurovault.image</ext-link>:3179). We displayed the resultant Z-value maps on fiducial maps obtained from the Connectome Workbench (v1.5.0, [<xref ref-type="bibr" rid="bib35">Marcus et al., 2011</xref>]) using the MNI Glasser brain template (<xref ref-type="bibr" rid="bib22">Glasser et al., 2016</xref>).</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This study was approved by the Ethics Committee of the University of Western Ontario and subjects were informed about the experimental procedures and provided informed written consent.</p></fn><fn fn-type="other"><p>All experimental methods described were performed in accordance with the guidelines of the Canadian Council of Animal Care policy and a protocol approved by the Animal Care Committee of the University of Western Ontario Council on Animal Care (#2021-111). Animals were monitoring during the acquisition sessions by a veterinary technician.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86327-mdarchecklist1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All fMRI and eye tracking data generated and analysed as well as the scripts used have been deposited in Github and the link has been provided in the manuscript. Here the link: <ext-link ext-link-type="uri" xlink:href="https://github.com/audreydureux/Theory-of-mind_Human_Marmosets_Paper">https://github.com/audreydureux/Theory-of-mind_Human_Marmosets_Paper</ext-link> (copy archived at <xref ref-type="bibr" rid="bib16">Dureux, 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>Support was provided by the Canadian Institutes of Health Research (FRN 148365), the Canada First Research Excellence Fund to BrainsCAN, and a Discovery grant by the Natural Sciences and Engineering Research Council of Canada. We are grateful to Drs. Sarah White and Uta Frith for access to the Frith-Happé animation videos. We also wish to thank Cheryl Vander Tuin, Whitney Froese, Hannah Pettypiece, and Miranda Bellyou for animal preparation and care, Dr. Alex Li and Trevor Szekeres for scanning assistance, Dr. Kyle Gilbert and Peter Zeman for coil designs.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abell</surname><given-names>F</given-names></name><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Do triangles play tricks? Attribution of mental states to animated shapes in normal and abnormal development</article-title><source>Cognitive Development</source><volume>15</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/S0885-2014(00)00014-9</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atsumi</surname><given-names>T</given-names></name><name><surname>Nagasaka</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Perception of chasing in squirrel monkeys (Saimiri sciureus)</article-title><source>Animal Cognition</source><volume>18</volume><fpage>1243</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1007/s10071-015-0893-x</pub-id><pub-id pub-id-type="pmid">26156787</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atsumi</surname><given-names>T</given-names></name><name><surname>Koda</surname><given-names>H</given-names></name><name><surname>Masataka</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Goal attribution to inanimate moving objects by Japanese macaques (Macaca fuscata)</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>40033</elocation-id><pub-id pub-id-type="doi">10.1038/srep40033</pub-id><pub-id pub-id-type="pmid">28053305</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Burgess</surname><given-names>GC</given-names></name><name><surname>Harms</surname><given-names>MP</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Curtiss</surname><given-names>S</given-names></name><name><surname>Dixit</surname><given-names>S</given-names></name><name><surname>Feldt</surname><given-names>C</given-names></name><name><surname>Nolan</surname><given-names>D</given-names></name><name><surname>Bryant</surname><given-names>E</given-names></name><name><surname>Hartley</surname><given-names>T</given-names></name><name><surname>Footer</surname><given-names>O</given-names></name><name><surname>Bjork</surname><given-names>JM</given-names></name><name><surname>Poldrack</surname><given-names>R</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>Function in the human connectome: task-fMRI and individual differences in behavior</article-title><source>NeuroImage</source><volume>80</volume><fpage>169</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.033</pub-id><pub-id pub-id-type="pmid">23684877</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bliksted</surname><given-names>V</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name><name><surname>Videbech</surname><given-names>P</given-names></name><name><surname>Fagerlund</surname><given-names>B</given-names></name><name><surname>Emborg</surname><given-names>C</given-names></name><name><surname>Simonsen</surname><given-names>A</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Campbell-Meiklejohn</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hyper- and hypomentalizing in patients with first-episode schizophrenia: fMRI and behavioral studies</article-title><source>Schizophrenia Bulletin</source><volume>45</volume><fpage>377</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1093/schbul/sby027</pub-id><pub-id pub-id-type="pmid">29534245</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowler</surname><given-names>DM</given-names></name><name><surname>Thommen</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attribution of mechanical and social causality to animated displays by children with autism</article-title><source>Autism</source><volume>4</volume><fpage>147</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1177/1362361300004002004</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkart</surname><given-names>JM</given-names></name><name><surname>Hrdy</surname><given-names>SB</given-names></name><name><surname>Van Schaik</surname><given-names>CP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Cooperative breeding and human cognitive evolution</article-title><source>Evolutionary Anthropology</source><volume>18</volume><fpage>175</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1002/evan.20222</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkart</surname><given-names>JM</given-names></name><name><surname>Finkenwirth</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Marmosets as model species in neuroscience and evolutionary anthropology</article-title><source>Neuroscience Research</source><volume>93</volume><fpage>8</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2014.09.003</pub-id><pub-id pub-id-type="pmid">25242577</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrington</surname><given-names>SJ</given-names></name><name><surname>Bailey</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Are there theory of mind regions in the brain? A review of the neuroimaging literature</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>2313</fpage><lpage>2335</lpage><pub-id pub-id-type="doi">10.1002/hbm.20671</pub-id><pub-id pub-id-type="pmid">19034900</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carruthers</surname><given-names>P</given-names></name><name><surname>Smith</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="1996">1996</year><source>Theories of Theories of Mind</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511597985</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelli</surname><given-names>F</given-names></name><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Movement and mind: a functional imaging study of perception and interpretation of complex intentional movement patterns</article-title><source>NeuroImage</source><volume>12</volume><fpage>314</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0612</pub-id><pub-id pub-id-type="pmid">10944414</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelli</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Autism, Asperger syndrome and brain mechanisms for the attribution of mental states to animated shapes</article-title><source>Brain</source><volume>125</volume><fpage>1839</fpage><lpage>1849</lpage><pub-id pub-id-type="doi">10.1093/brain/awf189</pub-id><pub-id pub-id-type="pmid">12135974</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Gau</surname><given-names>SSF</given-names></name><name><surname>Wu</surname><given-names>YY</given-names></name><name><surname>Chou</surname><given-names>TL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Neural substrates of theory of mind in adults with autism spectrum disorder: An fMRI study of the social animation task</article-title><source>Journal of the Formosan Medical Association = Taiwan Yi Zhi</source><volume>122</volume><fpage>621</fpage><lpage>628</lpage><pub-id pub-id-type="doi">10.1016/j.jfma.2022.10.009</pub-id><pub-id pub-id-type="pmid">36344388</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cléry</surname><given-names>JC</given-names></name><name><surname>Hori</surname><given-names>Y</given-names></name><name><surname>Schaeffer</surname><given-names>DJ</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural network of social interaction observation in marmosets</article-title><source>eLife</source><volume>10</volume><elocation-id>e65012</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65012</pub-id><pub-id pub-id-type="pmid">33787492</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research, an International Journal</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dureux</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Theory-of-Mind_Human_Marmosets_Paper</data-title><version designator="swh:1:rev:0453c93c481cd3a1f4bfe21bea21a538c5fc53e7">swh:1:rev:0453c93c481cd3a1f4bfe21bea21a538c5fc53e7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2503b3212158bb8b1bf22db6948d699976310e5a;origin=https://github.com/audreydureux/Theory-of-mind_Human_Marmosets_Paper;visit=swh:1:snp:6a4f9fd13d4d1b955ebe320dce9aa4ad0fe5448f;anchor=swh:1:rev:0453c93c481cd3a1f4bfe21bea21a538c5fc53e7">https://archive.softwareheritage.org/swh:1:dir:2503b3212158bb8b1bf22db6948d699976310e5a;origin=https://github.com/audreydureux/Theory-of-mind_Human_Marmosets_Paper;visit=swh:1:snp:6a4f9fd13d4d1b955ebe320dce9aa4ad0fe5448f;anchor=swh:1:rev:0453c93c481cd3a1f4bfe21bea21a538c5fc53e7</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prefrontal-hippocampal interactions in episodic memory</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>547</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.74</pub-id><pub-id pub-id-type="pmid">28655882</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>PC</given-names></name><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Baker</surname><given-names>SC</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Other minds in the brain: a functional imaging study of “theory of mind” in story comprehension</article-title><source>Cognition</source><volume>57</volume><fpage>109</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(95)00692-r</pub-id><pub-id pub-id-type="pmid">8556839</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallagher</surname><given-names>HL</given-names></name><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Brunswick</surname><given-names>N</given-names></name><name><surname>Fletcher</surname><given-names>PC</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Reading the mind in cartoons and stories: an fMRI study of “theory of mind” in verbal and nonverbal tasks</article-title><source>Neuropsychologia</source><volume>38</volume><fpage>11</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/s0028-3932(99)00053-6</pub-id><pub-id pub-id-type="pmid">10617288</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>KM</given-names></name><name><surname>Klassen</surname><given-names>LM</given-names></name><name><surname>Mashkovtsev</surname><given-names>A</given-names></name><name><surname>Zeman</surname><given-names>P</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Gati</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Radiofrequency coil for routine ultra-high-field imaging with an unobstructed visual field</article-title><source>NMR in Biomedicine</source><volume>34</volume><elocation-id>e4457</elocation-id><pub-id pub-id-type="doi">10.1002/nbm.4457</pub-id><pub-id pub-id-type="pmid">33305466</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>KM</given-names></name><name><surname>Dureux</surname><given-names>A</given-names></name><name><surname>Jafari</surname><given-names>A</given-names></name><name><surname>Zanini</surname><given-names>A</given-names></name><name><surname>Zeman</surname><given-names>P</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A radiofrequency coil to facilitate task-based fMRI of awake marmosets</article-title><source>Journal of Neuroscience Methods</source><volume>383</volume><elocation-id>109737</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2022.109737</pub-id><pub-id pub-id-type="pmid">36341968</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Marcus</surname><given-names>DS</given-names></name><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Harms</surname><given-names>MP</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The human connectome project’s neuroimaging approach</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1175</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1038/nn.4361</pub-id><pub-id pub-id-type="pmid">27571196</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Koralek</surname><given-names>AC</given-names></name><name><surname>Bryan</surname><given-names>RE</given-names></name><name><surname>Montgomery</surname><given-names>KJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Two takes on the social brain: a comparison of theory of mind tasks</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1803</fpage><lpage>1814</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.11.1803</pub-id><pub-id pub-id-type="pmid">17958483</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halassa</surname><given-names>MM</given-names></name><name><surname>Sherman</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Thalamocortical circuit motifs: A general framework</article-title><source>Neuron</source><volume>103</volume><fpage>762</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.005</pub-id><pub-id pub-id-type="pmid">31487527</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Happé</surname><given-names>FGE</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>An advanced test of theory of mind: understanding of story characters’ thoughts and feelings by able autistic, mentally handicapped, and normal children and adults</article-title><source>Journal of Autism and Developmental Disorders</source><volume>24</volume><fpage>129</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1007/BF02172093</pub-id><pub-id pub-id-type="pmid">8040158</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heider</surname><given-names>F</given-names></name><name><surname>Simmel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1944">1944</year><article-title>An experimental study of apparent behavior</article-title><source>The American Journal of Psychology</source><volume>57</volume><elocation-id>243</elocation-id><pub-id pub-id-type="doi">10.2307/1416950</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>J</given-names></name><name><surname>Mitz</surname><given-names>AR</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>NIMH MonkeyLogic: Behavioral control and data acquisition in MATLAB</article-title><source>Journal of Neuroscience Methods</source><volume>323</volume><fpage>13</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.05.002</pub-id><pub-id pub-id-type="pmid">31071345</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janak</surname><given-names>PH</given-names></name><name><surname>Tye</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>From circuits to behaviour in the amygdala</article-title><source>Nature</source><volume>517</volume><fpage>284</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1038/nature14188</pub-id><pub-id pub-id-type="pmid">25592533</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>KD</given-names></name><name><surname>Barker</surname><given-names>K</given-names></name><name><surname>Schaeffer</surname><given-names>L</given-names></name><name><surname>Schaeffer</surname><given-names>D</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Methods for chair restraint and training of the common marmoset on oculomotor tasks</article-title><source>Journal of Neurophysiology</source><volume>119</volume><fpage>1636</fpage><lpage>1646</lpage><pub-id pub-id-type="doi">10.1152/jn.00866.2017</pub-id><pub-id pub-id-type="pmid">29364068</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>AM</given-names></name><name><surname>Zwickel</surname><given-names>J</given-names></name><name><surname>Prinz</surname><given-names>W</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Animated triangles: an eye tracking investigation</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>62</volume><fpage>1189</fpage><lpage>1197</lpage><pub-id pub-id-type="doi">10.1080/17470210802384214</pub-id><pub-id pub-id-type="pmid">19085338</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Attributing social meaning to ambiguous visual stimuli in higher-functioning autism and Asperger syndrome: The Social Attribution Task</article-title><source>Journal of Child Psychology and Psychiatry, and Allied Disciplines</source><volume>41</volume><fpage>831</fpage><lpage>846</lpage><pub-id pub-id-type="pmid">11079426</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krupenye</surname><given-names>C</given-names></name><name><surname>Hare</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bonobos prefer individuals that hinder others over those that help</article-title><source>Current Biology</source><volume>28</volume><fpage>280</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.061</pub-id><pub-id pub-id-type="pmid">29307556</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kupferberg</surname><given-names>A</given-names></name><name><surname>Glasauer</surname><given-names>S</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Do robots have goals? How agent cues influence action understanding in non-human primates</article-title><source>Behavioural Brain Research</source><volume>246</volume><fpage>47</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2013.01.047</pub-id><pub-id pub-id-type="pmid">23454673</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Ye</surname><given-names>FQ</given-names></name><name><surname>Yen</surname><given-names>CCC</given-names></name><name><surname>Newman</surname><given-names>JD</given-names></name><name><surname>Glen</surname><given-names>D</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Silva</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A digital 3D atlas of the marmoset brain based on multi-modal MRI</article-title><source>NeuroImage</source><volume>169</volume><fpage>106</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.004</pub-id><pub-id pub-id-type="pmid">29208569</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>DS</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Olsen</surname><given-names>T</given-names></name><name><surname>Hodge</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Prior</surname><given-names>F</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Laumann</surname><given-names>T</given-names></name><name><surname>Curtiss</surname><given-names>SW</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Informatics and data mining tools and strategies for the human connectome project</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00004</pub-id><pub-id pub-id-type="pmid">21743807</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijering</surname><given-names>B</given-names></name><name><surname>van Rijn</surname><given-names>H</given-names></name><name><surname>Taatgen</surname><given-names>NA</given-names></name><name><surname>Verbrugge</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>What eye movements can tell about theory of mind in a strategic game</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e45961</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0045961</pub-id><pub-id pub-id-type="pmid">23029341</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Silva</surname><given-names>AC</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Marmosets: A neuroscientific model of human social behavior</article-title><source>Neuron</source><volume>90</volume><fpage>219</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.018</pub-id><pub-id pub-id-type="pmid">27100195</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Premack</surname><given-names>D</given-names></name><name><surname>Woodruff</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Does the chimpanzee have a theory of mind?</article-title><source>Behavioral and Brain Sciences</source><volume>1</volume><fpage>515</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1017/S0140525X00076512</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roumazeilles</surname><given-names>L</given-names></name><name><surname>Schurz</surname><given-names>M</given-names></name><name><surname>Lojkiewiez</surname><given-names>M</given-names></name><name><surname>Verhagen</surname><given-names>L</given-names></name><name><surname>Schüffelgen</surname><given-names>U</given-names></name><name><surname>Marche</surname><given-names>K</given-names></name><name><surname>Mahmoodi</surname><given-names>A</given-names></name><name><surname>Emberton</surname><given-names>A</given-names></name><name><surname>Simpson</surname><given-names>K</given-names></name><name><surname>Joly</surname><given-names>O</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Social prediction modulates activity of macaque superior temporal cortex</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabh2392</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abh2392</pub-id><pub-id pub-id-type="pmid">34524842</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarfati</surname><given-names>Y</given-names></name><name><surname>Hardy-Baylé</surname><given-names>MC</given-names></name><name><surname>Besche</surname><given-names>C</given-names></name><name><surname>Widlöcher</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Attribution of intentions to others in people with schizophrenia: a non-verbal exploration with comic strips</article-title><source>Schizophrenia Research</source><volume>25</volume><fpage>199</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1016/s0920-9964(97)00025-x</pub-id><pub-id pub-id-type="pmid">9264175</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schafroth</surname><given-names>JL</given-names></name><name><surname>Basile</surname><given-names>BM</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>No evidence that monkeys attribute mental states to animated shapes in the Heider-Simmel videos</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>3050</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-82702-6</pub-id><pub-id pub-id-type="pmid">33542404</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurz</surname><given-names>M</given-names></name><name><surname>Radua</surname><given-names>J</given-names></name><name><surname>Tholen</surname><given-names>MG</given-names></name><name><surname>Maliske</surname><given-names>L</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Kanske</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Toward A hierarchical model of social cognition: A neuroimaging meta-analysis and integrative review of empathy and theory of mind</article-title><source>Psychological Bulletin</source><volume>147</volume><fpage>293</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1037/bul0000303</pub-id><pub-id pub-id-type="pmid">33151703</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Bannister</surname><given-names>PR</given-names></name><name><surname>De Luca</surname><given-names>M</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name><name><surname>Flitney</surname><given-names>DE</given-names></name><name><surname>Niazy</surname><given-names>RK</given-names></name><name><surname>Saunders</surname><given-names>J</given-names></name><name><surname>Vickers</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>De Stefano</surname><given-names>N</given-names></name><name><surname>Brady</surname><given-names>JM</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23 Suppl 1</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uller</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Disposition to recognize goals in infant chimpanzees</article-title><source>Animal Cognition</source><volume>7</volume><fpage>154</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1007/s10071-003-0204-9</pub-id><pub-id pub-id-type="pmid">14685823</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandewouw</surname><given-names>MM</given-names></name><name><surname>Safar</surname><given-names>K</given-names></name><name><surname>Mossad</surname><given-names>SI</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Lerch</surname><given-names>JP</given-names></name><name><surname>Anagnostou</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Do shapes have feelings? Social attribution in children with autism spectrum disorder and attention-deficit/hyperactivity disorder</article-title><source>Translational Psychiatry</source><volume>11</volume><elocation-id>493</elocation-id><pub-id pub-id-type="doi">10.1038/s41398-021-01625-y</pub-id><pub-id pub-id-type="pmid">34564704</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Overwalle</surname><given-names>F</given-names></name><name><surname>Baetens</surname><given-names>K</given-names></name><name><surname>Mariën</surname><given-names>P</given-names></name><name><surname>Vandekerckhove</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Social cognition and the cerebellum: A meta-analysis of over 350 fMRI studies</article-title><source>NeuroImage</source><volume>86</volume><fpage>554</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.09.033</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>B</given-names></name><name><surname>Jahn</surname><given-names>A</given-names></name><name><surname>Hyatt</surname><given-names>CS</given-names></name><name><surname>Owens</surname><given-names>MM</given-names></name><name><surname>Carter</surname><given-names>NT</given-names></name><name><surname>Sweet</surname><given-names>LH</given-names></name><name><surname>Miller</surname><given-names>JD</given-names></name><name><surname>Haas</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Investigating the neural substrates of Antagonistic Externalizing and social-cognitive Theory of Mind: an fMRI examination of functional activity and synchrony</article-title><source>Personality Neuroscience</source><volume>4</volume><elocation-id>e1</elocation-id><pub-id pub-id-type="doi">10.1017/pen.2020.12</pub-id><pub-id pub-id-type="pmid">33954274</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheatley</surname><given-names>T</given-names></name><name><surname>Milleville</surname><given-names>SC</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Understanding animate agents: distinct roles for the social network and mirror system</article-title><source>Psychological Science</source><volume>18</volume><fpage>469</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2007.01923.x</pub-id><pub-id pub-id-type="pmid">17576256</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>H</given-names></name><name><surname>Perner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Beliefs about beliefs: representation and constraining function of wrong beliefs in young children’s understanding of deception</article-title><source>Cognition</source><volume>13</volume><fpage>103</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(83)90004-5</pub-id><pub-id pub-id-type="pmid">6681741</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yovel</surname><given-names>G</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Face recognition systems in monkey and human: are they the same thing?</article-title><source>F1000prime Reports</source><volume>5</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.12703/P5-10</pub-id><pub-id pub-id-type="pmid">23585928</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86327.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Irish</surname><given-names>Muireann</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0384j8v12</institution-id><institution>University of Sydney</institution></institution-wrap><country>Australia</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2023.01.16.524238" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.16.524238"/></front-stub><body><p>Dureux and colleagues provide important evidence regarding the capacity for mental state attribution in a highly social non-human primate species, the marmoset. Their findings suggest that marmosets and humans visually track abstract stimuli more closely during ToM animations and display differential activation of large-scale networks implicated in social processing. These findings will be of wide interest to scientists interested in social cognition.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86327.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Irish</surname><given-names>Muireann</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0384j8v12</institution-id><institution>University of Sydney</institution></institution-wrap><country>Australia</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.16.524238">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.01.16.524238v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Gaze patterns and brain activations in humans and marmosets in the Frith-Happé theory-of-mind animation task&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1. In the abstract, the claim that there is no evidence that nonhuman primates attribute mental states to moving shapes is false. You even cite some of this positive evidence (e.g., Uller, 2004; Atsumi et al., 2015; 2017). There is also evidence that they don't (Kupferberg et al., 2013; Burkart et al., 2012; Schafroth et al., 2021). The abstract would be stronger if written to represent the state of the field more accurately.</p><p>2. The overall conclusion as stated in the abstract, at the end of the introduction, and in the discussion is not warranted by the evidence. Indeed, the abstract completely fails to mention that the marmosets failed to show the human-like pattern of longer fixations on the ToM videos. Many readers will likely interpret this evidence as primarily against the idea that marmosets view the ToM videos in a human-like way, or as equivocal evidence at best. This report will be a stronger piece of science if it accurately describes the results.</p><p>3. The authors need to explicitly mention the rationale for omitting the original Goal-Directed condition from the Frith-Happé task. We cannot necessarily conclude that the marmosets are engaged in mental state attribution on the basis of these brain activation patterns – it could reflect the processing of distinct biological movements or the unfolding of an event narrative. If the authors proceed in pushing this data without the Goal-Directed videos, they must address their rationale for not testing these videos.</p><p>4. Were there any differences between the different types of videos used? For instance, was there any difference between videos in which the interaction between the two shapes is more obvious (e.g. coaxing vs seducing)?</p><p>5. Did the authors also present social videos to their animals? If so did they also observe additional recruitment of IPa and TPO areas like Clery and colleagues for social videos compared to Frith and Happe's videos (Cléry et al., 2021)?</p><p>6. Humans and marmosets recruit a distinct set of subcortical structures during the viewing of video clips; for instance dorsal thalamus and cerebellum in humans, the hippocampus and amygdala in marmosets. How do the authors interpret this difference?</p><p>7. Unlike marmosets, rhesus macaques are not sensitive to the type of the Frith and Happe social illusion (Roumazeilles et al., 2021; Schafroth et al., 2021). The authors might want to discuss the singularity of the marmosets from an evolutionary perspective.</p><p>8. The justification for looking in marmosets could be read to imply that macaque monkeys do not live in family groups or share important social similarities with humans. Both species share many social similarities (and many social differences) with humans. Marmosets are a good species to study; this section would benefit from a more accurate rationale.</p><p>9. Because it is one of the main metrics in the Klein and Schafroth papers, and thus readers will want to see it for sake of comparison, the authors should include a figure showing the overall fixation durations as a function of category and species.</p><p>10. The results about looking time to the large triangle need to follow up on the interaction between species and conditions so that readers know how to interpret it.</p><p>11. Are the bars in Figure 2 meant to add up to 1 for any given participant? If you analyzed the total time fixating on either shape, would marmosets be spending less time looking at the shapes overall than humans?</p><p>12. Readers will likely want clarification in cases where the same area showed stronger activation for ToM videos AND Random videos. I assume it was in different voxels in the same larger area, but this could be explicit.</p><p>13. The claim that these maps represent &quot;dedicated brain networks&quot; for ToM or Random videos (line 188) is too strong. These brain areas are used for many things.</p><p>14. For many of the sentences in the imaging results, the comparison needs to be made explicit. For example Line 193 – higher bilateral activation than what? Line 196 – greater activations than what? Line 202 – a larger network than what? Etc.</p><p>15. The description of Klein et al., (2009) on Lines 289-293 might be read to imply that they were attributing mentalizing without good reason. Klein also collected intentionality scores, which correlated with the viewing metric. This could be rephrased to be more accurate.</p><p>16. The inclusion of the authors as subjects is odd. Some readers will view it as a big red flag. The authors clearly know their own hypothesis and likely have a vested interest in a particular outcome. For the strongest report, the authors should remove their own data. At the very least, the authors need to demonstrate that the inclusion/exclusion of their unblinded data doesn't affect the interpretation of the human results.</p><p>17. The method should state whether the subjects had experienced these animations before (e.g., they're shown in some psychology and neuroscience classes).</p><p>18. The description of the monkey reward contingencies needs to be clearer about whether the monkeys were rewarded only during calibration or during videos as well, and whether any reward during videos was contingent on keeping their eyes on the screen.</p><p>19. Because this is a social task when the scans were normalized to MNI space, did the authors divide the human participants into those with and without a paracingulate sulcus?</p><p>20. The authors need to better specify what counts as a &quot;baseline&quot; for the fMRI comparisons. They should also briefly justify why this is an informative comparison.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I very much enjoyed reading this manuscript and believe it has the potential to make an important contribution to animal literature as well as social cognition more broadly.</p><p>As highlighted in the public review, my main query is in relation to the omission of the Goal-Directed condition of the Frith-Happé task. While the evidence presented here certainly suggests that marmosets process ToM animations in a different manner than Random animations, we are somewhat constrained in what we can interpret from these findings. As the authors note, we cannot necessarily conclude that the marmosets are engaged in mental state attribution on the basis of these brain activation patterns.</p><p>A more compelling argument would stem from the inclusion of the Goal-Directed condition in which the triangles arguably do interact but in a purely physical manner, i.e., there is no mental state attribution. I was surprised that this condition was not included as its omission somewhat limits the extent to which any conclusion regarding ToM can be drawn. Could the activations observed in the ToM condition reflect the processing of an event or narrative as it unfolds, rather than the cycling of random movements in the Random condition? I ask this question as previous studies using the Frith-Happé animations in dementia populations note that the mental state attribution judgements on ToM trials were conferred only at the end of the video (i.e., once the overall event narrative had been seen) whereas patients were adept at conferring a judgment of &quot;no interaction&quot; early during the viewing of Random animations (Ref: Synn et al. 2018 J. Alz Dis).</p><p>I wonder whether this interpretation might also reflect the curious finding of stronger medial PFC activation in Random trials versus ToM trials in humans, and no clear mPFC activation in the ToM trials. This seems very much at odds with the wider literature on the brain regions necessary for ToM, which often place the medial PFC at the heart of the social brain.</p><p>I very much appreciated the check using the independent HCP dataset. This was a very nice inclusion to ensure that the shortened version corresponded well with previous reports.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In their study, Dureux and colleagues are investigating the sensitivity of a highly social non-human primate species, the marmoset, to social illusion using the Frith and Happe task. Although this task is often considered a non-verbal TOM task, its relevance to investigate TOM has been disputed. For instance, the Frith and Happe task does not recruit in humans a similar network as other false-belief tasks and social gambling tasks (Schurz et al., 2020). While the authors might want to revise, or at least discuss their use of the TOM concept further, their results clearly show that marmosets distinguish the two types of videos shown to them.</p><p>Were there any differences between the different types of videos used? For instance, was there any difference between videos in which the interaction between the two shapes is more obvious (e.g. coaxing vs seducing).</p><p>Did the authors also present social videos to their animals? If so did they also observe additional recruitment of IPa and TPO areas like Clery and colleagues for social videos compared to Frith and Happe's videos (Cléry et al., 2021)?</p><p>Humans and marmosets recruit a distinct set of subcortical structures during the viewing of video clips; for instance dorsal thalamus and cerebellum in humans, and the hippocampus and amygdala in marmosets. How do the authors interpret this difference?</p><p>Unlike marmosets, rhesus macaques are not sensitive to the type of the Frith and Happe social illusion (Roumazeilles et al., 2021; Schafroth et al., 2021). The authors might want to discuss the singularity of the marmosets from an evolutionary perspective.</p><p>Refs:</p><p>Cléry JC, Hori Y, Schaeffer DJ, Menon RS, Everling S. 2021. Neural network of social interaction observation in marmosets. <italic>eLife</italic> 10:e65012. doi:10.7554/<italic>eLife</italic>.65012</p><p>Roumazeilles L, Schurz M, Lojkiewiez M, Verhagen L, Schüffelgen U, Marche K, Mahmoodi A, Emberton A, Simpson K, Joly O, Khamassi M, Rushworth MFS, Mars RB, Sallet J. 2021. Social prediction modulates activity of macaque superior temporal cortex (preprint). Neuroscience. doi:10.1101/2021.01.22.427803</p><p>Schafroth JL, Basile BM, Martin A, Murray EA. 2021. No evidence that monkeys attribute mental states to animated shapes in the Heider-Simmel videos. Sci Rep 11:3050. doi:10.1038/s41598-021-82702-6</p><p>Schurz M, Radua J, Tholen MG, Maliske L, Margulies DS, Mars RB, Sallet J, Kanske P. 2020. Toward a hierarchical model of social cognition: A neuroimaging meta-analysis and integrative review of empathy and theory of mind. Psychological Bulletin. doi:10.1037/bul0000303</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This study is strong in many ways, and the goal is a good one. The below recommendations will help strengthen it further:</p><p>In the abstract, the claim that there is no evidence that nonhuman primates attribute mental states to moving shapes is false. You even cite some of this positive evidence (e.g., Uller, 2004; Atsumi et al., 2015; 2017). There is also evidence that they don't (Kupferberg et al., 2013; Burkart et al., 2012; Schafroth et al., 2021). The abstract would be stronger if written to represent the state of the field more accurately.</p><p>The overall conclusion as stated in the abstract, at the end of the introduction, and in the discussion is not warranted by the evidence. Indeed, the abstract completely fails to mention that the marmosets failed to show the human-like pattern of longer fixations on the ToM videos. Many readers will likely interpret this evidence as primarily against the idea that marmosets view the ToM videos in a human-like way, or as equivocal evidence at best. This report will be a stronger piece of science if it accurately describes the results.</p><p>The justification for looking in marmosets could be read to imply that macaque monkeys do not live in family groups or share important social similarities with humans. Both species share many social similarities (and many social differences) with humans. Marmosets are a good species to study; this section would benefit from a more accurate rationale.</p><p>Because it is one of the main metrics in the Klein and Schafroth papers, and thus readers will want to see it for sake of comparison, the authors should include a figure showing the overall fixation durations as a function of category and species.</p><p>The results about looking time to the large triangle need to follow up on the interaction between species and conditions so that readers know how to interpret it.</p><p>The sentence on lines 97-99 might be an incomplete sentence.</p><p>Are the bars in Figure 2 meant to add up to 1 for any given participant? If you analyzed the total time fixating on either shape, would marmosets be spending less time looking at the shapes overall than humans?</p><p>Overall, the figures are quite informative and aesthetically pleasing.</p><p>HCP should be explained the first time it is used.</p><p>Readers will likely want clarification in cases where the same area showed stronger activation for ToM videos AND Random videos. I assume it was in different voxels in the same larger area, but this could be explicit.</p><p>The claim that these maps represent &quot;dedicated brain networks&quot; for ToM or Random videos (line 188) is too strong. These brain areas are used for many things.</p><p>For many of the sentences in the imaging results, the comparison needs to be made explicit. For example Line 193 – higher bilateral activation than what? Line 196 – greater activations than what? Line 202 – a larger network than what? Etc.</p><p>The description of Klein et al., (2009) on Lines 289-293 might be read to imply that they were attributing mentalizing without good reason. Klein also collected intentionality scores, which correlated with the viewing metric. This could be rephrased to be more accurate.</p><p>In general, the discussion could be strengthened by avoiding repeating the results in as much detail.</p><p>The inclusion of the authors as subjects is odd. Some readers will view it as a big red flag. The authors clearly know their own hypothesis and likely have a vested interest in a particular outcome. For the strongest report, the authors should remove their own data. At the very least, the authors need to demonstrate that the inclusion/exclusion of their unblinded data doesn't affect the interpretation of the human results.</p><p>The method should state whether the subjects had experienced these animations before (e.g., they're shown in some psychology and neuroscience classes).</p><p>If the authors proceed in pushing this data without the Goal-Directed videos, they need to at least address their rationale for not testing these videos.</p><p>The description of the monkey reward contingencies needs to be clearer about whether the monkeys were rewarded only during calibration or during videos as well, and whether any reward during videos was contingent on keeping their eyes on the screen.</p><p>Because this is a social task when the scans were normalized to MNI space, did the authors divide the human participants into those with and without a paracingulate sulcus?</p><p>The authors need to better specify what counts as a &quot;baseline&quot; for the fMRI comparisons. They should also briefly justify why this is an informative comparison.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86327.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. In the abstract, the claim that there is no evidence that nonhuman primates attribute mental states to moving shapes is false. You even cite some of this positive evidence (e.g., Uller, 2004; Atsumi et al., 2015; 2017). There is also evidence that they don't (Kupferberg et al., 2013; Burkart et al., 2012; Schafroth et al., 2021). The abstract would be stronger if written to represent the state of the field more accurately.</p></disp-quote><p>We greatly appreciate the reviewer's insightful comments. In response, we have revised the abstract to better align with the existing literature concerning nonhuman primates’ abilities to attribute mental states to moving shapes. The specific modifications can be found on page 2 of the revised manuscript.</p><p>Page 2: “Theory of Mind (ToM) refers to the cognitive ability to attribute mental states to other individuals. This ability extends even to the attribution of mental states to animations featuring simple geometric shapes, such as the Frith-Happé animations in which two triangles move either purposelessly (Random condition), exhibit purely physical movement (Goal-directed condition), or move as if one triangle is reacting to the other triangle’s mental states (ToM condition). While this capacity in humans has been thoroughly established, research on nonhuman primates has yielded inconsistent results.</p><p>This study explored how marmosets (Callithrix jacchus), a highly social primate species, process Frith-Happé animations by examining gaze patterns and brain activations of marmosets and humans as they observed these animations. We revealed that both marmosets and humans exhibited longer fixations on one of the triangles in ToM animations, compared to other conditions. However, we did not observe the same pattern of longer overall fixation duration on the ToM animations in marmosets as identified in humans. Furthermore, our findings reveal that both species activated extensive and comparable brain networks when viewing ToM versus Random animations, suggesting that marmosets differentiate between these scenarios similarly to humans. While marmosets did not mimic human overall fixation patterns, their gaze behavior and neural activations indicate a distinction between ToM and non-ToM scenarios. This study expands our understanding of nonhuman primate cognitive abilities, shedding light on potential similarities and differences in ToM processing between marmosets and humans.”</p><disp-quote content-type="editor-comment"><p>2. The overall conclusion as stated in the abstract, at the end of the introduction, and in the discussion is not warranted by the evidence. Indeed, the abstract completely fails to mention that the marmosets failed to show the human-like pattern of longer fixations on the ToM videos. Many readers will likely interpret this evidence as primarily against the idea that marmosets view the ToM videos in a human-like way, or as equivocal evidence at best. This report will be a stronger piece of science if it accurately describes the results.</p></disp-quote><p>We agree that it is crucial to precisely represent the results of our study, including the nuanced details about marmosets' reactions to the ToM videos. To this end, we have revised the abstract, introduction, and Discussion sections to provide a more balanced and precise interpretation of our findings.</p><p>The major revisions appear on page 2 in the abstract, on page 4 in the introduction, and between pages 14 to 20 in the Discussion section. We believe that these changes will ensure the results and conclusions of the study are conveyed more accurately and transparently.</p><p>The revised content reads as follows:</p><p>Page 2: “Theory of Mind (ToM) refers to the cognitive ability to attribute mental states to other individuals. This ability extends even to the attribution of mental states to animations featuring simple geometric shapes, such as the Frith-Happé animations in which two triangles move either purposelessly (Random condition), exhibit purely physical movement (Goal-directed condition), or move as if one triangle is reacting to the other triangle’s mental states (ToM condition). While this capacity in humans has been thoroughly established, research on nonhuman primates has yielded inconsistent results.</p><p>This study explored how marmosets (Callithrix jacchus), a highly social primate species, process Frith-Happé animations by examining gaze patterns and brain activations of marmosets and humans as they observed these animations. We revealed that both marmosets and humans exhibited longer fixations on one of the triangles in ToM animations, compared to other conditions. However, we did not observe the same pattern of longer overall fixation duration on the ToM animations in marmosets as identified in humans. Furthermore, our findings reveal that both species activated extensive and comparable brain networks when viewing ToM versus Random animations, suggesting that marmosets differentiate between these scenarios similarly to humans. While marmosets did not mimic human overall fixation patterns, their gaze behavior and neural activations indicate a distinction between ToM and non-ToM scenarios. This study expands our understanding of nonhuman primate cognitive abilities, shedding light on potential similarities and differences in ToM processing between marmosets and humans.”</p><p>Pages 4-5: “Although the spontaneous attribution of mental states to moving shapes has been well established in humans, it remains uncertain whether other primate species share this capacity. There is some evidence suggesting that monkeys can attribute goals to agents with varying levels of similarity and familiarity to conspecifics, including human agents, monkey robots, moving geometric boxes, animated shapes, and simple moving dots (Atsumi et al., 2017; Atsumi and Nagasaka, 2015; Krupenye and Hare, 2018; Kupferberg et al., 2013; Uller, 2004). However, the findings in this area are somewhat mixed, with some studies investigating the attribution of goals to inanimate moving objects yielding inconclusive results (Atsumi and Nagasaka, 2015; Kupferberg et al., 2013). Nonhuman primates' spontaneous attribution of mental states to Frith-Happé animations is even less certain. While human subjects exhibit longer eye fixations when viewing the ToM condition compared to the Random condition of the Frith-Happé animations (Klein et al., 2009), a recent eye tracking study in macaque monkeys did not observe similar differences (Schafroth et al., 2021). Similarly, a recent fMRI study conducted on macaques found no discernible differences in activations between ToM and random Frith-Happé animations (Roumazeilles et al., 2021).</p><p>In this study, we investigated the behaviour and brain activations of New World common marmoset monkeys (Callithrix jacchus) while they viewed Frith-Happé animations. Living in closely-knit family groups, marmosets exhibit significant social parallels with humans, including prosocial behavior, imitation, and cooperative breeding. These characteristics establish them as a promising nonhuman primate model for investigating social cognition (Burkart et al., 2009; Burkart and Finkenwirth, 2015; Miller et al., 2016). To directly compare humans and marmosets in their response to these animations, we employed high-speed video eye-tracking to record eye movements in eleven healthy humans and eleven marmoset monkeys. Additionally, we conducted ultra-high field fMRI scans on ten healthy humans at 7T and six common marmoset monkeys at 9.4T. These combined methods allowed us to examine the visual behavior and brain activations of both species while they observed the Frith-Happé animations.”</p><p>Pages 14-15: “In our first experiment, we examined the gaze patterns of marmosets and humans during the viewing of these video animations. Klein et al. (2009) reported differing fixation durations for these animations, where the longest fixations were observed for ToM animations, followed by GD animations and the shortest fixations for Random animations. They further reported that the intentionality score – derived from verbal descriptions of the animations – followed a similar pattern: highest for ToM, lowest for Random, and intermediate for GD animations. This validated the degree of mental state attribution according to the categories and established that animations provoking mentalizing (ToM condition) were associated with long fixations. This, in turn, supports the use of fixation durations as a nonverbal metric for mentalizing capacity (Klein et al., 2009; Meijering et al., 2012). Our results with human subjects, which demonstrated longer fixation durations for the ToM animations compared to the GD and Random animations, paralleled those of Klein et al. (2009). However, unlike Klein et al.'s findings, we did not observe intermediate durations for GD animations in our study.</p><p>Interestingly, our marmoset data did not align with the human findings but instead resonated more with Schafroth et al. (2021)'s observations in macaque monkeys, which did not show significant differences in fixation durations across the three animation types.”</p><p>Pages 19-20: “In summary, our study reveals novel insights into how New World marmosets, akin to humans, differentially process abstract animations that depict complex social interactions and animations that display purely physical or random movements. Our findings, supported by both specific gaze behaviors (i.e., the proportion of time spent on the red triangle, despite the inconclusiveness of overall fixation) and distinct neural activation patterns, shed light on the marmosets' capacity to interpret social cues embedded in these animations.</p><p>The differences observed between humans, marmosets, and macaques underscore the diverse cognitive strategies that primate species have evolved to decipher social information. This diversity may be influenced by unique evolutionary pressures that arise from varying social structures and lifestyles. Like macaque monkeys, humans often live in large, hierarchically organized social groups where status influences access to resources. However, both humans and marmosets share a common trait: a high degree of cooperative care for offspring within the group, with individuals other than the biological parents participating in child-rearing. These distinctive social dynamics of marmosets and humans may have driven the development of unique social cognitive abilities. This could explain their enhanced sensitivity to abstract social cues in the Frith-Happé animations.</p><p>Nonetheless, it is crucial to emphasize that even though marmosets respond to the social cues in the Frith-Happé animations, this does not automatically imply that they possess mental-state attributions comparable to humans. As such, future research including a range of tasks, from sensory-affective components to more abstract and decoupled representations of others' mental states (Schurz et al., 2020), will be fundamental in further unravelling the complexities of the evolution and functioning of the theory of mind across the primate lineage.”</p><disp-quote content-type="editor-comment"><p>3. The authors need to explicitly mention the rationale for omitting the original Goal-Directed condition from the Frith-Happé task. We cannot necessarily conclude that the marmosets are engaged in mental state attribution on the basis of these brain activation patterns – it could reflect the processing of distinct biological movements or the unfolding of an event narrative. If the authors proceed in pushing this data without the Goal-Directed videos, they must address their rationale for not testing these videos.</p></disp-quote><p>We appreciate the reviewer's feedback about the omission of the Goal-Directed condition from the Frith-Happé task in our study. In our revised manuscript, we have elaborated on the factors influencing our decision to focus primarily on the ToM and Random conditions. These factors were two-fold:</p><p>1. Influence from prior fMRI studies: Many previous fMRI studies using the Frith-Happé animated triangles task with human and macaque subjects have only employed the ToM and Random conditions. These conditions represent the two extremes, with ToM depicting scenarios with mental interactions and Random showing scenarios absent of mental interactions. GD condition is situated between these two extremes, depicting physical interaction among the triangles without suggesting mental state attribution.</p><p>2. Practical considerations: The duration of each video clip in the Frith-Happé task (19.5 seconds) presented challenges for keeping marmoset subjects alert and focused during longer scanning sessions.</p><p>Given these constraints, we made the decision to limit the number of conditions presented in a single run.</p><p>However, understanding the value of including the Goal-Directed condition, we have performed an additional eye-tracking experiment incorporating all three conditions: ToM, Goal-Directed, and Random. The results from this experiment provided further insights into the gaze patterns during these conditions, adding depth to our understanding of marmoset behavior during the different conditions.</p><p>Moreover, while our data reveal distinct patterns of brain activation and gaze behavior during the ToM condition, we recognize and emphasize in our revised manuscript that these patterns do not conclusively prove that marmosets attribute mental states in the same way as humans.</p><p>These adjustments, along with the findings from the new eye-tracking experiment, have been integrated into the revised manuscript. The relevant sections in the methods, results, and discussion have been modified accordingly. These revisions can be found on pages 5 to 7 (Results section), 14 to 20 (Discussion section), and 23-24 (method section) of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>4. Were there any differences between the different types of videos used? For instance, was there any difference between videos in which the interaction between the two shapes is more obvious (e.g. coaxing vs seducing)?</p></disp-quote><p>We appreciate the reviewer's interest in the distinct types of videos used in our study. In this investigation, we did not specifically analyze the responses to videos where the interaction between the two shapes was more or less pronounced, such as in coaxing versus seducing scenarios. Our primary focus was on contrasting the overall responses elicited by ToM animations and Random animations. Due to our experimental design, we did not have enough repetitions of each distinct type of video within each condition to provide the statistical power necessary for such an analysis.</p><p>We recognize that dissecting responses to different types of ToM animations might reveal further insights into the specificity of neural responses, and this is an intriguing area for future research.</p><disp-quote content-type="editor-comment"><p>5. Did the authors also present social videos to their animals? If so did they also observe additional recruitment of IPa and TPO areas like Clery and colleagues for social videos compared to Frith and Happe's videos (Cléry et al., 2021)?</p></disp-quote><p>Our study specifically focused on neural responses to the abstract social scenarios represented by the Frith-Happé animations. Consequently, realistic social videos were not included in our experimental design.</p><p>The study by Cléry et al. (2021) indeed demonstrates that the comparison social versus non-social realistic videos predominantly revealed a fronto-parietal network with additional temporal region engagement. The social condition in their study seems to recruit not only several areas that we observed to be activated in our ToM condition, but also additional temporal regions such as the IPa, TPO and TE1 areas. This suggests that these areas may play a significant role in the processing of more realistic social cues, thereby adding complexity to the social brain network.</p><p>Although our current study did not directly investigate this aspect, we agree that comparing the neural responses elicited by abstract versus realistic social scenarios in marmosets could provide valuable insights into the extent and adaptability of the social brain network. This approach would further our understanding of the neural substrates that underpin various facets of social cognition, depending on the complexity and realism of the presented stimuli. In future research, we plan to consider including such comparative investigations in our experimental design. We appreciate the reviewer's suggestion.</p><disp-quote content-type="editor-comment"><p>6. Humans and marmosets recruit a distinct set of subcortical structures during the viewing of video clips; for instance dorsal thalamus and cerebellum in humans, the hippocampus and amygdala in marmosets. How do the authors interpret this difference?</p></disp-quote><p>We are grateful to the reviewer for drawing attention to the distinct set of subcortical structures engaged by humans and marmosets during the viewing of video clips. The divergent patterns may reflect species-specific social cognitive strategies.</p><p>In humans, the involvement of the dorsal thalamus, which serves as a critical hub for information relay between various subcortical areas and the cortex, may indicate the necessity for complex information processing in interpreting the animations (e.g., Halassa and Sherman, 2019). The activation of the cerebellum, beyond its traditional role in motor functions, supports recent findings of its involvement in social cognition (e.g., Van Overwalle et al., 2014). The activations in a small portion of the amygdala may reflect emotional processing tied to understanding of the social scenarios in the animations (e.g., Janak and Tye, 2015).</p><p>On the other hand, the activation of the hippocampus and amygdala in marmosets might reflect a more emotion-driven interpretation of the animations. The recruitment of the hippocampus could suggest the role of memory in interpreting the animations by remembering past interactions to help interpret current social scenarios (e.g., Eichenbaum, 2017), while the more extended activation of the amygdala in marmosets might imply a higher degree of emotional processing compared to humans (e.g., Janak and Tye, 2015).</p><p>In response, we have included a detailed discussion in our revised manuscript, which takes into account the potential roles of these structures in complex information processing, social cognition, emotional processing, and memory recall in the context of interpreting the animations. We emphasize that these interpretations remain speculative and underscore the need for further research to confirm these observations.</p><p>We have now expanded our discussion on this topic in the revised manuscript, specifically on pages 18-19:</p><p>&quot; Regarding the distinct subcortical activations observed in humans and marmosets, it's important to consider the specific social cognitive demands that might be unique to each species. The involvement of the dorsal thalamus, cerebellum, and a small portion of the amygdala in humans may reflect the complexities of information processing, social cognition, and emotional involvement required to interpret the ToM animations (e.g., Halassa and Sherman, 2019; Janak and Tye, 2015; Van Overwalle et al., 2014). Conversely, the activation of the amygdala and hippocampus in marmosets could suggest a more emotion- and memory-based processing of the social stimuli (e.g., Eichenbaum, 2017; Van Overwalle et al., 2014). However, it's critical to consider that these interpretations are speculative and would require further study for confirmation.”</p><disp-quote content-type="editor-comment"><p>7. Unlike marmosets, rhesus macaques are not sensitive to the type of the Frith and Happe social illusion (Roumazeilles et al., 2021; Schafroth et al., 2021). The authors might want to discuss the singularity of the marmosets from an evolutionary perspective.</p></disp-quote><p>As suggested, in our revised manuscript, we have now incorporated a comprehensive discussion regarding the uniqueness of marmosets from an evolutionary perspective, especially in light of the different results obtained in a similar study conducted on rhesus macaques. We highlight the divergent evolutionary trajectories of New World monkeys (such as marmosets) and Old-World monkeys (such as macaques), which may contribute to the differential sensitivity to abstract social cues embedded in animations.</p><p>We also underscore the diverse cognitive strategies that primate species employ in deciphering social information, influenced by unique evolutionary pressures arising from varying social structures and lifestyles.</p><p>The associated changes in our manuscript can be found in the Discussion section on pages 19-20, which reads:</p><p>“Interestingly, our results differed from those obtained by Roumazeilles et al. (2021) in their fMRI study conducted in macaques using the same animations. Roumazeilles and colleagues reported no differences in activation between ToM and Random animations, suggesting that rhesus macaques may not respond to the social cues presented by the ToM Frith-Happé animations. This disparity between our marmoset findings and those of macaques raises intriguing questions about potential differences in the evolutionary development of ToM processing within non-human primates. Marmosets, as New World monkeys, are part of an evolutionary lineage that diverged earlier than the lineage of Old-World monkeys such as macaques. This difference in lineage might lead to distinct evolutionary trajectories in cognitive processing, which could include varying sensitivity to abstract social cues in animations.</p><p>In summary, our study reveals novel insights into how New World marmosets, akin to humans, differentially process abstract animations that depict complex social interactions and animations that display purely physical or random movements. Our findings, supported by both specific gaze behaviors (i.e., the proportion of time spent on the red triangle, despite the inconclusiveness of overall fixation) and distinct neural activation patterns, shed light on the marmosets' capacity to interpret social cues embedded in these animations.</p><p>The differences observed between humans, marmosets, and macaques underscore the diverse cognitive strategies that primate species have evolved to decipher social information. This diversity may be influenced by unique evolutionary pressures that arise from varying social structures and lifestyles. Like macaque monkeys, humans often live in large, hierarchically organized social groups where status influences access to resources. However, both humans and marmosets share a common trait: a high degree of cooperative care for offspring within the group, with individuals other than the biological parents participating in child-rearing. These distinctive social dynamics of marmosets and humans may have driven the development of unique social cognitive abilities. This could explain their enhanced sensitivity to abstract social cues in the Frith-Happé animations.</p><p>Nonetheless, it is crucial to emphasize that even though marmosets respond to the social cues in the Frith-Happé animations, this does not automatically imply that they possess mental-state attributions comparable to humans. As such, future research including a range of tasks, from sensory-affective components to more abstract and decoupled representations of others' mental states (Schurz et al., 2020), will be fundamental in further unravelling the complexities of the evolution and functioning of the theory of mind across the primate lineage.”</p><disp-quote content-type="editor-comment"><p>8. The justification for looking in marmosets could be read to imply that macaque monkeys do not live in family groups or share important social similarities with humans. Both species share many social similarities (and many social differences) with humans. Marmosets are a good species to study; this section would benefit from a more accurate rationale.</p></disp-quote><p>In response to the reviewer's comment, we have clarified our previous section. Our intention was to highlight the unique social aspects of marmosets that make them a suitable species for studying social cognition, not to imply that macaques do not have their own set of social similarities with humans. As suggested, we have now revised the sentence on page 4 to enhance its clarity and accuracy, which now reads:</p><p>Page 4: “Living in closely-knit family groups, marmosets exhibit significant social parallels with humans, including prosocial behavior, imitation, and cooperative breeding. These characteristics establish them as a promising nonhuman primate model for investigating social cognition (Burkart et al., 2009; Burkart and Finkenwirth, 2015; Miller et al., 2016).”</p><disp-quote content-type="editor-comment"><p>9. Because it is one of the main metrics in the Klein and Schafroth papers, and thus readers will want to see it for sake of comparison, the authors should include a figure showing the overall fixation durations as a function of category and species.</p></disp-quote><p>In accordance with the reviewer's recommendation, we have included a new figure (Figure 2) into our revised manuscript, which can be found on page 39. This figure graphically represents the overall fixation durations as a function of both animation category and species. This facilitates a more comprehensive comparison of fixation durations between humans and marmosets across the different animation conditions (Random, Goal-directed, and ToM). This comparison is in alignment with the data representation found in the Klein and Schafroth paper.</p><disp-quote content-type="editor-comment"><p>10. The results about looking time to the large triangle need to follow up on the interaction between species and conditions so that readers know how to interpret it.</p></disp-quote><p>In response to the reviewer's comment, we have provided a more detailed analysis of the interaction between species and conditions for the proportion of time spent looking at the large red triangle. We found that both humans and marmosets spent a greater proportion of time looking at the red triangle in the ToM condition compared to the GD and Random conditions. However, while humans also allocated more time to the red triangle in GD compared to Random animations, marmosets did not show any difference between these two conditions. The updated text can be found in the Results section on pages 5 to 7, which now reads as follows:</p><p>Pages 6-7: “To further analyze the gaze patterns of both humans and marmosets, we next measured the proportion of time subjects looked at each of the triangles in the videos (Figure 2B). We conducted mixed ANOVAs on the proportion of time the radial distance between the current gaze position and each triangle was within 4 visual degrees for each triangle separately.</p><p>Importantly, we observed a significant interaction between species and condition for the proportion of time spent looking at the large red triangle (F(2,40)=9.83, p&lt;.001, ηp2 = .330). Specifically, both humans (Figure 2B left) and marmosets (Figure 2B right) spent a greater proportion of time looking at the red triangle in ToM compared to the GD and Random videos (For humans, ToM vs GD: Δ=.23, p&lt;.001 and ToM vs Random: Δ=.31, p&lt;.001 ; For marmosets, ToM vs GD: Δ=.13, p&lt;.01 and ToM vs Random: Δ=.13, p&lt;.01). However, while humans also allocated a greater proportion of time to the red triangle in GD compared to Random animations (Δ=.08, p=.05), marmosets did not show any difference between these two conditions (Δ=.0003, p=1).</p><p>For the small blue triangle, we also observed a significant interaction of species and condition (F(2,40)=3.54, p=.04, ηp2=.151) but the comparisons were not resistant to the p value adjustment by Bonferroni correction. Therefore, humans and marmosets spent the same proportion of time looking at the blue triangle in the three different types of videos (For humans, ToM vs GD: Δ=-.02, p=1, ToM vs Random: Δ=.04, p=1 and GD vs Random: Δ=.07, p=.23 ; For marmosets, ToM vs GD: Δ=-.05, p=.89, ToM vs Random: Δ=.07, p=.66 and GD vs Random: Δ=-.02, p=1; Figure 2B).</p><p>These results highlight the variation in gaze patterns observed in both humans and marmosets when their focus is directed towards the large red triangle during the viewing of ToM, GD, and Random videos. Notably, humans show a gradient of proportion of time spent looking at the red triangle across the three conditions, with the smallest proportion in Random videos and the greatest proportion in ToM videos. In contrast, marmosets exhibit a different pattern, spending more time looking at the red triangle in ToM videos, but allocating the same proportion of time to look at the red triangle in both Random and GD videos. This finding suggests that while humans demonstrate distinct attentional preferences for the red triangle across the three conditions, marmosets exhibit a similar attentional focus on the red triangle in the Random and GD conditions, but their pattern differs in the ToM condition. This suggests that marmosets process the Random and GD conditions in a similar manner, but their processing of the ToM condition is distinct, indicating a differential response to stimuli representing social interactions.”</p><disp-quote content-type="editor-comment"><p>11. Are the bars in Figure 2 meant to add up to 1 for any given participant? If you analyzed the total time fixating on either shape, would marmosets be spending less time looking at the shapes overall than humans?</p></disp-quote><p>The values in the original Figure 2 do not total 1 for each participant, as there are instances where the triangles either overlap or are proximate enough that the eye position falls within the defined radius for both shapes simultaneously. Responding to the second query, after conducting an additional analysis, we found a significant species effect on the total time spent fixating on either shape (<italic>F</italic><sub>(1,20)</sub>=14.38, <italic>p</italic>=.001, <italic>η<sub>p</sub><sup>2</sup></italic>=.42). This indicates that humans tend to look at the triangles more frequently than marmosets (Δ=.16, <italic>p</italic>=.001).</p><disp-quote content-type="editor-comment"><p>12. Readers will likely want clarification in cases where the same area showed stronger activation for ToM videos AND Random videos. I assume it was in different voxels in the same larger area, but this could be explicit.</p></disp-quote><p>In response to the reviewer's request for clarification, we have made it explicit in our manuscript that while larger areas of the brain showed stronger activation for both ToM and Random videos, the specific voxels within these areas exhibiting this activation were typically distinct. Furthermore, we note that in some instances, both conditions activated the same voxels, but the degree of activation differed, suggesting spatial and intensity variation within the same areas. This elaboration can be found in the sections discussing functional brain activations in humans (pages 7-8) and marmosets (page 11). This can be read:</p><p>Pages 7-8 (Functional brain activations while watching ToM and Random Frith-Happé’s animations in humans): “Both ToM (Figure 3A) and Random (Figure 3B) videos activated a large bilateral network. While the same larger areas were activated in both conditions, the specific voxels showing this activation within those areas were typically distinct. In some cases, both conditions activated the same voxels, but the degree of activation differed. This suggests a degree of both spatial and intensity variation in the activations for the two conditions within the same areas.”</p><p>Page 11 (Functional brain activations while watching ToM and Random Frith-Happé’s animations in marmosets): “Both the ToM (Figure 4A) and Random (Figure 4B) animations activated an extensive network involving a variety of areas in the occipito-temporal, parietal and frontal regions. As in human subjects, it should be noted that while both conditions elicited strong activation in some of the same larger areas, these activations might have either occurred in distinct voxels within those areas, or the same voxels were activated to varying degrees for both conditions. This suggests distinct yet overlapping patterns of neural processing for the ToM and Random conditions.”</p><disp-quote content-type="editor-comment"><p>13. The claim that these maps represent &quot;dedicated brain networks&quot; for ToM or Random videos (line 188) is too strong. These brain areas are used for many things.</p></disp-quote><p>In response to the reviewer's comment, we agree that the term &quot;dedicated brain networks&quot; could potentially imply exclusivity, which is not our intention. We understand that these brain areas participate in a variety of cognitive functions. To address this, we have modified our phrasing on page 10 to &quot;brain networks activated during the processing of ToM or Random videos&quot; to more accurately represent our findings.</p><disp-quote content-type="editor-comment"><p>14. For many of the sentences in the imaging results, the comparison needs to be made explicit. For example Line 193 – higher bilateral activation than what? Line 196 – greater activations than what? Line 202 – a larger network than what? Etc.</p></disp-quote><p>We agree with the reviewer's observation about the need for explicit comparisons in our imaging results. To address this, we have now revised certain sentences in the Results section to provide clear and specific comparisons. The updated descriptions can be found in the Results section on pages 7 to 12 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>15. The description of Klein et al., (2009) on Lines 289-293 might be read to imply that they were attributing mentalizing without good reason. Klein also collected intentionality scores, which correlated with the viewing metric. This could be rephrased to be more accurate.</p></disp-quote><p>We concur with the reviewer's suggestion for a more accurate interpretation of Klein et al., 2009. Our intention was not to undermine the work by Klein et al. To address this, we have adjusted the phrasing within the Discussion section on page 15 of our manuscript, emphasizing Klein et al.'s valuable contribution through their correlation of intentionality scores with fixation durations. These revisions result in a more balanced and accurate representation of their work.</p><p>Page 15: “In our first experiment, we examined the gaze patterns of marmosets and humans during the viewing of these video animations. Klein et al. (2009) reported differing fixation durations for these animations, where the longest fixations were observed for ToM animations, followed by GD animations and the shortest fixations for Random animations. They further reported that the intentionality score – derived from verbal descriptions of the animations – followed a similar pattern: highest for ToM, lowest for Random, and intermediate for GD animations. This validated the degree of mental state attribution according to the categories and established that animations provoking mentalizing (ToM condition) were associated with long fixations. This, in turn, supports the use of fixation durations as a nonverbal metric for mentalizing capacity (Klein et al., 2009; Meijering et al., 2012).”</p><disp-quote content-type="editor-comment"><p>16. The inclusion of the authors as subjects is odd. Some readers will view it as a big red flag. The authors clearly know their own hypothesis and likely have a vested interest in a particular outcome. For the strongest report, the authors should remove their own data. At the very least, the authors need to demonstrate that the inclusion/exclusion of their unblinded data doesn't affect the interpretation of the human results.</p></disp-quote><p>We acknowledge the reviewer's concern regarding the inclusion of authors as subjects, and potential bias it could introduce. In response to this, we have excluded the data from the authors who initially participated in the study and replaced it with new data from subjects unrelated to the authorship of this work. For the eye tracking experiment, given the introduction of a new “Goal-Directed condition”, we carried out the experiment with eleven new participants, none of whom are authors of this study. Regarding the fMRI experiment, we substituted the data collected from the three author-participants with data from three additional participants who were not informed about the study's hypothesis.</p><p>The re-analyzed results, accounting for the updated participant pool, can now be found in the sections: “Gaze patterns for Frith-Happé’s ToM, GD and Random animations in humans and marmosets” (pages 5-7), “Functional brain activations while watching ToM and Random Frith-Happé’s animations in humans” (pages 7-10), and “Comparison of functional brain activations in humans and marmosets” (pages 13-14). We also updated the figures 2, 3 and 5 and the figures supplement 1 and 2 on pages 39, 40, 42, 43, and 44, respectively.</p><p>The revised participant information is detailed in the methods section on page 22:</p><p>“Eleven healthy humans (4 females, 25-42 years, mean age: 30.7 years) participated in the eye tracking experiment. Among these, five individuals, along with eight additional subjects (4 females, 26-45 years), took part in the fMRI experiment.”</p><disp-quote content-type="editor-comment"><p>17. The method should state whether the subjects had experienced these animations before (e.g., they're shown in some psychology and neuroscience classes).</p></disp-quote><p>We agree that detailing whether subjects had previous exposure to the animations is essential for the study's integrity. As a result, we have incorporated the following statement into the Methods section on page 22:</p><p>Page 22: “Importantly, all subjects confirmed they had not previously been exposed to the Frith-Happé animation videos used in our study.”</p><disp-quote content-type="editor-comment"><p>18. The description of the monkey reward contingencies needs to be clearer about whether the monkeys were rewarded only during calibration or during videos as well, and whether any reward during videos was contingent on keeping their eyes on the screen.</p></disp-quote><p>We appreciate the reviewer's suggestion to provide additional clarification on the reward contingencies for the monkey in our study. The monkeys received rewards only at the initial and final stages of each session, but not during the calibration or the viewing of the videos. Consequently, we have updated the text in the Methods section, now found on page 23, as follows:</p><p>“Monkeys were rewarded at the beginning and end of each session. Crucially, no rewards were provided during the calibration or while the videos were played.”</p><disp-quote content-type="editor-comment"><p>19. Because this is a social task when the scans were normalized to MNI space, did the authors divide the human participants into those with and without a paracingulate sulcus?</p></disp-quote><p>We appreciate the reviewer's insightful comment. While we normalized our MRI scans to MNI space in this study, we did not differentiate among participants based on the presence or absence of a paracingulate sulcus. The reviewer’s suggestion to consider this factor into account in our analyses is indeed valuable and will be considered in our future studies involving a larger pool of participants.</p><disp-quote content-type="editor-comment"><p>20. The authors need to better specify what counts as a &quot;baseline&quot; for the fMRI comparisons. They should also briefly justify why this is an informative comparison.</p></disp-quote><p>We agree that the definition and justification for our selected &quot;baseline&quot; in the fMRI comparisons should be more explicit. In our study, the &quot;baseline&quot; denotes brain activity when subjects are in a 'resting state' – a state of neutral alertness – specifically during the presentation of a circular black cue between video clips. The comparison to this baseline is valuable because it allows us to isolate and constrast brain activity associated with task-specific conditions, such as ToM or Random animations. We have added a more detailed explanation in the Methods section on page 30 of the revised manuscript. It now reads:</p><p>“First, we identified brain regions involved in the processing of ToM and Random animations by contrasting each condition with a baseline (i.e., ToM condition &gt; baseline and Random condition &gt; baseline contrasts). This baseline brain activation recorded during the presentation of the circular black cue between video clips (i.e., baseline blocks of 15 sec, see above), reflects 'resting state' activation. By comparing it to the brain activation during ToM and Random animations, we could specifically highlight the task-related activations and isolate brain regions engaged during each condition.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I very much enjoyed reading this manuscript and believe it has the potential to make an important contribution to animal literature as well as social cognition more broadly.</p><p>As highlighted in the public review, my main query is in relation to the omission of the Goal-Directed condition of the Frith-Happé task. While the evidence presented here certainly suggests that marmosets process ToM animations in a different manner than Random animations, we are somewhat constrained in what we can interpret from these findings. As the authors note, we cannot necessarily conclude that the marmosets are engaged in mental state attribution on the basis of these brain activation patterns.</p></disp-quote><p>We are grateful for the reviewer's encouraging words, thoughtful evaluation, and constructive comments on our manuscript. We agree with the reviewer's remarks regarding the omission of the Goal-Directed condition in the Frith-Happé task and recognize the interpretative constraints this places on our findings. We have attempted to address all points in both our responses below and in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>A more compelling argument would stem from the inclusion of the Goal-Directed condition in which the triangles arguably do interact but in a purely physical manner, i.e., there is no mental state attribution. I was surprised that this condition was not included as its omission somewhat limits the extent to which any conclusion regarding ToM can be drawn. Could the activations observed in the ToM condition reflect the processing of an event or narrative as it unfolds, rather than the cycling of random movements in the Random condition? I ask this question as previous studies using the Frith-Happé animations in dementia populations note that the mental state attribution judgements on ToM trials were conferred only at the end of the video (i.e., once the overall event narrative had been seen) whereas patients were adept at conferring a judgment of &quot;no interaction&quot; early during the viewing of Random animations (Ref: Synn et al. 2018 J. Alz Dis).</p></disp-quote><p>We appreciate the reviewer's insightful comment concerning the absence of the Goal-Directed (GD) condition in our study. We understand that integrating this condition could have offered a valuable contrast and enriched our understanding of the associated processing mechanisms.</p><p>In our study, our initial strategy was to concentrate on the two extreme conditions: ToM and Random animations. These represent scenarios with mental interactions and scenarios with absence of mental interactions, respectively. This choice was influenced by some previous fMRI studies using the Frith-Happé animated triangles task in humans and macaques, which primarily focused on these two conditions (Gobbini et al., 2007; Barch et al., 2013; Bliksted et al., 2019; Vandewouw et al., 2021; Weiss et al., 2021; Chen et al., 2023; Roumazeilles et al., 2021). Additionally, the duration of each video clip (19.5 sec) posed practical challenges in incorporating all the conditions with a sufficient number of repetitions in the fMRI task design for marmoset subjects. It was crucial for us to ensure that the subjects remained alert and focused throughout the entire scanning session, which becomes increasingly difficult with longer runs. Consequently, we chose to center our attention on the ToM and Random conditions, as the GD condition is situated between these two extremes, depicting physical interaction among the triangles without suggesting mental state attribution.</p><p>Nevertheless, we recognize the potential limitations of not incorporating the GD condition and the possible insights it might offer. In response to the reviewer's feedback, we conducted an additional eye-tracking experiment that included all three conditions: ToM, GD, and Random. This experiment involved 11 human subjects and 11 marmosets, with all ToM, GD and Random video clips presented once in a single run. The results from this experiment provided additional insights into the gaze patterns during the different conditions, complementing our initial findings.</p><p>We have updated the manuscript to clarify the choice of two conditions for the fMRI experiment and to incorporate the findings from the new eye-tracking experiment. The relevant sections in the methods, results, and discussion have been modified accordingly. These revisions can be found on pages 23-24, 5 to 7, and 14 to 20, respectively.</p><p>The reviewer's insightful question regarding whether the observed activations in the ToM condition might simply reflect the processing of an event or narrative as it unfolds, rather than mental state attribution, is an important consideration. We understand from the current literature, including the Synn et al. (2018) study mentioned by the reviewer, that distinguishing between these two processes can be challenging, particularly given the dynamic nature of the stimuli used. The ToM condition intrinsically involves the progression of an event or narrative, which is necessary for subjects to infer the mental states of the characters.</p><p>However, we believe our results provide evidence of the specific involvement of certain brain regions in mental state attribution. The enhanced activation of certain brain regions (e.g., TPJ and STS) during the ToM condition compared to Random condition aligns with several prior fMRI studies (Barch et al., 2013; Castelli et al., 2000; Chen et al., 2023; Gobbini et al., 2007; Vandewouw et al., 2021; Weiss et al., 2021; Wheatley et al., 2007). This suggests our observed activations may extend beyond merely event or narrative processing. For marmosets, it's more challenging to make definitive conclusions as no previous fMRI studies have used the same animations. However, the new eye-tracking experiment results show marmosets spend more time focused on the red triangle in ToM videos but allocate similar minimal attention to the red triangle in both Random and GD videos. This suggests that marmosets process the Random and GD conditions similarly, but differently for ToM animations that represent mental interactions. Nevertheless, conclusive interpretation remains challenging, and this is indeed a matter that warrants further exploration. We appreciate the reviewer's critical perspective on this aspect of our study.</p><disp-quote content-type="editor-comment"><p>I wonder whether this interpretation might also reflect the curious finding of stronger medial PFC activation in Random trials versus ToM trials in humans, and no clear mPFC activation in the ToM trials. This seems very much at odds with the wider literature on the brain regions necessary for ToM, which often place the medial PFC at the heart of the social brain.</p></disp-quote><p>We appreciate the reviewer's observation concerning the surprising patterns of activation in the medial prefrontal cortex (mPFC) in our study. While many studies have indeed associated mPFC with ToM tasks, our findings of stronger mPFC activation during Random animations compared to ToM animations in humans, and the lack of clear mPFC activation in ToM trials, appear to diverge from the wider literature.</p><p>We would like to highlight that the role of mPFC in ToM may be more nuanced. For instance, a recent meta-analysis conducted by Schurz et al. (2021) demonstrated that social animation tasks, such as the Frith-Happé animated triangles task we used, tend to engage an intermediate cluster between cognitive and affective clusters. This cluster involves a variety of brain regions, including temporo-parietal areas, anterior temporal areas, dorso-posterior medial prefrontal cortex, and inferior frontal areas. This suggests that such tasks may not uniformly engage the entirety of the mPFC and might involve more the dorsal part.</p><p>Our study revealed stronger activations in the ventral part of the mPFC for Random versus ToM animations. These activations could reflect other processes, such as attentional control. Notably, our results align closely with those of the Human Connectome Project by Barch et al. (2013), who also observed stronger activations for Random versus ToM animations in similar ventral parts of the mPFC.</p><p>These observations underscore the complexity of the neural substrates of ToM and the potential influence of task designs on the patterns of brain activation. We concur that further research is needed to fully understand these complex issues, and we greatly appreciate the reviewer's contribution to this ongoing discussion.</p><p>While previous fMRI studies using the Frith-Happé task have found dorsal mPFC activation for the ToM animations, we did not observe a clear activation pattern in ToM trials in our study. This discrepancy could be attributable to a variety of factors. Even minor differences in task design or the specific versions of the animations used could lead to different cognitive processes being engaged during the task. The methodological aspects, such as statistical power, could also have contributed to the differences in our findings. Furthermore, the number of participants can affect the resulting brain activation patterns.</p><p>We added this explanation concerning the possible issues in the Discussion section on page 17, which now read:</p><p>“Our slightly adapted versions of the Frith-Happé animations led to a similar distinct pattern of brain activations, with an exception for the lack of activations in the dorsal part of the medial prefrontal cortex. This discrepancy could be attributable to various factors, including differences in task design, methodological aspects such as statistical power, or variations in participants characteristics.”</p><disp-quote content-type="editor-comment"><p>I very much appreciated the check using the independent HCP dataset. This was a very nice inclusion to ensure that the shortened version corresponded well with previous reports.</p></disp-quote><p>We are pleased to hear that the reviewer appreciates our use of the independent HCP dataset to validate our results. We thank the reviewer for the positive feedback on this aspect of our study.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In their study, Dureux and colleagues are investigating the sensitivity of a highly social non-human primate species, the marmoset, to social illusion using the Frith and Happe task. Although this task is often considered a non-verbal TOM task, its relevance to investigate TOM has been disputed. For instance, the Frith and Happe task does not recruit in humans a similar network as other false-belief tasks and social gambling tasks (Schurz et al., 2020). While the authors might want to revise, or at least discuss their use of the TOM concept further, their results clearly show that marmosets distinguish the two types of videos shown to them.</p></disp-quote><p>We are grateful for the reviewer's thoughtful comments and critique. We appreciate the thoughtful reference to the study by Schurz et al. (2020). Indeed, as the reviewer has rightly highlighted, the Frith-Happé task has been debated for its relevancy to investigate ToM, given the divergence in neural networks it recruits compared to other ToM tasks, such as false-belief tasks and social gambling tasks, as demonstrated by Schurz et al. (2020).</p><p>In light of these insightful comments, we have revised our manuscript to acknowledge these differences and argue that the Frith-Happé task still holds value in the study of social cognition, albeit with a potential focus on different facets of this complex construct compared to more traditional ToM tasks. This understanding is reflected in our findings, showing that both humans and marmosets can distinguish between the types of videos in the Frith-Happé task.</p><p>The revised sections in our manuscript addressing these issues now read:</p><p>Page 17: “Overall, the ToM network we identified, as well as that reported by Barch et al. (2013), appear to be more extensive than those described in studies employing more complex experimental paradigms to study ToM. This aligns with the recent meta-analysis conducted by Schurz and colleagues (2020), which demonstrated that the network activated by simpler, non-verbal stimuli like social animations differs from the traditional network, with involvement of both cognitive and affective networks (Schurz et al., 2020).”</p><p>Page 20: “As such, future research including a range of tasks, from sensory-affective components to more abstract and decoupled representations of others' mental states (Schurz et al., 2020), will be fundamental in further unravelling the complexities of the evolution and functioning of the theory of mind across the primate lineage.”</p><p>We believe these revisions provide a more nuanced understanding of our study within the larger context of ToM research, and we thank the reviewer for prompting this important discussion. We address all the other points raised, below and in the manuscript.</p><disp-quote content-type="editor-comment"><p>Were there any differences between the different types of videos used? For instance, was there any difference between videos in which the interaction between the two shapes is more obvious (e.g. coaxing vs seducing).</p></disp-quote><p>We appreciate the reviewer's interest in the distinct types of videos used in our study. In this investigation, we did not specifically analyze the responses to videos where the interaction between the two shapes was more or less pronounced, such as in coaxing versus seducing scenarios. Our primary focus was on contrasting the overall responses elicited by Theory of Mind (ToM) animations and Random animations. Due to the design of our experiment, we did not include a sufficient number of repetitions for each distinct type of video within each condition to afford the statistical power necessary for such a comparison. We acknowledge that assessing responses to different types of ToM animations may provide additional insights into the specificity of neural responses and consider this an interesting avenue for future research.</p><disp-quote content-type="editor-comment"><p>Did the authors also present social videos to their animals? If so did they also observe additional recruitment of IPa and TPO areas like Clery and colleagues for social videos compared to Frith and Happe's videos (Cléry et al., 2021)?</p></disp-quote><p>In this study, our main focus was to examine the neural responses to the Frith-Happé animations, which represent abstract social scenarios. As such, we did not include realistic social videos to our animals in our current experimental design.</p><p>The study by Cléry et al. (2021) indeed demonstrates that the comparison social versus non-social realistic videos predominantly revealed a fronto-parietal network with additional temporal region engagement. The social condition in their study seems to recruit not only several areas that we observed to be activated in our ToM condition, but also additional temporal regions such as the IPa, TPO and TE1 areas. This suggests that these areas may play a significant role in the processing of more realistic social cues, thereby adding complexity to the social brain network.</p><p>Although our current study did not directly investigate this aspect, we agree that comparing the neural responses elicited by abstract versus realistic social scenarios in marmosets could provide valuable insights into the extent and adaptability of the social brain network. This approach would further our understanding of the neural substrates that underpin various facets of social cognition, depending on the complexity and realism of the presented stimuli. In future research, we plan to consider including such comparative investigations in our experimental design. We appreciate the reviewer's suggestion.</p><disp-quote content-type="editor-comment"><p>Humans and marmosets recruit a distinct set of subcortical structures during the viewing of video clips; for instance dorsal thalamus and cerebellum in humans, and the hippocampus and amygdala in marmosets. How do the authors interpret this difference?</p></disp-quote><p>We appreciate the reviewer for pointing out this difference in subcortical recruitment between humans and marmosets during the viewing of ToM versus Random video clips. The divergent patterns may reflect species-specific aspects of social cognition.</p><p>In humans, the involvement of the dorsal thalamus, which serves as a critical hub for information relay between various subcortical areas and the cortex, may indicate the necessity for complex information processing in interpreting the animations (e.g., Halassa and Sherman, 2019). The activation of the cerebellum, beyond its traditional role in motor functions, supports recent findings of its involvement in social cognition (e.g., Van Overwalle et al., 2014). The activations in a small portion of the amygdala may reflect emotional processing tied to understanding of the social scenarios in the animations (e.g., Janak and Tye, 2015).</p><p>On the other hand, the activation of the hippocampus and amygdala in marmosets might reflect a more emotion-driven interpretation of the animations. The recruitment of the hippocampus could suggest the role of memory in interpreting the animations by remembering past interactions to help interpret current social scenarios (e.g., Eichenbaum, 2017), while the more extended activation of the amygdala in marmosets might imply a higher degree of emotional processing compared to humans (e.g., Janak and Tye, 2015).</p><p>We have now expanded our discussion on this topic in the revised manuscript, specifically on pages 18-19:</p><p>&quot;Regarding the distinct subcortical activations observed in humans and marmosets, it's important to consider the specific social cognitive demands that might be unique to each species. The involvement of the dorsal thalamus, cerebellum, and a small portion of the amygdala in humans may reflect the complexities of information processing, social cognition, and emotional involvement required to interpret the ToM animations (e.g., Halassa and Sherman, 2019; Janak and Tye, 2015; Van Overwalle et al., 2014). Conversely, the activation of the amygdala and hippocampus in marmosets could suggest a more emotion- and memory-based processing of the social stimuli (e.g., Eichenbaum, 2017; Van Overwalle et al., 2014). However, it's critical to consider that these interpretations are speculative and would require further study for confirmation.”</p><disp-quote content-type="editor-comment"><p>Unlike marmosets, rhesus macaques are not sensitive to the type of the Frith and Happe social illusion (Roumazeilles et al., 2021; Schafroth et al., 2021). The authors might want to discuss the singularity of the marmosets from an evolutionary perspective.</p></disp-quote><p>As suggested, we have now discussed the singularity of the marmosets from an evolutionary perspective in the Discussion section on pages 19-20, which reads:</p><p>“Interestingly, our results differed from those obtained by Roumazeilles et al. (2021) in their fMRI study conducted in macaques using the same animations. Roumazeilles and colleagues reported no differences in activation between ToM and Random animations, suggesting that rhesus macaques may not respond to the social cues presented by the ToM Frith-Happé animations. This disparity between our marmoset findings and those of macaques raises intriguing questions about potential differences in the evolutionary development of ToM processing within non-human primates. Marmosets, as New World monkeys, are part of an evolutionary lineage that diverged earlier than the lineage of Old-World monkeys such as macaques. This difference in lineage might lead to distinct evolutionary trajectories in cognitive processing, which could include varying sensitivity to abstract social cues in animations.</p><p>In summary, our study reveals novel insights into how New World marmosets, akin to humans, differentially process abstract animations that depict complex social interactions and animations that display purely physical or random movements. Our findings, supported by both specific gaze behaviors (i.e., the proportion of time spent on the red triangle, despite the inconclusiveness of overall fixation) and distinct neural activation patterns, shed light on the marmosets' capacity to interpret social cues embedded in these animations.</p><p>The differences observed between humans, marmosets, and macaques underscore the diverse cognitive strategies that primate species have evolved to decipher social information. This diversity may be influenced by unique evolutionary pressures that arise from varying social structures and lifestyles. Like macaque monkeys, humans often live in large, hierarchically organized social groups where status influences access to resources. However, both humans and marmosets share a common trait: a high degree of cooperative care for offspring within the group, with individuals other than the biological parents participating in child-rearing. These distinctive social dynamics of marmosets and humans may have driven the development of unique social cognitive abilities. This could explain their enhanced sensitivity to abstract social cues in the Frith-Happé animations.</p><p>Nonetheless, it is crucial to emphasize that even though marmosets respond to the social cues in the Frith-Happé animations, this does not automatically imply that they possess mental-state attributions comparable to humans. As such, future research including a range of tasks, from sensory-affective components to more abstract and decoupled representations of others' mental states (Schurz et al., 2020), will be fundamental in further unravelling the complexities of the evolution and functioning of the theory of mind across the primate lineage.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This study is strong in many ways, and the goal is a good one. The below recommendations will help strengthen it further:</p></disp-quote><p>We are truly appreciative of the reviewer's thorough assessment and constructive feedback on our manuscript. We have addressed all raised points both in our responses below and in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>In the abstract, the claim that there is no evidence that nonhuman primates attribute mental states to moving shapes is false. You even cite some of this positive evidence (e.g., Uller, 2004; Atsumi et al., 2015; 2017). There is also evidence that they don't (Kupferberg et al., 2013; Burkart et al., 2012; Schafroth et al., 2021). The abstract would be stronger if written to represent the state of the field more accurately.</p></disp-quote><p>We thank the reviewer for drawing our attention to this. We've updated the abstract to reflect the current state of the field and our findings more accurately. Please refer to page 2 for the revised version.</p><p>Page 2: “Theory of Mind (ToM) refers to the cognitive ability to attribute mental states to other individuals. This ability extends even to the attribution of mental states to animations featuring simple geometric shapes, such as the Frith-Happé animations in which two triangles move either purposelessly (Random condition), exhibit purely physical movement (Goal-directed condition), or move as if one triangle is reacting to the other triangle’s mental states (ToM condition). While this capacity in humans has been thoroughly established, research on nonhuman primates has yielded inconsistent results.</p><p>This study explored how marmosets (Callithrix jacchus), a highly social primate species, process Frith-Happé animations by examining gaze patterns and brain activations of marmosets and humans as they observed these animations. We revealed that both marmosets and humans exhibited longer fixations on one of the triangles in ToM animations, compared to other conditions. However, we did not observe the same pattern of longer overall fixation duration on the ToM animations in marmosets as identified in humans. Furthermore, our findings reveal that both species activated extensive and comparable brain networks when viewing ToM versus Random animations, suggesting that marmosets differentiate between these scenarios similarly to humans. While marmosets did not mimic human overall fixation patterns, their gaze behavior and neural activations indicate a distinction between ToM and non-ToM scenarios. This study expands our understanding of nonhuman primate cognitive abilities, shedding light on potential similarities and differences in ToM processing between marmosets and humans.”</p><disp-quote content-type="editor-comment"><p>The overall conclusion as stated in the abstract, at the end of the introduction, and in the discussion is not warranted by the evidence. Indeed, the abstract completely fails to mention that the marmosets failed to show the human-like pattern of longer fixations on the ToM videos. Many readers will likely interpret this evidence as primarily against the idea that marmosets view the ToM videos in a human-like way, or as equivocal evidence at best. This report will be a stronger piece of science if it accurately describes the results.</p></disp-quote><p>We agree with the reviewer's comment regarding the absence of certain result descriptions in the abstract, introduction, and discussion. In response to this feedback, we have conducted major revisions in these three sections, adding the missing information and elaborating on the overall conclusion. The primary changes can be found on page 2 (abstract), pages 4-5 (introduction), and pages 14 to 20 (Discussion). The revised content reads as follows:</p><p>Page 2: “Theory of Mind (ToM) refers to the cognitive ability to attribute mental states to other individuals. This ability extends even to the attribution of mental states to animations featuring simple geometric shapes, such as the Frith-Happé animations in which two triangles move either purposelessly (Random condition), exhibit purely physical movement (Goal-directed condition), or move as if one triangle is reacting to the other triangle’s mental states (ToM condition). While this capacity in humans has been thoroughly established, research on nonhuman primates has yielded inconsistent results.</p><p>This study explored how marmosets (Callithrix jacchus), a highly social primate species, process Frith-Happé animations by examining gaze patterns and brain activations of marmosets and humans as they observed these animations. We revealed that both marmosets and humans exhibited longer fixations on one of the triangles in ToM animations, compared to other conditions. However, we did not observe the same pattern of longer overall fixation duration on the ToM animations in marmosets as identified in humans. Furthermore, our findings reveal that both species activated extensive and comparable brain networks when viewing ToM versus Random animations, suggesting that marmosets differentiate between these scenarios similarly to humans. While marmosets did not mimic human overall fixation patterns, their gaze behavior and neural activations indicate a distinction between ToM and non-ToM scenarios. This study expands our understanding of nonhuman primate cognitive abilities, shedding light on potential similarities and differences in ToM processing between marmosets and humans.”</p><p>Pages 4-5: “Although the spontaneous attribution of mental states to moving shapes has been well established in humans, it remains uncertain whether other primate species share this capacity. There is some evidence suggesting that monkeys can attribute goals to agents with varying levels of similarity and familiarity to conspecifics, including human agents, monkey robots, moving geometric boxes, animated shapes, and simple moving dots (Atsumi et al., 2017; Atsumi and Nagasaka, 2015; Krupenye and Hare, 2018; Kupferberg et al., 2013; Uller, 2004). However, the findings in this area are somewhat mixed, with some studies investigating the attribution of goals to inanimate moving objects yielding inconclusive results (Atsumi and Nagasaka, 2015; Kupferberg et al., 2013). Nonhuman primates' spontaneous attribution of mental states to Frith-Happé animations is even less certain. While human subjects exhibit longer eye fixations when viewing the ToM condition compared to the Random condition of the Frith-Happé animations (Klein et al., 2009), a recent eye tracking study in macaque monkeys did not observe similar differences (Schafroth et al., 2021). Similarly, a recent fMRI study conducted on macaques found no discernible differences in activations between ToM and random Frith-Happé animations (Roumazeilles et al., 2021).</p><p>In this study, we investigated the behaviour and brain activations of New World common marmoset monkeys (Callithrix jacchus) while they viewed Frith-Happé animations. Living in closely-knit family groups, marmosets exhibit significant social parallels with humans, including prosocial behavior, imitation, and cooperative breeding. These characteristics establish them as a promising nonhuman primate model for investigating social cognition (Burkart et al., 2009; Burkart and Finkenwirth, 2015; Miller et al., 2016). To directly compare humans and marmosets in their response to these animations, we employed high-speed video eye-tracking to record eye movements in eleven healthy humans and eleven marmoset monkeys. Additionally, we conducted ultra-high field fMRI scans on ten healthy humans at 7T and six common marmoset monkeys at 9.4T. These combined methods allowed us to examine the visual behavior and brain activations of both species while they observed the Frith-Happé animations.”</p><p>Pages 14-15: “In our first experiment, we examined the gaze patterns of marmosets and humans during the viewing of these video animations. Klein et al. (2009) reported differing fixation durations for these animations, where the longest fixations were observed for ToM animations, followed by GD animations and the shortest fixations for Random animations. They further reported that the intentionality score – derived from verbal descriptions of the animations – followed a similar pattern: highest for ToM, lowest for Random, and intermediate for GD animations. This validated the degree of mental state attribution according to the categories and established that animations provoking mentalizing (ToM condition) were associated with long fixations. This, in turn, supports the use of fixation durations as a nonverbal metric for mentalizing capacity (Klein et al., 2009; Meijering et al., 2012). Our results with human subjects, which demonstrated longer fixation durations for the ToM animations compared to the GD and Random animations, paralleled those of Klein et al. (2009). However, unlike Klein et al.'s findings, we did not observe intermediate durations for GD animations in our study.</p><p>Interestingly, our marmoset data did not align with the human findings but instead resonated more with Schafroth et al. (2021)'s observations in macaque monkeys, which did not show significant differences in fixation durations across the three animation types.”</p><p>Pages 19-20: “In summary, our study reveals novel insights into how New World marmosets, akin to humans, differentially process abstract animations that depict complex social interactions and animations that display purely physical or random movements. Our findings, supported by both specific gaze behaviors (i.e., the proportion of time spent on the red triangle, despite the inconclusiveness of overall fixation) and distinct neural activation patterns, shed light on the marmosets' capacity to interpret social cues embedded in these animations.</p><p>The differences observed between humans, marmosets, and macaques underscore the diverse cognitive strategies that primate species have evolved to decipher social information. This diversity may be influenced by unique evolutionary pressures that arise from varying social structures and lifestyles. Like macaque monkeys, humans often live in large, hierarchically organized social groups where status influences access to resources. However, both humans and marmosets share a common trait: a high degree of cooperative care for offspring within the group, with individuals other than the biological parents participating in child-rearing. These distinctive social dynamics of marmosets and humans may have driven the development of unique social cognitive abilities. This could explain their enhanced sensitivity to abstract social cues in the Frith-Happé animations.</p><p>Nonetheless, it is crucial to emphasize that even though marmosets respond to the social cues in the Frith-Happé animations, this does not automatically imply that they possess mental-state attributions comparable to humans. As such, future research including a range of tasks, from sensory-affective components to more abstract and decoupled representations of others' mental states (Schurz et al., 2020), will be fundamental in further unravelling the complexities of the evolution and functioning of the theory of mind across the primate lineage.”</p><disp-quote content-type="editor-comment"><p>The justification for looking in marmosets could be read to imply that macaque monkeys do not live in family groups or share important social similarities with humans. Both species share many social similarities (and many social differences) with humans. Marmosets are a good species to study; this section would benefit from a more accurate rationale.</p></disp-quote><p>We apologize for any confusion our previous wording may have caused. We have now revised the sentence on page 4 to enhance its clarity and accuracy, which now reads:</p><p>Page 4: “Living in closely-knit family groups, marmosets exhibit significant social parallels with humans, including prosocial behavior, imitation, and cooperative breeding. These characteristics establish them as a promising nonhuman primate model for investigating social cognition (Burkart et al., 2009; Burkart and Finkenwirth, 2015; Miller et al., 2016).”</p><disp-quote content-type="editor-comment"><p>Because it is one of the main metrics in the Klein and Schafroth papers, and thus readers will want to see it for sake of comparison, the authors should include a figure showing the overall fixation durations as a function of category and species.</p></disp-quote><p>In response to the reviewer’s suggestion, we have added a new figure (Figure 2) on page 39, which presents the overall fixation durations as a function of both animation category and species. This figure provides a direct comparison of fixation durations between humans and marmosets across the different animation conditions (Random, Goal-directed, and ToM). We believe this additional visualization will assist readers in better understanding the overall durations of fixation across species and conditions and enable direct comparison with the findings of Klein and Schafroth.</p><disp-quote content-type="editor-comment"><p>The results about looking time to the large triangle need to follow up on the interaction between species and conditions so that readers know how to interpret it.</p></disp-quote><p>As we have now included the Goal-directed condition in our eye-tracking experiment, we have substantially revised the section “Gaze patterns for Frith-Happé’s ToM, GD and Random animations in humans and marmosets” in the results (pages 5 to 7). In response to the reviewer's comments, we have provided a more detailed analysis of the interaction between species and conditions regarding the looking time spent on the large triangle, which now reads as follows:</p><p>Pages 6-7: “To further analyze the gaze patterns of both humans and marmosets, we next measured the proportion of time subjects looked at each of the triangles in the videos (Figure 2B). We conducted mixed ANOVAs on the proportion of time the radial distance between the current gaze position and each triangle was within 4 visual degrees for each triangle separately.</p><p>Importantly, we observed a significant interaction between species and condition for the proportion of time spent looking at the large red triangle (F(2,40)=9.83, p&lt;.001, ηp2 = .330). Specifically, both humans (Figure 2B left) and marmosets (Figure 2B right) spent a greater proportion of time looking at the red triangle in ToM compared to the GD and Random videos (For humans, ToM vs GD: Δ=.23, p&lt;.001 and ToM vs Random: Δ=.31, p&lt;.001 ; For marmosets, ToM vs GD: Δ=.13, p&lt;.01 and ToM vs Random: Δ=.13, p&lt;.01). However, while humans also allocated a greater proportion of time to the red triangle in GD compared to Random animations (Δ=.08, p=.05), marmosets did not show any difference between these two conditions (Δ=.0003, p=1).</p><p>For the small blue triangle, we also observed a significant interaction of species and condition (F(2,40)=3.54, p=.04, ηp2=.151) but the comparisons were not resistant to the p value adjustment by Bonferroni correction. Therefore, humans and marmosets spent the same proportion of time looking at the blue triangle in the three different types of videos (For humans, ToM vs GD: Δ=-.02, p=1, ToM vs Random: Δ=.04, p=1 and GD vs Random: Δ=.07, p=.23 ; For marmosets, ToM vs GD: Δ=-.05, p=.89, ToM vs Random: Δ=.07, p=.66 and GD vs Random: Δ=-.02, p=1; Figure 2B).</p><p>These results highlight the variation in gaze patterns observed in both humans and marmosets when their focus is directed towards the large red triangle during the viewing of ToM, GD, and Random videos. Notably, humans show a gradient of proportion of time spent looking at the red triangle across the three conditions, with the smallest proportion in Random videos and the greatest proportion in ToM videos. In contrast, marmosets exhibit a different pattern, spending more time looking at the red triangle in ToM videos, but allocating the same proportion of time to look at the red triangle in both Random and GD videos. This finding suggests that while humans demonstrate distinct attentional preferences for the red triangle across the three conditions, marmosets exhibit a similar attentional focus on the red triangle in the Random and GD conditions, but their pattern differs in the ToM condition. This suggests that marmosets process the Random and GD conditions in a similar manner, but their processing of the ToM condition is distinct, indicating a differential response to stimuli representing social interactions.”</p><disp-quote content-type="editor-comment"><p>The sentence on lines 97-99 might be an incomplete sentence.</p></disp-quote><p>We appreciate the reviewer's attention to detail and acknowledge the oversight in the sentence structure on lines 97-99. We have revised this sentence and the entire paragraph on pages 5 to 7, under the heading &quot;Gaze patterns for Frith-Happé’s ToM, GD and Random animations in humans and marmosets&quot;. This revision takes into account the new results obtained after adding the Goal-Directed condition to the experiment.</p><disp-quote content-type="editor-comment"><p>Are the bars in Figure 2 meant to add up to 1 for any given participant? If you analyzed the total time fixating on either shape, would marmosets be spending less time looking at the shapes overall than humans?</p></disp-quote><p>We thank the reviewer for the question. The values in the previous Figure 2 are not intended to add up to 1. This is because there are instances where the triangles overlap or are in close enough proximity that the eye position falls within the defined radius for both simultaneously. In response to the second question, we have conducted an analysis on the total time spent fixating on either shape. Our findings revealed a significant effect of species (<italic>F</italic><sub>(1,20)</sub>=14.38, <italic>p</italic>=.001, <italic>η<sub>p</sub><sup>2</sup></italic>=.42), indicating that humans tend to look at the triangles more frequently than marmosets (Δ=.16, <italic>p</italic>=.001).</p><disp-quote content-type="editor-comment"><p>Overall, the figures are quite informative and aesthetically pleasing.</p></disp-quote><p>We sincerely thank the reviewer for their positive remarks about the figures in our study.</p><disp-quote content-type="editor-comment"><p>HCP should be explained the first time it is used.</p></disp-quote><p>We agree with the reviewer's point that all abbreviations should be clearly explained when first introduced. We have now amended the text to clarify this at the first instance where HCP appears on page 10. We thank the reviewer for bringing this oversight to our attention.</p><disp-quote content-type="editor-comment"><p>Readers will likely want clarification in cases where the same area showed stronger activation for ToM videos AND Random videos. I assume it was in different voxels in the same larger area, but this could be explicit.</p></disp-quote><p>We appreciate the reviewer's suggestion and agree that clarity on this issue is essential.</p><p>We have now added a sentence on this point in the two relevant sections of fMRI results on humans and marmosets, on pages 7-8 and 11, to ensure this is explicitly stated and clear to the reader. This can be read:</p><p>Pages 7-8 (Functional brain activations while watching ToM and Random Frith-Happé’s animations in humans): “Both ToM (Figure 3A) and Random (Figure 3B) videos activated a large bilateral network. While the same larger areas were activated in both conditions, the specific voxels showing this activation within those areas were typically distinct. In some cases, both conditions activated the same voxels, but the degree of activation differed. This suggests a degree of both spatial and intensity variation in the activations for the two conditions within the same areas. (…)”</p><p>Page 11 (Functional brain activations while watching ToM and Random Frith-Happé’s animations in marmosets): “Both the ToM (Figure 4A) and Random (Figure 4B) animations activated an extensive network involving a variety of areas in the occipito-temporal, parietal and frontal regions. As in human subjects, it should be noted that while both conditions elicited strong activation in some of the same larger areas, these activations might have either occurred in distinct voxels within those areas, or the same voxels were activated to varying degrees for both conditions. This suggests distinct yet overlapping patterns of neural processing for the ToM and Random conditions.”</p><disp-quote content-type="editor-comment"><p>The claim that these maps represent &quot;dedicated brain networks&quot; for ToM or Random videos (line 188) is too strong. These brain areas are used for many things.</p></disp-quote><p>We agree with the reviewer's concern regarding the term &quot;dedicated brain networks&quot;. We understand that the use of this term could be misinterpreted as implying exclusivity, which is not the case. These areas are indeed involved in various cognitive functions. We have modified the statement on page 10 to indicate that these are &quot;brain networks activated during the processing of ToM or Random videos&quot; instead of &quot;dedicated brain networks&quot;. We appreciate this valuable input.</p><disp-quote content-type="editor-comment"><p>For many of the sentences in the imaging results, the comparison needs to be made explicit. For example Line 193 – higher bilateral activation than what? Line 196 – greater activations than what? Line 202 – a larger network than what? Etc.</p></disp-quote><p>We appreciate the reviewer's attention to detail in pointing out the need for clear and explicit comparisons in our imaging results. We recognize that some sentences may lack specificity, leading to potential confusion. We have now revised these sentences in the Results section to clearly specify the comparisons being made in each case. The updated descriptions can be found in the Results section, on pages 7 to 12.</p><disp-quote content-type="editor-comment"><p>The description of Klein et al., (2009) on Lines 289-293 might be read to imply that they were attributing mentalizing without good reason. Klein also collected intentionality scores, which correlated with the viewing metric. This could be rephrased to be more accurate.</p></disp-quote><p>Thank you for pointing out the potential misinterpretation of our description of Klein et al., 2009. Our intention was not to undermine the work by Klein et al. We have revised the phrasing in our manuscript, on page 15, to better reflect this aspect of their study:</p><p>Page 15: “In our first experiment, we examined the gaze patterns of marmosets and humans during the viewing of these video animations. Klein et al. (2009) reported differing fixation durations for these animations, where the longest fixations were observed for ToM animations, followed by GD animations and the shortest fixations for Random animations. They further reported that the intentionality score – derived from verbal descriptions of the animations – followed a similar pattern: highest for ToM, lowest for Random, and intermediate for GD animations. This validated the degree of mental state attribution according to the categories and established that animations provoking mentalizing (ToM condition) were associated with long fixations. This, in turn, supports the use of fixation durations as a nonverbal metric for mentalizing capacity (Klein et al., 2009; Meijering et al., 2012).”</p><disp-quote content-type="editor-comment"><p>In general, the discussion could be strengthened by avoiding repeating the results in as much detail.</p></disp-quote><p>We appreciate the reviewer's feedback and agree that a more concise discussion could make the manuscript more effective. We have revised the Discussion section to provide a more focused analysis and interpretation of the results, limiting repetition from the Results section.</p><p>The updated Discussion section can be found from page 14 to 20 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>The inclusion of the authors as subjects is odd. Some readers will view it as a big red flag. The authors clearly know their own hypothesis and likely have a vested interest in a particular outcome. For the strongest report, the authors should remove their own data. At the very least, the authors need to demonstrate that the inclusion/exclusion of their unblinded data doesn't affect the interpretation of the human results.</p></disp-quote><p>We appreciate the reviewer's concern about the potential bias introduced by the inclusion of authors as subjects in our study. Taking this into consideration, we have removed the data from the three authors who initially participated in the study. For the eye tracking experiment, we have now introduced a new “Goal-Directed condition” and conducted the experiment with eleven new subjects, none of whom are authors of this study. For the fMRI experiment, we replaced the data from the three author-subjects with data from three additional subjects who were not privy to the study's hypothesis.</p><p>Consequently, we replaced the previous results with these new findings and made the necessary modifications to several sections of the manuscript as well as on the figures 2, 3 and 5 and the figures supplement 1 and 2. These updates can be found in the sections: “Gaze patterns for Frith-Happé’s ToM, GD and Random animations in humans and marmosets” on pages 5 to 7, “Functional brain activations while watching ToM and Random Frith-Happé’s animations in humans” on pages 7 to 10, and “Comparison of functional brain activations in humans and marmosets” on pages 13-14. The updated figures can be found on pages 39, 40, 42, 43, and 44, respectively.</p><p>We have also updated the participant information in the methods section, on page 22, to now read:</p><p>“Eleven healthy humans (4 females, 25-42 years, mean age: 30.7 years) participated in the eye tracking experiment. Among these, five individuals, along with eight additional subjects (4 females, 26-45 years), took part in the fMRI experiment.”</p><disp-quote content-type="editor-comment"><p>The method should state whether the subjects had experienced these animations before (e.g., they're shown in some psychology and neuroscience classes).</p></disp-quote><p>Thank you for pointing out the necessity to include this information. We understand that the participants' previous exposure to these animations could potentially affect the results. We have added the following sentence to the Methods section:</p><p>Page 22: “Importantly, all subjects confirmed they had not previously been exposed to the Frith-Happé animation videos used in our study.”</p><disp-quote content-type="editor-comment"><p>If the authors proceed in pushing this data without the Goal-Directed videos, they need to at least address their rationale for not testing these videos.</p></disp-quote><p>We appreciate this important point, raised also by the first reviewer. As previously mentioned, our initial strategy was to focus on the two extreme conditions: ToM and Random, representing scenarios with and without mental interactions, respectively. This choice was influenced by some previous fMRI studies using the Frith-Happé animated triangles task in humans and macaques, which predominantly examined these two conditions (Gobbini et al., 2007; Barch et al., 2013; Bliksted et al., 2019; Vandewouw et al., 2021; Weiss et al., 2021; Chen et al., 2023; Roumazeilles et al., 2021). We also faced practical challenges in incorporating all conditions with a sufficient number of repetitions into the fMRI task design for marmoset subjects, given the substantial duration of each video clip (19.5 sec). As such, we chose to focus our investigation on the ToM and Random conditions, as the Goal-Directed (GD) condition falls between these two extremes, depicting physical interaction among the triangles without suggesting mental state attribution. Recognizing the potential limitations of not including the GD condition, we conducted an additional eye-tracking experiment that encompassed all three conditions.</p><p>We have updated the manuscript to clarify the choice of conditions for the fMRI experiment and to incorporate the findings from the new eye-tracking experiment. Relevant modifications have been made in the methods, results, and Discussion sections. These revisions can be found on pages 23-24, 5 to 7, and 14 to 20, respectively.</p><disp-quote content-type="editor-comment"><p>The description of the monkey reward contingencies needs to be clearer about whether the monkeys were rewarded only during calibration or during videos as well, and whether any reward during videos was contingent on keeping their eyes on the screen.</p></disp-quote><p>We apologize for any previous ambiguity in the text. It is important to clarify that the monkeys were rewarded solely during the initial and final stages of the sessions, and no rewards were administered during the calibration or the experiment. Accordingly, we have updated the description in the Methods section, now stated on page 23 as:</p><p>“Monkeys were rewarded at the beginning and end of each session. Crucially, no rewards were provided during the calibration or while the videos were played.”</p><disp-quote content-type="editor-comment"><p>Because this is a social task when the scans were normalized to MNI space, did the authors divide the human participants into those with and without a paracingulate sulcus?</p></disp-quote><p>We thank the reviewer for the insightful question. Indeed, in this study, we normalized the MRI scans of human participants to MNI space, providing a standardized representation of the brain. However, we did not separate participants based on the presence or absence of a paracingulate sulcus in our analysis. Your suggestion to incorporate this anatomical variability is intriguing, especially given its potential implications in social cognition research. We appreciate this thoughtful suggestion and will certainly consider it in our future studies.</p><disp-quote content-type="editor-comment"><p>The authors need to better specify what counts as a &quot;baseline&quot; for the fMRI comparisons. They should also briefly justify why this is an informative comparison.</p></disp-quote><p>Apologies for any confusion regarding our baseline condition in the fMRI comparisons. In this study, our baseline refers to the brain's activity when the subjects are not engaged in the tasks (i.e., viewing ToM or Random animations). More specifically, we have defined the baseline as the brain activity during the presentation of a circular black cue between video clips. Selecting this as the baseline is crucial as it presents a 'resting state' scenario – a state where the brain is not actively engaged in processing task-specific stimuli but is instead in a neutral, alert state. This choice of baseline allows us to identify and compare increased activity in different brain regions during the ToM and Random conditions relative to this resting state. This, in turn, aids our understanding of the specific functional brain regions associated with the processing of these specific conditions. We have now clarified this point in the Methods section of the manuscript on page 30. It now reads:</p><p>“First, we identified brain regions involved in the processing of ToM and Random animations by contrasting each condition with a baseline (i.e., ToM condition &gt; baseline and Random condition &gt; baseline contrasts). This baseline brain activation recorded during the presentation of the circular black cue between video clips (i.e., baseline blocks of 15 sec, see above), reflects 'resting state' activation. By comparing it to the brain activation during ToM and Random animations, we could specifically highlight the task-related activations and isolate brain regions engaged during each condition.”</p></body></sub-article></article>