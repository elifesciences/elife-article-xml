<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75688</article-id><article-id pub-id-type="doi">10.7554/eLife.75688</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Flexible utilization of spatial- and motor-based codes for the storage of visuo-spatial information</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-261335"><name><surname>Henderson</surname><given-names>Margaret M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9375-6680</contrib-id><email>mmhender@cmu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-262508"><name><surname>Rademaker</surname><given-names>Rosanne L</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-4966"><name><surname>Serences</surname><given-names>John T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>Neurosciences Graduate Program, University of California, San Diego</institution></institution-wrap><addr-line><named-content content-type="city">San Diego</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Department of Machine Learning, Carnegie Mellon University</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Neuroscience Institute, Carnegie Mellon University</institution></institution-wrap><addr-line><named-content content-type="city">Pittsburgh</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>Department of Psychology, University of California, San Diego</institution></institution-wrap><addr-line><named-content content-type="city">San Diego</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ygt2y02</institution-id><institution>Ernst Strüngmann Institute (ESI) for Neuroscience in Cooperation with Max Planck Society</institution></institution-wrap><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0168r3w48</institution-id><institution>Kavli Foundation for the Brain and Mind, University of California, San Diego</institution></institution-wrap><addr-line><named-content content-type="city">San Diego</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75688</elocation-id><history><date date-type="received" iso-8601-date="2021-11-19"><day>19</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-04-24"><day>24</day><month>04</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-07-09"><day>09</day><month>07</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.07.08.451663"/></event></pub-history><permissions><copyright-statement>© 2022, Henderson et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Henderson et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75688-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75688-figures-v1.pdf"/><abstract><p>Working memory provides flexible storage of information in service of upcoming behavioral goals. Some models propose specific fixed loci and mechanisms for the storage of visual information in working memory, such as sustained spiking in parietal and prefrontal cortex during working memory maintenance. An alternative view is that information can be remembered in a flexible format that best suits current behavioral goals. For example, remembered visual information might be stored in sensory areas for easier comparison to future sensory inputs, or might be re-coded into a more abstract action-oriented format and stored in motor areas. Here, we tested this hypothesis using a visuo-spatial working memory task where the required behavioral response was either known or unknown during the memory delay period. Using functional magnetic resonance imaging (fMRI) and multivariate decoding, we found that there was less information about remembered spatial position in early visual and parietal regions when the required response was known versus unknown. Furthermore, a representation of the planned motor action emerged in primary somatosensory, primary motor, and premotor cortex during the same task condition where spatial information was reduced in early visual cortex. These results suggest that the neural networks supporting working memory can be strategically reconfigured depending on specific behavioral requirements during a canonical visual working memory paradigm.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>working memory</kwd><kwd>vision</kwd><kwd>sensory recruitment</kwd><kwd>motor planning</kwd><kwd>fMRI</kwd><kwd>neural decoding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01-EY025872</award-id><principal-award-recipient><name><surname>Serences</surname><given-names>John T</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32-MH020002</award-id><principal-award-recipient><name><surname>Henderson</surname><given-names>Margaret M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>European Union Horizon 2020 Research and Innovation Program</institution></institution-wrap></funding-source><award-id>Marie Sklodowska-Curie Grant Agreement No 743941</award-id><principal-award-recipient><name><surname>Rademaker</surname><given-names>Rosanne L</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>MH087214</award-id><principal-award-recipient><name><surname>Serences</surname><given-names>John T</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>When human participants are able to plan responses in a visuo-spatial working memory task, sensory-like representations of remembered spatial position in early visual and parietal cortex adaptively trade off with motor-like representations of upcoming actions in sensorimotor cortex.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Working memory (WM) is thought to provide a flexible mental workspace which allows organisms to hold information in mind about past experiences and use it to guide future behavior (<xref ref-type="bibr" rid="bib4">Atkinson and Shiffrin, 1968</xref>; <xref ref-type="bibr" rid="bib6">Baddeley and Hitch, 1974</xref>; <xref ref-type="bibr" rid="bib49">Jonides et al., 2016</xref>). This system supports a wide range of cognitive tasks, each with its own specific demands and processing constraints. Due to these varied demands, it is likely that the neural mechanisms of WM are not universal across all tasks, but adaptively adjust to the requirements of the current situation. Among the many possible differences in task requirements, one key factor is the degree to which a task encourages a perceptual versus an action-oriented coding format. For instance, in tasks that require memory for fine visual details, such as searching for a specific kind of bird based on a picture in a guidebook, the best strategy might be to represent information in a format resembling past visual inputs (a perceptual, or ‘sensory-like’ code). Other tasks, such as remembering a series of directions for driving to your friend’s house, permit the use of multiple strategies. This driving task could be achieved using a perceptual coding strategy, such as maintaining a visuo-spatial representation of a street map. However, a better approach might be to re-code the visual information into another format, such as a series of motor plans for upcoming actions (an action-oriented, or ‘motor-like’ code). Such flexible re-coding of information from a more perceptual to a more action-oriented code could serve to reduce the dimensionality of representations when the correspondence between a remembered stimulus and a required action is known in advance. Thus, even tasks that are often thought to share a core component (e.g. memory in a visual format) might be accomplished via very different strategies and neural codes. Critically, past studies arguing that WM is supported by different neural loci often use tasks that differentially rely on perceptual versus action-oriented codes. As a result, evaluating claims about neural mechanisms is challenging without directly accounting for the potential impact of task demands in shaping how information is stored and used in WM.</p><p>The paradigms used in human neuroimaging experiments typically sit at the perceptual end of this continuum. In most fMRI studies of visual WM, participants are required to remember precise values of continuously varying features, and to report these remembered features using responses that cannot be pre-planned during the delay period (<xref ref-type="bibr" rid="bib1">Albers et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Christophel et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Ester et al., 2009</xref>; <xref ref-type="bibr" rid="bib40">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib59">Lorenc et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib76">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="bib96">Xing et al., 2013</xref>). Such tasks may encourage top-down recruitment of the same early sensory areas that support high-precision perceptual representations (<xref ref-type="bibr" rid="bib5">Awh and Jonides, 2001</xref>; <xref ref-type="bibr" rid="bib36">Gazzaley and Nobre, 2012</xref>; <xref ref-type="bibr" rid="bib68">Pasternak and Greenlee, 2005</xref>; <xref ref-type="bibr" rid="bib77">Serences, 2016</xref>; <xref ref-type="bibr" rid="bib85">Sreenivasan et al., 2014</xref>). Consistent with this idea of sensory recruitment, most studies find that patterns of voxel activation measured in visual cortex encode information about specific feature values held in memory, supporting the role of visual cortex in maintaining detailed visual representations during WM.</p><p>However, different neural loci are often implicated using visual WM tasks that enable action-oriented codes. One example is a memory-guided saccade task employed to study spatial WM using non-human primate (NHP) electrophysiological approaches. In these tasks, the position of a briefly presented cue is remembered, but that position is also predictive of the saccade that must be made at the end of the trial. Thus, the animal could solve the task by re-coding information from a spatial code into a motor code. Single unit recordings made during these tasks suggest an important role for the prefrontal cortex (PFC) in maintaining remembered information across brief delay periods (<xref ref-type="bibr" rid="bib33">Funahashi et al., 1989</xref>; <xref ref-type="bibr" rid="bib35">Fuster and Alexander, 1971</xref>; <xref ref-type="bibr" rid="bib38">Goldman-Rakic, 1995</xref>). Consistent with these findings, others have suggested that action-oriented WM in general may rely more heavily on areas involved in planning and motor production, and less on early sensory cortex (<xref ref-type="bibr" rid="bib9">Boettcher et al., 2021</xref>; <xref ref-type="bibr" rid="bib19">Curtis et al., 2004</xref>; <xref ref-type="bibr" rid="bib63">Myers et al., 2017</xref>). However, not all past studies have explicitly controlled the possibility for response-related remapping of information, making the boundary between perceptual and action-oriented codes unclear.</p><p>Based on the work discussed above, visual WM may be implemented quite flexibly, as opposed to having a singular neural locus or mechanism. Consistent with this idea, there are indications that even information related to the same stimulus might be stored at several different loci and in different formats (<xref ref-type="bibr" rid="bib44">Iamshchinina et al., 2021</xref>; <xref ref-type="bibr" rid="bib54">Lee et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib77">Serences, 2016</xref>). At the same time, few experiments have directly tested whether representations of memorized features in early visual cortex are subject to task modulation. Indeed, an alternative theory of WM storage suggests that memory-related signals in early sensory areas are epiphenomenal, in which case, task goals are not expected to affect the strength of representations in this part of the brain (<xref ref-type="bibr" rid="bib53">Leavitt et al., 2017</xref>; <xref ref-type="bibr" rid="bib98">Xu, 2020</xref>). Thus, our understanding of how behavioral requirements influence the memory networks and the codes that route information between early sensory and other brain regions is incomplete. In particular, it is not yet clear the extent to which perceptual and action-oriented codes may be jointly employed within the context of a single WM paradigm.</p><p>Here we study visuo-spatial WM to determine if the neural mechanisms supporting information storage are flexible in the face of changing behavioral requirements, or if, alternatively, the same mechanisms are recruited irrespective of the ability to employ different strategies. Participants performed a visuo-spatial WM task in which the required behavioral response on each trial was either not known until the end of the memory delay (‘uninformative’ condition), or known in advance of the memory delay (‘informative’ condition; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). This manipulation was intended to promote the use of perceptual (‘sensory-like’, spatial) and action-oriented (‘motor-like’) memory codes, respectively. We then compared representations of perceptual and action-related mnemonic information in early visual, parietal, and sensorimotor cortical regions.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task design, behavioral results, and univariate results.</title><p>(<bold>A</bold>) During each trial of the main working memory task, participants remembered the spatial position of a target dot presented at a random angular position (7° eccentricity from fixation). After a 16 s delay, participants made a binary response to indicate which half of a ‘response’ disk the target dot position had been presented on. At the beginning of the delay, a ‘preview’ disk was shown that either exactly matched the response disk (top row; ‘informative’) or had a random orientation relative to the response disk (bottom row; ‘uninformative’). In the example trial depicted, a participant in the informative condition would have pressed the button corresponding to the lighter gray side of the disk. By contrast, in the uninformative condition a participant would have pressed the button corresponding to the dark gray side, as only the final response disk was relevant to their response (see <italic>Methods, Task: Main working memory</italic> for more details). (<bold>B</bold>) Average behavioral accuracy (left) and response time (right) in the informative and uninformative conditions (individual participants are shown with gray lines). Error bars represent ±1 SEM across participants. Significance of condition differences was computed using paired t-tests (a single asterisk indicates p&lt;0.05, and three asterisks indicate p&lt;0.001). (<bold>C</bold>) In a separate spatial working memory mapping task, participants remembered a target dot position for 12 s and responded by moving a probe dot around an invisible circle to match the remembered position (see <italic>Methods, Task: Spatial working memory mapping</italic>). This mapping task was used to generate an independent dataset to train decoding models (see <italic>Methods, Analysis: Spatial position decoding</italic>). (<bold>D</bold>) Univariate hemodynamic response functions in three representative regions of interest from early visual cortex (V1), parietal cortex (IPS0), and motor cortex (M1) during the informative (dark blue) and uninformative (light blue) conditions of the main working memory task. Timepoint zero indicates the onset of the memory target. Shaded gray rectangles indicate the time periods when the ‘preview’ disk was onscreen (3.5–4.5 s) and when the response disk was onscreen (16.5–18.5 s). Shaded error bars represent ±1 SEM across participants. Gray dots indicate timepoints showing a significant condition difference (evaluated using a Wilcoxon signed-rank test with permutation, all p-values &lt;0.05; see <italic>Methods, Analysis: Univariate</italic> for details). This plot shows three representative ROIs, see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for data from all ROIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Hemodynamic response function in each region of interest (ROI) during the informative (dark blue) and uninformative (light blue) conditions, full set of ROIs.</title><p>Timepoint zero indicates the time of target onset; shaded gray rectangles indicate the time periods when the ‘preview’ disk was onscreen (3.5–4.5 s) and when the response disk was onscreen (16.5–18.5 s). Shaded error bars represent ±1 SEM across participants. Gray dots indicate timepoints showing a significant condition difference, evaluated using a Wilcoxon signed-rank test with permutation, see <italic>Methods, Analysis: Univariate</italic> for details.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Univariate responses in sensorimotor regions of interest (ROIs), separated by which finger was used to make a behavioral response.</title><p>(<bold>A</bold>) Informative condition. (<bold>B</bold>) Uninformative condition. For each voxel, hemodynamic response functions (HRFs) were computed separately for trials in each condition where the response finger was contralateral (blue) or ipsilateral (pink) relative to the voxel’s hemisphere. Contralateral and ipsilateral HRFs were then combined across all voxels in both hemispheres for this figure. Timepoint zero indicates the time of target onset; shaded gray rectangles indicate the time periods when the ‘preview’ disk was onscreen (3.5–4.5 s) and when the response disk was onscreen (16.5–18.5 s). Shaded error bars represent ±1 SEM across participants. Gray dots indicate timepoints showing a significant difference between contralateral and ipsilateral trials, evaluated using a Wilcoxon signed-rank test with permutation, see <italic>Methods, Analysis: Univariate</italic> for details.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig1-figsupp2-v1.tif"/></fig></fig-group><p>To preview, we found that information about remembered spatial positions in retinotopic visual cortex decreased when the required response was known in advance, in accordance with a decreased reliance on visual cortex for information storage when action-oriented coding could be utilized. Furthermore, this decrease was accompanied by the emergence of an action-oriented memory representation in somatosensory, motor, and premotor cortex. Cross-generalization of decoding from independent model-training tasks further supports a shift from ‘sensory-like’ to ‘motor-like’ when comparing the two task conditions. These results demonstrate that the neural networks supporting WM – even in the context of a single paradigm often used to study visuo-spatial WM – can be strategically reconfigured depending on task requirements.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>While in the magnetic resonance imaging (MRI) scanner, participants performed a spatial WM task in which they were required to remember the spatial position of a briefly presented target dot across a 16-s delay period. The small white target dot could appear anywhere along an invisible circle with radius of 7°. After the delay, participants reported the dot’s position by comparing their memory to a response probe – a disk with two halves (light and dark gray). They used their left or right index finger to indicate on which of the two halves of the disk the target dot had been presented (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We manipulated participants’ ability to pre-plan their motor action by presenting a ‘preview’ disk at the beginning of the delay period. The preview disk was preceded by a cue indicating whether the trial was part of the ‘informative’ or ‘uninformative’ condition. On informative trials<italic>,</italic> the preview disk matched the response disk, allowing participants to anticipate their button press at the end of the delay. On uninformative trials, the preview disk orientation was random with respect to that of the response disk, requiring that participants maintain a precise representation of the target dot position. Informative and uninformative trials were randomly intermixed throughout each run.</p><p>Task performance was overall better on trials where participants could plan their motor action in advance compared with trials when they could not (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Participants were significantly faster in the informative condition (informative mean ± SEM: 0.57 ± 0.03 s; uninformative: 1.08 ± 0.06 s; t<sub>(5)</sub> = –9.618; p&lt;0.001) and also more accurate in the informative condition (informative mean ± SEM: 93.92 ± 2.12%; uninformative: 89.83 ± 1.12%; t<sub>(5)</sub>=3.463; p=0.018). These behavioral benefits suggest that participants used the preview disk to pre-plan their motor action in the informative condition.</p><p>Next, we examined the average fMRI responses in both visual and sensorimotor cortical areas, which were each independently localized (see <italic>Methods, Identifying regions of interest</italic>). Our visual ROIs were retinotopically defined areas in occipital and parietal cortex, and our sensorimotor ROIs were action-related regions of primary somatosensory cortex (S1), primary motor cortex (M1), and premotor cortex (PMc). We used linear deconvolution to calculate the average hemodynamic response function (HRF) for voxels in each ROI during each task condition (see <italic>Methods, Analysis: Univariate</italic>; <xref ref-type="bibr" rid="bib21">Dale, 1999</xref>; <xref ref-type="bibr" rid="bib20">Dale and Buckner, 1997</xref>).</p><p>As expected, the BOLD signal in all retinotopic ROIs increased following visual stimulation (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, left two panels; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). We also replicated the typical finding that occipital retinotopic areas V1–hV4 do not show sustained BOLD activation during the memory delay period (<xref ref-type="bibr" rid="bib65">Offen et al., 2009</xref>; <xref ref-type="bibr" rid="bib72">Riggall and Postle, 2012</xref>; <xref ref-type="bibr" rid="bib76">Serences et al., 2009</xref>; <xref ref-type="bibr" rid="bib40">Harrison and Tong, 2009</xref>) in either of our two task conditions. Also as expected, mean BOLD responses in parietal areas IPS0-3 showed elevated late delay period activation in the uninformative condition relative to the informative condition. This pattern is consistent with the use of a spatial code in the uninformative condition and a motor code in the informative condition (<xref ref-type="bibr" rid="bib18">Curtis and D’Esposito, 2003</xref>; <xref ref-type="bibr" rid="bib24">D’Esposito, 2007</xref>; <xref ref-type="bibr" rid="bib28">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib72">Riggall and Postle, 2012</xref>). Sensorimotor areas S1, M1, and PMc showed a condition difference in the opposite direction, with higher mean BOLD responses in the informative compared with the uninformative condition at several timepoints early in the delay period, consistent with an increased reliance on an action-oriented memory code (<xref ref-type="bibr" rid="bib13">Calderon et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Donner et al., 2009</xref>). Note that these univariate results include voxels from both hemispheres. As expected, reliable interhemispheric differences can also be observed in sensorimotor areas, with higher activation in the hemisphere contralateral to the planned button press in the informative condition (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>The univariate results described above are consistent with the idea of an information ‘handoff’ between cortical regions as a function of perceptual and action-oriented task strategies. This brings up the important question of how such a task-related shift is reflected in population-level representations across the brain, and whether it coincides with a shift from a more sensory-like spatial memory code to a more motor-like memory code. To evaluate this question, we used a multivariate linear classifier to decode the angular spatial position of the remembered dot, based on multivoxel activation patterns measured in each ROI during the delay period (<xref ref-type="fig" rid="fig2">Figure 2</xref>). By assessing spatial decoding accuracy in each ROI, we could look at the extent to which the underlying neural code reflected a sensory-like visual representation of memorized spatial position.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Visual and parietal areas represent spatial position more strongly during trials that require sensory-like spatial memory than during trials that allow re-coding into action-oriented memory.</title><p>(<bold>A</bold>) Schematic of the decoding procedure. Continuous values of angular position were divided into eight discrete bins, and four binary decoders were trained to discriminate between patterns corresponding to bins 180° apart. The final decoding accuracy was the average accuracy over these four binary decoders. (<bold>B</bold>) Decoding accuracy for each region of interest (ROI) and task condition. The spatial decoder was always trained on data from the delay period of an independent spatial working memory mapping task (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), and tested on data from the delay period of the main working memory task (averaged within a window 8–12.8 s from start of trial; see <italic>Methods, Analysis: Spatial position decoding</italic> for more details). Error bars reflect ±1 SEM across participants, and light gray lines indicate individual participants. Dots above bars and pairs of bars indicate the level of statistical significance within each condition, and between conditions, respectively (two-tailed p-values obtained using a Wilcoxon signed-rank test with permutation testing, see <italic>Methods, Analysis: Spatial position decoding</italic>). Dot sizes reflect significance level. (<bold>C</bold>) Spatial decoding accuracy over time in three example ROIs. Timepoint zero indicates the target onset time. Shaded gray rectangles indicate the periods of time when the ‘preview’ (3.5–4.5 s) and ‘response’ (16.5–18.5 s) disks were onscreen. Shaded error bars represent ±1 SEM across participants, colored dots indicate significance of decoding within each condition, and gray dots indicate significant condition differences, with dot sizes reflecting significance levels as in B. Gray brackets just above the x-axis in (<bold>C</bold>) indicate the time range over which data were averaged to produce (<bold>B</bold>) (i.e. 8–12.8 s). See <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for time-resolved spatial decoding in all visual and motor ROIs.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Time-resolved spatial decoding accuracy in every region of interest (ROI).</title><p>All decoding was done using the spatial working memory mapping task as a training set (see <italic>Methods, Analysis: Spatial position decoding</italic> for details). Timepoint zero indicates the time of target onset; shaded gray rectangles indicate the periods of time when the ‘preview’ disk was onscreen (3.5–4.5 s) and when the response disk was onscreen (16.5–18.5 s). Shaded error bars represent ±1 SEM across participants, colored dots indicate significance of decoding within each condition, and gray dots indicate significant condition differences, with dot sizes reflecting significance levels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Spatial decoding performance differs across conditions, even when training and testing a decoder within each task condition separately.</title><p>See <italic>Methods, Analysis: Spatial position decoding</italic> for details on classification procedure. Notably, within-condition spatial decoding showed a highly similar pattern of results to the analysis using the independent training set (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), though the condition differences were slightly smaller (two-way repeated measures ANOVA with ROI and task condition as factors: main effect of ROI: F<sub>(13,65)</sub> = 12.873, p&lt;0.001; main effect of condition: F<sub>(1,5)</sub> = 11.461, p=0.017; ROI × condition interaction: F<sub>(13,65)</sub> = 2.581, p=0.011; p-values obtained using permutation test; see <italic>Methods</italic>). Error bars reflect ±1 SEM across participants, and light gray lines indicate individual participants. Dots above bars and pairs of bars indicate the statistical significance of decoding within each condition, and of condition differences, respectively, both evaluated using non-parametric statistics. Dot sizes reflect significance level.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Decoding accuracy for the orientation of the ‘preview’ disk stimulus (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>).</title><p>Decoding accuracy for this stimulus, which briefly appeared on the screen early in the delay period, was similar across conditions (two-way repeated measures ANOVA with ROI and task condition as factors: main effect of ROI: F<sub>(13,65)</sub> = 13.4, p&lt;0.001; main effect of task condition: F<sub>(1,5)</sub> = 2.164, p=0.193; ROI × condition interaction F<sub>(13,65)</sub> = 0.500, p=0.932; p-values obtained using permutation test; see <italic>Methods</italic>). Decoding was performed using data averaged over a time window 4.8–9.6 s into the trial, see <italic>Methods, Analysis: Disk orientation decoding</italic> for details. Error bars reflect ±1 SEM across participants, and light gray lines indicate individual participants. Dots above bars and pairs of bars indicate the statistical significance of decoding within each condition, and of condition differences, respectively, both evaluated using non-parametric statistics. Note that in this analysis, no significant condition differences were detected, and thus there are no dots above pairs of bars. Dot sizes reflect significance level.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig2-figsupp3-v1.tif"/></fig></fig-group><p>To facilitate an independent comparison of decoding accuracy between the informative and the uninformative task conditions in the main WM task, we used data from an independent spatial WM task to train the classifier (<xref ref-type="bibr" rid="bib84">Sprague et al., 2019</xref>; <xref ref-type="fig" rid="fig1">Figure 1C</xref>; see <italic>Methods, Task: Spatial working memory mapping</italic>). Before performing the decoding analysis, we subtracted from each single-trial activation pattern the mean across voxels on the same trial. This was done to ensure that any condition-specific changes in the mean BOLD responses did not contribute to differences in classification accuracy (see <italic>Methods, Analysis: Spatial position decoding</italic>). We then sorted the continuous angular positions into eight non-overlapping bins and used a decoding scheme with four binary classifiers, where each binary classifier was independently trained to discriminate between spatial positions that fell into bins separated by 180° (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="bibr" rid="bib70">Rademaker et al., 2019</xref>). The final decoding accuracy for each task condition reflects the average decoding accuracy across all four of these binary classifiers, where chance is 50% (see <italic>Methods, Analysis: Spatial position decoding</italic>).</p><p>The results of this multivariate analysis demonstrate that across the uninformative and the informative conditions, spatial decoding accuracy was strongest in early visual areas V1, V2, V3, and V3AB, and became progressively weaker at more anterior regions of the visual hierarchy (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Spatial decoding accuracy was at chance in the three sensorimotor areas S1, M1, and PMc. Importantly, there was also a pronounced effect of task condition (<xref ref-type="fig" rid="fig2">Figure 2B and C</xref>). Decoding accuracy in most retinotopic areas was significantly higher for the uninformative condition, in which participants were forced to rely on a spatial memory code, compared to the informative condition, in which participants could convert to an action-oriented memory for the button press required at the end of the delay (two-way repeated measures ANOVA with ROI and task condition as factors: main effect of ROI: F<sub>(13,65)</sub> = 24.548, p&lt;0.001; main effect of task condition: F<sub>(1,5)</sub> = 35.537, p=0.001; ROI × condition interaction F<sub>(13,65)</sub> = 5.757, p&lt;0.001; p-values obtained using permutation test; see <italic>Methods, Analysis: Spatial position decoding</italic>). Pairwise comparisons showed that spatial decoding accuracy was significantly higher in the uninformative compared to the informative condition in V1–hV4, LO2, and IPS0. In later IPS subregions (IPS1-3), spatial decoding accuracy was above chance in the uninformative memory condition, but at chance in the informative condition, without a significant difference between conditions. Finally, time-resolved analyses revealed that this difference between the conditions was not present during and immediately after encoding of the target dot, when the trial condition was not yet known. Instead, condition differences emerged approximately 5–6 s after the presentation of the preview disk and persisted until the final response disk appeared (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Importantly, the difference in spatial decoding accuracy between task conditions replicated when we trained and tested a decoder using data from within each condition of the main WM task as opposed to training on the independent spatial WM task. This indicates that the difference between task conditions was robust to a range of analysis choices (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>; see <italic>Methods, Analysis: Spatial position decoding</italic>).</p><p>In addition to demonstrating the robustness of our findings, the two separate analysis approaches described above allow us to distinguish between two potential explanations for the observed condition difference. While one possibility is that visual cortex activation patterns contain <italic>less overall information</italic> about spatial position in the informative relative to the uninformative condition, another possibility is that the <italic>format of the representations differs</italic> between the two task conditions (e.g. <xref ref-type="bibr" rid="bib60">Lorenc et al., 2020</xref>; <xref ref-type="bibr" rid="bib92">Vaziri-Pashkam and Xu, 2017</xref>). Specifically, compared to the informative condition, the sensory-like spatial format used in the uninformative condition may have been more similar to the format of the independent spatial WM task (which also required a sensory-like strategy because the starting position of the response dot was unknown until the end of the trial; see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). This similarity could have led to higher cross-generalization of the decoder to the uninformative condition, even if the overall spatial information content was similar between the two conditions in the main WM task. The fact that the condition difference persists when training and testing within condition rules this latter possibility out. Instead, our analyses support the interpretation that spatial WM representations in visual cortex are adaptively enhanced in the uninformative condition (when a sensory-like code is required) relative to the informative condition (where an action-oriented code can be used).</p><p>Is the quality of mnemonic spatial representations selectively modulated between task conditions, or might non-mnemonic factors – such as global arousal level – contribute to the observed differences in decoding accuracy? If non-mnemonic factors play a role, they should interact with the processing of all stimuli shown after the condition cue, including the ‘preview disk’ stimulus (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). A supplementary analysis revealed that the orientation of the preview disk (i.e. the boundary between its light- and dark-gray sides) could be decoded with above chance accuracy in several visual cortex ROIs in both conditions, but without a significant difference between conditions (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>; see <italic>Methods, Analysis: Preview disk orientation decoding</italic> for details). This indicates that neural processing of the preview disk, a visual stimulus physically presented on the screen, was not modulated by task condition. Therefore, the effect of task condition on information content was specific to the spatial memory representation itself, and not due to a more global modulatory effect or difference in the signal-to-noise ratio of representations in early visual cortex.</p><p>In addition to spatial position decoding, we investigated the possibility of an action-oriented format of the mnemonic code by decoding upcoming actions. Since motor responses were always made with the left or the right index finger, we trained a binary linear classifier to predict which finger was associated with the required button-press on each trial based on delay period activation patterns in each ROI (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, see <italic>Methods, Analysis: Action decoding</italic> for detailed classification procedure). This analysis revealed above chance decoding of upcoming actions in each of the three sensorimotor ROIs (S1, M1, and PMc) in the informative condition but not the uninformative condition. In contrast, all retinotopic visual ROIs showed chance level action decoding for both conditions (two-way repeated measures ANOVA with ROI and task condition as factors: main effect of ROI: F<sub>(13,65)</sub> = 4.003, p&lt;0.001; main effect of condition F<sub>(1,5)</sub> = 3.802, p=0.106; ROI × condition interaction: F<sub>(13,65)</sub> = 2.937, p=0.001; p-values obtained using permutation test; see <italic>Methods, Analysis: Action decoding</italic>). A time-resolved decoding analysis revealed that, in the informative condition, information about a participants’ upcoming action began to emerge approximately 4 s after the onset of the preview disk stimulus, decreased slightly toward the end of the delay period, then rose steeply after the response disk onset when the participant actually executed a motor action (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The increase in action decoding accuracy during the last part of the trial appeared sooner for the informative condition than the uninformative condition, in agreement with the speeding of behavioral response times in the informative condition (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Action-oriented memory representations can be decoded from sensorimotor regions of interest (ROIs) during the delay period.</title><p>(<bold>A</bold>) A linear decoder was trained to classify the finger (left or right index) associated with the correct motor action on each trial, using data measured during the delay period of trials in each task condition separately (averaged within a window 8–12.8 s from start of trial; see <italic>Methods, Analysis: Action decoding</italic> for more details). Error bars reflect ±1 SEM across participants, and light gray lines indicate individual participants. Dots above bars and pairs of bars indicate the statistical significance of decoding accuracy within each condition, and of condition differences, respectively (two-tailed p-values obtained using a Wilcoxon signed-rank test with permutation testing, see <italic>Methods, Analysis: Action decoding</italic>). Dot sizes reflect significance level. (<bold>B</bold>) Action decoding accuracy over time in three example ROIs. Timepoint zero indicates the target onset time. Shaded gray rectangles indicate the periods during which the ‘preview’ (3.5–4.5 s) and ‘response’ (16.5–18.5 s) disks were onscreen. Shaded error bars represent ±1 SEM across participants. Colored dots indicate significance of decoding accuracy within each condition, and gray dots indicate significant condition differences, with dot sizes reflecting significance levels as in A. Gray brackets just above the x-axis in (<bold>B</bold>) indicate the time range in which data were averaged to produce (<bold>A</bold>) (8–12.8 s). For time-resolved decoding in all ROIs, see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Time-resolved action decoding accuracy in every region of interest (ROI).</title><p>All decoding was done using data from the same task condition for training and testing (see <italic>Methods, Analysis: Action decoding</italic> for details). Timepoint zero indicates the time of target onset; shaded gray rectangles indicate the periods of time when the ‘preview’ disk was onscreen (3.5–4.5 s) and when the response disk was onscreen (16.5–18.5 s). Shaded error bars represent ±1 SEM across participants, colored dots indicate significance of decoding within each condition, and gray dots indicate significant condition differences, with dot sizes reflecting significance levels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig3-figsupp1-v1.tif"/></fig></fig-group><p>Together, the above results support the hypothesis that the informative and uninformative conditions differentially engaged regions of cortex and neural coding formats for information storage during WM. To characterize this difference in coding format in a more concrete way, we next leveraged two additional independent model training tasks. First, we used a spatial localizer during which participants viewed high-contrast flickering checkerboards at various spatial positions (see <italic>Methods, Task: Spatial localizer</italic>). With this dataset, we trained a classifier on sensory-driven visuo-spatial responses (using the same spatial position decoding method as described previously, see <xref ref-type="fig" rid="fig2">Figure 2A</xref>). We found that spatial position information generalized from this sensory-driven perceptual training data to both conditions of our main WM task (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This result supports the idea that the coding format used during spatial WM is sensory-like in nature, closely resembling the responses in visual cortex to spatially localized perceptual input. Importantly, decoding performance in early visual and parietal areas was again higher for the uninformative than the informative condition. This implies that the uninformative condition of our task resulted in a stronger and more ‘sensory-like’ memory code. The second independent model training task was a sensorimotor cortex localizer where participants physically pressed buttons with their left and right index fingers (see <italic>Methods, Task: Sensorimotor cortex localizer</italic>). We trained a decoder on this task, labeling trials according to which finger was used to physically press the buttons on each trial. We then tested this decoder on delay period activation in our main WM task. This analysis revealed above-chance decoding accuracy of the participant’s upcoming action in S1, M1, and PMc, in the informative condition only (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). This result supports the idea that the memory representations in the informative condition were stored in a more motor-like format which closely resembled signals measured during actual button press responses.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Perceptual and action-oriented memory representations generalize from signals associated with perceptual input and physical button-press responses, respectively.</title><p>(<bold>A</bold>) Decoding accuracy of spatial memory position when training on data from an independent sensory localizer (see <italic>Methods, Task: Spatial localizer</italic>, and <italic>Methods, Analysis: Spatial position decoding</italic> for details on this task and on decoding procedure). (<bold>B</bold>) Decoding accuracy of action-oriented memory codes (i.e. the finger associated with the correct motor action on each trial) when training on data from an independent button pressing task (see <italic>Methods, Task: Sensorimotor cortex localizer</italic> and <italic>Methods, Analysis: Action decoding</italic> for details on this task and on decoding procedure). In both panels, error bars reflect ±1 SEM across participants, and light gray lines indicate individual participants. Dots above bars and pairs of bars indicate the statistical significance of decoding accuracy within each condition, and of condition differences, respectively, both evaluated using non-parametric statistics. Dot sizes reflect significance level. Inserts show a cartoon of the localizer tasks used to train the decoder for these analyses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-75688-fig4-v1.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Past studies of visual WM have led to competing theories of how information is stored in the brain. Some work has emphasized the recruitment of early sensory areas, while other work has emphasized parietal and frontal association areas. Nevertheless, a general consensus exists that WM is a flexible resource (<xref ref-type="bibr" rid="bib6">Baddeley and Hitch, 1974</xref>; <xref ref-type="bibr" rid="bib36">Gazzaley and Nobre, 2012</xref>; <xref ref-type="bibr" rid="bib44">Iamshchinina et al., 2021</xref>; <xref ref-type="bibr" rid="bib77">Serences, 2016</xref>). Here we demonstrate, within a single spatial WM paradigm, that task requirements are a critical determinant of <italic>how</italic> and <italic>where</italic> WM is implemented in the brain. These data provide a partial unifying explanation for divergent prior findings that implicate different regions in visual WM (<xref ref-type="bibr" rid="bib8">Bettencourt and Xu, 2015</xref>; <xref ref-type="bibr" rid="bib29">Ester et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Iamshchinina et al., 2021</xref>; <xref ref-type="bibr" rid="bib70">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="bib97">Xu, 2018</xref>; <xref ref-type="bibr" rid="bib98">Xu, 2020</xref>). More importantly, however, our data show that WM flexibly engages different cortical areas and coding formats, even in the context of a task that is commonly used to study a single underlying construct (i.e. visuo-spatial WM). These findings highlight the goal-oriented nature of WM: the brain’s mechanisms for storage are not fully specified by the type of information being remembered, but instead depend on how the encoded information will be used to guide future behavior.</p><p>We used a spatial WM task wherein participants could anticipate their behavioral response ahead of the delay interval on half of the trials (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), encouraging re-coding of visuo-spatial memory information to a more action-oriented representational format. On the other half of trials, participants were unable to anticipate their behavioral response, and had to rely on a sensory-like spatial memory representation. The decoding accuracy of a remembered position from activation patterns in visual cortex was lower when participants had the opportunity to anticipate their eventual motor action, compared to when they had to rely exclusively on sensory-like visual memory. This effect was consistent across a range of early visual and parietal ROIs (<xref ref-type="fig" rid="fig2">Figure 2</xref>), supporting the idea that the recruitment of visual cortex for WM storage is task dependent. Conversely, during the same task condition where visuo-spatial representations became weaker in visual cortex, sensorimotor ROIs showed above-chance decoding of the participant’s planned action during the delay period (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We additionally show that the format of these two memory representations was markedly different: the spatial memory code generalized from an independent task in which stimuli at various spatial positions were actively viewed, whereas the action-related memory code generalized from an independent task in which participants made actual alternating button-press responses (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Together, these results demonstrate that rather than having one fixed locus or mechanism, visual WM can be supported by different loci and coding schemes that are adapted to current task requirements.</p><p>While we found widespread decoding of the contents of WM, we generally did not find sustained increases in univariate BOLD responses during the delay (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Specifically, visual areas V1–hV4 and LO2 showed no sustained BOLD responses or differences in the mean BOLD signal between conditions during the delay period, despite these areas showing condition differences in spatial decoding accuracy. This adds to an existing body of research showing a dissociation between univariate and multivariate indices of WM storage (<xref ref-type="bibr" rid="bib25">Emrich et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Ester et al., 2015</xref>; <xref ref-type="bibr" rid="bib40">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib72">Riggall and Postle, 2012</xref>; <xref ref-type="bibr" rid="bib76">Serences et al., 2009</xref>). It additionally suggests that the observed differences in decoding performance cannot be explained by global changes in signal due to arousal or task difficulty, but are instead attributable to differences in the amount of information represented within population-level patterns of activation in visual cortex. Furthermore, we found that information about the preview disk (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>), a physical stimulus that appeared briefly on the screen early in the delay period, was not significantly different between task conditions (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). This finding further highlights that the observed modulation of spatial decoding accuracy in visual cortex was specific to the memory representation, and not due to a non-specific change in signal-to-noise ratio within early visual areas.</p><p>At the same time, in multiple subregions of IPS, we did observe higher univariate delay period activation for the uninformative condition relative to the informative condition. In contrast, the opposite was true in sensorimotor areas (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). This finding parallels previous work showing higher univariate delay period activation in IPS during a task where oculomotor responses were decoupled from spatial memory items (i.e. encouraging a more perceptual strategy), whereas oculomotor areas showed higher activation when responses and memoranda were yoked (i.e. encouraging a more action-oriented strategy; <xref ref-type="bibr" rid="bib19">Curtis et al., 2004</xref>). This past work suggested specialized networks for perceptual ‘sensory-like’ versus action-oriented ‘motor-like’ memories. Our current demonstration of a shift from visual to sensorimotor codes further builds on this by showing how the neural systems underlying WM may be flexibly reconfigured, and memory representations flexibly reformatted, based on behavioral requirements.</p><p>While information about the remembered spatial position was substantially lower in the informative condition than the uninformative condition, decoding accuracy did not fall entirely to chance in early visual cortex during informative cue trials (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). With the possible exception of area V1, this was true even when looking at decoding at the very end of the WM delay period (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Several factors may have contributed to this above chance decoding accuracy. First, it is possible that on some fraction of informative trials, participants failed to properly encode or process the condition cue, and as a result maintained a high-precision spatial representation throughout the delay period. Such lapses of attention are possible, given that the conditions in our task were randomly interleaved on a trial-by-trial basis. Second, it is possible that participants faithfully followed the informative cue, and made use of an action-oriented strategy, but some amount of visuo-spatial memory information was nonetheless maintained in visual cortex. This would suggest that the recruitment of sensory-like codes in visual cortex is at least partially obligatory when performing a visuo-spatial WM task. This account is consistent with past findings showing that items which are not immediately relevant for a task, but will be relevant later (i.e. ‘unattended memory items’), can still be decoded from early visual cortex (<xref ref-type="bibr" rid="bib44">Iamshchinina et al., 2021</xref>; re-analysis of data from <xref ref-type="bibr" rid="bib15">Christophel et al., 2018</xref>; but see also <xref ref-type="bibr" rid="bib55">Lewis-Peacock et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">LaRocque et al., 2017</xref>). However, this account is <italic>inconsistent</italic> with past findings showing that when a once-maintained item is no longer needed for behavior, it is no longer decodable from early visual areas (<xref ref-type="bibr" rid="bib40">Harrison and Tong, 2009</xref>; <xref ref-type="bibr" rid="bib55">Lewis-Peacock et al., 2012</xref>; <xref ref-type="bibr" rid="bib82">Sprague et al., 2014</xref>; <xref ref-type="bibr" rid="bib83">Sprague et al., 2016</xref>; <xref ref-type="bibr" rid="bib60">Lorenc et al., 2020</xref>). In our paradigm, we cannot rule out that – even on trials where participants could completely discard spatial memory information in favor of an action-oriented code – some amount of visuo-spatial information was still maintained in visual cortex, reflecting a partially distributed code. Some support for the idea that sensory- and action-like codes can be maintained and accessed simultaneously during a memory task comes from a study by <xref ref-type="bibr" rid="bib88">van Ede et al., 2019a</xref>, who showed such simultaneity using EEG during a task where both action-related and sensory-like codes were relevant for performance. Further experiments will be needed to determine how such distributed codes may be engaged during memory tasks in general, and how the extent to which a neural code is more distributed versus more punctate may change as a function of task requirements.</p><p>We observed above-chance delay period decoding of participants’ planned actions in S1, M1, and PMc in the informative condition. Each of these areas has previously been shown to exhibit preparatory motor activity in the context of tasks involving delayed reaching or finger pressing (<xref ref-type="bibr" rid="bib3">Ariani et al., 2022</xref>; <xref ref-type="bibr" rid="bib13">Calderon et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Cisek and Kalaska, 2005</xref>; <xref ref-type="bibr" rid="bib23">Donner et al., 2009</xref>). In our data, information regarding the upcoming action became detectable around 4 s after the onset of the preview disk – roughly the same time that spatial decoding accuracy in visual cortex dropped in the informative compared to the uninformative condition (<xref ref-type="fig" rid="fig2">Figure 2C</xref> and <xref ref-type="fig" rid="fig3">Figure 3B</xref>). While suggestive, the temporal resolution of fMRI does not allow us to draw firm conclusions about the relative timing of each process. Nevertheless, these results are broadly consistent with a theory in which action selection unfolds in parallel to task-related processing of visual input (<xref ref-type="bibr" rid="bib17">Cisek and Kalaska, 2010</xref>; <xref ref-type="bibr" rid="bib23">Donner et al., 2009</xref>; <xref ref-type="bibr" rid="bib50">Klein-Flügge and Bestmann, 2012</xref>; <xref ref-type="bibr" rid="bib88">van Ede et al., 2019a</xref>). Additionally, we found that information about upcoming actions declined toward the end of the delay period, dropping to chance just before the response disk appeared (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This time course is consistent with a transient signal related to the initial formation of an action-oriented memory representation, and aligns with recent EEG findings in human motor cortex following a retro-cue in a visual WM task (<xref ref-type="bibr" rid="bib9">Boettcher et al., 2021</xref>).</p><p>Past work has investigated the influence of action-oriented coding on WM representations. For instance, it has been shown that high priority items, meaning those that are relevant for upcoming actions, tend to be represented more robustly than items that are not immediately relevant to behavior (<xref ref-type="bibr" rid="bib14">Christophel et al., 2012</xref>; <xref ref-type="bibr" rid="bib55">Lewis-Peacock et al., 2012</xref>; <xref ref-type="bibr" rid="bib60">Lorenc et al., 2020</xref>; <xref ref-type="bibr" rid="bib73">Rose et al., 2016</xref>; <xref ref-type="bibr" rid="bib83">Sprague et al., 2016</xref>; but see also <xref ref-type="bibr" rid="bib7">Barbosa et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">Iamshchinina et al., 2021</xref>). Prioritized WM representations can be further reconfigured so that they are optimized for future behavior, which may include the activation of circuits related to motor output (<xref ref-type="bibr" rid="bib63">Myers et al., 2017</xref>; <xref ref-type="bibr" rid="bib64">Nobre and Stokes, 2019</xref>; <xref ref-type="bibr" rid="bib75">Schneider et al., 2017</xref>; <xref ref-type="bibr" rid="bib80">Souza and Oberauer, 2016</xref>; <xref ref-type="bibr" rid="bib88">van Ede et al., 2019a</xref>). These past studies of action preparation have typically used relatively coarse measures of visual information content such as decoding which of two items was currently selected. In contrast, here we measured the information about a continuous remembered feature value that was reflected in patterns of activation in visual cortex. We simultaneously showed a decrease in spatial information in visual cortex and an increase in action-related information in sensorimotor cortex when an action-oriented mnemonic format was encouraged. This finding solidifies the evidence showing that information may shift between qualitatively different kinds of codes as a function of behavioral requirements, as opposed to just waxing and waning over time within the same format/brain areas.</p><p>Past work on saccadic eye movements further supports the link between the neural circuits that support WM and those that support action preparation and execution. For instance, prior work has demonstrated that when retrieving or selecting an item within WM, eye movements exhibit a systematic bias toward the spatial position at which the item was originally presented, even without a physical stimulus to guide eye movements (<xref ref-type="bibr" rid="bib81">Spivey and Geng, 2001</xref>; <xref ref-type="bibr" rid="bib30">Ferreira et al., 2008</xref>; <xref ref-type="bibr" rid="bib89">van Ede et al., 2019b</xref>; <xref ref-type="bibr" rid="bib90">van Ede et al., 2020</xref>). These eye movements may play a functional role in retrieving items from WM (<xref ref-type="bibr" rid="bib30">Ferreira et al., 2008</xref>; <xref ref-type="bibr" rid="bib89">van Ede et al., 2019b</xref>) and can index processes such as attentional selection within WM (<xref ref-type="bibr" rid="bib89">van Ede et al., 2019b</xref>; <xref ref-type="bibr" rid="bib90">van Ede et al., 2020</xref>). In other work, the interaction between memory and action has been shown in the opposite direction, where the prioritization of a feature or location for motor actions can lead to prioritization in WM (<xref ref-type="bibr" rid="bib43">Heuer et al., 2020</xref>). For example, when participants make a saccade during the delay period of a visual WM task, memory performance is enhanced for items originally presented at the saccade target location (<xref ref-type="bibr" rid="bib66">Ohl and Rolfs, 2020</xref>). This effect occurs even when saccade target locations conflict with top-down attentional goals, suggesting it reflects an automatic mechanism for selection based on action planning (<xref ref-type="bibr" rid="bib66">Ohl and Rolfs, 2020</xref>; <xref ref-type="bibr" rid="bib43">Heuer et al., 2020</xref>). Finally, the planning of both memory-guided saccades and anti-saccades is associated with topographically specific activation in early visual cortex measured with fMRI, suggesting that neural representations of upcoming actions are coded similarly to representations used for WM storage (<xref ref-type="bibr" rid="bib74">Saber et al., 2015</xref>; <xref ref-type="bibr" rid="bib71">Rahmati et al., 2018</xref>). Together, these findings suggest an important functional association between WM and motor planning within the domain of eye movement control. Our work expands upon this idea by demonstrating the engagement of motor circuits for the maintenance of action-oriented WM representations using a paradigm outside the realm of eye movement planning. Additionally, while prior work suggested a close correspondence between perceptual and action-oriented WM codes, our work demonstrates that these two types of representations can also be strategically dissociated depending on task demands and the nature of the motor response being planned (i.e. a manual response versus a saccadic response).</p><p>Finally, beyond the impact of more perceptual and action-oriented WM strategies, other aspects of task requirements have been shown to influence the neural correlates of WM. For example, one experiment found that information about visual objects could be decoded from extrastriate visual areas on blocks of trials that required memory for visual details, but from PFC on blocks of trials where only the object’s category had to be maintained (<xref ref-type="bibr" rid="bib54">Lee et al., 2013</xref>). Similarly, another experiment showed that instructing participants to maintain items in a visual, verbal, or semantic code resulted in format-specific patterns of neural activation measured with fMRI (<xref ref-type="bibr" rid="bib56">Lewis-Peacock et al., 2015</xref>). These findings, along with our results, further support a framework in which behavioral requirements strongly influence the format of WM representations.</p><p>In our action-decoding analyses (<xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>), we interpret the decoded representations as primarily reflecting the planning of upcoming actions and maintenance of an action plan in WM, rather than reflecting overt motor actions. This interpretation is justified because participants were explicitly instructed to withhold any physical finger movements or button presses until the end of the delay period, when the probe disk appeared. We observed participants throughout their behavioral training and scanning sessions to ensure that no noticeable finger movements were performed. Nevertheless, we cannot rule out the possibility that some amount of low-grade motor activity (such as a subtle increase in pressure on the button box, or increased muscle tone in the finger) occurred during the delay period in the informative condition and contributed to our decoding results. Further experiments incorporating a method such as electromyography recordings from the finger muscles would be necessary to resolve this. At the same time, this possibility of low-grade motor activity does not substantially change the interpretation of our results, namely that the representation of upcoming actions in motor, premotor, and supplementary motor cortex reflects the maintenance of a prospective action plan within WM. As detailed in the preceding paragraphs, a large body of work has drawn a link between action preparation and WM in the brain, with motor circuits often actively engaged by the selection and manipulation of items within WM, and motor actions in turn influencing the quality of memory representations. Based on this framework, the action-oriented representations in our task, independent of whether they may reflect a contribution from physical motor activity, are within the range of representations that can be defined as WM.</p><p>In contrast to human neuroimaging experiments, which often find evidence for sensory recruitment (i.e. WM decoding from early sensory cortices), research with NHPs has generally found less evidence for maintenance of feature-specific codes in early sensory regions. The difference in results between human and primate studies may be partially accounted for by differences in measurement modality. For example, the BOLD response is driven in part by modulations of local field potentials (<xref ref-type="bibr" rid="bib10">Boynton, 2011</xref>; <xref ref-type="bibr" rid="bib37">Goense and Logothetis, 2008</xref>; <xref ref-type="bibr" rid="bib57">Logothetis et al., 2001</xref>; <xref ref-type="bibr" rid="bib58">Logothetis and Wandell, 2004</xref>), which may contain information about remembered features, such as motion direction, even when single unit spike rates do not (<xref ref-type="bibr" rid="bib61">Mendoza-Halliday et al., 2014</xref>). Part of the discrepancy may also be related to differences in tasks used in human studies versus NHP studies. For example, some NHP studies use tasks that allow for motor coding strategies, such as the traditional memory-guided saccade paradigm (<xref ref-type="bibr" rid="bib33">Funahashi et al., 1989</xref>; <xref ref-type="bibr" rid="bib35">Fuster and Alexander, 1971</xref>; <xref ref-type="bibr" rid="bib38">Goldman-Rakic, 1995</xref>). At the same time, other NHP experiments have dissociated sensory and action-related information (<xref ref-type="bibr" rid="bib34">Funahashi et al., 1993</xref>; <xref ref-type="bibr" rid="bib61">Mendoza-Halliday et al., 2014</xref>; <xref ref-type="bibr" rid="bib62">Miller et al., 1996</xref>; <xref ref-type="bibr" rid="bib67">Panichello et al., 2019</xref>), suggesting that the difference between perceptual and action-oriented coding alone cannot account for differences between studies. Another aspect of task design that may contribute to these differences is that NHP tasks often require animals to remember one of a small, discrete set of stimuli (e.g. <xref ref-type="bibr" rid="bib34">Funahashi et al., 1993</xref>; <xref ref-type="bibr" rid="bib61">Mendoza-Halliday et al., 2014</xref>; <xref ref-type="bibr" rid="bib62">Miller et al., 1996</xref>). This may encourage a more discretized categorical representation rather than a highly detailed representation, which may rely less strongly on the finely tuned neural populations of early sensory areas (<xref ref-type="bibr" rid="bib54">Lee et al., 2013</xref>). For example, some NHP electrophysiology studies have shown spiking activity in V1 that reflects the contents of WM using tasks that require precise spatial representations that may not be easily re-coded into a non-sensory format (e.g. the curve-tracing task used by Van Kerkoerle and colleagues; <xref ref-type="bibr" rid="bib86">Supèr et al., 2001</xref>; <xref ref-type="bibr" rid="bib91">van Kerkoerle et al., 2017</xref>).</p><p>Together, our findings suggest that the neural mechanisms that underlie WM are dynamic and can be flexibly adjusted, even in the context of a single paradigm typically used to study visuo-spatial WM. When our participants were given the possibility to switch from a purely visual to a motor-based WM code, the amount of information regarding precise spatial position dropped in early visual and parietal cortex, while information about an upcoming action became decodable from sensorimotor cortex. This experiment highlights how in a single paradigm, we can measure multiple dissociable mechanisms supporting WM. More broadly, these results open the door for future experiments to explore other task factors that may dynamically alter how WM is represented in the brain, and further push the boundaries of our knowledge about WM and cognitive flexibility.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Six participants (two male) between the ages of 20 and 34 were recruited from the University of California, San Diego (UCSD) community (mean age 27.2 ± 2.7 years). All had normal or corrected-to-normal vision. One additional participant (male) participated in an early pilot version of the study, but is not included here as they did not complete the final experiment. The study protocol was approved by the Institutional Review Board at UCSD, and all participants provided written informed consent. Each participant performed a behavioral training session lasting approximately 30 min, followed by three or four scanning sessions, each lasting approximately 2 hr. Participants were compensated at a rate of $10 /hr for behavioral training and $20 /hr for the scanning sessions. Participants were also given ‘bonus’ money for correct performance on certain trials in the main behavioral task (see <italic>Task: Main working memory</italic>), up to a maximum of $40 bonus.</p></sec><sec id="s4-2"><title>Magnetic resonance imaging (MRI)</title><p>All MRI scanning was performed on a General Electric (GE) Discovery MR750 3.0T research-dedicated scanner at the UC San Diego Keck Center for Functional Magnetic Resonance Imaging. Functional echo-planar imaging (EPI) data were acquired using a Nova Medical 32-channel head coil (NMSC075-32-3GE-MR750) and the Stanford Simultaneous Multislice (SMS) EPI sequence (MUX EPI), with a multiband factor of 8 and 9 axial slices per band (total slices = 72; 2 mm<sup>3</sup> isotropic; 0 mm gap; matrix = 104 × 104; field of view (FOV) = 20.8 cm; repetition time/time to echo (TR/TE) = 800/35 ms; flip angle = 52°; inplane acceleration = 1). Image reconstruction and un-aliasing procedures were performed on servers hosted by Amazon Web Services, using reconstruction code from the Stanford Center for Neural Imaging. The initial 16 TRs collected at sequence onset served as reference images required for the transformation from k-space to image space. Two short (17 s) ‘topup’ datasets were collected during each session, using forward and reverse phase-encoding directions. These images were used to estimate susceptibility-induced off-resonance fields (<xref ref-type="bibr" rid="bib2">Andersson et al., 2003</xref>) and to correct signal distortion in EPI sequences using FSL topup functionality (<xref ref-type="bibr" rid="bib47">Jenkinson et al., 2012</xref>).</p><p>In addition to the experimental scanning sessions, each participant participated in a separate retinotopic mapping session during which we also acquired a high-resolution anatomical scan. This anatomical T1 image was used for segmentation, flattening, and delineation of the retinotopic mapping data. For four out of the six participants, the anatomical scan was obtained using an in vivo eight-channel head coil with accelerated parallel imaging (GE ASSET on a FSPGR T1-weighted sequence; 1 × 1 × 1 mm<sup>3</sup> voxel size; 8136 ms TR; 3172 ms TE; 8° flip angle; 172 slices; 1 mm slice gap; 256 × 192 cm matrix size), and for the remaining two participants this scan was collected using the same 32-channel head coil used for functional scanning (anatomical scan parameters used with 32-channel coil were identical to those used with the eight-channel coil). Anatomical scans collected with the 32-channel head coil were corrected for inhomogeneities in signal intensity using GE’s ‘phased array uniformity enhancement’ method.</p></sec><sec id="s4-3"><title>Pre-processing of MRI data</title><p>All pre-processing of MRI data was performed using software tools developed and distributed by FreeSurfer and FSL (available at <ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu">https://surfer.nmr.mgh.harvard.edu</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">http://www.fmrib.ox.ac.uk/fsl</ext-link>). First, we used the recon-all utility in the FreeSurfer analysis suite (<xref ref-type="bibr" rid="bib21">Dale, 1999</xref>) to perform cortical surface gray-white matter volumetric segmentation of anatomical T1 scans. The segmented T1 data were used to define cortical meshes on which we specified retinotopic ROIs used for subsequent analyses (see <italic>Identifying ROIs</italic>). T1 data were also used to align multisession functional data into a common space: for each of the experimental scan sessions, the first volume of the first run was used as a template to align the functional data from that session to the anatomical data. Co-registration was performed using FreeSurfer’s manual and automated boundary-based registration tools (<xref ref-type="bibr" rid="bib39">Greve and Fischl, 2009</xref>). The resulting transformation matrices were then used to transform every four-dimensional functional volume into a common space, using FSL FLIRT (<xref ref-type="bibr" rid="bib46">Jenkinson et al., 2002</xref>; <xref ref-type="bibr" rid="bib45">Jenkinson and Smith, 2001</xref>). Next, motion correction was performed using FSL MCFLIRT (<xref ref-type="bibr" rid="bib46">Jenkinson et al., 2002</xref>), without spatial smoothing, with a final sinc interpolation stage, and 12° of freedom. Finally, slow drifts in the data were removed using a high-pass filter (1/40 Hz cutoff). No additional spatial smoothing was performed.</p><p>The above steps were performed for all functional runs, including the main WM task (see <italic>Task: Main working memory</italic>), spatial WM mapping (see <italic>Task: Spatial working memory mapping</italic>), sensorimotor cortex localizer (see <italic>Task: Sensorimotor cortex localizer</italic>), and spatial localizer (see <italic>Task: Spatial localizer</italic>). Following this initial pre-processing, for all run types, we normalized the time series data by z-scoring each voxel’s signal across each entire scan run (this and all subsequent analyses were performed in Matlab 2018b; see <xref ref-type="bibr" rid="bib42">Henderson, 2022</xref> for our code). Deconvolution (see <italic>Analysis: Univariate</italic>) of main task data was performed on this continuous z-scored data. Next, we epoched the data based on the start time of each trial. Since trial events were jittered slightly with respect to TR onsets, we rounded trial start times to the nearest TR. This epoched data was used for time-resolved decoding analyses (<xref ref-type="fig" rid="fig2">Figure 2C</xref> and <xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). For the time-averaged analyses (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), we obtained a single estimate of each voxel’s response during each trial. For the main task, we obtained this value using an average of the timepoints from 10 to 16 TRs (8–12.8 s) after trial onset, which falls in the delay period of the task. For the spatial WM mapping task, we used an average of the timepoints from 6 to 12 TRs (4.8–9.6 s) after trial onset, which falls in the delay period of this task. For the spatial localizer task, we averaged over the timepoints from 4 to 7 TRs (3.2–5.6 s) after stimulus onset. For the button-pressing task (see <italic>Task: Sensorimotor cortex localizer</italic>) we averaged over the timepoints from 4 to 7 TRs (3.2–5.6 s) after trial onset. The data from these latter two tasks was additionally used to identify voxels based on their spatial selectivity and action selectivity, respectively (see <italic>Identifying ROIs</italic>).</p></sec><sec id="s4-4"><title>Identifying regions of interest (ROIs)</title><p>We followed previously published retinotopic mapping protocols to define the visual areas V1, V2, V3, V3AB, hV4, IPS0, IPS1, IPS2, and IPS3 (<xref ref-type="bibr" rid="bib26">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="bib48">Jerde and Curtis, 2013</xref>; <xref ref-type="bibr" rid="bib78">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="bib87">Swisher et al., 2007</xref>; <xref ref-type="bibr" rid="bib93">Wandell et al., 2007</xref>; <xref ref-type="bibr" rid="bib94">Winawer and Witthoft, 2015</xref>). Participants performed mapping runs in which they viewed a contrast-reversing black and white checkerboard stimulus (4 Hz) that was configured as either a rotating wedge (10 cycles, 36 s/cycle), an expanding ring (10 cycles, 32 s/cycle), or a bowtie (8 cycles, 40 s/cycle). To increase the quality of retinotopic data from parietal regions, participants performed a covert attention task on the rotating wedge stimulus, which required them to detect contrast dimming events that occurred occasionally (on average, one event every 7.5 s) in a row of the checkerboard (mean accuracy = 74.4 ± 3.6%). The maximum eccentricity of the stimulus was 9.3°.</p><p>After mapping the individual retinotopic ROIs for each participant, we used data from our spatial localizer task (see <italic>Task: Spatial localizer</italic>) to identify voxels within each retinotopic ROI that were selective for the region of visual space in which our spatial memory positions could appear. Data from this localizer task were analyzed using a general linear model (GLM) implemented in FSL’s FEAT (FMRI Expert Analysis Tool, version 6.00). Brain extraction (<xref ref-type="bibr" rid="bib79">Smith, 2002</xref>) and pre-whitening (<xref ref-type="bibr" rid="bib95">Woolrich et al., 2001</xref>) were performed on individual runs before analysis. Predicted BOLD responses for each of a series of checkerboard wedges were generated by convolving the stimulus sequence with a canonical gamma hemodynamic response (phase = 0 s, s.d. = 3 s, lag = 6 s). Individual runs were combined using a standard weighted fixed effects analysis. For each of the 24 possible wedge positions, we identified voxels that were significantly more activated by that position than by all other positions (p&lt;0.05, false discovery rate corrected). We then merged the sets of voxels that were identified by each of these 24 tests and used this merged map to select voxels from each retinotopic ROI for further analysis.</p><p>In addition to mapping visual ROIs, we also mapped several sensorimotor ROIs. We did this by intersecting data from a simple button-press task (see <italic>Task: Sensorimotor cortex localizer</italic>) with anatomical definitions of motor cortex. Data from the sensorimotor localizer were analyzed using a GLM in FSL’s FEAT, as described above for the spatial localizer. Predicted BOLD responses for left- and right-handed button presses were generated by convolving the stimulus sequence with a gamma HRF (phase = 0 s, s.d. = 3 s, lag = 5 s). We identified voxels that showed significantly more activation for the contralateral index finger than the ipsilateral index finger (p&lt;0.05, false discovery rate corrected). This procedure was done separately within each hemisphere. We then defined each sensorimotor ROI by intersecting the map of above-threshold voxels with the anatomical definitions of Brodmann’s areas (BAs) identified by FreeSurfer’s recon-all segmentation procedure (<xref ref-type="bibr" rid="bib22">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib31">Fischl et al., 2008</xref>). Specifically, the functionally defined mask was intersected with BA 6 to define premotor cortex (PMc), with BA 4 to define primary motor cortex (M1), and with BA 1, 2, and 3 combined to define primary somatosensory cortex (S1) (<xref ref-type="bibr" rid="bib12">Brodmann, 1909</xref>; <xref ref-type="bibr" rid="bib32">Fulton, 1935</xref>; <xref ref-type="bibr" rid="bib69">Penfield and Boldrey, 1937</xref>). Final sizes of all visual and sensorimotor motor ROIs are reported in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></sec><sec id="s4-5"><title>Task: Main working memory</title><p>For all tasks described here, stimuli were projected onto a screen 21.3 cm wide × 16 cm high, fixed to the inside of the scanner bore just above the participant’s chest. The screen was viewed through a tilted mirror attached to the headcoil, from a viewing distance of 49 cm. This resulted in a maximum vertical extent (i.e. bottom to top) of 18.5°, and a maximum vertical eccentricity (i.e. central fixation to top) of 9.3°. The background was always a mid-gray color, and the fixation point was always a black circle with radius 0.2°. All stimuli were generated using Ubuntu 14.04, Matlab 2017b, and the Psychophysics toolbox (<xref ref-type="bibr" rid="bib11">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib51">Kleiner et al., 2007</xref>).</p><p>During runs of the main WM task, the overall task that participants performed was to remember the position of a small target dot, maintain it across a delay period, and then report which side of a spatial boundary the remembered position had fallen on (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each trial began with the fixation point turning green for 750 ms, to alert the participant that the spatial memory target was about to appear. Next, a white target dot (radius = 0.15°) appeared for 500 ms at a pseudo-random position on an imaginary ring 7° away from fixation (details on target positions given two paragraphs down). Participants were required to remember the precise position of this target dot. After presentation of this spatial memory target, the fixation point turned back to black for 1 s, then turned either red or blue for 2 s. This color cue indicated to the participant whether the current trial was an ‘informative’ or ‘uninformative’ trial (see next paragraph for explanation of the conditions). Next, a disk stimulus appeared for 1 s. This stimulus consisted of a circle 9.7° in radius, divided into two equal halves, with each side a different shade of gray (visibly lighter and darker relative to the mean-gray background; see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The disk could be rotated about its center by an amount between 1 and 360°. To avoid the disk overlapping with the fixation point, an aperture of radius 0.4° was cut out of the middle of the disk, creating a donut-like shape. The inner and outer edges of the donut were smoothed with a 2D Gaussian kernel (size = 0.5°, sigma = 0.5°), but the boundary between the two halves of the disk was sharp. This ‘preview disk’ stimulus was followed by a 12 s delay period. Following the delay period, a second disk stimulus appeared for 2 s, serving as the response probe (i.e. the ‘response disk’). At this point, participants responded with a button press to indicate which side of the response disk the memory target had been presented on. Specifically, they used either their left or right index finger to indicate whether the target dot fell within the light gray or dark gray side of the response disk. On each scan run, the light gray shade and dark gray shade were each associated with one response finger, and the mapping between shade of gray (light or dark) and finger (left or right index finger) was counterbalanced across sessions within each participant. Participants were reminded of the mapping between shades of gray and response fingers at the start of each scan run.</p><p>For trials in the ‘informative’ condition, the orientation of the response disk was identical to that of the preview disk. For trials in the ‘uninformative’ condition, the orientation of the response disk was random (and unrelated to the orientation of the preview disk). Thus, for the informative condition, participants had complete knowledge of the required action as soon as the preview disk appeared, but for the uninformative condition, they had no ability to anticipate the required action. Participants were instructed not to make any physical finger movements until the response disk appeared. Trials of the two task conditions (i.e. informative/uninformative) were randomly interleaved within every scan run. At the start of each scan run, the participant was shown an instruction screen which reminded them of the color/condition mapping and the shade of gray/finger mapping that was in effect for that session. The mapping between color (red/blue) and condition was counterbalanced across participants, but fixed within a given participant.</p><p>Each run of the main task consisted of 20 trials, with each trial followed by an inter-trial interval jittered randomly within a range of 1–5 s. The total length of each run was 466 s. Participants performed 10 runs of this task per scan session, and completed a total of 20 runs (or 400 trials) across two separate scan sessions. All counterbalancing was performed at the session level: within each session, there were 100 trials of each condition (informative or uninformative), and on each of these 100 trials the memory target was presented at a unique polar angle position. Specifically, target positions were sampled uniformly along a 360° circle, with a minimum spacing of 3.6° between possible target positions. The orientation of the response disk (which determined the spatial boundary to which the memory target position was compared) also took 100 unique and uniformly spaced values along a 360° circle within each condition. The possible disk orientations were shifted by 1.8° relative to the possible spatial memory positions, so that the memory position was never exactly on the boundary of the disk. To ensure that the joint distribution of memory positions and boundary positions was close to uniform, we broke the 100 possible positions along the 360° circle into 10 bins of 10 positions each. Across all 100 trials of each condition, each combination of the bin for spatial memory position and the bin for boundary orientation was used once. For the informative condition, the preview disk always took on the same orientation as the response disk. For the uninformative condition, the preview disk was assigned a random orientation, using the same 100 orientations used for the response disk but in a random order. Finally, trials for both task conditions were randomly shuffled and split evenly into 10 runs. As a result, task condition, memory target position, and response disk orientation were balanced across each session, but not balanced within individual runs.</p><p>To encourage participants to encode the spatial positions with high precision, we rewarded participants monetarily for correct performance on ‘hard’ trials on which the spatial memory target was close to the boundary. These ‘hard’ trials were identified as those where the spatial memory item and the boundary belonged in the same bin, according to the angular position bins described above. Participants received $1 for correct performance on each ‘hard’ trial, for a maximum of $40. Across participants, the average bonus received was $32.83 ± 2.86.</p><p>At no time during the experiment or the training period was the purpose of the experiment revealed to participants. The purpose of this was to ensure that participants would not explicitly use a strategy that could lead to an alteration in their neural activation patterns in the different task conditions. Three of the six participants were members of the research group conducting the experiments, yet, our effects were reliable on a single-participant basis across all participants.</p></sec><sec id="s4-6"><title>Task: Spatial working memory mapping</title><p>Participants also performed an additional WM task while in the scanner, which served as training data for our classification analyses (see <italic>Analysis: Spatial position decoding</italic>). Identical to the main WM task, each trial began with the fixation point briefly turning green (750 ms), followed by a spatial memory target item (500 ms) at a random position (at 7° from fixation). The disappearance of the target was followed by a 12-s delay period, after which a white probe dot (radius = 0.15°) appeared at a random position (independent from the target position, also at 7° from fixation). Participants moved this probe dot around an invisible circle to match the position at which the memory target had been presented. Participants used the four fingers of their right hand to press different buttons that corresponded to fast (120°/s, outer two fingers) or slow (40°/s, inner two fingers) movement of the probe dot in a counterclockwise (left two fingers) or clockwise (right two fingers) direction. This response period lasted 3 s, during which participants were able to move the dot back and forth to adjust its position as much as they wished. Once the 3-s response period was over, the probe dot disappeared and participants had no further ability to change their response. The final position of the probe dot was taken as the participant’s response, and no further (visual) feedback of the response was provided.</p><p>Each run of the spatial WM mapping task consisted of 20 trials, with trials separated by an inter-trial interval jittered randomly within a range of 1–5 s. The total run length was 406 s. Participants performed 10 runs of this task in total, collected during a single scanning session. Across all 200 trials of the task, the spatial position of memory targets took on 200 distinct values uniformly spaced within a 360° space (i.e. 1.8° apart). To ensure a fairly even sampling of this space within individual runs, we binned these 200 possible positions into 20 bins of 10 positions each and generated a sequence where each run sampled from each bin once. The random starting position of the probe dot on each trial was generated using an identical procedure, but independently of the memory targets, so that there was no association between the position of the spatial memory target and the probe start position. The absolute average angular error across six participants on this task was 7.0 ± 0.9°.</p></sec><sec id="s4-7"><title>Task: Spatial localizer</title><p>We ran a spatial localizer task for two purposes, namely, to identify voxels having spatial selectivity within the region of space spanned by the memory positions (see <italic>Identifying ROIs</italic>), and to serve as training data for our classification analyses (see <italic>Analysis: Spatial position decoding</italic>). In this task, participants viewed black and white checkerboard wedges flickering at a rate of 4 Hz. Wedges had a width of 15° (polar angle), spanned an eccentricity range of 4.4–9.3° (visual angle), and were positioned at 24 different positions around an imaginary circle. Possible wedge center positions were offset from the cardinal axes by 7.5° (i.e. a wedge was never centered on the horizontal or vertical meridian). Each run included four wedge presentations at each position, totaling 96 trials. The sequence of positions was random with the constraint that consecutively presented wedges never appeared in the same quadrant. Trials were 3 s each and were not separated by an inter-trial interval. The total run length was 313 s. During each run, participants performed a change-detection task at fixation, where they responded with a button press any time the fixation point increased or decreased in brightness. A total of 20 brightness changes occurred in each run, at times that were random with respect to trial onsets. The magnitude of brightness changes was adjusted manually at the start of each run to control the difficulty. Average detection performance (hit rate) was 76.7 ± 4.2%. Participants performed between 8 and 16 total runs of this task. For some participants, some of these runs were collected as part of a separate experiment.</p></sec><sec id="s4-8"><title>Task: Sensorimotor cortex localizer</title><p>Participants also performed a sensorimotor cortex localizer task in the scanner. Analogous to our use of the spatial localizer task, this data served a dual purpose: it was used to identify ROIs in motor and somatosensory cortex that were selective for contralateral index finger button presses (see <italic>Identifying ROIs</italic>), and as a training set for one of our classification analyses (see <italic>Analysis: Action decoding</italic>). Participants attended a black fixation point (0.2°), and responded to brief (1000 ms) color changes of the fixation point by pressing a button with their left or right index finger. The fixation dot changed to either magenta or cyan to indicate which finger should be used, and each color change was separated by an inter-trial interval randomly jittered in the range of 2–6 s. Each run was 319 s long, and included 60 total trials (i.e. 60 button presses), with 30 trials for each finger randomly interleaved. The color/finger mapping was switched on alternating runs. Participants were instructed to respond as quickly as possible to each color change. Average performance on this task was 92.8 ± 3.0% correct, and average behavioral response time was 530 ± 23 ms. Each participant performed six runs of this task.</p></sec><sec id="s4-9"><title>Analysis: Univariate</title><p>In order to estimate an HRF for each voxel during each condition (<xref ref-type="fig" rid="fig1">Figure 1D</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) we used linear deconvolution. We constructed a finite impulse response model (<xref ref-type="bibr" rid="bib21">Dale, 1999</xref>) that included a series of regressors for trials in each task condition: one regressor marking the onset of a spatial memory target item, followed by a series of temporally shifted versions of that regressor (to model the BOLD response at each subsequent time point in the trial). The model also included a constant regressor for each of the 20 total runs. The data used as input to this GLM was z-scored on a voxel-by-voxel basis within runs (see <italic>Pre-processing of MRI data</italic>). Estimated HRFs for the two conditions were averaged across all voxels within each ROI. To evaluate whether the mean BOLD signal in each ROI differed significantly between conditions, we used a permutation test. First, for each ROI and timepoint, we computed a Wilcoxon signed rank statistic comparing the activation values for each participant from condition 1 to the activation values for each participant from condition 2. Then, we performed 1000 iterations of shuffling the condition labels within each participant (swapping the condition labels for each participant with 50% probability). We then computed a signed rank statistic from the shuffled values on each iteration. Finally, we computed a two-tailed p-value for each ROI and timepoint by computing the number of iterations on which the shuffled signed rank statistic was ≥ the real statistic, and the number of iterations on which the shuffled statistic was ≤ the real statistic, and taking the smaller of these two values. We obtained the final p-value by dividing this value by the number of iterations and multiplying by 2.</p><p>For the analysis in which we separated contralateral and ipsilateral response trials (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), we performed the same procedure described above, except that for each voxel, we separated its responses into trials where the correct behavioral response corresponded to the index finger contralateral or ipsilateral to the voxel’s brain hemisphere. This resulted in a separate estimate of each voxel’s HRF for contralateral and ipsilateral trials in each condition. We then averaged the HRFs from each task condition and response type (contralateral or ipsilateral) across all voxels in both hemispheres of each ROI. Finally, within each condition, we tested the difference between contralateral and ipsilateral trials with a signed rank test with permutation as described above.</p></sec><sec id="s4-10"><title>Analysis: Spatial position decoding</title><p>We used linear classification to measure representations of remembered spatial position information in each visual and sensorimotor ROI during the main WM task. Since we were interested in assessing the coding format of memory representations, we separately performed decoding using three different approaches. In the first approach (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), we trained the decoder on independent data measured while participants were <italic>remembering</italic> a given spatial position (see <italic>Task: Spatial working memory mapping</italic>). In the second approach (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), we trained the decoder on independent data collected when participants were <italic>perceiving</italic> a physical stimulus at a given spatial position (see <italic>Task: Spatial localizer</italic>). In the final method (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), we used cross-validation to train and test our decoder using data from within each condition of the main WM task.</p><p>Before performing each of these classification methods, we first mean-centered the voxel activation pattern from each trial in each task by subtracting the mean across all voxels from each trial. Next, we binned all trials of the main WM task (see <italic>Task: Main working memory</italic>) and the spatial WM mapping task (see <italic>Task: Spatial working memory mapping</italic>) into eight angular position bins that each spanned 45°, with the first bin centered at 0°. Trials from the spatial localizer task (see <italic>Task: Spatial localizer</italic>) were binned into eight angular position bins that each spanned 60° (i.e. the bins were slightly overlapping and some wedge positions contributed to multiple bins; similar results were obtained using bins that were entirely non-overlapping). We then performed binary classification between pairs of bins that were 180° apart (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>), using a linear classifier based on the normalized Euclidean distance (for more details see <xref ref-type="bibr" rid="bib41">Henderson and Serences, 2019</xref>). This meant that we constructed four separate binary classifiers, each operating on approximately one-fourth of the data, with a chance decoding value of 50%. We then averaged across the four binary classifiers to get a single index of classification accuracy for each ROI and task condition.</p><p>For the results shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, the training set for these classifiers consisted of data from the spatial WM mapping task (averaged within a fixed time window during the delay period 4.8–9.6 s after trial onset), and the test set consisted of data from the main WM task (either averaged over a window of 8–12.8 s after trial onset, or at each individual TR following trial onset). For the results based on sensory-driven responses (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), the training set consisted of data from the spatial localizer task (averaged over a window 3.2–5.6 s after stimulus onset), and the test set consisted of data from the main WM task (same time window defined above). For the within-condition analyses (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), the training and testing sets both consisted of data from a single condition in the main WM task (either informative or uninformative; same time window defined above). Each binary classifier was cross-validated by leaving out two trials at a time for testing (leaving out one trial per class ensures the training set was always balanced), and looping over cross-validation folds so that every trial served as a test trial once.</p><p>To test whether decoding performance was significantly above chance in each ROI and condition of the main WM task, we used a permutation test. On each of 1000 iterations, we shuffled the binary labels for the training set, trained a classifier on this shuffled data, and then computed how well this decoder predicted the binary labels in the test set. For each iteration, we then computed a Wilcoxon signed rank statistic comparing the N participants’ real decoding values to the N participants’ shuffled decoding values. A signed rank statistic greater than 0 indicated the median of the real decoding values was greater than the median of the shuffled decoding values, and a statistic less than zero indicated the median of the null decoding values was greater than the median of the real values. We obtained a one-tailed p-value for each ROI and task condition across all participants by counting the number of iterations on which the signed rank statistic was less than or equal to zero, and dividing by the total number of iterations.</p><p>To test whether decoding performance differed significantly between the two task conditions within each ROI, we used a permutation test. First, for each ROI, we computed a Wilcoxon signed rank statistic comparing the N participants’ decoding values from condition 1 to the N participants’ decoding values from condition 2. Then, we performed 1000 iterations of shuffling the condition labels within each participant (swapping the condition labels for each participant with 50% probability). We then computed a signed rank statistic from the shuffled values. Finally, we computed a two-tailed p-value for each ROI by computing the number of iterations on which the shuffled signed rank statistic was ≥ the real statistic, and the number of iterations on which the shuffled statistic was ≤ the real statistic, and taking the smaller of these two values. We obtained the final p-value by dividing this value by the total number of iterations and multiplying by 2.</p><p>The above procedures were used for all time-averaged and time-resolved decoding analyses (i.e. for time-resolved analyses we repeated the same statistical procedures at each timepoint separately). For the time-averaged decoding accuracies, we also performed a two-way repeated measures ANOVA with factors of ROI, condition, and a ROI × condition interaction (implemented using ranova.m). We performed a permutation test where we shuffled the decoding scores within each participant 1000 times, and computed an F-statistic for each effect on the shuffled data. Across all permutations, we obtained a null-distribution of F-values for effects of ROI, condition, and the ROI × condition interaction. The final p-values for each effect were based on the number of times the shuffled F-statistic for that effect was greater than or equal to the real F-statistic, divided by the total number of iterations (similar to method used in <xref ref-type="bibr" rid="bib70">Rademaker et al., 2019</xref>). F-statistics reported in the text reflect the F-statistic obtained using the real (unshuffled) data.</p></sec><sec id="s4-11"><title>Analysis: Preview disk orientation decoding</title><p>To evaluate whether the difference between our task conditions was specific to the memory representations or a more global difference in pattern signal-to-noise ratio, we performed linear decoding of the orientation of the preview disk stimulus (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). The disk stimulus (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) consisted of a light and a dark half, separated by a linear boundary that could range in orientation from 0 to 180° (for this analysis, we ignored the coloration of the disk stimulus, meaning which side was light gray and which was dark gray, and focused only on the orientation of the boundary). To run the decoding analysis, we first binned all trials according to the boundary orientation, using four bins that were centered at 0, 45, 90, and 135°, each bin 45° in size. We then trained and tested two separate linear decoders: one that classified the difference between the 0 and 90° bins, and one that classified the difference between the 45 and 135° bins. The final decoding accuracy value was the average of the accuracy from the two individual classifiers. This decoding approach is conceptually similar to that used for the spatial decoding analysis (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and <italic>Analysis: Spatial position decoding</italic>) and also similar to that used by <xref ref-type="bibr" rid="bib70">Rademaker et al., 2019</xref> for orientation decoding. The classifiers were always trained on data from one task condition at a time, and cross-validated by leaving one session of data out at a time (train on session 1, test on session 2, or vice versa). To capture the peak response to the preview disk presentation, for this analysis we used data averaged over a time window from 4.8 to 9.6 s after the trial start (note the preview disk appeared 3.5 s into the trial). As in the spatial decoding analyses, we mean-centered the voxel activation pattern for each trial by subtracting the mean activation across voxels before performing the classification analysis. All statistical testing for the results of disk orientation decoding was done in an identical manner to the method described for spatial decoding accuracy (see <italic>Analysis: Spatial position decoding</italic>).</p></sec><sec id="s4-12"><title>Analysis: Action decoding</title><p>We performed linear classification (as above) to measure the representation of information related to left or right index finger button presses in each ROI. To assess the coding format of action representations, we separately performed decoding using two different approaches. In the first approach, we used data from the main WM task to train and test the classifier (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Here, action classification was always done using data from one task condition at a time (i.e. informative or uninformative trials). The decoder was always trained on data from one session and tested on the other session. Because the mapping of disk side color (light or dark gray) to finger was always switched between the two sessions, this ensured that the information detected by the classifier was not related to the luminance of the half of the disk corresponding to the response finger. In the second approach, we trained the classifier using data from a separate task during which participants were <italic>physically pressing a button</italic> (see <italic>Task: Sensorimotor cortex localizer</italic>), and we tested using data from the main WM task (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Irrespective of the training-testing approach used, classification was based on trials labeled according to the finger (left or right index) that corresponded to the correct response. This means that all trials were included (also those where the incorrect button, or no button, was pressed), which ensured that the training set for the classifier was balanced. Note that qualitatively similar results were obtained when using correct trials only, or when using the participant’s actual response as the label for the decoder.</p><p>The above procedure was used for both time-averaged (<xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="fig" rid="fig4">Figure 4B</xref>) and time-resolved action decoding (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). For time-averaged decoding, single trial responses were obtained by averaging over specified time windows after trial onset (8–12.8 s for the main WM task; 3.2–5.6 s for the sensorimotor cortex localizer). For time-resolved decoding on the main WM task data, the training and testing set each consisted of data from the TR of interest (but from different sessions, as described above). All statistical tests on the results of action decoding were performed in an identical manner to the statistics on the results of spatial decoding (see <italic>Analysis: Spatial position decoding</italic>).</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, eLife</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Investigation, Methodology, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Software, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study protocol was approved by the Institutional Review Board at UCSD (approval code 180067), and all participants provided written informed consent and consent to publish.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Table listing the number of voxels in each ROI for each participant and hemisphere.</title><p>Sizes of retinotopic visual ROIs (V1-IPS3) are after thresholding with a spatial localizer (see <italic>Methods, Task: Spatial Localizer</italic>). S1, M1, and PMc were defined using a button-pressing task (see <italic>Methods, Task: Sensorimotor Cortex Localizer</italic>). All analyses in this paper were done using bilateral ROIs (i.e. concatenating the left and right hemispheres of each ROI).</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-75688-supp1-v1.xlsx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-75688-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The dataset from this manuscript is publicly available on Open Science Framework at <ext-link ext-link-type="uri" xlink:href="https://osf.io/te5g2/">https://osf.io/te5g2/</ext-link>. All code associated with the manuscript is publicly available on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/mmhenderson/wm_flex">https://github.com/mmhenderson/wm_flex</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0ba549286a2b0f7cb57b44d9781093c78c7f9c54;origin=https://github.com/mmhenderson/wm_flex;visit=swh:1:snp:6883d982e52f71626f0ab61e7ab72b09576d73b3;anchor=swh:1:rev:a25485d9983bd13b99f99f98d24b0faeaff50005">swh:1:rev:a25485d9983bd13b99f99f98d24b0faeaff50005</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>MM</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Open Data 2022: Flexible utilization of spatial- and motor-based codes for the storage of visuo-spatial information</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/te5g2/">10.17605/OSF.IO/TE5G2</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Funded by NEI R01-EY025872 to JS, NIMH Training Grant in Cognitive Neuroscience (T32-MH020002) to MH, and a European Union’s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie Grant Agreement No 743,941 to RR and NIH-MH087214. We thank Kirsten Adam, Timothy Sheehan, and Sunyoung Park for helpful discussions and data collection.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Skare</surname><given-names>S</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>How to correct susceptibility distortions in spin-echo echo-planar images: application to diffusion tensor imaging</article-title><source>NeuroImage</source><volume>20</volume><fpage>870</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00336-7</pub-id><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ariani</surname><given-names>G</given-names></name><name><surname>Pruszynski</surname><given-names>JA</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Motor planning brings human primary somatosensory cortex into action-specific preparatory states</article-title><source>eLife</source><volume>11</volume><elocation-id>e69517</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.69517</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atkinson</surname><given-names>RC</given-names></name><name><surname>Shiffrin</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Human Memory: A Proposed System and its Control Processes</article-title><source>Psychology of Learning and Motivation</source><volume>2</volume><fpage>89</fpage><lpage>195</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Awh</surname><given-names>E</given-names></name><name><surname>Jonides</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Overlapping mechanisms of attention and spatial working memory</article-title><source>Trends in Cognitive Sciences</source><volume>5</volume><fpage>119</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(00)01593-x</pub-id><pub-id pub-id-type="pmid">11239812</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>AD</given-names></name><name><surname>Hitch</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Working memory</article-title><source>Psychology of Learning and Motivation</source><volume>8</volume><fpage>47</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/S0079-7421(08)60452-1</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Soldevilla</surname><given-names>DL</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unattended Short-Term Memories Are Maintained in Active Neural Representations</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://psyarxiv.com/qv6fu/">https://psyarxiv.com/qv6fu/</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bettencourt</surname><given-names>KC</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decoding the content of visual short-term memory under distraction in occipital and parietal areas</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/nn.4174</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettcher</surname><given-names>SEP</given-names></name><name><surname>Gresch</surname><given-names>D</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Output planning at the input stage in visual working memory</article-title><source>Science Advances</source><volume>7</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abe8212</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spikes, BOLD, attention, and awareness: A comparison of electrophysiological and fMRI signals in V1</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1167/11.5.12</pub-id><pub-id pub-id-type="pmid">22199162</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodmann</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1909">1909</year><article-title>Physiology of the cortex as an organ</article-title><source>Brodmann’s Localisation in the Cerebral Cortex</source><volume>1</volume><fpage>239</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1007/b138298</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calderon</surname><given-names>CB</given-names></name><name><surname>Van Opstal</surname><given-names>F</given-names></name><name><surname>Peigneux</surname><given-names>P</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name><name><surname>Gevers</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task-Relevant Information Modulates Primary Motor Cortex Activity Before Movement Onset</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><elocation-id>93</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00093</pub-id><pub-id pub-id-type="pmid">29593518</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decoding the contents of visual short-term memory from human visual and parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12983</fpage><lpage>12989</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0184-12.2012</pub-id><pub-id pub-id-type="pmid">22993415</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical specialization for attended versus unattended working memory</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>494</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0094-4</pub-id><pub-id pub-id-type="pmid">29507410</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural Correlates of Reaching Decisions in Dorsal Premotor Cortex: Specification of Multiple Direction Choices and Final Selection of Action</article-title><source>Neuron</source><volume>45</volume><fpage>801</fpage><lpage>814</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.01.027</pub-id><pub-id pub-id-type="pmid">15748854</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural Mechanisms for Interacting with a World Full of Action Choices</article-title><source>Annual Review of Neuroscience</source><volume>33</volume><fpage>269</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135409</pub-id><pub-id pub-id-type="pmid">20345247</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Persistent activity in the prefrontal cortex during working memory</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>415</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00197-9</pub-id><pub-id pub-id-type="pmid">12963473</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Rao</surname><given-names>VY</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Maintenance of spatial and motor codes during oculomotor delayed response tasks</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>3944</fpage><lpage>3952</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5640-03.2004</pub-id><pub-id pub-id-type="pmid">15102910</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Selective averaging of rapidly presented individual trials using fMRI</article-title><source>Human Brain Mapping</source><volume>5</volume><fpage>329</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1997)5:5&lt;329::AID-HBM1&gt;3.0.CO;2-5</pub-id><pub-id pub-id-type="pmid">20408237</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Optimal experimental design for event-related fMRI</article-title><source>Human Brain Mapping</source><volume>8</volume><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">10524601</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical Surface-Based Analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Buildup of choice-predictive activity in human motor cortex during perceptual decision making</article-title><source>Current Biology</source><volume>19</volume><fpage>1581</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.07.066</pub-id><pub-id pub-id-type="pmid">19747828</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>From cognitive to neural models of working memory</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>362</volume><fpage>761</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2086</pub-id><pub-id pub-id-type="pmid">17400538</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emrich</surname><given-names>SM</given-names></name><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Larocque</surname><given-names>JJ</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Distributed patterns of activity in sensory cortex reflect the precision of multiple items maintained in visual short-term memory</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>6516</fpage><lpage>6523</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5732-12.2013</pub-id><pub-id pub-id-type="pmid">23575849</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>SA</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title><source>Cerebral Cortex (New York, N.Y)</source><volume>7</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id><pub-id pub-id-type="pmid">9087826</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spatially global representations in human primary visual cortex during working memory maintenance</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>15258</fpage><lpage>15265</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4388-09.2009</pub-id><pub-id pub-id-type="pmid">19955378</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Parietal and Frontal Cortex Encode Stimulus-Specific Mnemonic Representations during Visual Working Memory</article-title><source>Neuron</source><volume>87</volume><fpage>893</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.07.013</pub-id><pub-id pub-id-type="pmid">26257053</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How Do Visual and Parietal Cortex Contribute to Visual Short-Term Memory?</article-title><source>ENeuro</source><volume>3</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1523/ENEURO.0041-16.2016</pub-id><pub-id pub-id-type="pmid">27294194</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferreira</surname><given-names>F</given-names></name><name><surname>Apel</surname><given-names>J</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Taking a new look at looking at nothing</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>405</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.07.007</pub-id><pub-id pub-id-type="pmid">18805041</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Rajendran</surname><given-names>N</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Augustinack</surname><given-names>J</given-names></name><name><surname>Hinds</surname><given-names>O</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical folding patterns and predicting cytoarchitecture</article-title><source>Cerebral Cortex (New York, N.Y)</source><volume>18</volume><fpage>1973</fpage><lpage>1980</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm225</pub-id><pub-id pub-id-type="pmid">18079129</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fulton</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="1935">1935</year><article-title>A note on the definition of the “motor” and “premotor” areas</article-title><source>Brain</source><volume>58</volume><fpage>311</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1093/brain/58.2.311</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname><given-names>S</given-names></name><name><surname>Bruce</surname><given-names>CJ</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Mnemonic coding of visual space in the monkey’s dorsolateral prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>61</volume><fpage>331</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1152/jn.1989.61.2.331</pub-id><pub-id pub-id-type="pmid">2918358</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname><given-names>S</given-names></name><name><surname>Chafee</surname><given-names>MV</given-names></name><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Prefrontal neuronal activity in rhesus monkeys performing a delayed anti-saccade task</article-title><source>Nature</source><volume>365</volume><fpage>753</fpage><lpage>756</lpage><pub-id pub-id-type="doi">10.1038/365753a0</pub-id><pub-id pub-id-type="pmid">8413653</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname><given-names>JM</given-names></name><name><surname>Alexander</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Neuron activity related to short-term memory</article-title><source>Science (New York, N.Y.)</source><volume>173</volume><fpage>652</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1126/science.173.3997.652</pub-id><pub-id pub-id-type="pmid">4998337</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gazzaley</surname><given-names>A</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Top-down modulation: bridging selective attention and working memory</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>129</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.11.014</pub-id><pub-id pub-id-type="pmid">22209601</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goense</surname><given-names>JBM</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neurophysiology of the BOLD fMRI signal in awake monkeys</article-title><source>Current Biology</source><volume>18</volume><fpage>631</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.03.054</pub-id><pub-id pub-id-type="pmid">18439825</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Cellular basis of working memory</article-title><source>Neuron</source><volume>14</volume><fpage>477</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/0896-6273(95)90304-6</pub-id><pub-id pub-id-type="pmid">7695894</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id><pub-id pub-id-type="pmid">19225460</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>M</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human frontoparietal cortex represents behaviorally relevant target status based on abstract object features</article-title><source>Journal of Neurophysiology</source><volume>121</volume><fpage>1410</fpage><lpage>1427</lpage><pub-id pub-id-type="doi">10.1152/jn.00015.2019</pub-id><pub-id pub-id-type="pmid">30759040</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>wm_flex</data-title><version designator="a25485d">a25485d</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/mmhenderson/wm_flex">https://github.com/mmhenderson/wm_flex</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heuer</surname><given-names>A</given-names></name><name><surname>Ohl</surname><given-names>S</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Memory for action: a functional view of selection in visual working memory</article-title><source>Visual Cognition</source><volume>28</volume><fpage>388</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1080/13506285.2020.1764156</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Essential considerations for exploring visual working memory storage in the human brain</article-title><source>Visual Cognition</source><volume>29</volume><fpage>425</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1080/13506285.2021.1915902</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A global optimisation method for robust affine registration of brain images</article-title><source>Medical Image Analysis</source><volume>5</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/s1361-8415(01)00036-6</pub-id><pub-id pub-id-type="pmid">11516708</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(02)91132-8</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FSL</article-title><source>NeuroImage</source><volume>62</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id><pub-id pub-id-type="pmid">21979382</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerde</surname><given-names>TA</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Maps of space in human frontoparietal cortex</article-title><source>Journal of Physiology, Paris</source><volume>107</volume><fpage>510</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1016/j.jphysparis.2013.04.002</pub-id><pub-id pub-id-type="pmid">23603831</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonides</surname><given-names>J</given-names></name><name><surname>Lacey</surname><given-names>SC</given-names></name><name><surname>Nee</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Processes of Working Memory in Mind and Brain</article-title><source>Current Directions in Psychological Science</source><volume>14</volume><fpage>2</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1111/j.0963-7214.2005.00323.x</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein-Flügge</surname><given-names>MC</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Time-dependent changes in human corticospinal excitability reveal value-based competition for action during decision processing</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>8373</fpage><lpage>8382</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0270-12.2012</pub-id><pub-id pub-id-type="pmid">22699917</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Pelli</surname><given-names>DG</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name><name><surname>Wolf</surname><given-names>T</given-names></name><name><surname>Niehorster</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3? A free cross-platform toolkit for psychophysiscs with Matlab and GNU/Octave</article-title><source>In Cognitive and Computational Psychophysics</source><volume>36</volume><elocation-id>v070821</elocation-id><pub-id pub-id-type="doi">10.1068/v070821</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LaRocque</surname><given-names>JJ</given-names></name><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Emrich</surname><given-names>SM</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Within-Category Decoding of Information in Different Attentional States in Short-Term Memory</article-title><source>Cerebral Cortex (New York, N.Y)</source><volume>27</volume><fpage>4881</fpage><lpage>4890</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw283</pub-id><pub-id pub-id-type="pmid">27702811</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leavitt</surname><given-names>ML</given-names></name><name><surname>Mendoza-Halliday</surname><given-names>D</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sustained Activity Encoding Working Memories: Not Fully Distributed</article-title><source>Trends in Neurosciences</source><volume>40</volume><fpage>328</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.04.004</pub-id><pub-id pub-id-type="pmid">28515011</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Goal-dependent dissociation of visual and prefrontal cortices during working memory</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>997</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nn.3452</pub-id><pub-id pub-id-type="pmid">23817547</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name><name><surname>Drysdale</surname><given-names>AT</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural evidence for a distinction between short-term memory and the focus of attention</article-title><source>Journal of Cognitive Neuroscience</source><volume>24</volume><fpage>61</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00140</pub-id><pub-id pub-id-type="pmid">21955164</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name><name><surname>Drysdale</surname><given-names>AT</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural Evidence for the Flexible Control of Mental Representations</article-title><source>Cerebral Cortex (New York, N.Y)</source><volume>25</volume><fpage>3303</fpage><lpage>3313</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu130</pub-id><pub-id pub-id-type="pmid">24935778</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Pauls</surname><given-names>J</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Trinath</surname><given-names>T</given-names></name><name><surname>Oeltermann</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neurophysiological investigation of the basis of the fMRI signal</article-title><source>Nature</source><volume>412</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1038/35084005</pub-id><pub-id pub-id-type="pmid">11449264</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Interpreting THE BOLD signal</article-title><source>Annual Review of Physiology</source><volume>66</volume><fpage>735</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1146/annurev.physiol.66.082602.092845</pub-id><pub-id pub-id-type="pmid">14977420</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenc</surname><given-names>ES</given-names></name><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Nee</surname><given-names>DE</given-names></name><name><surname>Vandenbroucke</surname><given-names>ARE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible Coding of Visual Working Memory Representations during Distraction</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>5267</fpage><lpage>5276</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3061-17.2018</pub-id><pub-id pub-id-type="pmid">29739867</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenc</surname><given-names>ES</given-names></name><name><surname>Vandenbroucke</surname><given-names>ARE</given-names></name><name><surname>Nee</surname><given-names>DE</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dissociable neural mechanisms underlie currently-relevant, future-relevant, and discarded working memory representations</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>11195</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-67634-x</pub-id><pub-id pub-id-type="pmid">32641712</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendoza-Halliday</surname><given-names>D</given-names></name><name><surname>Torres</surname><given-names>S</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sharp emergence of feature-selective sustained activity along the dorsal visual pathway</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1255</fpage><lpage>1262</lpage><pub-id pub-id-type="doi">10.1038/nn.3785</pub-id><pub-id pub-id-type="pmid">25108910</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Erickson</surname><given-names>CA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Neural mechanisms of visual working memory in prefrontal cortex of the macaque</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>5154</fpage><lpage>5167</lpage><pub-id pub-id-type="pmid">8756444</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prioritizing Information during Working Memory: Beyond Sustained Internal Attention</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>449</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.010</pub-id><pub-id pub-id-type="pmid">28454719</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Premembering Experience: A Hierarchy of Time-Scales for Proactive Attention</article-title><source>Neuron</source><volume>104</volume><fpage>132</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.08.030</pub-id><pub-id pub-id-type="pmid">31600510</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Offen</surname><given-names>S</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The role of early visual cortex in visual short-term memory and visual attention</article-title><source>Vision Research</source><volume>49</volume><fpage>1352</fpage><lpage>1362</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.12.022</pub-id><pub-id pub-id-type="pmid">18329065</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohl</surname><given-names>S</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bold moves: Inevitable saccadic selection in visual short-term memory</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.2.11</pub-id><pub-id pub-id-type="pmid">32106297</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Error-correcting dynamics in visual working memory</article-title><source>Nature Communications</source><volume>10</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-11298-3</pub-id><pub-id pub-id-type="pmid">31358740</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasternak</surname><given-names>T</given-names></name><name><surname>Greenlee</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Working memory in primate sensory systems</article-title><source>Nature Reviews. Neuroscience</source><volume>6</volume><fpage>97</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nrn1603</pub-id><pub-id pub-id-type="pmid">15654324</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penfield</surname><given-names>W</given-names></name><name><surname>Boldrey</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1937">1937</year><article-title>Somatic motor and sensory representation in the cerebral cortex of man as studied by electrical stimulation</article-title><source>Brain</source><volume>60</volume><fpage>389</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1093/brain/60.4.389</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id><pub-id pub-id-type="pmid">31263205</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahmati</surname><given-names>M</given-names></name><name><surname>Saber</surname><given-names>GT</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Population Dynamics of Early Visual Cortex during Working Memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>219</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01196</pub-id><pub-id pub-id-type="pmid">28984524</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The relationship between working memory storage and elevated activity as measured with functional magnetic resonance imaging</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12990</fpage><lpage>12998</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1892-12.2012</pub-id><pub-id pub-id-type="pmid">22993416</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>NS</given-names></name><name><surname>LaRocque</surname><given-names>JJ</given-names></name><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Gosseries</surname><given-names>O</given-names></name><name><surname>Starrett</surname><given-names>MJ</given-names></name><name><surname>Meyering</surname><given-names>EE</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reactivation of latent working memories with transcranial magnetic stimulation</article-title><source>Science (New York, N.Y.)</source><volume>354</volume><fpage>1136</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1126/science.aah7011</pub-id><pub-id pub-id-type="pmid">27934762</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saber</surname><given-names>GT</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Saccade Planning Evokes Topographically Specific Activity in the Dorsal and Ventral Streams</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>245</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1687-14.2015</pub-id><pub-id pub-id-type="pmid">25568118</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>D</given-names></name><name><surname>Barth</surname><given-names>A</given-names></name><name><surname>Wascher</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>On the contribution of motor planning to the retroactive cuing benefit in working memory: Evidence by mu and beta oscillatory activity in the EEG</article-title><source>NeuroImage</source><volume>162</volume><fpage>73</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.057</pub-id><pub-id pub-id-type="pmid">28847491</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus-Specific Delay Activity in Human Primary Visual Cortex</article-title><source>Psychological Science</source><volume>20</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02276.x</pub-id><pub-id pub-id-type="pmid">19170936</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural mechanisms of information storage in visual short-term memory</article-title><source>Vision Research</source><volume>128</volume><fpage>53</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2016.09.010</pub-id><pub-id pub-id-type="pmid">27668990</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Borders of Multiple Visual Areas in Humans Revealed by Functional Magnetic Resonance Imaging</article-title><source>Science</source><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1126/science.7754376</pub-id><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Fast robust automated brain extraction</article-title><source>Human Brain Mapping</source><volume>17</volume><fpage>143</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1002/hbm.10062</pub-id><pub-id pub-id-type="pmid">12391568</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souza</surname><given-names>AS</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>In search of the focus of attention in working memory: 13 years of the retro-cue effect</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>78</volume><fpage>1839</fpage><lpage>1860</lpage><pub-id pub-id-type="doi">10.3758/s13414-016-1108-5</pub-id><pub-id pub-id-type="pmid">27098647</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spivey</surname><given-names>MJ</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Oculomotor mechanisms activated by imagery and memory: eye movements to absent objects</article-title><source>Psychological Research</source><volume>65</volume><fpage>235</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/s004260100059</pub-id><pub-id pub-id-type="pmid">11789427</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Reconstructions of information in visual spatial working memory degrade with memory load</article-title><source>Current Biology</source><volume>24</volume><fpage>2174</fpage><lpage>2180</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.07.066</pub-id><pub-id pub-id-type="pmid">25201683</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Restoring Latent Visual Working Memory Representations in Human Cortex</article-title><source>Neuron</source><volume>91</volume><fpage>694</fpage><lpage>707</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.006</pub-id><pub-id pub-id-type="pmid">27497224</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Importance of Considering Model Choices When Interpreting Results in Computational Neuroimaging</article-title><source>ENeuro</source><volume>6</volume><elocation-id>ENEURO.0196-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0196-19.2019</pub-id><pub-id pub-id-type="pmid">31772033</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sreenivasan</surname><given-names>KK</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Revisiting the role of persistent neural activity during working memory</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>82</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.12.001</pub-id><pub-id pub-id-type="pmid">24439529</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name><name><surname>Lamme</surname><given-names>VAF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A neural correlate of working memory in the monkey primary visual cortex</article-title><source>Science (New York, N.Y.)</source><volume>293</volume><fpage>120</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1126/science.1060496</pub-id><pub-id pub-id-type="pmid">11441187</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swisher</surname><given-names>JD</given-names></name><name><surname>Halko</surname><given-names>MA</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>McMains</surname><given-names>SA</given-names></name><name><surname>Somers</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual topography of human intraparietal sulcus</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>5326</fpage><lpage>5337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0991-07.2007</pub-id><pub-id pub-id-type="pmid">17507555</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Chekroud</surname><given-names>SR</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Human gaze tracks attentional focusing in memorized visual space</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>462</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0549-y</pub-id><pub-id pub-id-type="pmid">31089296</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Chekroud</surname><given-names>SR</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Concurrent visual and motor selection during visual working memory guided action</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>477</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0335-6</pub-id><pub-id pub-id-type="pmid">30718904</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Board</surname><given-names>AG</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Goal-directed and stimulus-driven selection of internal representations</article-title><source>PNAS</source><volume>117</volume><fpage>24590</fpage><lpage>24598</lpage><pub-id pub-id-type="doi">10.1073/pnas.2013432117</pub-id><pub-id pub-id-type="pmid">32929036</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/ncomms13804</pub-id><pub-id pub-id-type="pmid">28054544</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Goal-Directed Visual Processing Differentially Impacts Human Ventral and Dorsal Visual Representations</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8767</fpage><lpage>8782</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3392-16.2017</pub-id><pub-id pub-id-type="pmid">28821655</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Brewer</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.10.012</pub-id><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Witthoft</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human V4 and ventral occipital retinotopic maps</article-title><source>Visual Neuroscience</source><volume>32</volume><elocation-id>E020</elocation-id><pub-id pub-id-type="doi">10.1017/S0952523815000176</pub-id><pub-id pub-id-type="pmid">26241699</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Ripley</surname><given-names>BD</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title><source>NeuroImage</source><volume>14</volume><fpage>1370</fpage><lpage>1386</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0931</pub-id><pub-id pub-id-type="pmid">11707093</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xing</surname><given-names>Y</given-names></name><name><surname>Ledgeway</surname><given-names>T</given-names></name><name><surname>McGraw</surname><given-names>PV</given-names></name><name><surname>Schluppeck</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Decoding working memory of stimulus contrast in early visual cortex</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>10301</fpage><lpage>10311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3754-12.2013</pub-id><pub-id pub-id-type="pmid">23785144</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sensory Cortex Is Nonessential in Working Memory Storage</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>192</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.12.008</pub-id><pub-id pub-id-type="pmid">29482822</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Revisit once more the sensory storage account of visual working memory</article-title><source>Visual Cognition</source><volume>28</volume><fpage>433</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1080/13506285.2020.1818659</pub-id><pub-id pub-id-type="pmid">33841024</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75688.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.07.08.451663" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.08.451663"/></front-stub><body><p>This rigorous, carefully designed and executed functional magnetic-resonance imaging study provides compelling evidence against a rigid, fixed model for how working-memory representations are maintained in the human brain. By analyzing patterns and strength of brain activity, the authors show that networks for maintaining contents in mind vary depending on the task demands and foreknowledge of anticipated responses. This manuscript will be of interest to scientists studying working memory, both in humans and in non-human primates.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75688.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>National Institute of Mental Health, National Institutes of Health</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Lorenc</surname><given-names>Elizabeth</given-names></name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Nobre</surname><given-names>Anna Christina</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.08.451663">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.07.08.451663v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Flexible utilization of spatial-and motor-based codes for the storage of visuo-spatial information&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by Chris Baker as the Senior and Reviewing Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Elizabeth Lorenc (Reviewer #1); Anna Christina Nobre (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers think the manuscript addresses an important question and are enthusiastic about many aspects of the study. However, both raise questions that have a potential impact on the interpretation. These concerns should be directly addressed in a revision (see below for reviewers' full comments).</p><p>In particular, please discuss/elaborate:</p><p>1) Why representations do not completely disappear from visual cortex when a button press can be planned in advance (R1).</p><p>2) Whether there may be a contribution of an actual finger movement (R1).</p><p>3) How the multiple differences between the informative x non-informative conditions affect the interpretation of the results (R2).</p><p>4) The potential contribution of anticipated difficulty, arousal, or other nuisance variables (R2).</p><p>5) The distinction between retrospective and prospective codes (R2).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This is a clear manuscript that was enjoyable to read!</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. One study that specifically speaks to motor readiness when responses are linked to items maintained in visual working memory is by van Ede, Chekroud, Stokes and Nobre 2019 Nature Neuroscience.</p><p>2. Reading between the lines, some of the few participants were experienced and perhaps non-naïve to the experimental questions? I think this may be worth stating.</p><p>3. When describing the task (in methods), it would be good to have a summary statement of what the participant's task actually was before stepping through the various events in the trial.</p><p>4. In figure 1, it seems counter-intuitive to place the spatial WM task (b) in between the main task (a) and its related behavioural results. Up to you, but maybe consider swapping order of (b) and (c) to have task and performance together.</p><p>5. A bit more detail on how participants reported the spatial location on the spatial WM mapping would be welcome. Could they correct/tinker with responses? Was there a confirmation response?</p><p>6. Similarly, was instruction for responding in core task about whether it was in 'lighter x darker' half? The text (results) says &quot;to indicate on which of the two halves of the disk the target dot had been presented…&quot; but it falls short of stating how the 2 halves were categorised (i.e., light x dark, counter x clockwise). I may have missed this somewhere.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75688.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers think the manuscript addresses an important question and are enthusiastic about many aspects of the study. However, both raise questions that have a potential impact on the interpretation. These concerns should be directly addressed in a revision (see below for reviewers' full comments).</p><p>In particular, please discuss/elaborate:</p><p>1) Why representations do not completely disappear from visual cortex when a button press can be planned in advance (R1).</p></disp-quote><p>We address this point in our reply to R1 Public Review (major comment 1).</p><disp-quote content-type="editor-comment"><p>2) Whether there may be a contribution of an actual finger movement (R1).</p></disp-quote><p>We address this point in our reply to R1 Public Review (major comment 2).</p><disp-quote content-type="editor-comment"><p>3) How the multiple differences between the informative x non-informative conditions affect the interpretation of the results (R2).</p></disp-quote><p>We address this point in our reply to R2 Public Review (major comment 1).</p><disp-quote content-type="editor-comment"><p>4) The potential contribution of anticipated difficulty, arousal, or other nuisance variables (R2).</p></disp-quote><p>We address this point in our reply to R2 Public Review (major comment 2).</p><disp-quote content-type="editor-comment"><p>5) The distinction between retrospective and prospective codes (R2).</p></disp-quote><p>We address this point in our reply to R2 Public Review (major comment 3).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. One study that specifically speaks to motor readiness when responses are linked to items maintained in visual working memory is by van Ede, Chekroud, Stokes and Nobre 2019 Nature Neuroscience.</p></disp-quote><p>Yes, we agree this study is very relevant to our work. We had cited it only briefly in the original version of our manuscript, and we have now added an additional discussion of the work towards the end of page 14.</p><disp-quote content-type="editor-comment"><p>2. Reading between the lines, some of the few participants were experienced and perhaps non-naïve to the experimental questions? I think this may be worth stating.</p></disp-quote><p>We have added a note about this to our methods section (page 22). All our participants were naïve to the experimental question. Three out of six were lab members who may have guessed at the purpose of the task manipulation, but the experimental question was not disclosed to them until after their participation. The other three participants were not lab members (though they were experienced MRI participants), and unlikely to have given much thought to the experimental question. Our effects were reliable on a single-participant basis across all participants (i.e., see single gray lines in Figure 2B).</p><disp-quote content-type="editor-comment"><p>3. When describing the task (in methods), it would be good to have a summary statement of what the participant's task actually was before stepping through the various events in the trial.</p></disp-quote><p>We have added this to our methods section (page 20).</p><disp-quote content-type="editor-comment"><p>4. In figure 1, it seems counter-intuitive to place the spatial WM task (b) in between the main task (a) and its related behavioural results. Up to you, but maybe consider swapping order of (b) and (c) to have task and performance together.</p></disp-quote><p>Good suggestion. We have switched the order of panels (b) and (c) in Figure 1.</p><disp-quote content-type="editor-comment"><p>5. A bit more detail on how participants reported the spatial location on the spatial WM mapping would be welcome. Could they correct/tinker with responses? Was there a confirmation response?</p></disp-quote><p>We have added some additional details on this task report method to the methods section (page 22). It now states that: “This response period lasted 3 seconds, during which participants were able to move the dot back-and-forth to adjust its position as much as they wished. Once the 3 second response period was over, the probe dot disappeared and participants had no further ability to change their response. The final position of the probe dot was taken as the participant’s response, and no further (visual) feedback of the response was provided. “</p><disp-quote content-type="editor-comment"><p>6. Similarly, was instruction for responding in core task about whether it was in 'lighter x darker' half? The text (results) says &quot;to indicate on which of the two halves of the disk the target dot had been presented…&quot; but it falls short of stating how the 2 halves were categorised (i.e., light x dark, counter x clockwise). I may have missed this somewhere.</p></disp-quote><p>We have added some additional details on this aspect of the task instructions to the methods section (page 21). This now states that: “Following the delay period, a second disk stimulus appeared for 2 seconds, serving as the response probe (i.e., the “response disk”). At this point, participants responded with a button press to indicate which side of the response disk the memory target had been presented on. Specifically, they used either their left or right index finger to indicate whether the target dot fell within the light gray or dark gray side of the response disk. On each scan run, the light gray shade and dark gray shade were each associated with one response finger, and the mapping between shade of gray (light or dark) and finger (left or right index finger) was counter-balanced across sessions within each participant. Participants were reminded of the mapping between shades of gray and response fingers at the start of each scan run.”</p></body></sub-article></article>